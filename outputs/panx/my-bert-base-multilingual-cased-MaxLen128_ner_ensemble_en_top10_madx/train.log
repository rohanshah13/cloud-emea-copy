01/14/2022 16:04:00 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ja', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:04:00 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/14/2022 16:04:00 - INFO - __main__ -   Seed = 1
01/14/2022 16:04:00 - INFO - root -   save model
01/14/2022 16:04:00 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ja', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:04:00 - INFO - __main__ -   Loading pretrained model and tokenizer
01/14/2022 16:04:03 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/14/2022 16:04:08 - INFO - __main__ -   Using lang2id = None
01/14/2022 16:04:08 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/14/2022 16:04:08 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/
01/14/2022 16:04:08 - INFO - root -   Trying to decide if add adapter
01/14/2022 16:04:08 - INFO - root -   loading task adapter
01/14/2022 16:04:08 - INFO - root -   loading lang adpater en/wiki@ukp,pt/wiki@ukp,id/wiki@ukp,cs/wiki@ukp,tr/wiki@ukp,eu/wiki@ukp,zh_yue/wiki@ukp,vi/wiki@ukp,fr/wiki@ukp,ja/wiki@ukp
01/14/2022 16:04:08 - INFO - __main__ -   Adapter Languages : ['en', 'pt', 'id', 'cs', 'tr', 'eu', 'zh_yue', 'vi', 'fr', 'ja'], Length : 10
01/14/2022 16:04:08 - INFO - __main__ -   Adapter Names ['en/wiki@ukp', 'pt/wiki@ukp', 'id/wiki@ukp', 'cs/wiki@ukp', 'tr/wiki@ukp', 'eu/wiki@ukp', 'zh_yue/wiki@ukp', 'vi/wiki@ukp', 'fr/wiki@ukp', 'ja/wiki@ukp'], Length : 10
01/14/2022 16:04:08 - INFO - __main__ -   Language = en
01/14/2022 16:04:08 - INFO - __main__ -   Adapter Name = en/wiki@ukp
01/14/2022 16:04:09 - INFO - __main__ -   Language = pt
01/14/2022 16:04:09 - INFO - __main__ -   Adapter Name = pt/wiki@ukp
01/14/2022 16:04:11 - INFO - __main__ -   Language = id
01/14/2022 16:04:11 - INFO - __main__ -   Adapter Name = id/wiki@ukp
01/14/2022 16:04:13 - INFO - __main__ -   Language = cs
01/14/2022 16:04:13 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
01/14/2022 16:04:16 - INFO - __main__ -   Language = tr
01/14/2022 16:04:16 - INFO - __main__ -   Adapter Name = tr/wiki@ukp
01/14/2022 16:04:18 - INFO - __main__ -   Language = eu
01/14/2022 16:04:18 - INFO - __main__ -   Adapter Name = eu/wiki@ukp
01/14/2022 16:04:20 - INFO - __main__ -   Language = zh_yue
01/14/2022 16:04:20 - INFO - __main__ -   Adapter Name = zh_yue/wiki@ukp
01/14/2022 16:04:22 - INFO - __main__ -   Language = vi
01/14/2022 16:04:22 - INFO - __main__ -   Adapter Name = vi/wiki@ukp
01/14/2022 16:04:25 - INFO - __main__ -   Language = fr
01/14/2022 16:04:25 - INFO - __main__ -   Adapter Name = fr/wiki@ukp
01/14/2022 16:04:27 - INFO - __main__ -   Language = ja
01/14/2022 16:04:27 - INFO - __main__ -   Adapter Name = ja/wiki@ukp
01/14/2022 16:04:32 - INFO - __main__ -   Args Adapter Weight = equal
01/14/2022 16:04:32 - INFO - __main__ -   Adapter Languages = ['en', 'pt', 'id', 'cs', 'tr', 'eu', 'zh_yue', 'vi', 'fr', 'ja']
01/14/2022 16:04:32 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/14/2022 16:04:32 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/14/2022 16:04:32 - INFO - __main__ -   Length of Adapter Weights = 10
01/14/2022 16:04:32 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/cached_test_ja_bert-base-multilingual-cased_128
01/14/2022 16:04:33 - INFO - __main__ -   ***** Running evaluation  in ja *****
01/14/2022 16:04:33 - INFO - __main__ -     Num examples = 10612
01/14/2022 16:04:33 - INFO - __main__ -     Batch size = 32
01/14/2022 16:04:33 - INFO - __main__ -   Batch number = 1
01/14/2022 16:04:33 - INFO - __main__ -   Batch number = 2
01/14/2022 16:04:33 - INFO - __main__ -   Batch number = 3
01/14/2022 16:04:34 - INFO - __main__ -   Batch number = 4
01/14/2022 16:04:34 - INFO - __main__ -   Batch number = 5
01/14/2022 16:04:34 - INFO - __main__ -   Batch number = 6
01/14/2022 16:04:35 - INFO - __main__ -   Batch number = 7
01/14/2022 16:04:35 - INFO - __main__ -   Batch number = 8
01/14/2022 16:04:35 - INFO - __main__ -   Batch number = 9
01/14/2022 16:04:35 - INFO - __main__ -   Batch number = 10
01/14/2022 16:04:36 - INFO - __main__ -   Batch number = 11
01/14/2022 16:04:36 - INFO - __main__ -   Batch number = 12
01/14/2022 16:04:36 - INFO - __main__ -   Batch number = 13
01/14/2022 16:04:37 - INFO - __main__ -   Batch number = 14
01/14/2022 16:04:37 - INFO - __main__ -   Batch number = 15
01/14/2022 16:04:37 - INFO - __main__ -   Batch number = 16
01/14/2022 16:04:37 - INFO - __main__ -   Batch number = 17
01/14/2022 16:04:38 - INFO - __main__ -   Batch number = 18
01/14/2022 16:04:38 - INFO - __main__ -   Batch number = 19
01/14/2022 16:04:38 - INFO - __main__ -   Batch number = 20
01/14/2022 16:04:38 - INFO - __main__ -   Batch number = 21
01/14/2022 16:04:39 - INFO - __main__ -   Batch number = 22
01/14/2022 16:04:39 - INFO - __main__ -   Batch number = 23
01/14/2022 16:04:39 - INFO - __main__ -   Batch number = 24
01/14/2022 16:04:40 - INFO - __main__ -   Batch number = 25
01/14/2022 16:04:40 - INFO - __main__ -   Batch number = 26
01/14/2022 16:04:40 - INFO - __main__ -   Batch number = 27
01/14/2022 16:04:40 - INFO - __main__ -   Batch number = 28
01/14/2022 16:04:41 - INFO - __main__ -   Batch number = 29
01/14/2022 16:04:41 - INFO - __main__ -   Batch number = 30
01/14/2022 16:04:41 - INFO - __main__ -   Batch number = 31
01/14/2022 16:04:42 - INFO - __main__ -   Batch number = 32
01/14/2022 16:04:42 - INFO - __main__ -   Batch number = 33
01/14/2022 16:04:42 - INFO - __main__ -   Batch number = 34
01/14/2022 16:04:42 - INFO - __main__ -   Batch number = 35
01/14/2022 16:04:43 - INFO - __main__ -   Batch number = 36
01/14/2022 16:04:43 - INFO - __main__ -   Batch number = 37
01/14/2022 16:04:43 - INFO - __main__ -   Batch number = 38
01/14/2022 16:04:44 - INFO - __main__ -   Batch number = 39
01/14/2022 16:04:44 - INFO - __main__ -   Batch number = 40
01/14/2022 16:04:44 - INFO - __main__ -   Batch number = 41
01/14/2022 16:04:44 - INFO - __main__ -   Batch number = 42
01/14/2022 16:04:45 - INFO - __main__ -   Batch number = 43
01/14/2022 16:04:45 - INFO - __main__ -   Batch number = 44
01/14/2022 16:04:45 - INFO - __main__ -   Batch number = 45
01/14/2022 16:04:46 - INFO - __main__ -   Batch number = 46
01/14/2022 16:04:46 - INFO - __main__ -   Batch number = 47
01/14/2022 16:04:46 - INFO - __main__ -   Batch number = 48
01/14/2022 16:04:46 - INFO - __main__ -   Batch number = 49
01/14/2022 16:04:47 - INFO - __main__ -   Batch number = 50
01/14/2022 16:04:47 - INFO - __main__ -   Batch number = 51
01/14/2022 16:04:47 - INFO - __main__ -   Batch number = 52
01/14/2022 16:04:48 - INFO - __main__ -   Batch number = 53
01/14/2022 16:04:48 - INFO - __main__ -   Batch number = 54
01/14/2022 16:04:48 - INFO - __main__ -   Batch number = 55
01/14/2022 16:04:48 - INFO - __main__ -   Batch number = 56
01/14/2022 16:04:49 - INFO - __main__ -   Batch number = 57
01/14/2022 16:04:49 - INFO - __main__ -   Batch number = 58
01/14/2022 16:04:49 - INFO - __main__ -   Batch number = 59
01/14/2022 16:04:50 - INFO - __main__ -   Batch number = 60
01/14/2022 16:04:50 - INFO - __main__ -   Batch number = 61
01/14/2022 16:04:50 - INFO - __main__ -   Batch number = 62
01/14/2022 16:04:50 - INFO - __main__ -   Batch number = 63
01/14/2022 16:04:51 - INFO - __main__ -   Batch number = 64
01/14/2022 16:04:51 - INFO - __main__ -   Batch number = 65
01/14/2022 16:04:51 - INFO - __main__ -   Batch number = 66
01/14/2022 16:04:52 - INFO - __main__ -   Batch number = 67
01/14/2022 16:04:52 - INFO - __main__ -   Batch number = 68
01/14/2022 16:04:52 - INFO - __main__ -   Batch number = 69
01/14/2022 16:04:52 - INFO - __main__ -   Batch number = 70
01/14/2022 16:04:53 - INFO - __main__ -   Batch number = 71
01/14/2022 16:04:53 - INFO - __main__ -   Batch number = 72
01/14/2022 16:04:53 - INFO - __main__ -   Batch number = 73
01/14/2022 16:04:54 - INFO - __main__ -   Batch number = 74
01/14/2022 16:04:54 - INFO - __main__ -   Batch number = 75
01/14/2022 16:04:54 - INFO - __main__ -   Batch number = 76
01/14/2022 16:04:54 - INFO - __main__ -   Batch number = 77
01/14/2022 16:04:55 - INFO - __main__ -   Batch number = 78
01/14/2022 16:04:55 - INFO - __main__ -   Batch number = 79
01/14/2022 16:04:55 - INFO - __main__ -   Batch number = 80
01/14/2022 16:04:56 - INFO - __main__ -   Batch number = 81
01/14/2022 16:04:56 - INFO - __main__ -   Batch number = 82
01/14/2022 16:04:56 - INFO - __main__ -   Batch number = 83
01/14/2022 16:04:56 - INFO - __main__ -   Batch number = 84
01/14/2022 16:04:57 - INFO - __main__ -   Batch number = 85
01/14/2022 16:04:57 - INFO - __main__ -   Batch number = 86
01/14/2022 16:04:57 - INFO - __main__ -   Batch number = 87
01/14/2022 16:04:58 - INFO - __main__ -   Batch number = 88
01/14/2022 16:04:58 - INFO - __main__ -   Batch number = 89
01/14/2022 16:04:58 - INFO - __main__ -   Batch number = 90
01/14/2022 16:04:58 - INFO - __main__ -   Batch number = 91
01/14/2022 16:04:59 - INFO - __main__ -   Batch number = 92
01/14/2022 16:04:59 - INFO - __main__ -   Batch number = 93
01/14/2022 16:04:59 - INFO - __main__ -   Batch number = 94
01/14/2022 16:05:00 - INFO - __main__ -   Batch number = 95
01/14/2022 16:05:00 - INFO - __main__ -   Batch number = 96
01/14/2022 16:05:00 - INFO - __main__ -   Batch number = 97
01/14/2022 16:05:00 - INFO - __main__ -   Batch number = 98
01/14/2022 16:05:01 - INFO - __main__ -   Batch number = 99
01/14/2022 16:05:01 - INFO - __main__ -   Batch number = 100
01/14/2022 16:05:01 - INFO - __main__ -   Batch number = 101
01/14/2022 16:05:02 - INFO - __main__ -   Batch number = 102
01/14/2022 16:05:02 - INFO - __main__ -   Batch number = 103
01/14/2022 16:05:02 - INFO - __main__ -   Batch number = 104
01/14/2022 16:05:02 - INFO - __main__ -   Batch number = 105
01/14/2022 16:05:03 - INFO - __main__ -   Batch number = 106
01/14/2022 16:05:03 - INFO - __main__ -   Batch number = 107
01/14/2022 16:05:03 - INFO - __main__ -   Batch number = 108
01/14/2022 16:05:04 - INFO - __main__ -   Batch number = 109
01/14/2022 16:05:04 - INFO - __main__ -   Batch number = 110
01/14/2022 16:05:04 - INFO - __main__ -   Batch number = 111
01/14/2022 16:05:04 - INFO - __main__ -   Batch number = 112
01/14/2022 16:05:05 - INFO - __main__ -   Batch number = 113
01/14/2022 16:05:05 - INFO - __main__ -   Batch number = 114
01/14/2022 16:05:05 - INFO - __main__ -   Batch number = 115
01/14/2022 16:05:06 - INFO - __main__ -   Batch number = 116
01/14/2022 16:05:06 - INFO - __main__ -   Batch number = 117
01/14/2022 16:05:06 - INFO - __main__ -   Batch number = 118
01/14/2022 16:05:06 - INFO - __main__ -   Batch number = 119
01/14/2022 16:05:07 - INFO - __main__ -   Batch number = 120
01/14/2022 16:05:07 - INFO - __main__ -   Batch number = 121
01/14/2022 16:05:07 - INFO - __main__ -   Batch number = 122
01/14/2022 16:05:08 - INFO - __main__ -   Batch number = 123
01/14/2022 16:05:08 - INFO - __main__ -   Batch number = 124
01/14/2022 16:05:08 - INFO - __main__ -   Batch number = 125
01/14/2022 16:05:08 - INFO - __main__ -   Batch number = 126
01/14/2022 16:05:09 - INFO - __main__ -   Batch number = 127
01/14/2022 16:05:09 - INFO - __main__ -   Batch number = 128
01/14/2022 16:05:09 - INFO - __main__ -   Batch number = 129
01/14/2022 16:05:10 - INFO - __main__ -   Batch number = 130
01/14/2022 16:05:10 - INFO - __main__ -   Batch number = 131
01/14/2022 16:05:10 - INFO - __main__ -   Batch number = 132
01/14/2022 16:05:10 - INFO - __main__ -   Batch number = 133
01/14/2022 16:05:11 - INFO - __main__ -   Batch number = 134
01/14/2022 16:05:11 - INFO - __main__ -   Batch number = 135
01/14/2022 16:05:11 - INFO - __main__ -   Batch number = 136
01/14/2022 16:05:12 - INFO - __main__ -   Batch number = 137
01/14/2022 16:05:12 - INFO - __main__ -   Batch number = 138
01/14/2022 16:05:12 - INFO - __main__ -   Batch number = 139
01/14/2022 16:05:13 - INFO - __main__ -   Batch number = 140
01/14/2022 16:05:13 - INFO - __main__ -   Batch number = 141
01/14/2022 16:05:13 - INFO - __main__ -   Batch number = 142
01/14/2022 16:05:13 - INFO - __main__ -   Batch number = 143
01/14/2022 16:05:14 - INFO - __main__ -   Batch number = 144
01/14/2022 16:05:14 - INFO - __main__ -   Batch number = 145
01/14/2022 16:05:14 - INFO - __main__ -   Batch number = 146
01/14/2022 16:05:15 - INFO - __main__ -   Batch number = 147
01/14/2022 16:05:15 - INFO - __main__ -   Batch number = 148
01/14/2022 16:05:15 - INFO - __main__ -   Batch number = 149
01/14/2022 16:05:16 - INFO - __main__ -   Batch number = 150
01/14/2022 16:05:16 - INFO - __main__ -   Batch number = 151
01/14/2022 16:05:16 - INFO - __main__ -   Batch number = 152
01/14/2022 16:05:16 - INFO - __main__ -   Batch number = 153
01/14/2022 16:05:17 - INFO - __main__ -   Batch number = 154
01/14/2022 16:05:17 - INFO - __main__ -   Batch number = 155
01/14/2022 16:05:17 - INFO - __main__ -   Batch number = 156
01/14/2022 16:05:18 - INFO - __main__ -   Batch number = 157
01/14/2022 16:05:18 - INFO - __main__ -   Batch number = 158
01/14/2022 16:05:18 - INFO - __main__ -   Batch number = 159
01/14/2022 16:05:18 - INFO - __main__ -   Batch number = 160
01/14/2022 16:05:19 - INFO - __main__ -   Batch number = 161
01/14/2022 16:05:19 - INFO - __main__ -   Batch number = 162
01/14/2022 16:05:19 - INFO - __main__ -   Batch number = 163
01/14/2022 16:05:20 - INFO - __main__ -   Batch number = 164
01/14/2022 16:05:20 - INFO - __main__ -   Batch number = 165
01/14/2022 16:05:20 - INFO - __main__ -   Batch number = 166
01/14/2022 16:05:21 - INFO - __main__ -   Batch number = 167
01/14/2022 16:05:21 - INFO - __main__ -   Batch number = 168
01/14/2022 16:05:21 - INFO - __main__ -   Batch number = 169
01/14/2022 16:05:22 - INFO - __main__ -   Batch number = 170
01/14/2022 16:05:22 - INFO - __main__ -   Batch number = 171
01/14/2022 16:05:22 - INFO - __main__ -   Batch number = 172
01/14/2022 16:05:23 - INFO - __main__ -   Batch number = 173
01/14/2022 16:05:23 - INFO - __main__ -   Batch number = 174
01/14/2022 16:05:23 - INFO - __main__ -   Batch number = 175
01/14/2022 16:05:23 - INFO - __main__ -   Batch number = 176
01/14/2022 16:05:24 - INFO - __main__ -   Batch number = 177
01/14/2022 16:05:24 - INFO - __main__ -   Batch number = 178
01/14/2022 16:05:24 - INFO - __main__ -   Batch number = 179
01/14/2022 16:05:25 - INFO - __main__ -   Batch number = 180
01/14/2022 16:05:25 - INFO - __main__ -   Batch number = 181
01/14/2022 16:05:25 - INFO - __main__ -   Batch number = 182
01/14/2022 16:05:26 - INFO - __main__ -   Batch number = 183
01/14/2022 16:05:26 - INFO - __main__ -   Batch number = 184
01/14/2022 16:05:26 - INFO - __main__ -   Batch number = 185
01/14/2022 16:05:27 - INFO - __main__ -   Batch number = 186
01/14/2022 16:05:27 - INFO - __main__ -   Batch number = 187
01/14/2022 16:05:27 - INFO - __main__ -   Batch number = 188
01/14/2022 16:05:27 - INFO - __main__ -   Batch number = 189
01/14/2022 16:05:28 - INFO - __main__ -   Batch number = 190
01/14/2022 16:05:28 - INFO - __main__ -   Batch number = 191
01/14/2022 16:05:28 - INFO - __main__ -   Batch number = 192
01/14/2022 16:05:29 - INFO - __main__ -   Batch number = 193
01/14/2022 16:05:29 - INFO - __main__ -   Batch number = 194
01/14/2022 16:05:29 - INFO - __main__ -   Batch number = 195
01/14/2022 16:05:30 - INFO - __main__ -   Batch number = 196
01/14/2022 16:05:30 - INFO - __main__ -   Batch number = 197
01/14/2022 16:05:30 - INFO - __main__ -   Batch number = 198
01/14/2022 16:05:31 - INFO - __main__ -   Batch number = 199
01/14/2022 16:05:31 - INFO - __main__ -   Batch number = 200
01/14/2022 16:05:31 - INFO - __main__ -   Batch number = 201
01/14/2022 16:05:31 - INFO - __main__ -   Batch number = 202
01/14/2022 16:05:32 - INFO - __main__ -   Batch number = 203
01/14/2022 16:05:32 - INFO - __main__ -   Batch number = 204
01/14/2022 16:05:32 - INFO - __main__ -   Batch number = 205
01/14/2022 16:05:33 - INFO - __main__ -   Batch number = 206
01/14/2022 16:05:33 - INFO - __main__ -   Batch number = 207
01/14/2022 16:05:33 - INFO - __main__ -   Batch number = 208
01/14/2022 16:05:34 - INFO - __main__ -   Batch number = 209
01/14/2022 16:05:34 - INFO - __main__ -   Batch number = 210
01/14/2022 16:05:34 - INFO - __main__ -   Batch number = 211
01/14/2022 16:05:35 - INFO - __main__ -   Batch number = 212
01/14/2022 16:05:35 - INFO - __main__ -   Batch number = 213
01/14/2022 16:05:35 - INFO - __main__ -   Batch number = 214
01/14/2022 16:05:35 - INFO - __main__ -   Batch number = 215
01/14/2022 16:05:36 - INFO - __main__ -   Batch number = 216
01/14/2022 16:05:36 - INFO - __main__ -   Batch number = 217
01/14/2022 16:05:36 - INFO - __main__ -   Batch number = 218
01/14/2022 16:05:37 - INFO - __main__ -   Batch number = 219
01/14/2022 16:05:37 - INFO - __main__ -   Batch number = 220
01/14/2022 16:05:37 - INFO - __main__ -   Batch number = 221
01/14/2022 16:05:38 - INFO - __main__ -   Batch number = 222
01/14/2022 16:05:38 - INFO - __main__ -   Batch number = 223
01/14/2022 16:05:38 - INFO - __main__ -   Batch number = 224
01/14/2022 16:05:39 - INFO - __main__ -   Batch number = 225
01/14/2022 16:05:39 - INFO - __main__ -   Batch number = 226
01/14/2022 16:05:39 - INFO - __main__ -   Batch number = 227
01/14/2022 16:05:40 - INFO - __main__ -   Batch number = 228
01/14/2022 16:05:40 - INFO - __main__ -   Batch number = 229
01/14/2022 16:05:40 - INFO - __main__ -   Batch number = 230
01/14/2022 16:05:40 - INFO - __main__ -   Batch number = 231
01/14/2022 16:05:41 - INFO - __main__ -   Batch number = 232
01/14/2022 16:05:41 - INFO - __main__ -   Batch number = 233
01/14/2022 16:05:41 - INFO - __main__ -   Batch number = 234
01/14/2022 16:05:42 - INFO - __main__ -   Batch number = 235
01/14/2022 16:05:42 - INFO - __main__ -   Batch number = 236
01/14/2022 16:05:42 - INFO - __main__ -   Batch number = 237
01/14/2022 16:05:43 - INFO - __main__ -   Batch number = 238
01/14/2022 16:05:43 - INFO - __main__ -   Batch number = 239
01/14/2022 16:05:43 - INFO - __main__ -   Batch number = 240
01/14/2022 16:05:43 - INFO - __main__ -   Batch number = 241
01/14/2022 16:05:44 - INFO - __main__ -   Batch number = 242
01/14/2022 16:05:44 - INFO - __main__ -   Batch number = 243
01/14/2022 16:05:44 - INFO - __main__ -   Batch number = 244
01/14/2022 16:05:45 - INFO - __main__ -   Batch number = 245
01/14/2022 16:05:45 - INFO - __main__ -   Batch number = 246
01/14/2022 16:05:45 - INFO - __main__ -   Batch number = 247
01/14/2022 16:05:46 - INFO - __main__ -   Batch number = 248
01/14/2022 16:05:46 - INFO - __main__ -   Batch number = 249
01/14/2022 16:05:46 - INFO - __main__ -   Batch number = 250
01/14/2022 16:05:47 - INFO - __main__ -   Batch number = 251
01/14/2022 16:05:47 - INFO - __main__ -   Batch number = 252
01/14/2022 16:05:47 - INFO - __main__ -   Batch number = 253
01/14/2022 16:05:48 - INFO - __main__ -   Batch number = 254
01/14/2022 16:05:48 - INFO - __main__ -   Batch number = 255
01/14/2022 16:05:48 - INFO - __main__ -   Batch number = 256
01/14/2022 16:05:48 - INFO - __main__ -   Batch number = 257
01/14/2022 16:05:49 - INFO - __main__ -   Batch number = 258
01/14/2022 16:05:49 - INFO - __main__ -   Batch number = 259
01/14/2022 16:05:49 - INFO - __main__ -   Batch number = 260
01/14/2022 16:05:50 - INFO - __main__ -   Batch number = 261
01/14/2022 16:05:50 - INFO - __main__ -   Batch number = 262
01/14/2022 16:05:50 - INFO - __main__ -   Batch number = 263
01/14/2022 16:05:51 - INFO - __main__ -   Batch number = 264
01/14/2022 16:05:51 - INFO - __main__ -   Batch number = 265
01/14/2022 16:05:51 - INFO - __main__ -   Batch number = 266
01/14/2022 16:05:52 - INFO - __main__ -   Batch number = 267
01/14/2022 16:05:52 - INFO - __main__ -   Batch number = 268
01/14/2022 16:05:52 - INFO - __main__ -   Batch number = 269
01/14/2022 16:05:52 - INFO - __main__ -   Batch number = 270
01/14/2022 16:05:53 - INFO - __main__ -   Batch number = 271
01/14/2022 16:05:53 - INFO - __main__ -   Batch number = 272
01/14/2022 16:05:53 - INFO - __main__ -   Batch number = 273
01/14/2022 16:05:54 - INFO - __main__ -   Batch number = 274
01/14/2022 16:05:54 - INFO - __main__ -   Batch number = 275
01/14/2022 16:05:54 - INFO - __main__ -   Batch number = 276
01/14/2022 16:05:55 - INFO - __main__ -   Batch number = 277
01/14/2022 16:05:55 - INFO - __main__ -   Batch number = 278
01/14/2022 16:05:55 - INFO - __main__ -   Batch number = 279
01/14/2022 16:05:55 - INFO - __main__ -   Batch number = 280
01/14/2022 16:05:56 - INFO - __main__ -   Batch number = 281
01/14/2022 16:05:56 - INFO - __main__ -   Batch number = 282
01/14/2022 16:05:56 - INFO - __main__ -   Batch number = 283
01/14/2022 16:05:57 - INFO - __main__ -   Batch number = 284
01/14/2022 16:05:57 - INFO - __main__ -   Batch number = 285
01/14/2022 16:05:57 - INFO - __main__ -   Batch number = 286
01/14/2022 16:05:58 - INFO - __main__ -   Batch number = 287
01/14/2022 16:05:58 - INFO - __main__ -   Batch number = 288
01/14/2022 16:05:58 - INFO - __main__ -   Batch number = 289
01/14/2022 16:05:59 - INFO - __main__ -   Batch number = 290
01/14/2022 16:05:59 - INFO - __main__ -   Batch number = 291
01/14/2022 16:05:59 - INFO - __main__ -   Batch number = 292
01/14/2022 16:05:59 - INFO - __main__ -   Batch number = 293
01/14/2022 16:06:00 - INFO - __main__ -   Batch number = 294
01/14/2022 16:06:00 - INFO - __main__ -   Batch number = 295
01/14/2022 16:06:00 - INFO - __main__ -   Batch number = 296
01/14/2022 16:06:01 - INFO - __main__ -   Batch number = 297
01/14/2022 16:06:01 - INFO - __main__ -   Batch number = 298
01/14/2022 16:06:01 - INFO - __main__ -   Batch number = 299
01/14/2022 16:06:02 - INFO - __main__ -   Batch number = 300
01/14/2022 16:06:02 - INFO - __main__ -   Batch number = 301
01/14/2022 16:06:02 - INFO - __main__ -   Batch number = 302
01/14/2022 16:06:03 - INFO - __main__ -   Batch number = 303
01/14/2022 16:06:03 - INFO - __main__ -   Batch number = 304
01/14/2022 16:06:03 - INFO - __main__ -   Batch number = 305
01/14/2022 16:06:03 - INFO - __main__ -   Batch number = 306
01/14/2022 16:06:04 - INFO - __main__ -   Batch number = 307
01/14/2022 16:06:04 - INFO - __main__ -   Batch number = 308
01/14/2022 16:06:04 - INFO - __main__ -   Batch number = 309
01/14/2022 16:06:05 - INFO - __main__ -   Batch number = 310
01/14/2022 16:06:05 - INFO - __main__ -   Batch number = 311
01/14/2022 16:06:05 - INFO - __main__ -   Batch number = 312
01/14/2022 16:06:05 - INFO - __main__ -   Batch number = 313
01/14/2022 16:06:06 - INFO - __main__ -   Batch number = 314
01/14/2022 16:06:06 - INFO - __main__ -   Batch number = 315
01/14/2022 16:06:06 - INFO - __main__ -   Batch number = 316
01/14/2022 16:06:07 - INFO - __main__ -   Batch number = 317
01/14/2022 16:06:07 - INFO - __main__ -   Batch number = 318
01/14/2022 16:06:07 - INFO - __main__ -   Batch number = 319
01/14/2022 16:06:08 - INFO - __main__ -   Batch number = 320
01/14/2022 16:06:08 - INFO - __main__ -   Batch number = 321
01/14/2022 16:06:08 - INFO - __main__ -   Batch number = 322
01/14/2022 16:06:09 - INFO - __main__ -   Batch number = 323
01/14/2022 16:06:09 - INFO - __main__ -   Batch number = 324
01/14/2022 16:06:09 - INFO - __main__ -   Batch number = 325
01/14/2022 16:06:09 - INFO - __main__ -   Batch number = 326
01/14/2022 16:06:10 - INFO - __main__ -   Batch number = 327
01/14/2022 16:06:10 - INFO - __main__ -   Batch number = 328
01/14/2022 16:06:11 - INFO - __main__ -   Batch number = 329
01/14/2022 16:06:11 - INFO - __main__ -   Batch number = 330
01/14/2022 16:06:11 - INFO - __main__ -   Batch number = 331
01/14/2022 16:06:12 - INFO - __main__ -   Batch number = 332
01/14/2022 16:06:15 - INFO - __main__ -   ***** Evaluation result  in ja *****
01/14/2022 16:06:15 - INFO - __main__ -     f1 = 0.20212637136421982
01/14/2022 16:06:15 - INFO - __main__ -     loss = 3.8501403769814826
01/14/2022 16:06:15 - INFO - __main__ -     precision = 0.15298042378473495
01/14/2022 16:06:15 - INFO - __main__ -     recall = 0.2977949047313209
01/14/2022 16:06:18 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ja', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:06:18 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/14/2022 16:06:18 - INFO - __main__ -   Seed = 2
01/14/2022 16:06:18 - INFO - root -   save model
01/14/2022 16:06:18 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ja', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:06:18 - INFO - __main__ -   Loading pretrained model and tokenizer
01/14/2022 16:06:21 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/14/2022 16:06:26 - INFO - __main__ -   Using lang2id = None
01/14/2022 16:06:26 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/14/2022 16:06:26 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/
01/14/2022 16:06:26 - INFO - root -   Trying to decide if add adapter
01/14/2022 16:06:26 - INFO - root -   loading task adapter
01/14/2022 16:06:26 - INFO - root -   loading lang adpater en/wiki@ukp,pt/wiki@ukp,id/wiki@ukp,tr/wiki@ukp,cs/wiki@ukp,vi/wiki@ukp,eu/wiki@ukp,fa/wiki@ukp,zh_yue/wiki@ukp,ja/wiki@ukp
01/14/2022 16:06:26 - INFO - __main__ -   Adapter Languages : ['en', 'pt', 'id', 'tr', 'cs', 'vi', 'eu', 'fa', 'zh_yue', 'ja'], Length : 10
01/14/2022 16:06:26 - INFO - __main__ -   Adapter Names ['en/wiki@ukp', 'pt/wiki@ukp', 'id/wiki@ukp', 'tr/wiki@ukp', 'cs/wiki@ukp', 'vi/wiki@ukp', 'eu/wiki@ukp', 'fa/wiki@ukp', 'zh_yue/wiki@ukp', 'ja/wiki@ukp'], Length : 10
01/14/2022 16:06:26 - INFO - __main__ -   Language = en
01/14/2022 16:06:26 - INFO - __main__ -   Adapter Name = en/wiki@ukp
01/14/2022 16:06:28 - INFO - __main__ -   Language = pt
01/14/2022 16:06:28 - INFO - __main__ -   Adapter Name = pt/wiki@ukp
01/14/2022 16:06:30 - INFO - __main__ -   Language = id
01/14/2022 16:06:30 - INFO - __main__ -   Adapter Name = id/wiki@ukp
01/14/2022 16:06:32 - INFO - __main__ -   Language = tr
01/14/2022 16:06:32 - INFO - __main__ -   Adapter Name = tr/wiki@ukp
01/14/2022 16:06:34 - INFO - __main__ -   Language = cs
01/14/2022 16:06:34 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
01/14/2022 16:06:36 - INFO - __main__ -   Language = vi
01/14/2022 16:06:36 - INFO - __main__ -   Adapter Name = vi/wiki@ukp
01/14/2022 16:06:37 - INFO - __main__ -   Language = eu
01/14/2022 16:06:37 - INFO - __main__ -   Adapter Name = eu/wiki@ukp
01/14/2022 16:06:39 - INFO - __main__ -   Language = fa
01/14/2022 16:06:39 - INFO - __main__ -   Adapter Name = fa/wiki@ukp
01/14/2022 16:06:41 - INFO - __main__ -   Language = zh_yue
01/14/2022 16:06:41 - INFO - __main__ -   Adapter Name = zh_yue/wiki@ukp
01/14/2022 16:06:43 - INFO - __main__ -   Language = ja
01/14/2022 16:06:43 - INFO - __main__ -   Adapter Name = ja/wiki@ukp
01/14/2022 16:06:47 - INFO - __main__ -   Args Adapter Weight = equal
01/14/2022 16:06:47 - INFO - __main__ -   Adapter Languages = ['en', 'pt', 'id', 'tr', 'cs', 'vi', 'eu', 'fa', 'zh_yue', 'ja']
01/14/2022 16:06:47 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/14/2022 16:06:47 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/14/2022 16:06:47 - INFO - __main__ -   Length of Adapter Weights = 10
01/14/2022 16:06:47 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/cached_test_ja_bert-base-multilingual-cased_128
01/14/2022 16:06:48 - INFO - __main__ -   ***** Running evaluation  in ja *****
01/14/2022 16:06:48 - INFO - __main__ -     Num examples = 10612
01/14/2022 16:06:48 - INFO - __main__ -     Batch size = 32
01/14/2022 16:06:49 - INFO - __main__ -   Batch number = 1
01/14/2022 16:06:49 - INFO - __main__ -   Batch number = 2
01/14/2022 16:06:49 - INFO - __main__ -   Batch number = 3
01/14/2022 16:06:49 - INFO - __main__ -   Batch number = 4
01/14/2022 16:06:50 - INFO - __main__ -   Batch number = 5
01/14/2022 16:06:50 - INFO - __main__ -   Batch number = 6
01/14/2022 16:06:50 - INFO - __main__ -   Batch number = 7
01/14/2022 16:06:51 - INFO - __main__ -   Batch number = 8
01/14/2022 16:06:51 - INFO - __main__ -   Batch number = 9
01/14/2022 16:06:51 - INFO - __main__ -   Batch number = 10
01/14/2022 16:06:52 - INFO - __main__ -   Batch number = 11
01/14/2022 16:06:52 - INFO - __main__ -   Batch number = 12
01/14/2022 16:06:52 - INFO - __main__ -   Batch number = 13
01/14/2022 16:06:52 - INFO - __main__ -   Batch number = 14
01/14/2022 16:06:53 - INFO - __main__ -   Batch number = 15
01/14/2022 16:06:53 - INFO - __main__ -   Batch number = 16
01/14/2022 16:06:53 - INFO - __main__ -   Batch number = 17
01/14/2022 16:06:53 - INFO - __main__ -   Batch number = 18
01/14/2022 16:06:54 - INFO - __main__ -   Batch number = 19
01/14/2022 16:06:54 - INFO - __main__ -   Batch number = 20
01/14/2022 16:06:54 - INFO - __main__ -   Batch number = 21
01/14/2022 16:06:55 - INFO - __main__ -   Batch number = 22
01/14/2022 16:06:55 - INFO - __main__ -   Batch number = 23
01/14/2022 16:06:55 - INFO - __main__ -   Batch number = 24
01/14/2022 16:06:56 - INFO - __main__ -   Batch number = 25
01/14/2022 16:06:56 - INFO - __main__ -   Batch number = 26
01/14/2022 16:06:56 - INFO - __main__ -   Batch number = 27
01/14/2022 16:06:56 - INFO - __main__ -   Batch number = 28
01/14/2022 16:06:57 - INFO - __main__ -   Batch number = 29
01/14/2022 16:06:57 - INFO - __main__ -   Batch number = 30
01/14/2022 16:06:57 - INFO - __main__ -   Batch number = 31
01/14/2022 16:06:58 - INFO - __main__ -   Batch number = 32
01/14/2022 16:06:58 - INFO - __main__ -   Batch number = 33
01/14/2022 16:06:58 - INFO - __main__ -   Batch number = 34
01/14/2022 16:06:58 - INFO - __main__ -   Batch number = 35
01/14/2022 16:06:59 - INFO - __main__ -   Batch number = 36
01/14/2022 16:06:59 - INFO - __main__ -   Batch number = 37
01/14/2022 16:06:59 - INFO - __main__ -   Batch number = 38
01/14/2022 16:07:00 - INFO - __main__ -   Batch number = 39
01/14/2022 16:07:00 - INFO - __main__ -   Batch number = 40
01/14/2022 16:07:00 - INFO - __main__ -   Batch number = 41
01/14/2022 16:07:00 - INFO - __main__ -   Batch number = 42
01/14/2022 16:07:01 - INFO - __main__ -   Batch number = 43
01/14/2022 16:07:01 - INFO - __main__ -   Batch number = 44
01/14/2022 16:07:01 - INFO - __main__ -   Batch number = 45
01/14/2022 16:07:02 - INFO - __main__ -   Batch number = 46
01/14/2022 16:07:02 - INFO - __main__ -   Batch number = 47
01/14/2022 16:07:02 - INFO - __main__ -   Batch number = 48
01/14/2022 16:07:02 - INFO - __main__ -   Batch number = 49
01/14/2022 16:07:03 - INFO - __main__ -   Batch number = 50
01/14/2022 16:07:03 - INFO - __main__ -   Batch number = 51
01/14/2022 16:07:03 - INFO - __main__ -   Batch number = 52
01/14/2022 16:07:04 - INFO - __main__ -   Batch number = 53
01/14/2022 16:07:04 - INFO - __main__ -   Batch number = 54
01/14/2022 16:07:04 - INFO - __main__ -   Batch number = 55
01/14/2022 16:07:04 - INFO - __main__ -   Batch number = 56
01/14/2022 16:07:05 - INFO - __main__ -   Batch number = 57
01/14/2022 16:07:05 - INFO - __main__ -   Batch number = 58
01/14/2022 16:07:05 - INFO - __main__ -   Batch number = 59
01/14/2022 16:07:06 - INFO - __main__ -   Batch number = 60
01/14/2022 16:07:06 - INFO - __main__ -   Batch number = 61
01/14/2022 16:07:06 - INFO - __main__ -   Batch number = 62
01/14/2022 16:07:06 - INFO - __main__ -   Batch number = 63
01/14/2022 16:07:07 - INFO - __main__ -   Batch number = 64
01/14/2022 16:07:07 - INFO - __main__ -   Batch number = 65
01/14/2022 16:07:07 - INFO - __main__ -   Batch number = 66
01/14/2022 16:07:08 - INFO - __main__ -   Batch number = 67
01/14/2022 16:07:08 - INFO - __main__ -   Batch number = 68
01/14/2022 16:07:08 - INFO - __main__ -   Batch number = 69
01/14/2022 16:07:08 - INFO - __main__ -   Batch number = 70
01/14/2022 16:07:09 - INFO - __main__ -   Batch number = 71
01/14/2022 16:07:09 - INFO - __main__ -   Batch number = 72
01/14/2022 16:07:09 - INFO - __main__ -   Batch number = 73
01/14/2022 16:07:10 - INFO - __main__ -   Batch number = 74
01/14/2022 16:07:10 - INFO - __main__ -   Batch number = 75
01/14/2022 16:07:10 - INFO - __main__ -   Batch number = 76
01/14/2022 16:07:11 - INFO - __main__ -   Batch number = 77
01/14/2022 16:07:11 - INFO - __main__ -   Batch number = 78
01/14/2022 16:07:11 - INFO - __main__ -   Batch number = 79
01/14/2022 16:07:11 - INFO - __main__ -   Batch number = 80
01/14/2022 16:07:12 - INFO - __main__ -   Batch number = 81
01/14/2022 16:07:12 - INFO - __main__ -   Batch number = 82
01/14/2022 16:07:12 - INFO - __main__ -   Batch number = 83
01/14/2022 16:07:13 - INFO - __main__ -   Batch number = 84
01/14/2022 16:07:13 - INFO - __main__ -   Batch number = 85
01/14/2022 16:07:13 - INFO - __main__ -   Batch number = 86
01/14/2022 16:07:13 - INFO - __main__ -   Batch number = 87
01/14/2022 16:07:14 - INFO - __main__ -   Batch number = 88
01/14/2022 16:07:14 - INFO - __main__ -   Batch number = 89
01/14/2022 16:07:14 - INFO - __main__ -   Batch number = 90
01/14/2022 16:07:15 - INFO - __main__ -   Batch number = 91
01/14/2022 16:07:15 - INFO - __main__ -   Batch number = 92
01/14/2022 16:07:15 - INFO - __main__ -   Batch number = 93
01/14/2022 16:07:16 - INFO - __main__ -   Batch number = 94
01/14/2022 16:07:16 - INFO - __main__ -   Batch number = 95
01/14/2022 16:07:16 - INFO - __main__ -   Batch number = 96
01/14/2022 16:07:16 - INFO - __main__ -   Batch number = 97
01/14/2022 16:07:17 - INFO - __main__ -   Batch number = 98
01/14/2022 16:07:17 - INFO - __main__ -   Batch number = 99
01/14/2022 16:07:17 - INFO - __main__ -   Batch number = 100
01/14/2022 16:07:18 - INFO - __main__ -   Batch number = 101
01/14/2022 16:07:18 - INFO - __main__ -   Batch number = 102
01/14/2022 16:07:18 - INFO - __main__ -   Batch number = 103
01/14/2022 16:07:19 - INFO - __main__ -   Batch number = 104
01/14/2022 16:07:19 - INFO - __main__ -   Batch number = 105
01/14/2022 16:07:19 - INFO - __main__ -   Batch number = 106
01/14/2022 16:07:20 - INFO - __main__ -   Batch number = 107
01/14/2022 16:07:20 - INFO - __main__ -   Batch number = 108
01/14/2022 16:07:20 - INFO - __main__ -   Batch number = 109
01/14/2022 16:07:21 - INFO - __main__ -   Batch number = 110
01/14/2022 16:07:21 - INFO - __main__ -   Batch number = 111
01/14/2022 16:07:21 - INFO - __main__ -   Batch number = 112
01/14/2022 16:07:21 - INFO - __main__ -   Batch number = 113
01/14/2022 16:07:22 - INFO - __main__ -   Batch number = 114
01/14/2022 16:07:22 - INFO - __main__ -   Batch number = 115
01/14/2022 16:07:22 - INFO - __main__ -   Batch number = 116
01/14/2022 16:07:23 - INFO - __main__ -   Batch number = 117
01/14/2022 16:07:23 - INFO - __main__ -   Batch number = 118
01/14/2022 16:07:23 - INFO - __main__ -   Batch number = 119
01/14/2022 16:07:24 - INFO - __main__ -   Batch number = 120
01/14/2022 16:07:24 - INFO - __main__ -   Batch number = 121
01/14/2022 16:07:24 - INFO - __main__ -   Batch number = 122
01/14/2022 16:07:24 - INFO - __main__ -   Batch number = 123
01/14/2022 16:07:25 - INFO - __main__ -   Batch number = 124
01/14/2022 16:07:25 - INFO - __main__ -   Batch number = 125
01/14/2022 16:07:25 - INFO - __main__ -   Batch number = 126
01/14/2022 16:07:26 - INFO - __main__ -   Batch number = 127
01/14/2022 16:07:26 - INFO - __main__ -   Batch number = 128
01/14/2022 16:07:26 - INFO - __main__ -   Batch number = 129
01/14/2022 16:07:27 - INFO - __main__ -   Batch number = 130
01/14/2022 16:07:27 - INFO - __main__ -   Batch number = 131
01/14/2022 16:07:27 - INFO - __main__ -   Batch number = 132
01/14/2022 16:07:27 - INFO - __main__ -   Batch number = 133
01/14/2022 16:07:28 - INFO - __main__ -   Batch number = 134
01/14/2022 16:07:28 - INFO - __main__ -   Batch number = 135
01/14/2022 16:07:28 - INFO - __main__ -   Batch number = 136
01/14/2022 16:07:29 - INFO - __main__ -   Batch number = 137
01/14/2022 16:07:29 - INFO - __main__ -   Batch number = 138
01/14/2022 16:07:29 - INFO - __main__ -   Batch number = 139
01/14/2022 16:07:30 - INFO - __main__ -   Batch number = 140
01/14/2022 16:07:30 - INFO - __main__ -   Batch number = 141
01/14/2022 16:07:30 - INFO - __main__ -   Batch number = 142
01/14/2022 16:07:31 - INFO - __main__ -   Batch number = 143
01/14/2022 16:07:31 - INFO - __main__ -   Batch number = 144
01/14/2022 16:07:31 - INFO - __main__ -   Batch number = 145
01/14/2022 16:07:32 - INFO - __main__ -   Batch number = 146
01/14/2022 16:07:32 - INFO - __main__ -   Batch number = 147
01/14/2022 16:07:32 - INFO - __main__ -   Batch number = 148
01/14/2022 16:07:32 - INFO - __main__ -   Batch number = 149
01/14/2022 16:07:33 - INFO - __main__ -   Batch number = 150
01/14/2022 16:07:33 - INFO - __main__ -   Batch number = 151
01/14/2022 16:07:33 - INFO - __main__ -   Batch number = 152
01/14/2022 16:07:34 - INFO - __main__ -   Batch number = 153
01/14/2022 16:07:34 - INFO - __main__ -   Batch number = 154
01/14/2022 16:07:34 - INFO - __main__ -   Batch number = 155
01/14/2022 16:07:35 - INFO - __main__ -   Batch number = 156
01/14/2022 16:07:35 - INFO - __main__ -   Batch number = 157
01/14/2022 16:07:35 - INFO - __main__ -   Batch number = 158
01/14/2022 16:07:36 - INFO - __main__ -   Batch number = 159
01/14/2022 16:07:36 - INFO - __main__ -   Batch number = 160
01/14/2022 16:07:36 - INFO - __main__ -   Batch number = 161
01/14/2022 16:07:37 - INFO - __main__ -   Batch number = 162
01/14/2022 16:07:37 - INFO - __main__ -   Batch number = 163
01/14/2022 16:07:37 - INFO - __main__ -   Batch number = 164
01/14/2022 16:07:38 - INFO - __main__ -   Batch number = 165
01/14/2022 16:07:38 - INFO - __main__ -   Batch number = 166
01/14/2022 16:07:38 - INFO - __main__ -   Batch number = 167
01/14/2022 16:07:38 - INFO - __main__ -   Batch number = 168
01/14/2022 16:07:39 - INFO - __main__ -   Batch number = 169
01/14/2022 16:07:39 - INFO - __main__ -   Batch number = 170
01/14/2022 16:07:39 - INFO - __main__ -   Batch number = 171
01/14/2022 16:07:40 - INFO - __main__ -   Batch number = 172
01/14/2022 16:07:40 - INFO - __main__ -   Batch number = 173
01/14/2022 16:07:40 - INFO - __main__ -   Batch number = 174
01/14/2022 16:07:41 - INFO - __main__ -   Batch number = 175
01/14/2022 16:07:41 - INFO - __main__ -   Batch number = 176
01/14/2022 16:07:41 - INFO - __main__ -   Batch number = 177
01/14/2022 16:07:42 - INFO - __main__ -   Batch number = 178
01/14/2022 16:07:42 - INFO - __main__ -   Batch number = 179
01/14/2022 16:07:42 - INFO - __main__ -   Batch number = 180
01/14/2022 16:07:43 - INFO - __main__ -   Batch number = 181
01/14/2022 16:07:43 - INFO - __main__ -   Batch number = 182
01/14/2022 16:07:43 - INFO - __main__ -   Batch number = 183
01/14/2022 16:07:43 - INFO - __main__ -   Batch number = 184
01/14/2022 16:07:44 - INFO - __main__ -   Batch number = 185
01/14/2022 16:07:44 - INFO - __main__ -   Batch number = 186
01/14/2022 16:07:44 - INFO - __main__ -   Batch number = 187
01/14/2022 16:07:45 - INFO - __main__ -   Batch number = 188
01/14/2022 16:07:45 - INFO - __main__ -   Batch number = 189
01/14/2022 16:07:45 - INFO - __main__ -   Batch number = 190
01/14/2022 16:07:46 - INFO - __main__ -   Batch number = 191
01/14/2022 16:07:46 - INFO - __main__ -   Batch number = 192
01/14/2022 16:07:46 - INFO - __main__ -   Batch number = 193
01/14/2022 16:07:47 - INFO - __main__ -   Batch number = 194
01/14/2022 16:07:47 - INFO - __main__ -   Batch number = 195
01/14/2022 16:07:47 - INFO - __main__ -   Batch number = 196
01/14/2022 16:07:47 - INFO - __main__ -   Batch number = 197
01/14/2022 16:07:48 - INFO - __main__ -   Batch number = 198
01/14/2022 16:07:48 - INFO - __main__ -   Batch number = 199
01/14/2022 16:07:48 - INFO - __main__ -   Batch number = 200
01/14/2022 16:07:49 - INFO - __main__ -   Batch number = 201
01/14/2022 16:07:49 - INFO - __main__ -   Batch number = 202
01/14/2022 16:07:49 - INFO - __main__ -   Batch number = 203
01/14/2022 16:07:50 - INFO - __main__ -   Batch number = 204
01/14/2022 16:07:50 - INFO - __main__ -   Batch number = 205
01/14/2022 16:07:50 - INFO - __main__ -   Batch number = 206
01/14/2022 16:07:51 - INFO - __main__ -   Batch number = 207
01/14/2022 16:07:51 - INFO - __main__ -   Batch number = 208
01/14/2022 16:07:51 - INFO - __main__ -   Batch number = 209
01/14/2022 16:07:51 - INFO - __main__ -   Batch number = 210
01/14/2022 16:07:52 - INFO - __main__ -   Batch number = 211
01/14/2022 16:07:52 - INFO - __main__ -   Batch number = 212
01/14/2022 16:07:52 - INFO - __main__ -   Batch number = 213
01/14/2022 16:07:53 - INFO - __main__ -   Batch number = 214
01/14/2022 16:07:53 - INFO - __main__ -   Batch number = 215
01/14/2022 16:07:53 - INFO - __main__ -   Batch number = 216
01/14/2022 16:07:54 - INFO - __main__ -   Batch number = 217
01/14/2022 16:07:54 - INFO - __main__ -   Batch number = 218
01/14/2022 16:07:54 - INFO - __main__ -   Batch number = 219
01/14/2022 16:07:54 - INFO - __main__ -   Batch number = 220
01/14/2022 16:07:55 - INFO - __main__ -   Batch number = 221
01/14/2022 16:07:55 - INFO - __main__ -   Batch number = 222
01/14/2022 16:07:55 - INFO - __main__ -   Batch number = 223
01/14/2022 16:07:56 - INFO - __main__ -   Batch number = 224
01/14/2022 16:07:56 - INFO - __main__ -   Batch number = 225
01/14/2022 16:07:56 - INFO - __main__ -   Batch number = 226
01/14/2022 16:07:57 - INFO - __main__ -   Batch number = 227
01/14/2022 16:07:57 - INFO - __main__ -   Batch number = 228
01/14/2022 16:07:57 - INFO - __main__ -   Batch number = 229
01/14/2022 16:07:57 - INFO - __main__ -   Batch number = 230
01/14/2022 16:07:58 - INFO - __main__ -   Batch number = 231
01/14/2022 16:07:58 - INFO - __main__ -   Batch number = 232
01/14/2022 16:07:58 - INFO - __main__ -   Batch number = 233
01/14/2022 16:07:59 - INFO - __main__ -   Batch number = 234
01/14/2022 16:07:59 - INFO - __main__ -   Batch number = 235
01/14/2022 16:07:59 - INFO - __main__ -   Batch number = 236
01/14/2022 16:08:00 - INFO - __main__ -   Batch number = 237
01/14/2022 16:08:00 - INFO - __main__ -   Batch number = 238
01/14/2022 16:08:00 - INFO - __main__ -   Batch number = 239
01/14/2022 16:08:00 - INFO - __main__ -   Batch number = 240
01/14/2022 16:08:01 - INFO - __main__ -   Batch number = 241
01/14/2022 16:08:01 - INFO - __main__ -   Batch number = 242
01/14/2022 16:08:01 - INFO - __main__ -   Batch number = 243
01/14/2022 16:08:02 - INFO - __main__ -   Batch number = 244
01/14/2022 16:08:02 - INFO - __main__ -   Batch number = 245
01/14/2022 16:08:02 - INFO - __main__ -   Batch number = 246
01/14/2022 16:08:03 - INFO - __main__ -   Batch number = 247
01/14/2022 16:08:03 - INFO - __main__ -   Batch number = 248
01/14/2022 16:08:03 - INFO - __main__ -   Batch number = 249
01/14/2022 16:08:03 - INFO - __main__ -   Batch number = 250
01/14/2022 16:08:04 - INFO - __main__ -   Batch number = 251
01/14/2022 16:08:04 - INFO - __main__ -   Batch number = 252
01/14/2022 16:08:04 - INFO - __main__ -   Batch number = 253
01/14/2022 16:08:05 - INFO - __main__ -   Batch number = 254
01/14/2022 16:08:05 - INFO - __main__ -   Batch number = 255
01/14/2022 16:08:05 - INFO - __main__ -   Batch number = 256
01/14/2022 16:08:06 - INFO - __main__ -   Batch number = 257
01/14/2022 16:08:06 - INFO - __main__ -   Batch number = 258
01/14/2022 16:08:06 - INFO - __main__ -   Batch number = 259
01/14/2022 16:08:07 - INFO - __main__ -   Batch number = 260
01/14/2022 16:08:07 - INFO - __main__ -   Batch number = 261
01/14/2022 16:08:07 - INFO - __main__ -   Batch number = 262
01/14/2022 16:08:07 - INFO - __main__ -   Batch number = 263
01/14/2022 16:08:08 - INFO - __main__ -   Batch number = 264
01/14/2022 16:08:08 - INFO - __main__ -   Batch number = 265
01/14/2022 16:08:08 - INFO - __main__ -   Batch number = 266
01/14/2022 16:08:09 - INFO - __main__ -   Batch number = 267
01/14/2022 16:08:09 - INFO - __main__ -   Batch number = 268
01/14/2022 16:08:09 - INFO - __main__ -   Batch number = 269
01/14/2022 16:08:10 - INFO - __main__ -   Batch number = 270
01/14/2022 16:08:10 - INFO - __main__ -   Batch number = 271
01/14/2022 16:08:10 - INFO - __main__ -   Batch number = 272
01/14/2022 16:08:11 - INFO - __main__ -   Batch number = 273
01/14/2022 16:08:11 - INFO - __main__ -   Batch number = 274
01/14/2022 16:08:11 - INFO - __main__ -   Batch number = 275
01/14/2022 16:08:11 - INFO - __main__ -   Batch number = 276
01/14/2022 16:08:12 - INFO - __main__ -   Batch number = 277
01/14/2022 16:08:12 - INFO - __main__ -   Batch number = 278
01/14/2022 16:08:12 - INFO - __main__ -   Batch number = 279
01/14/2022 16:08:13 - INFO - __main__ -   Batch number = 280
01/14/2022 16:08:13 - INFO - __main__ -   Batch number = 281
01/14/2022 16:08:13 - INFO - __main__ -   Batch number = 282
01/14/2022 16:08:14 - INFO - __main__ -   Batch number = 283
01/14/2022 16:08:14 - INFO - __main__ -   Batch number = 284
01/14/2022 16:08:14 - INFO - __main__ -   Batch number = 285
01/14/2022 16:08:14 - INFO - __main__ -   Batch number = 286
01/14/2022 16:08:15 - INFO - __main__ -   Batch number = 287
01/14/2022 16:08:15 - INFO - __main__ -   Batch number = 288
01/14/2022 16:08:15 - INFO - __main__ -   Batch number = 289
01/14/2022 16:08:16 - INFO - __main__ -   Batch number = 290
01/14/2022 16:08:16 - INFO - __main__ -   Batch number = 291
01/14/2022 16:08:16 - INFO - __main__ -   Batch number = 292
01/14/2022 16:08:17 - INFO - __main__ -   Batch number = 293
01/14/2022 16:08:17 - INFO - __main__ -   Batch number = 294
01/14/2022 16:08:17 - INFO - __main__ -   Batch number = 295
01/14/2022 16:08:17 - INFO - __main__ -   Batch number = 296
01/14/2022 16:08:18 - INFO - __main__ -   Batch number = 297
01/14/2022 16:08:18 - INFO - __main__ -   Batch number = 298
01/14/2022 16:08:18 - INFO - __main__ -   Batch number = 299
01/14/2022 16:08:19 - INFO - __main__ -   Batch number = 300
01/14/2022 16:08:19 - INFO - __main__ -   Batch number = 301
01/14/2022 16:08:19 - INFO - __main__ -   Batch number = 302
01/14/2022 16:08:20 - INFO - __main__ -   Batch number = 303
01/14/2022 16:08:20 - INFO - __main__ -   Batch number = 304
01/14/2022 16:08:20 - INFO - __main__ -   Batch number = 305
01/14/2022 16:08:21 - INFO - __main__ -   Batch number = 306
01/14/2022 16:08:21 - INFO - __main__ -   Batch number = 307
01/14/2022 16:08:21 - INFO - __main__ -   Batch number = 308
01/14/2022 16:08:22 - INFO - __main__ -   Batch number = 309
01/14/2022 16:08:22 - INFO - __main__ -   Batch number = 310
01/14/2022 16:08:22 - INFO - __main__ -   Batch number = 311
01/14/2022 16:08:22 - INFO - __main__ -   Batch number = 312
01/14/2022 16:08:23 - INFO - __main__ -   Batch number = 313
01/14/2022 16:08:23 - INFO - __main__ -   Batch number = 314
01/14/2022 16:08:23 - INFO - __main__ -   Batch number = 315
01/14/2022 16:08:24 - INFO - __main__ -   Batch number = 316
01/14/2022 16:08:24 - INFO - __main__ -   Batch number = 317
01/14/2022 16:08:24 - INFO - __main__ -   Batch number = 318
01/14/2022 16:08:25 - INFO - __main__ -   Batch number = 319
01/14/2022 16:08:25 - INFO - __main__ -   Batch number = 320
01/14/2022 16:08:25 - INFO - __main__ -   Batch number = 321
01/14/2022 16:08:26 - INFO - __main__ -   Batch number = 322
01/14/2022 16:08:26 - INFO - __main__ -   Batch number = 323
01/14/2022 16:08:26 - INFO - __main__ -   Batch number = 324
01/14/2022 16:08:27 - INFO - __main__ -   Batch number = 325
01/14/2022 16:08:27 - INFO - __main__ -   Batch number = 326
01/14/2022 16:08:27 - INFO - __main__ -   Batch number = 327
01/14/2022 16:08:27 - INFO - __main__ -   Batch number = 328
01/14/2022 16:08:28 - INFO - __main__ -   Batch number = 329
01/14/2022 16:08:28 - INFO - __main__ -   Batch number = 330
01/14/2022 16:08:28 - INFO - __main__ -   Batch number = 331
01/14/2022 16:08:29 - INFO - __main__ -   Batch number = 332
01/14/2022 16:08:32 - INFO - __main__ -   ***** Evaluation result  in ja *****
01/14/2022 16:08:32 - INFO - __main__ -     f1 = 0.19917680744452398
01/14/2022 16:08:32 - INFO - __main__ -     loss = 4.604087640003986
01/14/2022 16:08:32 - INFO - __main__ -     precision = 0.14505881202958523
01/14/2022 16:08:32 - INFO - __main__ -     recall = 0.3177049882252194
01/14/2022 16:08:35 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ja', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:08:35 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/14/2022 16:08:35 - INFO - __main__ -   Seed = 3
01/14/2022 16:08:35 - INFO - root -   save model
01/14/2022 16:08:35 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ja', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:08:35 - INFO - __main__ -   Loading pretrained model and tokenizer
01/14/2022 16:08:37 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/14/2022 16:08:43 - INFO - __main__ -   Using lang2id = None
01/14/2022 16:08:43 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/14/2022 16:08:43 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/
01/14/2022 16:08:43 - INFO - root -   Trying to decide if add adapter
01/14/2022 16:08:43 - INFO - root -   loading task adapter
01/14/2022 16:08:43 - INFO - root -   loading lang adpater en/wiki@ukp,pt/wiki@ukp,id/wiki@ukp,tr/wiki@ukp,vi/wiki@ukp,fa/wiki@ukp,eu/wiki@ukp,zh_yue/wiki@ukp,cs/wiki@ukp,ja/wiki@ukp
01/14/2022 16:08:43 - INFO - __main__ -   Adapter Languages : ['en', 'pt', 'id', 'tr', 'vi', 'fa', 'eu', 'zh_yue', 'cs', 'ja'], Length : 10
01/14/2022 16:08:43 - INFO - __main__ -   Adapter Names ['en/wiki@ukp', 'pt/wiki@ukp', 'id/wiki@ukp', 'tr/wiki@ukp', 'vi/wiki@ukp', 'fa/wiki@ukp', 'eu/wiki@ukp', 'zh_yue/wiki@ukp', 'cs/wiki@ukp', 'ja/wiki@ukp'], Length : 10
01/14/2022 16:08:43 - INFO - __main__ -   Language = en
01/14/2022 16:08:43 - INFO - __main__ -   Adapter Name = en/wiki@ukp
01/14/2022 16:08:44 - INFO - __main__ -   Language = pt
01/14/2022 16:08:44 - INFO - __main__ -   Adapter Name = pt/wiki@ukp
01/14/2022 16:08:46 - INFO - __main__ -   Language = id
01/14/2022 16:08:46 - INFO - __main__ -   Adapter Name = id/wiki@ukp
01/14/2022 16:08:48 - INFO - __main__ -   Language = tr
01/14/2022 16:08:48 - INFO - __main__ -   Adapter Name = tr/wiki@ukp
01/14/2022 16:08:51 - INFO - __main__ -   Language = vi
01/14/2022 16:08:51 - INFO - __main__ -   Adapter Name = vi/wiki@ukp
01/14/2022 16:08:53 - INFO - __main__ -   Language = fa
01/14/2022 16:08:53 - INFO - __main__ -   Adapter Name = fa/wiki@ukp
01/14/2022 16:08:55 - INFO - __main__ -   Language = eu
01/14/2022 16:08:55 - INFO - __main__ -   Adapter Name = eu/wiki@ukp
01/14/2022 16:08:57 - INFO - __main__ -   Language = zh_yue
01/14/2022 16:08:57 - INFO - __main__ -   Adapter Name = zh_yue/wiki@ukp
01/14/2022 16:08:59 - INFO - __main__ -   Language = cs
01/14/2022 16:08:59 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
01/14/2022 16:09:01 - INFO - __main__ -   Language = ja
01/14/2022 16:09:01 - INFO - __main__ -   Adapter Name = ja/wiki@ukp
01/14/2022 16:09:05 - INFO - __main__ -   Args Adapter Weight = equal
01/14/2022 16:09:06 - INFO - __main__ -   Adapter Languages = ['en', 'pt', 'id', 'tr', 'vi', 'fa', 'eu', 'zh_yue', 'cs', 'ja']
01/14/2022 16:09:06 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/14/2022 16:09:06 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/14/2022 16:09:06 - INFO - __main__ -   Length of Adapter Weights = 10
01/14/2022 16:09:06 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/cached_test_ja_bert-base-multilingual-cased_128
01/14/2022 16:09:07 - INFO - __main__ -   ***** Running evaluation  in ja *****
01/14/2022 16:09:07 - INFO - __main__ -     Num examples = 10612
01/14/2022 16:09:07 - INFO - __main__ -     Batch size = 32
01/14/2022 16:09:07 - INFO - __main__ -   Batch number = 1
01/14/2022 16:09:07 - INFO - __main__ -   Batch number = 2
01/14/2022 16:09:07 - INFO - __main__ -   Batch number = 3
01/14/2022 16:09:08 - INFO - __main__ -   Batch number = 4
01/14/2022 16:09:08 - INFO - __main__ -   Batch number = 5
01/14/2022 16:09:08 - INFO - __main__ -   Batch number = 6
01/14/2022 16:09:08 - INFO - __main__ -   Batch number = 7
01/14/2022 16:09:09 - INFO - __main__ -   Batch number = 8
01/14/2022 16:09:09 - INFO - __main__ -   Batch number = 9
01/14/2022 16:09:09 - INFO - __main__ -   Batch number = 10
01/14/2022 16:09:10 - INFO - __main__ -   Batch number = 11
01/14/2022 16:09:10 - INFO - __main__ -   Batch number = 12
01/14/2022 16:09:10 - INFO - __main__ -   Batch number = 13
01/14/2022 16:09:10 - INFO - __main__ -   Batch number = 14
01/14/2022 16:09:11 - INFO - __main__ -   Batch number = 15
01/14/2022 16:09:11 - INFO - __main__ -   Batch number = 16
01/14/2022 16:09:11 - INFO - __main__ -   Batch number = 17
01/14/2022 16:09:12 - INFO - __main__ -   Batch number = 18
01/14/2022 16:09:12 - INFO - __main__ -   Batch number = 19
01/14/2022 16:09:12 - INFO - __main__ -   Batch number = 20
01/14/2022 16:09:12 - INFO - __main__ -   Batch number = 21
01/14/2022 16:09:13 - INFO - __main__ -   Batch number = 22
01/14/2022 16:09:13 - INFO - __main__ -   Batch number = 23
01/14/2022 16:09:13 - INFO - __main__ -   Batch number = 24
01/14/2022 16:09:14 - INFO - __main__ -   Batch number = 25
01/14/2022 16:09:14 - INFO - __main__ -   Batch number = 26
01/14/2022 16:09:14 - INFO - __main__ -   Batch number = 27
01/14/2022 16:09:14 - INFO - __main__ -   Batch number = 28
01/14/2022 16:09:15 - INFO - __main__ -   Batch number = 29
01/14/2022 16:09:15 - INFO - __main__ -   Batch number = 30
01/14/2022 16:09:15 - INFO - __main__ -   Batch number = 31
01/14/2022 16:09:16 - INFO - __main__ -   Batch number = 32
01/14/2022 16:09:16 - INFO - __main__ -   Batch number = 33
01/14/2022 16:09:16 - INFO - __main__ -   Batch number = 34
01/14/2022 16:09:16 - INFO - __main__ -   Batch number = 35
01/14/2022 16:09:17 - INFO - __main__ -   Batch number = 36
01/14/2022 16:09:17 - INFO - __main__ -   Batch number = 37
01/14/2022 16:09:17 - INFO - __main__ -   Batch number = 38
01/14/2022 16:09:18 - INFO - __main__ -   Batch number = 39
01/14/2022 16:09:18 - INFO - __main__ -   Batch number = 40
01/14/2022 16:09:18 - INFO - __main__ -   Batch number = 41
01/14/2022 16:09:18 - INFO - __main__ -   Batch number = 42
01/14/2022 16:09:19 - INFO - __main__ -   Batch number = 43
01/14/2022 16:09:19 - INFO - __main__ -   Batch number = 44
01/14/2022 16:09:19 - INFO - __main__ -   Batch number = 45
01/14/2022 16:09:19 - INFO - __main__ -   Batch number = 46
01/14/2022 16:09:20 - INFO - __main__ -   Batch number = 47
01/14/2022 16:09:20 - INFO - __main__ -   Batch number = 48
01/14/2022 16:09:20 - INFO - __main__ -   Batch number = 49
01/14/2022 16:09:21 - INFO - __main__ -   Batch number = 50
01/14/2022 16:09:21 - INFO - __main__ -   Batch number = 51
01/14/2022 16:09:21 - INFO - __main__ -   Batch number = 52
01/14/2022 16:09:21 - INFO - __main__ -   Batch number = 53
01/14/2022 16:09:22 - INFO - __main__ -   Batch number = 54
01/14/2022 16:09:22 - INFO - __main__ -   Batch number = 55
01/14/2022 16:09:22 - INFO - __main__ -   Batch number = 56
01/14/2022 16:09:23 - INFO - __main__ -   Batch number = 57
01/14/2022 16:09:23 - INFO - __main__ -   Batch number = 58
01/14/2022 16:09:23 - INFO - __main__ -   Batch number = 59
01/14/2022 16:09:23 - INFO - __main__ -   Batch number = 60
01/14/2022 16:09:24 - INFO - __main__ -   Batch number = 61
01/14/2022 16:09:24 - INFO - __main__ -   Batch number = 62
01/14/2022 16:09:24 - INFO - __main__ -   Batch number = 63
01/14/2022 16:09:25 - INFO - __main__ -   Batch number = 64
01/14/2022 16:09:25 - INFO - __main__ -   Batch number = 65
01/14/2022 16:09:25 - INFO - __main__ -   Batch number = 66
01/14/2022 16:09:26 - INFO - __main__ -   Batch number = 67
01/14/2022 16:09:26 - INFO - __main__ -   Batch number = 68
01/14/2022 16:09:26 - INFO - __main__ -   Batch number = 69
01/14/2022 16:09:26 - INFO - __main__ -   Batch number = 70
01/14/2022 16:09:27 - INFO - __main__ -   Batch number = 71
01/14/2022 16:09:27 - INFO - __main__ -   Batch number = 72
01/14/2022 16:09:27 - INFO - __main__ -   Batch number = 73
01/14/2022 16:09:28 - INFO - __main__ -   Batch number = 74
01/14/2022 16:09:28 - INFO - __main__ -   Batch number = 75
01/14/2022 16:09:28 - INFO - __main__ -   Batch number = 76
01/14/2022 16:09:28 - INFO - __main__ -   Batch number = 77
01/14/2022 16:09:29 - INFO - __main__ -   Batch number = 78
01/14/2022 16:09:29 - INFO - __main__ -   Batch number = 79
01/14/2022 16:09:29 - INFO - __main__ -   Batch number = 80
01/14/2022 16:09:30 - INFO - __main__ -   Batch number = 81
01/14/2022 16:09:30 - INFO - __main__ -   Batch number = 82
01/14/2022 16:09:30 - INFO - __main__ -   Batch number = 83
01/14/2022 16:09:31 - INFO - __main__ -   Batch number = 84
01/14/2022 16:09:31 - INFO - __main__ -   Batch number = 85
01/14/2022 16:09:31 - INFO - __main__ -   Batch number = 86
01/14/2022 16:09:31 - INFO - __main__ -   Batch number = 87
01/14/2022 16:09:32 - INFO - __main__ -   Batch number = 88
01/14/2022 16:09:32 - INFO - __main__ -   Batch number = 89
01/14/2022 16:09:32 - INFO - __main__ -   Batch number = 90
01/14/2022 16:09:33 - INFO - __main__ -   Batch number = 91
01/14/2022 16:09:33 - INFO - __main__ -   Batch number = 92
01/14/2022 16:09:33 - INFO - __main__ -   Batch number = 93
01/14/2022 16:09:33 - INFO - __main__ -   Batch number = 94
01/14/2022 16:09:34 - INFO - __main__ -   Batch number = 95
01/14/2022 16:09:34 - INFO - __main__ -   Batch number = 96
01/14/2022 16:09:34 - INFO - __main__ -   Batch number = 97
01/14/2022 16:09:35 - INFO - __main__ -   Batch number = 98
01/14/2022 16:09:35 - INFO - __main__ -   Batch number = 99
01/14/2022 16:09:35 - INFO - __main__ -   Batch number = 100
01/14/2022 16:09:36 - INFO - __main__ -   Batch number = 101
01/14/2022 16:09:36 - INFO - __main__ -   Batch number = 102
01/14/2022 16:09:36 - INFO - __main__ -   Batch number = 103
01/14/2022 16:09:37 - INFO - __main__ -   Batch number = 104
01/14/2022 16:09:37 - INFO - __main__ -   Batch number = 105
01/14/2022 16:09:37 - INFO - __main__ -   Batch number = 106
01/14/2022 16:09:37 - INFO - __main__ -   Batch number = 107
01/14/2022 16:09:38 - INFO - __main__ -   Batch number = 108
01/14/2022 16:09:38 - INFO - __main__ -   Batch number = 109
01/14/2022 16:09:38 - INFO - __main__ -   Batch number = 110
01/14/2022 16:09:39 - INFO - __main__ -   Batch number = 111
01/14/2022 16:09:39 - INFO - __main__ -   Batch number = 112
01/14/2022 16:09:39 - INFO - __main__ -   Batch number = 113
01/14/2022 16:09:40 - INFO - __main__ -   Batch number = 114
01/14/2022 16:09:40 - INFO - __main__ -   Batch number = 115
01/14/2022 16:09:40 - INFO - __main__ -   Batch number = 116
01/14/2022 16:09:41 - INFO - __main__ -   Batch number = 117
01/14/2022 16:09:41 - INFO - __main__ -   Batch number = 118
01/14/2022 16:09:41 - INFO - __main__ -   Batch number = 119
01/14/2022 16:09:42 - INFO - __main__ -   Batch number = 120
01/14/2022 16:09:42 - INFO - __main__ -   Batch number = 121
01/14/2022 16:09:42 - INFO - __main__ -   Batch number = 122
01/14/2022 16:09:42 - INFO - __main__ -   Batch number = 123
01/14/2022 16:09:43 - INFO - __main__ -   Batch number = 124
01/14/2022 16:09:43 - INFO - __main__ -   Batch number = 125
01/14/2022 16:09:43 - INFO - __main__ -   Batch number = 126
01/14/2022 16:09:44 - INFO - __main__ -   Batch number = 127
01/14/2022 16:09:44 - INFO - __main__ -   Batch number = 128
01/14/2022 16:09:44 - INFO - __main__ -   Batch number = 129
01/14/2022 16:09:45 - INFO - __main__ -   Batch number = 130
01/14/2022 16:09:45 - INFO - __main__ -   Batch number = 131
01/14/2022 16:09:45 - INFO - __main__ -   Batch number = 132
01/14/2022 16:09:46 - INFO - __main__ -   Batch number = 133
01/14/2022 16:09:46 - INFO - __main__ -   Batch number = 134
01/14/2022 16:09:46 - INFO - __main__ -   Batch number = 135
01/14/2022 16:09:47 - INFO - __main__ -   Batch number = 136
01/14/2022 16:09:47 - INFO - __main__ -   Batch number = 137
01/14/2022 16:09:47 - INFO - __main__ -   Batch number = 138
01/14/2022 16:09:47 - INFO - __main__ -   Batch number = 139
01/14/2022 16:09:48 - INFO - __main__ -   Batch number = 140
01/14/2022 16:09:48 - INFO - __main__ -   Batch number = 141
01/14/2022 16:09:48 - INFO - __main__ -   Batch number = 142
01/14/2022 16:09:49 - INFO - __main__ -   Batch number = 143
01/14/2022 16:09:49 - INFO - __main__ -   Batch number = 144
01/14/2022 16:09:49 - INFO - __main__ -   Batch number = 145
01/14/2022 16:09:50 - INFO - __main__ -   Batch number = 146
01/14/2022 16:09:50 - INFO - __main__ -   Batch number = 147
01/14/2022 16:09:50 - INFO - __main__ -   Batch number = 148
01/14/2022 16:09:50 - INFO - __main__ -   Batch number = 149
01/14/2022 16:09:51 - INFO - __main__ -   Batch number = 150
01/14/2022 16:09:51 - INFO - __main__ -   Batch number = 151
01/14/2022 16:09:51 - INFO - __main__ -   Batch number = 152
01/14/2022 16:09:52 - INFO - __main__ -   Batch number = 153
01/14/2022 16:09:52 - INFO - __main__ -   Batch number = 154
01/14/2022 16:09:52 - INFO - __main__ -   Batch number = 155
01/14/2022 16:09:53 - INFO - __main__ -   Batch number = 156
01/14/2022 16:09:53 - INFO - __main__ -   Batch number = 157
01/14/2022 16:09:53 - INFO - __main__ -   Batch number = 158
01/14/2022 16:09:53 - INFO - __main__ -   Batch number = 159
01/14/2022 16:09:54 - INFO - __main__ -   Batch number = 160
01/14/2022 16:09:54 - INFO - __main__ -   Batch number = 161
01/14/2022 16:09:54 - INFO - __main__ -   Batch number = 162
01/14/2022 16:09:55 - INFO - __main__ -   Batch number = 163
01/14/2022 16:09:55 - INFO - __main__ -   Batch number = 164
01/14/2022 16:09:55 - INFO - __main__ -   Batch number = 165
01/14/2022 16:09:56 - INFO - __main__ -   Batch number = 166
01/14/2022 16:09:56 - INFO - __main__ -   Batch number = 167
01/14/2022 16:09:56 - INFO - __main__ -   Batch number = 168
01/14/2022 16:09:57 - INFO - __main__ -   Batch number = 169
01/14/2022 16:09:57 - INFO - __main__ -   Batch number = 170
01/14/2022 16:09:57 - INFO - __main__ -   Batch number = 171
01/14/2022 16:09:57 - INFO - __main__ -   Batch number = 172
01/14/2022 16:09:58 - INFO - __main__ -   Batch number = 173
01/14/2022 16:09:58 - INFO - __main__ -   Batch number = 174
01/14/2022 16:09:58 - INFO - __main__ -   Batch number = 175
01/14/2022 16:09:59 - INFO - __main__ -   Batch number = 176
01/14/2022 16:09:59 - INFO - __main__ -   Batch number = 177
01/14/2022 16:09:59 - INFO - __main__ -   Batch number = 178
01/14/2022 16:10:00 - INFO - __main__ -   Batch number = 179
01/14/2022 16:10:00 - INFO - __main__ -   Batch number = 180
01/14/2022 16:10:00 - INFO - __main__ -   Batch number = 181
01/14/2022 16:10:01 - INFO - __main__ -   Batch number = 182
01/14/2022 16:10:01 - INFO - __main__ -   Batch number = 183
01/14/2022 16:10:01 - INFO - __main__ -   Batch number = 184
01/14/2022 16:10:02 - INFO - __main__ -   Batch number = 185
01/14/2022 16:10:02 - INFO - __main__ -   Batch number = 186
01/14/2022 16:10:02 - INFO - __main__ -   Batch number = 187
01/14/2022 16:10:02 - INFO - __main__ -   Batch number = 188
01/14/2022 16:10:03 - INFO - __main__ -   Batch number = 189
01/14/2022 16:10:03 - INFO - __main__ -   Batch number = 190
01/14/2022 16:10:03 - INFO - __main__ -   Batch number = 191
01/14/2022 16:10:04 - INFO - __main__ -   Batch number = 192
01/14/2022 16:10:04 - INFO - __main__ -   Batch number = 193
01/14/2022 16:10:04 - INFO - __main__ -   Batch number = 194
01/14/2022 16:10:05 - INFO - __main__ -   Batch number = 195
01/14/2022 16:10:05 - INFO - __main__ -   Batch number = 196
01/14/2022 16:10:05 - INFO - __main__ -   Batch number = 197
01/14/2022 16:10:05 - INFO - __main__ -   Batch number = 198
01/14/2022 16:10:06 - INFO - __main__ -   Batch number = 199
01/14/2022 16:10:06 - INFO - __main__ -   Batch number = 200
01/14/2022 16:10:06 - INFO - __main__ -   Batch number = 201
01/14/2022 16:10:07 - INFO - __main__ -   Batch number = 202
01/14/2022 16:10:07 - INFO - __main__ -   Batch number = 203
01/14/2022 16:10:07 - INFO - __main__ -   Batch number = 204
01/14/2022 16:10:08 - INFO - __main__ -   Batch number = 205
01/14/2022 16:10:08 - INFO - __main__ -   Batch number = 206
01/14/2022 16:10:08 - INFO - __main__ -   Batch number = 207
01/14/2022 16:10:09 - INFO - __main__ -   Batch number = 208
01/14/2022 16:10:09 - INFO - __main__ -   Batch number = 209
01/14/2022 16:10:09 - INFO - __main__ -   Batch number = 210
01/14/2022 16:10:09 - INFO - __main__ -   Batch number = 211
01/14/2022 16:10:10 - INFO - __main__ -   Batch number = 212
01/14/2022 16:10:10 - INFO - __main__ -   Batch number = 213
01/14/2022 16:10:10 - INFO - __main__ -   Batch number = 214
01/14/2022 16:10:11 - INFO - __main__ -   Batch number = 215
01/14/2022 16:10:11 - INFO - __main__ -   Batch number = 216
01/14/2022 16:10:11 - INFO - __main__ -   Batch number = 217
01/14/2022 16:10:12 - INFO - __main__ -   Batch number = 218
01/14/2022 16:10:12 - INFO - __main__ -   Batch number = 219
01/14/2022 16:10:12 - INFO - __main__ -   Batch number = 220
01/14/2022 16:10:13 - INFO - __main__ -   Batch number = 221
01/14/2022 16:10:13 - INFO - __main__ -   Batch number = 222
01/14/2022 16:10:13 - INFO - __main__ -   Batch number = 223
01/14/2022 16:10:13 - INFO - __main__ -   Batch number = 224
01/14/2022 16:10:14 - INFO - __main__ -   Batch number = 225
01/14/2022 16:10:14 - INFO - __main__ -   Batch number = 226
01/14/2022 16:10:14 - INFO - __main__ -   Batch number = 227
01/14/2022 16:10:15 - INFO - __main__ -   Batch number = 228
01/14/2022 16:10:15 - INFO - __main__ -   Batch number = 229
01/14/2022 16:10:15 - INFO - __main__ -   Batch number = 230
01/14/2022 16:10:16 - INFO - __main__ -   Batch number = 231
01/14/2022 16:10:16 - INFO - __main__ -   Batch number = 232
01/14/2022 16:10:16 - INFO - __main__ -   Batch number = 233
01/14/2022 16:10:17 - INFO - __main__ -   Batch number = 234
01/14/2022 16:10:17 - INFO - __main__ -   Batch number = 235
01/14/2022 16:10:17 - INFO - __main__ -   Batch number = 236
01/14/2022 16:10:17 - INFO - __main__ -   Batch number = 237
01/14/2022 16:10:18 - INFO - __main__ -   Batch number = 238
01/14/2022 16:10:18 - INFO - __main__ -   Batch number = 239
01/14/2022 16:10:18 - INFO - __main__ -   Batch number = 240
01/14/2022 16:10:19 - INFO - __main__ -   Batch number = 241
01/14/2022 16:10:19 - INFO - __main__ -   Batch number = 242
01/14/2022 16:10:19 - INFO - __main__ -   Batch number = 243
01/14/2022 16:10:20 - INFO - __main__ -   Batch number = 244
01/14/2022 16:10:20 - INFO - __main__ -   Batch number = 245
01/14/2022 16:10:20 - INFO - __main__ -   Batch number = 246
01/14/2022 16:10:20 - INFO - __main__ -   Batch number = 247
01/14/2022 16:10:21 - INFO - __main__ -   Batch number = 248
01/14/2022 16:10:21 - INFO - __main__ -   Batch number = 249
01/14/2022 16:10:21 - INFO - __main__ -   Batch number = 250
01/14/2022 16:10:22 - INFO - __main__ -   Batch number = 251
01/14/2022 16:10:22 - INFO - __main__ -   Batch number = 252
01/14/2022 16:10:22 - INFO - __main__ -   Batch number = 253
01/14/2022 16:10:23 - INFO - __main__ -   Batch number = 254
01/14/2022 16:10:23 - INFO - __main__ -   Batch number = 255
01/14/2022 16:10:23 - INFO - __main__ -   Batch number = 256
01/14/2022 16:10:24 - INFO - __main__ -   Batch number = 257
01/14/2022 16:10:24 - INFO - __main__ -   Batch number = 258
01/14/2022 16:10:24 - INFO - __main__ -   Batch number = 259
01/14/2022 16:10:25 - INFO - __main__ -   Batch number = 260
01/14/2022 16:10:25 - INFO - __main__ -   Batch number = 261
01/14/2022 16:10:25 - INFO - __main__ -   Batch number = 262
01/14/2022 16:10:26 - INFO - __main__ -   Batch number = 263
01/14/2022 16:10:26 - INFO - __main__ -   Batch number = 264
01/14/2022 16:10:26 - INFO - __main__ -   Batch number = 265
01/14/2022 16:10:26 - INFO - __main__ -   Batch number = 266
01/14/2022 16:10:27 - INFO - __main__ -   Batch number = 267
01/14/2022 16:10:27 - INFO - __main__ -   Batch number = 268
01/14/2022 16:10:27 - INFO - __main__ -   Batch number = 269
01/14/2022 16:10:28 - INFO - __main__ -   Batch number = 270
01/14/2022 16:10:28 - INFO - __main__ -   Batch number = 271
01/14/2022 16:10:28 - INFO - __main__ -   Batch number = 272
01/14/2022 16:10:29 - INFO - __main__ -   Batch number = 273
01/14/2022 16:10:29 - INFO - __main__ -   Batch number = 274
01/14/2022 16:10:29 - INFO - __main__ -   Batch number = 275
01/14/2022 16:10:29 - INFO - __main__ -   Batch number = 276
01/14/2022 16:10:30 - INFO - __main__ -   Batch number = 277
01/14/2022 16:10:30 - INFO - __main__ -   Batch number = 278
01/14/2022 16:10:30 - INFO - __main__ -   Batch number = 279
01/14/2022 16:10:31 - INFO - __main__ -   Batch number = 280
01/14/2022 16:10:31 - INFO - __main__ -   Batch number = 281
01/14/2022 16:10:31 - INFO - __main__ -   Batch number = 282
01/14/2022 16:10:32 - INFO - __main__ -   Batch number = 283
01/14/2022 16:10:32 - INFO - __main__ -   Batch number = 284
01/14/2022 16:10:32 - INFO - __main__ -   Batch number = 285
01/14/2022 16:10:32 - INFO - __main__ -   Batch number = 286
01/14/2022 16:10:33 - INFO - __main__ -   Batch number = 287
01/14/2022 16:10:33 - INFO - __main__ -   Batch number = 288
01/14/2022 16:10:33 - INFO - __main__ -   Batch number = 289
01/14/2022 16:10:34 - INFO - __main__ -   Batch number = 290
01/14/2022 16:10:34 - INFO - __main__ -   Batch number = 291
01/14/2022 16:10:34 - INFO - __main__ -   Batch number = 292
01/14/2022 16:10:35 - INFO - __main__ -   Batch number = 293
01/14/2022 16:10:35 - INFO - __main__ -   Batch number = 294
01/14/2022 16:10:35 - INFO - __main__ -   Batch number = 295
01/14/2022 16:10:35 - INFO - __main__ -   Batch number = 296
01/14/2022 16:10:36 - INFO - __main__ -   Batch number = 297
01/14/2022 16:10:36 - INFO - __main__ -   Batch number = 298
01/14/2022 16:10:36 - INFO - __main__ -   Batch number = 299
01/14/2022 16:10:37 - INFO - __main__ -   Batch number = 300
01/14/2022 16:10:37 - INFO - __main__ -   Batch number = 301
01/14/2022 16:10:37 - INFO - __main__ -   Batch number = 302
01/14/2022 16:10:38 - INFO - __main__ -   Batch number = 303
01/14/2022 16:10:38 - INFO - __main__ -   Batch number = 304
01/14/2022 16:10:38 - INFO - __main__ -   Batch number = 305
01/14/2022 16:10:39 - INFO - __main__ -   Batch number = 306
01/14/2022 16:10:39 - INFO - __main__ -   Batch number = 307
01/14/2022 16:10:39 - INFO - __main__ -   Batch number = 308
01/14/2022 16:10:39 - INFO - __main__ -   Batch number = 309
01/14/2022 16:10:40 - INFO - __main__ -   Batch number = 310
01/14/2022 16:10:40 - INFO - __main__ -   Batch number = 311
01/14/2022 16:10:40 - INFO - __main__ -   Batch number = 312
01/14/2022 16:10:41 - INFO - __main__ -   Batch number = 313
01/14/2022 16:10:41 - INFO - __main__ -   Batch number = 314
01/14/2022 16:10:41 - INFO - __main__ -   Batch number = 315
01/14/2022 16:10:42 - INFO - __main__ -   Batch number = 316
01/14/2022 16:10:42 - INFO - __main__ -   Batch number = 317
01/14/2022 16:10:42 - INFO - __main__ -   Batch number = 318
01/14/2022 16:10:43 - INFO - __main__ -   Batch number = 319
01/14/2022 16:10:43 - INFO - __main__ -   Batch number = 320
01/14/2022 16:10:43 - INFO - __main__ -   Batch number = 321
01/14/2022 16:10:44 - INFO - __main__ -   Batch number = 322
01/14/2022 16:10:44 - INFO - __main__ -   Batch number = 323
01/14/2022 16:10:44 - INFO - __main__ -   Batch number = 324
01/14/2022 16:10:45 - INFO - __main__ -   Batch number = 325
01/14/2022 16:10:45 - INFO - __main__ -   Batch number = 326
01/14/2022 16:10:45 - INFO - __main__ -   Batch number = 327
01/14/2022 16:10:45 - INFO - __main__ -   Batch number = 328
01/14/2022 16:10:46 - INFO - __main__ -   Batch number = 329
01/14/2022 16:10:46 - INFO - __main__ -   Batch number = 330
01/14/2022 16:10:46 - INFO - __main__ -   Batch number = 331
01/14/2022 16:10:47 - INFO - __main__ -   Batch number = 332
01/14/2022 16:10:51 - INFO - __main__ -   ***** Evaluation result  in ja *****
01/14/2022 16:10:51 - INFO - __main__ -     f1 = 0.21671729807005002
01/14/2022 16:10:51 - INFO - __main__ -     loss = 4.282998661678958
01/14/2022 16:10:51 - INFO - __main__ -     precision = 0.15773045033327915
01/14/2022 16:10:51 - INFO - __main__ -     recall = 0.34617854849068724
01/14/2022 16:12:55 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='zh', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:12:55 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/14/2022 16:12:55 - INFO - __main__ -   Seed = 1
01/14/2022 16:12:55 - INFO - root -   save model
01/14/2022 16:12:55 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='zh', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:12:55 - INFO - __main__ -   Loading pretrained model and tokenizer
01/14/2022 16:12:57 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/14/2022 16:13:03 - INFO - __main__ -   Using lang2id = None
01/14/2022 16:13:03 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/14/2022 16:13:03 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/
01/14/2022 16:13:03 - INFO - root -   Trying to decide if add adapter
01/14/2022 16:13:03 - INFO - root -   loading task adapter
01/14/2022 16:13:03 - INFO - root -   loading lang adpater en/wiki@ukp,pt/wiki@ukp,id/wiki@ukp,cs/wiki@ukp,tr/wiki@ukp,eu/wiki@ukp,zh_yue/wiki@ukp,vi/wiki@ukp,fr/wiki@ukp,zh/wiki@ukp
01/14/2022 16:13:03 - INFO - __main__ -   Adapter Languages : ['en', 'pt', 'id', 'cs', 'tr', 'eu', 'zh_yue', 'vi', 'fr', 'zh'], Length : 10
01/14/2022 16:13:03 - INFO - __main__ -   Adapter Names ['en/wiki@ukp', 'pt/wiki@ukp', 'id/wiki@ukp', 'cs/wiki@ukp', 'tr/wiki@ukp', 'eu/wiki@ukp', 'zh_yue/wiki@ukp', 'vi/wiki@ukp', 'fr/wiki@ukp', 'zh/wiki@ukp'], Length : 10
01/14/2022 16:13:03 - INFO - __main__ -   Language = en
01/14/2022 16:13:03 - INFO - __main__ -   Adapter Name = en/wiki@ukp
01/14/2022 16:13:05 - INFO - __main__ -   Language = pt
01/14/2022 16:13:05 - INFO - __main__ -   Adapter Name = pt/wiki@ukp
01/14/2022 16:13:06 - INFO - __main__ -   Language = id
01/14/2022 16:13:06 - INFO - __main__ -   Adapter Name = id/wiki@ukp
01/14/2022 16:13:08 - INFO - __main__ -   Language = cs
01/14/2022 16:13:08 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
01/14/2022 16:13:10 - INFO - __main__ -   Language = tr
01/14/2022 16:13:10 - INFO - __main__ -   Adapter Name = tr/wiki@ukp
01/14/2022 16:13:13 - INFO - __main__ -   Language = eu
01/14/2022 16:13:13 - INFO - __main__ -   Adapter Name = eu/wiki@ukp
01/14/2022 16:13:15 - INFO - __main__ -   Language = zh_yue
01/14/2022 16:13:15 - INFO - __main__ -   Adapter Name = zh_yue/wiki@ukp
01/14/2022 16:13:17 - INFO - __main__ -   Language = vi
01/14/2022 16:13:17 - INFO - __main__ -   Adapter Name = vi/wiki@ukp
01/14/2022 16:13:19 - INFO - __main__ -   Language = fr
01/14/2022 16:13:19 - INFO - __main__ -   Adapter Name = fr/wiki@ukp
01/14/2022 16:13:20 - INFO - __main__ -   Language = zh
01/14/2022 16:13:20 - INFO - __main__ -   Adapter Name = zh/wiki@ukp
01/14/2022 16:13:25 - INFO - __main__ -   Args Adapter Weight = equal
01/14/2022 16:13:25 - INFO - __main__ -   Adapter Languages = ['en', 'pt', 'id', 'cs', 'tr', 'eu', 'zh_yue', 'vi', 'fr', 'zh']
01/14/2022 16:13:25 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/14/2022 16:13:25 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/14/2022 16:13:25 - INFO - __main__ -   Length of Adapter Weights = 10
01/14/2022 16:13:25 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/cached_test_zh_bert-base-multilingual-cased_128
01/14/2022 16:13:26 - INFO - __main__ -   ***** Running evaluation  in zh *****
01/14/2022 16:13:26 - INFO - __main__ -     Num examples = 10257
01/14/2022 16:13:26 - INFO - __main__ -     Batch size = 32
01/14/2022 16:13:26 - INFO - __main__ -   Batch number = 1
01/14/2022 16:13:26 - INFO - __main__ -   Batch number = 2
01/14/2022 16:13:27 - INFO - __main__ -   Batch number = 3
01/14/2022 16:13:27 - INFO - __main__ -   Batch number = 4
01/14/2022 16:13:27 - INFO - __main__ -   Batch number = 5
01/14/2022 16:13:27 - INFO - __main__ -   Batch number = 6
01/14/2022 16:13:28 - INFO - __main__ -   Batch number = 7
01/14/2022 16:13:28 - INFO - __main__ -   Batch number = 8
01/14/2022 16:13:28 - INFO - __main__ -   Batch number = 9
01/14/2022 16:13:29 - INFO - __main__ -   Batch number = 10
01/14/2022 16:13:29 - INFO - __main__ -   Batch number = 11
01/14/2022 16:13:29 - INFO - __main__ -   Batch number = 12
01/14/2022 16:13:29 - INFO - __main__ -   Batch number = 13
01/14/2022 16:13:30 - INFO - __main__ -   Batch number = 14
01/14/2022 16:13:30 - INFO - __main__ -   Batch number = 15
01/14/2022 16:13:30 - INFO - __main__ -   Batch number = 16
01/14/2022 16:13:31 - INFO - __main__ -   Batch number = 17
01/14/2022 16:13:31 - INFO - __main__ -   Batch number = 18
01/14/2022 16:13:31 - INFO - __main__ -   Batch number = 19
01/14/2022 16:13:31 - INFO - __main__ -   Batch number = 20
01/14/2022 16:13:32 - INFO - __main__ -   Batch number = 21
01/14/2022 16:13:32 - INFO - __main__ -   Batch number = 22
01/14/2022 16:13:32 - INFO - __main__ -   Batch number = 23
01/14/2022 16:13:32 - INFO - __main__ -   Batch number = 24
01/14/2022 16:13:33 - INFO - __main__ -   Batch number = 25
01/14/2022 16:13:33 - INFO - __main__ -   Batch number = 26
01/14/2022 16:13:33 - INFO - __main__ -   Batch number = 27
01/14/2022 16:13:34 - INFO - __main__ -   Batch number = 28
01/14/2022 16:13:34 - INFO - __main__ -   Batch number = 29
01/14/2022 16:13:34 - INFO - __main__ -   Batch number = 30
01/14/2022 16:13:34 - INFO - __main__ -   Batch number = 31
01/14/2022 16:13:35 - INFO - __main__ -   Batch number = 32
01/14/2022 16:13:35 - INFO - __main__ -   Batch number = 33
01/14/2022 16:13:35 - INFO - __main__ -   Batch number = 34
01/14/2022 16:13:36 - INFO - __main__ -   Batch number = 35
01/14/2022 16:13:36 - INFO - __main__ -   Batch number = 36
01/14/2022 16:13:36 - INFO - __main__ -   Batch number = 37
01/14/2022 16:13:36 - INFO - __main__ -   Batch number = 38
01/14/2022 16:13:37 - INFO - __main__ -   Batch number = 39
01/14/2022 16:13:37 - INFO - __main__ -   Batch number = 40
01/14/2022 16:13:37 - INFO - __main__ -   Batch number = 41
01/14/2022 16:13:38 - INFO - __main__ -   Batch number = 42
01/14/2022 16:13:38 - INFO - __main__ -   Batch number = 43
01/14/2022 16:13:38 - INFO - __main__ -   Batch number = 44
01/14/2022 16:13:38 - INFO - __main__ -   Batch number = 45
01/14/2022 16:13:39 - INFO - __main__ -   Batch number = 46
01/14/2022 16:13:39 - INFO - __main__ -   Batch number = 47
01/14/2022 16:13:39 - INFO - __main__ -   Batch number = 48
01/14/2022 16:13:40 - INFO - __main__ -   Batch number = 49
01/14/2022 16:13:40 - INFO - __main__ -   Batch number = 50
01/14/2022 16:13:40 - INFO - __main__ -   Batch number = 51
01/14/2022 16:13:41 - INFO - __main__ -   Batch number = 52
01/14/2022 16:13:41 - INFO - __main__ -   Batch number = 53
01/14/2022 16:13:41 - INFO - __main__ -   Batch number = 54
01/14/2022 16:13:41 - INFO - __main__ -   Batch number = 55
01/14/2022 16:13:42 - INFO - __main__ -   Batch number = 56
01/14/2022 16:13:42 - INFO - __main__ -   Batch number = 57
01/14/2022 16:13:42 - INFO - __main__ -   Batch number = 58
01/14/2022 16:13:42 - INFO - __main__ -   Batch number = 59
01/14/2022 16:13:43 - INFO - __main__ -   Batch number = 60
01/14/2022 16:13:43 - INFO - __main__ -   Batch number = 61
01/14/2022 16:13:43 - INFO - __main__ -   Batch number = 62
01/14/2022 16:13:44 - INFO - __main__ -   Batch number = 63
01/14/2022 16:13:44 - INFO - __main__ -   Batch number = 64
01/14/2022 16:13:44 - INFO - __main__ -   Batch number = 65
01/14/2022 16:13:44 - INFO - __main__ -   Batch number = 66
01/14/2022 16:13:45 - INFO - __main__ -   Batch number = 67
01/14/2022 16:13:45 - INFO - __main__ -   Batch number = 68
01/14/2022 16:13:45 - INFO - __main__ -   Batch number = 69
01/14/2022 16:13:46 - INFO - __main__ -   Batch number = 70
01/14/2022 16:13:46 - INFO - __main__ -   Batch number = 71
01/14/2022 16:13:46 - INFO - __main__ -   Batch number = 72
01/14/2022 16:13:46 - INFO - __main__ -   Batch number = 73
01/14/2022 16:13:47 - INFO - __main__ -   Batch number = 74
01/14/2022 16:13:47 - INFO - __main__ -   Batch number = 75
01/14/2022 16:13:47 - INFO - __main__ -   Batch number = 76
01/14/2022 16:13:48 - INFO - __main__ -   Batch number = 77
01/14/2022 16:13:48 - INFO - __main__ -   Batch number = 78
01/14/2022 16:13:48 - INFO - __main__ -   Batch number = 79
01/14/2022 16:13:48 - INFO - __main__ -   Batch number = 80
01/14/2022 16:13:49 - INFO - __main__ -   Batch number = 81
01/14/2022 16:13:49 - INFO - __main__ -   Batch number = 82
01/14/2022 16:13:49 - INFO - __main__ -   Batch number = 83
01/14/2022 16:13:50 - INFO - __main__ -   Batch number = 84
01/14/2022 16:13:50 - INFO - __main__ -   Batch number = 85
01/14/2022 16:13:50 - INFO - __main__ -   Batch number = 86
01/14/2022 16:13:51 - INFO - __main__ -   Batch number = 87
01/14/2022 16:13:51 - INFO - __main__ -   Batch number = 88
01/14/2022 16:13:51 - INFO - __main__ -   Batch number = 89
01/14/2022 16:13:51 - INFO - __main__ -   Batch number = 90
01/14/2022 16:13:52 - INFO - __main__ -   Batch number = 91
01/14/2022 16:13:52 - INFO - __main__ -   Batch number = 92
01/14/2022 16:13:52 - INFO - __main__ -   Batch number = 93
01/14/2022 16:13:53 - INFO - __main__ -   Batch number = 94
01/14/2022 16:13:53 - INFO - __main__ -   Batch number = 95
01/14/2022 16:13:53 - INFO - __main__ -   Batch number = 96
01/14/2022 16:13:53 - INFO - __main__ -   Batch number = 97
01/14/2022 16:13:54 - INFO - __main__ -   Batch number = 98
01/14/2022 16:13:54 - INFO - __main__ -   Batch number = 99
01/14/2022 16:13:54 - INFO - __main__ -   Batch number = 100
01/14/2022 16:13:55 - INFO - __main__ -   Batch number = 101
01/14/2022 16:13:55 - INFO - __main__ -   Batch number = 102
01/14/2022 16:13:55 - INFO - __main__ -   Batch number = 103
01/14/2022 16:13:55 - INFO - __main__ -   Batch number = 104
01/14/2022 16:13:56 - INFO - __main__ -   Batch number = 105
01/14/2022 16:13:56 - INFO - __main__ -   Batch number = 106
01/14/2022 16:13:56 - INFO - __main__ -   Batch number = 107
01/14/2022 16:13:57 - INFO - __main__ -   Batch number = 108
01/14/2022 16:13:57 - INFO - __main__ -   Batch number = 109
01/14/2022 16:13:57 - INFO - __main__ -   Batch number = 110
01/14/2022 16:13:57 - INFO - __main__ -   Batch number = 111
01/14/2022 16:13:58 - INFO - __main__ -   Batch number = 112
01/14/2022 16:13:58 - INFO - __main__ -   Batch number = 113
01/14/2022 16:13:58 - INFO - __main__ -   Batch number = 114
01/14/2022 16:13:59 - INFO - __main__ -   Batch number = 115
01/14/2022 16:13:59 - INFO - __main__ -   Batch number = 116
01/14/2022 16:13:59 - INFO - __main__ -   Batch number = 117
01/14/2022 16:13:59 - INFO - __main__ -   Batch number = 118
01/14/2022 16:14:00 - INFO - __main__ -   Batch number = 119
01/14/2022 16:14:00 - INFO - __main__ -   Batch number = 120
01/14/2022 16:14:00 - INFO - __main__ -   Batch number = 121
01/14/2022 16:14:01 - INFO - __main__ -   Batch number = 122
01/14/2022 16:14:01 - INFO - __main__ -   Batch number = 123
01/14/2022 16:14:01 - INFO - __main__ -   Batch number = 124
01/14/2022 16:14:01 - INFO - __main__ -   Batch number = 125
01/14/2022 16:14:02 - INFO - __main__ -   Batch number = 126
01/14/2022 16:14:02 - INFO - __main__ -   Batch number = 127
01/14/2022 16:14:02 - INFO - __main__ -   Batch number = 128
01/14/2022 16:14:03 - INFO - __main__ -   Batch number = 129
01/14/2022 16:14:03 - INFO - __main__ -   Batch number = 130
01/14/2022 16:14:03 - INFO - __main__ -   Batch number = 131
01/14/2022 16:14:03 - INFO - __main__ -   Batch number = 132
01/14/2022 16:14:04 - INFO - __main__ -   Batch number = 133
01/14/2022 16:14:04 - INFO - __main__ -   Batch number = 134
01/14/2022 16:14:04 - INFO - __main__ -   Batch number = 135
01/14/2022 16:14:05 - INFO - __main__ -   Batch number = 136
01/14/2022 16:14:05 - INFO - __main__ -   Batch number = 137
01/14/2022 16:14:05 - INFO - __main__ -   Batch number = 138
01/14/2022 16:14:06 - INFO - __main__ -   Batch number = 139
01/14/2022 16:14:06 - INFO - __main__ -   Batch number = 140
01/14/2022 16:14:06 - INFO - __main__ -   Batch number = 141
01/14/2022 16:14:06 - INFO - __main__ -   Batch number = 142
01/14/2022 16:14:07 - INFO - __main__ -   Batch number = 143
01/14/2022 16:14:07 - INFO - __main__ -   Batch number = 144
01/14/2022 16:14:07 - INFO - __main__ -   Batch number = 145
01/14/2022 16:14:08 - INFO - __main__ -   Batch number = 146
01/14/2022 16:14:08 - INFO - __main__ -   Batch number = 147
01/14/2022 16:14:08 - INFO - __main__ -   Batch number = 148
01/14/2022 16:14:09 - INFO - __main__ -   Batch number = 149
01/14/2022 16:14:09 - INFO - __main__ -   Batch number = 150
01/14/2022 16:14:09 - INFO - __main__ -   Batch number = 151
01/14/2022 16:14:10 - INFO - __main__ -   Batch number = 152
01/14/2022 16:14:10 - INFO - __main__ -   Batch number = 153
01/14/2022 16:14:10 - INFO - __main__ -   Batch number = 154
01/14/2022 16:14:10 - INFO - __main__ -   Batch number = 155
01/14/2022 16:14:11 - INFO - __main__ -   Batch number = 156
01/14/2022 16:14:11 - INFO - __main__ -   Batch number = 157
01/14/2022 16:14:11 - INFO - __main__ -   Batch number = 158
01/14/2022 16:14:12 - INFO - __main__ -   Batch number = 159
01/14/2022 16:14:12 - INFO - __main__ -   Batch number = 160
01/14/2022 16:14:12 - INFO - __main__ -   Batch number = 161
01/14/2022 16:14:13 - INFO - __main__ -   Batch number = 162
01/14/2022 16:14:13 - INFO - __main__ -   Batch number = 163
01/14/2022 16:14:13 - INFO - __main__ -   Batch number = 164
01/14/2022 16:14:14 - INFO - __main__ -   Batch number = 165
01/14/2022 16:14:14 - INFO - __main__ -   Batch number = 166
01/14/2022 16:14:14 - INFO - __main__ -   Batch number = 167
01/14/2022 16:14:14 - INFO - __main__ -   Batch number = 168
01/14/2022 16:14:15 - INFO - __main__ -   Batch number = 169
01/14/2022 16:14:15 - INFO - __main__ -   Batch number = 170
01/14/2022 16:14:15 - INFO - __main__ -   Batch number = 171
01/14/2022 16:14:16 - INFO - __main__ -   Batch number = 172
01/14/2022 16:14:16 - INFO - __main__ -   Batch number = 173
01/14/2022 16:14:16 - INFO - __main__ -   Batch number = 174
01/14/2022 16:14:17 - INFO - __main__ -   Batch number = 175
01/14/2022 16:14:17 - INFO - __main__ -   Batch number = 176
01/14/2022 16:14:17 - INFO - __main__ -   Batch number = 177
01/14/2022 16:14:17 - INFO - __main__ -   Batch number = 178
01/14/2022 16:14:18 - INFO - __main__ -   Batch number = 179
01/14/2022 16:14:18 - INFO - __main__ -   Batch number = 180
01/14/2022 16:14:18 - INFO - __main__ -   Batch number = 181
01/14/2022 16:14:19 - INFO - __main__ -   Batch number = 182
01/14/2022 16:14:19 - INFO - __main__ -   Batch number = 183
01/14/2022 16:14:19 - INFO - __main__ -   Batch number = 184
01/14/2022 16:14:20 - INFO - __main__ -   Batch number = 185
01/14/2022 16:14:20 - INFO - __main__ -   Batch number = 186
01/14/2022 16:14:20 - INFO - __main__ -   Batch number = 187
01/14/2022 16:14:21 - INFO - __main__ -   Batch number = 188
01/14/2022 16:14:21 - INFO - __main__ -   Batch number = 189
01/14/2022 16:14:21 - INFO - __main__ -   Batch number = 190
01/14/2022 16:14:22 - INFO - __main__ -   Batch number = 191
01/14/2022 16:14:22 - INFO - __main__ -   Batch number = 192
01/14/2022 16:14:22 - INFO - __main__ -   Batch number = 193
01/14/2022 16:14:22 - INFO - __main__ -   Batch number = 194
01/14/2022 16:14:23 - INFO - __main__ -   Batch number = 195
01/14/2022 16:14:23 - INFO - __main__ -   Batch number = 196
01/14/2022 16:14:23 - INFO - __main__ -   Batch number = 197
01/14/2022 16:14:24 - INFO - __main__ -   Batch number = 198
01/14/2022 16:14:24 - INFO - __main__ -   Batch number = 199
01/14/2022 16:14:24 - INFO - __main__ -   Batch number = 200
01/14/2022 16:14:25 - INFO - __main__ -   Batch number = 201
01/14/2022 16:14:25 - INFO - __main__ -   Batch number = 202
01/14/2022 16:14:25 - INFO - __main__ -   Batch number = 203
01/14/2022 16:14:26 - INFO - __main__ -   Batch number = 204
01/14/2022 16:14:26 - INFO - __main__ -   Batch number = 205
01/14/2022 16:14:26 - INFO - __main__ -   Batch number = 206
01/14/2022 16:14:26 - INFO - __main__ -   Batch number = 207
01/14/2022 16:14:27 - INFO - __main__ -   Batch number = 208
01/14/2022 16:14:27 - INFO - __main__ -   Batch number = 209
01/14/2022 16:14:27 - INFO - __main__ -   Batch number = 210
01/14/2022 16:14:28 - INFO - __main__ -   Batch number = 211
01/14/2022 16:14:28 - INFO - __main__ -   Batch number = 212
01/14/2022 16:14:28 - INFO - __main__ -   Batch number = 213
01/14/2022 16:14:29 - INFO - __main__ -   Batch number = 214
01/14/2022 16:14:29 - INFO - __main__ -   Batch number = 215
01/14/2022 16:14:29 - INFO - __main__ -   Batch number = 216
01/14/2022 16:14:30 - INFO - __main__ -   Batch number = 217
01/14/2022 16:14:30 - INFO - __main__ -   Batch number = 218
01/14/2022 16:14:30 - INFO - __main__ -   Batch number = 219
01/14/2022 16:14:31 - INFO - __main__ -   Batch number = 220
01/14/2022 16:14:31 - INFO - __main__ -   Batch number = 221
01/14/2022 16:14:31 - INFO - __main__ -   Batch number = 222
01/14/2022 16:14:31 - INFO - __main__ -   Batch number = 223
01/14/2022 16:14:32 - INFO - __main__ -   Batch number = 224
01/14/2022 16:14:32 - INFO - __main__ -   Batch number = 225
01/14/2022 16:14:32 - INFO - __main__ -   Batch number = 226
01/14/2022 16:14:33 - INFO - __main__ -   Batch number = 227
01/14/2022 16:14:33 - INFO - __main__ -   Batch number = 228
01/14/2022 16:14:33 - INFO - __main__ -   Batch number = 229
01/14/2022 16:14:34 - INFO - __main__ -   Batch number = 230
01/14/2022 16:14:34 - INFO - __main__ -   Batch number = 231
01/14/2022 16:14:34 - INFO - __main__ -   Batch number = 232
01/14/2022 16:14:34 - INFO - __main__ -   Batch number = 233
01/14/2022 16:14:35 - INFO - __main__ -   Batch number = 234
01/14/2022 16:14:35 - INFO - __main__ -   Batch number = 235
01/14/2022 16:14:35 - INFO - __main__ -   Batch number = 236
01/14/2022 16:14:36 - INFO - __main__ -   Batch number = 237
01/14/2022 16:14:36 - INFO - __main__ -   Batch number = 238
01/14/2022 16:14:36 - INFO - __main__ -   Batch number = 239
01/14/2022 16:14:37 - INFO - __main__ -   Batch number = 240
01/14/2022 16:14:37 - INFO - __main__ -   Batch number = 241
01/14/2022 16:14:37 - INFO - __main__ -   Batch number = 242
01/14/2022 16:14:38 - INFO - __main__ -   Batch number = 243
01/14/2022 16:14:38 - INFO - __main__ -   Batch number = 244
01/14/2022 16:14:38 - INFO - __main__ -   Batch number = 245
01/14/2022 16:14:38 - INFO - __main__ -   Batch number = 246
01/14/2022 16:14:39 - INFO - __main__ -   Batch number = 247
01/14/2022 16:14:39 - INFO - __main__ -   Batch number = 248
01/14/2022 16:14:39 - INFO - __main__ -   Batch number = 249
01/14/2022 16:14:40 - INFO - __main__ -   Batch number = 250
01/14/2022 16:14:40 - INFO - __main__ -   Batch number = 251
01/14/2022 16:14:40 - INFO - __main__ -   Batch number = 252
01/14/2022 16:14:41 - INFO - __main__ -   Batch number = 253
01/14/2022 16:14:41 - INFO - __main__ -   Batch number = 254
01/14/2022 16:14:41 - INFO - __main__ -   Batch number = 255
01/14/2022 16:14:42 - INFO - __main__ -   Batch number = 256
01/14/2022 16:14:42 - INFO - __main__ -   Batch number = 257
01/14/2022 16:14:42 - INFO - __main__ -   Batch number = 258
01/14/2022 16:14:42 - INFO - __main__ -   Batch number = 259
01/14/2022 16:14:43 - INFO - __main__ -   Batch number = 260
01/14/2022 16:14:43 - INFO - __main__ -   Batch number = 261
01/14/2022 16:14:43 - INFO - __main__ -   Batch number = 262
01/14/2022 16:14:44 - INFO - __main__ -   Batch number = 263
01/14/2022 16:14:44 - INFO - __main__ -   Batch number = 264
01/14/2022 16:14:44 - INFO - __main__ -   Batch number = 265
01/14/2022 16:14:45 - INFO - __main__ -   Batch number = 266
01/14/2022 16:14:45 - INFO - __main__ -   Batch number = 267
01/14/2022 16:14:45 - INFO - __main__ -   Batch number = 268
01/14/2022 16:14:46 - INFO - __main__ -   Batch number = 269
01/14/2022 16:14:46 - INFO - __main__ -   Batch number = 270
01/14/2022 16:14:46 - INFO - __main__ -   Batch number = 271
01/14/2022 16:14:46 - INFO - __main__ -   Batch number = 272
01/14/2022 16:14:47 - INFO - __main__ -   Batch number = 273
01/14/2022 16:14:47 - INFO - __main__ -   Batch number = 274
01/14/2022 16:14:47 - INFO - __main__ -   Batch number = 275
01/14/2022 16:14:48 - INFO - __main__ -   Batch number = 276
01/14/2022 16:14:48 - INFO - __main__ -   Batch number = 277
01/14/2022 16:14:48 - INFO - __main__ -   Batch number = 278
01/14/2022 16:14:49 - INFO - __main__ -   Batch number = 279
01/14/2022 16:14:49 - INFO - __main__ -   Batch number = 280
01/14/2022 16:14:49 - INFO - __main__ -   Batch number = 281
01/14/2022 16:14:49 - INFO - __main__ -   Batch number = 282
01/14/2022 16:14:50 - INFO - __main__ -   Batch number = 283
01/14/2022 16:14:50 - INFO - __main__ -   Batch number = 284
01/14/2022 16:14:50 - INFO - __main__ -   Batch number = 285
01/14/2022 16:14:51 - INFO - __main__ -   Batch number = 286
01/14/2022 16:14:51 - INFO - __main__ -   Batch number = 287
01/14/2022 16:14:51 - INFO - __main__ -   Batch number = 288
01/14/2022 16:14:52 - INFO - __main__ -   Batch number = 289
01/14/2022 16:14:52 - INFO - __main__ -   Batch number = 290
01/14/2022 16:14:52 - INFO - __main__ -   Batch number = 291
01/14/2022 16:14:52 - INFO - __main__ -   Batch number = 292
01/14/2022 16:14:53 - INFO - __main__ -   Batch number = 293
01/14/2022 16:14:53 - INFO - __main__ -   Batch number = 294
01/14/2022 16:14:53 - INFO - __main__ -   Batch number = 295
01/14/2022 16:14:54 - INFO - __main__ -   Batch number = 296
01/14/2022 16:14:54 - INFO - __main__ -   Batch number = 297
01/14/2022 16:14:54 - INFO - __main__ -   Batch number = 298
01/14/2022 16:14:55 - INFO - __main__ -   Batch number = 299
01/14/2022 16:14:55 - INFO - __main__ -   Batch number = 300
01/14/2022 16:14:55 - INFO - __main__ -   Batch number = 301
01/14/2022 16:14:56 - INFO - __main__ -   Batch number = 302
01/14/2022 16:14:56 - INFO - __main__ -   Batch number = 303
01/14/2022 16:14:56 - INFO - __main__ -   Batch number = 304
01/14/2022 16:14:56 - INFO - __main__ -   Batch number = 305
01/14/2022 16:14:57 - INFO - __main__ -   Batch number = 306
01/14/2022 16:14:57 - INFO - __main__ -   Batch number = 307
01/14/2022 16:14:57 - INFO - __main__ -   Batch number = 308
01/14/2022 16:14:58 - INFO - __main__ -   Batch number = 309
01/14/2022 16:14:58 - INFO - __main__ -   Batch number = 310
01/14/2022 16:14:58 - INFO - __main__ -   Batch number = 311
01/14/2022 16:14:59 - INFO - __main__ -   Batch number = 312
01/14/2022 16:14:59 - INFO - __main__ -   Batch number = 313
01/14/2022 16:14:59 - INFO - __main__ -   Batch number = 314
01/14/2022 16:14:59 - INFO - __main__ -   Batch number = 315
01/14/2022 16:15:00 - INFO - __main__ -   Batch number = 316
01/14/2022 16:15:00 - INFO - __main__ -   Batch number = 317
01/14/2022 16:15:00 - INFO - __main__ -   Batch number = 318
01/14/2022 16:15:01 - INFO - __main__ -   Batch number = 319
01/14/2022 16:15:01 - INFO - __main__ -   Batch number = 320
01/14/2022 16:15:01 - INFO - __main__ -   Batch number = 321
01/14/2022 16:15:04 - INFO - __main__ -   ***** Evaluation result  in zh *****
01/14/2022 16:15:04 - INFO - __main__ -     f1 = 0.33014956704277093
01/14/2022 16:15:04 - INFO - __main__ -     loss = 3.2364519478004667
01/14/2022 16:15:04 - INFO - __main__ -     precision = 0.2460016423571736
01/14/2022 16:15:04 - INFO - __main__ -     recall = 0.501794687724336
01/14/2022 16:15:06 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='zh', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:15:06 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/14/2022 16:15:06 - INFO - __main__ -   Seed = 2
01/14/2022 16:15:06 - INFO - root -   save model
01/14/2022 16:15:06 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='zh', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:15:06 - INFO - __main__ -   Loading pretrained model and tokenizer
01/14/2022 16:15:09 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/14/2022 16:15:15 - INFO - __main__ -   Using lang2id = None
01/14/2022 16:15:15 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/14/2022 16:15:15 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/
01/14/2022 16:15:15 - INFO - root -   Trying to decide if add adapter
01/14/2022 16:15:15 - INFO - root -   loading task adapter
01/14/2022 16:15:15 - INFO - root -   loading lang adpater en/wiki@ukp,pt/wiki@ukp,id/wiki@ukp,tr/wiki@ukp,cs/wiki@ukp,vi/wiki@ukp,eu/wiki@ukp,fa/wiki@ukp,zh_yue/wiki@ukp,zh/wiki@ukp
01/14/2022 16:15:15 - INFO - __main__ -   Adapter Languages : ['en', 'pt', 'id', 'tr', 'cs', 'vi', 'eu', 'fa', 'zh_yue', 'zh'], Length : 10
01/14/2022 16:15:15 - INFO - __main__ -   Adapter Names ['en/wiki@ukp', 'pt/wiki@ukp', 'id/wiki@ukp', 'tr/wiki@ukp', 'cs/wiki@ukp', 'vi/wiki@ukp', 'eu/wiki@ukp', 'fa/wiki@ukp', 'zh_yue/wiki@ukp', 'zh/wiki@ukp'], Length : 10
01/14/2022 16:15:15 - INFO - __main__ -   Language = en
01/14/2022 16:15:15 - INFO - __main__ -   Adapter Name = en/wiki@ukp
01/14/2022 16:15:16 - INFO - __main__ -   Language = pt
01/14/2022 16:15:16 - INFO - __main__ -   Adapter Name = pt/wiki@ukp
01/14/2022 16:15:18 - INFO - __main__ -   Language = id
01/14/2022 16:15:18 - INFO - __main__ -   Adapter Name = id/wiki@ukp
01/14/2022 16:15:20 - INFO - __main__ -   Language = tr
01/14/2022 16:15:20 - INFO - __main__ -   Adapter Name = tr/wiki@ukp
01/14/2022 16:15:22 - INFO - __main__ -   Language = cs
01/14/2022 16:15:22 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
01/14/2022 16:15:24 - INFO - __main__ -   Language = vi
01/14/2022 16:15:24 - INFO - __main__ -   Adapter Name = vi/wiki@ukp
01/14/2022 16:15:26 - INFO - __main__ -   Language = eu
01/14/2022 16:15:26 - INFO - __main__ -   Adapter Name = eu/wiki@ukp
01/14/2022 16:15:29 - INFO - __main__ -   Language = fa
01/14/2022 16:15:29 - INFO - __main__ -   Adapter Name = fa/wiki@ukp
01/14/2022 16:15:31 - INFO - __main__ -   Language = zh_yue
01/14/2022 16:15:31 - INFO - __main__ -   Adapter Name = zh_yue/wiki@ukp
01/14/2022 16:15:33 - INFO - __main__ -   Language = zh
01/14/2022 16:15:33 - INFO - __main__ -   Adapter Name = zh/wiki@ukp
01/14/2022 16:15:37 - INFO - __main__ -   Args Adapter Weight = equal
01/14/2022 16:15:37 - INFO - __main__ -   Adapter Languages = ['en', 'pt', 'id', 'tr', 'cs', 'vi', 'eu', 'fa', 'zh_yue', 'zh']
01/14/2022 16:15:37 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/14/2022 16:15:37 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/14/2022 16:15:37 - INFO - __main__ -   Length of Adapter Weights = 10
01/14/2022 16:15:37 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/cached_test_zh_bert-base-multilingual-cased_128
01/14/2022 16:15:39 - INFO - __main__ -   ***** Running evaluation  in zh *****
01/14/2022 16:15:39 - INFO - __main__ -     Num examples = 10257
01/14/2022 16:15:39 - INFO - __main__ -     Batch size = 32
01/14/2022 16:15:39 - INFO - __main__ -   Batch number = 1
01/14/2022 16:15:39 - INFO - __main__ -   Batch number = 2
01/14/2022 16:15:39 - INFO - __main__ -   Batch number = 3
01/14/2022 16:15:40 - INFO - __main__ -   Batch number = 4
01/14/2022 16:15:40 - INFO - __main__ -   Batch number = 5
01/14/2022 16:15:40 - INFO - __main__ -   Batch number = 6
01/14/2022 16:15:40 - INFO - __main__ -   Batch number = 7
01/14/2022 16:15:41 - INFO - __main__ -   Batch number = 8
01/14/2022 16:15:41 - INFO - __main__ -   Batch number = 9
01/14/2022 16:15:41 - INFO - __main__ -   Batch number = 10
01/14/2022 16:15:42 - INFO - __main__ -   Batch number = 11
01/14/2022 16:15:42 - INFO - __main__ -   Batch number = 12
01/14/2022 16:15:42 - INFO - __main__ -   Batch number = 13
01/14/2022 16:15:42 - INFO - __main__ -   Batch number = 14
01/14/2022 16:15:43 - INFO - __main__ -   Batch number = 15
01/14/2022 16:15:43 - INFO - __main__ -   Batch number = 16
01/14/2022 16:15:43 - INFO - __main__ -   Batch number = 17
01/14/2022 16:15:44 - INFO - __main__ -   Batch number = 18
01/14/2022 16:15:44 - INFO - __main__ -   Batch number = 19
01/14/2022 16:15:44 - INFO - __main__ -   Batch number = 20
01/14/2022 16:15:45 - INFO - __main__ -   Batch number = 21
01/14/2022 16:15:45 - INFO - __main__ -   Batch number = 22
01/14/2022 16:15:45 - INFO - __main__ -   Batch number = 23
01/14/2022 16:15:45 - INFO - __main__ -   Batch number = 24
01/14/2022 16:15:46 - INFO - __main__ -   Batch number = 25
01/14/2022 16:15:46 - INFO - __main__ -   Batch number = 26
01/14/2022 16:15:46 - INFO - __main__ -   Batch number = 27
01/14/2022 16:15:46 - INFO - __main__ -   Batch number = 28
01/14/2022 16:15:47 - INFO - __main__ -   Batch number = 29
01/14/2022 16:15:47 - INFO - __main__ -   Batch number = 30
01/14/2022 16:15:47 - INFO - __main__ -   Batch number = 31
01/14/2022 16:15:48 - INFO - __main__ -   Batch number = 32
01/14/2022 16:15:48 - INFO - __main__ -   Batch number = 33
01/14/2022 16:15:48 - INFO - __main__ -   Batch number = 34
01/14/2022 16:15:48 - INFO - __main__ -   Batch number = 35
01/14/2022 16:15:49 - INFO - __main__ -   Batch number = 36
01/14/2022 16:15:49 - INFO - __main__ -   Batch number = 37
01/14/2022 16:15:49 - INFO - __main__ -   Batch number = 38
01/14/2022 16:15:50 - INFO - __main__ -   Batch number = 39
01/14/2022 16:15:50 - INFO - __main__ -   Batch number = 40
01/14/2022 16:15:50 - INFO - __main__ -   Batch number = 41
01/14/2022 16:15:50 - INFO - __main__ -   Batch number = 42
01/14/2022 16:15:51 - INFO - __main__ -   Batch number = 43
01/14/2022 16:15:51 - INFO - __main__ -   Batch number = 44
01/14/2022 16:15:51 - INFO - __main__ -   Batch number = 45
01/14/2022 16:15:52 - INFO - __main__ -   Batch number = 46
01/14/2022 16:15:52 - INFO - __main__ -   Batch number = 47
01/14/2022 16:15:52 - INFO - __main__ -   Batch number = 48
01/14/2022 16:15:52 - INFO - __main__ -   Batch number = 49
01/14/2022 16:15:53 - INFO - __main__ -   Batch number = 50
01/14/2022 16:15:53 - INFO - __main__ -   Batch number = 51
01/14/2022 16:15:53 - INFO - __main__ -   Batch number = 52
01/14/2022 16:15:54 - INFO - __main__ -   Batch number = 53
01/14/2022 16:15:54 - INFO - __main__ -   Batch number = 54
01/14/2022 16:15:54 - INFO - __main__ -   Batch number = 55
01/14/2022 16:15:54 - INFO - __main__ -   Batch number = 56
01/14/2022 16:15:55 - INFO - __main__ -   Batch number = 57
01/14/2022 16:15:55 - INFO - __main__ -   Batch number = 58
01/14/2022 16:15:55 - INFO - __main__ -   Batch number = 59
01/14/2022 16:15:56 - INFO - __main__ -   Batch number = 60
01/14/2022 16:15:56 - INFO - __main__ -   Batch number = 61
01/14/2022 16:15:56 - INFO - __main__ -   Batch number = 62
01/14/2022 16:15:56 - INFO - __main__ -   Batch number = 63
01/14/2022 16:15:57 - INFO - __main__ -   Batch number = 64
01/14/2022 16:15:57 - INFO - __main__ -   Batch number = 65
01/14/2022 16:15:57 - INFO - __main__ -   Batch number = 66
01/14/2022 16:15:58 - INFO - __main__ -   Batch number = 67
01/14/2022 16:15:58 - INFO - __main__ -   Batch number = 68
01/14/2022 16:15:58 - INFO - __main__ -   Batch number = 69
01/14/2022 16:15:58 - INFO - __main__ -   Batch number = 70
01/14/2022 16:15:59 - INFO - __main__ -   Batch number = 71
01/14/2022 16:15:59 - INFO - __main__ -   Batch number = 72
01/14/2022 16:15:59 - INFO - __main__ -   Batch number = 73
01/14/2022 16:16:00 - INFO - __main__ -   Batch number = 74
01/14/2022 16:16:00 - INFO - __main__ -   Batch number = 75
01/14/2022 16:16:00 - INFO - __main__ -   Batch number = 76
01/14/2022 16:16:00 - INFO - __main__ -   Batch number = 77
01/14/2022 16:16:01 - INFO - __main__ -   Batch number = 78
01/14/2022 16:16:01 - INFO - __main__ -   Batch number = 79
01/14/2022 16:16:01 - INFO - __main__ -   Batch number = 80
01/14/2022 16:16:02 - INFO - __main__ -   Batch number = 81
01/14/2022 16:16:02 - INFO - __main__ -   Batch number = 82
01/14/2022 16:16:02 - INFO - __main__ -   Batch number = 83
01/14/2022 16:16:02 - INFO - __main__ -   Batch number = 84
01/14/2022 16:16:03 - INFO - __main__ -   Batch number = 85
01/14/2022 16:16:03 - INFO - __main__ -   Batch number = 86
01/14/2022 16:16:03 - INFO - __main__ -   Batch number = 87
01/14/2022 16:16:04 - INFO - __main__ -   Batch number = 88
01/14/2022 16:16:04 - INFO - __main__ -   Batch number = 89
01/14/2022 16:16:04 - INFO - __main__ -   Batch number = 90
01/14/2022 16:16:04 - INFO - __main__ -   Batch number = 91
01/14/2022 16:16:05 - INFO - __main__ -   Batch number = 92
01/14/2022 16:16:05 - INFO - __main__ -   Batch number = 93
01/14/2022 16:16:05 - INFO - __main__ -   Batch number = 94
01/14/2022 16:16:06 - INFO - __main__ -   Batch number = 95
01/14/2022 16:16:06 - INFO - __main__ -   Batch number = 96
01/14/2022 16:16:06 - INFO - __main__ -   Batch number = 97
01/14/2022 16:16:07 - INFO - __main__ -   Batch number = 98
01/14/2022 16:16:07 - INFO - __main__ -   Batch number = 99
01/14/2022 16:16:07 - INFO - __main__ -   Batch number = 100
01/14/2022 16:16:07 - INFO - __main__ -   Batch number = 101
01/14/2022 16:16:08 - INFO - __main__ -   Batch number = 102
01/14/2022 16:16:08 - INFO - __main__ -   Batch number = 103
01/14/2022 16:16:08 - INFO - __main__ -   Batch number = 104
01/14/2022 16:16:09 - INFO - __main__ -   Batch number = 105
01/14/2022 16:16:09 - INFO - __main__ -   Batch number = 106
01/14/2022 16:16:09 - INFO - __main__ -   Batch number = 107
01/14/2022 16:16:09 - INFO - __main__ -   Batch number = 108
01/14/2022 16:16:10 - INFO - __main__ -   Batch number = 109
01/14/2022 16:16:10 - INFO - __main__ -   Batch number = 110
01/14/2022 16:16:10 - INFO - __main__ -   Batch number = 111
01/14/2022 16:16:11 - INFO - __main__ -   Batch number = 112
01/14/2022 16:16:11 - INFO - __main__ -   Batch number = 113
01/14/2022 16:16:11 - INFO - __main__ -   Batch number = 114
01/14/2022 16:16:12 - INFO - __main__ -   Batch number = 115
01/14/2022 16:16:12 - INFO - __main__ -   Batch number = 116
01/14/2022 16:16:12 - INFO - __main__ -   Batch number = 117
01/14/2022 16:16:13 - INFO - __main__ -   Batch number = 118
01/14/2022 16:16:13 - INFO - __main__ -   Batch number = 119
01/14/2022 16:16:13 - INFO - __main__ -   Batch number = 120
01/14/2022 16:16:13 - INFO - __main__ -   Batch number = 121
01/14/2022 16:16:14 - INFO - __main__ -   Batch number = 122
01/14/2022 16:16:14 - INFO - __main__ -   Batch number = 123
01/14/2022 16:16:14 - INFO - __main__ -   Batch number = 124
01/14/2022 16:16:15 - INFO - __main__ -   Batch number = 125
01/14/2022 16:16:15 - INFO - __main__ -   Batch number = 126
01/14/2022 16:16:15 - INFO - __main__ -   Batch number = 127
01/14/2022 16:16:16 - INFO - __main__ -   Batch number = 128
01/14/2022 16:16:16 - INFO - __main__ -   Batch number = 129
01/14/2022 16:16:16 - INFO - __main__ -   Batch number = 130
01/14/2022 16:16:17 - INFO - __main__ -   Batch number = 131
01/14/2022 16:16:17 - INFO - __main__ -   Batch number = 132
01/14/2022 16:16:17 - INFO - __main__ -   Batch number = 133
01/14/2022 16:16:17 - INFO - __main__ -   Batch number = 134
01/14/2022 16:16:18 - INFO - __main__ -   Batch number = 135
01/14/2022 16:16:18 - INFO - __main__ -   Batch number = 136
01/14/2022 16:16:18 - INFO - __main__ -   Batch number = 137
01/14/2022 16:16:19 - INFO - __main__ -   Batch number = 138
01/14/2022 16:16:19 - INFO - __main__ -   Batch number = 139
01/14/2022 16:16:19 - INFO - __main__ -   Batch number = 140
01/14/2022 16:16:20 - INFO - __main__ -   Batch number = 141
01/14/2022 16:16:20 - INFO - __main__ -   Batch number = 142
01/14/2022 16:16:20 - INFO - __main__ -   Batch number = 143
01/14/2022 16:16:21 - INFO - __main__ -   Batch number = 144
01/14/2022 16:16:21 - INFO - __main__ -   Batch number = 145
01/14/2022 16:16:21 - INFO - __main__ -   Batch number = 146
01/14/2022 16:16:21 - INFO - __main__ -   Batch number = 147
01/14/2022 16:16:22 - INFO - __main__ -   Batch number = 148
01/14/2022 16:16:22 - INFO - __main__ -   Batch number = 149
01/14/2022 16:16:22 - INFO - __main__ -   Batch number = 150
01/14/2022 16:16:23 - INFO - __main__ -   Batch number = 151
01/14/2022 16:16:23 - INFO - __main__ -   Batch number = 152
01/14/2022 16:16:23 - INFO - __main__ -   Batch number = 153
01/14/2022 16:16:24 - INFO - __main__ -   Batch number = 154
01/14/2022 16:16:24 - INFO - __main__ -   Batch number = 155
01/14/2022 16:16:24 - INFO - __main__ -   Batch number = 156
01/14/2022 16:16:25 - INFO - __main__ -   Batch number = 157
01/14/2022 16:16:25 - INFO - __main__ -   Batch number = 158
01/14/2022 16:16:25 - INFO - __main__ -   Batch number = 159
01/14/2022 16:16:26 - INFO - __main__ -   Batch number = 160
01/14/2022 16:16:26 - INFO - __main__ -   Batch number = 161
01/14/2022 16:16:26 - INFO - __main__ -   Batch number = 162
01/14/2022 16:16:26 - INFO - __main__ -   Batch number = 163
01/14/2022 16:16:27 - INFO - __main__ -   Batch number = 164
01/14/2022 16:16:27 - INFO - __main__ -   Batch number = 165
01/14/2022 16:16:27 - INFO - __main__ -   Batch number = 166
01/14/2022 16:16:28 - INFO - __main__ -   Batch number = 167
01/14/2022 16:16:28 - INFO - __main__ -   Batch number = 168
01/14/2022 16:16:28 - INFO - __main__ -   Batch number = 169
01/14/2022 16:16:29 - INFO - __main__ -   Batch number = 170
01/14/2022 16:16:29 - INFO - __main__ -   Batch number = 171
01/14/2022 16:16:29 - INFO - __main__ -   Batch number = 172
01/14/2022 16:16:29 - INFO - __main__ -   Batch number = 173
01/14/2022 16:16:30 - INFO - __main__ -   Batch number = 174
01/14/2022 16:16:30 - INFO - __main__ -   Batch number = 175
01/14/2022 16:16:30 - INFO - __main__ -   Batch number = 176
01/14/2022 16:16:31 - INFO - __main__ -   Batch number = 177
01/14/2022 16:16:31 - INFO - __main__ -   Batch number = 178
01/14/2022 16:16:31 - INFO - __main__ -   Batch number = 179
01/14/2022 16:16:31 - INFO - __main__ -   Batch number = 180
01/14/2022 16:16:32 - INFO - __main__ -   Batch number = 181
01/14/2022 16:16:32 - INFO - __main__ -   Batch number = 182
01/14/2022 16:16:32 - INFO - __main__ -   Batch number = 183
01/14/2022 16:16:33 - INFO - __main__ -   Batch number = 184
01/14/2022 16:16:33 - INFO - __main__ -   Batch number = 185
01/14/2022 16:16:33 - INFO - __main__ -   Batch number = 186
01/14/2022 16:16:34 - INFO - __main__ -   Batch number = 187
01/14/2022 16:16:34 - INFO - __main__ -   Batch number = 188
01/14/2022 16:16:34 - INFO - __main__ -   Batch number = 189
01/14/2022 16:16:34 - INFO - __main__ -   Batch number = 190
01/14/2022 16:16:35 - INFO - __main__ -   Batch number = 191
01/14/2022 16:16:35 - INFO - __main__ -   Batch number = 192
01/14/2022 16:16:35 - INFO - __main__ -   Batch number = 193
01/14/2022 16:16:36 - INFO - __main__ -   Batch number = 194
01/14/2022 16:16:36 - INFO - __main__ -   Batch number = 195
01/14/2022 16:16:36 - INFO - __main__ -   Batch number = 196
01/14/2022 16:16:36 - INFO - __main__ -   Batch number = 197
01/14/2022 16:16:37 - INFO - __main__ -   Batch number = 198
01/14/2022 16:16:37 - INFO - __main__ -   Batch number = 199
01/14/2022 16:16:37 - INFO - __main__ -   Batch number = 200
01/14/2022 16:16:38 - INFO - __main__ -   Batch number = 201
01/14/2022 16:16:38 - INFO - __main__ -   Batch number = 202
01/14/2022 16:16:38 - INFO - __main__ -   Batch number = 203
01/14/2022 16:16:39 - INFO - __main__ -   Batch number = 204
01/14/2022 16:16:39 - INFO - __main__ -   Batch number = 205
01/14/2022 16:16:39 - INFO - __main__ -   Batch number = 206
01/14/2022 16:16:40 - INFO - __main__ -   Batch number = 207
01/14/2022 16:16:40 - INFO - __main__ -   Batch number = 208
01/14/2022 16:16:40 - INFO - __main__ -   Batch number = 209
01/14/2022 16:16:41 - INFO - __main__ -   Batch number = 210
01/14/2022 16:16:41 - INFO - __main__ -   Batch number = 211
01/14/2022 16:16:41 - INFO - __main__ -   Batch number = 212
01/14/2022 16:16:42 - INFO - __main__ -   Batch number = 213
01/14/2022 16:16:42 - INFO - __main__ -   Batch number = 214
01/14/2022 16:16:42 - INFO - __main__ -   Batch number = 215
01/14/2022 16:16:42 - INFO - __main__ -   Batch number = 216
01/14/2022 16:16:43 - INFO - __main__ -   Batch number = 217
01/14/2022 16:16:43 - INFO - __main__ -   Batch number = 218
01/14/2022 16:16:43 - INFO - __main__ -   Batch number = 219
01/14/2022 16:16:44 - INFO - __main__ -   Batch number = 220
01/14/2022 16:16:44 - INFO - __main__ -   Batch number = 221
01/14/2022 16:16:44 - INFO - __main__ -   Batch number = 222
01/14/2022 16:16:45 - INFO - __main__ -   Batch number = 223
01/14/2022 16:16:45 - INFO - __main__ -   Batch number = 224
01/14/2022 16:16:45 - INFO - __main__ -   Batch number = 225
01/14/2022 16:16:46 - INFO - __main__ -   Batch number = 226
01/14/2022 16:16:46 - INFO - __main__ -   Batch number = 227
01/14/2022 16:16:46 - INFO - __main__ -   Batch number = 228
01/14/2022 16:16:46 - INFO - __main__ -   Batch number = 229
01/14/2022 16:16:47 - INFO - __main__ -   Batch number = 230
01/14/2022 16:16:47 - INFO - __main__ -   Batch number = 231
01/14/2022 16:16:47 - INFO - __main__ -   Batch number = 232
01/14/2022 16:16:48 - INFO - __main__ -   Batch number = 233
01/14/2022 16:16:48 - INFO - __main__ -   Batch number = 234
01/14/2022 16:16:48 - INFO - __main__ -   Batch number = 235
01/14/2022 16:16:49 - INFO - __main__ -   Batch number = 236
01/14/2022 16:16:49 - INFO - __main__ -   Batch number = 237
01/14/2022 16:16:49 - INFO - __main__ -   Batch number = 238
01/14/2022 16:16:50 - INFO - __main__ -   Batch number = 239
01/14/2022 16:16:50 - INFO - __main__ -   Batch number = 240
01/14/2022 16:16:50 - INFO - __main__ -   Batch number = 241
01/14/2022 16:16:50 - INFO - __main__ -   Batch number = 242
01/14/2022 16:16:51 - INFO - __main__ -   Batch number = 243
01/14/2022 16:16:51 - INFO - __main__ -   Batch number = 244
01/14/2022 16:16:51 - INFO - __main__ -   Batch number = 245
01/14/2022 16:16:52 - INFO - __main__ -   Batch number = 246
01/14/2022 16:16:52 - INFO - __main__ -   Batch number = 247
01/14/2022 16:16:52 - INFO - __main__ -   Batch number = 248
01/14/2022 16:16:53 - INFO - __main__ -   Batch number = 249
01/14/2022 16:16:53 - INFO - __main__ -   Batch number = 250
01/14/2022 16:16:53 - INFO - __main__ -   Batch number = 251
01/14/2022 16:16:54 - INFO - __main__ -   Batch number = 252
01/14/2022 16:16:54 - INFO - __main__ -   Batch number = 253
01/14/2022 16:16:54 - INFO - __main__ -   Batch number = 254
01/14/2022 16:16:54 - INFO - __main__ -   Batch number = 255
01/14/2022 16:16:55 - INFO - __main__ -   Batch number = 256
01/14/2022 16:16:55 - INFO - __main__ -   Batch number = 257
01/14/2022 16:16:55 - INFO - __main__ -   Batch number = 258
01/14/2022 16:16:56 - INFO - __main__ -   Batch number = 259
01/14/2022 16:16:56 - INFO - __main__ -   Batch number = 260
01/14/2022 16:16:56 - INFO - __main__ -   Batch number = 261
01/14/2022 16:16:57 - INFO - __main__ -   Batch number = 262
01/14/2022 16:16:57 - INFO - __main__ -   Batch number = 263
01/14/2022 16:16:57 - INFO - __main__ -   Batch number = 264
01/14/2022 16:16:57 - INFO - __main__ -   Batch number = 265
01/14/2022 16:16:58 - INFO - __main__ -   Batch number = 266
01/14/2022 16:16:58 - INFO - __main__ -   Batch number = 267
01/14/2022 16:16:58 - INFO - __main__ -   Batch number = 268
01/14/2022 16:16:59 - INFO - __main__ -   Batch number = 269
01/14/2022 16:16:59 - INFO - __main__ -   Batch number = 270
01/14/2022 16:16:59 - INFO - __main__ -   Batch number = 271
01/14/2022 16:17:00 - INFO - __main__ -   Batch number = 272
01/14/2022 16:17:00 - INFO - __main__ -   Batch number = 273
01/14/2022 16:17:00 - INFO - __main__ -   Batch number = 274
01/14/2022 16:17:01 - INFO - __main__ -   Batch number = 275
01/14/2022 16:17:01 - INFO - __main__ -   Batch number = 276
01/14/2022 16:17:01 - INFO - __main__ -   Batch number = 277
01/14/2022 16:17:02 - INFO - __main__ -   Batch number = 278
01/14/2022 16:17:02 - INFO - __main__ -   Batch number = 279
01/14/2022 16:17:02 - INFO - __main__ -   Batch number = 280
01/14/2022 16:17:03 - INFO - __main__ -   Batch number = 281
01/14/2022 16:17:03 - INFO - __main__ -   Batch number = 282
01/14/2022 16:17:03 - INFO - __main__ -   Batch number = 283
01/14/2022 16:17:03 - INFO - __main__ -   Batch number = 284
01/14/2022 16:17:04 - INFO - __main__ -   Batch number = 285
01/14/2022 16:17:04 - INFO - __main__ -   Batch number = 286
01/14/2022 16:17:04 - INFO - __main__ -   Batch number = 287
01/14/2022 16:17:05 - INFO - __main__ -   Batch number = 288
01/14/2022 16:17:05 - INFO - __main__ -   Batch number = 289
01/14/2022 16:17:05 - INFO - __main__ -   Batch number = 290
01/14/2022 16:17:06 - INFO - __main__ -   Batch number = 291
01/14/2022 16:17:06 - INFO - __main__ -   Batch number = 292
01/14/2022 16:17:06 - INFO - __main__ -   Batch number = 293
01/14/2022 16:17:07 - INFO - __main__ -   Batch number = 294
01/14/2022 16:17:07 - INFO - __main__ -   Batch number = 295
01/14/2022 16:17:07 - INFO - __main__ -   Batch number = 296
01/14/2022 16:17:07 - INFO - __main__ -   Batch number = 297
01/14/2022 16:17:08 - INFO - __main__ -   Batch number = 298
01/14/2022 16:17:08 - INFO - __main__ -   Batch number = 299
01/14/2022 16:17:08 - INFO - __main__ -   Batch number = 300
01/14/2022 16:17:09 - INFO - __main__ -   Batch number = 301
01/14/2022 16:17:09 - INFO - __main__ -   Batch number = 302
01/14/2022 16:17:09 - INFO - __main__ -   Batch number = 303
01/14/2022 16:17:10 - INFO - __main__ -   Batch number = 304
01/14/2022 16:17:10 - INFO - __main__ -   Batch number = 305
01/14/2022 16:17:10 - INFO - __main__ -   Batch number = 306
01/14/2022 16:17:11 - INFO - __main__ -   Batch number = 307
01/14/2022 16:17:11 - INFO - __main__ -   Batch number = 308
01/14/2022 16:17:11 - INFO - __main__ -   Batch number = 309
01/14/2022 16:17:11 - INFO - __main__ -   Batch number = 310
01/14/2022 16:17:12 - INFO - __main__ -   Batch number = 311
01/14/2022 16:17:12 - INFO - __main__ -   Batch number = 312
01/14/2022 16:17:12 - INFO - __main__ -   Batch number = 313
01/14/2022 16:17:13 - INFO - __main__ -   Batch number = 314
01/14/2022 16:17:13 - INFO - __main__ -   Batch number = 315
01/14/2022 16:17:13 - INFO - __main__ -   Batch number = 316
01/14/2022 16:17:14 - INFO - __main__ -   Batch number = 317
01/14/2022 16:17:14 - INFO - __main__ -   Batch number = 318
01/14/2022 16:17:14 - INFO - __main__ -   Batch number = 319
01/14/2022 16:17:14 - INFO - __main__ -   Batch number = 320
01/14/2022 16:17:15 - INFO - __main__ -   Batch number = 321
01/14/2022 16:17:17 - INFO - __main__ -   ***** Evaluation result  in zh *****
01/14/2022 16:17:17 - INFO - __main__ -     f1 = 0.33907065694939786
01/14/2022 16:17:17 - INFO - __main__ -     loss = 3.4539700096641375
01/14/2022 16:17:17 - INFO - __main__ -     precision = 0.25079328669189893
01/14/2022 16:17:17 - INFO - __main__ -     recall = 0.5232511765175082
01/14/2022 16:17:20 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='zh', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:17:20 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/14/2022 16:17:20 - INFO - __main__ -   Seed = 3
01/14/2022 16:17:20 - INFO - root -   save model
01/14/2022 16:17:20 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='zh', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:17:20 - INFO - __main__ -   Loading pretrained model and tokenizer
01/14/2022 16:17:22 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/14/2022 16:17:28 - INFO - __main__ -   Using lang2id = None
01/14/2022 16:17:28 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/14/2022 16:17:28 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/
01/14/2022 16:17:28 - INFO - root -   Trying to decide if add adapter
01/14/2022 16:17:28 - INFO - root -   loading task adapter
01/14/2022 16:17:28 - INFO - root -   loading lang adpater en/wiki@ukp,pt/wiki@ukp,id/wiki@ukp,tr/wiki@ukp,vi/wiki@ukp,fa/wiki@ukp,eu/wiki@ukp,zh_yue/wiki@ukp,cs/wiki@ukp,zh/wiki@ukp
01/14/2022 16:17:28 - INFO - __main__ -   Adapter Languages : ['en', 'pt', 'id', 'tr', 'vi', 'fa', 'eu', 'zh_yue', 'cs', 'zh'], Length : 10
01/14/2022 16:17:28 - INFO - __main__ -   Adapter Names ['en/wiki@ukp', 'pt/wiki@ukp', 'id/wiki@ukp', 'tr/wiki@ukp', 'vi/wiki@ukp', 'fa/wiki@ukp', 'eu/wiki@ukp', 'zh_yue/wiki@ukp', 'cs/wiki@ukp', 'zh/wiki@ukp'], Length : 10
01/14/2022 16:17:28 - INFO - __main__ -   Language = en
01/14/2022 16:17:28 - INFO - __main__ -   Adapter Name = en/wiki@ukp
01/14/2022 16:17:29 - INFO - __main__ -   Language = pt
01/14/2022 16:17:29 - INFO - __main__ -   Adapter Name = pt/wiki@ukp
01/14/2022 16:17:31 - INFO - __main__ -   Language = id
01/14/2022 16:17:31 - INFO - __main__ -   Adapter Name = id/wiki@ukp
01/14/2022 16:17:33 - INFO - __main__ -   Language = tr
01/14/2022 16:17:33 - INFO - __main__ -   Adapter Name = tr/wiki@ukp
01/14/2022 16:17:35 - INFO - __main__ -   Language = vi
01/14/2022 16:17:35 - INFO - __main__ -   Adapter Name = vi/wiki@ukp
01/14/2022 16:17:38 - INFO - __main__ -   Language = fa
01/14/2022 16:17:38 - INFO - __main__ -   Adapter Name = fa/wiki@ukp
01/14/2022 16:17:40 - INFO - __main__ -   Language = eu
01/14/2022 16:17:40 - INFO - __main__ -   Adapter Name = eu/wiki@ukp
01/14/2022 16:17:42 - INFO - __main__ -   Language = zh_yue
01/14/2022 16:17:42 - INFO - __main__ -   Adapter Name = zh_yue/wiki@ukp
01/14/2022 16:17:44 - INFO - __main__ -   Language = cs
01/14/2022 16:17:44 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
01/14/2022 16:17:46 - INFO - __main__ -   Language = zh
01/14/2022 16:17:46 - INFO - __main__ -   Adapter Name = zh/wiki@ukp
01/14/2022 16:17:50 - INFO - __main__ -   Args Adapter Weight = equal
01/14/2022 16:17:50 - INFO - __main__ -   Adapter Languages = ['en', 'pt', 'id', 'tr', 'vi', 'fa', 'eu', 'zh_yue', 'cs', 'zh']
01/14/2022 16:17:50 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/14/2022 16:17:50 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/14/2022 16:17:50 - INFO - __main__ -   Length of Adapter Weights = 10
01/14/2022 16:17:50 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/cached_test_zh_bert-base-multilingual-cased_128
01/14/2022 16:17:51 - INFO - __main__ -   ***** Running evaluation  in zh *****
01/14/2022 16:17:51 - INFO - __main__ -     Num examples = 10257
01/14/2022 16:17:51 - INFO - __main__ -     Batch size = 32
01/14/2022 16:17:51 - INFO - __main__ -   Batch number = 1
01/14/2022 16:17:52 - INFO - __main__ -   Batch number = 2
01/14/2022 16:17:52 - INFO - __main__ -   Batch number = 3
01/14/2022 16:17:52 - INFO - __main__ -   Batch number = 4
01/14/2022 16:17:52 - INFO - __main__ -   Batch number = 5
01/14/2022 16:17:53 - INFO - __main__ -   Batch number = 6
01/14/2022 16:17:53 - INFO - __main__ -   Batch number = 7
01/14/2022 16:17:53 - INFO - __main__ -   Batch number = 8
01/14/2022 16:17:53 - INFO - __main__ -   Batch number = 9
01/14/2022 16:17:54 - INFO - __main__ -   Batch number = 10
01/14/2022 16:17:54 - INFO - __main__ -   Batch number = 11
01/14/2022 16:17:54 - INFO - __main__ -   Batch number = 12
01/14/2022 16:17:55 - INFO - __main__ -   Batch number = 13
01/14/2022 16:17:55 - INFO - __main__ -   Batch number = 14
01/14/2022 16:17:55 - INFO - __main__ -   Batch number = 15
01/14/2022 16:17:56 - INFO - __main__ -   Batch number = 16
01/14/2022 16:17:56 - INFO - __main__ -   Batch number = 17
01/14/2022 16:17:56 - INFO - __main__ -   Batch number = 18
01/14/2022 16:17:57 - INFO - __main__ -   Batch number = 19
01/14/2022 16:17:57 - INFO - __main__ -   Batch number = 20
01/14/2022 16:17:57 - INFO - __main__ -   Batch number = 21
01/14/2022 16:17:57 - INFO - __main__ -   Batch number = 22
01/14/2022 16:17:58 - INFO - __main__ -   Batch number = 23
01/14/2022 16:17:58 - INFO - __main__ -   Batch number = 24
01/14/2022 16:17:58 - INFO - __main__ -   Batch number = 25
01/14/2022 16:17:59 - INFO - __main__ -   Batch number = 26
01/14/2022 16:17:59 - INFO - __main__ -   Batch number = 27
01/14/2022 16:17:59 - INFO - __main__ -   Batch number = 28
01/14/2022 16:17:59 - INFO - __main__ -   Batch number = 29
01/14/2022 16:18:00 - INFO - __main__ -   Batch number = 30
01/14/2022 16:18:00 - INFO - __main__ -   Batch number = 31
01/14/2022 16:18:00 - INFO - __main__ -   Batch number = 32
01/14/2022 16:18:01 - INFO - __main__ -   Batch number = 33
01/14/2022 16:18:01 - INFO - __main__ -   Batch number = 34
01/14/2022 16:18:01 - INFO - __main__ -   Batch number = 35
01/14/2022 16:18:01 - INFO - __main__ -   Batch number = 36
01/14/2022 16:18:02 - INFO - __main__ -   Batch number = 37
01/14/2022 16:18:02 - INFO - __main__ -   Batch number = 38
01/14/2022 16:18:02 - INFO - __main__ -   Batch number = 39
01/14/2022 16:18:02 - INFO - __main__ -   Batch number = 40
01/14/2022 16:18:03 - INFO - __main__ -   Batch number = 41
01/14/2022 16:18:03 - INFO - __main__ -   Batch number = 42
01/14/2022 16:18:03 - INFO - __main__ -   Batch number = 43
01/14/2022 16:18:04 - INFO - __main__ -   Batch number = 44
01/14/2022 16:18:04 - INFO - __main__ -   Batch number = 45
01/14/2022 16:18:04 - INFO - __main__ -   Batch number = 46
01/14/2022 16:18:04 - INFO - __main__ -   Batch number = 47
01/14/2022 16:18:05 - INFO - __main__ -   Batch number = 48
01/14/2022 16:18:05 - INFO - __main__ -   Batch number = 49
01/14/2022 16:18:05 - INFO - __main__ -   Batch number = 50
01/14/2022 16:18:06 - INFO - __main__ -   Batch number = 51
01/14/2022 16:18:06 - INFO - __main__ -   Batch number = 52
01/14/2022 16:18:06 - INFO - __main__ -   Batch number = 53
01/14/2022 16:18:06 - INFO - __main__ -   Batch number = 54
01/14/2022 16:18:07 - INFO - __main__ -   Batch number = 55
01/14/2022 16:18:07 - INFO - __main__ -   Batch number = 56
01/14/2022 16:18:07 - INFO - __main__ -   Batch number = 57
01/14/2022 16:18:08 - INFO - __main__ -   Batch number = 58
01/14/2022 16:18:08 - INFO - __main__ -   Batch number = 59
01/14/2022 16:18:08 - INFO - __main__ -   Batch number = 60
01/14/2022 16:18:08 - INFO - __main__ -   Batch number = 61
01/14/2022 16:18:09 - INFO - __main__ -   Batch number = 62
01/14/2022 16:18:09 - INFO - __main__ -   Batch number = 63
01/14/2022 16:18:09 - INFO - __main__ -   Batch number = 64
01/14/2022 16:18:10 - INFO - __main__ -   Batch number = 65
01/14/2022 16:18:10 - INFO - __main__ -   Batch number = 66
01/14/2022 16:18:10 - INFO - __main__ -   Batch number = 67
01/14/2022 16:18:11 - INFO - __main__ -   Batch number = 68
01/14/2022 16:18:11 - INFO - __main__ -   Batch number = 69
01/14/2022 16:18:11 - INFO - __main__ -   Batch number = 70
01/14/2022 16:18:11 - INFO - __main__ -   Batch number = 71
01/14/2022 16:18:12 - INFO - __main__ -   Batch number = 72
01/14/2022 16:18:12 - INFO - __main__ -   Batch number = 73
01/14/2022 16:18:12 - INFO - __main__ -   Batch number = 74
01/14/2022 16:18:13 - INFO - __main__ -   Batch number = 75
01/14/2022 16:18:13 - INFO - __main__ -   Batch number = 76
01/14/2022 16:18:13 - INFO - __main__ -   Batch number = 77
01/14/2022 16:18:13 - INFO - __main__ -   Batch number = 78
01/14/2022 16:18:14 - INFO - __main__ -   Batch number = 79
01/14/2022 16:18:14 - INFO - __main__ -   Batch number = 80
01/14/2022 16:18:14 - INFO - __main__ -   Batch number = 81
01/14/2022 16:18:15 - INFO - __main__ -   Batch number = 82
01/14/2022 16:18:15 - INFO - __main__ -   Batch number = 83
01/14/2022 16:18:15 - INFO - __main__ -   Batch number = 84
01/14/2022 16:18:16 - INFO - __main__ -   Batch number = 85
01/14/2022 16:18:16 - INFO - __main__ -   Batch number = 86
01/14/2022 16:18:16 - INFO - __main__ -   Batch number = 87
01/14/2022 16:18:17 - INFO - __main__ -   Batch number = 88
01/14/2022 16:18:17 - INFO - __main__ -   Batch number = 89
01/14/2022 16:18:17 - INFO - __main__ -   Batch number = 90
01/14/2022 16:18:17 - INFO - __main__ -   Batch number = 91
01/14/2022 16:18:18 - INFO - __main__ -   Batch number = 92
01/14/2022 16:18:18 - INFO - __main__ -   Batch number = 93
01/14/2022 16:18:18 - INFO - __main__ -   Batch number = 94
01/14/2022 16:18:19 - INFO - __main__ -   Batch number = 95
01/14/2022 16:18:19 - INFO - __main__ -   Batch number = 96
01/14/2022 16:18:19 - INFO - __main__ -   Batch number = 97
01/14/2022 16:18:19 - INFO - __main__ -   Batch number = 98
01/14/2022 16:18:20 - INFO - __main__ -   Batch number = 99
01/14/2022 16:18:20 - INFO - __main__ -   Batch number = 100
01/14/2022 16:18:20 - INFO - __main__ -   Batch number = 101
01/14/2022 16:18:21 - INFO - __main__ -   Batch number = 102
01/14/2022 16:18:21 - INFO - __main__ -   Batch number = 103
01/14/2022 16:18:21 - INFO - __main__ -   Batch number = 104
01/14/2022 16:18:22 - INFO - __main__ -   Batch number = 105
01/14/2022 16:18:22 - INFO - __main__ -   Batch number = 106
01/14/2022 16:18:22 - INFO - __main__ -   Batch number = 107
01/14/2022 16:18:22 - INFO - __main__ -   Batch number = 108
01/14/2022 16:18:23 - INFO - __main__ -   Batch number = 109
01/14/2022 16:18:23 - INFO - __main__ -   Batch number = 110
01/14/2022 16:18:23 - INFO - __main__ -   Batch number = 111
01/14/2022 16:18:24 - INFO - __main__ -   Batch number = 112
01/14/2022 16:18:24 - INFO - __main__ -   Batch number = 113
01/14/2022 16:18:24 - INFO - __main__ -   Batch number = 114
01/14/2022 16:18:25 - INFO - __main__ -   Batch number = 115
01/14/2022 16:18:25 - INFO - __main__ -   Batch number = 116
01/14/2022 16:18:25 - INFO - __main__ -   Batch number = 117
01/14/2022 16:18:26 - INFO - __main__ -   Batch number = 118
01/14/2022 16:18:26 - INFO - __main__ -   Batch number = 119
01/14/2022 16:18:26 - INFO - __main__ -   Batch number = 120
01/14/2022 16:18:27 - INFO - __main__ -   Batch number = 121
01/14/2022 16:18:27 - INFO - __main__ -   Batch number = 122
01/14/2022 16:18:27 - INFO - __main__ -   Batch number = 123
01/14/2022 16:18:27 - INFO - __main__ -   Batch number = 124
01/14/2022 16:18:28 - INFO - __main__ -   Batch number = 125
01/14/2022 16:18:28 - INFO - __main__ -   Batch number = 126
01/14/2022 16:18:28 - INFO - __main__ -   Batch number = 127
01/14/2022 16:18:29 - INFO - __main__ -   Batch number = 128
01/14/2022 16:18:29 - INFO - __main__ -   Batch number = 129
01/14/2022 16:18:29 - INFO - __main__ -   Batch number = 130
01/14/2022 16:18:30 - INFO - __main__ -   Batch number = 131
01/14/2022 16:18:30 - INFO - __main__ -   Batch number = 132
01/14/2022 16:18:30 - INFO - __main__ -   Batch number = 133
01/14/2022 16:18:31 - INFO - __main__ -   Batch number = 134
01/14/2022 16:18:31 - INFO - __main__ -   Batch number = 135
01/14/2022 16:18:31 - INFO - __main__ -   Batch number = 136
01/14/2022 16:18:31 - INFO - __main__ -   Batch number = 137
01/14/2022 16:18:32 - INFO - __main__ -   Batch number = 138
01/14/2022 16:18:32 - INFO - __main__ -   Batch number = 139
01/14/2022 16:18:32 - INFO - __main__ -   Batch number = 140
01/14/2022 16:18:33 - INFO - __main__ -   Batch number = 141
01/14/2022 16:18:33 - INFO - __main__ -   Batch number = 142
01/14/2022 16:18:33 - INFO - __main__ -   Batch number = 143
01/14/2022 16:18:34 - INFO - __main__ -   Batch number = 144
01/14/2022 16:18:34 - INFO - __main__ -   Batch number = 145
01/14/2022 16:18:34 - INFO - __main__ -   Batch number = 146
01/14/2022 16:18:35 - INFO - __main__ -   Batch number = 147
01/14/2022 16:18:35 - INFO - __main__ -   Batch number = 148
01/14/2022 16:18:35 - INFO - __main__ -   Batch number = 149
01/14/2022 16:18:35 - INFO - __main__ -   Batch number = 150
01/14/2022 16:18:36 - INFO - __main__ -   Batch number = 151
01/14/2022 16:18:36 - INFO - __main__ -   Batch number = 152
01/14/2022 16:18:36 - INFO - __main__ -   Batch number = 153
01/14/2022 16:18:37 - INFO - __main__ -   Batch number = 154
01/14/2022 16:18:37 - INFO - __main__ -   Batch number = 155
01/14/2022 16:18:37 - INFO - __main__ -   Batch number = 156
01/14/2022 16:18:38 - INFO - __main__ -   Batch number = 157
01/14/2022 16:18:38 - INFO - __main__ -   Batch number = 158
01/14/2022 16:18:38 - INFO - __main__ -   Batch number = 159
01/14/2022 16:18:38 - INFO - __main__ -   Batch number = 160
01/14/2022 16:18:39 - INFO - __main__ -   Batch number = 161
01/14/2022 16:18:39 - INFO - __main__ -   Batch number = 162
01/14/2022 16:18:39 - INFO - __main__ -   Batch number = 163
01/14/2022 16:18:40 - INFO - __main__ -   Batch number = 164
01/14/2022 16:18:40 - INFO - __main__ -   Batch number = 165
01/14/2022 16:18:40 - INFO - __main__ -   Batch number = 166
01/14/2022 16:18:40 - INFO - __main__ -   Batch number = 167
01/14/2022 16:18:41 - INFO - __main__ -   Batch number = 168
01/14/2022 16:18:41 - INFO - __main__ -   Batch number = 169
01/14/2022 16:18:41 - INFO - __main__ -   Batch number = 170
01/14/2022 16:18:42 - INFO - __main__ -   Batch number = 171
01/14/2022 16:18:42 - INFO - __main__ -   Batch number = 172
01/14/2022 16:18:42 - INFO - __main__ -   Batch number = 173
01/14/2022 16:18:42 - INFO - __main__ -   Batch number = 174
01/14/2022 16:18:43 - INFO - __main__ -   Batch number = 175
01/14/2022 16:18:43 - INFO - __main__ -   Batch number = 176
01/14/2022 16:18:43 - INFO - __main__ -   Batch number = 177
01/14/2022 16:18:44 - INFO - __main__ -   Batch number = 178
01/14/2022 16:18:44 - INFO - __main__ -   Batch number = 179
01/14/2022 16:18:44 - INFO - __main__ -   Batch number = 180
01/14/2022 16:18:45 - INFO - __main__ -   Batch number = 181
01/14/2022 16:18:45 - INFO - __main__ -   Batch number = 182
01/14/2022 16:18:45 - INFO - __main__ -   Batch number = 183
01/14/2022 16:18:45 - INFO - __main__ -   Batch number = 184
01/14/2022 16:18:46 - INFO - __main__ -   Batch number = 185
01/14/2022 16:18:46 - INFO - __main__ -   Batch number = 186
01/14/2022 16:18:46 - INFO - __main__ -   Batch number = 187
01/14/2022 16:18:47 - INFO - __main__ -   Batch number = 188
01/14/2022 16:18:47 - INFO - __main__ -   Batch number = 189
01/14/2022 16:18:47 - INFO - __main__ -   Batch number = 190
01/14/2022 16:18:48 - INFO - __main__ -   Batch number = 191
01/14/2022 16:18:48 - INFO - __main__ -   Batch number = 192
01/14/2022 16:18:48 - INFO - __main__ -   Batch number = 193
01/14/2022 16:18:49 - INFO - __main__ -   Batch number = 194
01/14/2022 16:18:49 - INFO - __main__ -   Batch number = 195
01/14/2022 16:18:49 - INFO - __main__ -   Batch number = 196
01/14/2022 16:18:49 - INFO - __main__ -   Batch number = 197
01/14/2022 16:18:50 - INFO - __main__ -   Batch number = 198
01/14/2022 16:18:50 - INFO - __main__ -   Batch number = 199
01/14/2022 16:18:50 - INFO - __main__ -   Batch number = 200
01/14/2022 16:18:51 - INFO - __main__ -   Batch number = 201
01/14/2022 16:18:51 - INFO - __main__ -   Batch number = 202
01/14/2022 16:18:51 - INFO - __main__ -   Batch number = 203
01/14/2022 16:18:52 - INFO - __main__ -   Batch number = 204
01/14/2022 16:18:52 - INFO - __main__ -   Batch number = 205
01/14/2022 16:18:52 - INFO - __main__ -   Batch number = 206
01/14/2022 16:18:52 - INFO - __main__ -   Batch number = 207
01/14/2022 16:18:53 - INFO - __main__ -   Batch number = 208
01/14/2022 16:18:53 - INFO - __main__ -   Batch number = 209
01/14/2022 16:18:53 - INFO - __main__ -   Batch number = 210
01/14/2022 16:18:54 - INFO - __main__ -   Batch number = 211
01/14/2022 16:18:54 - INFO - __main__ -   Batch number = 212
01/14/2022 16:18:54 - INFO - __main__ -   Batch number = 213
01/14/2022 16:18:55 - INFO - __main__ -   Batch number = 214
01/14/2022 16:18:55 - INFO - __main__ -   Batch number = 215
01/14/2022 16:18:55 - INFO - __main__ -   Batch number = 216
01/14/2022 16:18:56 - INFO - __main__ -   Batch number = 217
01/14/2022 16:18:56 - INFO - __main__ -   Batch number = 218
01/14/2022 16:18:56 - INFO - __main__ -   Batch number = 219
01/14/2022 16:18:57 - INFO - __main__ -   Batch number = 220
01/14/2022 16:18:57 - INFO - __main__ -   Batch number = 221
01/14/2022 16:18:57 - INFO - __main__ -   Batch number = 222
01/14/2022 16:18:58 - INFO - __main__ -   Batch number = 223
01/14/2022 16:18:58 - INFO - __main__ -   Batch number = 224
01/14/2022 16:18:58 - INFO - __main__ -   Batch number = 225
01/14/2022 16:18:58 - INFO - __main__ -   Batch number = 226
01/14/2022 16:18:59 - INFO - __main__ -   Batch number = 227
01/14/2022 16:18:59 - INFO - __main__ -   Batch number = 228
01/14/2022 16:18:59 - INFO - __main__ -   Batch number = 229
01/14/2022 16:19:00 - INFO - __main__ -   Batch number = 230
01/14/2022 16:19:00 - INFO - __main__ -   Batch number = 231
01/14/2022 16:19:00 - INFO - __main__ -   Batch number = 232
01/14/2022 16:19:01 - INFO - __main__ -   Batch number = 233
01/14/2022 16:19:01 - INFO - __main__ -   Batch number = 234
01/14/2022 16:19:01 - INFO - __main__ -   Batch number = 235
01/14/2022 16:19:02 - INFO - __main__ -   Batch number = 236
01/14/2022 16:19:02 - INFO - __main__ -   Batch number = 237
01/14/2022 16:19:02 - INFO - __main__ -   Batch number = 238
01/14/2022 16:19:02 - INFO - __main__ -   Batch number = 239
01/14/2022 16:19:03 - INFO - __main__ -   Batch number = 240
01/14/2022 16:19:03 - INFO - __main__ -   Batch number = 241
01/14/2022 16:19:03 - INFO - __main__ -   Batch number = 242
01/14/2022 16:19:04 - INFO - __main__ -   Batch number = 243
01/14/2022 16:19:04 - INFO - __main__ -   Batch number = 244
01/14/2022 16:19:04 - INFO - __main__ -   Batch number = 245
01/14/2022 16:19:05 - INFO - __main__ -   Batch number = 246
01/14/2022 16:19:05 - INFO - __main__ -   Batch number = 247
01/14/2022 16:19:05 - INFO - __main__ -   Batch number = 248
01/14/2022 16:19:06 - INFO - __main__ -   Batch number = 249
01/14/2022 16:19:06 - INFO - __main__ -   Batch number = 250
01/14/2022 16:19:06 - INFO - __main__ -   Batch number = 251
01/14/2022 16:19:06 - INFO - __main__ -   Batch number = 252
01/14/2022 16:19:07 - INFO - __main__ -   Batch number = 253
01/14/2022 16:19:07 - INFO - __main__ -   Batch number = 254
01/14/2022 16:19:07 - INFO - __main__ -   Batch number = 255
01/14/2022 16:19:08 - INFO - __main__ -   Batch number = 256
01/14/2022 16:19:08 - INFO - __main__ -   Batch number = 257
01/14/2022 16:19:08 - INFO - __main__ -   Batch number = 258
01/14/2022 16:19:08 - INFO - __main__ -   Batch number = 259
01/14/2022 16:19:09 - INFO - __main__ -   Batch number = 260
01/14/2022 16:19:09 - INFO - __main__ -   Batch number = 261
01/14/2022 16:19:09 - INFO - __main__ -   Batch number = 262
01/14/2022 16:19:10 - INFO - __main__ -   Batch number = 263
01/14/2022 16:19:10 - INFO - __main__ -   Batch number = 264
01/14/2022 16:19:10 - INFO - __main__ -   Batch number = 265
01/14/2022 16:19:11 - INFO - __main__ -   Batch number = 266
01/14/2022 16:19:11 - INFO - __main__ -   Batch number = 267
01/14/2022 16:19:11 - INFO - __main__ -   Batch number = 268
01/14/2022 16:19:12 - INFO - __main__ -   Batch number = 269
01/14/2022 16:19:12 - INFO - __main__ -   Batch number = 270
01/14/2022 16:19:12 - INFO - __main__ -   Batch number = 271
01/14/2022 16:19:12 - INFO - __main__ -   Batch number = 272
01/14/2022 16:19:13 - INFO - __main__ -   Batch number = 273
01/14/2022 16:19:13 - INFO - __main__ -   Batch number = 274
01/14/2022 16:19:14 - INFO - __main__ -   Batch number = 275
01/14/2022 16:19:14 - INFO - __main__ -   Batch number = 276
01/14/2022 16:19:14 - INFO - __main__ -   Batch number = 277
01/14/2022 16:19:14 - INFO - __main__ -   Batch number = 278
01/14/2022 16:19:15 - INFO - __main__ -   Batch number = 279
01/14/2022 16:19:15 - INFO - __main__ -   Batch number = 280
01/14/2022 16:19:15 - INFO - __main__ -   Batch number = 281
01/14/2022 16:19:16 - INFO - __main__ -   Batch number = 282
01/14/2022 16:19:16 - INFO - __main__ -   Batch number = 283
01/14/2022 16:19:16 - INFO - __main__ -   Batch number = 284
01/14/2022 16:19:17 - INFO - __main__ -   Batch number = 285
01/14/2022 16:19:17 - INFO - __main__ -   Batch number = 286
01/14/2022 16:19:17 - INFO - __main__ -   Batch number = 287
01/14/2022 16:19:18 - INFO - __main__ -   Batch number = 288
01/14/2022 16:19:18 - INFO - __main__ -   Batch number = 289
01/14/2022 16:19:18 - INFO - __main__ -   Batch number = 290
01/14/2022 16:19:18 - INFO - __main__ -   Batch number = 291
01/14/2022 16:19:19 - INFO - __main__ -   Batch number = 292
01/14/2022 16:19:19 - INFO - __main__ -   Batch number = 293
01/14/2022 16:19:19 - INFO - __main__ -   Batch number = 294
01/14/2022 16:19:20 - INFO - __main__ -   Batch number = 295
01/14/2022 16:19:20 - INFO - __main__ -   Batch number = 296
01/14/2022 16:19:20 - INFO - __main__ -   Batch number = 297
01/14/2022 16:19:21 - INFO - __main__ -   Batch number = 298
01/14/2022 16:19:21 - INFO - __main__ -   Batch number = 299
01/14/2022 16:19:21 - INFO - __main__ -   Batch number = 300
01/14/2022 16:19:21 - INFO - __main__ -   Batch number = 301
01/14/2022 16:19:22 - INFO - __main__ -   Batch number = 302
01/14/2022 16:19:22 - INFO - __main__ -   Batch number = 303
01/14/2022 16:19:22 - INFO - __main__ -   Batch number = 304
01/14/2022 16:19:23 - INFO - __main__ -   Batch number = 305
01/14/2022 16:19:23 - INFO - __main__ -   Batch number = 306
01/14/2022 16:19:23 - INFO - __main__ -   Batch number = 307
01/14/2022 16:19:24 - INFO - __main__ -   Batch number = 308
01/14/2022 16:19:24 - INFO - __main__ -   Batch number = 309
01/14/2022 16:19:24 - INFO - __main__ -   Batch number = 310
01/14/2022 16:19:25 - INFO - __main__ -   Batch number = 311
01/14/2022 16:19:25 - INFO - __main__ -   Batch number = 312
01/14/2022 16:19:25 - INFO - __main__ -   Batch number = 313
01/14/2022 16:19:25 - INFO - __main__ -   Batch number = 314
01/14/2022 16:19:26 - INFO - __main__ -   Batch number = 315
01/14/2022 16:19:26 - INFO - __main__ -   Batch number = 316
01/14/2022 16:19:26 - INFO - __main__ -   Batch number = 317
01/14/2022 16:19:27 - INFO - __main__ -   Batch number = 318
01/14/2022 16:19:27 - INFO - __main__ -   Batch number = 319
01/14/2022 16:19:27 - INFO - __main__ -   Batch number = 320
01/14/2022 16:19:28 - INFO - __main__ -   Batch number = 321
01/14/2022 16:19:30 - INFO - __main__ -   ***** Evaluation result  in zh *****
01/14/2022 16:19:30 - INFO - __main__ -     f1 = 0.36124249009331455
01/14/2022 16:19:30 - INFO - __main__ -     loss = 3.883939821772115
01/14/2022 16:19:30 - INFO - __main__ -     precision = 0.265821356008729
01/14/2022 16:19:30 - INFO - __main__ -     recall = 0.5635319454414932
01/14/2022 16:20:51 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ar', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:20:51 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/14/2022 16:20:51 - INFO - __main__ -   Seed = 1
01/14/2022 16:20:51 - INFO - root -   save model
01/14/2022 16:20:51 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ar', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:20:51 - INFO - __main__ -   Loading pretrained model and tokenizer
01/14/2022 16:20:53 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/14/2022 16:20:59 - INFO - __main__ -   Using lang2id = None
01/14/2022 16:20:59 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/14/2022 16:20:59 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/
01/14/2022 16:20:59 - INFO - root -   Trying to decide if add adapter
01/14/2022 16:20:59 - INFO - root -   loading task adapter
01/14/2022 16:20:59 - INFO - root -   loading lang adpater en/wiki@ukp,pt/wiki@ukp,id/wiki@ukp,cs/wiki@ukp,tr/wiki@ukp,eu/wiki@ukp,zh_yue/wiki@ukp,vi/wiki@ukp,fr/wiki@ukp,ar/wiki@ukp
01/14/2022 16:20:59 - INFO - __main__ -   Adapter Languages : ['en', 'pt', 'id', 'cs', 'tr', 'eu', 'zh_yue', 'vi', 'fr', 'ar'], Length : 10
01/14/2022 16:20:59 - INFO - __main__ -   Adapter Names ['en/wiki@ukp', 'pt/wiki@ukp', 'id/wiki@ukp', 'cs/wiki@ukp', 'tr/wiki@ukp', 'eu/wiki@ukp', 'zh_yue/wiki@ukp', 'vi/wiki@ukp', 'fr/wiki@ukp', 'ar/wiki@ukp'], Length : 10
01/14/2022 16:20:59 - INFO - __main__ -   Language = en
01/14/2022 16:20:59 - INFO - __main__ -   Adapter Name = en/wiki@ukp
01/14/2022 16:21:00 - INFO - __main__ -   Language = pt
01/14/2022 16:21:00 - INFO - __main__ -   Adapter Name = pt/wiki@ukp
01/14/2022 16:21:03 - INFO - __main__ -   Language = id
01/14/2022 16:21:03 - INFO - __main__ -   Adapter Name = id/wiki@ukp
01/14/2022 16:21:05 - INFO - __main__ -   Language = cs
01/14/2022 16:21:05 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
01/14/2022 16:21:08 - INFO - __main__ -   Language = tr
01/14/2022 16:21:08 - INFO - __main__ -   Adapter Name = tr/wiki@ukp
01/14/2022 16:21:10 - INFO - __main__ -   Language = eu
01/14/2022 16:21:10 - INFO - __main__ -   Adapter Name = eu/wiki@ukp
01/14/2022 16:21:13 - INFO - __main__ -   Language = zh_yue
01/14/2022 16:21:13 - INFO - __main__ -   Adapter Name = zh_yue/wiki@ukp
01/14/2022 16:21:15 - INFO - __main__ -   Language = vi
01/14/2022 16:21:15 - INFO - __main__ -   Adapter Name = vi/wiki@ukp
01/14/2022 16:21:18 - INFO - __main__ -   Language = fr
01/14/2022 16:21:18 - INFO - __main__ -   Adapter Name = fr/wiki@ukp
01/14/2022 16:21:19 - INFO - __main__ -   Language = ar
01/14/2022 16:21:19 - INFO - __main__ -   Adapter Name = ar/wiki@ukp
01/14/2022 16:21:24 - INFO - __main__ -   Args Adapter Weight = equal
01/14/2022 16:21:24 - INFO - __main__ -   Adapter Languages = ['en', 'pt', 'id', 'cs', 'tr', 'eu', 'zh_yue', 'vi', 'fr', 'ar']
01/14/2022 16:21:24 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/14/2022 16:21:24 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/14/2022 16:21:24 - INFO - __main__ -   Length of Adapter Weights = 10
01/14/2022 16:21:24 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/cached_test_ar_bert-base-multilingual-cased_128
01/14/2022 16:21:25 - INFO - __main__ -   ***** Running evaluation  in ar *****
01/14/2022 16:21:25 - INFO - __main__ -     Num examples = 10000
01/14/2022 16:21:25 - INFO - __main__ -     Batch size = 32
01/14/2022 16:21:25 - INFO - __main__ -   Batch number = 1
01/14/2022 16:21:25 - INFO - __main__ -   Batch number = 2
01/14/2022 16:21:25 - INFO - __main__ -   Batch number = 3
01/14/2022 16:21:26 - INFO - __main__ -   Batch number = 4
01/14/2022 16:21:26 - INFO - __main__ -   Batch number = 5
01/14/2022 16:21:26 - INFO - __main__ -   Batch number = 6
01/14/2022 16:21:26 - INFO - __main__ -   Batch number = 7
01/14/2022 16:21:27 - INFO - __main__ -   Batch number = 8
01/14/2022 16:21:27 - INFO - __main__ -   Batch number = 9
01/14/2022 16:21:27 - INFO - __main__ -   Batch number = 10
01/14/2022 16:21:27 - INFO - __main__ -   Batch number = 11
01/14/2022 16:21:28 - INFO - __main__ -   Batch number = 12
01/14/2022 16:21:28 - INFO - __main__ -   Batch number = 13
01/14/2022 16:21:28 - INFO - __main__ -   Batch number = 14
01/14/2022 16:21:29 - INFO - __main__ -   Batch number = 15
01/14/2022 16:21:29 - INFO - __main__ -   Batch number = 16
01/14/2022 16:21:29 - INFO - __main__ -   Batch number = 17
01/14/2022 16:21:29 - INFO - __main__ -   Batch number = 18
01/14/2022 16:21:30 - INFO - __main__ -   Batch number = 19
01/14/2022 16:21:30 - INFO - __main__ -   Batch number = 20
01/14/2022 16:21:30 - INFO - __main__ -   Batch number = 21
01/14/2022 16:21:31 - INFO - __main__ -   Batch number = 22
01/14/2022 16:21:31 - INFO - __main__ -   Batch number = 23
01/14/2022 16:21:31 - INFO - __main__ -   Batch number = 24
01/14/2022 16:21:31 - INFO - __main__ -   Batch number = 25
01/14/2022 16:21:32 - INFO - __main__ -   Batch number = 26
01/14/2022 16:21:32 - INFO - __main__ -   Batch number = 27
01/14/2022 16:21:32 - INFO - __main__ -   Batch number = 28
01/14/2022 16:21:33 - INFO - __main__ -   Batch number = 29
01/14/2022 16:21:33 - INFO - __main__ -   Batch number = 30
01/14/2022 16:21:33 - INFO - __main__ -   Batch number = 31
01/14/2022 16:21:33 - INFO - __main__ -   Batch number = 32
01/14/2022 16:21:34 - INFO - __main__ -   Batch number = 33
01/14/2022 16:21:34 - INFO - __main__ -   Batch number = 34
01/14/2022 16:21:34 - INFO - __main__ -   Batch number = 35
01/14/2022 16:21:35 - INFO - __main__ -   Batch number = 36
01/14/2022 16:21:35 - INFO - __main__ -   Batch number = 37
01/14/2022 16:21:35 - INFO - __main__ -   Batch number = 38
01/14/2022 16:21:35 - INFO - __main__ -   Batch number = 39
01/14/2022 16:21:36 - INFO - __main__ -   Batch number = 40
01/14/2022 16:21:36 - INFO - __main__ -   Batch number = 41
01/14/2022 16:21:36 - INFO - __main__ -   Batch number = 42
01/14/2022 16:21:37 - INFO - __main__ -   Batch number = 43
01/14/2022 16:21:37 - INFO - __main__ -   Batch number = 44
01/14/2022 16:21:37 - INFO - __main__ -   Batch number = 45
01/14/2022 16:21:37 - INFO - __main__ -   Batch number = 46
01/14/2022 16:21:38 - INFO - __main__ -   Batch number = 47
01/14/2022 16:21:38 - INFO - __main__ -   Batch number = 48
01/14/2022 16:21:38 - INFO - __main__ -   Batch number = 49
01/14/2022 16:21:38 - INFO - __main__ -   Batch number = 50
01/14/2022 16:21:39 - INFO - __main__ -   Batch number = 51
01/14/2022 16:21:39 - INFO - __main__ -   Batch number = 52
01/14/2022 16:21:39 - INFO - __main__ -   Batch number = 53
01/14/2022 16:21:40 - INFO - __main__ -   Batch number = 54
01/14/2022 16:21:40 - INFO - __main__ -   Batch number = 55
01/14/2022 16:21:40 - INFO - __main__ -   Batch number = 56
01/14/2022 16:21:40 - INFO - __main__ -   Batch number = 57
01/14/2022 16:21:41 - INFO - __main__ -   Batch number = 58
01/14/2022 16:21:41 - INFO - __main__ -   Batch number = 59
01/14/2022 16:21:41 - INFO - __main__ -   Batch number = 60
01/14/2022 16:21:42 - INFO - __main__ -   Batch number = 61
01/14/2022 16:21:42 - INFO - __main__ -   Batch number = 62
01/14/2022 16:21:42 - INFO - __main__ -   Batch number = 63
01/14/2022 16:21:43 - INFO - __main__ -   Batch number = 64
01/14/2022 16:21:43 - INFO - __main__ -   Batch number = 65
01/14/2022 16:21:43 - INFO - __main__ -   Batch number = 66
01/14/2022 16:21:43 - INFO - __main__ -   Batch number = 67
01/14/2022 16:21:44 - INFO - __main__ -   Batch number = 68
01/14/2022 16:21:44 - INFO - __main__ -   Batch number = 69
01/14/2022 16:21:44 - INFO - __main__ -   Batch number = 70
01/14/2022 16:21:44 - INFO - __main__ -   Batch number = 71
01/14/2022 16:21:45 - INFO - __main__ -   Batch number = 72
01/14/2022 16:21:45 - INFO - __main__ -   Batch number = 73
01/14/2022 16:21:45 - INFO - __main__ -   Batch number = 74
01/14/2022 16:21:46 - INFO - __main__ -   Batch number = 75
01/14/2022 16:21:46 - INFO - __main__ -   Batch number = 76
01/14/2022 16:21:46 - INFO - __main__ -   Batch number = 77
01/14/2022 16:21:47 - INFO - __main__ -   Batch number = 78
01/14/2022 16:21:47 - INFO - __main__ -   Batch number = 79
01/14/2022 16:21:47 - INFO - __main__ -   Batch number = 80
01/14/2022 16:21:47 - INFO - __main__ -   Batch number = 81
01/14/2022 16:21:48 - INFO - __main__ -   Batch number = 82
01/14/2022 16:21:48 - INFO - __main__ -   Batch number = 83
01/14/2022 16:21:48 - INFO - __main__ -   Batch number = 84
01/14/2022 16:21:48 - INFO - __main__ -   Batch number = 85
01/14/2022 16:21:49 - INFO - __main__ -   Batch number = 86
01/14/2022 16:21:49 - INFO - __main__ -   Batch number = 87
01/14/2022 16:21:49 - INFO - __main__ -   Batch number = 88
01/14/2022 16:21:50 - INFO - __main__ -   Batch number = 89
01/14/2022 16:21:50 - INFO - __main__ -   Batch number = 90
01/14/2022 16:21:50 - INFO - __main__ -   Batch number = 91
01/14/2022 16:21:50 - INFO - __main__ -   Batch number = 92
01/14/2022 16:21:51 - INFO - __main__ -   Batch number = 93
01/14/2022 16:21:51 - INFO - __main__ -   Batch number = 94
01/14/2022 16:21:51 - INFO - __main__ -   Batch number = 95
01/14/2022 16:21:52 - INFO - __main__ -   Batch number = 96
01/14/2022 16:21:52 - INFO - __main__ -   Batch number = 97
01/14/2022 16:21:52 - INFO - __main__ -   Batch number = 98
01/14/2022 16:21:52 - INFO - __main__ -   Batch number = 99
01/14/2022 16:21:53 - INFO - __main__ -   Batch number = 100
01/14/2022 16:21:53 - INFO - __main__ -   Batch number = 101
01/14/2022 16:21:53 - INFO - __main__ -   Batch number = 102
01/14/2022 16:21:54 - INFO - __main__ -   Batch number = 103
01/14/2022 16:21:54 - INFO - __main__ -   Batch number = 104
01/14/2022 16:21:54 - INFO - __main__ -   Batch number = 105
01/14/2022 16:21:54 - INFO - __main__ -   Batch number = 106
01/14/2022 16:21:55 - INFO - __main__ -   Batch number = 107
01/14/2022 16:21:55 - INFO - __main__ -   Batch number = 108
01/14/2022 16:21:55 - INFO - __main__ -   Batch number = 109
01/14/2022 16:21:56 - INFO - __main__ -   Batch number = 110
01/14/2022 16:21:56 - INFO - __main__ -   Batch number = 111
01/14/2022 16:21:56 - INFO - __main__ -   Batch number = 112
01/14/2022 16:21:56 - INFO - __main__ -   Batch number = 113
01/14/2022 16:21:57 - INFO - __main__ -   Batch number = 114
01/14/2022 16:21:57 - INFO - __main__ -   Batch number = 115
01/14/2022 16:21:57 - INFO - __main__ -   Batch number = 116
01/14/2022 16:21:58 - INFO - __main__ -   Batch number = 117
01/14/2022 16:21:58 - INFO - __main__ -   Batch number = 118
01/14/2022 16:21:58 - INFO - __main__ -   Batch number = 119
01/14/2022 16:21:58 - INFO - __main__ -   Batch number = 120
01/14/2022 16:21:59 - INFO - __main__ -   Batch number = 121
01/14/2022 16:21:59 - INFO - __main__ -   Batch number = 122
01/14/2022 16:21:59 - INFO - __main__ -   Batch number = 123
01/14/2022 16:22:00 - INFO - __main__ -   Batch number = 124
01/14/2022 16:22:00 - INFO - __main__ -   Batch number = 125
01/14/2022 16:22:00 - INFO - __main__ -   Batch number = 126
01/14/2022 16:22:00 - INFO - __main__ -   Batch number = 127
01/14/2022 16:22:01 - INFO - __main__ -   Batch number = 128
01/14/2022 16:22:01 - INFO - __main__ -   Batch number = 129
01/14/2022 16:22:01 - INFO - __main__ -   Batch number = 130
01/14/2022 16:22:02 - INFO - __main__ -   Batch number = 131
01/14/2022 16:22:02 - INFO - __main__ -   Batch number = 132
01/14/2022 16:22:02 - INFO - __main__ -   Batch number = 133
01/14/2022 16:22:03 - INFO - __main__ -   Batch number = 134
01/14/2022 16:22:03 - INFO - __main__ -   Batch number = 135
01/14/2022 16:22:03 - INFO - __main__ -   Batch number = 136
01/14/2022 16:22:03 - INFO - __main__ -   Batch number = 137
01/14/2022 16:22:04 - INFO - __main__ -   Batch number = 138
01/14/2022 16:22:04 - INFO - __main__ -   Batch number = 139
01/14/2022 16:22:04 - INFO - __main__ -   Batch number = 140
01/14/2022 16:22:05 - INFO - __main__ -   Batch number = 141
01/14/2022 16:22:05 - INFO - __main__ -   Batch number = 142
01/14/2022 16:22:05 - INFO - __main__ -   Batch number = 143
01/14/2022 16:22:05 - INFO - __main__ -   Batch number = 144
01/14/2022 16:22:06 - INFO - __main__ -   Batch number = 145
01/14/2022 16:22:06 - INFO - __main__ -   Batch number = 146
01/14/2022 16:22:06 - INFO - __main__ -   Batch number = 147
01/14/2022 16:22:07 - INFO - __main__ -   Batch number = 148
01/14/2022 16:22:07 - INFO - __main__ -   Batch number = 149
01/14/2022 16:22:07 - INFO - __main__ -   Batch number = 150
01/14/2022 16:22:07 - INFO - __main__ -   Batch number = 151
01/14/2022 16:22:08 - INFO - __main__ -   Batch number = 152
01/14/2022 16:22:08 - INFO - __main__ -   Batch number = 153
01/14/2022 16:22:08 - INFO - __main__ -   Batch number = 154
01/14/2022 16:22:09 - INFO - __main__ -   Batch number = 155
01/14/2022 16:22:09 - INFO - __main__ -   Batch number = 156
01/14/2022 16:22:09 - INFO - __main__ -   Batch number = 157
01/14/2022 16:22:10 - INFO - __main__ -   Batch number = 158
01/14/2022 16:22:10 - INFO - __main__ -   Batch number = 159
01/14/2022 16:22:10 - INFO - __main__ -   Batch number = 160
01/14/2022 16:22:10 - INFO - __main__ -   Batch number = 161
01/14/2022 16:22:11 - INFO - __main__ -   Batch number = 162
01/14/2022 16:22:11 - INFO - __main__ -   Batch number = 163
01/14/2022 16:22:11 - INFO - __main__ -   Batch number = 164
01/14/2022 16:22:12 - INFO - __main__ -   Batch number = 165
01/14/2022 16:22:12 - INFO - __main__ -   Batch number = 166
01/14/2022 16:22:12 - INFO - __main__ -   Batch number = 167
01/14/2022 16:22:12 - INFO - __main__ -   Batch number = 168
01/14/2022 16:22:13 - INFO - __main__ -   Batch number = 169
01/14/2022 16:22:13 - INFO - __main__ -   Batch number = 170
01/14/2022 16:22:13 - INFO - __main__ -   Batch number = 171
01/14/2022 16:22:14 - INFO - __main__ -   Batch number = 172
01/14/2022 16:22:14 - INFO - __main__ -   Batch number = 173
01/14/2022 16:22:14 - INFO - __main__ -   Batch number = 174
01/14/2022 16:22:14 - INFO - __main__ -   Batch number = 175
01/14/2022 16:22:15 - INFO - __main__ -   Batch number = 176
01/14/2022 16:22:15 - INFO - __main__ -   Batch number = 177
01/14/2022 16:22:15 - INFO - __main__ -   Batch number = 178
01/14/2022 16:22:16 - INFO - __main__ -   Batch number = 179
01/14/2022 16:22:16 - INFO - __main__ -   Batch number = 180
01/14/2022 16:22:16 - INFO - __main__ -   Batch number = 181
01/14/2022 16:22:17 - INFO - __main__ -   Batch number = 182
01/14/2022 16:22:17 - INFO - __main__ -   Batch number = 183
01/14/2022 16:22:17 - INFO - __main__ -   Batch number = 184
01/14/2022 16:22:17 - INFO - __main__ -   Batch number = 185
01/14/2022 16:22:18 - INFO - __main__ -   Batch number = 186
01/14/2022 16:22:18 - INFO - __main__ -   Batch number = 187
01/14/2022 16:22:18 - INFO - __main__ -   Batch number = 188
01/14/2022 16:22:19 - INFO - __main__ -   Batch number = 189
01/14/2022 16:22:19 - INFO - __main__ -   Batch number = 190
01/14/2022 16:22:19 - INFO - __main__ -   Batch number = 191
01/14/2022 16:22:20 - INFO - __main__ -   Batch number = 192
01/14/2022 16:22:20 - INFO - __main__ -   Batch number = 193
01/14/2022 16:22:20 - INFO - __main__ -   Batch number = 194
01/14/2022 16:22:21 - INFO - __main__ -   Batch number = 195
01/14/2022 16:22:21 - INFO - __main__ -   Batch number = 196
01/14/2022 16:22:21 - INFO - __main__ -   Batch number = 197
01/14/2022 16:22:21 - INFO - __main__ -   Batch number = 198
01/14/2022 16:22:22 - INFO - __main__ -   Batch number = 199
01/14/2022 16:22:22 - INFO - __main__ -   Batch number = 200
01/14/2022 16:22:22 - INFO - __main__ -   Batch number = 201
01/14/2022 16:22:23 - INFO - __main__ -   Batch number = 202
01/14/2022 16:22:23 - INFO - __main__ -   Batch number = 203
01/14/2022 16:22:23 - INFO - __main__ -   Batch number = 204
01/14/2022 16:22:24 - INFO - __main__ -   Batch number = 205
01/14/2022 16:22:24 - INFO - __main__ -   Batch number = 206
01/14/2022 16:22:24 - INFO - __main__ -   Batch number = 207
01/14/2022 16:22:25 - INFO - __main__ -   Batch number = 208
01/14/2022 16:22:25 - INFO - __main__ -   Batch number = 209
01/14/2022 16:22:25 - INFO - __main__ -   Batch number = 210
01/14/2022 16:22:26 - INFO - __main__ -   Batch number = 211
01/14/2022 16:22:26 - INFO - __main__ -   Batch number = 212
01/14/2022 16:22:26 - INFO - __main__ -   Batch number = 213
01/14/2022 16:22:27 - INFO - __main__ -   Batch number = 214
01/14/2022 16:22:27 - INFO - __main__ -   Batch number = 215
01/14/2022 16:22:27 - INFO - __main__ -   Batch number = 216
01/14/2022 16:22:27 - INFO - __main__ -   Batch number = 217
01/14/2022 16:22:28 - INFO - __main__ -   Batch number = 218
01/14/2022 16:22:28 - INFO - __main__ -   Batch number = 219
01/14/2022 16:22:28 - INFO - __main__ -   Batch number = 220
01/14/2022 16:22:29 - INFO - __main__ -   Batch number = 221
01/14/2022 16:22:29 - INFO - __main__ -   Batch number = 222
01/14/2022 16:22:29 - INFO - __main__ -   Batch number = 223
01/14/2022 16:22:30 - INFO - __main__ -   Batch number = 224
01/14/2022 16:22:30 - INFO - __main__ -   Batch number = 225
01/14/2022 16:22:30 - INFO - __main__ -   Batch number = 226
01/14/2022 16:22:30 - INFO - __main__ -   Batch number = 227
01/14/2022 16:22:31 - INFO - __main__ -   Batch number = 228
01/14/2022 16:22:31 - INFO - __main__ -   Batch number = 229
01/14/2022 16:22:31 - INFO - __main__ -   Batch number = 230
01/14/2022 16:22:32 - INFO - __main__ -   Batch number = 231
01/14/2022 16:22:32 - INFO - __main__ -   Batch number = 232
01/14/2022 16:22:32 - INFO - __main__ -   Batch number = 233
01/14/2022 16:22:32 - INFO - __main__ -   Batch number = 234
01/14/2022 16:22:33 - INFO - __main__ -   Batch number = 235
01/14/2022 16:22:33 - INFO - __main__ -   Batch number = 236
01/14/2022 16:22:33 - INFO - __main__ -   Batch number = 237
01/14/2022 16:22:34 - INFO - __main__ -   Batch number = 238
01/14/2022 16:22:34 - INFO - __main__ -   Batch number = 239
01/14/2022 16:22:34 - INFO - __main__ -   Batch number = 240
01/14/2022 16:22:35 - INFO - __main__ -   Batch number = 241
01/14/2022 16:22:35 - INFO - __main__ -   Batch number = 242
01/14/2022 16:22:35 - INFO - __main__ -   Batch number = 243
01/14/2022 16:22:36 - INFO - __main__ -   Batch number = 244
01/14/2022 16:22:36 - INFO - __main__ -   Batch number = 245
01/14/2022 16:22:36 - INFO - __main__ -   Batch number = 246
01/14/2022 16:22:36 - INFO - __main__ -   Batch number = 247
01/14/2022 16:22:37 - INFO - __main__ -   Batch number = 248
01/14/2022 16:22:37 - INFO - __main__ -   Batch number = 249
01/14/2022 16:22:37 - INFO - __main__ -   Batch number = 250
01/14/2022 16:22:38 - INFO - __main__ -   Batch number = 251
01/14/2022 16:22:38 - INFO - __main__ -   Batch number = 252
01/14/2022 16:22:38 - INFO - __main__ -   Batch number = 253
01/14/2022 16:22:39 - INFO - __main__ -   Batch number = 254
01/14/2022 16:22:39 - INFO - __main__ -   Batch number = 255
01/14/2022 16:22:39 - INFO - __main__ -   Batch number = 256
01/14/2022 16:22:39 - INFO - __main__ -   Batch number = 257
01/14/2022 16:22:40 - INFO - __main__ -   Batch number = 258
01/14/2022 16:22:40 - INFO - __main__ -   Batch number = 259
01/14/2022 16:22:40 - INFO - __main__ -   Batch number = 260
01/14/2022 16:22:41 - INFO - __main__ -   Batch number = 261
01/14/2022 16:22:41 - INFO - __main__ -   Batch number = 262
01/14/2022 16:22:41 - INFO - __main__ -   Batch number = 263
01/14/2022 16:22:41 - INFO - __main__ -   Batch number = 264
01/14/2022 16:22:42 - INFO - __main__ -   Batch number = 265
01/14/2022 16:22:42 - INFO - __main__ -   Batch number = 266
01/14/2022 16:22:42 - INFO - __main__ -   Batch number = 267
01/14/2022 16:22:43 - INFO - __main__ -   Batch number = 268
01/14/2022 16:22:43 - INFO - __main__ -   Batch number = 269
01/14/2022 16:22:43 - INFO - __main__ -   Batch number = 270
01/14/2022 16:22:44 - INFO - __main__ -   Batch number = 271
01/14/2022 16:22:44 - INFO - __main__ -   Batch number = 272
01/14/2022 16:22:44 - INFO - __main__ -   Batch number = 273
01/14/2022 16:22:45 - INFO - __main__ -   Batch number = 274
01/14/2022 16:22:45 - INFO - __main__ -   Batch number = 275
01/14/2022 16:22:45 - INFO - __main__ -   Batch number = 276
01/14/2022 16:22:45 - INFO - __main__ -   Batch number = 277
01/14/2022 16:22:46 - INFO - __main__ -   Batch number = 278
01/14/2022 16:22:46 - INFO - __main__ -   Batch number = 279
01/14/2022 16:22:46 - INFO - __main__ -   Batch number = 280
01/14/2022 16:22:47 - INFO - __main__ -   Batch number = 281
01/14/2022 16:22:47 - INFO - __main__ -   Batch number = 282
01/14/2022 16:22:47 - INFO - __main__ -   Batch number = 283
01/14/2022 16:22:48 - INFO - __main__ -   Batch number = 284
01/14/2022 16:22:48 - INFO - __main__ -   Batch number = 285
01/14/2022 16:22:48 - INFO - __main__ -   Batch number = 286
01/14/2022 16:22:49 - INFO - __main__ -   Batch number = 287
01/14/2022 16:22:49 - INFO - __main__ -   Batch number = 288
01/14/2022 16:22:49 - INFO - __main__ -   Batch number = 289
01/14/2022 16:22:49 - INFO - __main__ -   Batch number = 290
01/14/2022 16:22:50 - INFO - __main__ -   Batch number = 291
01/14/2022 16:22:50 - INFO - __main__ -   Batch number = 292
01/14/2022 16:22:50 - INFO - __main__ -   Batch number = 293
01/14/2022 16:22:51 - INFO - __main__ -   Batch number = 294
01/14/2022 16:22:51 - INFO - __main__ -   Batch number = 295
01/14/2022 16:22:51 - INFO - __main__ -   Batch number = 296
01/14/2022 16:22:52 - INFO - __main__ -   Batch number = 297
01/14/2022 16:22:52 - INFO - __main__ -   Batch number = 298
01/14/2022 16:22:52 - INFO - __main__ -   Batch number = 299
01/14/2022 16:22:52 - INFO - __main__ -   Batch number = 300
01/14/2022 16:22:53 - INFO - __main__ -   Batch number = 301
01/14/2022 16:22:53 - INFO - __main__ -   Batch number = 302
01/14/2022 16:22:53 - INFO - __main__ -   Batch number = 303
01/14/2022 16:22:54 - INFO - __main__ -   Batch number = 304
01/14/2022 16:22:54 - INFO - __main__ -   Batch number = 305
01/14/2022 16:22:54 - INFO - __main__ -   Batch number = 306
01/14/2022 16:22:55 - INFO - __main__ -   Batch number = 307
01/14/2022 16:22:55 - INFO - __main__ -   Batch number = 308
01/14/2022 16:22:55 - INFO - __main__ -   Batch number = 309
01/14/2022 16:22:55 - INFO - __main__ -   Batch number = 310
01/14/2022 16:22:56 - INFO - __main__ -   Batch number = 311
01/14/2022 16:22:56 - INFO - __main__ -   Batch number = 312
01/14/2022 16:22:56 - INFO - __main__ -   Batch number = 313
01/14/2022 16:22:58 - INFO - __main__ -   ***** Evaluation result  in ar *****
01/14/2022 16:22:58 - INFO - __main__ -     f1 = 0.333161909659724
01/14/2022 16:22:58 - INFO - __main__ -     loss = 5.3353420271279335
01/14/2022 16:22:58 - INFO - __main__ -     precision = 0.3219047619047619
01/14/2022 16:22:58 - INFO - __main__ -     recall = 0.34523492317257304
01/14/2022 16:23:00 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ar', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:23:00 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/14/2022 16:23:00 - INFO - __main__ -   Seed = 2
01/14/2022 16:23:00 - INFO - root -   save model
01/14/2022 16:23:00 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ar', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:23:00 - INFO - __main__ -   Loading pretrained model and tokenizer
01/14/2022 16:23:03 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/14/2022 16:23:08 - INFO - __main__ -   Using lang2id = None
01/14/2022 16:23:08 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/14/2022 16:23:08 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/
01/14/2022 16:23:08 - INFO - root -   Trying to decide if add adapter
01/14/2022 16:23:08 - INFO - root -   loading task adapter
01/14/2022 16:23:08 - INFO - root -   loading lang adpater en/wiki@ukp,pt/wiki@ukp,id/wiki@ukp,tr/wiki@ukp,cs/wiki@ukp,vi/wiki@ukp,eu/wiki@ukp,fa/wiki@ukp,zh_yue/wiki@ukp,ar/wiki@ukp
01/14/2022 16:23:08 - INFO - __main__ -   Adapter Languages : ['en', 'pt', 'id', 'tr', 'cs', 'vi', 'eu', 'fa', 'zh_yue', 'ar'], Length : 10
01/14/2022 16:23:08 - INFO - __main__ -   Adapter Names ['en/wiki@ukp', 'pt/wiki@ukp', 'id/wiki@ukp', 'tr/wiki@ukp', 'cs/wiki@ukp', 'vi/wiki@ukp', 'eu/wiki@ukp', 'fa/wiki@ukp', 'zh_yue/wiki@ukp', 'ar/wiki@ukp'], Length : 10
01/14/2022 16:23:08 - INFO - __main__ -   Language = en
01/14/2022 16:23:08 - INFO - __main__ -   Adapter Name = en/wiki@ukp
01/14/2022 16:23:09 - INFO - __main__ -   Language = pt
01/14/2022 16:23:09 - INFO - __main__ -   Adapter Name = pt/wiki@ukp
01/14/2022 16:23:11 - INFO - __main__ -   Language = id
01/14/2022 16:23:11 - INFO - __main__ -   Adapter Name = id/wiki@ukp
01/14/2022 16:23:12 - INFO - __main__ -   Language = tr
01/14/2022 16:23:12 - INFO - __main__ -   Adapter Name = tr/wiki@ukp
01/14/2022 16:23:14 - INFO - __main__ -   Language = cs
01/14/2022 16:23:14 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
01/14/2022 16:23:15 - INFO - __main__ -   Language = vi
01/14/2022 16:23:15 - INFO - __main__ -   Adapter Name = vi/wiki@ukp
01/14/2022 16:23:17 - INFO - __main__ -   Language = eu
01/14/2022 16:23:17 - INFO - __main__ -   Adapter Name = eu/wiki@ukp
01/14/2022 16:23:19 - INFO - __main__ -   Language = fa
01/14/2022 16:23:19 - INFO - __main__ -   Adapter Name = fa/wiki@ukp
01/14/2022 16:23:20 - INFO - __main__ -   Language = zh_yue
01/14/2022 16:23:20 - INFO - __main__ -   Adapter Name = zh_yue/wiki@ukp
01/14/2022 16:23:22 - INFO - __main__ -   Language = ar
01/14/2022 16:23:22 - INFO - __main__ -   Adapter Name = ar/wiki@ukp
01/14/2022 16:23:26 - INFO - __main__ -   Args Adapter Weight = equal
01/14/2022 16:23:26 - INFO - __main__ -   Adapter Languages = ['en', 'pt', 'id', 'tr', 'cs', 'vi', 'eu', 'fa', 'zh_yue', 'ar']
01/14/2022 16:23:26 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/14/2022 16:23:26 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/14/2022 16:23:26 - INFO - __main__ -   Length of Adapter Weights = 10
01/14/2022 16:23:26 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/cached_test_ar_bert-base-multilingual-cased_128
01/14/2022 16:23:27 - INFO - __main__ -   ***** Running evaluation  in ar *****
01/14/2022 16:23:27 - INFO - __main__ -     Num examples = 10000
01/14/2022 16:23:27 - INFO - __main__ -     Batch size = 32
01/14/2022 16:23:27 - INFO - __main__ -   Batch number = 1
01/14/2022 16:23:27 - INFO - __main__ -   Batch number = 2
01/14/2022 16:23:28 - INFO - __main__ -   Batch number = 3
01/14/2022 16:23:28 - INFO - __main__ -   Batch number = 4
01/14/2022 16:23:28 - INFO - __main__ -   Batch number = 5
01/14/2022 16:23:29 - INFO - __main__ -   Batch number = 6
01/14/2022 16:23:29 - INFO - __main__ -   Batch number = 7
01/14/2022 16:23:29 - INFO - __main__ -   Batch number = 8
01/14/2022 16:23:29 - INFO - __main__ -   Batch number = 9
01/14/2022 16:23:30 - INFO - __main__ -   Batch number = 10
01/14/2022 16:23:30 - INFO - __main__ -   Batch number = 11
01/14/2022 16:23:30 - INFO - __main__ -   Batch number = 12
01/14/2022 16:23:31 - INFO - __main__ -   Batch number = 13
01/14/2022 16:23:31 - INFO - __main__ -   Batch number = 14
01/14/2022 16:23:31 - INFO - __main__ -   Batch number = 15
01/14/2022 16:23:31 - INFO - __main__ -   Batch number = 16
01/14/2022 16:23:32 - INFO - __main__ -   Batch number = 17
01/14/2022 16:23:32 - INFO - __main__ -   Batch number = 18
01/14/2022 16:23:32 - INFO - __main__ -   Batch number = 19
01/14/2022 16:23:33 - INFO - __main__ -   Batch number = 20
01/14/2022 16:23:33 - INFO - __main__ -   Batch number = 21
01/14/2022 16:23:33 - INFO - __main__ -   Batch number = 22
01/14/2022 16:23:33 - INFO - __main__ -   Batch number = 23
01/14/2022 16:23:34 - INFO - __main__ -   Batch number = 24
01/14/2022 16:23:34 - INFO - __main__ -   Batch number = 25
01/14/2022 16:23:34 - INFO - __main__ -   Batch number = 26
01/14/2022 16:23:35 - INFO - __main__ -   Batch number = 27
01/14/2022 16:23:35 - INFO - __main__ -   Batch number = 28
01/14/2022 16:23:35 - INFO - __main__ -   Batch number = 29
01/14/2022 16:23:35 - INFO - __main__ -   Batch number = 30
01/14/2022 16:23:36 - INFO - __main__ -   Batch number = 31
01/14/2022 16:23:36 - INFO - __main__ -   Batch number = 32
01/14/2022 16:23:36 - INFO - __main__ -   Batch number = 33
01/14/2022 16:23:36 - INFO - __main__ -   Batch number = 34
01/14/2022 16:23:37 - INFO - __main__ -   Batch number = 35
01/14/2022 16:23:37 - INFO - __main__ -   Batch number = 36
01/14/2022 16:23:37 - INFO - __main__ -   Batch number = 37
01/14/2022 16:23:38 - INFO - __main__ -   Batch number = 38
01/14/2022 16:23:38 - INFO - __main__ -   Batch number = 39
01/14/2022 16:23:38 - INFO - __main__ -   Batch number = 40
01/14/2022 16:23:38 - INFO - __main__ -   Batch number = 41
01/14/2022 16:23:39 - INFO - __main__ -   Batch number = 42
01/14/2022 16:23:39 - INFO - __main__ -   Batch number = 43
01/14/2022 16:23:39 - INFO - __main__ -   Batch number = 44
01/14/2022 16:23:40 - INFO - __main__ -   Batch number = 45
01/14/2022 16:23:40 - INFO - __main__ -   Batch number = 46
01/14/2022 16:23:40 - INFO - __main__ -   Batch number = 47
01/14/2022 16:23:40 - INFO - __main__ -   Batch number = 48
01/14/2022 16:23:41 - INFO - __main__ -   Batch number = 49
01/14/2022 16:23:41 - INFO - __main__ -   Batch number = 50
01/14/2022 16:23:41 - INFO - __main__ -   Batch number = 51
01/14/2022 16:23:42 - INFO - __main__ -   Batch number = 52
01/14/2022 16:23:42 - INFO - __main__ -   Batch number = 53
01/14/2022 16:23:42 - INFO - __main__ -   Batch number = 54
01/14/2022 16:23:42 - INFO - __main__ -   Batch number = 55
01/14/2022 16:23:43 - INFO - __main__ -   Batch number = 56
01/14/2022 16:23:43 - INFO - __main__ -   Batch number = 57
01/14/2022 16:23:43 - INFO - __main__ -   Batch number = 58
01/14/2022 16:23:44 - INFO - __main__ -   Batch number = 59
01/14/2022 16:23:44 - INFO - __main__ -   Batch number = 60
01/14/2022 16:23:44 - INFO - __main__ -   Batch number = 61
01/14/2022 16:23:44 - INFO - __main__ -   Batch number = 62
01/14/2022 16:23:45 - INFO - __main__ -   Batch number = 63
01/14/2022 16:23:45 - INFO - __main__ -   Batch number = 64
01/14/2022 16:23:45 - INFO - __main__ -   Batch number = 65
01/14/2022 16:23:46 - INFO - __main__ -   Batch number = 66
01/14/2022 16:23:46 - INFO - __main__ -   Batch number = 67
01/14/2022 16:23:46 - INFO - __main__ -   Batch number = 68
01/14/2022 16:23:46 - INFO - __main__ -   Batch number = 69
01/14/2022 16:23:47 - INFO - __main__ -   Batch number = 70
01/14/2022 16:23:47 - INFO - __main__ -   Batch number = 71
01/14/2022 16:23:47 - INFO - __main__ -   Batch number = 72
01/14/2022 16:23:48 - INFO - __main__ -   Batch number = 73
01/14/2022 16:23:48 - INFO - __main__ -   Batch number = 74
01/14/2022 16:23:48 - INFO - __main__ -   Batch number = 75
01/14/2022 16:23:49 - INFO - __main__ -   Batch number = 76
01/14/2022 16:23:49 - INFO - __main__ -   Batch number = 77
01/14/2022 16:23:49 - INFO - __main__ -   Batch number = 78
01/14/2022 16:23:49 - INFO - __main__ -   Batch number = 79
01/14/2022 16:23:50 - INFO - __main__ -   Batch number = 80
01/14/2022 16:23:50 - INFO - __main__ -   Batch number = 81
01/14/2022 16:23:50 - INFO - __main__ -   Batch number = 82
01/14/2022 16:23:51 - INFO - __main__ -   Batch number = 83
01/14/2022 16:23:51 - INFO - __main__ -   Batch number = 84
01/14/2022 16:23:51 - INFO - __main__ -   Batch number = 85
01/14/2022 16:23:52 - INFO - __main__ -   Batch number = 86
01/14/2022 16:23:52 - INFO - __main__ -   Batch number = 87
01/14/2022 16:23:52 - INFO - __main__ -   Batch number = 88
01/14/2022 16:23:52 - INFO - __main__ -   Batch number = 89
01/14/2022 16:23:53 - INFO - __main__ -   Batch number = 90
01/14/2022 16:23:53 - INFO - __main__ -   Batch number = 91
01/14/2022 16:23:53 - INFO - __main__ -   Batch number = 92
01/14/2022 16:23:54 - INFO - __main__ -   Batch number = 93
01/14/2022 16:23:54 - INFO - __main__ -   Batch number = 94
01/14/2022 16:23:54 - INFO - __main__ -   Batch number = 95
01/14/2022 16:23:55 - INFO - __main__ -   Batch number = 96
01/14/2022 16:23:55 - INFO - __main__ -   Batch number = 97
01/14/2022 16:23:55 - INFO - __main__ -   Batch number = 98
01/14/2022 16:23:56 - INFO - __main__ -   Batch number = 99
01/14/2022 16:23:56 - INFO - __main__ -   Batch number = 100
01/14/2022 16:23:56 - INFO - __main__ -   Batch number = 101
01/14/2022 16:23:57 - INFO - __main__ -   Batch number = 102
01/14/2022 16:23:57 - INFO - __main__ -   Batch number = 103
01/14/2022 16:23:57 - INFO - __main__ -   Batch number = 104
01/14/2022 16:23:58 - INFO - __main__ -   Batch number = 105
01/14/2022 16:23:58 - INFO - __main__ -   Batch number = 106
01/14/2022 16:23:58 - INFO - __main__ -   Batch number = 107
01/14/2022 16:23:58 - INFO - __main__ -   Batch number = 108
01/14/2022 16:23:59 - INFO - __main__ -   Batch number = 109
01/14/2022 16:23:59 - INFO - __main__ -   Batch number = 110
01/14/2022 16:23:59 - INFO - __main__ -   Batch number = 111
01/14/2022 16:24:00 - INFO - __main__ -   Batch number = 112
01/14/2022 16:24:00 - INFO - __main__ -   Batch number = 113
01/14/2022 16:24:00 - INFO - __main__ -   Batch number = 114
01/14/2022 16:24:01 - INFO - __main__ -   Batch number = 115
01/14/2022 16:24:01 - INFO - __main__ -   Batch number = 116
01/14/2022 16:24:01 - INFO - __main__ -   Batch number = 117
01/14/2022 16:24:02 - INFO - __main__ -   Batch number = 118
01/14/2022 16:24:02 - INFO - __main__ -   Batch number = 119
01/14/2022 16:24:02 - INFO - __main__ -   Batch number = 120
01/14/2022 16:24:02 - INFO - __main__ -   Batch number = 121
01/14/2022 16:24:03 - INFO - __main__ -   Batch number = 122
01/14/2022 16:24:03 - INFO - __main__ -   Batch number = 123
01/14/2022 16:24:03 - INFO - __main__ -   Batch number = 124
01/14/2022 16:24:04 - INFO - __main__ -   Batch number = 125
01/14/2022 16:24:04 - INFO - __main__ -   Batch number = 126
01/14/2022 16:24:04 - INFO - __main__ -   Batch number = 127
01/14/2022 16:24:04 - INFO - __main__ -   Batch number = 128
01/14/2022 16:24:05 - INFO - __main__ -   Batch number = 129
01/14/2022 16:24:05 - INFO - __main__ -   Batch number = 130
01/14/2022 16:24:05 - INFO - __main__ -   Batch number = 131
01/14/2022 16:24:06 - INFO - __main__ -   Batch number = 132
01/14/2022 16:24:06 - INFO - __main__ -   Batch number = 133
01/14/2022 16:24:06 - INFO - __main__ -   Batch number = 134
01/14/2022 16:24:07 - INFO - __main__ -   Batch number = 135
01/14/2022 16:24:07 - INFO - __main__ -   Batch number = 136
01/14/2022 16:24:07 - INFO - __main__ -   Batch number = 137
01/14/2022 16:24:07 - INFO - __main__ -   Batch number = 138
01/14/2022 16:24:08 - INFO - __main__ -   Batch number = 139
01/14/2022 16:24:08 - INFO - __main__ -   Batch number = 140
01/14/2022 16:24:08 - INFO - __main__ -   Batch number = 141
01/14/2022 16:24:09 - INFO - __main__ -   Batch number = 142
01/14/2022 16:24:09 - INFO - __main__ -   Batch number = 143
01/14/2022 16:24:09 - INFO - __main__ -   Batch number = 144
01/14/2022 16:24:10 - INFO - __main__ -   Batch number = 145
01/14/2022 16:24:10 - INFO - __main__ -   Batch number = 146
01/14/2022 16:24:10 - INFO - __main__ -   Batch number = 147
01/14/2022 16:24:10 - INFO - __main__ -   Batch number = 148
01/14/2022 16:24:11 - INFO - __main__ -   Batch number = 149
01/14/2022 16:24:11 - INFO - __main__ -   Batch number = 150
01/14/2022 16:24:11 - INFO - __main__ -   Batch number = 151
01/14/2022 16:24:12 - INFO - __main__ -   Batch number = 152
01/14/2022 16:24:12 - INFO - __main__ -   Batch number = 153
01/14/2022 16:24:12 - INFO - __main__ -   Batch number = 154
01/14/2022 16:24:13 - INFO - __main__ -   Batch number = 155
01/14/2022 16:24:13 - INFO - __main__ -   Batch number = 156
01/14/2022 16:24:13 - INFO - __main__ -   Batch number = 157
01/14/2022 16:24:14 - INFO - __main__ -   Batch number = 158
01/14/2022 16:24:14 - INFO - __main__ -   Batch number = 159
01/14/2022 16:24:14 - INFO - __main__ -   Batch number = 160
01/14/2022 16:24:14 - INFO - __main__ -   Batch number = 161
01/14/2022 16:24:15 - INFO - __main__ -   Batch number = 162
01/14/2022 16:24:15 - INFO - __main__ -   Batch number = 163
01/14/2022 16:24:15 - INFO - __main__ -   Batch number = 164
01/14/2022 16:24:16 - INFO - __main__ -   Batch number = 165
01/14/2022 16:24:16 - INFO - __main__ -   Batch number = 166
01/14/2022 16:24:16 - INFO - __main__ -   Batch number = 167
01/14/2022 16:24:17 - INFO - __main__ -   Batch number = 168
01/14/2022 16:24:17 - INFO - __main__ -   Batch number = 169
01/14/2022 16:24:17 - INFO - __main__ -   Batch number = 170
01/14/2022 16:24:18 - INFO - __main__ -   Batch number = 171
01/14/2022 16:24:18 - INFO - __main__ -   Batch number = 172
01/14/2022 16:24:18 - INFO - __main__ -   Batch number = 173
01/14/2022 16:24:18 - INFO - __main__ -   Batch number = 174
01/14/2022 16:24:19 - INFO - __main__ -   Batch number = 175
01/14/2022 16:24:19 - INFO - __main__ -   Batch number = 176
01/14/2022 16:24:19 - INFO - __main__ -   Batch number = 177
01/14/2022 16:24:20 - INFO - __main__ -   Batch number = 178
01/14/2022 16:24:20 - INFO - __main__ -   Batch number = 179
01/14/2022 16:24:20 - INFO - __main__ -   Batch number = 180
01/14/2022 16:24:21 - INFO - __main__ -   Batch number = 181
01/14/2022 16:24:21 - INFO - __main__ -   Batch number = 182
01/14/2022 16:24:21 - INFO - __main__ -   Batch number = 183
01/14/2022 16:24:21 - INFO - __main__ -   Batch number = 184
01/14/2022 16:24:22 - INFO - __main__ -   Batch number = 185
01/14/2022 16:24:22 - INFO - __main__ -   Batch number = 186
01/14/2022 16:24:22 - INFO - __main__ -   Batch number = 187
01/14/2022 16:24:23 - INFO - __main__ -   Batch number = 188
01/14/2022 16:24:23 - INFO - __main__ -   Batch number = 189
01/14/2022 16:24:23 - INFO - __main__ -   Batch number = 190
01/14/2022 16:24:24 - INFO - __main__ -   Batch number = 191
01/14/2022 16:24:24 - INFO - __main__ -   Batch number = 192
01/14/2022 16:24:24 - INFO - __main__ -   Batch number = 193
01/14/2022 16:24:25 - INFO - __main__ -   Batch number = 194
01/14/2022 16:24:25 - INFO - __main__ -   Batch number = 195
01/14/2022 16:24:25 - INFO - __main__ -   Batch number = 196
01/14/2022 16:24:25 - INFO - __main__ -   Batch number = 197
01/14/2022 16:24:26 - INFO - __main__ -   Batch number = 198
01/14/2022 16:24:26 - INFO - __main__ -   Batch number = 199
01/14/2022 16:24:26 - INFO - __main__ -   Batch number = 200
01/14/2022 16:24:27 - INFO - __main__ -   Batch number = 201
01/14/2022 16:24:27 - INFO - __main__ -   Batch number = 202
01/14/2022 16:24:27 - INFO - __main__ -   Batch number = 203
01/14/2022 16:24:28 - INFO - __main__ -   Batch number = 204
01/14/2022 16:24:28 - INFO - __main__ -   Batch number = 205
01/14/2022 16:24:28 - INFO - __main__ -   Batch number = 206
01/14/2022 16:24:28 - INFO - __main__ -   Batch number = 207
01/14/2022 16:24:29 - INFO - __main__ -   Batch number = 208
01/14/2022 16:24:29 - INFO - __main__ -   Batch number = 209
01/14/2022 16:24:29 - INFO - __main__ -   Batch number = 210
01/14/2022 16:24:30 - INFO - __main__ -   Batch number = 211
01/14/2022 16:24:30 - INFO - __main__ -   Batch number = 212
01/14/2022 16:24:30 - INFO - __main__ -   Batch number = 213
01/14/2022 16:24:31 - INFO - __main__ -   Batch number = 214
01/14/2022 16:24:31 - INFO - __main__ -   Batch number = 215
01/14/2022 16:24:31 - INFO - __main__ -   Batch number = 216
01/14/2022 16:24:31 - INFO - __main__ -   Batch number = 217
01/14/2022 16:24:32 - INFO - __main__ -   Batch number = 218
01/14/2022 16:24:32 - INFO - __main__ -   Batch number = 219
01/14/2022 16:24:32 - INFO - __main__ -   Batch number = 220
01/14/2022 16:24:33 - INFO - __main__ -   Batch number = 221
01/14/2022 16:24:33 - INFO - __main__ -   Batch number = 222
01/14/2022 16:24:33 - INFO - __main__ -   Batch number = 223
01/14/2022 16:24:34 - INFO - __main__ -   Batch number = 224
01/14/2022 16:24:34 - INFO - __main__ -   Batch number = 225
01/14/2022 16:24:34 - INFO - __main__ -   Batch number = 226
01/14/2022 16:24:35 - INFO - __main__ -   Batch number = 227
01/14/2022 16:24:35 - INFO - __main__ -   Batch number = 228
01/14/2022 16:24:35 - INFO - __main__ -   Batch number = 229
01/14/2022 16:24:36 - INFO - __main__ -   Batch number = 230
01/14/2022 16:24:36 - INFO - __main__ -   Batch number = 231
01/14/2022 16:24:36 - INFO - __main__ -   Batch number = 232
01/14/2022 16:24:36 - INFO - __main__ -   Batch number = 233
01/14/2022 16:24:37 - INFO - __main__ -   Batch number = 234
01/14/2022 16:24:37 - INFO - __main__ -   Batch number = 235
01/14/2022 16:24:37 - INFO - __main__ -   Batch number = 236
01/14/2022 16:24:38 - INFO - __main__ -   Batch number = 237
01/14/2022 16:24:38 - INFO - __main__ -   Batch number = 238
01/14/2022 16:24:38 - INFO - __main__ -   Batch number = 239
01/14/2022 16:24:39 - INFO - __main__ -   Batch number = 240
01/14/2022 16:24:39 - INFO - __main__ -   Batch number = 241
01/14/2022 16:24:39 - INFO - __main__ -   Batch number = 242
01/14/2022 16:24:39 - INFO - __main__ -   Batch number = 243
01/14/2022 16:24:40 - INFO - __main__ -   Batch number = 244
01/14/2022 16:24:40 - INFO - __main__ -   Batch number = 245
01/14/2022 16:24:40 - INFO - __main__ -   Batch number = 246
01/14/2022 16:24:41 - INFO - __main__ -   Batch number = 247
01/14/2022 16:24:41 - INFO - __main__ -   Batch number = 248
01/14/2022 16:24:41 - INFO - __main__ -   Batch number = 249
01/14/2022 16:24:41 - INFO - __main__ -   Batch number = 250
01/14/2022 16:24:42 - INFO - __main__ -   Batch number = 251
01/14/2022 16:24:42 - INFO - __main__ -   Batch number = 252
01/14/2022 16:24:42 - INFO - __main__ -   Batch number = 253
01/14/2022 16:24:43 - INFO - __main__ -   Batch number = 254
01/14/2022 16:24:43 - INFO - __main__ -   Batch number = 255
01/14/2022 16:24:43 - INFO - __main__ -   Batch number = 256
01/14/2022 16:24:44 - INFO - __main__ -   Batch number = 257
01/14/2022 16:24:44 - INFO - __main__ -   Batch number = 258
01/14/2022 16:24:44 - INFO - __main__ -   Batch number = 259
01/14/2022 16:24:44 - INFO - __main__ -   Batch number = 260
01/14/2022 16:24:45 - INFO - __main__ -   Batch number = 261
01/14/2022 16:24:45 - INFO - __main__ -   Batch number = 262
01/14/2022 16:24:45 - INFO - __main__ -   Batch number = 263
01/14/2022 16:24:46 - INFO - __main__ -   Batch number = 264
01/14/2022 16:24:46 - INFO - __main__ -   Batch number = 265
01/14/2022 16:24:46 - INFO - __main__ -   Batch number = 266
01/14/2022 16:24:47 - INFO - __main__ -   Batch number = 267
01/14/2022 16:24:48 - INFO - __main__ -   Batch number = 268
01/14/2022 16:24:48 - INFO - __main__ -   Batch number = 269
01/14/2022 16:24:48 - INFO - __main__ -   Batch number = 270
01/14/2022 16:24:49 - INFO - __main__ -   Batch number = 271
01/14/2022 16:24:49 - INFO - __main__ -   Batch number = 272
01/14/2022 16:24:49 - INFO - __main__ -   Batch number = 273
01/14/2022 16:24:49 - INFO - __main__ -   Batch number = 274
01/14/2022 16:24:50 - INFO - __main__ -   Batch number = 275
01/14/2022 16:24:50 - INFO - __main__ -   Batch number = 276
01/14/2022 16:24:50 - INFO - __main__ -   Batch number = 277
01/14/2022 16:24:51 - INFO - __main__ -   Batch number = 278
01/14/2022 16:24:51 - INFO - __main__ -   Batch number = 279
01/14/2022 16:24:51 - INFO - __main__ -   Batch number = 280
01/14/2022 16:24:52 - INFO - __main__ -   Batch number = 281
01/14/2022 16:24:52 - INFO - __main__ -   Batch number = 282
01/14/2022 16:24:52 - INFO - __main__ -   Batch number = 283
01/14/2022 16:24:53 - INFO - __main__ -   Batch number = 284
01/14/2022 16:24:53 - INFO - __main__ -   Batch number = 285
01/14/2022 16:24:53 - INFO - __main__ -   Batch number = 286
01/14/2022 16:24:53 - INFO - __main__ -   Batch number = 287
01/14/2022 16:24:54 - INFO - __main__ -   Batch number = 288
01/14/2022 16:24:54 - INFO - __main__ -   Batch number = 289
01/14/2022 16:24:54 - INFO - __main__ -   Batch number = 290
01/14/2022 16:24:55 - INFO - __main__ -   Batch number = 291
01/14/2022 16:24:55 - INFO - __main__ -   Batch number = 292
01/14/2022 16:24:55 - INFO - __main__ -   Batch number = 293
01/14/2022 16:24:56 - INFO - __main__ -   Batch number = 294
01/14/2022 16:24:56 - INFO - __main__ -   Batch number = 295
01/14/2022 16:24:56 - INFO - __main__ -   Batch number = 296
01/14/2022 16:24:56 - INFO - __main__ -   Batch number = 297
01/14/2022 16:24:57 - INFO - __main__ -   Batch number = 298
01/14/2022 16:24:57 - INFO - __main__ -   Batch number = 299
01/14/2022 16:24:57 - INFO - __main__ -   Batch number = 300
01/14/2022 16:24:58 - INFO - __main__ -   Batch number = 301
01/14/2022 16:24:58 - INFO - __main__ -   Batch number = 302
01/14/2022 16:24:58 - INFO - __main__ -   Batch number = 303
01/14/2022 16:24:58 - INFO - __main__ -   Batch number = 304
01/14/2022 16:24:59 - INFO - __main__ -   Batch number = 305
01/14/2022 16:24:59 - INFO - __main__ -   Batch number = 306
01/14/2022 16:24:59 - INFO - __main__ -   Batch number = 307
01/14/2022 16:25:00 - INFO - __main__ -   Batch number = 308
01/14/2022 16:25:00 - INFO - __main__ -   Batch number = 309
01/14/2022 16:25:00 - INFO - __main__ -   Batch number = 310
01/14/2022 16:25:01 - INFO - __main__ -   Batch number = 311
01/14/2022 16:25:01 - INFO - __main__ -   Batch number = 312
01/14/2022 16:25:01 - INFO - __main__ -   Batch number = 313
01/14/2022 16:25:03 - INFO - __main__ -   ***** Evaluation result  in ar *****
01/14/2022 16:25:03 - INFO - __main__ -     f1 = 0.3475425870079061
01/14/2022 16:25:03 - INFO - __main__ -     loss = 5.097786715236335
01/14/2022 16:25:03 - INFO - __main__ -     precision = 0.32110851720762107
01/14/2022 16:25:03 - INFO - __main__ -     recall = 0.37871924682476243
01/14/2022 16:25:05 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ar', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:25:05 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/14/2022 16:25:05 - INFO - __main__ -   Seed = 3
01/14/2022 16:25:05 - INFO - root -   save model
01/14/2022 16:25:05 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ar', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:25:05 - INFO - __main__ -   Loading pretrained model and tokenizer
01/14/2022 16:25:07 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/14/2022 16:25:13 - INFO - __main__ -   Using lang2id = None
01/14/2022 16:25:13 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/14/2022 16:25:13 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/
01/14/2022 16:25:13 - INFO - root -   Trying to decide if add adapter
01/14/2022 16:25:13 - INFO - root -   loading task adapter
01/14/2022 16:25:13 - INFO - root -   loading lang adpater en/wiki@ukp,pt/wiki@ukp,id/wiki@ukp,tr/wiki@ukp,vi/wiki@ukp,fa/wiki@ukp,eu/wiki@ukp,zh_yue/wiki@ukp,cs/wiki@ukp,ar/wiki@ukp
01/14/2022 16:25:13 - INFO - __main__ -   Adapter Languages : ['en', 'pt', 'id', 'tr', 'vi', 'fa', 'eu', 'zh_yue', 'cs', 'ar'], Length : 10
01/14/2022 16:25:13 - INFO - __main__ -   Adapter Names ['en/wiki@ukp', 'pt/wiki@ukp', 'id/wiki@ukp', 'tr/wiki@ukp', 'vi/wiki@ukp', 'fa/wiki@ukp', 'eu/wiki@ukp', 'zh_yue/wiki@ukp', 'cs/wiki@ukp', 'ar/wiki@ukp'], Length : 10
01/14/2022 16:25:13 - INFO - __main__ -   Language = en
01/14/2022 16:25:13 - INFO - __main__ -   Adapter Name = en/wiki@ukp
01/14/2022 16:25:14 - INFO - __main__ -   Language = pt
01/14/2022 16:25:14 - INFO - __main__ -   Adapter Name = pt/wiki@ukp
01/14/2022 16:25:15 - INFO - __main__ -   Language = id
01/14/2022 16:25:15 - INFO - __main__ -   Adapter Name = id/wiki@ukp
01/14/2022 16:25:17 - INFO - __main__ -   Language = tr
01/14/2022 16:25:17 - INFO - __main__ -   Adapter Name = tr/wiki@ukp
01/14/2022 16:25:19 - INFO - __main__ -   Language = vi
01/14/2022 16:25:19 - INFO - __main__ -   Adapter Name = vi/wiki@ukp
01/14/2022 16:25:21 - INFO - __main__ -   Language = fa
01/14/2022 16:25:21 - INFO - __main__ -   Adapter Name = fa/wiki@ukp
01/14/2022 16:25:21 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='jv', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:25:21 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/14/2022 16:25:21 - INFO - __main__ -   Seed = 1
01/14/2022 16:25:21 - INFO - root -   save model
01/14/2022 16:25:21 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='jv', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:25:21 - INFO - __main__ -   Loading pretrained model and tokenizer
01/14/2022 16:25:22 - INFO - __main__ -   Language = eu
01/14/2022 16:25:22 - INFO - __main__ -   Adapter Name = eu/wiki@ukp
01/14/2022 16:25:23 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/14/2022 16:25:24 - INFO - __main__ -   Language = zh_yue
01/14/2022 16:25:24 - INFO - __main__ -   Adapter Name = zh_yue/wiki@ukp
01/14/2022 16:25:25 - INFO - __main__ -   Language = cs
01/14/2022 16:25:25 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
01/14/2022 16:25:27 - INFO - __main__ -   Language = ar
01/14/2022 16:25:27 - INFO - __main__ -   Adapter Name = ar/wiki@ukp
01/14/2022 16:25:29 - INFO - __main__ -   Using lang2id = None
01/14/2022 16:25:29 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/14/2022 16:25:29 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/
01/14/2022 16:25:29 - INFO - root -   Trying to decide if add adapter
01/14/2022 16:25:29 - INFO - root -   loading task adapter
01/14/2022 16:25:29 - INFO - root -   loading lang adpater en/wiki@ukp,pt/wiki@ukp,id/wiki@ukp,cs/wiki@ukp,tr/wiki@ukp,eu/wiki@ukp,zh_yue/wiki@ukp,vi/wiki@ukp,fr/wiki@ukp,jv/wiki@ukp
01/14/2022 16:25:29 - INFO - __main__ -   Adapter Languages : ['en', 'pt', 'id', 'cs', 'tr', 'eu', 'zh_yue', 'vi', 'fr', 'jv'], Length : 10
01/14/2022 16:25:29 - INFO - __main__ -   Adapter Names ['en/wiki@ukp', 'pt/wiki@ukp', 'id/wiki@ukp', 'cs/wiki@ukp', 'tr/wiki@ukp', 'eu/wiki@ukp', 'zh_yue/wiki@ukp', 'vi/wiki@ukp', 'fr/wiki@ukp', 'jv/wiki@ukp'], Length : 10
01/14/2022 16:25:29 - INFO - __main__ -   Language = en
01/14/2022 16:25:29 - INFO - __main__ -   Adapter Name = en/wiki@ukp
01/14/2022 16:25:31 - INFO - __main__ -   Language = pt
01/14/2022 16:25:31 - INFO - __main__ -   Adapter Name = pt/wiki@ukp
01/14/2022 16:25:31 - INFO - __main__ -   Args Adapter Weight = equal
01/14/2022 16:25:31 - INFO - __main__ -   Adapter Languages = ['en', 'pt', 'id', 'tr', 'vi', 'fa', 'eu', 'zh_yue', 'cs', 'ar']
01/14/2022 16:25:31 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/14/2022 16:25:31 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/14/2022 16:25:31 - INFO - __main__ -   Length of Adapter Weights = 10
01/14/2022 16:25:31 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/cached_test_ar_bert-base-multilingual-cased_128
01/14/2022 16:25:32 - INFO - __main__ -   ***** Running evaluation  in ar *****
01/14/2022 16:25:32 - INFO - __main__ -     Num examples = 10000
01/14/2022 16:25:32 - INFO - __main__ -     Batch size = 32
01/14/2022 16:25:32 - INFO - __main__ -   Batch number = 1
01/14/2022 16:25:33 - INFO - __main__ -   Batch number = 2
01/14/2022 16:25:33 - INFO - __main__ -   Batch number = 3
01/14/2022 16:25:33 - INFO - __main__ -   Language = id
01/14/2022 16:25:33 - INFO - __main__ -   Adapter Name = id/wiki@ukp
01/14/2022 16:25:33 - INFO - __main__ -   Batch number = 4
01/14/2022 16:25:33 - INFO - __main__ -   Batch number = 5
01/14/2022 16:25:34 - INFO - __main__ -   Batch number = 6
01/14/2022 16:25:34 - INFO - __main__ -   Batch number = 7
01/14/2022 16:25:34 - INFO - __main__ -   Batch number = 8
01/14/2022 16:25:35 - INFO - __main__ -   Batch number = 9
01/14/2022 16:25:35 - INFO - __main__ -   Batch number = 10
01/14/2022 16:25:35 - INFO - __main__ -   Batch number = 11
01/14/2022 16:25:35 - INFO - __main__ -   Language = cs
01/14/2022 16:25:35 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
01/14/2022 16:25:35 - INFO - __main__ -   Batch number = 12
01/14/2022 16:25:36 - INFO - __main__ -   Batch number = 13
01/14/2022 16:25:36 - INFO - __main__ -   Batch number = 14
01/14/2022 16:25:36 - INFO - __main__ -   Batch number = 15
01/14/2022 16:25:36 - INFO - __main__ -   Batch number = 16
01/14/2022 16:25:37 - INFO - __main__ -   Batch number = 17
01/14/2022 16:25:37 - INFO - __main__ -   Batch number = 18
01/14/2022 16:25:37 - INFO - __main__ -   Batch number = 19
01/14/2022 16:25:37 - INFO - __main__ -   Language = tr
01/14/2022 16:25:37 - INFO - __main__ -   Adapter Name = tr/wiki@ukp
01/14/2022 16:25:38 - INFO - __main__ -   Batch number = 20
01/14/2022 16:25:38 - INFO - __main__ -   Batch number = 21
01/14/2022 16:25:38 - INFO - __main__ -   Batch number = 22
01/14/2022 16:25:38 - INFO - __main__ -   Batch number = 23
01/14/2022 16:25:39 - INFO - __main__ -   Batch number = 24
01/14/2022 16:25:39 - INFO - __main__ -   Batch number = 25
01/14/2022 16:25:39 - INFO - __main__ -   Batch number = 26
01/14/2022 16:25:39 - INFO - __main__ -   Language = eu
01/14/2022 16:25:39 - INFO - __main__ -   Adapter Name = eu/wiki@ukp
01/14/2022 16:25:40 - INFO - __main__ -   Batch number = 27
01/14/2022 16:25:40 - INFO - __main__ -   Batch number = 28
01/14/2022 16:25:40 - INFO - __main__ -   Batch number = 29
01/14/2022 16:25:41 - INFO - __main__ -   Batch number = 30
01/14/2022 16:25:41 - INFO - __main__ -   Batch number = 31
01/14/2022 16:25:41 - INFO - __main__ -   Batch number = 32
01/14/2022 16:25:41 - INFO - __main__ -   Batch number = 33
01/14/2022 16:25:42 - INFO - __main__ -   Language = zh_yue
01/14/2022 16:25:42 - INFO - __main__ -   Adapter Name = zh_yue/wiki@ukp
01/14/2022 16:25:42 - INFO - __main__ -   Batch number = 34
01/14/2022 16:25:42 - INFO - __main__ -   Batch number = 35
01/14/2022 16:25:42 - INFO - __main__ -   Batch number = 36
01/14/2022 16:25:43 - INFO - __main__ -   Batch number = 37
01/14/2022 16:25:43 - INFO - __main__ -   Batch number = 38
01/14/2022 16:25:43 - INFO - __main__ -   Batch number = 39
01/14/2022 16:25:43 - INFO - __main__ -   Batch number = 40
01/14/2022 16:25:44 - INFO - __main__ -   Batch number = 41
01/14/2022 16:25:44 - INFO - __main__ -   Language = vi
01/14/2022 16:25:44 - INFO - __main__ -   Adapter Name = vi/wiki@ukp
01/14/2022 16:25:44 - INFO - __main__ -   Batch number = 42
01/14/2022 16:25:44 - INFO - __main__ -   Batch number = 43
01/14/2022 16:25:44 - INFO - __main__ -   Batch number = 44
01/14/2022 16:25:45 - INFO - __main__ -   Batch number = 45
01/14/2022 16:25:45 - INFO - __main__ -   Batch number = 46
01/14/2022 16:25:45 - INFO - __main__ -   Batch number = 47
01/14/2022 16:25:46 - INFO - __main__ -   Batch number = 48
01/14/2022 16:25:46 - INFO - __main__ -   Language = fr
01/14/2022 16:25:46 - INFO - __main__ -   Adapter Name = fr/wiki@ukp
01/14/2022 16:25:46 - INFO - __main__ -   Batch number = 49
01/14/2022 16:25:46 - INFO - __main__ -   Batch number = 50
01/14/2022 16:25:47 - INFO - __main__ -   Batch number = 51
01/14/2022 16:25:47 - INFO - __main__ -   Language = jv
01/14/2022 16:25:47 - INFO - __main__ -   Adapter Name = jv/wiki@ukp
01/14/2022 16:25:47 - INFO - __main__ -   Batch number = 52
01/14/2022 16:25:47 - INFO - __main__ -   Batch number = 53
01/14/2022 16:25:47 - INFO - __main__ -   Batch number = 54
01/14/2022 16:25:48 - INFO - __main__ -   Batch number = 55
01/14/2022 16:25:48 - INFO - __main__ -   Batch number = 56
01/14/2022 16:25:48 - INFO - __main__ -   Batch number = 57
01/14/2022 16:25:49 - INFO - __main__ -   Batch number = 58
01/14/2022 16:25:49 - INFO - __main__ -   Batch number = 59
01/14/2022 16:25:49 - INFO - __main__ -   Batch number = 60
01/14/2022 16:25:49 - INFO - __main__ -   Batch number = 61
01/14/2022 16:25:50 - INFO - __main__ -   Batch number = 62
01/14/2022 16:25:50 - INFO - __main__ -   Batch number = 63
01/14/2022 16:25:50 - INFO - __main__ -   Batch number = 64
01/14/2022 16:25:51 - INFO - __main__ -   Batch number = 65
01/14/2022 16:25:51 - INFO - __main__ -   Batch number = 66
01/14/2022 16:25:51 - INFO - __main__ -   Batch number = 67
01/14/2022 16:25:51 - INFO - __main__ -   Batch number = 68
01/14/2022 16:25:52 - INFO - __main__ -   Args Adapter Weight = equal
01/14/2022 16:25:52 - INFO - __main__ -   Adapter Languages = ['en', 'pt', 'id', 'cs', 'tr', 'eu', 'zh_yue', 'vi', 'fr', 'jv']
01/14/2022 16:25:52 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/14/2022 16:25:52 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/14/2022 16:25:52 - INFO - __main__ -   Length of Adapter Weights = 10
01/14/2022 16:25:52 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/cached_test_jv_bert-base-multilingual-cased_128
01/14/2022 16:25:52 - INFO - __main__ -   ***** Running evaluation  in jv *****
01/14/2022 16:25:52 - INFO - __main__ -     Num examples = 100
01/14/2022 16:25:52 - INFO - __main__ -     Batch size = 32
01/14/2022 16:25:52 - INFO - __main__ -   Batch number = 1
01/14/2022 16:25:52 - INFO - __main__ -   Batch number = 69
01/14/2022 16:25:52 - INFO - __main__ -   Batch number = 2
01/14/2022 16:25:52 - INFO - __main__ -   Batch number = 70
01/14/2022 16:25:52 - INFO - __main__ -   Batch number = 3
01/14/2022 16:25:52 - INFO - __main__ -   Batch number = 71
01/14/2022 16:25:53 - INFO - __main__ -   Batch number = 4
01/14/2022 16:25:53 - INFO - __main__ -   Batch number = 72
01/14/2022 16:25:53 - INFO - __main__ -   ***** Evaluation result  in jv *****
01/14/2022 16:25:53 - INFO - __main__ -     f1 = 0.5912408759124088
01/14/2022 16:25:53 - INFO - __main__ -     loss = 2.6263967156410217
01/14/2022 16:25:53 - INFO - __main__ -     precision = 0.5159235668789809
01/14/2022 16:25:53 - INFO - __main__ -     recall = 0.6923076923076923
01/14/2022 16:25:53 - INFO - __main__ -   Batch number = 73
01/14/2022 16:25:53 - INFO - __main__ -   Batch number = 74
01/14/2022 16:25:53 - INFO - __main__ -   Batch number = 75
01/14/2022 16:25:54 - INFO - __main__ -   Batch number = 76
01/14/2022 16:25:54 - INFO - __main__ -   Batch number = 77
01/14/2022 16:25:54 - INFO - __main__ -   Batch number = 78
01/14/2022 16:25:55 - INFO - __main__ -   Batch number = 79
01/14/2022 16:25:55 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='jv', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:25:55 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/14/2022 16:25:55 - INFO - __main__ -   Seed = 2
01/14/2022 16:25:55 - INFO - root -   save model
01/14/2022 16:25:55 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='jv', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:25:55 - INFO - __main__ -   Loading pretrained model and tokenizer
01/14/2022 16:25:55 - INFO - __main__ -   Batch number = 80
01/14/2022 16:25:55 - INFO - __main__ -   Batch number = 81
01/14/2022 16:25:55 - INFO - __main__ -   Batch number = 82
01/14/2022 16:25:56 - INFO - __main__ -   Batch number = 83
01/14/2022 16:25:56 - INFO - __main__ -   Batch number = 84
01/14/2022 16:25:56 - INFO - __main__ -   Batch number = 85
01/14/2022 16:25:57 - INFO - __main__ -   Batch number = 86
01/14/2022 16:25:57 - INFO - __main__ -   Batch number = 87
01/14/2022 16:25:57 - INFO - __main__ -   Batch number = 88
01/14/2022 16:25:57 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/14/2022 16:25:58 - INFO - __main__ -   Batch number = 89
01/14/2022 16:25:58 - INFO - __main__ -   Batch number = 90
01/14/2022 16:25:58 - INFO - __main__ -   Batch number = 91
01/14/2022 16:25:58 - INFO - __main__ -   Batch number = 92
01/14/2022 16:25:59 - INFO - __main__ -   Batch number = 93
01/14/2022 16:25:59 - INFO - __main__ -   Batch number = 94
01/14/2022 16:25:59 - INFO - __main__ -   Batch number = 95
01/14/2022 16:26:00 - INFO - __main__ -   Batch number = 96
01/14/2022 16:26:00 - INFO - __main__ -   Batch number = 97
01/14/2022 16:26:00 - INFO - __main__ -   Batch number = 98
01/14/2022 16:26:00 - INFO - __main__ -   Batch number = 99
01/14/2022 16:26:01 - INFO - __main__ -   Batch number = 100
01/14/2022 16:26:01 - INFO - __main__ -   Batch number = 101
01/14/2022 16:26:01 - INFO - __main__ -   Batch number = 102
01/14/2022 16:26:02 - INFO - __main__ -   Batch number = 103
01/14/2022 16:26:02 - INFO - __main__ -   Batch number = 104
01/14/2022 16:26:02 - INFO - __main__ -   Batch number = 105
01/14/2022 16:26:02 - INFO - __main__ -   Batch number = 106
01/14/2022 16:26:03 - INFO - __main__ -   Batch number = 107
01/14/2022 16:26:03 - INFO - __main__ -   Batch number = 108
01/14/2022 16:26:03 - INFO - __main__ -   Using lang2id = None
01/14/2022 16:26:03 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/14/2022 16:26:03 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/
01/14/2022 16:26:03 - INFO - root -   Trying to decide if add adapter
01/14/2022 16:26:03 - INFO - root -   loading task adapter
01/14/2022 16:26:03 - INFO - root -   loading lang adpater en/wiki@ukp,pt/wiki@ukp,id/wiki@ukp,tr/wiki@ukp,cs/wiki@ukp,vi/wiki@ukp,eu/wiki@ukp,fa/wiki@ukp,zh_yue/wiki@ukp,jv/wiki@ukp
01/14/2022 16:26:03 - INFO - __main__ -   Adapter Languages : ['en', 'pt', 'id', 'tr', 'cs', 'vi', 'eu', 'fa', 'zh_yue', 'jv'], Length : 10
01/14/2022 16:26:03 - INFO - __main__ -   Adapter Names ['en/wiki@ukp', 'pt/wiki@ukp', 'id/wiki@ukp', 'tr/wiki@ukp', 'cs/wiki@ukp', 'vi/wiki@ukp', 'eu/wiki@ukp', 'fa/wiki@ukp', 'zh_yue/wiki@ukp', 'jv/wiki@ukp'], Length : 10
01/14/2022 16:26:03 - INFO - __main__ -   Language = en
01/14/2022 16:26:03 - INFO - __main__ -   Adapter Name = en/wiki@ukp
01/14/2022 16:26:03 - INFO - __main__ -   Batch number = 109
01/14/2022 16:26:04 - INFO - __main__ -   Batch number = 110
01/14/2022 16:26:04 - INFO - __main__ -   Batch number = 111
01/14/2022 16:26:04 - INFO - __main__ -   Batch number = 112
01/14/2022 16:26:04 - INFO - __main__ -   Batch number = 113
01/14/2022 16:26:05 - INFO - __main__ -   Language = pt
01/14/2022 16:26:05 - INFO - __main__ -   Adapter Name = pt/wiki@ukp
01/14/2022 16:26:05 - INFO - __main__ -   Batch number = 114
01/14/2022 16:26:05 - INFO - __main__ -   Batch number = 115
01/14/2022 16:26:05 - INFO - __main__ -   Batch number = 116
01/14/2022 16:26:06 - INFO - __main__ -   Batch number = 117
01/14/2022 16:26:06 - INFO - __main__ -   Batch number = 118
01/14/2022 16:26:06 - INFO - __main__ -   Batch number = 119
01/14/2022 16:26:07 - INFO - __main__ -   Batch number = 120
01/14/2022 16:26:07 - INFO - __main__ -   Language = id
01/14/2022 16:26:07 - INFO - __main__ -   Adapter Name = id/wiki@ukp
01/14/2022 16:26:07 - INFO - __main__ -   Batch number = 121
01/14/2022 16:26:07 - INFO - __main__ -   Batch number = 122
01/14/2022 16:26:07 - INFO - __main__ -   Batch number = 123
01/14/2022 16:26:08 - INFO - __main__ -   Batch number = 124
01/14/2022 16:26:08 - INFO - __main__ -   Batch number = 125
01/14/2022 16:26:08 - INFO - __main__ -   Batch number = 126
01/14/2022 16:26:08 - INFO - __main__ -   Language = tr
01/14/2022 16:26:08 - INFO - __main__ -   Adapter Name = tr/wiki@ukp
01/14/2022 16:26:09 - INFO - __main__ -   Batch number = 127
01/14/2022 16:26:09 - INFO - __main__ -   Batch number = 128
01/14/2022 16:26:09 - INFO - __main__ -   Batch number = 129
01/14/2022 16:26:09 - INFO - __main__ -   Batch number = 130
01/14/2022 16:26:10 - INFO - __main__ -   Batch number = 131
01/14/2022 16:26:10 - INFO - __main__ -   Batch number = 132
01/14/2022 16:26:10 - INFO - __main__ -   Language = cs
01/14/2022 16:26:10 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
01/14/2022 16:26:10 - INFO - __main__ -   Batch number = 133
01/14/2022 16:26:11 - INFO - __main__ -   Batch number = 134
01/14/2022 16:26:11 - INFO - __main__ -   Batch number = 135
01/14/2022 16:26:11 - INFO - __main__ -   Batch number = 136
01/14/2022 16:26:12 - INFO - __main__ -   Batch number = 137
01/14/2022 16:26:12 - INFO - __main__ -   Batch number = 138
01/14/2022 16:26:12 - INFO - __main__ -   Language = vi
01/14/2022 16:26:12 - INFO - __main__ -   Adapter Name = vi/wiki@ukp
01/14/2022 16:26:12 - INFO - __main__ -   Batch number = 139
01/14/2022 16:26:12 - INFO - __main__ -   Batch number = 140
01/14/2022 16:26:13 - INFO - __main__ -   Batch number = 141
01/14/2022 16:26:13 - INFO - __main__ -   Batch number = 142
01/14/2022 16:26:13 - INFO - __main__ -   Language = eu
01/14/2022 16:26:13 - INFO - __main__ -   Adapter Name = eu/wiki@ukp
01/14/2022 16:26:13 - INFO - __main__ -   Batch number = 143
01/14/2022 16:26:14 - INFO - __main__ -   Batch number = 144
01/14/2022 16:26:14 - INFO - __main__ -   Batch number = 145
01/14/2022 16:26:14 - INFO - __main__ -   Batch number = 146
01/14/2022 16:26:15 - INFO - __main__ -   Batch number = 147
01/14/2022 16:26:15 - INFO - __main__ -   Batch number = 148
01/14/2022 16:26:15 - INFO - __main__ -   Language = fa
01/14/2022 16:26:15 - INFO - __main__ -   Adapter Name = fa/wiki@ukp
01/14/2022 16:26:15 - INFO - __main__ -   Batch number = 149
01/14/2022 16:26:15 - INFO - __main__ -   Batch number = 150
01/14/2022 16:26:16 - INFO - __main__ -   Batch number = 151
01/14/2022 16:26:16 - INFO - __main__ -   Batch number = 152
01/14/2022 16:26:16 - INFO - __main__ -   Batch number = 153
01/14/2022 16:26:17 - INFO - __main__ -   Language = zh_yue
01/14/2022 16:26:17 - INFO - __main__ -   Adapter Name = zh_yue/wiki@ukp
01/14/2022 16:26:17 - INFO - __main__ -   Batch number = 154
01/14/2022 16:26:17 - INFO - __main__ -   Batch number = 155
01/14/2022 16:26:17 - INFO - __main__ -   Batch number = 156
01/14/2022 16:26:18 - INFO - __main__ -   Batch number = 157
01/14/2022 16:26:18 - INFO - __main__ -   Batch number = 158
01/14/2022 16:26:18 - INFO - __main__ -   Batch number = 159
01/14/2022 16:26:18 - INFO - __main__ -   Batch number = 160
01/14/2022 16:26:19 - INFO - __main__ -   Language = jv
01/14/2022 16:26:19 - INFO - __main__ -   Adapter Name = jv/wiki@ukp
01/14/2022 16:26:19 - INFO - __main__ -   Batch number = 161
01/14/2022 16:26:19 - INFO - __main__ -   Batch number = 162
01/14/2022 16:26:19 - INFO - __main__ -   Batch number = 163
01/14/2022 16:26:20 - INFO - __main__ -   Batch number = 164
01/14/2022 16:26:20 - INFO - __main__ -   Batch number = 165
01/14/2022 16:26:20 - INFO - __main__ -   Batch number = 166
01/14/2022 16:26:21 - INFO - __main__ -   Batch number = 167
01/14/2022 16:26:21 - INFO - __main__ -   Batch number = 168
01/14/2022 16:26:21 - INFO - __main__ -   Batch number = 169
01/14/2022 16:26:21 - INFO - __main__ -   Batch number = 170
01/14/2022 16:26:22 - INFO - __main__ -   Batch number = 171
01/14/2022 16:26:22 - INFO - __main__ -   Batch number = 172
01/14/2022 16:26:22 - INFO - __main__ -   Batch number = 173
01/14/2022 16:26:23 - INFO - __main__ -   Batch number = 174
01/14/2022 16:26:23 - INFO - __main__ -   Batch number = 175
01/14/2022 16:26:23 - INFO - __main__ -   Args Adapter Weight = equal
01/14/2022 16:26:23 - INFO - __main__ -   Adapter Languages = ['en', 'pt', 'id', 'tr', 'cs', 'vi', 'eu', 'fa', 'zh_yue', 'jv']
01/14/2022 16:26:23 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/14/2022 16:26:23 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/14/2022 16:26:23 - INFO - __main__ -   Length of Adapter Weights = 10
01/14/2022 16:26:23 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/cached_test_jv_bert-base-multilingual-cased_128
01/14/2022 16:26:23 - INFO - __main__ -   ***** Running evaluation  in jv *****
01/14/2022 16:26:23 - INFO - __main__ -     Num examples = 100
01/14/2022 16:26:23 - INFO - __main__ -     Batch size = 32
01/14/2022 16:26:23 - INFO - __main__ -   Batch number = 1
01/14/2022 16:26:23 - INFO - __main__ -   Batch number = 176
01/14/2022 16:26:23 - INFO - __main__ -   Batch number = 2
01/14/2022 16:26:24 - INFO - __main__ -   Batch number = 177
01/14/2022 16:26:24 - INFO - __main__ -   Batch number = 3
01/14/2022 16:26:24 - INFO - __main__ -   Batch number = 178
01/14/2022 16:26:24 - INFO - __main__ -   Batch number = 4
01/14/2022 16:26:24 - INFO - __main__ -   ***** Evaluation result  in jv *****
01/14/2022 16:26:24 - INFO - __main__ -     f1 = 0.6538461538461539
01/14/2022 16:26:24 - INFO - __main__ -     loss = 2.799691364169121
01/14/2022 16:26:24 - INFO - __main__ -     precision = 0.5944055944055944
01/14/2022 16:26:24 - INFO - __main__ -     recall = 0.7264957264957265
01/14/2022 16:26:24 - INFO - __main__ -   Batch number = 179
01/14/2022 16:26:24 - INFO - __main__ -   Batch number = 180
01/14/2022 16:26:25 - INFO - __main__ -   Batch number = 181
01/14/2022 16:26:25 - INFO - __main__ -   Batch number = 182
01/14/2022 16:26:25 - INFO - __main__ -   Batch number = 183
01/14/2022 16:26:26 - INFO - __main__ -   Batch number = 184
01/14/2022 16:26:26 - INFO - __main__ -   Batch number = 185
01/14/2022 16:26:26 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='jv', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:26:26 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/14/2022 16:26:26 - INFO - __main__ -   Seed = 3
01/14/2022 16:26:26 - INFO - root -   save model
01/14/2022 16:26:26 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='jv', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:26:26 - INFO - __main__ -   Loading pretrained model and tokenizer
01/14/2022 16:26:26 - INFO - __main__ -   Batch number = 186
01/14/2022 16:26:27 - INFO - __main__ -   Batch number = 187
01/14/2022 16:26:27 - INFO - __main__ -   Batch number = 188
01/14/2022 16:26:27 - INFO - __main__ -   Batch number = 189
01/14/2022 16:26:27 - INFO - __main__ -   Batch number = 190
01/14/2022 16:26:28 - INFO - __main__ -   Batch number = 191
01/14/2022 16:26:28 - INFO - __main__ -   Batch number = 192
01/14/2022 16:26:28 - INFO - __main__ -   Batch number = 193
01/14/2022 16:26:29 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/14/2022 16:26:29 - INFO - __main__ -   Batch number = 194
01/14/2022 16:26:29 - INFO - __main__ -   Batch number = 195
01/14/2022 16:26:29 - INFO - __main__ -   Batch number = 196
01/14/2022 16:26:30 - INFO - __main__ -   Batch number = 197
01/14/2022 16:26:30 - INFO - __main__ -   Batch number = 198
01/14/2022 16:26:30 - INFO - __main__ -   Batch number = 199
01/14/2022 16:26:31 - INFO - __main__ -   Batch number = 200
01/14/2022 16:26:31 - INFO - __main__ -   Batch number = 201
01/14/2022 16:26:31 - INFO - __main__ -   Batch number = 202
01/14/2022 16:26:31 - INFO - __main__ -   Batch number = 203
01/14/2022 16:26:32 - INFO - __main__ -   Batch number = 204
01/14/2022 16:26:32 - INFO - __main__ -   Batch number = 205
01/14/2022 16:26:32 - INFO - __main__ -   Batch number = 206
01/14/2022 16:26:33 - INFO - __main__ -   Batch number = 207
01/14/2022 16:26:33 - INFO - __main__ -   Batch number = 208
01/14/2022 16:26:33 - INFO - __main__ -   Batch number = 209
01/14/2022 16:26:34 - INFO - __main__ -   Batch number = 210
01/14/2022 16:26:34 - INFO - __main__ -   Batch number = 211
01/14/2022 16:26:34 - INFO - __main__ -   Batch number = 212
01/14/2022 16:26:34 - INFO - __main__ -   Batch number = 213
01/14/2022 16:26:35 - INFO - __main__ -   Using lang2id = None
01/14/2022 16:26:35 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/14/2022 16:26:35 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/
01/14/2022 16:26:35 - INFO - root -   Trying to decide if add adapter
01/14/2022 16:26:35 - INFO - root -   loading task adapter
01/14/2022 16:26:35 - INFO - root -   loading lang adpater en/wiki@ukp,pt/wiki@ukp,id/wiki@ukp,tr/wiki@ukp,vi/wiki@ukp,fa/wiki@ukp,eu/wiki@ukp,zh_yue/wiki@ukp,cs/wiki@ukp,jv/wiki@ukp
01/14/2022 16:26:35 - INFO - __main__ -   Adapter Languages : ['en', 'pt', 'id', 'tr', 'vi', 'fa', 'eu', 'zh_yue', 'cs', 'jv'], Length : 10
01/14/2022 16:26:35 - INFO - __main__ -   Adapter Names ['en/wiki@ukp', 'pt/wiki@ukp', 'id/wiki@ukp', 'tr/wiki@ukp', 'vi/wiki@ukp', 'fa/wiki@ukp', 'eu/wiki@ukp', 'zh_yue/wiki@ukp', 'cs/wiki@ukp', 'jv/wiki@ukp'], Length : 10
01/14/2022 16:26:35 - INFO - __main__ -   Language = en
01/14/2022 16:26:35 - INFO - __main__ -   Adapter Name = en/wiki@ukp
01/14/2022 16:26:35 - INFO - __main__ -   Batch number = 214
01/14/2022 16:26:35 - INFO - __main__ -   Batch number = 215
01/14/2022 16:26:35 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='sw', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:26:35 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/14/2022 16:26:35 - INFO - __main__ -   Seed = 1
01/14/2022 16:26:35 - INFO - root -   save model
01/14/2022 16:26:35 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='sw', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:26:35 - INFO - __main__ -   Loading pretrained model and tokenizer
01/14/2022 16:26:35 - INFO - __main__ -   Batch number = 216
01/14/2022 16:26:35 - INFO - __main__ -   Language = pt
01/14/2022 16:26:35 - INFO - __main__ -   Adapter Name = pt/wiki@ukp
01/14/2022 16:26:36 - INFO - __main__ -   Batch number = 217
01/14/2022 16:26:36 - INFO - __main__ -   Batch number = 218
01/14/2022 16:26:36 - INFO - __main__ -   Batch number = 219
01/14/2022 16:26:37 - INFO - __main__ -   Batch number = 220
01/14/2022 16:26:37 - INFO - __main__ -   Batch number = 221
01/14/2022 16:26:37 - INFO - __main__ -   Batch number = 222
01/14/2022 16:26:37 - INFO - __main__ -   Batch number = 223
01/14/2022 16:26:38 - INFO - __main__ -   Language = id
01/14/2022 16:26:38 - INFO - __main__ -   Adapter Name = id/wiki@ukp
01/14/2022 16:26:38 - INFO - __main__ -   Batch number = 224
01/14/2022 16:26:38 - INFO - __main__ -   Batch number = 225
01/14/2022 16:26:38 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/14/2022 16:26:38 - INFO - __main__ -   Batch number = 226
01/14/2022 16:26:39 - INFO - __main__ -   Batch number = 227
01/14/2022 16:26:39 - INFO - __main__ -   Batch number = 228
01/14/2022 16:26:39 - INFO - __main__ -   Batch number = 229
01/14/2022 16:26:40 - INFO - __main__ -   Batch number = 230
01/14/2022 16:26:40 - INFO - __main__ -   Language = tr
01/14/2022 16:26:40 - INFO - __main__ -   Adapter Name = tr/wiki@ukp
01/14/2022 16:26:40 - INFO - __main__ -   Batch number = 231
01/14/2022 16:26:40 - INFO - __main__ -   Batch number = 232
01/14/2022 16:26:40 - INFO - __main__ -   Batch number = 233
01/14/2022 16:26:41 - INFO - __main__ -   Batch number = 234
01/14/2022 16:26:41 - INFO - __main__ -   Batch number = 235
01/14/2022 16:26:41 - INFO - __main__ -   Batch number = 236
01/14/2022 16:26:42 - INFO - __main__ -   Batch number = 237
01/14/2022 16:26:42 - INFO - __main__ -   Language = vi
01/14/2022 16:26:42 - INFO - __main__ -   Adapter Name = vi/wiki@ukp
01/14/2022 16:26:42 - INFO - __main__ -   Batch number = 238
01/14/2022 16:26:42 - INFO - __main__ -   Batch number = 239
01/14/2022 16:26:42 - INFO - __main__ -   Batch number = 240
01/14/2022 16:26:43 - INFO - __main__ -   Batch number = 241
01/14/2022 16:26:43 - INFO - __main__ -   Batch number = 242
01/14/2022 16:26:43 - INFO - __main__ -   Batch number = 243
01/14/2022 16:26:44 - INFO - __main__ -   Batch number = 244
01/14/2022 16:26:44 - INFO - __main__ -   Using lang2id = None
01/14/2022 16:26:44 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/14/2022 16:26:44 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/
01/14/2022 16:26:44 - INFO - root -   Trying to decide if add adapter
01/14/2022 16:26:44 - INFO - root -   loading task adapter
01/14/2022 16:26:44 - INFO - root -   loading lang adpater en/wiki@ukp,pt/wiki@ukp,id/wiki@ukp,cs/wiki@ukp,tr/wiki@ukp,eu/wiki@ukp,zh_yue/wiki@ukp,vi/wiki@ukp,fr/wiki@ukp,sw/wiki@ukp
01/14/2022 16:26:44 - INFO - __main__ -   Adapter Languages : ['en', 'pt', 'id', 'cs', 'tr', 'eu', 'zh_yue', 'vi', 'fr', 'sw'], Length : 10
01/14/2022 16:26:44 - INFO - __main__ -   Adapter Names ['en/wiki@ukp', 'pt/wiki@ukp', 'id/wiki@ukp', 'cs/wiki@ukp', 'tr/wiki@ukp', 'eu/wiki@ukp', 'zh_yue/wiki@ukp', 'vi/wiki@ukp', 'fr/wiki@ukp', 'sw/wiki@ukp'], Length : 10
01/14/2022 16:26:44 - INFO - __main__ -   Language = en
01/14/2022 16:26:44 - INFO - __main__ -   Adapter Name = en/wiki@ukp
01/14/2022 16:26:44 - INFO - __main__ -   Batch number = 245
01/14/2022 16:26:44 - INFO - __main__ -   Language = fa
01/14/2022 16:26:44 - INFO - __main__ -   Adapter Name = fa/wiki@ukp
01/14/2022 16:26:44 - INFO - __main__ -   Batch number = 246
01/14/2022 16:26:45 - INFO - __main__ -   Batch number = 247
01/14/2022 16:26:45 - INFO - __main__ -   Language = pt
01/14/2022 16:26:45 - INFO - __main__ -   Adapter Name = pt/wiki@ukp
01/14/2022 16:26:45 - INFO - __main__ -   Batch number = 248
01/14/2022 16:26:45 - INFO - __main__ -   Batch number = 249
01/14/2022 16:26:45 - INFO - __main__ -   Batch number = 250
01/14/2022 16:26:46 - INFO - __main__ -   Batch number = 251
01/14/2022 16:26:46 - INFO - __main__ -   Batch number = 252
01/14/2022 16:26:46 - INFO - __main__ -   Language = eu
01/14/2022 16:26:46 - INFO - __main__ -   Adapter Name = eu/wiki@ukp
01/14/2022 16:26:46 - INFO - __main__ -   Batch number = 253
01/14/2022 16:26:47 - INFO - __main__ -   Batch number = 254
01/14/2022 16:26:47 - INFO - __main__ -   Language = id
01/14/2022 16:26:47 - INFO - __main__ -   Adapter Name = id/wiki@ukp
01/14/2022 16:26:47 - INFO - __main__ -   Batch number = 255
01/14/2022 16:26:47 - INFO - __main__ -   Batch number = 256
01/14/2022 16:26:48 - INFO - __main__ -   Batch number = 257
01/14/2022 16:26:48 - INFO - __main__ -   Batch number = 258
01/14/2022 16:26:48 - INFO - __main__ -   Batch number = 259
01/14/2022 16:26:48 - INFO - __main__ -   Batch number = 260
01/14/2022 16:26:49 - INFO - __main__ -   Language = zh_yue
01/14/2022 16:26:49 - INFO - __main__ -   Adapter Name = zh_yue/wiki@ukp
01/14/2022 16:26:49 - INFO - __main__ -   Batch number = 261
01/14/2022 16:26:49 - INFO - __main__ -   Language = cs
01/14/2022 16:26:49 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
01/14/2022 16:26:49 - INFO - __main__ -   Batch number = 262
01/14/2022 16:26:49 - INFO - __main__ -   Batch number = 263
01/14/2022 16:26:50 - INFO - __main__ -   Batch number = 264
01/14/2022 16:26:50 - INFO - __main__ -   Batch number = 265
01/14/2022 16:26:50 - INFO - __main__ -   Batch number = 266
01/14/2022 16:26:50 - INFO - __main__ -   Batch number = 267
01/14/2022 16:26:51 - INFO - __main__ -   Batch number = 268
01/14/2022 16:26:51 - INFO - __main__ -   Language = cs
01/14/2022 16:26:51 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
01/14/2022 16:26:51 - INFO - __main__ -   Batch number = 269
01/14/2022 16:26:51 - INFO - __main__ -   Language = tr
01/14/2022 16:26:51 - INFO - __main__ -   Adapter Name = tr/wiki@ukp
01/14/2022 16:26:51 - INFO - __main__ -   Batch number = 270
01/14/2022 16:26:52 - INFO - __main__ -   Batch number = 271
01/14/2022 16:26:52 - INFO - __main__ -   Batch number = 272
01/14/2022 16:26:52 - INFO - __main__ -   Batch number = 273
01/14/2022 16:26:53 - INFO - __main__ -   Batch number = 274
01/14/2022 16:26:53 - INFO - __main__ -   Language = jv
01/14/2022 16:26:53 - INFO - __main__ -   Adapter Name = jv/wiki@ukp
01/14/2022 16:26:53 - INFO - __main__ -   Batch number = 275
01/14/2022 16:26:53 - INFO - __main__ -   Batch number = 276
01/14/2022 16:26:53 - INFO - __main__ -   Language = eu
01/14/2022 16:26:53 - INFO - __main__ -   Adapter Name = eu/wiki@ukp
01/14/2022 16:26:53 - INFO - __main__ -   Batch number = 277
01/14/2022 16:26:54 - INFO - __main__ -   Batch number = 278
01/14/2022 16:26:54 - INFO - __main__ -   Batch number = 279
01/14/2022 16:26:54 - INFO - __main__ -   Batch number = 280
01/14/2022 16:26:55 - INFO - __main__ -   Batch number = 281
01/14/2022 16:26:55 - INFO - __main__ -   Batch number = 282
01/14/2022 16:26:55 - INFO - __main__ -   Batch number = 283
01/14/2022 16:26:55 - INFO - __main__ -   Language = zh_yue
01/14/2022 16:26:55 - INFO - __main__ -   Adapter Name = zh_yue/wiki@ukp
01/14/2022 16:26:55 - INFO - __main__ -   Batch number = 284
01/14/2022 16:26:56 - INFO - __main__ -   Batch number = 285
01/14/2022 16:26:56 - INFO - __main__ -   Batch number = 286
01/14/2022 16:26:56 - INFO - __main__ -   Batch number = 287
01/14/2022 16:26:57 - INFO - __main__ -   Batch number = 288
01/14/2022 16:26:57 - INFO - __main__ -   Batch number = 289
01/14/2022 16:26:57 - INFO - __main__ -   Batch number = 290
01/14/2022 16:26:57 - INFO - __main__ -   Args Adapter Weight = equal
01/14/2022 16:26:57 - INFO - __main__ -   Adapter Languages = ['en', 'pt', 'id', 'tr', 'vi', 'fa', 'eu', 'zh_yue', 'cs', 'jv']
01/14/2022 16:26:57 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/14/2022 16:26:57 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/14/2022 16:26:57 - INFO - __main__ -   Length of Adapter Weights = 10
01/14/2022 16:26:57 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/cached_test_jv_bert-base-multilingual-cased_128
01/14/2022 16:26:58 - INFO - __main__ -   ***** Running evaluation  in jv *****
01/14/2022 16:26:58 - INFO - __main__ -     Num examples = 100
01/14/2022 16:26:58 - INFO - __main__ -     Batch size = 32
01/14/2022 16:26:58 - INFO - __main__ -   Batch number = 291
01/14/2022 16:26:58 - INFO - __main__ -   Batch number = 1
01/14/2022 16:26:58 - INFO - __main__ -   Language = vi
01/14/2022 16:26:58 - INFO - __main__ -   Adapter Name = vi/wiki@ukp
01/14/2022 16:26:58 - INFO - __main__ -   Batch number = 292
01/14/2022 16:26:58 - INFO - __main__ -   Batch number = 2
01/14/2022 16:26:58 - INFO - __main__ -   Batch number = 293
01/14/2022 16:26:58 - INFO - __main__ -   Batch number = 3
01/14/2022 16:26:58 - INFO - __main__ -   Batch number = 4
01/14/2022 16:26:58 - INFO - __main__ -   Batch number = 294
01/14/2022 16:26:58 - INFO - __main__ -   ***** Evaluation result  in jv *****
01/14/2022 16:26:58 - INFO - __main__ -     f1 = 0.5555555555555556
01/14/2022 16:26:58 - INFO - __main__ -     loss = 3.3965357840061188
01/14/2022 16:26:58 - INFO - __main__ -     precision = 0.49019607843137253
01/14/2022 16:26:58 - INFO - __main__ -     recall = 0.6410256410256411
01/14/2022 16:26:59 - INFO - __main__ -   Batch number = 295
01/14/2022 16:26:59 - INFO - __main__ -   Batch number = 296
01/14/2022 16:26:59 - INFO - __main__ -   Batch number = 297
01/14/2022 16:27:00 - INFO - __main__ -   Batch number = 298
01/14/2022 16:27:00 - INFO - __main__ -   Language = fr
01/14/2022 16:27:00 - INFO - __main__ -   Adapter Name = fr/wiki@ukp
01/14/2022 16:27:00 - INFO - __main__ -   Batch number = 299
01/14/2022 16:27:00 - INFO - __main__ -   Batch number = 300
01/14/2022 16:27:01 - INFO - __main__ -   Batch number = 301
01/14/2022 16:27:01 - INFO - __main__ -   Language = sw
01/14/2022 16:27:01 - INFO - __main__ -   Adapter Name = sw/wiki@ukp
01/14/2022 16:27:01 - INFO - __main__ -   Batch number = 302
01/14/2022 16:27:01 - INFO - __main__ -   Batch number = 303
01/14/2022 16:27:01 - INFO - __main__ -   Batch number = 304
01/14/2022 16:27:02 - INFO - __main__ -   Batch number = 305
01/14/2022 16:27:02 - INFO - __main__ -   Batch number = 306
01/14/2022 16:27:02 - INFO - __main__ -   Batch number = 307
01/14/2022 16:27:03 - INFO - __main__ -   Batch number = 308
01/14/2022 16:27:03 - INFO - __main__ -   Batch number = 309
01/14/2022 16:27:03 - INFO - __main__ -   Batch number = 310
01/14/2022 16:27:04 - INFO - __main__ -   Batch number = 311
01/14/2022 16:27:04 - INFO - __main__ -   Batch number = 312
01/14/2022 16:27:04 - INFO - __main__ -   Batch number = 313
01/14/2022 16:27:06 - INFO - __main__ -   ***** Evaluation result  in ar *****
01/14/2022 16:27:06 - INFO - __main__ -     f1 = 0.36017038505335075
01/14/2022 16:27:06 - INFO - __main__ -     loss = 5.191140273889413
01/14/2022 16:27:06 - INFO - __main__ -     precision = 0.34291680051397366
01/14/2022 16:27:06 - INFO - __main__ -     recall = 0.37925215383248956
01/14/2022 16:27:06 - INFO - __main__ -   Args Adapter Weight = equal
01/14/2022 16:27:06 - INFO - __main__ -   Adapter Languages = ['en', 'pt', 'id', 'cs', 'tr', 'eu', 'zh_yue', 'vi', 'fr', 'sw']
01/14/2022 16:27:06 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/14/2022 16:27:06 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/14/2022 16:27:06 - INFO - __main__ -   Length of Adapter Weights = 10
01/14/2022 16:27:06 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/cached_test_sw_bert-base-multilingual-cased_128
01/14/2022 16:27:06 - INFO - __main__ -   ***** Running evaluation  in sw *****
01/14/2022 16:27:06 - INFO - __main__ -     Num examples = 1000
01/14/2022 16:27:06 - INFO - __main__ -     Batch size = 32
01/14/2022 16:27:06 - INFO - __main__ -   Batch number = 1
01/14/2022 16:27:07 - INFO - __main__ -   Batch number = 2
01/14/2022 16:27:07 - INFO - __main__ -   Batch number = 3
01/14/2022 16:27:07 - INFO - __main__ -   Batch number = 4
01/14/2022 16:27:07 - INFO - __main__ -   Batch number = 5
01/14/2022 16:27:08 - INFO - __main__ -   Batch number = 6
01/14/2022 16:27:08 - INFO - __main__ -   Batch number = 7
01/14/2022 16:27:08 - INFO - __main__ -   Batch number = 8
01/14/2022 16:27:08 - INFO - __main__ -   Batch number = 9
01/14/2022 16:27:09 - INFO - __main__ -   Batch number = 10
01/14/2022 16:27:09 - INFO - __main__ -   Batch number = 11
01/14/2022 16:27:09 - INFO - __main__ -   Batch number = 12
01/14/2022 16:27:10 - INFO - __main__ -   Batch number = 13
01/14/2022 16:27:10 - INFO - __main__ -   Batch number = 14
01/14/2022 16:27:10 - INFO - __main__ -   Batch number = 15
01/14/2022 16:27:10 - INFO - __main__ -   Batch number = 16
01/14/2022 16:27:11 - INFO - __main__ -   Batch number = 17
01/14/2022 16:27:11 - INFO - __main__ -   Batch number = 18
01/14/2022 16:27:11 - INFO - __main__ -   Batch number = 19
01/14/2022 16:27:12 - INFO - __main__ -   Batch number = 20
01/14/2022 16:27:12 - INFO - __main__ -   Batch number = 21
01/14/2022 16:27:12 - INFO - __main__ -   Batch number = 22
01/14/2022 16:27:12 - INFO - __main__ -   Batch number = 23
01/14/2022 16:27:13 - INFO - __main__ -   Batch number = 24
01/14/2022 16:27:13 - INFO - __main__ -   Batch number = 25
01/14/2022 16:27:13 - INFO - __main__ -   Batch number = 26
01/14/2022 16:27:14 - INFO - __main__ -   Batch number = 27
01/14/2022 16:27:14 - INFO - __main__ -   Batch number = 28
01/14/2022 16:27:14 - INFO - __main__ -   Batch number = 29
01/14/2022 16:27:14 - INFO - __main__ -   Batch number = 30
01/14/2022 16:27:15 - INFO - __main__ -   Batch number = 31
01/14/2022 16:27:15 - INFO - __main__ -   Batch number = 32
01/14/2022 16:27:15 - INFO - __main__ -   ***** Evaluation result  in sw *****
01/14/2022 16:27:15 - INFO - __main__ -     f1 = 0.6042823156225218
01/14/2022 16:27:15 - INFO - __main__ -     loss = 3.31079875677824
01/14/2022 16:27:15 - INFO - __main__ -     precision = 0.5737951807228916
01/14/2022 16:27:15 - INFO - __main__ -     recall = 0.6381909547738693
01/14/2022 16:27:17 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='sw', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:27:17 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/14/2022 16:27:17 - INFO - __main__ -   Seed = 2
01/14/2022 16:27:17 - INFO - root -   save model
01/14/2022 16:27:17 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='sw', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:27:17 - INFO - __main__ -   Loading pretrained model and tokenizer
01/14/2022 16:27:20 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/14/2022 16:27:26 - INFO - __main__ -   Using lang2id = None
01/14/2022 16:27:26 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/14/2022 16:27:26 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/
01/14/2022 16:27:26 - INFO - root -   Trying to decide if add adapter
01/14/2022 16:27:26 - INFO - root -   loading task adapter
01/14/2022 16:27:26 - INFO - root -   loading lang adpater en/wiki@ukp,pt/wiki@ukp,id/wiki@ukp,tr/wiki@ukp,cs/wiki@ukp,vi/wiki@ukp,eu/wiki@ukp,fa/wiki@ukp,zh_yue/wiki@ukp,sw/wiki@ukp
01/14/2022 16:27:26 - INFO - __main__ -   Adapter Languages : ['en', 'pt', 'id', 'tr', 'cs', 'vi', 'eu', 'fa', 'zh_yue', 'sw'], Length : 10
01/14/2022 16:27:26 - INFO - __main__ -   Adapter Names ['en/wiki@ukp', 'pt/wiki@ukp', 'id/wiki@ukp', 'tr/wiki@ukp', 'cs/wiki@ukp', 'vi/wiki@ukp', 'eu/wiki@ukp', 'fa/wiki@ukp', 'zh_yue/wiki@ukp', 'sw/wiki@ukp'], Length : 10
01/14/2022 16:27:26 - INFO - __main__ -   Language = en
01/14/2022 16:27:26 - INFO - __main__ -   Adapter Name = en/wiki@ukp
01/14/2022 16:27:27 - INFO - __main__ -   Language = pt
01/14/2022 16:27:27 - INFO - __main__ -   Adapter Name = pt/wiki@ukp
01/14/2022 16:27:28 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='is', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:27:28 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/14/2022 16:27:28 - INFO - __main__ -   Seed = 1
01/14/2022 16:27:28 - INFO - root -   save model
01/14/2022 16:27:28 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='is', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:27:28 - INFO - __main__ -   Loading pretrained model and tokenizer
01/14/2022 16:27:29 - INFO - __main__ -   Language = id
01/14/2022 16:27:29 - INFO - __main__ -   Adapter Name = id/wiki@ukp
01/14/2022 16:27:31 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/14/2022 16:27:31 - INFO - __main__ -   Language = tr
01/14/2022 16:27:31 - INFO - __main__ -   Adapter Name = tr/wiki@ukp
01/14/2022 16:27:33 - INFO - __main__ -   Language = cs
01/14/2022 16:27:33 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
01/14/2022 16:27:36 - INFO - __main__ -   Language = vi
01/14/2022 16:27:36 - INFO - __main__ -   Adapter Name = vi/wiki@ukp
01/14/2022 16:27:36 - INFO - __main__ -   Using lang2id = None
01/14/2022 16:27:36 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/14/2022 16:27:36 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/
01/14/2022 16:27:36 - INFO - root -   Trying to decide if add adapter
01/14/2022 16:27:36 - INFO - root -   loading task adapter
01/14/2022 16:27:37 - INFO - root -   loading lang adpater en/wiki@ukp,pt/wiki@ukp,id/wiki@ukp,cs/wiki@ukp,tr/wiki@ukp,eu/wiki@ukp,zh_yue/wiki@ukp,vi/wiki@ukp,fr/wiki@ukp,is/wiki@ukp
01/14/2022 16:27:37 - INFO - __main__ -   Adapter Languages : ['en', 'pt', 'id', 'cs', 'tr', 'eu', 'zh_yue', 'vi', 'fr', 'is'], Length : 10
01/14/2022 16:27:37 - INFO - __main__ -   Adapter Names ['en/wiki@ukp', 'pt/wiki@ukp', 'id/wiki@ukp', 'cs/wiki@ukp', 'tr/wiki@ukp', 'eu/wiki@ukp', 'zh_yue/wiki@ukp', 'vi/wiki@ukp', 'fr/wiki@ukp', 'is/wiki@ukp'], Length : 10
01/14/2022 16:27:37 - INFO - __main__ -   Language = en
01/14/2022 16:27:37 - INFO - __main__ -   Adapter Name = en/wiki@ukp
01/14/2022 16:27:37 - INFO - __main__ -   Language = pt
01/14/2022 16:27:37 - INFO - __main__ -   Adapter Name = pt/wiki@ukp
01/14/2022 16:27:38 - INFO - __main__ -   Language = eu
01/14/2022 16:27:38 - INFO - __main__ -   Adapter Name = eu/wiki@ukp
01/14/2022 16:27:39 - INFO - __main__ -   Language = id
01/14/2022 16:27:39 - INFO - __main__ -   Adapter Name = id/wiki@ukp
01/14/2022 16:27:40 - INFO - __main__ -   Language = fa
01/14/2022 16:27:40 - INFO - __main__ -   Adapter Name = fa/wiki@ukp
01/14/2022 16:27:42 - INFO - __main__ -   Language = cs
01/14/2022 16:27:42 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
01/14/2022 16:27:42 - INFO - __main__ -   Language = zh_yue
01/14/2022 16:27:42 - INFO - __main__ -   Adapter Name = zh_yue/wiki@ukp
01/14/2022 16:27:44 - INFO - __main__ -   Language = tr
01/14/2022 16:27:44 - INFO - __main__ -   Adapter Name = tr/wiki@ukp
01/14/2022 16:27:44 - INFO - __main__ -   Language = sw
01/14/2022 16:27:44 - INFO - __main__ -   Adapter Name = sw/wiki@ukp
01/14/2022 16:27:46 - INFO - __main__ -   Language = eu
01/14/2022 16:27:46 - INFO - __main__ -   Adapter Name = eu/wiki@ukp
01/14/2022 16:27:48 - INFO - __main__ -   Language = zh_yue
01/14/2022 16:27:48 - INFO - __main__ -   Adapter Name = zh_yue/wiki@ukp
01/14/2022 16:27:49 - INFO - __main__ -   Args Adapter Weight = equal
01/14/2022 16:27:49 - INFO - __main__ -   Adapter Languages = ['en', 'pt', 'id', 'tr', 'cs', 'vi', 'eu', 'fa', 'zh_yue', 'sw']
01/14/2022 16:27:49 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/14/2022 16:27:49 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/14/2022 16:27:49 - INFO - __main__ -   Length of Adapter Weights = 10
01/14/2022 16:27:49 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/cached_test_sw_bert-base-multilingual-cased_128
01/14/2022 16:27:49 - INFO - __main__ -   ***** Running evaluation  in sw *****
01/14/2022 16:27:49 - INFO - __main__ -     Num examples = 1000
01/14/2022 16:27:49 - INFO - __main__ -     Batch size = 32
01/14/2022 16:27:49 - INFO - __main__ -   Batch number = 1
01/14/2022 16:27:49 - INFO - __main__ -   Batch number = 2
01/14/2022 16:27:49 - INFO - __main__ -   Batch number = 3
01/14/2022 16:27:49 - INFO - __main__ -   Batch number = 4
01/14/2022 16:27:50 - INFO - __main__ -   Batch number = 5
01/14/2022 16:27:50 - INFO - __main__ -   Batch number = 6
01/14/2022 16:27:50 - INFO - __main__ -   Batch number = 7
01/14/2022 16:27:51 - INFO - __main__ -   Language = vi
01/14/2022 16:27:51 - INFO - __main__ -   Adapter Name = vi/wiki@ukp
01/14/2022 16:27:51 - INFO - __main__ -   Batch number = 8
01/14/2022 16:27:51 - INFO - __main__ -   Batch number = 9
01/14/2022 16:27:51 - INFO - __main__ -   Batch number = 10
01/14/2022 16:27:51 - INFO - __main__ -   Batch number = 11
01/14/2022 16:27:52 - INFO - __main__ -   Batch number = 12
01/14/2022 16:27:52 - INFO - __main__ -   Batch number = 13
01/14/2022 16:27:52 - INFO - __main__ -   Batch number = 14
01/14/2022 16:27:53 - INFO - __main__ -   Batch number = 15
01/14/2022 16:27:53 - INFO - __main__ -   Language = fr
01/14/2022 16:27:53 - INFO - __main__ -   Adapter Name = fr/wiki@ukp
01/14/2022 16:27:53 - INFO - __main__ -   Batch number = 16
01/14/2022 16:27:53 - INFO - __main__ -   Batch number = 17
01/14/2022 16:27:54 - INFO - __main__ -   Batch number = 18
01/14/2022 16:27:54 - INFO - __main__ -   Language = is
01/14/2022 16:27:54 - INFO - __main__ -   Adapter Name = is/wiki@ukp
01/14/2022 16:27:54 - INFO - __main__ -   Batch number = 19
01/14/2022 16:27:54 - INFO - __main__ -   Batch number = 20
01/14/2022 16:27:54 - INFO - __main__ -   Batch number = 21
01/14/2022 16:27:55 - INFO - __main__ -   Batch number = 22
01/14/2022 16:27:55 - INFO - __main__ -   Batch number = 23
01/14/2022 16:27:55 - INFO - __main__ -   Batch number = 24
01/14/2022 16:27:56 - INFO - __main__ -   Batch number = 25
01/14/2022 16:27:56 - INFO - __main__ -   Batch number = 26
01/14/2022 16:27:56 - INFO - __main__ -   Batch number = 27
01/14/2022 16:27:56 - INFO - __main__ -   Batch number = 28
01/14/2022 16:27:57 - INFO - __main__ -   Batch number = 29
01/14/2022 16:27:57 - INFO - __main__ -   Batch number = 30
01/14/2022 16:27:57 - INFO - __main__ -   Batch number = 31
01/14/2022 16:27:58 - INFO - __main__ -   Batch number = 32
01/14/2022 16:27:58 - INFO - __main__ -   ***** Evaluation result  in sw *****
01/14/2022 16:27:58 - INFO - __main__ -     f1 = 0.6304769290422645
01/14/2022 16:27:58 - INFO - __main__ -     loss = 2.5837175473570824
01/14/2022 16:27:58 - INFO - __main__ -     precision = 0.5870036101083033
01/14/2022 16:27:58 - INFO - __main__ -     recall = 0.6809045226130653
01/14/2022 16:27:58 - INFO - __main__ -   Args Adapter Weight = equal
01/14/2022 16:27:58 - INFO - __main__ -   Adapter Languages = ['en', 'pt', 'id', 'cs', 'tr', 'eu', 'zh_yue', 'vi', 'fr', 'is']
01/14/2022 16:27:58 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/14/2022 16:27:58 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/14/2022 16:27:58 - INFO - __main__ -   Length of Adapter Weights = 10
01/14/2022 16:27:58 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/cached_test_is_bert-base-multilingual-cased_128
01/14/2022 16:27:59 - INFO - __main__ -   ***** Running evaluation  in is *****
01/14/2022 16:27:59 - INFO - __main__ -     Num examples = 1000
01/14/2022 16:27:59 - INFO - __main__ -     Batch size = 32
01/14/2022 16:27:59 - INFO - __main__ -   Batch number = 1
01/14/2022 16:27:59 - INFO - __main__ -   Batch number = 2
01/14/2022 16:27:59 - INFO - __main__ -   Batch number = 3
01/14/2022 16:27:59 - INFO - __main__ -   Batch number = 4
01/14/2022 16:28:00 - INFO - __main__ -   Batch number = 5
01/14/2022 16:28:00 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='sw', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:28:00 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/14/2022 16:28:00 - INFO - __main__ -   Seed = 3
01/14/2022 16:28:00 - INFO - root -   save model
01/14/2022 16:28:00 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='sw', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:28:00 - INFO - __main__ -   Loading pretrained model and tokenizer
01/14/2022 16:28:00 - INFO - __main__ -   Batch number = 6
01/14/2022 16:28:00 - INFO - __main__ -   Batch number = 7
01/14/2022 16:28:01 - INFO - __main__ -   Batch number = 8
01/14/2022 16:28:01 - INFO - __main__ -   Batch number = 9
01/14/2022 16:28:01 - INFO - __main__ -   Batch number = 10
01/14/2022 16:28:01 - INFO - __main__ -   Batch number = 11
01/14/2022 16:28:02 - INFO - __main__ -   Batch number = 12
01/14/2022 16:28:02 - INFO - __main__ -   Batch number = 13
01/14/2022 16:28:02 - INFO - __main__ -   Batch number = 14
01/14/2022 16:28:03 - INFO - __main__ -   Batch number = 15
01/14/2022 16:28:03 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/14/2022 16:28:03 - INFO - __main__ -   Batch number = 16
01/14/2022 16:28:03 - INFO - __main__ -   Batch number = 17
01/14/2022 16:28:03 - INFO - __main__ -   Batch number = 18
01/14/2022 16:28:04 - INFO - __main__ -   Batch number = 19
01/14/2022 16:28:04 - INFO - __main__ -   Batch number = 20
01/14/2022 16:28:04 - INFO - __main__ -   Batch number = 21
01/14/2022 16:28:04 - INFO - __main__ -   Batch number = 22
01/14/2022 16:28:05 - INFO - __main__ -   Batch number = 23
01/14/2022 16:28:05 - INFO - __main__ -   Batch number = 24
01/14/2022 16:28:05 - INFO - __main__ -   Batch number = 25
01/14/2022 16:28:06 - INFO - __main__ -   Batch number = 26
01/14/2022 16:28:06 - INFO - __main__ -   Batch number = 27
01/14/2022 16:28:06 - INFO - __main__ -   Batch number = 28
01/14/2022 16:28:07 - INFO - __main__ -   Batch number = 29
01/14/2022 16:28:07 - INFO - __main__ -   Batch number = 30
01/14/2022 16:28:07 - INFO - __main__ -   Batch number = 31
01/14/2022 16:28:07 - INFO - __main__ -   Batch number = 32
01/14/2022 16:28:08 - INFO - __main__ -   ***** Evaluation result  in is *****
01/14/2022 16:28:08 - INFO - __main__ -     f1 = 0.6669181440965674
01/14/2022 16:28:08 - INFO - __main__ -     loss = 1.2107287272810936
01/14/2022 16:28:08 - INFO - __main__ -     precision = 0.6199158485273493
01/14/2022 16:28:08 - INFO - __main__ -     recall = 0.7216326530612245
01/14/2022 16:28:09 - INFO - __main__ -   Using lang2id = None
01/14/2022 16:28:09 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/14/2022 16:28:09 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/
01/14/2022 16:28:09 - INFO - root -   Trying to decide if add adapter
01/14/2022 16:28:09 - INFO - root -   loading task adapter
01/14/2022 16:28:09 - INFO - root -   loading lang adpater en/wiki@ukp,pt/wiki@ukp,id/wiki@ukp,tr/wiki@ukp,vi/wiki@ukp,fa/wiki@ukp,eu/wiki@ukp,zh_yue/wiki@ukp,cs/wiki@ukp,sw/wiki@ukp
01/14/2022 16:28:09 - INFO - __main__ -   Adapter Languages : ['en', 'pt', 'id', 'tr', 'vi', 'fa', 'eu', 'zh_yue', 'cs', 'sw'], Length : 10
01/14/2022 16:28:09 - INFO - __main__ -   Adapter Names ['en/wiki@ukp', 'pt/wiki@ukp', 'id/wiki@ukp', 'tr/wiki@ukp', 'vi/wiki@ukp', 'fa/wiki@ukp', 'eu/wiki@ukp', 'zh_yue/wiki@ukp', 'cs/wiki@ukp', 'sw/wiki@ukp'], Length : 10
01/14/2022 16:28:09 - INFO - __main__ -   Language = en
01/14/2022 16:28:09 - INFO - __main__ -   Adapter Name = en/wiki@ukp
01/14/2022 16:28:10 - INFO - __main__ -   Language = pt
01/14/2022 16:28:10 - INFO - __main__ -   Adapter Name = pt/wiki@ukp
01/14/2022 16:28:10 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='is', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:28:10 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/14/2022 16:28:10 - INFO - __main__ -   Seed = 2
01/14/2022 16:28:10 - INFO - root -   save model
01/14/2022 16:28:10 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='is', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:28:10 - INFO - __main__ -   Loading pretrained model and tokenizer
01/14/2022 16:28:11 - INFO - __main__ -   Language = id
01/14/2022 16:28:11 - INFO - __main__ -   Adapter Name = id/wiki@ukp
01/14/2022 16:28:13 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/14/2022 16:28:13 - INFO - __main__ -   Language = tr
01/14/2022 16:28:13 - INFO - __main__ -   Adapter Name = tr/wiki@ukp
01/14/2022 16:28:14 - INFO - __main__ -   Language = vi
01/14/2022 16:28:14 - INFO - __main__ -   Adapter Name = vi/wiki@ukp
01/14/2022 16:28:16 - INFO - __main__ -   Language = fa
01/14/2022 16:28:16 - INFO - __main__ -   Adapter Name = fa/wiki@ukp
01/14/2022 16:28:18 - INFO - __main__ -   Language = eu
01/14/2022 16:28:18 - INFO - __main__ -   Adapter Name = eu/wiki@ukp
01/14/2022 16:28:18 - INFO - __main__ -   Using lang2id = None
01/14/2022 16:28:18 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/14/2022 16:28:18 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/
01/14/2022 16:28:18 - INFO - root -   Trying to decide if add adapter
01/14/2022 16:28:18 - INFO - root -   loading task adapter
01/14/2022 16:28:18 - INFO - root -   loading lang adpater en/wiki@ukp,pt/wiki@ukp,id/wiki@ukp,tr/wiki@ukp,cs/wiki@ukp,vi/wiki@ukp,eu/wiki@ukp,fa/wiki@ukp,zh_yue/wiki@ukp,is/wiki@ukp
01/14/2022 16:28:18 - INFO - __main__ -   Adapter Languages : ['en', 'pt', 'id', 'tr', 'cs', 'vi', 'eu', 'fa', 'zh_yue', 'is'], Length : 10
01/14/2022 16:28:18 - INFO - __main__ -   Adapter Names ['en/wiki@ukp', 'pt/wiki@ukp', 'id/wiki@ukp', 'tr/wiki@ukp', 'cs/wiki@ukp', 'vi/wiki@ukp', 'eu/wiki@ukp', 'fa/wiki@ukp', 'zh_yue/wiki@ukp', 'is/wiki@ukp'], Length : 10
01/14/2022 16:28:18 - INFO - __main__ -   Language = en
01/14/2022 16:28:18 - INFO - __main__ -   Adapter Name = en/wiki@ukp
01/14/2022 16:28:19 - INFO - __main__ -   Language = zh_yue
01/14/2022 16:28:19 - INFO - __main__ -   Adapter Name = zh_yue/wiki@ukp
01/14/2022 16:28:19 - INFO - __main__ -   Language = pt
01/14/2022 16:28:19 - INFO - __main__ -   Adapter Name = pt/wiki@ukp
01/14/2022 16:28:21 - INFO - __main__ -   Language = cs
01/14/2022 16:28:21 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
01/14/2022 16:28:21 - INFO - __main__ -   Language = id
01/14/2022 16:28:21 - INFO - __main__ -   Adapter Name = id/wiki@ukp
01/14/2022 16:28:22 - INFO - __main__ -   Language = sw
01/14/2022 16:28:22 - INFO - __main__ -   Adapter Name = sw/wiki@ukp
01/14/2022 16:28:24 - INFO - __main__ -   Language = tr
01/14/2022 16:28:24 - INFO - __main__ -   Adapter Name = tr/wiki@ukp
01/14/2022 16:28:26 - INFO - __main__ -   Language = cs
01/14/2022 16:28:26 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
01/14/2022 16:28:27 - INFO - __main__ -   Args Adapter Weight = equal
01/14/2022 16:28:27 - INFO - __main__ -   Adapter Languages = ['en', 'pt', 'id', 'tr', 'vi', 'fa', 'eu', 'zh_yue', 'cs', 'sw']
01/14/2022 16:28:27 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/14/2022 16:28:27 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/14/2022 16:28:27 - INFO - __main__ -   Length of Adapter Weights = 10
01/14/2022 16:28:27 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/cached_test_sw_bert-base-multilingual-cased_128
01/14/2022 16:28:27 - INFO - __main__ -   ***** Running evaluation  in sw *****
01/14/2022 16:28:27 - INFO - __main__ -     Num examples = 1000
01/14/2022 16:28:27 - INFO - __main__ -     Batch size = 32
01/14/2022 16:28:27 - INFO - __main__ -   Batch number = 1
01/14/2022 16:28:27 - INFO - __main__ -   Batch number = 2
01/14/2022 16:28:28 - INFO - __main__ -   Batch number = 3
01/14/2022 16:28:28 - INFO - __main__ -   Language = vi
01/14/2022 16:28:28 - INFO - __main__ -   Adapter Name = vi/wiki@ukp
01/14/2022 16:28:28 - INFO - __main__ -   Batch number = 4
01/14/2022 16:28:28 - INFO - __main__ -   Batch number = 5
01/14/2022 16:28:28 - INFO - __main__ -   Batch number = 6
01/14/2022 16:28:29 - INFO - __main__ -   Batch number = 7
01/14/2022 16:28:29 - INFO - __main__ -   Batch number = 8
01/14/2022 16:28:29 - INFO - __main__ -   Batch number = 9
01/14/2022 16:28:30 - INFO - __main__ -   Batch number = 10
01/14/2022 16:28:30 - INFO - __main__ -   Batch number = 11
01/14/2022 16:28:30 - INFO - __main__ -   Language = eu
01/14/2022 16:28:30 - INFO - __main__ -   Adapter Name = eu/wiki@ukp
01/14/2022 16:28:30 - INFO - __main__ -   Batch number = 12
01/14/2022 16:28:31 - INFO - __main__ -   Batch number = 13
01/14/2022 16:28:31 - INFO - __main__ -   Batch number = 14
01/14/2022 16:28:31 - INFO - __main__ -   Batch number = 15
01/14/2022 16:28:31 - INFO - __main__ -   Batch number = 16
01/14/2022 16:28:32 - INFO - __main__ -   Batch number = 17
01/14/2022 16:28:32 - INFO - __main__ -   Batch number = 18
01/14/2022 16:28:32 - INFO - __main__ -   Language = fa
01/14/2022 16:28:32 - INFO - __main__ -   Adapter Name = fa/wiki@ukp
01/14/2022 16:28:32 - INFO - __main__ -   Batch number = 19
01/14/2022 16:28:33 - INFO - __main__ -   Batch number = 20
01/14/2022 16:28:33 - INFO - __main__ -   Batch number = 21
01/14/2022 16:28:33 - INFO - __main__ -   Batch number = 22
01/14/2022 16:28:33 - INFO - __main__ -   Batch number = 23
01/14/2022 16:28:34 - INFO - __main__ -   Batch number = 24
01/14/2022 16:28:34 - INFO - __main__ -   Batch number = 25
01/14/2022 16:28:34 - INFO - __main__ -   Batch number = 26
01/14/2022 16:28:35 - INFO - __main__ -   Batch number = 27
01/14/2022 16:28:35 - INFO - __main__ -   Language = zh_yue
01/14/2022 16:28:35 - INFO - __main__ -   Adapter Name = zh_yue/wiki@ukp
01/14/2022 16:28:35 - INFO - __main__ -   Batch number = 28
01/14/2022 16:28:35 - INFO - __main__ -   Batch number = 29
01/14/2022 16:28:35 - INFO - __main__ -   Batch number = 30
01/14/2022 16:28:36 - INFO - __main__ -   Batch number = 31
01/14/2022 16:28:36 - INFO - __main__ -   Batch number = 32
01/14/2022 16:28:36 - INFO - __main__ -   ***** Evaluation result  in sw *****
01/14/2022 16:28:36 - INFO - __main__ -     f1 = 0.6183115338882283
01/14/2022 16:28:36 - INFO - __main__ -     loss = 3.2396500781178474
01/14/2022 16:28:36 - INFO - __main__ -     precision = 0.5869074492099323
01/14/2022 16:28:36 - INFO - __main__ -     recall = 0.6532663316582915
01/14/2022 16:28:37 - INFO - __main__ -   Language = is
01/14/2022 16:28:37 - INFO - __main__ -   Adapter Name = is/wiki@ukp
01/14/2022 16:28:41 - INFO - __main__ -   Args Adapter Weight = equal
01/14/2022 16:28:41 - INFO - __main__ -   Adapter Languages = ['en', 'pt', 'id', 'tr', 'cs', 'vi', 'eu', 'fa', 'zh_yue', 'is']
01/14/2022 16:28:41 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/14/2022 16:28:41 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/14/2022 16:28:41 - INFO - __main__ -   Length of Adapter Weights = 10
01/14/2022 16:28:41 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/cached_test_is_bert-base-multilingual-cased_128
01/14/2022 16:28:41 - INFO - __main__ -   ***** Running evaluation  in is *****
01/14/2022 16:28:41 - INFO - __main__ -     Num examples = 1000
01/14/2022 16:28:41 - INFO - __main__ -     Batch size = 32
01/14/2022 16:28:41 - INFO - __main__ -   Batch number = 1
01/14/2022 16:28:41 - INFO - __main__ -   Batch number = 2
01/14/2022 16:28:42 - INFO - __main__ -   Batch number = 3
01/14/2022 16:28:42 - INFO - __main__ -   Batch number = 4
01/14/2022 16:28:42 - INFO - __main__ -   Batch number = 5
01/14/2022 16:28:42 - INFO - __main__ -   Batch number = 6
01/14/2022 16:28:43 - INFO - __main__ -   Batch number = 7
01/14/2022 16:28:43 - INFO - __main__ -   Batch number = 8
01/14/2022 16:28:43 - INFO - __main__ -   Batch number = 9
01/14/2022 16:28:44 - INFO - __main__ -   Batch number = 10
01/14/2022 16:28:44 - INFO - __main__ -   Batch number = 11
01/14/2022 16:28:44 - INFO - __main__ -   Batch number = 12
01/14/2022 16:28:44 - INFO - __main__ -   Batch number = 13
01/14/2022 16:28:45 - INFO - __main__ -   Batch number = 14
01/14/2022 16:28:45 - INFO - __main__ -   Batch number = 15
01/14/2022 16:28:45 - INFO - __main__ -   Batch number = 16
01/14/2022 16:28:46 - INFO - __main__ -   Batch number = 17
01/14/2022 16:28:46 - INFO - __main__ -   Batch number = 18
01/14/2022 16:28:46 - INFO - __main__ -   Batch number = 19
01/14/2022 16:28:46 - INFO - __main__ -   Batch number = 20
01/14/2022 16:28:47 - INFO - __main__ -   Batch number = 21
01/14/2022 16:28:47 - INFO - __main__ -   Batch number = 22
01/14/2022 16:28:47 - INFO - __main__ -   Batch number = 23
01/14/2022 16:28:47 - INFO - __main__ -   Batch number = 24
01/14/2022 16:28:48 - INFO - __main__ -   Batch number = 25
01/14/2022 16:28:48 - INFO - __main__ -   Batch number = 26
01/14/2022 16:28:48 - INFO - __main__ -   Batch number = 27
01/14/2022 16:28:49 - INFO - __main__ -   Batch number = 28
01/14/2022 16:28:49 - INFO - __main__ -   Batch number = 29
01/14/2022 16:28:49 - INFO - __main__ -   Batch number = 30
01/14/2022 16:28:49 - INFO - __main__ -   Batch number = 31
01/14/2022 16:28:50 - INFO - __main__ -   Batch number = 32
01/14/2022 16:28:50 - INFO - __main__ -   ***** Evaluation result  in is *****
01/14/2022 16:28:50 - INFO - __main__ -     f1 = 0.6755521706016756
01/14/2022 16:28:50 - INFO - __main__ -     loss = 1.2199478317052126
01/14/2022 16:28:50 - INFO - __main__ -     precision = 0.6331192005710207
01/14/2022 16:28:50 - INFO - __main__ -     recall = 0.7240816326530612
01/14/2022 16:28:52 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='is', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:28:52 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/14/2022 16:28:52 - INFO - __main__ -   Seed = 3
01/14/2022 16:28:52 - INFO - root -   save model
01/14/2022 16:28:52 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='is', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:28:52 - INFO - __main__ -   Loading pretrained model and tokenizer
01/14/2022 16:28:55 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/14/2022 16:29:00 - INFO - __main__ -   Using lang2id = None
01/14/2022 16:29:00 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/14/2022 16:29:00 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/
01/14/2022 16:29:00 - INFO - root -   Trying to decide if add adapter
01/14/2022 16:29:00 - INFO - root -   loading task adapter
01/14/2022 16:29:00 - INFO - root -   loading lang adpater en/wiki@ukp,pt/wiki@ukp,id/wiki@ukp,tr/wiki@ukp,vi/wiki@ukp,fa/wiki@ukp,eu/wiki@ukp,zh_yue/wiki@ukp,cs/wiki@ukp,is/wiki@ukp
01/14/2022 16:29:00 - INFO - __main__ -   Adapter Languages : ['en', 'pt', 'id', 'tr', 'vi', 'fa', 'eu', 'zh_yue', 'cs', 'is'], Length : 10
01/14/2022 16:29:00 - INFO - __main__ -   Adapter Names ['en/wiki@ukp', 'pt/wiki@ukp', 'id/wiki@ukp', 'tr/wiki@ukp', 'vi/wiki@ukp', 'fa/wiki@ukp', 'eu/wiki@ukp', 'zh_yue/wiki@ukp', 'cs/wiki@ukp', 'is/wiki@ukp'], Length : 10
01/14/2022 16:29:00 - INFO - __main__ -   Language = en
01/14/2022 16:29:00 - INFO - __main__ -   Adapter Name = en/wiki@ukp
01/14/2022 16:29:01 - INFO - __main__ -   Language = pt
01/14/2022 16:29:01 - INFO - __main__ -   Adapter Name = pt/wiki@ukp
01/14/2022 16:29:03 - INFO - __main__ -   Language = id
01/14/2022 16:29:03 - INFO - __main__ -   Adapter Name = id/wiki@ukp
01/14/2022 16:29:04 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='my', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:29:04 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/14/2022 16:29:04 - INFO - __main__ -   Seed = 1
01/14/2022 16:29:04 - INFO - root -   save model
01/14/2022 16:29:04 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='my', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:29:04 - INFO - __main__ -   Loading pretrained model and tokenizer
01/14/2022 16:29:05 - INFO - __main__ -   Language = tr
01/14/2022 16:29:05 - INFO - __main__ -   Adapter Name = tr/wiki@ukp
01/14/2022 16:29:07 - INFO - __main__ -   Language = vi
01/14/2022 16:29:07 - INFO - __main__ -   Adapter Name = vi/wiki@ukp
01/14/2022 16:29:07 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/14/2022 16:29:08 - INFO - __main__ -   Language = fa
01/14/2022 16:29:08 - INFO - __main__ -   Adapter Name = fa/wiki@ukp
01/14/2022 16:29:10 - INFO - __main__ -   Language = eu
01/14/2022 16:29:10 - INFO - __main__ -   Adapter Name = eu/wiki@ukp
01/14/2022 16:29:11 - INFO - __main__ -   Language = zh_yue
01/14/2022 16:29:11 - INFO - __main__ -   Adapter Name = zh_yue/wiki@ukp
01/14/2022 16:29:12 - INFO - __main__ -   Using lang2id = None
01/14/2022 16:29:12 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/14/2022 16:29:12 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/
01/14/2022 16:29:12 - INFO - root -   Trying to decide if add adapter
01/14/2022 16:29:12 - INFO - root -   loading task adapter
01/14/2022 16:29:12 - INFO - root -   loading lang adpater en/wiki@ukp,pt/wiki@ukp,id/wiki@ukp,cs/wiki@ukp,tr/wiki@ukp,eu/wiki@ukp,zh_yue/wiki@ukp,vi/wiki@ukp,fr/wiki@ukp,my/wiki@ukp
01/14/2022 16:29:12 - INFO - __main__ -   Adapter Languages : ['en', 'pt', 'id', 'cs', 'tr', 'eu', 'zh_yue', 'vi', 'fr', 'my'], Length : 10
01/14/2022 16:29:12 - INFO - __main__ -   Adapter Names ['en/wiki@ukp', 'pt/wiki@ukp', 'id/wiki@ukp', 'cs/wiki@ukp', 'tr/wiki@ukp', 'eu/wiki@ukp', 'zh_yue/wiki@ukp', 'vi/wiki@ukp', 'fr/wiki@ukp', 'my/wiki@ukp'], Length : 10
01/14/2022 16:29:12 - INFO - __main__ -   Language = en
01/14/2022 16:29:12 - INFO - __main__ -   Adapter Name = en/wiki@ukp
01/14/2022 16:29:13 - INFO - __main__ -   Language = cs
01/14/2022 16:29:13 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
01/14/2022 16:29:13 - INFO - __main__ -   Language = pt
01/14/2022 16:29:13 - INFO - __main__ -   Adapter Name = pt/wiki@ukp
01/14/2022 16:29:14 - INFO - __main__ -   Language = is
01/14/2022 16:29:14 - INFO - __main__ -   Adapter Name = is/wiki@ukp
01/14/2022 16:29:15 - INFO - __main__ -   Language = id
01/14/2022 16:29:15 - INFO - __main__ -   Adapter Name = id/wiki@ukp
01/14/2022 16:29:18 - INFO - __main__ -   Language = cs
01/14/2022 16:29:18 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
01/14/2022 16:29:19 - INFO - __main__ -   Args Adapter Weight = equal
01/14/2022 16:29:19 - INFO - __main__ -   Adapter Languages = ['en', 'pt', 'id', 'tr', 'vi', 'fa', 'eu', 'zh_yue', 'cs', 'is']
01/14/2022 16:29:19 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/14/2022 16:29:19 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/14/2022 16:29:19 - INFO - __main__ -   Length of Adapter Weights = 10
01/14/2022 16:29:19 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/cached_test_is_bert-base-multilingual-cased_128
01/14/2022 16:29:19 - INFO - __main__ -   ***** Running evaluation  in is *****
01/14/2022 16:29:19 - INFO - __main__ -     Num examples = 1000
01/14/2022 16:29:19 - INFO - __main__ -     Batch size = 32
01/14/2022 16:29:19 - INFO - __main__ -   Batch number = 1
01/14/2022 16:29:19 - INFO - __main__ -   Batch number = 2
01/14/2022 16:29:19 - INFO - __main__ -   Batch number = 3
01/14/2022 16:29:20 - INFO - __main__ -   Language = tr
01/14/2022 16:29:20 - INFO - __main__ -   Adapter Name = tr/wiki@ukp
01/14/2022 16:29:20 - INFO - __main__ -   Batch number = 4
01/14/2022 16:29:20 - INFO - __main__ -   Batch number = 5
01/14/2022 16:29:20 - INFO - __main__ -   Batch number = 6
01/14/2022 16:29:21 - INFO - __main__ -   Batch number = 7
01/14/2022 16:29:21 - INFO - __main__ -   Batch number = 8
01/14/2022 16:29:21 - INFO - __main__ -   Batch number = 9
01/14/2022 16:29:21 - INFO - __main__ -   Batch number = 10
01/14/2022 16:29:22 - INFO - __main__ -   Batch number = 11
01/14/2022 16:29:22 - INFO - __main__ -   Language = eu
01/14/2022 16:29:22 - INFO - __main__ -   Adapter Name = eu/wiki@ukp
01/14/2022 16:29:22 - INFO - __main__ -   Batch number = 12
01/14/2022 16:29:22 - INFO - __main__ -   Batch number = 13
01/14/2022 16:29:23 - INFO - __main__ -   Batch number = 14
01/14/2022 16:29:23 - INFO - __main__ -   Batch number = 15
01/14/2022 16:29:23 - INFO - __main__ -   Batch number = 16
01/14/2022 16:29:24 - INFO - __main__ -   Batch number = 17
01/14/2022 16:29:24 - INFO - __main__ -   Language = zh_yue
01/14/2022 16:29:24 - INFO - __main__ -   Adapter Name = zh_yue/wiki@ukp
01/14/2022 16:29:24 - INFO - __main__ -   Batch number = 18
01/14/2022 16:29:24 - INFO - __main__ -   Batch number = 19
01/14/2022 16:29:25 - INFO - __main__ -   Batch number = 20
01/14/2022 16:29:25 - INFO - __main__ -   Batch number = 21
01/14/2022 16:29:25 - INFO - __main__ -   Batch number = 22
01/14/2022 16:29:25 - INFO - __main__ -   Batch number = 23
01/14/2022 16:29:26 - INFO - __main__ -   Batch number = 24
01/14/2022 16:29:26 - INFO - __main__ -   Batch number = 25
01/14/2022 16:29:26 - INFO - __main__ -   Language = vi
01/14/2022 16:29:26 - INFO - __main__ -   Adapter Name = vi/wiki@ukp
01/14/2022 16:29:26 - INFO - __main__ -   Batch number = 26
01/14/2022 16:29:26 - INFO - __main__ -   Batch number = 27
01/14/2022 16:29:27 - INFO - __main__ -   Batch number = 28
01/14/2022 16:29:27 - INFO - __main__ -   Batch number = 29
01/14/2022 16:29:27 - INFO - __main__ -   Batch number = 30
01/14/2022 16:29:28 - INFO - __main__ -   Batch number = 31
01/14/2022 16:29:28 - INFO - __main__ -   Batch number = 32
01/14/2022 16:29:28 - INFO - __main__ -   ***** Evaluation result  in is *****
01/14/2022 16:29:28 - INFO - __main__ -     f1 = 0.6454749439042633
01/14/2022 16:29:28 - INFO - __main__ -     loss = 1.452005023136735
01/14/2022 16:29:28 - INFO - __main__ -     precision = 0.5955831608005521
01/14/2022 16:29:28 - INFO - __main__ -     recall = 0.7044897959183674
01/14/2022 16:29:28 - INFO - __main__ -   Language = fr
01/14/2022 16:29:28 - INFO - __main__ -   Adapter Name = fr/wiki@ukp
01/14/2022 16:29:29 - INFO - __main__ -   Language = my
01/14/2022 16:29:29 - INFO - __main__ -   Adapter Name = my/wiki@ukp
01/14/2022 16:29:34 - INFO - __main__ -   Args Adapter Weight = equal
01/14/2022 16:29:34 - INFO - __main__ -   Adapter Languages = ['en', 'pt', 'id', 'cs', 'tr', 'eu', 'zh_yue', 'vi', 'fr', 'my']
01/14/2022 16:29:34 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/14/2022 16:29:34 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/14/2022 16:29:34 - INFO - __main__ -   Length of Adapter Weights = 10
01/14/2022 16:29:34 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/cached_test_my_bert-base-multilingual-cased_128
01/14/2022 16:29:34 - INFO - __main__ -   ***** Running evaluation  in my *****
01/14/2022 16:29:34 - INFO - __main__ -     Num examples = 110
01/14/2022 16:29:34 - INFO - __main__ -     Batch size = 32
01/14/2022 16:29:34 - INFO - __main__ -   Batch number = 1
01/14/2022 16:29:34 - INFO - __main__ -   Batch number = 2
01/14/2022 16:29:35 - INFO - __main__ -   Batch number = 3
01/14/2022 16:29:35 - INFO - __main__ -   Batch number = 4
01/14/2022 16:29:35 - INFO - __main__ -   ***** Evaluation result  in my *****
01/14/2022 16:29:35 - INFO - __main__ -     f1 = 0.3986486486486487
01/14/2022 16:29:35 - INFO - __main__ -     loss = 3.202756881713867
01/14/2022 16:29:35 - INFO - __main__ -     precision = 0.3333333333333333
01/14/2022 16:29:35 - INFO - __main__ -     recall = 0.4957983193277311
01/14/2022 16:29:37 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='my', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:29:37 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/14/2022 16:29:37 - INFO - __main__ -   Seed = 2
01/14/2022 16:29:37 - INFO - root -   save model
01/14/2022 16:29:37 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='my', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:29:37 - INFO - __main__ -   Loading pretrained model and tokenizer
01/14/2022 16:29:40 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/14/2022 16:29:46 - INFO - __main__ -   Using lang2id = None
01/14/2022 16:29:46 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/14/2022 16:29:46 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/
01/14/2022 16:29:46 - INFO - root -   Trying to decide if add adapter
01/14/2022 16:29:46 - INFO - root -   loading task adapter
01/14/2022 16:29:46 - INFO - root -   loading lang adpater en/wiki@ukp,pt/wiki@ukp,id/wiki@ukp,tr/wiki@ukp,cs/wiki@ukp,vi/wiki@ukp,eu/wiki@ukp,fa/wiki@ukp,zh_yue/wiki@ukp,my/wiki@ukp
01/14/2022 16:29:46 - INFO - __main__ -   Adapter Languages : ['en', 'pt', 'id', 'tr', 'cs', 'vi', 'eu', 'fa', 'zh_yue', 'my'], Length : 10
01/14/2022 16:29:46 - INFO - __main__ -   Adapter Names ['en/wiki@ukp', 'pt/wiki@ukp', 'id/wiki@ukp', 'tr/wiki@ukp', 'cs/wiki@ukp', 'vi/wiki@ukp', 'eu/wiki@ukp', 'fa/wiki@ukp', 'zh_yue/wiki@ukp', 'my/wiki@ukp'], Length : 10
01/14/2022 16:29:46 - INFO - __main__ -   Language = en
01/14/2022 16:29:46 - INFO - __main__ -   Adapter Name = en/wiki@ukp
01/14/2022 16:29:46 - INFO - __main__ -   Language = pt
01/14/2022 16:29:46 - INFO - __main__ -   Adapter Name = pt/wiki@ukp
01/14/2022 16:29:49 - INFO - __main__ -   Language = id
01/14/2022 16:29:49 - INFO - __main__ -   Adapter Name = id/wiki@ukp
01/14/2022 16:29:51 - INFO - __main__ -   Language = tr
01/14/2022 16:29:51 - INFO - __main__ -   Adapter Name = tr/wiki@ukp
01/14/2022 16:29:53 - INFO - __main__ -   Language = cs
01/14/2022 16:29:53 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
01/14/2022 16:29:55 - INFO - __main__ -   Language = vi
01/14/2022 16:29:55 - INFO - __main__ -   Adapter Name = vi/wiki@ukp
01/14/2022 16:29:57 - INFO - __main__ -   Language = eu
01/14/2022 16:29:57 - INFO - __main__ -   Adapter Name = eu/wiki@ukp
01/14/2022 16:30:00 - INFO - __main__ -   Language = fa
01/14/2022 16:30:00 - INFO - __main__ -   Adapter Name = fa/wiki@ukp
01/14/2022 16:30:02 - INFO - __main__ -   Language = zh_yue
01/14/2022 16:30:02 - INFO - __main__ -   Adapter Name = zh_yue/wiki@ukp
01/14/2022 16:30:04 - INFO - __main__ -   Language = my
01/14/2022 16:30:04 - INFO - __main__ -   Adapter Name = my/wiki@ukp
01/14/2022 16:30:08 - INFO - __main__ -   Args Adapter Weight = equal
01/14/2022 16:30:08 - INFO - __main__ -   Adapter Languages = ['en', 'pt', 'id', 'tr', 'cs', 'vi', 'eu', 'fa', 'zh_yue', 'my']
01/14/2022 16:30:08 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/14/2022 16:30:08 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/14/2022 16:30:08 - INFO - __main__ -   Length of Adapter Weights = 10
01/14/2022 16:30:08 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/cached_test_my_bert-base-multilingual-cased_128
01/14/2022 16:30:08 - INFO - __main__ -   ***** Running evaluation  in my *****
01/14/2022 16:30:08 - INFO - __main__ -     Num examples = 110
01/14/2022 16:30:08 - INFO - __main__ -     Batch size = 32
01/14/2022 16:30:08 - INFO - __main__ -   Batch number = 1
01/14/2022 16:30:08 - INFO - __main__ -   Batch number = 2
01/14/2022 16:30:09 - INFO - __main__ -   Batch number = 3
01/14/2022 16:30:09 - INFO - __main__ -   Batch number = 4
01/14/2022 16:30:09 - INFO - __main__ -   ***** Evaluation result  in my *****
01/14/2022 16:30:09 - INFO - __main__ -     f1 = 0.4317460317460317
01/14/2022 16:30:09 - INFO - __main__ -     loss = 3.3937275409698486
01/14/2022 16:30:09 - INFO - __main__ -     precision = 0.3469387755102041
01/14/2022 16:30:09 - INFO - __main__ -     recall = 0.5714285714285714
01/14/2022 16:30:11 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='my', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:30:11 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/14/2022 16:30:11 - INFO - __main__ -   Seed = 3
01/14/2022 16:30:11 - INFO - root -   save model
01/14/2022 16:30:11 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='my', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:30:11 - INFO - __main__ -   Loading pretrained model and tokenizer
01/14/2022 16:30:14 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/14/2022 16:30:20 - INFO - __main__ -   Using lang2id = None
01/14/2022 16:30:20 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/14/2022 16:30:20 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/
01/14/2022 16:30:20 - INFO - root -   Trying to decide if add adapter
01/14/2022 16:30:20 - INFO - root -   loading task adapter
01/14/2022 16:30:20 - INFO - root -   loading lang adpater en/wiki@ukp,pt/wiki@ukp,id/wiki@ukp,tr/wiki@ukp,vi/wiki@ukp,fa/wiki@ukp,eu/wiki@ukp,zh_yue/wiki@ukp,cs/wiki@ukp,my/wiki@ukp
01/14/2022 16:30:20 - INFO - __main__ -   Adapter Languages : ['en', 'pt', 'id', 'tr', 'vi', 'fa', 'eu', 'zh_yue', 'cs', 'my'], Length : 10
01/14/2022 16:30:20 - INFO - __main__ -   Adapter Names ['en/wiki@ukp', 'pt/wiki@ukp', 'id/wiki@ukp', 'tr/wiki@ukp', 'vi/wiki@ukp', 'fa/wiki@ukp', 'eu/wiki@ukp', 'zh_yue/wiki@ukp', 'cs/wiki@ukp', 'my/wiki@ukp'], Length : 10
01/14/2022 16:30:20 - INFO - __main__ -   Language = en
01/14/2022 16:30:20 - INFO - __main__ -   Adapter Name = en/wiki@ukp
01/14/2022 16:30:21 - INFO - __main__ -   Language = pt
01/14/2022 16:30:21 - INFO - __main__ -   Adapter Name = pt/wiki@ukp
01/14/2022 16:30:22 - INFO - __main__ -   Language = id
01/14/2022 16:30:22 - INFO - __main__ -   Adapter Name = id/wiki@ukp
01/14/2022 16:30:24 - INFO - __main__ -   Language = tr
01/14/2022 16:30:24 - INFO - __main__ -   Adapter Name = tr/wiki@ukp
01/14/2022 16:30:26 - INFO - __main__ -   Language = vi
01/14/2022 16:30:26 - INFO - __main__ -   Adapter Name = vi/wiki@ukp
01/14/2022 16:30:27 - INFO - __main__ -   Language = fa
01/14/2022 16:30:27 - INFO - __main__ -   Adapter Name = fa/wiki@ukp
01/14/2022 16:30:29 - INFO - __main__ -   Language = eu
01/14/2022 16:30:29 - INFO - __main__ -   Adapter Name = eu/wiki@ukp
01/14/2022 16:30:31 - INFO - __main__ -   Language = zh_yue
01/14/2022 16:30:31 - INFO - __main__ -   Adapter Name = zh_yue/wiki@ukp
01/14/2022 16:30:32 - INFO - __main__ -   Language = cs
01/14/2022 16:30:32 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
01/14/2022 16:30:34 - INFO - __main__ -   Language = my
01/14/2022 16:30:34 - INFO - __main__ -   Adapter Name = my/wiki@ukp
01/14/2022 16:30:38 - INFO - __main__ -   Args Adapter Weight = equal
01/14/2022 16:30:38 - INFO - __main__ -   Adapter Languages = ['en', 'pt', 'id', 'tr', 'vi', 'fa', 'eu', 'zh_yue', 'cs', 'my']
01/14/2022 16:30:38 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/14/2022 16:30:38 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/14/2022 16:30:38 - INFO - __main__ -   Length of Adapter Weights = 10
01/14/2022 16:30:38 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/cached_test_my_bert-base-multilingual-cased_128
01/14/2022 16:30:38 - INFO - __main__ -   ***** Running evaluation  in my *****
01/14/2022 16:30:38 - INFO - __main__ -     Num examples = 110
01/14/2022 16:30:38 - INFO - __main__ -     Batch size = 32
01/14/2022 16:30:38 - INFO - __main__ -   Batch number = 1
01/14/2022 16:30:39 - INFO - __main__ -   Batch number = 2
01/14/2022 16:30:39 - INFO - __main__ -   Batch number = 3
01/14/2022 16:30:39 - INFO - __main__ -   Batch number = 4
01/14/2022 16:30:39 - INFO - __main__ -   ***** Evaluation result  in my *****
01/14/2022 16:30:39 - INFO - __main__ -     f1 = 0.43278688524590164
01/14/2022 16:30:39 - INFO - __main__ -     loss = 3.5076140761375427
01/14/2022 16:30:39 - INFO - __main__ -     precision = 0.3548387096774194
01/14/2022 16:30:39 - INFO - __main__ -     recall = 0.5546218487394958
01/14/2022 16:31:37 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='qu', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:31:37 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/14/2022 16:31:37 - INFO - __main__ -   Seed = 1
01/14/2022 16:31:37 - INFO - root -   save model
01/14/2022 16:31:37 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='qu', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:31:37 - INFO - __main__ -   Loading pretrained model and tokenizer
01/14/2022 16:31:40 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/14/2022 16:31:46 - INFO - __main__ -   Using lang2id = None
01/14/2022 16:31:46 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/14/2022 16:31:46 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/
01/14/2022 16:31:46 - INFO - root -   Trying to decide if add adapter
01/14/2022 16:31:46 - INFO - root -   loading task adapter
01/14/2022 16:31:46 - INFO - root -   loading lang adpater en/wiki@ukp,pt/wiki@ukp,id/wiki@ukp,cs/wiki@ukp,tr/wiki@ukp,eu/wiki@ukp,zh_yue/wiki@ukp,vi/wiki@ukp,fr/wiki@ukp,qu/wiki@ukp
01/14/2022 16:31:46 - INFO - __main__ -   Adapter Languages : ['en', 'pt', 'id', 'cs', 'tr', 'eu', 'zh_yue', 'vi', 'fr', 'qu'], Length : 10
01/14/2022 16:31:46 - INFO - __main__ -   Adapter Names ['en/wiki@ukp', 'pt/wiki@ukp', 'id/wiki@ukp', 'cs/wiki@ukp', 'tr/wiki@ukp', 'eu/wiki@ukp', 'zh_yue/wiki@ukp', 'vi/wiki@ukp', 'fr/wiki@ukp', 'qu/wiki@ukp'], Length : 10
01/14/2022 16:31:46 - INFO - __main__ -   Language = en
01/14/2022 16:31:46 - INFO - __main__ -   Adapter Name = en/wiki@ukp
01/14/2022 16:31:47 - INFO - __main__ -   Language = pt
01/14/2022 16:31:47 - INFO - __main__ -   Adapter Name = pt/wiki@ukp
01/14/2022 16:31:49 - INFO - __main__ -   Language = id
01/14/2022 16:31:49 - INFO - __main__ -   Adapter Name = id/wiki@ukp
01/14/2022 16:31:52 - INFO - __main__ -   Language = cs
01/14/2022 16:31:52 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
01/14/2022 16:31:54 - INFO - __main__ -   Language = tr
01/14/2022 16:31:54 - INFO - __main__ -   Adapter Name = tr/wiki@ukp
01/14/2022 16:31:57 - INFO - __main__ -   Language = eu
01/14/2022 16:31:57 - INFO - __main__ -   Adapter Name = eu/wiki@ukp
01/14/2022 16:31:59 - INFO - __main__ -   Language = zh_yue
01/14/2022 16:31:59 - INFO - __main__ -   Adapter Name = zh_yue/wiki@ukp
01/14/2022 16:32:02 - INFO - __main__ -   Language = vi
01/14/2022 16:32:02 - INFO - __main__ -   Adapter Name = vi/wiki@ukp
01/14/2022 16:32:04 - INFO - __main__ -   Language = fr
01/14/2022 16:32:04 - INFO - __main__ -   Adapter Name = fr/wiki@ukp
01/14/2022 16:32:05 - INFO - __main__ -   Language = qu
01/14/2022 16:32:05 - INFO - __main__ -   Adapter Name = qu/wiki@ukp
01/14/2022 16:32:10 - INFO - __main__ -   Args Adapter Weight = equal
01/14/2022 16:32:10 - INFO - __main__ -   Adapter Languages = ['en', 'pt', 'id', 'cs', 'tr', 'eu', 'zh_yue', 'vi', 'fr', 'qu']
01/14/2022 16:32:10 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/14/2022 16:32:10 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/14/2022 16:32:10 - INFO - __main__ -   Length of Adapter Weights = 10
01/14/2022 16:32:10 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/cached_test_qu_bert-base-multilingual-cased_128
01/14/2022 16:32:10 - INFO - __main__ -   ***** Running evaluation  in qu *****
01/14/2022 16:32:10 - INFO - __main__ -     Num examples = 100
01/14/2022 16:32:10 - INFO - __main__ -     Batch size = 32
01/14/2022 16:32:10 - INFO - __main__ -   Batch number = 1
01/14/2022 16:32:11 - INFO - __main__ -   Batch number = 2
01/14/2022 16:32:11 - INFO - __main__ -   Batch number = 3
01/14/2022 16:32:11 - INFO - __main__ -   Batch number = 4
01/14/2022 16:32:11 - INFO - __main__ -   ***** Evaluation result  in qu *****
01/14/2022 16:32:11 - INFO - __main__ -     f1 = 0.6024096385542169
01/14/2022 16:32:11 - INFO - __main__ -     loss = 4.390545547008514
01/14/2022 16:32:11 - INFO - __main__ -     precision = 0.5555555555555556
01/14/2022 16:32:11 - INFO - __main__ -     recall = 0.6578947368421053
01/14/2022 16:32:13 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='cdo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:32:13 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/14/2022 16:32:13 - INFO - __main__ -   Seed = 1
01/14/2022 16:32:13 - INFO - root -   save model
01/14/2022 16:32:13 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='cdo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:32:13 - INFO - __main__ -   Loading pretrained model and tokenizer
01/14/2022 16:32:14 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='qu', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:32:14 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/14/2022 16:32:14 - INFO - __main__ -   Seed = 2
01/14/2022 16:32:14 - INFO - root -   save model
01/14/2022 16:32:14 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='qu', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:32:14 - INFO - __main__ -   Loading pretrained model and tokenizer
01/14/2022 16:32:16 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/14/2022 16:32:16 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/14/2022 16:32:22 - INFO - __main__ -   Using lang2id = None
01/14/2022 16:32:22 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/14/2022 16:32:22 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/
01/14/2022 16:32:22 - INFO - root -   Trying to decide if add adapter
01/14/2022 16:32:22 - INFO - root -   loading task adapter
01/14/2022 16:32:22 - INFO - root -   loading lang adpater en/wiki@ukp,pt/wiki@ukp,id/wiki@ukp,cs/wiki@ukp,tr/wiki@ukp,eu/wiki@ukp,zh_yue/wiki@ukp,vi/wiki@ukp,fr/wiki@ukp,cdo/wiki@ukp
01/14/2022 16:32:22 - INFO - __main__ -   Adapter Languages : ['en', 'pt', 'id', 'cs', 'tr', 'eu', 'zh_yue', 'vi', 'fr', 'cdo'], Length : 10
01/14/2022 16:32:22 - INFO - __main__ -   Adapter Names ['en/wiki@ukp', 'pt/wiki@ukp', 'id/wiki@ukp', 'cs/wiki@ukp', 'tr/wiki@ukp', 'eu/wiki@ukp', 'zh_yue/wiki@ukp', 'vi/wiki@ukp', 'fr/wiki@ukp', 'cdo/wiki@ukp'], Length : 10
01/14/2022 16:32:22 - INFO - __main__ -   Language = en
01/14/2022 16:32:22 - INFO - __main__ -   Adapter Name = en/wiki@ukp
01/14/2022 16:32:22 - INFO - __main__ -   Using lang2id = None
01/14/2022 16:32:22 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/14/2022 16:32:22 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/
01/14/2022 16:32:22 - INFO - root -   Trying to decide if add adapter
01/14/2022 16:32:22 - INFO - root -   loading task adapter
01/14/2022 16:32:22 - INFO - root -   loading lang adpater en/wiki@ukp,pt/wiki@ukp,id/wiki@ukp,tr/wiki@ukp,cs/wiki@ukp,vi/wiki@ukp,eu/wiki@ukp,fa/wiki@ukp,zh_yue/wiki@ukp,qu/wiki@ukp
01/14/2022 16:32:22 - INFO - __main__ -   Adapter Languages : ['en', 'pt', 'id', 'tr', 'cs', 'vi', 'eu', 'fa', 'zh_yue', 'qu'], Length : 10
01/14/2022 16:32:22 - INFO - __main__ -   Adapter Names ['en/wiki@ukp', 'pt/wiki@ukp', 'id/wiki@ukp', 'tr/wiki@ukp', 'cs/wiki@ukp', 'vi/wiki@ukp', 'eu/wiki@ukp', 'fa/wiki@ukp', 'zh_yue/wiki@ukp', 'qu/wiki@ukp'], Length : 10
01/14/2022 16:32:22 - INFO - __main__ -   Language = en
01/14/2022 16:32:22 - INFO - __main__ -   Adapter Name = en/wiki@ukp
01/14/2022 16:32:23 - INFO - __main__ -   Language = pt
01/14/2022 16:32:23 - INFO - __main__ -   Adapter Name = pt/wiki@ukp
01/14/2022 16:32:23 - INFO - __main__ -   Language = pt
01/14/2022 16:32:23 - INFO - __main__ -   Adapter Name = pt/wiki@ukp
01/14/2022 16:32:25 - INFO - __main__ -   Language = id
01/14/2022 16:32:25 - INFO - __main__ -   Adapter Name = id/wiki@ukp
01/14/2022 16:32:25 - INFO - __main__ -   Language = id
01/14/2022 16:32:25 - INFO - __main__ -   Adapter Name = id/wiki@ukp
01/14/2022 16:32:28 - INFO - __main__ -   Language = cs
01/14/2022 16:32:28 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
01/14/2022 16:32:28 - INFO - __main__ -   Language = tr
01/14/2022 16:32:28 - INFO - __main__ -   Adapter Name = tr/wiki@ukp
01/14/2022 16:32:30 - INFO - __main__ -   Language = tr
01/14/2022 16:32:30 - INFO - __main__ -   Adapter Name = tr/wiki@ukp
01/14/2022 16:32:30 - INFO - __main__ -   Language = cs
01/14/2022 16:32:30 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
01/14/2022 16:32:32 - INFO - __main__ -   Language = eu
01/14/2022 16:32:32 - INFO - __main__ -   Adapter Name = eu/wiki@ukp
01/14/2022 16:32:32 - INFO - __main__ -   Language = vi
01/14/2022 16:32:32 - INFO - __main__ -   Adapter Name = vi/wiki@ukp
01/14/2022 16:32:34 - INFO - __main__ -   Language = zh_yue
01/14/2022 16:32:34 - INFO - __main__ -   Adapter Name = zh_yue/wiki@ukp
01/14/2022 16:32:34 - INFO - __main__ -   Language = eu
01/14/2022 16:32:34 - INFO - __main__ -   Adapter Name = eu/wiki@ukp
01/14/2022 16:32:36 - INFO - __main__ -   Language = fa
01/14/2022 16:32:36 - INFO - __main__ -   Adapter Name = fa/wiki@ukp
01/14/2022 16:32:37 - INFO - __main__ -   Language = vi
01/14/2022 16:32:37 - INFO - __main__ -   Adapter Name = vi/wiki@ukp
01/14/2022 16:32:39 - INFO - __main__ -   Language = zh_yue
01/14/2022 16:32:39 - INFO - __main__ -   Adapter Name = zh_yue/wiki@ukp
01/14/2022 16:32:39 - INFO - __main__ -   Language = fr
01/14/2022 16:32:39 - INFO - __main__ -   Adapter Name = fr/wiki@ukp
01/14/2022 16:32:40 - INFO - __main__ -   Language = cdo
01/14/2022 16:32:40 - INFO - __main__ -   Adapter Name = cdo/wiki@ukp
01/14/2022 16:32:41 - INFO - __main__ -   Language = qu
01/14/2022 16:32:41 - INFO - __main__ -   Adapter Name = qu/wiki@ukp
01/14/2022 16:32:45 - INFO - __main__ -   Args Adapter Weight = equal
01/14/2022 16:32:45 - INFO - __main__ -   Adapter Languages = ['en', 'pt', 'id', 'cs', 'tr', 'eu', 'zh_yue', 'vi', 'fr', 'cdo']
01/14/2022 16:32:45 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/14/2022 16:32:45 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/14/2022 16:32:45 - INFO - __main__ -   Length of Adapter Weights = 10
01/14/2022 16:32:45 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/cached_test_cdo_bert-base-multilingual-cased_128
01/14/2022 16:32:45 - INFO - __main__ -   ***** Running evaluation  in cdo *****
01/14/2022 16:32:45 - INFO - __main__ -     Num examples = 101
01/14/2022 16:32:45 - INFO - __main__ -     Batch size = 32
01/14/2022 16:32:45 - INFO - __main__ -   Batch number = 1
01/14/2022 16:32:45 - INFO - __main__ -   Batch number = 2
01/14/2022 16:32:45 - INFO - __main__ -   Args Adapter Weight = equal
01/14/2022 16:32:45 - INFO - __main__ -   Adapter Languages = ['en', 'pt', 'id', 'tr', 'cs', 'vi', 'eu', 'fa', 'zh_yue', 'qu']
01/14/2022 16:32:45 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/14/2022 16:32:45 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/14/2022 16:32:45 - INFO - __main__ -   Length of Adapter Weights = 10
01/14/2022 16:32:45 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/cached_test_qu_bert-base-multilingual-cased_128
01/14/2022 16:32:45 - INFO - __main__ -   ***** Running evaluation  in qu *****
01/14/2022 16:32:45 - INFO - __main__ -     Num examples = 100
01/14/2022 16:32:45 - INFO - __main__ -     Batch size = 32
01/14/2022 16:32:45 - INFO - __main__ -   Batch number = 1
01/14/2022 16:32:45 - INFO - __main__ -   Batch number = 3
01/14/2022 16:32:45 - INFO - __main__ -   Batch number = 2
01/14/2022 16:32:46 - INFO - __main__ -   Batch number = 4
01/14/2022 16:32:46 - INFO - __main__ -   Batch number = 3
01/14/2022 16:32:46 - INFO - __main__ -   ***** Evaluation result  in cdo *****
01/14/2022 16:32:46 - INFO - __main__ -     f1 = 0.12669683257918551
01/14/2022 16:32:46 - INFO - __main__ -     loss = 2.535094916820526
01/14/2022 16:32:46 - INFO - __main__ -     precision = 0.13592233009708737
01/14/2022 16:32:46 - INFO - __main__ -     recall = 0.11864406779661017
01/14/2022 16:32:46 - INFO - __main__ -   Batch number = 4
01/14/2022 16:32:46 - INFO - __main__ -   ***** Evaluation result  in qu *****
01/14/2022 16:32:46 - INFO - __main__ -     f1 = 0.5873015873015873
01/14/2022 16:32:46 - INFO - __main__ -     loss = 3.8110600411891937
01/14/2022 16:32:46 - INFO - __main__ -     precision = 0.5362318840579711
01/14/2022 16:32:46 - INFO - __main__ -     recall = 0.6491228070175439
01/14/2022 16:32:46 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='cdo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:32:46 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/14/2022 16:32:46 - INFO - __main__ -   Seed = 1
01/14/2022 16:32:46 - INFO - root -   save model
01/14/2022 16:32:46 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='cdo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:32:46 - INFO - __main__ -   Loading pretrained model and tokenizer
01/14/2022 16:32:48 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='cdo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:32:48 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/14/2022 16:32:48 - INFO - __main__ -   Seed = 2
01/14/2022 16:32:48 - INFO - root -   save model
01/14/2022 16:32:48 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='cdo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:32:48 - INFO - __main__ -   Loading pretrained model and tokenizer
01/14/2022 16:32:48 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='qu', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:32:48 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/14/2022 16:32:48 - INFO - __main__ -   Seed = 3
01/14/2022 16:32:48 - INFO - root -   save model
01/14/2022 16:32:48 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='qu', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:32:48 - INFO - __main__ -   Loading pretrained model and tokenizer
01/14/2022 16:32:49 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/14/2022 16:32:51 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/14/2022 16:32:51 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/14/2022 16:32:57 - INFO - __main__ -   Using lang2id = None
01/14/2022 16:32:57 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/14/2022 16:32:57 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/
01/14/2022 16:32:57 - INFO - root -   Trying to decide if add adapter
01/14/2022 16:32:57 - INFO - root -   loading task adapter
01/14/2022 16:32:57 - INFO - __main__ -   Using lang2id = None
01/14/2022 16:32:57 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/14/2022 16:32:57 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/
01/14/2022 16:32:57 - INFO - root -   Trying to decide if add adapter
01/14/2022 16:32:57 - INFO - root -   loading task adapter
01/14/2022 16:32:57 - INFO - root -   loading lang adpater en/wiki@ukp,pt/wiki@ukp,id/wiki@ukp,tr/wiki@ukp,vi/wiki@ukp,fa/wiki@ukp,eu/wiki@ukp,zh_yue/wiki@ukp,cs/wiki@ukp,qu/wiki@ukp
01/14/2022 16:32:57 - INFO - __main__ -   Adapter Languages : ['en', 'pt', 'id', 'tr', 'vi', 'fa', 'eu', 'zh_yue', 'cs', 'qu'], Length : 10
01/14/2022 16:32:57 - INFO - __main__ -   Adapter Names ['en/wiki@ukp', 'pt/wiki@ukp', 'id/wiki@ukp', 'tr/wiki@ukp', 'vi/wiki@ukp', 'fa/wiki@ukp', 'eu/wiki@ukp', 'zh_yue/wiki@ukp', 'cs/wiki@ukp', 'qu/wiki@ukp'], Length : 10
01/14/2022 16:32:57 - INFO - __main__ -   Language = en
01/14/2022 16:32:57 - INFO - __main__ -   Adapter Name = en/wiki@ukp
01/14/2022 16:32:57 - INFO - root -   loading lang adpater en/wiki@ukp,pt/wiki@ukp,id/wiki@ukp,tr/wiki@ukp,cs/wiki@ukp,vi/wiki@ukp,eu/wiki@ukp,fa/wiki@ukp,zh_yue/wiki@ukp,cdo/wiki@ukp
01/14/2022 16:32:57 - INFO - __main__ -   Adapter Languages : ['en', 'pt', 'id', 'tr', 'cs', 'vi', 'eu', 'fa', 'zh_yue', 'cdo'], Length : 10
01/14/2022 16:32:57 - INFO - __main__ -   Adapter Names ['en/wiki@ukp', 'pt/wiki@ukp', 'id/wiki@ukp', 'tr/wiki@ukp', 'cs/wiki@ukp', 'vi/wiki@ukp', 'eu/wiki@ukp', 'fa/wiki@ukp', 'zh_yue/wiki@ukp', 'cdo/wiki@ukp'], Length : 10
01/14/2022 16:32:57 - INFO - __main__ -   Language = en
01/14/2022 16:32:57 - INFO - __main__ -   Adapter Name = en/wiki@ukp
01/14/2022 16:32:58 - INFO - __main__ -   Language = pt
01/14/2022 16:32:58 - INFO - __main__ -   Adapter Name = pt/wiki@ukp
01/14/2022 16:32:58 - INFO - __main__ -   Language = pt
01/14/2022 16:32:58 - INFO - __main__ -   Adapter Name = pt/wiki@ukp
01/14/2022 16:32:59 - INFO - __main__ -   Language = id
01/14/2022 16:32:59 - INFO - __main__ -   Adapter Name = id/wiki@ukp
01/14/2022 16:33:00 - INFO - __main__ -   Language = id
01/14/2022 16:33:00 - INFO - __main__ -   Adapter Name = id/wiki@ukp
01/14/2022 16:33:01 - INFO - __main__ -   Language = tr
01/14/2022 16:33:01 - INFO - __main__ -   Adapter Name = tr/wiki@ukp
01/14/2022 16:33:02 - INFO - __main__ -   Language = tr
01/14/2022 16:33:02 - INFO - __main__ -   Adapter Name = tr/wiki@ukp
01/14/2022 16:33:02 - INFO - __main__ -   Language = cs
01/14/2022 16:33:02 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
01/14/2022 16:33:03 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ilo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:33:03 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/14/2022 16:33:03 - INFO - __main__ -   Seed = 1
01/14/2022 16:33:03 - INFO - root -   save model
01/14/2022 16:33:03 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ilo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:33:03 - INFO - __main__ -   Loading pretrained model and tokenizer
01/14/2022 16:33:04 - INFO - __main__ -   Language = vi
01/14/2022 16:33:04 - INFO - __main__ -   Adapter Name = vi/wiki@ukp
01/14/2022 16:33:04 - INFO - __main__ -   Language = vi
01/14/2022 16:33:04 - INFO - __main__ -   Adapter Name = vi/wiki@ukp
01/14/2022 16:33:06 - INFO - __main__ -   Language = eu
01/14/2022 16:33:06 - INFO - __main__ -   Adapter Name = eu/wiki@ukp
01/14/2022 16:33:06 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/14/2022 16:33:06 - INFO - __main__ -   Language = fa
01/14/2022 16:33:06 - INFO - __main__ -   Adapter Name = fa/wiki@ukp
01/14/2022 16:33:08 - INFO - __main__ -   Language = fa
01/14/2022 16:33:08 - INFO - __main__ -   Adapter Name = fa/wiki@ukp
01/14/2022 16:33:08 - INFO - __main__ -   Language = eu
01/14/2022 16:33:08 - INFO - __main__ -   Adapter Name = eu/wiki@ukp
01/14/2022 16:33:09 - INFO - __main__ -   Language = zh_yue
01/14/2022 16:33:09 - INFO - __main__ -   Adapter Name = zh_yue/wiki@ukp
01/14/2022 16:33:11 - INFO - __main__ -   Language = cdo
01/14/2022 16:33:11 - INFO - __main__ -   Adapter Name = cdo/wiki@ukp
01/14/2022 16:33:11 - INFO - __main__ -   Language = zh_yue
01/14/2022 16:33:11 - INFO - __main__ -   Adapter Name = zh_yue/wiki@ukp
01/14/2022 16:33:12 - INFO - __main__ -   Using lang2id = None
01/14/2022 16:33:12 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/14/2022 16:33:12 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/
01/14/2022 16:33:12 - INFO - root -   Trying to decide if add adapter
01/14/2022 16:33:12 - INFO - root -   loading task adapter
01/14/2022 16:33:12 - INFO - root -   loading lang adpater en/wiki@ukp,pt/wiki@ukp,id/wiki@ukp,cs/wiki@ukp,tr/wiki@ukp,eu/wiki@ukp,zh_yue/wiki@ukp,vi/wiki@ukp,fr/wiki@ukp,ilo/wiki@ukp
01/14/2022 16:33:12 - INFO - __main__ -   Adapter Languages : ['en', 'pt', 'id', 'cs', 'tr', 'eu', 'zh_yue', 'vi', 'fr', 'ilo'], Length : 10
01/14/2022 16:33:12 - INFO - __main__ -   Adapter Names ['en/wiki@ukp', 'pt/wiki@ukp', 'id/wiki@ukp', 'cs/wiki@ukp', 'tr/wiki@ukp', 'eu/wiki@ukp', 'zh_yue/wiki@ukp', 'vi/wiki@ukp', 'fr/wiki@ukp', 'ilo/wiki@ukp'], Length : 10
01/14/2022 16:33:12 - INFO - __main__ -   Language = en
01/14/2022 16:33:12 - INFO - __main__ -   Adapter Name = en/wiki@ukp
01/14/2022 16:33:13 - INFO - __main__ -   Language = pt
01/14/2022 16:33:13 - INFO - __main__ -   Adapter Name = pt/wiki@ukp
01/14/2022 16:33:13 - INFO - __main__ -   Language = cs
01/14/2022 16:33:13 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
01/14/2022 16:33:15 - INFO - __main__ -   Args Adapter Weight = equal
01/14/2022 16:33:15 - INFO - __main__ -   Adapter Languages = ['en', 'pt', 'id', 'tr', 'cs', 'vi', 'eu', 'fa', 'zh_yue', 'cdo']
01/14/2022 16:33:15 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/14/2022 16:33:15 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/14/2022 16:33:15 - INFO - __main__ -   Length of Adapter Weights = 10
01/14/2022 16:33:15 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/cached_test_cdo_bert-base-multilingual-cased_128
01/14/2022 16:33:15 - INFO - __main__ -   ***** Running evaluation  in cdo *****
01/14/2022 16:33:15 - INFO - __main__ -     Num examples = 101
01/14/2022 16:33:15 - INFO - __main__ -     Batch size = 32
01/14/2022 16:33:15 - INFO - __main__ -   Batch number = 1
01/14/2022 16:33:15 - INFO - __main__ -   Language = id
01/14/2022 16:33:15 - INFO - __main__ -   Adapter Name = id/wiki@ukp
01/14/2022 16:33:15 - INFO - __main__ -   Language = qu
01/14/2022 16:33:15 - INFO - __main__ -   Adapter Name = qu/wiki@ukp
01/14/2022 16:33:15 - INFO - __main__ -   Batch number = 2
01/14/2022 16:33:15 - INFO - __main__ -   Batch number = 3
01/14/2022 16:33:16 - INFO - __main__ -   Batch number = 4
01/14/2022 16:33:16 - INFO - __main__ -   ***** Evaluation result  in cdo *****
01/14/2022 16:33:16 - INFO - __main__ -     f1 = 0.11258278145695363
01/14/2022 16:33:16 - INFO - __main__ -     loss = 4.374813675880432
01/14/2022 16:33:16 - INFO - __main__ -     precision = 0.09239130434782608
01/14/2022 16:33:16 - INFO - __main__ -     recall = 0.1440677966101695
01/14/2022 16:33:17 - INFO - __main__ -   Language = cs
01/14/2022 16:33:17 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
01/14/2022 16:33:18 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='cdo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:33:18 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/14/2022 16:33:18 - INFO - __main__ -   Seed = 3
01/14/2022 16:33:18 - INFO - root -   save model
01/14/2022 16:33:18 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='cdo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:33:18 - INFO - __main__ -   Loading pretrained model and tokenizer
01/14/2022 16:33:19 - INFO - __main__ -   Language = tr
01/14/2022 16:33:19 - INFO - __main__ -   Adapter Name = tr/wiki@ukp
01/14/2022 16:33:20 - INFO - __main__ -   Args Adapter Weight = equal
01/14/2022 16:33:20 - INFO - __main__ -   Adapter Languages = ['en', 'pt', 'id', 'tr', 'vi', 'fa', 'eu', 'zh_yue', 'cs', 'qu']
01/14/2022 16:33:20 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/14/2022 16:33:20 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/14/2022 16:33:20 - INFO - __main__ -   Length of Adapter Weights = 10
01/14/2022 16:33:20 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/cached_test_qu_bert-base-multilingual-cased_128
01/14/2022 16:33:20 - INFO - __main__ -   ***** Running evaluation  in qu *****
01/14/2022 16:33:20 - INFO - __main__ -     Num examples = 100
01/14/2022 16:33:20 - INFO - __main__ -     Batch size = 32
01/14/2022 16:33:20 - INFO - __main__ -   Batch number = 1
01/14/2022 16:33:20 - INFO - __main__ -   Batch number = 2
01/14/2022 16:33:20 - INFO - __main__ -   Batch number = 3
01/14/2022 16:33:20 - INFO - __main__ -   Batch number = 4
01/14/2022 16:33:21 - INFO - __main__ -   ***** Evaluation result  in qu *****
01/14/2022 16:33:21 - INFO - __main__ -     f1 = 0.5914396887159533
01/14/2022 16:33:21 - INFO - __main__ -     loss = 4.148501843214035
01/14/2022 16:33:21 - INFO - __main__ -     precision = 0.5314685314685315
01/14/2022 16:33:21 - INFO - __main__ -     recall = 0.6666666666666666
01/14/2022 16:33:21 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/14/2022 16:33:22 - INFO - __main__ -   Language = eu
01/14/2022 16:33:22 - INFO - __main__ -   Adapter Name = eu/wiki@ukp
01/14/2022 16:33:24 - INFO - __main__ -   Language = zh_yue
01/14/2022 16:33:24 - INFO - __main__ -   Adapter Name = zh_yue/wiki@ukp
01/14/2022 16:33:26 - INFO - __main__ -   Language = vi
01/14/2022 16:33:26 - INFO - __main__ -   Adapter Name = vi/wiki@ukp
01/14/2022 16:33:27 - INFO - __main__ -   Using lang2id = None
01/14/2022 16:33:27 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/14/2022 16:33:27 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/
01/14/2022 16:33:27 - INFO - root -   Trying to decide if add adapter
01/14/2022 16:33:27 - INFO - root -   loading task adapter
01/14/2022 16:33:27 - INFO - root -   loading lang adpater en/wiki@ukp,pt/wiki@ukp,id/wiki@ukp,tr/wiki@ukp,vi/wiki@ukp,fa/wiki@ukp,eu/wiki@ukp,zh_yue/wiki@ukp,cs/wiki@ukp,cdo/wiki@ukp
01/14/2022 16:33:27 - INFO - __main__ -   Adapter Languages : ['en', 'pt', 'id', 'tr', 'vi', 'fa', 'eu', 'zh_yue', 'cs', 'cdo'], Length : 10
01/14/2022 16:33:27 - INFO - __main__ -   Adapter Names ['en/wiki@ukp', 'pt/wiki@ukp', 'id/wiki@ukp', 'tr/wiki@ukp', 'vi/wiki@ukp', 'fa/wiki@ukp', 'eu/wiki@ukp', 'zh_yue/wiki@ukp', 'cs/wiki@ukp', 'cdo/wiki@ukp'], Length : 10
01/14/2022 16:33:27 - INFO - __main__ -   Language = en
01/14/2022 16:33:27 - INFO - __main__ -   Adapter Name = en/wiki@ukp
01/14/2022 16:33:28 - INFO - __main__ -   Language = pt
01/14/2022 16:33:28 - INFO - __main__ -   Adapter Name = pt/wiki@ukp
01/14/2022 16:33:28 - INFO - __main__ -   Language = fr
01/14/2022 16:33:28 - INFO - __main__ -   Adapter Name = fr/wiki@ukp
01/14/2022 16:33:29 - INFO - __main__ -   Language = ilo
01/14/2022 16:33:29 - INFO - __main__ -   Adapter Name = ilo/wiki@ukp
01/14/2022 16:33:30 - INFO - __main__ -   Language = id
01/14/2022 16:33:30 - INFO - __main__ -   Adapter Name = id/wiki@ukp
01/14/2022 16:33:32 - INFO - __main__ -   Language = tr
01/14/2022 16:33:32 - INFO - __main__ -   Adapter Name = tr/wiki@ukp
01/14/2022 16:33:34 - INFO - __main__ -   Args Adapter Weight = equal
01/14/2022 16:33:34 - INFO - __main__ -   Adapter Languages = ['en', 'pt', 'id', 'cs', 'tr', 'eu', 'zh_yue', 'vi', 'fr', 'ilo']
01/14/2022 16:33:34 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/14/2022 16:33:34 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/14/2022 16:33:34 - INFO - __main__ -   Length of Adapter Weights = 10
01/14/2022 16:33:34 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/cached_test_ilo_bert-base-multilingual-cased_128
01/14/2022 16:33:34 - INFO - __main__ -   ***** Running evaluation  in ilo *****
01/14/2022 16:33:34 - INFO - __main__ -     Num examples = 100
01/14/2022 16:33:34 - INFO - __main__ -     Batch size = 32
01/14/2022 16:33:34 - INFO - __main__ -   Batch number = 1
01/14/2022 16:33:34 - INFO - __main__ -   Batch number = 2
01/14/2022 16:33:34 - INFO - __main__ -   Batch number = 3
01/14/2022 16:33:34 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='xmf', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:33:34 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/14/2022 16:33:34 - INFO - __main__ -   Seed = 1
01/14/2022 16:33:34 - INFO - root -   save model
01/14/2022 16:33:34 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='xmf', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:33:34 - INFO - __main__ -   Loading pretrained model and tokenizer
01/14/2022 16:33:34 - INFO - __main__ -   Language = vi
01/14/2022 16:33:34 - INFO - __main__ -   Adapter Name = vi/wiki@ukp
01/14/2022 16:33:35 - INFO - __main__ -   Batch number = 4
01/14/2022 16:33:35 - INFO - __main__ -   ***** Evaluation result  in ilo *****
01/14/2022 16:33:35 - INFO - __main__ -     f1 = 0.5254237288135593
01/14/2022 16:33:35 - INFO - __main__ -     loss = 3.032944440841675
01/14/2022 16:33:35 - INFO - __main__ -     precision = 0.46616541353383456
01/14/2022 16:33:35 - INFO - __main__ -     recall = 0.6019417475728155
01/14/2022 16:33:37 - INFO - __main__ -   Language = fa
01/14/2022 16:33:37 - INFO - __main__ -   Adapter Name = fa/wiki@ukp
01/14/2022 16:33:37 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ilo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:33:37 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/14/2022 16:33:37 - INFO - __main__ -   Seed = 2
01/14/2022 16:33:37 - INFO - root -   save model
01/14/2022 16:33:37 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ilo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:33:37 - INFO - __main__ -   Loading pretrained model and tokenizer
01/14/2022 16:33:37 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/14/2022 16:33:39 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/14/2022 16:33:39 - INFO - __main__ -   Language = eu
01/14/2022 16:33:39 - INFO - __main__ -   Adapter Name = eu/wiki@ukp
01/14/2022 16:33:42 - INFO - __main__ -   Language = zh_yue
01/14/2022 16:33:42 - INFO - __main__ -   Adapter Name = zh_yue/wiki@ukp
01/14/2022 16:33:43 - INFO - __main__ -   Using lang2id = None
01/14/2022 16:33:43 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/14/2022 16:33:43 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/
01/14/2022 16:33:43 - INFO - root -   Trying to decide if add adapter
01/14/2022 16:33:43 - INFO - root -   loading task adapter
01/14/2022 16:33:43 - INFO - root -   loading lang adpater en/wiki@ukp,pt/wiki@ukp,id/wiki@ukp,cs/wiki@ukp,tr/wiki@ukp,eu/wiki@ukp,zh_yue/wiki@ukp,vi/wiki@ukp,fr/wiki@ukp,xmf/wiki@ukp
01/14/2022 16:33:43 - INFO - __main__ -   Adapter Languages : ['en', 'pt', 'id', 'cs', 'tr', 'eu', 'zh_yue', 'vi', 'fr', 'xmf'], Length : 10
01/14/2022 16:33:43 - INFO - __main__ -   Adapter Names ['en/wiki@ukp', 'pt/wiki@ukp', 'id/wiki@ukp', 'cs/wiki@ukp', 'tr/wiki@ukp', 'eu/wiki@ukp', 'zh_yue/wiki@ukp', 'vi/wiki@ukp', 'fr/wiki@ukp', 'xmf/wiki@ukp'], Length : 10
01/14/2022 16:33:43 - INFO - __main__ -   Language = en
01/14/2022 16:33:43 - INFO - __main__ -   Adapter Name = en/wiki@ukp
01/14/2022 16:33:44 - INFO - __main__ -   Language = cs
01/14/2022 16:33:44 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
01/14/2022 16:33:44 - INFO - __main__ -   Language = pt
01/14/2022 16:33:44 - INFO - __main__ -   Adapter Name = pt/wiki@ukp
01/14/2022 16:33:45 - INFO - __main__ -   Using lang2id = None
01/14/2022 16:33:45 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/14/2022 16:33:45 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/
01/14/2022 16:33:45 - INFO - root -   Trying to decide if add adapter
01/14/2022 16:33:45 - INFO - root -   loading task adapter
01/14/2022 16:33:45 - INFO - root -   loading lang adpater en/wiki@ukp,pt/wiki@ukp,id/wiki@ukp,tr/wiki@ukp,cs/wiki@ukp,vi/wiki@ukp,eu/wiki@ukp,fa/wiki@ukp,zh_yue/wiki@ukp,ilo/wiki@ukp
01/14/2022 16:33:45 - INFO - __main__ -   Adapter Languages : ['en', 'pt', 'id', 'tr', 'cs', 'vi', 'eu', 'fa', 'zh_yue', 'ilo'], Length : 10
01/14/2022 16:33:45 - INFO - __main__ -   Adapter Names ['en/wiki@ukp', 'pt/wiki@ukp', 'id/wiki@ukp', 'tr/wiki@ukp', 'cs/wiki@ukp', 'vi/wiki@ukp', 'eu/wiki@ukp', 'fa/wiki@ukp', 'zh_yue/wiki@ukp', 'ilo/wiki@ukp'], Length : 10
01/14/2022 16:33:45 - INFO - __main__ -   Language = en
01/14/2022 16:33:45 - INFO - __main__ -   Adapter Name = en/wiki@ukp
01/14/2022 16:33:46 - INFO - __main__ -   Language = id
01/14/2022 16:33:46 - INFO - __main__ -   Adapter Name = id/wiki@ukp
01/14/2022 16:33:46 - INFO - __main__ -   Language = pt
01/14/2022 16:33:46 - INFO - __main__ -   Adapter Name = pt/wiki@ukp
01/14/2022 16:33:46 - INFO - __main__ -   Language = cdo
01/14/2022 16:33:46 - INFO - __main__ -   Adapter Name = cdo/wiki@ukp
01/14/2022 16:33:47 - INFO - __main__ -   Language = cs
01/14/2022 16:33:47 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
01/14/2022 16:33:48 - INFO - __main__ -   Language = id
01/14/2022 16:33:48 - INFO - __main__ -   Adapter Name = id/wiki@ukp
01/14/2022 16:33:49 - INFO - __main__ -   Language = tr
01/14/2022 16:33:49 - INFO - __main__ -   Adapter Name = tr/wiki@ukp
01/14/2022 16:33:49 - INFO - __main__ -   Language = tr
01/14/2022 16:33:49 - INFO - __main__ -   Adapter Name = tr/wiki@ukp
01/14/2022 16:33:50 - INFO - __main__ -   Language = eu
01/14/2022 16:33:50 - INFO - __main__ -   Adapter Name = eu/wiki@ukp
01/14/2022 16:33:51 - INFO - __main__ -   Args Adapter Weight = equal
01/14/2022 16:33:51 - INFO - __main__ -   Adapter Languages = ['en', 'pt', 'id', 'tr', 'vi', 'fa', 'eu', 'zh_yue', 'cs', 'cdo']
01/14/2022 16:33:51 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/14/2022 16:33:51 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/14/2022 16:33:51 - INFO - __main__ -   Length of Adapter Weights = 10
01/14/2022 16:33:51 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/cached_test_cdo_bert-base-multilingual-cased_128
01/14/2022 16:33:51 - INFO - __main__ -   ***** Running evaluation  in cdo *****
01/14/2022 16:33:51 - INFO - __main__ -     Num examples = 101
01/14/2022 16:33:51 - INFO - __main__ -     Batch size = 32
01/14/2022 16:33:51 - INFO - __main__ -   Batch number = 1
01/14/2022 16:33:51 - INFO - __main__ -   Language = cs
01/14/2022 16:33:51 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
01/14/2022 16:33:51 - INFO - __main__ -   Batch number = 2
01/14/2022 16:33:51 - INFO - __main__ -   Batch number = 3
01/14/2022 16:33:52 - INFO - __main__ -   Language = zh_yue
01/14/2022 16:33:52 - INFO - __main__ -   Adapter Name = zh_yue/wiki@ukp
01/14/2022 16:33:52 - INFO - __main__ -   Batch number = 4
01/14/2022 16:33:52 - INFO - __main__ -   ***** Evaluation result  in cdo *****
01/14/2022 16:33:52 - INFO - __main__ -     f1 = 0.14705882352941174
01/14/2022 16:33:52 - INFO - __main__ -     loss = 2.6115408539772034
01/14/2022 16:33:52 - INFO - __main__ -     precision = 0.12987012987012986
01/14/2022 16:33:52 - INFO - __main__ -     recall = 0.1694915254237288
01/14/2022 16:33:53 - INFO - __main__ -   Language = vi
01/14/2022 16:33:53 - INFO - __main__ -   Adapter Name = vi/wiki@ukp
01/14/2022 16:33:53 - INFO - __main__ -   Language = vi
01/14/2022 16:33:53 - INFO - __main__ -   Adapter Name = vi/wiki@ukp
01/14/2022 16:33:54 - INFO - __main__ -   Language = eu
01/14/2022 16:33:54 - INFO - __main__ -   Adapter Name = eu/wiki@ukp
01/14/2022 16:33:55 - INFO - __main__ -   Language = fr
01/14/2022 16:33:55 - INFO - __main__ -   Adapter Name = fr/wiki@ukp
01/14/2022 16:33:56 - INFO - __main__ -   Language = xmf
01/14/2022 16:33:56 - INFO - __main__ -   Adapter Name = xmf/wiki@ukp
01/14/2022 16:33:56 - INFO - __main__ -   Language = fa
01/14/2022 16:33:56 - INFO - __main__ -   Adapter Name = fa/wiki@ukp
01/14/2022 16:33:58 - INFO - __main__ -   Language = zh_yue
01/14/2022 16:33:58 - INFO - __main__ -   Adapter Name = zh_yue/wiki@ukp
01/14/2022 16:33:58 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:33:58 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/14/2022 16:33:58 - INFO - __main__ -   Seed = 1
01/14/2022 16:33:58 - INFO - root -   save model
01/14/2022 16:33:58 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:33:58 - INFO - __main__ -   Loading pretrained model and tokenizer
01/14/2022 16:33:59 - INFO - __main__ -   Language = ilo
01/14/2022 16:33:59 - INFO - __main__ -   Adapter Name = ilo/wiki@ukp
01/14/2022 16:34:00 - INFO - __main__ -   Args Adapter Weight = equal
01/14/2022 16:34:00 - INFO - __main__ -   Adapter Languages = ['en', 'pt', 'id', 'cs', 'tr', 'eu', 'zh_yue', 'vi', 'fr', 'xmf']
01/14/2022 16:34:00 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/14/2022 16:34:00 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/14/2022 16:34:00 - INFO - __main__ -   Length of Adapter Weights = 10
01/14/2022 16:34:00 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/cached_test_xmf_bert-base-multilingual-cased_128
01/14/2022 16:34:00 - INFO - __main__ -   ***** Running evaluation  in xmf *****
01/14/2022 16:34:00 - INFO - __main__ -     Num examples = 100
01/14/2022 16:34:00 - INFO - __main__ -     Batch size = 32
01/14/2022 16:34:00 - INFO - __main__ -   Batch number = 1
01/14/2022 16:34:01 - INFO - __main__ -   Batch number = 2
01/14/2022 16:34:01 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/14/2022 16:34:01 - INFO - __main__ -   Batch number = 3
01/14/2022 16:34:01 - INFO - __main__ -   Batch number = 4
01/14/2022 16:34:01 - INFO - __main__ -   ***** Evaluation result  in xmf *****
01/14/2022 16:34:01 - INFO - __main__ -     f1 = 0.3609022556390977
01/14/2022 16:34:01 - INFO - __main__ -     loss = 3.0546153783798218
01/14/2022 16:34:01 - INFO - __main__ -     precision = 0.3287671232876712
01/14/2022 16:34:01 - INFO - __main__ -     recall = 0.4
01/14/2022 16:34:03 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='xmf', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:34:03 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/14/2022 16:34:03 - INFO - __main__ -   Seed = 2
01/14/2022 16:34:03 - INFO - root -   save model
01/14/2022 16:34:03 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='xmf', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:34:03 - INFO - __main__ -   Loading pretrained model and tokenizer
01/14/2022 16:34:04 - INFO - __main__ -   Args Adapter Weight = equal
01/14/2022 16:34:04 - INFO - __main__ -   Adapter Languages = ['en', 'pt', 'id', 'tr', 'cs', 'vi', 'eu', 'fa', 'zh_yue', 'ilo']
01/14/2022 16:34:04 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/14/2022 16:34:04 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/14/2022 16:34:04 - INFO - __main__ -   Length of Adapter Weights = 10
01/14/2022 16:34:04 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/cached_test_ilo_bert-base-multilingual-cased_128
01/14/2022 16:34:04 - INFO - __main__ -   ***** Running evaluation  in ilo *****
01/14/2022 16:34:04 - INFO - __main__ -     Num examples = 100
01/14/2022 16:34:04 - INFO - __main__ -     Batch size = 32
01/14/2022 16:34:04 - INFO - __main__ -   Batch number = 1
01/14/2022 16:34:04 - INFO - __main__ -   Batch number = 2
01/14/2022 16:34:04 - INFO - __main__ -   Batch number = 3
01/14/2022 16:34:04 - INFO - __main__ -   Batch number = 4
01/14/2022 16:34:04 - INFO - __main__ -   ***** Evaluation result  in ilo *****
01/14/2022 16:34:04 - INFO - __main__ -     f1 = 0.6098654708520179
01/14/2022 16:34:04 - INFO - __main__ -     loss = 2.545504331588745
01/14/2022 16:34:04 - INFO - __main__ -     precision = 0.5666666666666667
01/14/2022 16:34:04 - INFO - __main__ -     recall = 0.6601941747572816
01/14/2022 16:34:06 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/14/2022 16:34:07 - INFO - __main__ -   Using lang2id = None
01/14/2022 16:34:07 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/14/2022 16:34:07 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/
01/14/2022 16:34:07 - INFO - root -   Trying to decide if add adapter
01/14/2022 16:34:07 - INFO - root -   loading task adapter
01/14/2022 16:34:07 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ilo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:34:07 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/14/2022 16:34:07 - INFO - __main__ -   Seed = 3
01/14/2022 16:34:07 - INFO - root -   save model
01/14/2022 16:34:07 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ilo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:34:07 - INFO - __main__ -   Loading pretrained model and tokenizer
01/14/2022 16:34:07 - INFO - root -   loading lang adpater en/wiki@ukp,pt/wiki@ukp,id/wiki@ukp,cs/wiki@ukp,tr/wiki@ukp,eu/wiki@ukp,zh_yue/wiki@ukp,vi/wiki@ukp,fr/wiki@ukp,mi/wiki@ukp
01/14/2022 16:34:07 - INFO - __main__ -   Adapter Languages : ['en', 'pt', 'id', 'cs', 'tr', 'eu', 'zh_yue', 'vi', 'fr', 'mi'], Length : 10
01/14/2022 16:34:07 - INFO - __main__ -   Adapter Names ['en/wiki@ukp', 'pt/wiki@ukp', 'id/wiki@ukp', 'cs/wiki@ukp', 'tr/wiki@ukp', 'eu/wiki@ukp', 'zh_yue/wiki@ukp', 'vi/wiki@ukp', 'fr/wiki@ukp', 'mi/wiki@ukp'], Length : 10
01/14/2022 16:34:07 - INFO - __main__ -   Language = en
01/14/2022 16:34:07 - INFO - __main__ -   Adapter Name = en/wiki@ukp
01/14/2022 16:34:07 - INFO - __main__ -   Language = pt
01/14/2022 16:34:07 - INFO - __main__ -   Adapter Name = pt/wiki@ukp
01/14/2022 16:34:09 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/14/2022 16:34:10 - INFO - __main__ -   Language = id
01/14/2022 16:34:10 - INFO - __main__ -   Adapter Name = id/wiki@ukp
01/14/2022 16:34:12 - INFO - __main__ -   Using lang2id = None
01/14/2022 16:34:12 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/14/2022 16:34:12 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/
01/14/2022 16:34:12 - INFO - root -   Trying to decide if add adapter
01/14/2022 16:34:12 - INFO - root -   loading task adapter
01/14/2022 16:34:12 - INFO - __main__ -   Language = cs
01/14/2022 16:34:12 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
01/14/2022 16:34:12 - INFO - root -   loading lang adpater en/wiki@ukp,pt/wiki@ukp,id/wiki@ukp,tr/wiki@ukp,cs/wiki@ukp,vi/wiki@ukp,eu/wiki@ukp,fa/wiki@ukp,zh_yue/wiki@ukp,xmf/wiki@ukp
01/14/2022 16:34:12 - INFO - __main__ -   Adapter Languages : ['en', 'pt', 'id', 'tr', 'cs', 'vi', 'eu', 'fa', 'zh_yue', 'xmf'], Length : 10
01/14/2022 16:34:12 - INFO - __main__ -   Adapter Names ['en/wiki@ukp', 'pt/wiki@ukp', 'id/wiki@ukp', 'tr/wiki@ukp', 'cs/wiki@ukp', 'vi/wiki@ukp', 'eu/wiki@ukp', 'fa/wiki@ukp', 'zh_yue/wiki@ukp', 'xmf/wiki@ukp'], Length : 10
01/14/2022 16:34:12 - INFO - __main__ -   Language = en
01/14/2022 16:34:12 - INFO - __main__ -   Adapter Name = en/wiki@ukp
01/14/2022 16:34:13 - INFO - __main__ -   Language = pt
01/14/2022 16:34:13 - INFO - __main__ -   Adapter Name = pt/wiki@ukp
01/14/2022 16:34:14 - INFO - __main__ -   Language = tr
01/14/2022 16:34:14 - INFO - __main__ -   Adapter Name = tr/wiki@ukp
01/14/2022 16:34:15 - INFO - __main__ -   Language = id
01/14/2022 16:34:15 - INFO - __main__ -   Adapter Name = id/wiki@ukp
01/14/2022 16:34:15 - INFO - __main__ -   Using lang2id = None
01/14/2022 16:34:15 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/14/2022 16:34:15 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/
01/14/2022 16:34:15 - INFO - root -   Trying to decide if add adapter
01/14/2022 16:34:15 - INFO - root -   loading task adapter
01/14/2022 16:34:15 - INFO - root -   loading lang adpater en/wiki@ukp,pt/wiki@ukp,id/wiki@ukp,tr/wiki@ukp,vi/wiki@ukp,fa/wiki@ukp,eu/wiki@ukp,zh_yue/wiki@ukp,cs/wiki@ukp,ilo/wiki@ukp
01/14/2022 16:34:15 - INFO - __main__ -   Adapter Languages : ['en', 'pt', 'id', 'tr', 'vi', 'fa', 'eu', 'zh_yue', 'cs', 'ilo'], Length : 10
01/14/2022 16:34:15 - INFO - __main__ -   Adapter Names ['en/wiki@ukp', 'pt/wiki@ukp', 'id/wiki@ukp', 'tr/wiki@ukp', 'vi/wiki@ukp', 'fa/wiki@ukp', 'eu/wiki@ukp', 'zh_yue/wiki@ukp', 'cs/wiki@ukp', 'ilo/wiki@ukp'], Length : 10
01/14/2022 16:34:15 - INFO - __main__ -   Language = en
01/14/2022 16:34:15 - INFO - __main__ -   Adapter Name = en/wiki@ukp
01/14/2022 16:34:16 - INFO - __main__ -   Language = pt
01/14/2022 16:34:16 - INFO - __main__ -   Adapter Name = pt/wiki@ukp
01/14/2022 16:34:16 - INFO - __main__ -   Language = eu
01/14/2022 16:34:16 - INFO - __main__ -   Adapter Name = eu/wiki@ukp
01/14/2022 16:34:17 - INFO - __main__ -   Language = tr
01/14/2022 16:34:17 - INFO - __main__ -   Adapter Name = tr/wiki@ukp
01/14/2022 16:34:18 - INFO - __main__ -   Language = id
01/14/2022 16:34:18 - INFO - __main__ -   Adapter Name = id/wiki@ukp
01/14/2022 16:34:18 - INFO - __main__ -   Language = zh_yue
01/14/2022 16:34:18 - INFO - __main__ -   Adapter Name = zh_yue/wiki@ukp
01/14/2022 16:34:19 - INFO - __main__ -   Language = cs
01/14/2022 16:34:19 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
01/14/2022 16:34:20 - INFO - __main__ -   Language = tr
01/14/2022 16:34:20 - INFO - __main__ -   Adapter Name = tr/wiki@ukp
01/14/2022 16:34:21 - INFO - __main__ -   Language = vi
01/14/2022 16:34:21 - INFO - __main__ -   Adapter Name = vi/wiki@ukp
01/14/2022 16:34:21 - INFO - __main__ -   Language = vi
01/14/2022 16:34:21 - INFO - __main__ -   Adapter Name = vi/wiki@ukp
01/14/2022 16:34:23 - INFO - __main__ -   Language = vi
01/14/2022 16:34:23 - INFO - __main__ -   Adapter Name = vi/wiki@ukp
01/14/2022 16:34:23 - INFO - __main__ -   Language = fr
01/14/2022 16:34:23 - INFO - __main__ -   Adapter Name = fr/wiki@ukp
01/14/2022 16:34:23 - INFO - __main__ -   Language = eu
01/14/2022 16:34:23 - INFO - __main__ -   Adapter Name = eu/wiki@ukp
01/14/2022 16:34:24 - INFO - __main__ -   Language = mi
01/14/2022 16:34:24 - INFO - __main__ -   Adapter Name = mi/wiki@ukp
01/14/2022 16:34:25 - INFO - __main__ -   Language = fa
01/14/2022 16:34:25 - INFO - __main__ -   Adapter Name = fa/wiki@ukp
01/14/2022 16:34:26 - INFO - __main__ -   Language = fa
01/14/2022 16:34:26 - INFO - __main__ -   Adapter Name = fa/wiki@ukp
01/14/2022 16:34:27 - INFO - __main__ -   Language = eu
01/14/2022 16:34:27 - INFO - __main__ -   Adapter Name = eu/wiki@ukp
01/14/2022 16:34:28 - INFO - __main__ -   Language = zh_yue
01/14/2022 16:34:28 - INFO - __main__ -   Adapter Name = zh_yue/wiki@ukp
01/14/2022 16:34:28 - INFO - __main__ -   Args Adapter Weight = equal
01/14/2022 16:34:28 - INFO - __main__ -   Adapter Languages = ['en', 'pt', 'id', 'cs', 'tr', 'eu', 'zh_yue', 'vi', 'fr', 'mi']
01/14/2022 16:34:28 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/14/2022 16:34:28 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/14/2022 16:34:28 - INFO - __main__ -   Length of Adapter Weights = 10
01/14/2022 16:34:28 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/cached_test_mi_bert-base-multilingual-cased_128
01/14/2022 16:34:28 - INFO - __main__ -   ***** Running evaluation  in mi *****
01/14/2022 16:34:28 - INFO - __main__ -     Num examples = 100
01/14/2022 16:34:28 - INFO - __main__ -     Batch size = 32
01/14/2022 16:34:28 - INFO - __main__ -   Batch number = 1
01/14/2022 16:34:29 - INFO - __main__ -   Batch number = 2
01/14/2022 16:34:29 - INFO - __main__ -   Batch number = 3
01/14/2022 16:34:29 - INFO - __main__ -   Language = zh_yue
01/14/2022 16:34:29 - INFO - __main__ -   Adapter Name = zh_yue/wiki@ukp
01/14/2022 16:34:29 - INFO - __main__ -   Batch number = 4
01/14/2022 16:34:29 - INFO - __main__ -   ***** Evaluation result  in mi *****
01/14/2022 16:34:29 - INFO - __main__ -     f1 = 0.251685393258427
01/14/2022 16:34:29 - INFO - __main__ -     loss = 3.8612669706344604
01/14/2022 16:34:29 - INFO - __main__ -     precision = 0.2
01/14/2022 16:34:29 - INFO - __main__ -     recall = 0.3393939393939394
01/14/2022 16:34:30 - INFO - __main__ -   Language = xmf
01/14/2022 16:34:30 - INFO - __main__ -   Adapter Name = xmf/wiki@ukp
01/14/2022 16:34:31 - INFO - __main__ -   Language = cs
01/14/2022 16:34:31 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
01/14/2022 16:34:32 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:34:32 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/14/2022 16:34:32 - INFO - __main__ -   Seed = 2
01/14/2022 16:34:32 - INFO - root -   save model
01/14/2022 16:34:32 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:34:32 - INFO - __main__ -   Loading pretrained model and tokenizer
01/14/2022 16:34:34 - INFO - __main__ -   Language = ilo
01/14/2022 16:34:34 - INFO - __main__ -   Adapter Name = ilo/wiki@ukp
01/14/2022 16:34:34 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/14/2022 16:34:35 - INFO - __main__ -   Args Adapter Weight = equal
01/14/2022 16:34:35 - INFO - __main__ -   Adapter Languages = ['en', 'pt', 'id', 'tr', 'cs', 'vi', 'eu', 'fa', 'zh_yue', 'xmf']
01/14/2022 16:34:35 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/14/2022 16:34:35 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/14/2022 16:34:35 - INFO - __main__ -   Length of Adapter Weights = 10
01/14/2022 16:34:35 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/cached_test_xmf_bert-base-multilingual-cased_128
01/14/2022 16:34:35 - INFO - __main__ -   ***** Running evaluation  in xmf *****
01/14/2022 16:34:35 - INFO - __main__ -     Num examples = 100
01/14/2022 16:34:35 - INFO - __main__ -     Batch size = 32
01/14/2022 16:34:35 - INFO - __main__ -   Batch number = 1
01/14/2022 16:34:35 - INFO - __main__ -   Batch number = 2
01/14/2022 16:34:35 - INFO - __main__ -   Batch number = 3
01/14/2022 16:34:36 - INFO - __main__ -   Batch number = 4
01/14/2022 16:34:36 - INFO - __main__ -   ***** Evaluation result  in xmf *****
01/14/2022 16:34:36 - INFO - __main__ -     f1 = 0.339622641509434
01/14/2022 16:34:36 - INFO - __main__ -     loss = 4.5591830015182495
01/14/2022 16:34:36 - INFO - __main__ -     precision = 0.2727272727272727
01/14/2022 16:34:36 - INFO - __main__ -     recall = 0.45
01/14/2022 16:34:38 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='xmf', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:34:38 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/14/2022 16:34:38 - INFO - __main__ -   Seed = 3
01/14/2022 16:34:38 - INFO - root -   save model
01/14/2022 16:34:38 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='xmf', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:34:38 - INFO - __main__ -   Loading pretrained model and tokenizer
01/14/2022 16:34:38 - INFO - __main__ -   Args Adapter Weight = equal
01/14/2022 16:34:38 - INFO - __main__ -   Adapter Languages = ['en', 'pt', 'id', 'tr', 'vi', 'fa', 'eu', 'zh_yue', 'cs', 'ilo']
01/14/2022 16:34:38 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/14/2022 16:34:38 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/14/2022 16:34:38 - INFO - __main__ -   Length of Adapter Weights = 10
01/14/2022 16:34:38 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/cached_test_ilo_bert-base-multilingual-cased_128
01/14/2022 16:34:38 - INFO - __main__ -   ***** Running evaluation  in ilo *****
01/14/2022 16:34:38 - INFO - __main__ -     Num examples = 100
01/14/2022 16:34:38 - INFO - __main__ -     Batch size = 32
01/14/2022 16:34:38 - INFO - __main__ -   Batch number = 1
01/14/2022 16:34:38 - INFO - __main__ -   Batch number = 2
01/14/2022 16:34:39 - INFO - __main__ -   Batch number = 3
01/14/2022 16:34:39 - INFO - __main__ -   Batch number = 4
01/14/2022 16:34:39 - INFO - __main__ -   ***** Evaluation result  in ilo *****
01/14/2022 16:34:39 - INFO - __main__ -     f1 = 0.6216216216216217
01/14/2022 16:34:39 - INFO - __main__ -     loss = 2.8508920669555664
01/14/2022 16:34:39 - INFO - __main__ -     precision = 0.5798319327731093
01/14/2022 16:34:39 - INFO - __main__ -     recall = 0.6699029126213593
01/14/2022 16:34:40 - INFO - __main__ -   Using lang2id = None
01/14/2022 16:34:40 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/14/2022 16:34:40 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/
01/14/2022 16:34:40 - INFO - root -   Trying to decide if add adapter
01/14/2022 16:34:40 - INFO - root -   loading task adapter
01/14/2022 16:34:40 - INFO - root -   loading lang adpater en/wiki@ukp,pt/wiki@ukp,id/wiki@ukp,tr/wiki@ukp,cs/wiki@ukp,vi/wiki@ukp,eu/wiki@ukp,fa/wiki@ukp,zh_yue/wiki@ukp,mi/wiki@ukp
01/14/2022 16:34:40 - INFO - __main__ -   Adapter Languages : ['en', 'pt', 'id', 'tr', 'cs', 'vi', 'eu', 'fa', 'zh_yue', 'mi'], Length : 10
01/14/2022 16:34:40 - INFO - __main__ -   Adapter Names ['en/wiki@ukp', 'pt/wiki@ukp', 'id/wiki@ukp', 'tr/wiki@ukp', 'cs/wiki@ukp', 'vi/wiki@ukp', 'eu/wiki@ukp', 'fa/wiki@ukp', 'zh_yue/wiki@ukp', 'mi/wiki@ukp'], Length : 10
01/14/2022 16:34:40 - INFO - __main__ -   Language = en
01/14/2022 16:34:40 - INFO - __main__ -   Adapter Name = en/wiki@ukp
01/14/2022 16:34:41 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/14/2022 16:34:41 - INFO - __main__ -   Language = pt
01/14/2022 16:34:41 - INFO - __main__ -   Adapter Name = pt/wiki@ukp
01/14/2022 16:34:43 - INFO - __main__ -   Language = id
01/14/2022 16:34:43 - INFO - __main__ -   Adapter Name = id/wiki@ukp
01/14/2022 16:34:46 - INFO - __main__ -   Language = tr
01/14/2022 16:34:46 - INFO - __main__ -   Adapter Name = tr/wiki@ukp
01/14/2022 16:34:47 - INFO - __main__ -   Using lang2id = None
01/14/2022 16:34:47 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/14/2022 16:34:47 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/
01/14/2022 16:34:47 - INFO - root -   Trying to decide if add adapter
01/14/2022 16:34:47 - INFO - root -   loading task adapter
01/14/2022 16:34:47 - INFO - root -   loading lang adpater en/wiki@ukp,pt/wiki@ukp,id/wiki@ukp,tr/wiki@ukp,vi/wiki@ukp,fa/wiki@ukp,eu/wiki@ukp,zh_yue/wiki@ukp,cs/wiki@ukp,xmf/wiki@ukp
01/14/2022 16:34:47 - INFO - __main__ -   Adapter Languages : ['en', 'pt', 'id', 'tr', 'vi', 'fa', 'eu', 'zh_yue', 'cs', 'xmf'], Length : 10
01/14/2022 16:34:47 - INFO - __main__ -   Adapter Names ['en/wiki@ukp', 'pt/wiki@ukp', 'id/wiki@ukp', 'tr/wiki@ukp', 'vi/wiki@ukp', 'fa/wiki@ukp', 'eu/wiki@ukp', 'zh_yue/wiki@ukp', 'cs/wiki@ukp', 'xmf/wiki@ukp'], Length : 10
01/14/2022 16:34:47 - INFO - __main__ -   Language = en
01/14/2022 16:34:47 - INFO - __main__ -   Adapter Name = en/wiki@ukp
01/14/2022 16:34:48 - INFO - __main__ -   Language = pt
01/14/2022 16:34:48 - INFO - __main__ -   Adapter Name = pt/wiki@ukp
01/14/2022 16:34:48 - INFO - __main__ -   Language = cs
01/14/2022 16:34:48 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
01/14/2022 16:34:49 - INFO - __main__ -   Language = id
01/14/2022 16:34:49 - INFO - __main__ -   Adapter Name = id/wiki@ukp
01/14/2022 16:34:50 - INFO - __main__ -   Language = vi
01/14/2022 16:34:50 - INFO - __main__ -   Adapter Name = vi/wiki@ukp
01/14/2022 16:34:51 - INFO - __main__ -   Language = tr
01/14/2022 16:34:51 - INFO - __main__ -   Adapter Name = tr/wiki@ukp
01/14/2022 16:34:52 - INFO - __main__ -   Language = vi
01/14/2022 16:34:52 - INFO - __main__ -   Adapter Name = vi/wiki@ukp
01/14/2022 16:34:52 - INFO - __main__ -   Language = eu
01/14/2022 16:34:52 - INFO - __main__ -   Adapter Name = eu/wiki@ukp
01/14/2022 16:34:53 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mhr', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:34:53 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/14/2022 16:34:53 - INFO - __main__ -   Seed = 1
01/14/2022 16:34:53 - INFO - root -   save model
01/14/2022 16:34:53 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mhr', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:34:53 - INFO - __main__ -   Loading pretrained model and tokenizer
01/14/2022 16:34:54 - INFO - __main__ -   Language = fa
01/14/2022 16:34:54 - INFO - __main__ -   Adapter Name = fa/wiki@ukp
01/14/2022 16:34:54 - INFO - __main__ -   Language = fa
01/14/2022 16:34:54 - INFO - __main__ -   Adapter Name = fa/wiki@ukp
01/14/2022 16:34:55 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/14/2022 16:34:55 - INFO - __main__ -   Language = eu
01/14/2022 16:34:55 - INFO - __main__ -   Adapter Name = eu/wiki@ukp
01/14/2022 16:34:56 - INFO - __main__ -   Language = zh_yue
01/14/2022 16:34:56 - INFO - __main__ -   Adapter Name = zh_yue/wiki@ukp
01/14/2022 16:34:57 - INFO - __main__ -   Language = zh_yue
01/14/2022 16:34:57 - INFO - __main__ -   Adapter Name = zh_yue/wiki@ukp
01/14/2022 16:34:59 - INFO - __main__ -   Language = cs
01/14/2022 16:34:59 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
01/14/2022 16:34:59 - INFO - __main__ -   Language = mi
01/14/2022 16:34:59 - INFO - __main__ -   Adapter Name = mi/wiki@ukp
01/14/2022 16:35:00 - INFO - __main__ -   Language = xmf
01/14/2022 16:35:00 - INFO - __main__ -   Adapter Name = xmf/wiki@ukp
01/14/2022 16:35:01 - INFO - __main__ -   Using lang2id = None
01/14/2022 16:35:01 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/14/2022 16:35:01 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/
01/14/2022 16:35:01 - INFO - root -   Trying to decide if add adapter
01/14/2022 16:35:01 - INFO - root -   loading task adapter
01/14/2022 16:35:01 - INFO - root -   loading lang adpater en/wiki@ukp,pt/wiki@ukp,id/wiki@ukp,cs/wiki@ukp,tr/wiki@ukp,eu/wiki@ukp,zh_yue/wiki@ukp,vi/wiki@ukp,fr/wiki@ukp,mhr/wiki@ukp
01/14/2022 16:35:01 - INFO - __main__ -   Adapter Languages : ['en', 'pt', 'id', 'cs', 'tr', 'eu', 'zh_yue', 'vi', 'fr', 'mhr'], Length : 10
01/14/2022 16:35:01 - INFO - __main__ -   Adapter Names ['en/wiki@ukp', 'pt/wiki@ukp', 'id/wiki@ukp', 'cs/wiki@ukp', 'tr/wiki@ukp', 'eu/wiki@ukp', 'zh_yue/wiki@ukp', 'vi/wiki@ukp', 'fr/wiki@ukp', 'mhr/wiki@ukp'], Length : 10
01/14/2022 16:35:01 - INFO - __main__ -   Language = en
01/14/2022 16:35:01 - INFO - __main__ -   Adapter Name = en/wiki@ukp
01/14/2022 16:35:02 - INFO - __main__ -   Language = pt
01/14/2022 16:35:02 - INFO - __main__ -   Adapter Name = pt/wiki@ukp
01/14/2022 16:35:03 - INFO - __main__ -   Args Adapter Weight = equal
01/14/2022 16:35:03 - INFO - __main__ -   Adapter Languages = ['en', 'pt', 'id', 'tr', 'cs', 'vi', 'eu', 'fa', 'zh_yue', 'mi']
01/14/2022 16:35:03 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/14/2022 16:35:03 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/14/2022 16:35:03 - INFO - __main__ -   Length of Adapter Weights = 10
01/14/2022 16:35:03 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/cached_test_mi_bert-base-multilingual-cased_128
01/14/2022 16:35:03 - INFO - __main__ -   ***** Running evaluation  in mi *****
01/14/2022 16:35:03 - INFO - __main__ -     Num examples = 100
01/14/2022 16:35:03 - INFO - __main__ -     Batch size = 32
01/14/2022 16:35:03 - INFO - __main__ -   Batch number = 1
01/14/2022 16:35:03 - INFO - __main__ -   Batch number = 2
01/14/2022 16:35:04 - INFO - __main__ -   Batch number = 3
01/14/2022 16:35:04 - INFO - __main__ -   Batch number = 4
01/14/2022 16:35:04 - INFO - __main__ -   ***** Evaluation result  in mi *****
01/14/2022 16:35:04 - INFO - __main__ -     f1 = 0.10194174757281552
01/14/2022 16:35:04 - INFO - __main__ -     loss = 7.7254356145858765
01/14/2022 16:35:04 - INFO - __main__ -     precision = 0.08502024291497975
01/14/2022 16:35:04 - INFO - __main__ -     recall = 0.12727272727272726
01/14/2022 16:35:04 - INFO - __main__ -   Args Adapter Weight = equal
01/14/2022 16:35:04 - INFO - __main__ -   Adapter Languages = ['en', 'pt', 'id', 'tr', 'vi', 'fa', 'eu', 'zh_yue', 'cs', 'xmf']
01/14/2022 16:35:04 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/14/2022 16:35:04 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/14/2022 16:35:04 - INFO - __main__ -   Length of Adapter Weights = 10
01/14/2022 16:35:04 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/cached_test_xmf_bert-base-multilingual-cased_128
01/14/2022 16:35:04 - INFO - __main__ -   ***** Running evaluation  in xmf *****
01/14/2022 16:35:04 - INFO - __main__ -     Num examples = 100
01/14/2022 16:35:04 - INFO - __main__ -     Batch size = 32
01/14/2022 16:35:04 - INFO - __main__ -   Batch number = 1
01/14/2022 16:35:05 - INFO - __main__ -   Language = id
01/14/2022 16:35:05 - INFO - __main__ -   Adapter Name = id/wiki@ukp
01/14/2022 16:35:05 - INFO - __main__ -   Batch number = 2
01/14/2022 16:35:05 - INFO - __main__ -   Batch number = 3
01/14/2022 16:35:05 - INFO - __main__ -   Batch number = 4
01/14/2022 16:35:05 - INFO - __main__ -   ***** Evaluation result  in xmf *****
01/14/2022 16:35:05 - INFO - __main__ -     f1 = 0.2645161290322581
01/14/2022 16:35:05 - INFO - __main__ -     loss = 5.107606291770935
01/14/2022 16:35:05 - INFO - __main__ -     precision = 0.21578947368421053
01/14/2022 16:35:05 - INFO - __main__ -     recall = 0.3416666666666667
01/14/2022 16:35:06 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:35:06 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/14/2022 16:35:06 - INFO - __main__ -   Seed = 3
01/14/2022 16:35:06 - INFO - root -   save model
01/14/2022 16:35:06 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:35:06 - INFO - __main__ -   Loading pretrained model and tokenizer
01/14/2022 16:35:07 - INFO - __main__ -   Language = cs
01/14/2022 16:35:07 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
01/14/2022 16:35:09 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/14/2022 16:35:09 - INFO - __main__ -   Language = tr
01/14/2022 16:35:09 - INFO - __main__ -   Adapter Name = tr/wiki@ukp
01/14/2022 16:35:11 - INFO - __main__ -   Language = eu
01/14/2022 16:35:11 - INFO - __main__ -   Adapter Name = eu/wiki@ukp
01/14/2022 16:35:14 - INFO - __main__ -   Language = zh_yue
01/14/2022 16:35:14 - INFO - __main__ -   Adapter Name = zh_yue/wiki@ukp
01/14/2022 16:35:14 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='tk', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:35:14 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/14/2022 16:35:14 - INFO - __main__ -   Seed = 1
01/14/2022 16:35:14 - INFO - root -   save model
01/14/2022 16:35:14 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='tk', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:35:14 - INFO - __main__ -   Loading pretrained model and tokenizer
01/14/2022 16:35:15 - INFO - __main__ -   Using lang2id = None
01/14/2022 16:35:15 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/14/2022 16:35:15 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/
01/14/2022 16:35:15 - INFO - root -   Trying to decide if add adapter
01/14/2022 16:35:15 - INFO - root -   loading task adapter
01/14/2022 16:35:15 - INFO - root -   loading lang adpater en/wiki@ukp,pt/wiki@ukp,id/wiki@ukp,tr/wiki@ukp,vi/wiki@ukp,fa/wiki@ukp,eu/wiki@ukp,zh_yue/wiki@ukp,cs/wiki@ukp,mi/wiki@ukp
01/14/2022 16:35:15 - INFO - __main__ -   Adapter Languages : ['en', 'pt', 'id', 'tr', 'vi', 'fa', 'eu', 'zh_yue', 'cs', 'mi'], Length : 10
01/14/2022 16:35:15 - INFO - __main__ -   Adapter Names ['en/wiki@ukp', 'pt/wiki@ukp', 'id/wiki@ukp', 'tr/wiki@ukp', 'vi/wiki@ukp', 'fa/wiki@ukp', 'eu/wiki@ukp', 'zh_yue/wiki@ukp', 'cs/wiki@ukp', 'mi/wiki@ukp'], Length : 10
01/14/2022 16:35:15 - INFO - __main__ -   Language = en
01/14/2022 16:35:15 - INFO - __main__ -   Adapter Name = en/wiki@ukp
01/14/2022 16:35:16 - INFO - __main__ -   Language = vi
01/14/2022 16:35:16 - INFO - __main__ -   Adapter Name = vi/wiki@ukp
01/14/2022 16:35:16 - INFO - __main__ -   Language = pt
01/14/2022 16:35:16 - INFO - __main__ -   Adapter Name = pt/wiki@ukp
01/14/2022 16:35:17 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/14/2022 16:35:18 - INFO - __main__ -   Language = id
01/14/2022 16:35:18 - INFO - __main__ -   Adapter Name = id/wiki@ukp
01/14/2022 16:35:18 - INFO - __main__ -   Language = fr
01/14/2022 16:35:18 - INFO - __main__ -   Adapter Name = fr/wiki@ukp
01/14/2022 16:35:19 - INFO - __main__ -   Language = mhr
01/14/2022 16:35:19 - INFO - __main__ -   Adapter Name = mhr/wiki@ukp
01/14/2022 16:35:20 - INFO - __main__ -   Language = tr
01/14/2022 16:35:20 - INFO - __main__ -   Adapter Name = tr/wiki@ukp
01/14/2022 16:35:21 - INFO - __main__ -   Language = vi
01/14/2022 16:35:21 - INFO - __main__ -   Adapter Name = vi/wiki@ukp
01/14/2022 16:35:23 - INFO - __main__ -   Using lang2id = None
01/14/2022 16:35:23 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/14/2022 16:35:23 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/
01/14/2022 16:35:23 - INFO - root -   Trying to decide if add adapter
01/14/2022 16:35:23 - INFO - root -   loading task adapter
01/14/2022 16:35:23 - INFO - root -   loading lang adpater en/wiki@ukp,pt/wiki@ukp,id/wiki@ukp,cs/wiki@ukp,tr/wiki@ukp,eu/wiki@ukp,zh_yue/wiki@ukp,vi/wiki@ukp,fr/wiki@ukp,tk/wiki@ukp
01/14/2022 16:35:23 - INFO - __main__ -   Adapter Languages : ['en', 'pt', 'id', 'cs', 'tr', 'eu', 'zh_yue', 'vi', 'fr', 'tk'], Length : 10
01/14/2022 16:35:23 - INFO - __main__ -   Adapter Names ['en/wiki@ukp', 'pt/wiki@ukp', 'id/wiki@ukp', 'cs/wiki@ukp', 'tr/wiki@ukp', 'eu/wiki@ukp', 'zh_yue/wiki@ukp', 'vi/wiki@ukp', 'fr/wiki@ukp', 'tk/wiki@ukp'], Length : 10
01/14/2022 16:35:23 - INFO - __main__ -   Language = en
01/14/2022 16:35:23 - INFO - __main__ -   Adapter Name = en/wiki@ukp
01/14/2022 16:35:23 - INFO - __main__ -   Language = fa
01/14/2022 16:35:23 - INFO - __main__ -   Adapter Name = fa/wiki@ukp
01/14/2022 16:35:24 - INFO - __main__ -   Language = pt
01/14/2022 16:35:24 - INFO - __main__ -   Adapter Name = pt/wiki@ukp
01/14/2022 16:35:24 - INFO - __main__ -   Args Adapter Weight = equal
01/14/2022 16:35:24 - INFO - __main__ -   Adapter Languages = ['en', 'pt', 'id', 'cs', 'tr', 'eu', 'zh_yue', 'vi', 'fr', 'mhr']
01/14/2022 16:35:24 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/14/2022 16:35:24 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/14/2022 16:35:24 - INFO - __main__ -   Length of Adapter Weights = 10
01/14/2022 16:35:24 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/cached_test_mhr_bert-base-multilingual-cased_128
01/14/2022 16:35:24 - INFO - __main__ -   ***** Running evaluation  in mhr *****
01/14/2022 16:35:24 - INFO - __main__ -     Num examples = 100
01/14/2022 16:35:24 - INFO - __main__ -     Batch size = 32
01/14/2022 16:35:24 - INFO - __main__ -   Batch number = 1
01/14/2022 16:35:24 - INFO - __main__ -   Batch number = 2
01/14/2022 16:35:24 - INFO - __main__ -   Language = eu
01/14/2022 16:35:24 - INFO - __main__ -   Adapter Name = eu/wiki@ukp
01/14/2022 16:35:25 - INFO - __main__ -   Batch number = 3
01/14/2022 16:35:25 - INFO - __main__ -   Batch number = 4
01/14/2022 16:35:25 - INFO - __main__ -   ***** Evaluation result  in mhr *****
01/14/2022 16:35:25 - INFO - __main__ -     f1 = 0.462962962962963
01/14/2022 16:35:25 - INFO - __main__ -     loss = 2.1440657675266266
01/14/2022 16:35:25 - INFO - __main__ -     precision = 0.47619047619047616
01/14/2022 16:35:25 - INFO - __main__ -     recall = 0.45045045045045046
01/14/2022 16:35:26 - INFO - __main__ -   Language = id
01/14/2022 16:35:26 - INFO - __main__ -   Adapter Name = id/wiki@ukp
01/14/2022 16:35:26 - INFO - __main__ -   Language = zh_yue
01/14/2022 16:35:26 - INFO - __main__ -   Adapter Name = zh_yue/wiki@ukp
01/14/2022 16:35:27 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mhr', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:35:27 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/14/2022 16:35:27 - INFO - __main__ -   Seed = 2
01/14/2022 16:35:27 - INFO - root -   save model
01/14/2022 16:35:27 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mhr', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:35:27 - INFO - __main__ -   Loading pretrained model and tokenizer
01/14/2022 16:35:27 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='gn', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:35:27 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/14/2022 16:35:27 - INFO - __main__ -   Seed = 1
01/14/2022 16:35:27 - INFO - root -   save model
01/14/2022 16:35:27 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='gn', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:35:27 - INFO - __main__ -   Loading pretrained model and tokenizer
01/14/2022 16:35:28 - INFO - __main__ -   Language = cs
01/14/2022 16:35:28 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
01/14/2022 16:35:28 - INFO - __main__ -   Language = cs
01/14/2022 16:35:28 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
01/14/2022 16:35:30 - INFO - __main__ -   Language = mi
01/14/2022 16:35:30 - INFO - __main__ -   Adapter Name = mi/wiki@ukp
01/14/2022 16:35:30 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/14/2022 16:35:30 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/14/2022 16:35:30 - INFO - __main__ -   Language = tr
01/14/2022 16:35:30 - INFO - __main__ -   Adapter Name = tr/wiki@ukp
01/14/2022 16:35:33 - INFO - __main__ -   Language = eu
01/14/2022 16:35:33 - INFO - __main__ -   Adapter Name = eu/wiki@ukp
01/14/2022 16:35:34 - INFO - __main__ -   Args Adapter Weight = equal
01/14/2022 16:35:34 - INFO - __main__ -   Adapter Languages = ['en', 'pt', 'id', 'tr', 'vi', 'fa', 'eu', 'zh_yue', 'cs', 'mi']
01/14/2022 16:35:34 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/14/2022 16:35:34 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/14/2022 16:35:34 - INFO - __main__ -   Length of Adapter Weights = 10
01/14/2022 16:35:34 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/cached_test_mi_bert-base-multilingual-cased_128
01/14/2022 16:35:34 - INFO - __main__ -   ***** Running evaluation  in mi *****
01/14/2022 16:35:34 - INFO - __main__ -     Num examples = 100
01/14/2022 16:35:34 - INFO - __main__ -     Batch size = 32
01/14/2022 16:35:34 - INFO - __main__ -   Batch number = 1
01/14/2022 16:35:34 - INFO - __main__ -   Batch number = 2
01/14/2022 16:35:35 - INFO - __main__ -   Batch number = 3
01/14/2022 16:35:35 - INFO - __main__ -   Batch number = 4
01/14/2022 16:35:35 - INFO - __main__ -   Language = zh_yue
01/14/2022 16:35:35 - INFO - __main__ -   Adapter Name = zh_yue/wiki@ukp
01/14/2022 16:35:35 - INFO - __main__ -   ***** Evaluation result  in mi *****
01/14/2022 16:35:35 - INFO - __main__ -     f1 = 0.22222222222222224
01/14/2022 16:35:35 - INFO - __main__ -     loss = 3.913669526576996
01/14/2022 16:35:35 - INFO - __main__ -     precision = 0.16379310344827586
01/14/2022 16:35:35 - INFO - __main__ -     recall = 0.34545454545454546
01/14/2022 16:35:36 - INFO - __main__ -   Using lang2id = None
01/14/2022 16:35:36 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/14/2022 16:35:36 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/
01/14/2022 16:35:36 - INFO - root -   Trying to decide if add adapter
01/14/2022 16:35:36 - INFO - root -   loading task adapter
01/14/2022 16:35:36 - INFO - root -   loading lang adpater en/wiki@ukp,pt/wiki@ukp,id/wiki@ukp,tr/wiki@ukp,cs/wiki@ukp,vi/wiki@ukp,eu/wiki@ukp,fa/wiki@ukp,zh_yue/wiki@ukp,mhr/wiki@ukp
01/14/2022 16:35:36 - INFO - __main__ -   Adapter Languages : ['en', 'pt', 'id', 'tr', 'cs', 'vi', 'eu', 'fa', 'zh_yue', 'mhr'], Length : 10
01/14/2022 16:35:36 - INFO - __main__ -   Adapter Names ['en/wiki@ukp', 'pt/wiki@ukp', 'id/wiki@ukp', 'tr/wiki@ukp', 'cs/wiki@ukp', 'vi/wiki@ukp', 'eu/wiki@ukp', 'fa/wiki@ukp', 'zh_yue/wiki@ukp', 'mhr/wiki@ukp'], Length : 10
01/14/2022 16:35:36 - INFO - __main__ -   Language = en
01/14/2022 16:35:36 - INFO - __main__ -   Adapter Name = en/wiki@ukp
01/14/2022 16:35:36 - INFO - __main__ -   Using lang2id = None
01/14/2022 16:35:36 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/14/2022 16:35:36 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/
01/14/2022 16:35:36 - INFO - root -   Trying to decide if add adapter
01/14/2022 16:35:36 - INFO - root -   loading task adapter
01/14/2022 16:35:36 - INFO - root -   loading lang adpater en/wiki@ukp,pt/wiki@ukp,id/wiki@ukp,cs/wiki@ukp,tr/wiki@ukp,eu/wiki@ukp,zh_yue/wiki@ukp,vi/wiki@ukp,fr/wiki@ukp,gn/wiki@ukp
01/14/2022 16:35:36 - INFO - __main__ -   Adapter Languages : ['en', 'pt', 'id', 'cs', 'tr', 'eu', 'zh_yue', 'vi', 'fr', 'gn'], Length : 10
01/14/2022 16:35:36 - INFO - __main__ -   Adapter Names ['en/wiki@ukp', 'pt/wiki@ukp', 'id/wiki@ukp', 'cs/wiki@ukp', 'tr/wiki@ukp', 'eu/wiki@ukp', 'zh_yue/wiki@ukp', 'vi/wiki@ukp', 'fr/wiki@ukp', 'gn/wiki@ukp'], Length : 10
01/14/2022 16:35:36 - INFO - __main__ -   Language = en
01/14/2022 16:35:36 - INFO - __main__ -   Adapter Name = en/wiki@ukp
01/14/2022 16:35:37 - INFO - __main__ -   Language = pt
01/14/2022 16:35:37 - INFO - __main__ -   Adapter Name = pt/wiki@ukp
01/14/2022 16:35:37 - INFO - __main__ -   Language = pt
01/14/2022 16:35:37 - INFO - __main__ -   Adapter Name = pt/wiki@ukp
01/14/2022 16:35:37 - INFO - __main__ -   Language = vi
01/14/2022 16:35:37 - INFO - __main__ -   Adapter Name = vi/wiki@ukp
01/14/2022 16:35:39 - INFO - __main__ -   Language = id
01/14/2022 16:35:39 - INFO - __main__ -   Adapter Name = id/wiki@ukp
01/14/2022 16:35:39 - INFO - __main__ -   Language = fr
01/14/2022 16:35:39 - INFO - __main__ -   Adapter Name = fr/wiki@ukp
01/14/2022 16:35:39 - INFO - __main__ -   Language = id
01/14/2022 16:35:39 - INFO - __main__ -   Adapter Name = id/wiki@ukp
01/14/2022 16:35:40 - INFO - __main__ -   Language = tk
01/14/2022 16:35:40 - INFO - __main__ -   Adapter Name = tk/wiki@ukp
01/14/2022 16:35:43 - INFO - __main__ -   Language = tr
01/14/2022 16:35:43 - INFO - __main__ -   Adapter Name = tr/wiki@ukp
01/14/2022 16:35:43 - INFO - __main__ -   Language = cs
01/14/2022 16:35:43 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
01/14/2022 16:35:45 - INFO - __main__ -   Language = tr
01/14/2022 16:35:45 - INFO - __main__ -   Adapter Name = tr/wiki@ukp
01/14/2022 16:35:45 - INFO - __main__ -   Language = cs
01/14/2022 16:35:45 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
01/14/2022 16:35:46 - INFO - __main__ -   Args Adapter Weight = equal
01/14/2022 16:35:46 - INFO - __main__ -   Adapter Languages = ['en', 'pt', 'id', 'cs', 'tr', 'eu', 'zh_yue', 'vi', 'fr', 'tk']
01/14/2022 16:35:46 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/14/2022 16:35:46 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/14/2022 16:35:46 - INFO - __main__ -   Length of Adapter Weights = 10
01/14/2022 16:35:46 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/cached_test_tk_bert-base-multilingual-cased_128
01/14/2022 16:35:46 - INFO - __main__ -   ***** Running evaluation  in tk *****
01/14/2022 16:35:46 - INFO - __main__ -     Num examples = 101
01/14/2022 16:35:46 - INFO - __main__ -     Batch size = 32
01/14/2022 16:35:46 - INFO - __main__ -   Batch number = 1
01/14/2022 16:35:46 - INFO - __main__ -   Batch number = 2
01/14/2022 16:35:47 - INFO - __main__ -   Batch number = 3
01/14/2022 16:35:47 - INFO - __main__ -   Batch number = 4
01/14/2022 16:35:47 - INFO - __main__ -   ***** Evaluation result  in tk *****
01/14/2022 16:35:47 - INFO - __main__ -     f1 = 0.5462555066079295
01/14/2022 16:35:47 - INFO - __main__ -     loss = 2.9477867484092712
01/14/2022 16:35:47 - INFO - __main__ -     precision = 0.5081967213114754
01/14/2022 16:35:47 - INFO - __main__ -     recall = 0.5904761904761905
01/14/2022 16:35:47 - INFO - __main__ -   Language = eu
01/14/2022 16:35:47 - INFO - __main__ -   Adapter Name = eu/wiki@ukp
01/14/2022 16:35:48 - INFO - __main__ -   Language = vi
01/14/2022 16:35:48 - INFO - __main__ -   Adapter Name = vi/wiki@ukp
01/14/2022 16:35:50 - INFO - __main__ -   Language = zh_yue
01/14/2022 16:35:50 - INFO - __main__ -   Adapter Name = zh_yue/wiki@ukp
01/14/2022 16:35:50 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='tk', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:35:50 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/14/2022 16:35:50 - INFO - __main__ -   Seed = 2
01/14/2022 16:35:50 - INFO - root -   save model
01/14/2022 16:35:50 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='tk', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:35:50 - INFO - __main__ -   Loading pretrained model and tokenizer
01/14/2022 16:35:50 - INFO - __main__ -   Language = eu
01/14/2022 16:35:50 - INFO - __main__ -   Adapter Name = eu/wiki@ukp
01/14/2022 16:35:52 - INFO - __main__ -   Language = fa
01/14/2022 16:35:52 - INFO - __main__ -   Adapter Name = fa/wiki@ukp
01/14/2022 16:35:52 - INFO - __main__ -   Language = vi
01/14/2022 16:35:52 - INFO - __main__ -   Adapter Name = vi/wiki@ukp
01/14/2022 16:35:52 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/14/2022 16:35:54 - INFO - __main__ -   Language = zh_yue
01/14/2022 16:35:54 - INFO - __main__ -   Adapter Name = zh_yue/wiki@ukp
01/14/2022 16:35:54 - INFO - __main__ -   Language = fr
01/14/2022 16:35:54 - INFO - __main__ -   Adapter Name = fr/wiki@ukp
01/14/2022 16:35:55 - INFO - __main__ -   Language = gn
01/14/2022 16:35:55 - INFO - __main__ -   Adapter Name = gn/wiki@ukp
01/14/2022 16:35:56 - INFO - __main__ -   Language = mhr
01/14/2022 16:35:56 - INFO - __main__ -   Adapter Name = mhr/wiki@ukp
01/14/2022 16:35:58 - INFO - __main__ -   Using lang2id = None
01/14/2022 16:35:58 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/14/2022 16:35:58 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/
01/14/2022 16:35:58 - INFO - root -   Trying to decide if add adapter
01/14/2022 16:35:58 - INFO - root -   loading task adapter
01/14/2022 16:35:58 - INFO - root -   loading lang adpater en/wiki@ukp,pt/wiki@ukp,id/wiki@ukp,tr/wiki@ukp,cs/wiki@ukp,vi/wiki@ukp,eu/wiki@ukp,fa/wiki@ukp,zh_yue/wiki@ukp,tk/wiki@ukp
01/14/2022 16:35:58 - INFO - __main__ -   Adapter Languages : ['en', 'pt', 'id', 'tr', 'cs', 'vi', 'eu', 'fa', 'zh_yue', 'tk'], Length : 10
01/14/2022 16:35:58 - INFO - __main__ -   Adapter Names ['en/wiki@ukp', 'pt/wiki@ukp', 'id/wiki@ukp', 'tr/wiki@ukp', 'cs/wiki@ukp', 'vi/wiki@ukp', 'eu/wiki@ukp', 'fa/wiki@ukp', 'zh_yue/wiki@ukp', 'tk/wiki@ukp'], Length : 10
01/14/2022 16:35:58 - INFO - __main__ -   Language = en
01/14/2022 16:35:58 - INFO - __main__ -   Adapter Name = en/wiki@ukp
01/14/2022 16:35:59 - INFO - __main__ -   Language = pt
01/14/2022 16:35:59 - INFO - __main__ -   Adapter Name = pt/wiki@ukp
01/14/2022 16:36:00 - INFO - __main__ -   Args Adapter Weight = equal
01/14/2022 16:36:00 - INFO - __main__ -   Adapter Languages = ['en', 'pt', 'id', 'cs', 'tr', 'eu', 'zh_yue', 'vi', 'fr', 'gn']
01/14/2022 16:36:00 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/14/2022 16:36:00 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/14/2022 16:36:00 - INFO - __main__ -   Length of Adapter Weights = 10
01/14/2022 16:36:00 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/cached_test_gn_bert-base-multilingual-cased_128
01/14/2022 16:36:00 - INFO - __main__ -   ***** Running evaluation  in gn *****
01/14/2022 16:36:00 - INFO - __main__ -     Num examples = 102
01/14/2022 16:36:00 - INFO - __main__ -     Batch size = 32
01/14/2022 16:36:00 - INFO - __main__ -   Batch number = 1
01/14/2022 16:36:00 - INFO - __main__ -   Batch number = 2
01/14/2022 16:36:00 - INFO - __main__ -   Batch number = 3
01/14/2022 16:36:01 - INFO - __main__ -   Args Adapter Weight = equal
01/14/2022 16:36:01 - INFO - __main__ -   Adapter Languages = ['en', 'pt', 'id', 'tr', 'cs', 'vi', 'eu', 'fa', 'zh_yue', 'mhr']
01/14/2022 16:36:01 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/14/2022 16:36:01 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/14/2022 16:36:01 - INFO - __main__ -   Length of Adapter Weights = 10
01/14/2022 16:36:01 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/cached_test_mhr_bert-base-multilingual-cased_128
01/14/2022 16:36:01 - INFO - __main__ -   ***** Running evaluation  in mhr *****
01/14/2022 16:36:01 - INFO - __main__ -     Num examples = 100
01/14/2022 16:36:01 - INFO - __main__ -     Batch size = 32
01/14/2022 16:36:01 - INFO - __main__ -   Batch number = 1
01/14/2022 16:36:01 - INFO - __main__ -   Batch number = 4
01/14/2022 16:36:01 - INFO - __main__ -   ***** Evaluation result  in gn *****
01/14/2022 16:36:01 - INFO - __main__ -     f1 = 0.45312500000000006
01/14/2022 16:36:01 - INFO - __main__ -     loss = 3.794823944568634
01/14/2022 16:36:01 - INFO - __main__ -     precision = 0.3841059602649007
01/14/2022 16:36:01 - INFO - __main__ -     recall = 0.5523809523809524
01/14/2022 16:36:01 - INFO - __main__ -   Batch number = 2
01/14/2022 16:36:01 - INFO - __main__ -   Language = id
01/14/2022 16:36:01 - INFO - __main__ -   Adapter Name = id/wiki@ukp
01/14/2022 16:36:01 - INFO - __main__ -   Batch number = 3
01/14/2022 16:36:01 - INFO - __main__ -   Batch number = 4
01/14/2022 16:36:02 - INFO - __main__ -   ***** Evaluation result  in mhr *****
01/14/2022 16:36:02 - INFO - __main__ -     f1 = 0.5084745762711864
01/14/2022 16:36:02 - INFO - __main__ -     loss = 2.4748425483703613
01/14/2022 16:36:02 - INFO - __main__ -     precision = 0.48
01/14/2022 16:36:02 - INFO - __main__ -     recall = 0.5405405405405406
01/14/2022 16:36:03 - INFO - __main__ -   Language = tr
01/14/2022 16:36:03 - INFO - __main__ -   Adapter Name = tr/wiki@ukp
01/14/2022 16:36:03 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='gn', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:36:03 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/14/2022 16:36:03 - INFO - __main__ -   Seed = 2
01/14/2022 16:36:03 - INFO - root -   save model
01/14/2022 16:36:03 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='gn', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:36:03 - INFO - __main__ -   Loading pretrained model and tokenizer
01/14/2022 16:36:04 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mhr', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:36:04 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/14/2022 16:36:04 - INFO - __main__ -   Seed = 3
01/14/2022 16:36:04 - INFO - root -   save model
01/14/2022 16:36:04 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mhr', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:36:04 - INFO - __main__ -   Loading pretrained model and tokenizer
01/14/2022 16:36:04 - INFO - __main__ -   Language = cs
01/14/2022 16:36:04 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
01/14/2022 16:36:06 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/14/2022 16:36:06 - INFO - __main__ -   Language = vi
01/14/2022 16:36:06 - INFO - __main__ -   Adapter Name = vi/wiki@ukp
01/14/2022 16:36:07 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/14/2022 16:36:07 - INFO - __main__ -   Language = eu
01/14/2022 16:36:07 - INFO - __main__ -   Adapter Name = eu/wiki@ukp
01/14/2022 16:36:09 - INFO - __main__ -   Language = fa
01/14/2022 16:36:09 - INFO - __main__ -   Adapter Name = fa/wiki@ukp
01/14/2022 16:36:11 - INFO - __main__ -   Language = zh_yue
01/14/2022 16:36:11 - INFO - __main__ -   Adapter Name = zh_yue/wiki@ukp
01/14/2022 16:36:12 - INFO - __main__ -   Using lang2id = None
01/14/2022 16:36:12 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/14/2022 16:36:12 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/
01/14/2022 16:36:12 - INFO - root -   Trying to decide if add adapter
01/14/2022 16:36:12 - INFO - root -   loading task adapter
01/14/2022 16:36:12 - INFO - root -   loading lang adpater en/wiki@ukp,pt/wiki@ukp,id/wiki@ukp,tr/wiki@ukp,cs/wiki@ukp,vi/wiki@ukp,eu/wiki@ukp,fa/wiki@ukp,zh_yue/wiki@ukp,gn/wiki@ukp
01/14/2022 16:36:12 - INFO - __main__ -   Adapter Languages : ['en', 'pt', 'id', 'tr', 'cs', 'vi', 'eu', 'fa', 'zh_yue', 'gn'], Length : 10
01/14/2022 16:36:12 - INFO - __main__ -   Adapter Names ['en/wiki@ukp', 'pt/wiki@ukp', 'id/wiki@ukp', 'tr/wiki@ukp', 'cs/wiki@ukp', 'vi/wiki@ukp', 'eu/wiki@ukp', 'fa/wiki@ukp', 'zh_yue/wiki@ukp', 'gn/wiki@ukp'], Length : 10
01/14/2022 16:36:12 - INFO - __main__ -   Language = en
01/14/2022 16:36:12 - INFO - __main__ -   Adapter Name = en/wiki@ukp
01/14/2022 16:36:12 - INFO - __main__ -   Using lang2id = None
01/14/2022 16:36:12 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/14/2022 16:36:12 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/
01/14/2022 16:36:12 - INFO - root -   Trying to decide if add adapter
01/14/2022 16:36:12 - INFO - root -   loading task adapter
01/14/2022 16:36:12 - INFO - __main__ -   Language = tk
01/14/2022 16:36:12 - INFO - __main__ -   Adapter Name = tk/wiki@ukp
01/14/2022 16:36:12 - INFO - root -   loading lang adpater en/wiki@ukp,pt/wiki@ukp,id/wiki@ukp,tr/wiki@ukp,vi/wiki@ukp,fa/wiki@ukp,eu/wiki@ukp,zh_yue/wiki@ukp,cs/wiki@ukp,mhr/wiki@ukp
01/14/2022 16:36:12 - INFO - __main__ -   Adapter Languages : ['en', 'pt', 'id', 'tr', 'vi', 'fa', 'eu', 'zh_yue', 'cs', 'mhr'], Length : 10
01/14/2022 16:36:12 - INFO - __main__ -   Adapter Names ['en/wiki@ukp', 'pt/wiki@ukp', 'id/wiki@ukp', 'tr/wiki@ukp', 'vi/wiki@ukp', 'fa/wiki@ukp', 'eu/wiki@ukp', 'zh_yue/wiki@ukp', 'cs/wiki@ukp', 'mhr/wiki@ukp'], Length : 10
01/14/2022 16:36:12 - INFO - __main__ -   Language = en
01/14/2022 16:36:12 - INFO - __main__ -   Adapter Name = en/wiki@ukp
01/14/2022 16:36:13 - INFO - __main__ -   Language = pt
01/14/2022 16:36:13 - INFO - __main__ -   Adapter Name = pt/wiki@ukp
01/14/2022 16:36:13 - INFO - __main__ -   Language = pt
01/14/2022 16:36:13 - INFO - __main__ -   Adapter Name = pt/wiki@ukp
01/14/2022 16:36:14 - INFO - __main__ -   Language = id
01/14/2022 16:36:14 - INFO - __main__ -   Adapter Name = id/wiki@ukp
01/14/2022 16:36:15 - INFO - __main__ -   Language = id
01/14/2022 16:36:15 - INFO - __main__ -   Adapter Name = id/wiki@ukp
01/14/2022 16:36:16 - INFO - __main__ -   Language = tr
01/14/2022 16:36:16 - INFO - __main__ -   Adapter Name = tr/wiki@ukp
01/14/2022 16:36:16 - INFO - __main__ -   Language = tr
01/14/2022 16:36:16 - INFO - __main__ -   Adapter Name = tr/wiki@ukp
01/14/2022 16:36:17 - INFO - __main__ -   Args Adapter Weight = equal
01/14/2022 16:36:17 - INFO - __main__ -   Adapter Languages = ['en', 'pt', 'id', 'tr', 'cs', 'vi', 'eu', 'fa', 'zh_yue', 'tk']
01/14/2022 16:36:17 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/14/2022 16:36:17 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/14/2022 16:36:17 - INFO - __main__ -   Length of Adapter Weights = 10
01/14/2022 16:36:17 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/cached_test_tk_bert-base-multilingual-cased_128
01/14/2022 16:36:17 - INFO - __main__ -   ***** Running evaluation  in tk *****
01/14/2022 16:36:17 - INFO - __main__ -     Num examples = 101
01/14/2022 16:36:17 - INFO - __main__ -     Batch size = 32
01/14/2022 16:36:17 - INFO - __main__ -   Batch number = 1
01/14/2022 16:36:17 - INFO - __main__ -   Batch number = 2
01/14/2022 16:36:17 - INFO - __main__ -   Batch number = 3
01/14/2022 16:36:17 - INFO - __main__ -   Language = cs
01/14/2022 16:36:17 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
01/14/2022 16:36:18 - INFO - __main__ -   Batch number = 4
01/14/2022 16:36:18 - INFO - __main__ -   ***** Evaluation result  in tk *****
01/14/2022 16:36:18 - INFO - __main__ -     f1 = 0.5317460317460317
01/14/2022 16:36:18 - INFO - __main__ -     loss = 3.32015860080719
01/14/2022 16:36:18 - INFO - __main__ -     precision = 0.4557823129251701
01/14/2022 16:36:18 - INFO - __main__ -     recall = 0.638095238095238
01/14/2022 16:36:18 - INFO - __main__ -   Language = vi
01/14/2022 16:36:18 - INFO - __main__ -   Adapter Name = vi/wiki@ukp
01/14/2022 16:36:19 - INFO - __main__ -   Language = vi
01/14/2022 16:36:19 - INFO - __main__ -   Adapter Name = vi/wiki@ukp
01/14/2022 16:36:20 - INFO - __main__ -   Language = fa
01/14/2022 16:36:20 - INFO - __main__ -   Adapter Name = fa/wiki@ukp
01/14/2022 16:36:20 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='tk', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:36:20 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/14/2022 16:36:20 - INFO - __main__ -   Seed = 3
01/14/2022 16:36:20 - INFO - root -   save model
01/14/2022 16:36:20 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='tk', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:36:20 - INFO - __main__ -   Loading pretrained model and tokenizer
01/14/2022 16:36:21 - INFO - __main__ -   Language = eu
01/14/2022 16:36:21 - INFO - __main__ -   Adapter Name = eu/wiki@ukp
01/14/2022 16:36:22 - INFO - __main__ -   Language = eu
01/14/2022 16:36:22 - INFO - __main__ -   Adapter Name = eu/wiki@ukp
01/14/2022 16:36:22 - INFO - __main__ -   Language = fa
01/14/2022 16:36:22 - INFO - __main__ -   Adapter Name = fa/wiki@ukp
01/14/2022 16:36:23 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/14/2022 16:36:23 - INFO - __main__ -   Language = zh_yue
01/14/2022 16:36:23 - INFO - __main__ -   Adapter Name = zh_yue/wiki@ukp
01/14/2022 16:36:24 - INFO - __main__ -   Language = zh_yue
01/14/2022 16:36:24 - INFO - __main__ -   Adapter Name = zh_yue/wiki@ukp
01/14/2022 16:36:25 - INFO - __main__ -   Language = cs
01/14/2022 16:36:25 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
01/14/2022 16:36:25 - INFO - __main__ -   Language = gn
01/14/2022 16:36:25 - INFO - __main__ -   Adapter Name = gn/wiki@ukp
01/14/2022 16:36:26 - INFO - __main__ -   Language = mhr
01/14/2022 16:36:26 - INFO - __main__ -   Adapter Name = mhr/wiki@ukp
01/14/2022 16:36:29 - INFO - __main__ -   Using lang2id = None
01/14/2022 16:36:29 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/14/2022 16:36:29 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/
01/14/2022 16:36:29 - INFO - root -   Trying to decide if add adapter
01/14/2022 16:36:29 - INFO - root -   loading task adapter
01/14/2022 16:36:29 - INFO - root -   loading lang adpater en/wiki@ukp,pt/wiki@ukp,id/wiki@ukp,tr/wiki@ukp,vi/wiki@ukp,fa/wiki@ukp,eu/wiki@ukp,zh_yue/wiki@ukp,cs/wiki@ukp,tk/wiki@ukp
01/14/2022 16:36:29 - INFO - __main__ -   Adapter Languages : ['en', 'pt', 'id', 'tr', 'vi', 'fa', 'eu', 'zh_yue', 'cs', 'tk'], Length : 10
01/14/2022 16:36:29 - INFO - __main__ -   Adapter Names ['en/wiki@ukp', 'pt/wiki@ukp', 'id/wiki@ukp', 'tr/wiki@ukp', 'vi/wiki@ukp', 'fa/wiki@ukp', 'eu/wiki@ukp', 'zh_yue/wiki@ukp', 'cs/wiki@ukp', 'tk/wiki@ukp'], Length : 10
01/14/2022 16:36:29 - INFO - __main__ -   Language = en
01/14/2022 16:36:29 - INFO - __main__ -   Adapter Name = en/wiki@ukp
01/14/2022 16:36:30 - INFO - __main__ -   Language = pt
01/14/2022 16:36:30 - INFO - __main__ -   Adapter Name = pt/wiki@ukp
01/14/2022 16:36:30 - INFO - __main__ -   Args Adapter Weight = equal
01/14/2022 16:36:30 - INFO - __main__ -   Adapter Languages = ['en', 'pt', 'id', 'tr', 'cs', 'vi', 'eu', 'fa', 'zh_yue', 'gn']
01/14/2022 16:36:30 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/14/2022 16:36:30 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/14/2022 16:36:30 - INFO - __main__ -   Length of Adapter Weights = 10
01/14/2022 16:36:30 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/cached_test_gn_bert-base-multilingual-cased_128
01/14/2022 16:36:30 - INFO - __main__ -   ***** Running evaluation  in gn *****
01/14/2022 16:36:30 - INFO - __main__ -     Num examples = 102
01/14/2022 16:36:30 - INFO - __main__ -     Batch size = 32
01/14/2022 16:36:30 - INFO - __main__ -   Batch number = 1
01/14/2022 16:36:30 - INFO - __main__ -   Batch number = 2
01/14/2022 16:36:30 - INFO - __main__ -   Args Adapter Weight = equal
01/14/2022 16:36:30 - INFO - __main__ -   Adapter Languages = ['en', 'pt', 'id', 'tr', 'vi', 'fa', 'eu', 'zh_yue', 'cs', 'mhr']
01/14/2022 16:36:30 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/14/2022 16:36:30 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/14/2022 16:36:30 - INFO - __main__ -   Length of Adapter Weights = 10
01/14/2022 16:36:30 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/cached_test_mhr_bert-base-multilingual-cased_128
01/14/2022 16:36:30 - INFO - __main__ -   ***** Running evaluation  in mhr *****
01/14/2022 16:36:30 - INFO - __main__ -     Num examples = 100
01/14/2022 16:36:30 - INFO - __main__ -     Batch size = 32
01/14/2022 16:36:30 - INFO - __main__ -   Batch number = 1
01/14/2022 16:36:30 - INFO - __main__ -   Batch number = 3
01/14/2022 16:36:31 - INFO - __main__ -   Batch number = 2
01/14/2022 16:36:31 - INFO - __main__ -   Batch number = 4
01/14/2022 16:36:31 - INFO - __main__ -   ***** Evaluation result  in gn *****
01/14/2022 16:36:31 - INFO - __main__ -     f1 = 0.4794007490636704
01/14/2022 16:36:31 - INFO - __main__ -     loss = 3.5714155435562134
01/14/2022 16:36:31 - INFO - __main__ -     precision = 0.3950617283950617
01/14/2022 16:36:31 - INFO - __main__ -     recall = 0.6095238095238096
01/14/2022 16:36:31 - INFO - __main__ -   Batch number = 3
01/14/2022 16:36:31 - INFO - __main__ -   Batch number = 4
01/14/2022 16:36:31 - INFO - __main__ -   ***** Evaluation result  in mhr *****
01/14/2022 16:36:31 - INFO - __main__ -     f1 = 0.4266666666666667
01/14/2022 16:36:31 - INFO - __main__ -     loss = 2.789973735809326
01/14/2022 16:36:31 - INFO - __main__ -     precision = 0.42105263157894735
01/14/2022 16:36:31 - INFO - __main__ -     recall = 0.43243243243243246
01/14/2022 16:36:32 - INFO - __main__ -   Language = id
01/14/2022 16:36:32 - INFO - __main__ -   Adapter Name = id/wiki@ukp
01/14/2022 16:36:33 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='gn', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:36:33 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/14/2022 16:36:33 - INFO - __main__ -   Seed = 3
01/14/2022 16:36:33 - INFO - root -   save model
01/14/2022 16:36:33 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='gn', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ensemble_en_top10_madx//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:36:33 - INFO - __main__ -   Loading pretrained model and tokenizer
01/14/2022 16:36:34 - INFO - __main__ -   Language = tr
01/14/2022 16:36:34 - INFO - __main__ -   Adapter Name = tr/wiki@ukp
01/14/2022 16:36:36 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/14/2022 16:36:36 - INFO - __main__ -   Language = vi
01/14/2022 16:36:36 - INFO - __main__ -   Adapter Name = vi/wiki@ukp
01/14/2022 16:36:38 - INFO - __main__ -   Language = fa
01/14/2022 16:36:38 - INFO - __main__ -   Adapter Name = fa/wiki@ukp
01/14/2022 16:36:40 - INFO - __main__ -   Language = eu
01/14/2022 16:36:40 - INFO - __main__ -   Adapter Name = eu/wiki@ukp
01/14/2022 16:36:41 - INFO - __main__ -   Using lang2id = None
01/14/2022 16:36:41 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/14/2022 16:36:41 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/
01/14/2022 16:36:41 - INFO - root -   Trying to decide if add adapter
01/14/2022 16:36:41 - INFO - root -   loading task adapter
01/14/2022 16:36:41 - INFO - root -   loading lang adpater en/wiki@ukp,pt/wiki@ukp,id/wiki@ukp,tr/wiki@ukp,vi/wiki@ukp,fa/wiki@ukp,eu/wiki@ukp,zh_yue/wiki@ukp,cs/wiki@ukp,gn/wiki@ukp
01/14/2022 16:36:41 - INFO - __main__ -   Adapter Languages : ['en', 'pt', 'id', 'tr', 'vi', 'fa', 'eu', 'zh_yue', 'cs', 'gn'], Length : 10
01/14/2022 16:36:41 - INFO - __main__ -   Adapter Names ['en/wiki@ukp', 'pt/wiki@ukp', 'id/wiki@ukp', 'tr/wiki@ukp', 'vi/wiki@ukp', 'fa/wiki@ukp', 'eu/wiki@ukp', 'zh_yue/wiki@ukp', 'cs/wiki@ukp', 'gn/wiki@ukp'], Length : 10
01/14/2022 16:36:41 - INFO - __main__ -   Language = en
01/14/2022 16:36:41 - INFO - __main__ -   Adapter Name = en/wiki@ukp
01/14/2022 16:36:42 - INFO - __main__ -   Language = pt
01/14/2022 16:36:42 - INFO - __main__ -   Adapter Name = pt/wiki@ukp
01/14/2022 16:36:43 - INFO - __main__ -   Language = zh_yue
01/14/2022 16:36:43 - INFO - __main__ -   Adapter Name = zh_yue/wiki@ukp
01/14/2022 16:36:44 - INFO - __main__ -   Language = id
01/14/2022 16:36:44 - INFO - __main__ -   Adapter Name = id/wiki@ukp
01/14/2022 16:36:45 - INFO - __main__ -   Language = cs
01/14/2022 16:36:45 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
01/14/2022 16:36:45 - INFO - __main__ -   Language = tr
01/14/2022 16:36:45 - INFO - __main__ -   Adapter Name = tr/wiki@ukp
01/14/2022 16:36:47 - INFO - __main__ -   Language = tk
01/14/2022 16:36:47 - INFO - __main__ -   Adapter Name = tk/wiki@ukp
01/14/2022 16:36:47 - INFO - __main__ -   Language = vi
01/14/2022 16:36:47 - INFO - __main__ -   Adapter Name = vi/wiki@ukp
01/14/2022 16:36:49 - INFO - __main__ -   Language = fa
01/14/2022 16:36:49 - INFO - __main__ -   Adapter Name = fa/wiki@ukp
01/14/2022 16:36:50 - INFO - __main__ -   Language = eu
01/14/2022 16:36:50 - INFO - __main__ -   Adapter Name = eu/wiki@ukp
01/14/2022 16:36:52 - INFO - __main__ -   Args Adapter Weight = equal
01/14/2022 16:36:52 - INFO - __main__ -   Adapter Languages = ['en', 'pt', 'id', 'tr', 'vi', 'fa', 'eu', 'zh_yue', 'cs', 'tk']
01/14/2022 16:36:52 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/14/2022 16:36:52 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/14/2022 16:36:52 - INFO - __main__ -   Length of Adapter Weights = 10
01/14/2022 16:36:52 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/cached_test_tk_bert-base-multilingual-cased_128
01/14/2022 16:36:52 - INFO - __main__ -   ***** Running evaluation  in tk *****
01/14/2022 16:36:52 - INFO - __main__ -     Num examples = 101
01/14/2022 16:36:52 - INFO - __main__ -     Batch size = 32
01/14/2022 16:36:52 - INFO - __main__ -   Batch number = 1
01/14/2022 16:36:52 - INFO - __main__ -   Batch number = 2
01/14/2022 16:36:52 - INFO - __main__ -   Language = zh_yue
01/14/2022 16:36:52 - INFO - __main__ -   Adapter Name = zh_yue/wiki@ukp
01/14/2022 16:36:52 - INFO - __main__ -   Batch number = 3
01/14/2022 16:36:53 - INFO - __main__ -   Batch number = 4
01/14/2022 16:36:53 - INFO - __main__ -   ***** Evaluation result  in tk *****
01/14/2022 16:36:53 - INFO - __main__ -     f1 = 0.4773662551440329
01/14/2022 16:36:53 - INFO - __main__ -     loss = 3.5105076134204865
01/14/2022 16:36:53 - INFO - __main__ -     precision = 0.42028985507246375
01/14/2022 16:36:53 - INFO - __main__ -     recall = 0.5523809523809524
01/14/2022 16:36:54 - INFO - __main__ -   Language = cs
01/14/2022 16:36:54 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
01/14/2022 16:36:55 - INFO - __main__ -   Language = gn
01/14/2022 16:36:55 - INFO - __main__ -   Adapter Name = gn/wiki@ukp
01/14/2022 16:37:00 - INFO - __main__ -   Args Adapter Weight = equal
01/14/2022 16:37:00 - INFO - __main__ -   Adapter Languages = ['en', 'pt', 'id', 'tr', 'vi', 'fa', 'eu', 'zh_yue', 'cs', 'gn']
01/14/2022 16:37:00 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/14/2022 16:37:00 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/14/2022 16:37:00 - INFO - __main__ -   Length of Adapter Weights = 10
01/14/2022 16:37:00 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//panx/panx_processed_maxlen128/cached_test_gn_bert-base-multilingual-cased_128
01/14/2022 16:37:00 - INFO - __main__ -   ***** Running evaluation  in gn *****
01/14/2022 16:37:00 - INFO - __main__ -     Num examples = 102
01/14/2022 16:37:00 - INFO - __main__ -     Batch size = 32
01/14/2022 16:37:00 - INFO - __main__ -   Batch number = 1
01/14/2022 16:37:00 - INFO - __main__ -   Batch number = 2
01/14/2022 16:37:01 - INFO - __main__ -   Batch number = 3
01/14/2022 16:37:01 - INFO - __main__ -   Batch number = 4
01/14/2022 16:37:01 - INFO - __main__ -   ***** Evaluation result  in gn *****
01/14/2022 16:37:01 - INFO - __main__ -     f1 = 0.4184397163120568
01/14/2022 16:37:01 - INFO - __main__ -     loss = 3.559912621974945
01/14/2022 16:37:01 - INFO - __main__ -     precision = 0.3333333333333333
01/14/2022 16:37:01 - INFO - __main__ -     recall = 0.5619047619047619
