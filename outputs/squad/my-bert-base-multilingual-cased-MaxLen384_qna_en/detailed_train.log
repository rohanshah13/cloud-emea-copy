PyTorch version 1.10.0+cu111 available.
01/06/2022 17:12:37 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/06/2022 17:12:37 - INFO - root -   save model
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /root/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/06/2022 17:12:49 - INFO - __main__ -   lang2id = None
01/06/2022 17:12:53 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, data_dir='/root/Desktop/cloud-emea-copy/data//xquad', output_dir='/root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8', max_seq_length=384, train_file='/root/Desktop/cloud-emea-copy/data//xquad/train-v1.1.json', predict_file='/root/Desktop/cloud-emea-copy/data//xquad/dev-v1.1.json', eval_test_set=False, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=4, per_gpu_eval_batch_size=8, gradient_accumulation_steps=8, learning_rate=0.0003, weight_decay=0.0001, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=15.0, max_steps=-1, save_steps=1000, warmup_steps=500, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', eval_lang='en', predict_langs='en', train_lang='en', log_file='/root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='qna', threads=8, version_2_with_negative=False, verbose_logging=False, n_best_size=20, max_query_length=64, max_answer_length=30, doc_stride=128, null_score_diff_threshold=0.0, predict_task_adapter='output/squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8_s3/checkpoint-best/qna', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix=None, en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/06/2022 17:12:53 - INFO - __main__ -   Results: {}
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /root/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/06/2022 17:12:59 - INFO - __main__ -   lang2id = None
01/06/2022 17:12:59 - INFO - __main__ -   Evaluating the model on the test set of all languages specified
01/06/2022 17:12:59 - INFO - __main__ -   Adapter will be loaded from this path: output/squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8_s3/checkpoint-best/qna
01/06/2022 17:12:59 - INFO - root -   Trying to decide if add adapter
01/06/2022 17:12:59 - INFO - root -   loading task adapter
Loading module configuration from output/squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8_s3/checkpoint-best/qna/adapter_config.json
Adding adapter 'qna' of type 'text_task'.
Loading module weights from output/squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8_s3/checkpoint-best/qna/pytorch_adapter.bin
Loading module configuration from output/squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8_s3/checkpoint-best/qna/head_config.json
Loading module weights from output/squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8_s3/checkpoint-best/qna/pytorch_model_head.bin
01/06/2022 17:12:59 - INFO - root -   loading lang adpater en/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip.
Loading module configuration from /root/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
Loading module weights from /root/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/pytorch_adapter.bin
No matching prediction head found in '/root/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted'
01/06/2022 17:13:06 - INFO - __main__ -   Language adapter for en found
01/06/2022 17:13:06 - INFO - __main__ -   Set active language adapter to en
01/06/2022 17:13:06 - INFO - __main__ -   Args Adapter Weight = None
01/06/2022 17:13:06 - INFO - __main__ -   Adapter Languages = ['en']
01/06/2022 17:13:06 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
en en/wiki@ukp
  0%|          | 0/48 [00:00<?, ?it/s] 31%|███▏      | 15/48 [00:00<00:00, 149.37it/s] 62%|██████▎   | 30/48 [00:00<00:00, 120.60it/s] 94%|█████████▍| 45/48 [00:00<00:00, 130.17it/s]100%|██████████| 48/48 [00:00<00:00, 132.44it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<04:19,  4.57it/s]convert squad examples to features:  24%|██▍       | 289/1190 [00:00<00:00, 1075.02it/s]convert squad examples to features:  37%|███▋      | 436/1190 [00:00<00:01, 568.27it/s] convert squad examples to features:  75%|███████▌  | 897/1190 [00:00<00:00, 1257.98it/s]convert squad examples to features:  94%|█████████▍| 1121/1190 [00:01<00:00, 1306.05it/s]convert squad examples to features: 100%|██████████| 1190/1190 [00:01<00:00, 1096.52it/s]/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|██████████| 1190/1190 [00:00<00:00, 508789.17it/s]
01/06/2022 17:13:08 - INFO - __main__ -   Local Rank = -1
01/06/2022 17:13:08 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.en.json_bert-base-multilingual-cased_384_en
01/06/2022 17:13:09 - INFO - __main__ -   ***** Running evaluation  *****
01/06/2022 17:13:09 - INFO - __main__ -     Num examples = 1251
01/06/2022 17:13:09 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/157 [00:00<?, ?it/s]01/06/2022 17:13:09 - INFO - __main__ -   Batch Number = 1
01/06/2022 17:13:09 - INFO - __main__ -   Batch Number = 2
01/06/2022 17:13:09 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|▏         | 3/157 [00:00<00:05, 27.06it/s]01/06/2022 17:13:09 - INFO - __main__ -   Batch Number = 4
01/06/2022 17:13:09 - INFO - __main__ -   Batch Number = 5
01/06/2022 17:13:09 - INFO - __main__ -   Batch Number = 6
Evaluating:   4%|▍         | 6/157 [00:00<00:05, 28.58it/s]01/06/2022 17:13:09 - INFO - __main__ -   Batch Number = 7
01/06/2022 17:13:09 - INFO - __main__ -   Batch Number = 8
01/06/2022 17:13:09 - INFO - __main__ -   Batch Number = 9
01/06/2022 17:13:09 - INFO - __main__ -   Batch Number = 10
Evaluating:   6%|▋         | 10/157 [00:00<00:04, 31.28it/s]01/06/2022 17:13:09 - INFO - __main__ -   Batch Number = 11
01/06/2022 17:13:10 - INFO - __main__ -   Batch Number = 12
01/06/2022 17:13:10 - INFO - __main__ -   Batch Number = 13
01/06/2022 17:13:10 - INFO - __main__ -   Batch Number = 14
01/06/2022 17:13:10 - INFO - __main__ -   Batch Number = 15
Evaluating:  10%|▉         | 15/157 [00:00<00:03, 36.44it/s]01/06/2022 17:13:10 - INFO - __main__ -   Batch Number = 16
01/06/2022 17:13:10 - INFO - __main__ -   Batch Number = 17
01/06/2022 17:13:10 - INFO - __main__ -   Batch Number = 18
01/06/2022 17:13:10 - INFO - __main__ -   Batch Number = 19
01/06/2022 17:13:10 - INFO - __main__ -   Batch Number = 20
Evaluating:  13%|█▎        | 20/157 [00:00<00:03, 39.45it/s]01/06/2022 17:13:10 - INFO - __main__ -   Batch Number = 21
01/06/2022 17:13:10 - INFO - __main__ -   Batch Number = 22
01/06/2022 17:13:10 - INFO - __main__ -   Batch Number = 23
01/06/2022 17:13:10 - INFO - __main__ -   Batch Number = 24
01/06/2022 17:13:10 - INFO - __main__ -   Batch Number = 25
Evaluating:  16%|█▌        | 25/157 [00:00<00:03, 41.31it/s]01/06/2022 17:13:10 - INFO - __main__ -   Batch Number = 26
01/06/2022 17:13:10 - INFO - __main__ -   Batch Number = 27
01/06/2022 17:13:10 - INFO - __main__ -   Batch Number = 28
01/06/2022 17:13:10 - INFO - __main__ -   Batch Number = 29
01/06/2022 17:13:10 - INFO - __main__ -   Batch Number = 30
Evaluating:  19%|█▉        | 30/157 [00:00<00:03, 42.21it/s]01/06/2022 17:13:10 - INFO - __main__ -   Batch Number = 31
01/06/2022 17:13:10 - INFO - __main__ -   Batch Number = 32
01/06/2022 17:13:10 - INFO - __main__ -   Batch Number = 33
01/06/2022 17:13:10 - INFO - __main__ -   Batch Number = 34
01/06/2022 17:13:10 - INFO - __main__ -   Batch Number = 35
Evaluating:  22%|██▏       | 35/157 [00:00<00:02, 43.14it/s]01/06/2022 17:13:10 - INFO - __main__ -   Batch Number = 36
01/06/2022 17:13:10 - INFO - __main__ -   Batch Number = 37
01/06/2022 17:13:10 - INFO - __main__ -   Batch Number = 38
01/06/2022 17:13:10 - INFO - __main__ -   Batch Number = 39
01/06/2022 17:13:10 - INFO - __main__ -   Batch Number = 40
Evaluating:  25%|██▌       | 40/157 [00:01<00:02, 43.56it/s]01/06/2022 17:13:10 - INFO - __main__ -   Batch Number = 41
01/06/2022 17:13:10 - INFO - __main__ -   Batch Number = 42
01/06/2022 17:13:10 - INFO - __main__ -   Batch Number = 43
01/06/2022 17:13:10 - INFO - __main__ -   Batch Number = 44
01/06/2022 17:13:10 - INFO - __main__ -   Batch Number = 45
Evaluating:  29%|██▊       | 45/157 [00:01<00:02, 43.61it/s]01/06/2022 17:13:10 - INFO - __main__ -   Batch Number = 46
01/06/2022 17:13:10 - INFO - __main__ -   Batch Number = 47
01/06/2022 17:13:10 - INFO - __main__ -   Batch Number = 48
01/06/2022 17:13:10 - INFO - __main__ -   Batch Number = 49
01/06/2022 17:13:10 - INFO - __main__ -   Batch Number = 50
Evaluating:  32%|███▏      | 50/157 [00:01<00:02, 44.01it/s]01/06/2022 17:13:10 - INFO - __main__ -   Batch Number = 51
01/06/2022 17:13:10 - INFO - __main__ -   Batch Number = 52
01/06/2022 17:13:10 - INFO - __main__ -   Batch Number = 53
01/06/2022 17:13:10 - INFO - __main__ -   Batch Number = 54
01/06/2022 17:13:10 - INFO - __main__ -   Batch Number = 55
Evaluating:  35%|███▌      | 55/157 [00:01<00:02, 44.29it/s]01/06/2022 17:13:10 - INFO - __main__ -   Batch Number = 56
01/06/2022 17:13:11 - INFO - __main__ -   Batch Number = 57
01/06/2022 17:13:11 - INFO - __main__ -   Batch Number = 58
01/06/2022 17:13:11 - INFO - __main__ -   Batch Number = 59
01/06/2022 17:13:11 - INFO - __main__ -   Batch Number = 60
Evaluating:  38%|███▊      | 60/157 [00:01<00:02, 42.91it/s]01/06/2022 17:13:11 - INFO - __main__ -   Batch Number = 61
01/06/2022 17:13:11 - INFO - __main__ -   Batch Number = 62
01/06/2022 17:13:11 - INFO - __main__ -   Batch Number = 63
01/06/2022 17:13:11 - INFO - __main__ -   Batch Number = 64
01/06/2022 17:13:11 - INFO - __main__ -   Batch Number = 65
Evaluating:  41%|████▏     | 65/157 [00:01<00:02, 43.53it/s]01/06/2022 17:13:11 - INFO - __main__ -   Batch Number = 66
01/06/2022 17:13:11 - INFO - __main__ -   Batch Number = 67
01/06/2022 17:13:11 - INFO - __main__ -   Batch Number = 68
01/06/2022 17:13:11 - INFO - __main__ -   Batch Number = 69
01/06/2022 17:13:11 - INFO - __main__ -   Batch Number = 70
Evaluating:  45%|████▍     | 70/157 [00:01<00:01, 43.65it/s]01/06/2022 17:13:11 - INFO - __main__ -   Batch Number = 71
01/06/2022 17:13:11 - INFO - __main__ -   Batch Number = 72
01/06/2022 17:13:11 - INFO - __main__ -   Batch Number = 73
01/06/2022 17:13:11 - INFO - __main__ -   Batch Number = 74
01/06/2022 17:13:11 - INFO - __main__ -   Batch Number = 75
Evaluating:  48%|████▊     | 75/157 [00:01<00:01, 43.80it/s]01/06/2022 17:13:11 - INFO - __main__ -   Batch Number = 76
01/06/2022 17:13:11 - INFO - __main__ -   Batch Number = 77
01/06/2022 17:13:11 - INFO - __main__ -   Batch Number = 78
01/06/2022 17:13:11 - INFO - __main__ -   Batch Number = 79
01/06/2022 17:13:11 - INFO - __main__ -   Batch Number = 80
Evaluating:  51%|█████     | 80/157 [00:01<00:01, 43.93it/s]01/06/2022 17:13:11 - INFO - __main__ -   Batch Number = 81
01/06/2022 17:13:11 - INFO - __main__ -   Batch Number = 82
01/06/2022 17:13:11 - INFO - __main__ -   Batch Number = 83
01/06/2022 17:13:11 - INFO - __main__ -   Batch Number = 84
01/06/2022 17:13:11 - INFO - __main__ -   Batch Number = 85
Evaluating:  54%|█████▍    | 85/157 [00:02<00:01, 43.86it/s]01/06/2022 17:13:11 - INFO - __main__ -   Batch Number = 86
01/06/2022 17:13:11 - INFO - __main__ -   Batch Number = 87
01/06/2022 17:13:11 - INFO - __main__ -   Batch Number = 88
01/06/2022 17:13:11 - INFO - __main__ -   Batch Number = 89
01/06/2022 17:13:11 - INFO - __main__ -   Batch Number = 90
Evaluating:  57%|█████▋    | 90/157 [00:02<00:01, 43.73it/s]01/06/2022 17:13:11 - INFO - __main__ -   Batch Number = 91
01/06/2022 17:13:11 - INFO - __main__ -   Batch Number = 92
01/06/2022 17:13:11 - INFO - __main__ -   Batch Number = 93
01/06/2022 17:13:11 - INFO - __main__ -   Batch Number = 94
01/06/2022 17:13:11 - INFO - __main__ -   Batch Number = 95
Evaluating:  61%|██████    | 95/157 [00:02<00:01, 43.66it/s]01/06/2022 17:13:11 - INFO - __main__ -   Batch Number = 96
01/06/2022 17:13:11 - INFO - __main__ -   Batch Number = 97
01/06/2022 17:13:11 - INFO - __main__ -   Batch Number = 98
01/06/2022 17:13:11 - INFO - __main__ -   Batch Number = 99
01/06/2022 17:13:12 - INFO - __main__ -   Batch Number = 100
Evaluating:  64%|██████▎   | 100/157 [00:02<00:01, 43.77it/s]01/06/2022 17:13:12 - INFO - __main__ -   Batch Number = 101
01/06/2022 17:13:12 - INFO - __main__ -   Batch Number = 102
01/06/2022 17:13:12 - INFO - __main__ -   Batch Number = 103
01/06/2022 17:13:12 - INFO - __main__ -   Batch Number = 104
01/06/2022 17:13:12 - INFO - __main__ -   Batch Number = 105
Evaluating:  67%|██████▋   | 105/157 [00:02<00:01, 43.79it/s]01/06/2022 17:13:12 - INFO - __main__ -   Batch Number = 106
01/06/2022 17:13:12 - INFO - __main__ -   Batch Number = 107
01/06/2022 17:13:12 - INFO - __main__ -   Batch Number = 108
01/06/2022 17:13:12 - INFO - __main__ -   Batch Number = 109
01/06/2022 17:13:12 - INFO - __main__ -   Batch Number = 110
Evaluating:  70%|███████   | 110/157 [00:02<00:01, 44.01it/s]01/06/2022 17:13:12 - INFO - __main__ -   Batch Number = 111
01/06/2022 17:13:12 - INFO - __main__ -   Batch Number = 112
01/06/2022 17:13:12 - INFO - __main__ -   Batch Number = 113
01/06/2022 17:13:12 - INFO - __main__ -   Batch Number = 114
01/06/2022 17:13:12 - INFO - __main__ -   Batch Number = 115
Evaluating:  73%|███████▎  | 115/157 [00:02<00:00, 44.29it/s]01/06/2022 17:13:12 - INFO - __main__ -   Batch Number = 116
01/06/2022 17:13:12 - INFO - __main__ -   Batch Number = 117
01/06/2022 17:13:12 - INFO - __main__ -   Batch Number = 118
01/06/2022 17:13:12 - INFO - __main__ -   Batch Number = 119
01/06/2022 17:13:12 - INFO - __main__ -   Batch Number = 120
Evaluating:  76%|███████▋  | 120/157 [00:02<00:00, 44.30it/s]01/06/2022 17:13:12 - INFO - __main__ -   Batch Number = 121
01/06/2022 17:13:12 - INFO - __main__ -   Batch Number = 122
01/06/2022 17:13:12 - INFO - __main__ -   Batch Number = 123
01/06/2022 17:13:12 - INFO - __main__ -   Batch Number = 124
01/06/2022 17:13:12 - INFO - __main__ -   Batch Number = 125
Evaluating:  80%|███████▉  | 125/157 [00:02<00:00, 44.53it/s]01/06/2022 17:13:12 - INFO - __main__ -   Batch Number = 126
01/06/2022 17:13:12 - INFO - __main__ -   Batch Number = 127
01/06/2022 17:13:12 - INFO - __main__ -   Batch Number = 128
01/06/2022 17:13:12 - INFO - __main__ -   Batch Number = 129
01/06/2022 17:13:12 - INFO - __main__ -   Batch Number = 130
Evaluating:  83%|████████▎ | 130/157 [00:03<00:00, 44.50it/s]01/06/2022 17:13:12 - INFO - __main__ -   Batch Number = 131
01/06/2022 17:13:12 - INFO - __main__ -   Batch Number = 132
01/06/2022 17:13:12 - INFO - __main__ -   Batch Number = 133
01/06/2022 17:13:12 - INFO - __main__ -   Batch Number = 134
01/06/2022 17:13:12 - INFO - __main__ -   Batch Number = 135
Evaluating:  86%|████████▌ | 135/157 [00:03<00:00, 44.60it/s]01/06/2022 17:13:12 - INFO - __main__ -   Batch Number = 136
01/06/2022 17:13:12 - INFO - __main__ -   Batch Number = 137
01/06/2022 17:13:12 - INFO - __main__ -   Batch Number = 138
01/06/2022 17:13:12 - INFO - __main__ -   Batch Number = 139
01/06/2022 17:13:12 - INFO - __main__ -   Batch Number = 140
Evaluating:  89%|████████▉ | 140/157 [00:03<00:00, 44.49it/s]01/06/2022 17:13:12 - INFO - __main__ -   Batch Number = 141
01/06/2022 17:13:12 - INFO - __main__ -   Batch Number = 142
01/06/2022 17:13:12 - INFO - __main__ -   Batch Number = 143
01/06/2022 17:13:12 - INFO - __main__ -   Batch Number = 144
01/06/2022 17:13:13 - INFO - __main__ -   Batch Number = 145
Evaluating:  92%|█████████▏| 145/157 [00:03<00:00, 44.60it/s]01/06/2022 17:13:13 - INFO - __main__ -   Batch Number = 146
01/06/2022 17:13:13 - INFO - __main__ -   Batch Number = 147
01/06/2022 17:13:13 - INFO - __main__ -   Batch Number = 148
01/06/2022 17:13:13 - INFO - __main__ -   Batch Number = 149
01/06/2022 17:13:13 - INFO - __main__ -   Batch Number = 150
Evaluating:  96%|█████████▌| 150/157 [00:03<00:00, 44.53it/s]01/06/2022 17:13:13 - INFO - __main__ -   Batch Number = 151
01/06/2022 17:13:13 - INFO - __main__ -   Batch Number = 152
01/06/2022 17:13:13 - INFO - __main__ -   Batch Number = 153
01/06/2022 17:13:13 - INFO - __main__ -   Batch Number = 154
01/06/2022 17:13:13 - INFO - __main__ -   Batch Number = 155
Evaluating:  99%|█████████▊| 155/157 [00:03<00:00, 44.62it/s]01/06/2022 17:13:13 - INFO - __main__ -   Batch Number = 156
01/06/2022 17:13:13 - INFO - __main__ -   Batch Number = 157
Evaluating: 100%|██████████| 157/157 [00:03<00:00, 43.07it/s]
01/06/2022 17:13:13 - INFO - __main__ -     Evaluation done in total 3.646047 secs (0.002915 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/predictions_en_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/nbest_predictions_en_.json
01/06/2022 17:13:16 - INFO - __main__ -   Results = OrderedDict([('exact', 71.17647058823529), ('f1', 82.58369404836603), ('total', 1190), ('HasAns_exact', 71.17647058823529), ('HasAns_f1', 82.58369404836603), ('HasAns_total', 1190), ('best_exact', 71.17647058823529), ('best_exact_thresh', 0.0), ('best_f1', 82.58369404836603), ('best_f1_thresh', 0.0)])
PyTorch version 1.10.0+cu111 available.
01/06/2022 17:14:17 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/06/2022 17:14:17 - INFO - root -   save model
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /root/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/06/2022 17:14:23 - INFO - __main__ -   lang2id = None
01/06/2022 17:14:26 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, data_dir='/root/Desktop/cloud-emea-copy/data//xquad', output_dir='/root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8', max_seq_length=384, train_file='/root/Desktop/cloud-emea-copy/data//xquad/train-v1.1.json', predict_file='/root/Desktop/cloud-emea-copy/data//xquad/dev-v1.1.json', eval_test_set=False, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=4, per_gpu_eval_batch_size=8, gradient_accumulation_steps=8, learning_rate=0.0003, weight_decay=0.0001, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=15.0, max_steps=-1, save_steps=1000, warmup_steps=500, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', eval_lang='en', predict_langs='ar,de,el,es,hi,ru,th,tr,vi,zh', train_lang='en', log_file='/root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='qna', threads=8, version_2_with_negative=False, verbose_logging=False, n_best_size=20, max_query_length=64, max_answer_length=30, doc_stride=128, null_score_diff_threshold=0.0, predict_task_adapter='output/squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8_s1/checkpoint-best/qna', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix=None, en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/06/2022 17:14:26 - INFO - __main__ -   Results: {}
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /root/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/06/2022 17:14:33 - INFO - __main__ -   lang2id = None
01/06/2022 17:14:33 - INFO - __main__ -   Evaluating the model on the test set of all languages specified
01/06/2022 17:14:33 - INFO - __main__ -   Adapter will be loaded from this path: output/squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8_s1/checkpoint-best/qna
01/06/2022 17:14:33 - INFO - root -   Trying to decide if add adapter
01/06/2022 17:14:33 - INFO - root -   loading task adapter
Loading module configuration from output/squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8_s1/checkpoint-best/qna/adapter_config.json
Adding adapter 'qna' of type 'text_task'.
Loading module weights from output/squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8_s1/checkpoint-best/qna/pytorch_adapter.bin
Loading module configuration from output/squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8_s1/checkpoint-best/qna/head_config.json
Loading module weights from output/squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8_s1/checkpoint-best/qna/pytorch_model_head.bin
01/06/2022 17:14:33 - INFO - root -   loading lang adpater en/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip.
Loading module configuration from /root/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
Loading module weights from /root/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/pytorch_adapter.bin
No matching prediction head found in '/root/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted'
01/06/2022 17:14:35 - INFO - __main__ -   Language adapter for ar not found, using en instead
01/06/2022 17:14:35 - INFO - __main__ -   Set active language adapter to en
01/06/2022 17:14:35 - INFO - __main__ -   Args Adapter Weight = None
01/06/2022 17:14:35 - INFO - __main__ -   Adapter Languages = ['en']
01/06/2022 17:14:35 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
en en/wiki@ukp
  0%|          | 0/48 [00:00<?, ?it/s] 33%|███▎      | 16/48 [00:00<00:00, 139.33it/s] 62%|██████▎   | 30/48 [00:00<00:00, 133.22it/s] 96%|█████████▌| 46/48 [00:00<00:00, 142.27it/s]100%|██████████| 48/48 [00:00<00:00, 141.99it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<07:00,  2.83it/s]convert squad examples to features:  27%|██▋       | 321/1190 [00:00<00:01, 705.77it/s]convert squad examples to features:  35%|███▍      | 413/1190 [00:01<00:01, 415.85it/s]convert squad examples to features:  86%|████████▌ | 1025/1190 [00:01<00:00, 1276.96it/s]convert squad examples to features: 100%|██████████| 1190/1190 [00:01<00:00, 940.10it/s] /root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|██████████| 1190/1190 [00:00<00:00, 599114.36it/s]
01/06/2022 17:14:37 - INFO - __main__ -   Local Rank = -1
01/06/2022 17:14:37 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.ar.json_bert-base-multilingual-cased_384_ar
01/06/2022 17:14:39 - INFO - __main__ -   ***** Running evaluation  *****
01/06/2022 17:14:39 - INFO - __main__ -     Num examples = 1455
01/06/2022 17:14:39 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/182 [00:00<?, ?it/s]01/06/2022 17:14:39 - INFO - __main__ -   Batch Number = 1
01/06/2022 17:14:39 - INFO - __main__ -   Batch Number = 2
01/06/2022 17:14:39 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|▏         | 3/182 [00:00<00:06, 27.22it/s]01/06/2022 17:14:39 - INFO - __main__ -   Batch Number = 4
01/06/2022 17:14:39 - INFO - __main__ -   Batch Number = 5
01/06/2022 17:14:39 - INFO - __main__ -   Batch Number = 6
01/06/2022 17:14:39 - INFO - __main__ -   Batch Number = 7
Evaluating:   4%|▍         | 7/182 [00:00<00:06, 29.06it/s]01/06/2022 17:14:39 - INFO - __main__ -   Batch Number = 8
01/06/2022 17:14:39 - INFO - __main__ -   Batch Number = 9
01/06/2022 17:14:39 - INFO - __main__ -   Batch Number = 10
01/06/2022 17:14:39 - INFO - __main__ -   Batch Number = 11
Evaluating:   6%|▌         | 11/182 [00:00<00:05, 32.94it/s]01/06/2022 17:14:39 - INFO - __main__ -   Batch Number = 12
01/06/2022 17:14:39 - INFO - __main__ -   Batch Number = 13
01/06/2022 17:14:39 - INFO - __main__ -   Batch Number = 14
01/06/2022 17:14:39 - INFO - __main__ -   Batch Number = 15
Evaluating:   8%|▊         | 15/182 [00:00<00:04, 35.44it/s]01/06/2022 17:14:39 - INFO - __main__ -   Batch Number = 16
01/06/2022 17:14:39 - INFO - __main__ -   Batch Number = 17
01/06/2022 17:14:39 - INFO - __main__ -   Batch Number = 18
01/06/2022 17:14:39 - INFO - __main__ -   Batch Number = 19
01/06/2022 17:14:39 - INFO - __main__ -   Batch Number = 20
Evaluating:  11%|█         | 20/182 [00:00<00:04, 38.59it/s]01/06/2022 17:14:39 - INFO - __main__ -   Batch Number = 21
01/06/2022 17:14:39 - INFO - __main__ -   Batch Number = 22
01/06/2022 17:14:39 - INFO - __main__ -   Batch Number = 23
01/06/2022 17:14:39 - INFO - __main__ -   Batch Number = 24
01/06/2022 17:14:39 - INFO - __main__ -   Batch Number = 25
Evaluating:  14%|█▎        | 25/182 [00:00<00:03, 40.79it/s]01/06/2022 17:14:39 - INFO - __main__ -   Batch Number = 26
01/06/2022 17:14:39 - INFO - __main__ -   Batch Number = 27
01/06/2022 17:14:39 - INFO - __main__ -   Batch Number = 28
01/06/2022 17:14:39 - INFO - __main__ -   Batch Number = 29
01/06/2022 17:14:39 - INFO - __main__ -   Batch Number = 30
Evaluating:  16%|█▋        | 30/182 [00:00<00:03, 42.21it/s]01/06/2022 17:14:39 - INFO - __main__ -   Batch Number = 31
01/06/2022 17:14:39 - INFO - __main__ -   Batch Number = 32
01/06/2022 17:14:39 - INFO - __main__ -   Batch Number = 33
01/06/2022 17:14:39 - INFO - __main__ -   Batch Number = 34
01/06/2022 17:14:40 - INFO - __main__ -   Batch Number = 35
Evaluating:  19%|█▉        | 35/182 [00:00<00:03, 43.00it/s]01/06/2022 17:14:40 - INFO - __main__ -   Batch Number = 36
01/06/2022 17:14:40 - INFO - __main__ -   Batch Number = 37
01/06/2022 17:14:40 - INFO - __main__ -   Batch Number = 38
01/06/2022 17:14:40 - INFO - __main__ -   Batch Number = 39
01/06/2022 17:14:40 - INFO - __main__ -   Batch Number = 40
Evaluating:  22%|██▏       | 40/182 [00:01<00:03, 43.61it/s]01/06/2022 17:14:40 - INFO - __main__ -   Batch Number = 41
01/06/2022 17:14:40 - INFO - __main__ -   Batch Number = 42
01/06/2022 17:14:40 - INFO - __main__ -   Batch Number = 43
01/06/2022 17:14:40 - INFO - __main__ -   Batch Number = 44
01/06/2022 17:14:40 - INFO - __main__ -   Batch Number = 45
Evaluating:  25%|██▍       | 45/182 [00:01<00:03, 43.88it/s]01/06/2022 17:14:40 - INFO - __main__ -   Batch Number = 46
01/06/2022 17:14:40 - INFO - __main__ -   Batch Number = 47
01/06/2022 17:14:40 - INFO - __main__ -   Batch Number = 48
01/06/2022 17:14:40 - INFO - __main__ -   Batch Number = 49
01/06/2022 17:14:40 - INFO - __main__ -   Batch Number = 50
Evaluating:  27%|██▋       | 50/182 [00:01<00:02, 44.29it/s]01/06/2022 17:14:40 - INFO - __main__ -   Batch Number = 51
01/06/2022 17:14:40 - INFO - __main__ -   Batch Number = 52
01/06/2022 17:14:40 - INFO - __main__ -   Batch Number = 53
01/06/2022 17:14:40 - INFO - __main__ -   Batch Number = 54
01/06/2022 17:14:40 - INFO - __main__ -   Batch Number = 55
Evaluating:  30%|███       | 55/182 [00:01<00:02, 44.50it/s]01/06/2022 17:14:40 - INFO - __main__ -   Batch Number = 56
01/06/2022 17:14:40 - INFO - __main__ -   Batch Number = 57
01/06/2022 17:14:40 - INFO - __main__ -   Batch Number = 58
01/06/2022 17:14:40 - INFO - __main__ -   Batch Number = 59
01/06/2022 17:14:40 - INFO - __main__ -   Batch Number = 60
Evaluating:  33%|███▎      | 60/182 [00:01<00:02, 44.56it/s]01/06/2022 17:14:40 - INFO - __main__ -   Batch Number = 61
01/06/2022 17:14:40 - INFO - __main__ -   Batch Number = 62
01/06/2022 17:14:40 - INFO - __main__ -   Batch Number = 63
01/06/2022 17:14:40 - INFO - __main__ -   Batch Number = 64
01/06/2022 17:14:40 - INFO - __main__ -   Batch Number = 65
Evaluating:  36%|███▌      | 65/182 [00:01<00:02, 44.71it/s]01/06/2022 17:14:40 - INFO - __main__ -   Batch Number = 66
01/06/2022 17:14:40 - INFO - __main__ -   Batch Number = 67
01/06/2022 17:14:40 - INFO - __main__ -   Batch Number = 68
01/06/2022 17:14:40 - INFO - __main__ -   Batch Number = 69
01/06/2022 17:14:40 - INFO - __main__ -   Batch Number = 70
Evaluating:  38%|███▊      | 70/182 [00:01<00:02, 44.76it/s]01/06/2022 17:14:40 - INFO - __main__ -   Batch Number = 71
01/06/2022 17:14:40 - INFO - __main__ -   Batch Number = 72
01/06/2022 17:14:40 - INFO - __main__ -   Batch Number = 73
01/06/2022 17:14:40 - INFO - __main__ -   Batch Number = 74
01/06/2022 17:14:40 - INFO - __main__ -   Batch Number = 75
Evaluating:  41%|████      | 75/182 [00:01<00:02, 44.69it/s]01/06/2022 17:14:40 - INFO - __main__ -   Batch Number = 76
01/06/2022 17:14:40 - INFO - __main__ -   Batch Number = 77
01/06/2022 17:14:40 - INFO - __main__ -   Batch Number = 78
01/06/2022 17:14:40 - INFO - __main__ -   Batch Number = 79
01/06/2022 17:14:41 - INFO - __main__ -   Batch Number = 80
Evaluating:  44%|████▍     | 80/182 [00:01<00:02, 44.65it/s]01/06/2022 17:14:41 - INFO - __main__ -   Batch Number = 81
01/06/2022 17:14:41 - INFO - __main__ -   Batch Number = 82
01/06/2022 17:14:41 - INFO - __main__ -   Batch Number = 83
01/06/2022 17:14:41 - INFO - __main__ -   Batch Number = 84
01/06/2022 17:14:41 - INFO - __main__ -   Batch Number = 85
Evaluating:  47%|████▋     | 85/182 [00:02<00:02, 44.64it/s]01/06/2022 17:14:41 - INFO - __main__ -   Batch Number = 86
01/06/2022 17:14:41 - INFO - __main__ -   Batch Number = 87
01/06/2022 17:14:41 - INFO - __main__ -   Batch Number = 88
01/06/2022 17:14:41 - INFO - __main__ -   Batch Number = 89
01/06/2022 17:14:41 - INFO - __main__ -   Batch Number = 90
Evaluating:  49%|████▉     | 90/182 [00:02<00:02, 44.65it/s]01/06/2022 17:14:41 - INFO - __main__ -   Batch Number = 91
01/06/2022 17:14:41 - INFO - __main__ -   Batch Number = 92
01/06/2022 17:14:41 - INFO - __main__ -   Batch Number = 93
01/06/2022 17:14:41 - INFO - __main__ -   Batch Number = 94
01/06/2022 17:14:41 - INFO - __main__ -   Batch Number = 95
Evaluating:  52%|█████▏    | 95/182 [00:02<00:01, 44.87it/s]01/06/2022 17:14:41 - INFO - __main__ -   Batch Number = 96
01/06/2022 17:14:41 - INFO - __main__ -   Batch Number = 97
01/06/2022 17:14:41 - INFO - __main__ -   Batch Number = 98
01/06/2022 17:14:41 - INFO - __main__ -   Batch Number = 99
01/06/2022 17:14:41 - INFO - __main__ -   Batch Number = 100
Evaluating:  55%|█████▍    | 100/182 [00:02<00:01, 44.91it/s]01/06/2022 17:14:41 - INFO - __main__ -   Batch Number = 101
01/06/2022 17:14:41 - INFO - __main__ -   Batch Number = 102
01/06/2022 17:14:41 - INFO - __main__ -   Batch Number = 103
01/06/2022 17:14:41 - INFO - __main__ -   Batch Number = 104
01/06/2022 17:14:41 - INFO - __main__ -   Batch Number = 105
Evaluating:  58%|█████▊    | 105/182 [00:02<00:01, 44.86it/s]01/06/2022 17:14:41 - INFO - __main__ -   Batch Number = 106
01/06/2022 17:14:41 - INFO - __main__ -   Batch Number = 107
01/06/2022 17:14:41 - INFO - __main__ -   Batch Number = 108
01/06/2022 17:14:41 - INFO - __main__ -   Batch Number = 109
01/06/2022 17:14:41 - INFO - __main__ -   Batch Number = 110
Evaluating:  60%|██████    | 110/182 [00:02<00:01, 44.92it/s]01/06/2022 17:14:41 - INFO - __main__ -   Batch Number = 111
01/06/2022 17:14:41 - INFO - __main__ -   Batch Number = 112
01/06/2022 17:14:41 - INFO - __main__ -   Batch Number = 113
01/06/2022 17:14:41 - INFO - __main__ -   Batch Number = 114
01/06/2022 17:14:41 - INFO - __main__ -   Batch Number = 115
Evaluating:  63%|██████▎   | 115/182 [00:02<00:01, 44.99it/s]01/06/2022 17:14:41 - INFO - __main__ -   Batch Number = 116
01/06/2022 17:14:41 - INFO - __main__ -   Batch Number = 117
01/06/2022 17:14:41 - INFO - __main__ -   Batch Number = 118
01/06/2022 17:14:41 - INFO - __main__ -   Batch Number = 119
01/06/2022 17:14:41 - INFO - __main__ -   Batch Number = 120
Evaluating:  66%|██████▌   | 120/182 [00:02<00:01, 44.77it/s]01/06/2022 17:14:41 - INFO - __main__ -   Batch Number = 121
01/06/2022 17:14:41 - INFO - __main__ -   Batch Number = 122
01/06/2022 17:14:41 - INFO - __main__ -   Batch Number = 123
01/06/2022 17:14:41 - INFO - __main__ -   Batch Number = 124
01/06/2022 17:14:42 - INFO - __main__ -   Batch Number = 125
Evaluating:  69%|██████▊   | 125/182 [00:02<00:01, 44.86it/s]01/06/2022 17:14:42 - INFO - __main__ -   Batch Number = 126
01/06/2022 17:14:42 - INFO - __main__ -   Batch Number = 127
01/06/2022 17:14:42 - INFO - __main__ -   Batch Number = 128
01/06/2022 17:14:42 - INFO - __main__ -   Batch Number = 129
01/06/2022 17:14:42 - INFO - __main__ -   Batch Number = 130
Evaluating:  71%|███████▏  | 130/182 [00:03<00:01, 44.78it/s]01/06/2022 17:14:42 - INFO - __main__ -   Batch Number = 131
01/06/2022 17:14:42 - INFO - __main__ -   Batch Number = 132
01/06/2022 17:14:42 - INFO - __main__ -   Batch Number = 133
01/06/2022 17:14:42 - INFO - __main__ -   Batch Number = 134
01/06/2022 17:14:42 - INFO - __main__ -   Batch Number = 135
Evaluating:  74%|███████▍  | 135/182 [00:03<00:01, 44.75it/s]01/06/2022 17:14:42 - INFO - __main__ -   Batch Number = 136
01/06/2022 17:14:42 - INFO - __main__ -   Batch Number = 137
01/06/2022 17:14:42 - INFO - __main__ -   Batch Number = 138
01/06/2022 17:14:42 - INFO - __main__ -   Batch Number = 139
01/06/2022 17:14:42 - INFO - __main__ -   Batch Number = 140
Evaluating:  77%|███████▋  | 140/182 [00:03<00:00, 44.68it/s]01/06/2022 17:14:42 - INFO - __main__ -   Batch Number = 141
01/06/2022 17:14:42 - INFO - __main__ -   Batch Number = 142
01/06/2022 17:14:42 - INFO - __main__ -   Batch Number = 143
01/06/2022 17:14:42 - INFO - __main__ -   Batch Number = 144
01/06/2022 17:14:42 - INFO - __main__ -   Batch Number = 145
Evaluating:  80%|███████▉  | 145/182 [00:03<00:00, 44.78it/s]01/06/2022 17:14:42 - INFO - __main__ -   Batch Number = 146
01/06/2022 17:14:42 - INFO - __main__ -   Batch Number = 147
01/06/2022 17:14:42 - INFO - __main__ -   Batch Number = 148
01/06/2022 17:14:42 - INFO - __main__ -   Batch Number = 149
01/06/2022 17:14:42 - INFO - __main__ -   Batch Number = 150
Evaluating:  82%|████████▏ | 150/182 [00:03<00:00, 44.64it/s]01/06/2022 17:14:42 - INFO - __main__ -   Batch Number = 151
01/06/2022 17:14:42 - INFO - __main__ -   Batch Number = 152
01/06/2022 17:14:42 - INFO - __main__ -   Batch Number = 153
01/06/2022 17:14:42 - INFO - __main__ -   Batch Number = 154
01/06/2022 17:14:42 - INFO - __main__ -   Batch Number = 155
Evaluating:  85%|████████▌ | 155/182 [00:03<00:00, 44.88it/s]01/06/2022 17:14:42 - INFO - __main__ -   Batch Number = 156
01/06/2022 17:14:42 - INFO - __main__ -   Batch Number = 157
01/06/2022 17:14:42 - INFO - __main__ -   Batch Number = 158
01/06/2022 17:14:42 - INFO - __main__ -   Batch Number = 159
01/06/2022 17:14:42 - INFO - __main__ -   Batch Number = 160
Evaluating:  88%|████████▊ | 160/182 [00:03<00:00, 44.76it/s]01/06/2022 17:14:42 - INFO - __main__ -   Batch Number = 161
01/06/2022 17:14:42 - INFO - __main__ -   Batch Number = 162
01/06/2022 17:14:42 - INFO - __main__ -   Batch Number = 163
01/06/2022 17:14:42 - INFO - __main__ -   Batch Number = 164
01/06/2022 17:14:42 - INFO - __main__ -   Batch Number = 165
Evaluating:  91%|█████████ | 165/182 [00:03<00:00, 44.70it/s]01/06/2022 17:14:42 - INFO - __main__ -   Batch Number = 166
01/06/2022 17:14:42 - INFO - __main__ -   Batch Number = 167
01/06/2022 17:14:42 - INFO - __main__ -   Batch Number = 168
01/06/2022 17:14:42 - INFO - __main__ -   Batch Number = 169
01/06/2022 17:14:43 - INFO - __main__ -   Batch Number = 170
Evaluating:  93%|█████████▎| 170/182 [00:03<00:00, 44.41it/s]01/06/2022 17:14:43 - INFO - __main__ -   Batch Number = 171
01/06/2022 17:14:43 - INFO - __main__ -   Batch Number = 172
01/06/2022 17:14:43 - INFO - __main__ -   Batch Number = 173
01/06/2022 17:14:43 - INFO - __main__ -   Batch Number = 174
01/06/2022 17:14:43 - INFO - __main__ -   Batch Number = 175
Evaluating:  96%|█████████▌| 175/182 [00:04<00:00, 44.43it/s]01/06/2022 17:14:43 - INFO - __main__ -   Batch Number = 176
01/06/2022 17:14:43 - INFO - __main__ -   Batch Number = 177
01/06/2022 17:14:43 - INFO - __main__ -   Batch Number = 178
01/06/2022 17:14:43 - INFO - __main__ -   Batch Number = 179
01/06/2022 17:14:43 - INFO - __main__ -   Batch Number = 180
Evaluating:  99%|█████████▉| 180/182 [00:04<00:00, 44.43it/s]01/06/2022 17:14:43 - INFO - __main__ -   Batch Number = 181
01/06/2022 17:14:43 - INFO - __main__ -   Batch Number = 182
Evaluating: 100%|██████████| 182/182 [00:04<00:00, 43.56it/s]
01/06/2022 17:14:43 - INFO - __main__ -     Evaluation done in total 4.178599 secs (0.002872 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/predictions_ar_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/nbest_predictions_ar_.json
01/06/2022 17:14:46 - INFO - __main__ -   Results = OrderedDict([('exact', 38.0672268907563), ('f1', 53.90859006346229), ('total', 1190), ('HasAns_exact', 38.0672268907563), ('HasAns_f1', 53.90859006346229), ('HasAns_total', 1190), ('best_exact', 38.0672268907563), ('best_exact_thresh', 0.0), ('best_f1', 53.90859006346229), ('best_f1_thresh', 0.0)])
01/06/2022 17:14:46 - INFO - __main__ -   Language adapter for de not found, using en instead
01/06/2022 17:14:46 - INFO - __main__ -   Set active language adapter to en
01/06/2022 17:14:46 - INFO - __main__ -   Args Adapter Weight = None
01/06/2022 17:14:46 - INFO - __main__ -   Adapter Languages = ['en']
01/06/2022 17:14:46 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
  0%|          | 0/48 [00:00<?, ?it/s] 33%|███▎      | 16/48 [00:00<00:00, 121.79it/s] 60%|██████    | 29/48 [00:00<00:00, 114.15it/s] 92%|█████████▏| 44/48 [00:00<00:00, 125.38it/s]100%|██████████| 48/48 [00:00<00:00, 124.58it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<06:14,  3.18it/s]convert squad examples to features:  24%|██▍       | 289/1190 [00:00<00:01, 710.14it/s]convert squad examples to features:  32%|███▏      | 385/1190 [00:00<00:01, 444.00it/s]convert squad examples to features:  81%|████████  | 961/1190 [00:00<00:00, 1363.64it/s]convert squad examples to features: 100%|██████████| 1190/1190 [00:01<00:00, 935.45it/s]/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|██████████| 1190/1190 [00:00<00:00, 477081.03it/s]
01/06/2022 17:14:49 - INFO - __main__ -   Local Rank = -1
01/06/2022 17:14:49 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.de.json_bert-base-multilingual-cased_384_de
01/06/2022 17:14:49 - INFO - __main__ -   ***** Running evaluation  *****
01/06/2022 17:14:49 - INFO - __main__ -     Num examples = 1296
01/06/2022 17:14:49 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/162 [00:00<?, ?it/s]01/06/2022 17:14:49 - INFO - __main__ -   Batch Number = 1
01/06/2022 17:14:50 - INFO - __main__ -   Batch Number = 2
01/06/2022 17:14:50 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|▏         | 3/162 [00:00<00:05, 29.33it/s]01/06/2022 17:14:50 - INFO - __main__ -   Batch Number = 4
01/06/2022 17:14:50 - INFO - __main__ -   Batch Number = 5
01/06/2022 17:14:50 - INFO - __main__ -   Batch Number = 6
01/06/2022 17:14:50 - INFO - __main__ -   Batch Number = 7
Evaluating:   4%|▍         | 7/162 [00:00<00:04, 33.27it/s]01/06/2022 17:14:50 - INFO - __main__ -   Batch Number = 8
01/06/2022 17:14:50 - INFO - __main__ -   Batch Number = 9
01/06/2022 17:14:50 - INFO - __main__ -   Batch Number = 10
01/06/2022 17:14:50 - INFO - __main__ -   Batch Number = 11
Evaluating:   7%|▋         | 11/162 [00:00<00:04, 35.21it/s]01/06/2022 17:14:50 - INFO - __main__ -   Batch Number = 12
01/06/2022 17:14:50 - INFO - __main__ -   Batch Number = 13
01/06/2022 17:14:50 - INFO - __main__ -   Batch Number = 14
01/06/2022 17:14:50 - INFO - __main__ -   Batch Number = 15
01/06/2022 17:14:50 - INFO - __main__ -   Batch Number = 16
Evaluating:  10%|▉         | 16/162 [00:00<00:03, 39.04it/s]01/06/2022 17:14:50 - INFO - __main__ -   Batch Number = 17
01/06/2022 17:14:50 - INFO - __main__ -   Batch Number = 18
01/06/2022 17:14:50 - INFO - __main__ -   Batch Number = 19
01/06/2022 17:14:50 - INFO - __main__ -   Batch Number = 20
01/06/2022 17:14:50 - INFO - __main__ -   Batch Number = 21
Evaluating:  13%|█▎        | 21/162 [00:00<00:03, 41.00it/s]01/06/2022 17:14:50 - INFO - __main__ -   Batch Number = 22
01/06/2022 17:14:50 - INFO - __main__ -   Batch Number = 23
01/06/2022 17:14:50 - INFO - __main__ -   Batch Number = 24
01/06/2022 17:14:50 - INFO - __main__ -   Batch Number = 25
01/06/2022 17:14:50 - INFO - __main__ -   Batch Number = 26
Evaluating:  16%|█▌        | 26/162 [00:00<00:03, 42.30it/s]01/06/2022 17:14:50 - INFO - __main__ -   Batch Number = 27
01/06/2022 17:14:50 - INFO - __main__ -   Batch Number = 28
01/06/2022 17:14:50 - INFO - __main__ -   Batch Number = 29
01/06/2022 17:14:50 - INFO - __main__ -   Batch Number = 30
01/06/2022 17:14:50 - INFO - __main__ -   Batch Number = 31
Evaluating:  19%|█▉        | 31/162 [00:00<00:03, 43.29it/s]01/06/2022 17:14:50 - INFO - __main__ -   Batch Number = 32
01/06/2022 17:14:50 - INFO - __main__ -   Batch Number = 33
01/06/2022 17:14:50 - INFO - __main__ -   Batch Number = 34
01/06/2022 17:14:50 - INFO - __main__ -   Batch Number = 35
01/06/2022 17:14:50 - INFO - __main__ -   Batch Number = 36
Evaluating:  22%|██▏       | 36/162 [00:00<00:02, 43.71it/s]01/06/2022 17:14:50 - INFO - __main__ -   Batch Number = 37
01/06/2022 17:14:50 - INFO - __main__ -   Batch Number = 38
01/06/2022 17:14:50 - INFO - __main__ -   Batch Number = 39
01/06/2022 17:14:50 - INFO - __main__ -   Batch Number = 40
01/06/2022 17:14:50 - INFO - __main__ -   Batch Number = 41
Evaluating:  25%|██▌       | 41/162 [00:00<00:02, 44.12it/s]01/06/2022 17:14:50 - INFO - __main__ -   Batch Number = 42
01/06/2022 17:14:50 - INFO - __main__ -   Batch Number = 43
01/06/2022 17:14:51 - INFO - __main__ -   Batch Number = 44
01/06/2022 17:14:51 - INFO - __main__ -   Batch Number = 45
01/06/2022 17:14:51 - INFO - __main__ -   Batch Number = 46
Evaluating:  28%|██▊       | 46/162 [00:01<00:02, 44.35it/s]01/06/2022 17:14:51 - INFO - __main__ -   Batch Number = 47
01/06/2022 17:14:51 - INFO - __main__ -   Batch Number = 48
01/06/2022 17:14:51 - INFO - __main__ -   Batch Number = 49
01/06/2022 17:14:51 - INFO - __main__ -   Batch Number = 50
01/06/2022 17:14:51 - INFO - __main__ -   Batch Number = 51
Evaluating:  31%|███▏      | 51/162 [00:01<00:02, 44.41it/s]01/06/2022 17:14:51 - INFO - __main__ -   Batch Number = 52
01/06/2022 17:14:51 - INFO - __main__ -   Batch Number = 53
01/06/2022 17:14:51 - INFO - __main__ -   Batch Number = 54
01/06/2022 17:14:51 - INFO - __main__ -   Batch Number = 55
01/06/2022 17:14:51 - INFO - __main__ -   Batch Number = 56
Evaluating:  35%|███▍      | 56/162 [00:01<00:02, 44.60it/s]01/06/2022 17:14:51 - INFO - __main__ -   Batch Number = 57
01/06/2022 17:14:51 - INFO - __main__ -   Batch Number = 58
01/06/2022 17:14:51 - INFO - __main__ -   Batch Number = 59
01/06/2022 17:14:51 - INFO - __main__ -   Batch Number = 60
01/06/2022 17:14:51 - INFO - __main__ -   Batch Number = 61
Evaluating:  38%|███▊      | 61/162 [00:01<00:02, 44.69it/s]01/06/2022 17:14:51 - INFO - __main__ -   Batch Number = 62
01/06/2022 17:14:51 - INFO - __main__ -   Batch Number = 63
01/06/2022 17:14:51 - INFO - __main__ -   Batch Number = 64
01/06/2022 17:14:51 - INFO - __main__ -   Batch Number = 65
01/06/2022 17:14:51 - INFO - __main__ -   Batch Number = 66
Evaluating:  41%|████      | 66/162 [00:01<00:02, 44.66it/s]01/06/2022 17:14:51 - INFO - __main__ -   Batch Number = 67
01/06/2022 17:14:51 - INFO - __main__ -   Batch Number = 68
01/06/2022 17:14:51 - INFO - __main__ -   Batch Number = 69
01/06/2022 17:14:51 - INFO - __main__ -   Batch Number = 70
01/06/2022 17:14:51 - INFO - __main__ -   Batch Number = 71
Evaluating:  44%|████▍     | 71/162 [00:01<00:02, 44.77it/s]01/06/2022 17:14:51 - INFO - __main__ -   Batch Number = 72
01/06/2022 17:14:51 - INFO - __main__ -   Batch Number = 73
01/06/2022 17:14:51 - INFO - __main__ -   Batch Number = 74
01/06/2022 17:14:51 - INFO - __main__ -   Batch Number = 75
01/06/2022 17:14:51 - INFO - __main__ -   Batch Number = 76
Evaluating:  47%|████▋     | 76/162 [00:01<00:01, 44.82it/s]01/06/2022 17:14:51 - INFO - __main__ -   Batch Number = 77
01/06/2022 17:14:51 - INFO - __main__ -   Batch Number = 78
01/06/2022 17:14:51 - INFO - __main__ -   Batch Number = 79
01/06/2022 17:14:51 - INFO - __main__ -   Batch Number = 80
01/06/2022 17:14:51 - INFO - __main__ -   Batch Number = 81
Evaluating:  50%|█████     | 81/162 [00:01<00:01, 44.65it/s]01/06/2022 17:14:51 - INFO - __main__ -   Batch Number = 82
01/06/2022 17:14:51 - INFO - __main__ -   Batch Number = 83
01/06/2022 17:14:51 - INFO - __main__ -   Batch Number = 84
01/06/2022 17:14:51 - INFO - __main__ -   Batch Number = 85
01/06/2022 17:14:51 - INFO - __main__ -   Batch Number = 86
Evaluating:  53%|█████▎    | 86/162 [00:01<00:01, 44.70it/s]01/06/2022 17:14:51 - INFO - __main__ -   Batch Number = 87
01/06/2022 17:14:52 - INFO - __main__ -   Batch Number = 88
01/06/2022 17:14:52 - INFO - __main__ -   Batch Number = 89
01/06/2022 17:14:52 - INFO - __main__ -   Batch Number = 90
01/06/2022 17:14:52 - INFO - __main__ -   Batch Number = 91
Evaluating:  56%|█████▌    | 91/162 [00:02<00:01, 44.69it/s]01/06/2022 17:14:52 - INFO - __main__ -   Batch Number = 92
01/06/2022 17:14:52 - INFO - __main__ -   Batch Number = 93
01/06/2022 17:14:52 - INFO - __main__ -   Batch Number = 94
01/06/2022 17:14:52 - INFO - __main__ -   Batch Number = 95
01/06/2022 17:14:52 - INFO - __main__ -   Batch Number = 96
Evaluating:  59%|█████▉    | 96/162 [00:02<00:01, 44.66it/s]01/06/2022 17:14:52 - INFO - __main__ -   Batch Number = 97
01/06/2022 17:14:52 - INFO - __main__ -   Batch Number = 98
01/06/2022 17:14:52 - INFO - __main__ -   Batch Number = 99
01/06/2022 17:14:52 - INFO - __main__ -   Batch Number = 100
01/06/2022 17:14:52 - INFO - __main__ -   Batch Number = 101
Evaluating:  62%|██████▏   | 101/162 [00:02<00:01, 44.63it/s]01/06/2022 17:14:52 - INFO - __main__ -   Batch Number = 102
01/06/2022 17:14:52 - INFO - __main__ -   Batch Number = 103
01/06/2022 17:14:52 - INFO - __main__ -   Batch Number = 104
01/06/2022 17:14:52 - INFO - __main__ -   Batch Number = 105
01/06/2022 17:14:52 - INFO - __main__ -   Batch Number = 106
Evaluating:  65%|██████▌   | 106/162 [00:02<00:01, 44.76it/s]01/06/2022 17:14:52 - INFO - __main__ -   Batch Number = 107
01/06/2022 17:14:52 - INFO - __main__ -   Batch Number = 108
01/06/2022 17:14:52 - INFO - __main__ -   Batch Number = 109
01/06/2022 17:14:52 - INFO - __main__ -   Batch Number = 110
01/06/2022 17:14:52 - INFO - __main__ -   Batch Number = 111
Evaluating:  69%|██████▊   | 111/162 [00:02<00:01, 44.56it/s]01/06/2022 17:14:52 - INFO - __main__ -   Batch Number = 112
01/06/2022 17:14:52 - INFO - __main__ -   Batch Number = 113
01/06/2022 17:14:52 - INFO - __main__ -   Batch Number = 114
01/06/2022 17:14:52 - INFO - __main__ -   Batch Number = 115
01/06/2022 17:14:52 - INFO - __main__ -   Batch Number = 116
Evaluating:  72%|███████▏  | 116/162 [00:02<00:01, 44.70it/s]01/06/2022 17:14:52 - INFO - __main__ -   Batch Number = 117
01/06/2022 17:14:52 - INFO - __main__ -   Batch Number = 118
01/06/2022 17:14:52 - INFO - __main__ -   Batch Number = 119
01/06/2022 17:14:52 - INFO - __main__ -   Batch Number = 120
01/06/2022 17:14:52 - INFO - __main__ -   Batch Number = 121
Evaluating:  75%|███████▍  | 121/162 [00:02<00:00, 44.62it/s]01/06/2022 17:14:52 - INFO - __main__ -   Batch Number = 122
01/06/2022 17:14:52 - INFO - __main__ -   Batch Number = 123
01/06/2022 17:14:52 - INFO - __main__ -   Batch Number = 124
01/06/2022 17:14:52 - INFO - __main__ -   Batch Number = 125
01/06/2022 17:14:52 - INFO - __main__ -   Batch Number = 126
Evaluating:  78%|███████▊  | 126/162 [00:02<00:00, 44.56it/s]01/06/2022 17:14:52 - INFO - __main__ -   Batch Number = 127
01/06/2022 17:14:52 - INFO - __main__ -   Batch Number = 128
01/06/2022 17:14:52 - INFO - __main__ -   Batch Number = 129
01/06/2022 17:14:52 - INFO - __main__ -   Batch Number = 130
01/06/2022 17:14:52 - INFO - __main__ -   Batch Number = 131
Evaluating:  81%|████████  | 131/162 [00:03<00:00, 44.53it/s]01/06/2022 17:14:52 - INFO - __main__ -   Batch Number = 132
01/06/2022 17:14:53 - INFO - __main__ -   Batch Number = 133
01/06/2022 17:14:53 - INFO - __main__ -   Batch Number = 134
01/06/2022 17:14:53 - INFO - __main__ -   Batch Number = 135
01/06/2022 17:14:53 - INFO - __main__ -   Batch Number = 136
Evaluating:  84%|████████▍ | 136/162 [00:03<00:00, 44.43it/s]01/06/2022 17:14:53 - INFO - __main__ -   Batch Number = 137
01/06/2022 17:14:53 - INFO - __main__ -   Batch Number = 138
01/06/2022 17:14:53 - INFO - __main__ -   Batch Number = 139
01/06/2022 17:14:53 - INFO - __main__ -   Batch Number = 140
01/06/2022 17:14:53 - INFO - __main__ -   Batch Number = 141
Evaluating:  87%|████████▋ | 141/162 [00:03<00:00, 44.41it/s]01/06/2022 17:14:53 - INFO - __main__ -   Batch Number = 142
01/06/2022 17:14:53 - INFO - __main__ -   Batch Number = 143
01/06/2022 17:14:53 - INFO - __main__ -   Batch Number = 144
01/06/2022 17:14:53 - INFO - __main__ -   Batch Number = 145
01/06/2022 17:14:53 - INFO - __main__ -   Batch Number = 146
Evaluating:  90%|█████████ | 146/162 [00:03<00:00, 44.36it/s]01/06/2022 17:14:53 - INFO - __main__ -   Batch Number = 147
01/06/2022 17:14:53 - INFO - __main__ -   Batch Number = 148
01/06/2022 17:14:53 - INFO - __main__ -   Batch Number = 149
01/06/2022 17:14:53 - INFO - __main__ -   Batch Number = 150
01/06/2022 17:14:53 - INFO - __main__ -   Batch Number = 151
Evaluating:  93%|█████████▎| 151/162 [00:03<00:00, 44.47it/s]01/06/2022 17:14:53 - INFO - __main__ -   Batch Number = 152
01/06/2022 17:14:53 - INFO - __main__ -   Batch Number = 153
01/06/2022 17:14:53 - INFO - __main__ -   Batch Number = 154
01/06/2022 17:14:53 - INFO - __main__ -   Batch Number = 155
01/06/2022 17:14:53 - INFO - __main__ -   Batch Number = 156
Evaluating:  96%|█████████▋| 156/162 [00:03<00:00, 42.95it/s]01/06/2022 17:14:53 - INFO - __main__ -   Batch Number = 157
01/06/2022 17:14:53 - INFO - __main__ -   Batch Number = 158
01/06/2022 17:14:53 - INFO - __main__ -   Batch Number = 159
01/06/2022 17:14:53 - INFO - __main__ -   Batch Number = 160
01/06/2022 17:14:53 - INFO - __main__ -   Batch Number = 161
Evaluating:  99%|█████████▉| 161/162 [00:03<00:00, 43.35it/s]01/06/2022 17:14:53 - INFO - __main__ -   Batch Number = 162
Evaluating: 100%|██████████| 162/162 [00:03<00:00, 43.59it/s]
01/06/2022 17:14:53 - INFO - __main__ -     Evaluation done in total 3.716629 secs (0.002868 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/predictions_de_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/nbest_predictions_de_.json
01/06/2022 17:14:57 - INFO - __main__ -   Results = OrderedDict([('exact', 53.69747899159664), ('f1', 68.32462767503749), ('total', 1190), ('HasAns_exact', 53.69747899159664), ('HasAns_f1', 68.32462767503749), ('HasAns_total', 1190), ('best_exact', 53.69747899159664), ('best_exact_thresh', 0.0), ('best_f1', 68.32462767503749), ('best_f1_thresh', 0.0)])
01/06/2022 17:14:57 - INFO - __main__ -   Language adapter for el not found, using en instead
01/06/2022 17:14:57 - INFO - __main__ -   Set active language adapter to en
01/06/2022 17:14:57 - INFO - __main__ -   Args Adapter Weight = None
01/06/2022 17:14:57 - INFO - __main__ -   Adapter Languages = ['en']
01/06/2022 17:14:57 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
  0%|          | 0/48 [00:00<?, ?it/s] 29%|██▉       | 14/48 [00:00<00:00, 138.04it/s] 58%|█████▊    | 28/48 [00:00<00:00, 104.63it/s] 83%|████████▎ | 40/48 [00:00<00:00, 109.79it/s]100%|██████████| 48/48 [00:00<00:00, 114.57it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<07:29,  2.65it/s]convert squad examples to features:  22%|██▏       | 257/1190 [00:00<00:02, 458.04it/s]convert squad examples to features:  32%|███▏      | 385/1190 [00:01<00:04, 198.76it/s]convert squad examples to features: 100%|██████████| 1190/1190 [00:01<00:00, 645.05it/s]/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|██████████| 1190/1190 [00:00<00:00, 618107.96it/s]
01/06/2022 17:15:00 - INFO - __main__ -   Local Rank = -1
01/06/2022 17:15:00 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.el.json_bert-base-multilingual-cased_384_el
01/06/2022 17:15:02 - INFO - __main__ -   ***** Running evaluation  *****
01/06/2022 17:15:02 - INFO - __main__ -     Num examples = 2077
01/06/2022 17:15:02 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/260 [00:00<?, ?it/s]01/06/2022 17:15:02 - INFO - __main__ -   Batch Number = 1
01/06/2022 17:15:02 - INFO - __main__ -   Batch Number = 2
01/06/2022 17:15:02 - INFO - __main__ -   Batch Number = 3
Evaluating:   1%|          | 3/260 [00:00<00:08, 29.82it/s]01/06/2022 17:15:02 - INFO - __main__ -   Batch Number = 4
01/06/2022 17:15:02 - INFO - __main__ -   Batch Number = 5
01/06/2022 17:15:02 - INFO - __main__ -   Batch Number = 6
01/06/2022 17:15:02 - INFO - __main__ -   Batch Number = 7
Evaluating:   3%|▎         | 7/260 [00:00<00:08, 30.00it/s]01/06/2022 17:15:02 - INFO - __main__ -   Batch Number = 8
01/06/2022 17:15:02 - INFO - __main__ -   Batch Number = 9
01/06/2022 17:15:02 - INFO - __main__ -   Batch Number = 10
01/06/2022 17:15:02 - INFO - __main__ -   Batch Number = 11
01/06/2022 17:15:02 - INFO - __main__ -   Batch Number = 12
Evaluating:   5%|▍         | 12/260 [00:00<00:07, 35.42it/s]01/06/2022 17:15:02 - INFO - __main__ -   Batch Number = 13
01/06/2022 17:15:02 - INFO - __main__ -   Batch Number = 14
01/06/2022 17:15:02 - INFO - __main__ -   Batch Number = 15
01/06/2022 17:15:02 - INFO - __main__ -   Batch Number = 16
01/06/2022 17:15:02 - INFO - __main__ -   Batch Number = 17
Evaluating:   7%|▋         | 17/260 [00:00<00:06, 38.75it/s]01/06/2022 17:15:02 - INFO - __main__ -   Batch Number = 18
01/06/2022 17:15:02 - INFO - __main__ -   Batch Number = 19
01/06/2022 17:15:02 - INFO - __main__ -   Batch Number = 20
01/06/2022 17:15:02 - INFO - __main__ -   Batch Number = 21
01/06/2022 17:15:02 - INFO - __main__ -   Batch Number = 22
Evaluating:   8%|▊         | 22/260 [00:00<00:05, 40.92it/s]01/06/2022 17:15:02 - INFO - __main__ -   Batch Number = 23
01/06/2022 17:15:02 - INFO - __main__ -   Batch Number = 24
01/06/2022 17:15:02 - INFO - __main__ -   Batch Number = 25
01/06/2022 17:15:02 - INFO - __main__ -   Batch Number = 26
01/06/2022 17:15:02 - INFO - __main__ -   Batch Number = 27
Evaluating:  10%|█         | 27/260 [00:00<00:05, 42.09it/s]01/06/2022 17:15:02 - INFO - __main__ -   Batch Number = 28
01/06/2022 17:15:02 - INFO - __main__ -   Batch Number = 29
01/06/2022 17:15:02 - INFO - __main__ -   Batch Number = 30
01/06/2022 17:15:02 - INFO - __main__ -   Batch Number = 31
01/06/2022 17:15:02 - INFO - __main__ -   Batch Number = 32
Evaluating:  12%|█▏        | 32/260 [00:00<00:05, 42.88it/s]01/06/2022 17:15:02 - INFO - __main__ -   Batch Number = 33
01/06/2022 17:15:02 - INFO - __main__ -   Batch Number = 34
01/06/2022 17:15:02 - INFO - __main__ -   Batch Number = 35
01/06/2022 17:15:03 - INFO - __main__ -   Batch Number = 36
01/06/2022 17:15:03 - INFO - __main__ -   Batch Number = 37
Evaluating:  14%|█▍        | 37/260 [00:00<00:05, 43.45it/s]01/06/2022 17:15:03 - INFO - __main__ -   Batch Number = 38
01/06/2022 17:15:03 - INFO - __main__ -   Batch Number = 39
01/06/2022 17:15:03 - INFO - __main__ -   Batch Number = 40
01/06/2022 17:15:03 - INFO - __main__ -   Batch Number = 41
01/06/2022 17:15:03 - INFO - __main__ -   Batch Number = 42
Evaluating:  16%|█▌        | 42/260 [00:01<00:04, 43.89it/s]01/06/2022 17:15:03 - INFO - __main__ -   Batch Number = 43
01/06/2022 17:15:03 - INFO - __main__ -   Batch Number = 44
01/06/2022 17:15:03 - INFO - __main__ -   Batch Number = 45
01/06/2022 17:15:03 - INFO - __main__ -   Batch Number = 46
01/06/2022 17:15:03 - INFO - __main__ -   Batch Number = 47
Evaluating:  18%|█▊        | 47/260 [00:01<00:04, 44.05it/s]01/06/2022 17:15:03 - INFO - __main__ -   Batch Number = 48
01/06/2022 17:15:03 - INFO - __main__ -   Batch Number = 49
01/06/2022 17:15:03 - INFO - __main__ -   Batch Number = 50
01/06/2022 17:15:03 - INFO - __main__ -   Batch Number = 51
01/06/2022 17:15:03 - INFO - __main__ -   Batch Number = 52
Evaluating:  20%|██        | 52/260 [00:01<00:04, 44.33it/s]01/06/2022 17:15:03 - INFO - __main__ -   Batch Number = 53
01/06/2022 17:15:03 - INFO - __main__ -   Batch Number = 54
01/06/2022 17:15:03 - INFO - __main__ -   Batch Number = 55
01/06/2022 17:15:03 - INFO - __main__ -   Batch Number = 56
01/06/2022 17:15:03 - INFO - __main__ -   Batch Number = 57
Evaluating:  22%|██▏       | 57/260 [00:01<00:04, 44.44it/s]01/06/2022 17:15:03 - INFO - __main__ -   Batch Number = 58
01/06/2022 17:15:03 - INFO - __main__ -   Batch Number = 59
01/06/2022 17:15:03 - INFO - __main__ -   Batch Number = 60
01/06/2022 17:15:03 - INFO - __main__ -   Batch Number = 61
01/06/2022 17:15:03 - INFO - __main__ -   Batch Number = 62
Evaluating:  24%|██▍       | 62/260 [00:01<00:04, 44.52it/s]01/06/2022 17:15:03 - INFO - __main__ -   Batch Number = 63
01/06/2022 17:15:03 - INFO - __main__ -   Batch Number = 64
01/06/2022 17:15:03 - INFO - __main__ -   Batch Number = 65
01/06/2022 17:15:03 - INFO - __main__ -   Batch Number = 66
01/06/2022 17:15:03 - INFO - __main__ -   Batch Number = 67
Evaluating:  26%|██▌       | 67/260 [00:01<00:04, 44.64it/s]01/06/2022 17:15:03 - INFO - __main__ -   Batch Number = 68
01/06/2022 17:15:03 - INFO - __main__ -   Batch Number = 69
01/06/2022 17:15:03 - INFO - __main__ -   Batch Number = 70
01/06/2022 17:15:03 - INFO - __main__ -   Batch Number = 71
01/06/2022 17:15:03 - INFO - __main__ -   Batch Number = 72
Evaluating:  28%|██▊       | 72/260 [00:01<00:04, 44.74it/s]01/06/2022 17:15:03 - INFO - __main__ -   Batch Number = 73
01/06/2022 17:15:03 - INFO - __main__ -   Batch Number = 74
01/06/2022 17:15:03 - INFO - __main__ -   Batch Number = 75
01/06/2022 17:15:03 - INFO - __main__ -   Batch Number = 76
01/06/2022 17:15:03 - INFO - __main__ -   Batch Number = 77
Evaluating:  30%|██▉       | 77/260 [00:01<00:04, 44.62it/s]01/06/2022 17:15:03 - INFO - __main__ -   Batch Number = 78
01/06/2022 17:15:03 - INFO - __main__ -   Batch Number = 79
01/06/2022 17:15:03 - INFO - __main__ -   Batch Number = 80
01/06/2022 17:15:04 - INFO - __main__ -   Batch Number = 81
01/06/2022 17:15:04 - INFO - __main__ -   Batch Number = 82
Evaluating:  32%|███▏      | 82/260 [00:01<00:03, 44.67it/s]01/06/2022 17:15:04 - INFO - __main__ -   Batch Number = 83
01/06/2022 17:15:04 - INFO - __main__ -   Batch Number = 84
01/06/2022 17:15:04 - INFO - __main__ -   Batch Number = 85
01/06/2022 17:15:04 - INFO - __main__ -   Batch Number = 86
01/06/2022 17:15:04 - INFO - __main__ -   Batch Number = 87
Evaluating:  33%|███▎      | 87/260 [00:02<00:03, 44.69it/s]01/06/2022 17:15:04 - INFO - __main__ -   Batch Number = 88
01/06/2022 17:15:04 - INFO - __main__ -   Batch Number = 89
01/06/2022 17:15:04 - INFO - __main__ -   Batch Number = 90
01/06/2022 17:15:04 - INFO - __main__ -   Batch Number = 91
01/06/2022 17:15:04 - INFO - __main__ -   Batch Number = 92
Evaluating:  35%|███▌      | 92/260 [00:02<00:03, 44.58it/s]01/06/2022 17:15:04 - INFO - __main__ -   Batch Number = 93
01/06/2022 17:15:04 - INFO - __main__ -   Batch Number = 94
01/06/2022 17:15:04 - INFO - __main__ -   Batch Number = 95
01/06/2022 17:15:04 - INFO - __main__ -   Batch Number = 96
01/06/2022 17:15:04 - INFO - __main__ -   Batch Number = 97
Evaluating:  37%|███▋      | 97/260 [00:02<00:03, 44.63it/s]01/06/2022 17:15:04 - INFO - __main__ -   Batch Number = 98
01/06/2022 17:15:04 - INFO - __main__ -   Batch Number = 99
01/06/2022 17:15:04 - INFO - __main__ -   Batch Number = 100
01/06/2022 17:15:04 - INFO - __main__ -   Batch Number = 101
01/06/2022 17:15:04 - INFO - __main__ -   Batch Number = 102
Evaluating:  39%|███▉      | 102/260 [00:02<00:03, 44.68it/s]01/06/2022 17:15:04 - INFO - __main__ -   Batch Number = 103
01/06/2022 17:15:04 - INFO - __main__ -   Batch Number = 104
01/06/2022 17:15:04 - INFO - __main__ -   Batch Number = 105
01/06/2022 17:15:04 - INFO - __main__ -   Batch Number = 106
01/06/2022 17:15:04 - INFO - __main__ -   Batch Number = 107
Evaluating:  41%|████      | 107/260 [00:02<00:03, 44.64it/s]01/06/2022 17:15:04 - INFO - __main__ -   Batch Number = 108
01/06/2022 17:15:04 - INFO - __main__ -   Batch Number = 109
01/06/2022 17:15:04 - INFO - __main__ -   Batch Number = 110
01/06/2022 17:15:04 - INFO - __main__ -   Batch Number = 111
01/06/2022 17:15:04 - INFO - __main__ -   Batch Number = 112
Evaluating:  43%|████▎     | 112/260 [00:02<00:03, 44.61it/s]01/06/2022 17:15:04 - INFO - __main__ -   Batch Number = 113
01/06/2022 17:15:04 - INFO - __main__ -   Batch Number = 114
01/06/2022 17:15:04 - INFO - __main__ -   Batch Number = 115
01/06/2022 17:15:04 - INFO - __main__ -   Batch Number = 116
01/06/2022 17:15:04 - INFO - __main__ -   Batch Number = 117
Evaluating:  45%|████▌     | 117/260 [00:02<00:03, 44.66it/s]01/06/2022 17:15:04 - INFO - __main__ -   Batch Number = 118
01/06/2022 17:15:04 - INFO - __main__ -   Batch Number = 119
01/06/2022 17:15:04 - INFO - __main__ -   Batch Number = 120
01/06/2022 17:15:04 - INFO - __main__ -   Batch Number = 121
01/06/2022 17:15:04 - INFO - __main__ -   Batch Number = 122
Evaluating:  47%|████▋     | 122/260 [00:02<00:03, 42.93it/s]01/06/2022 17:15:04 - INFO - __main__ -   Batch Number = 123
01/06/2022 17:15:04 - INFO - __main__ -   Batch Number = 124
01/06/2022 17:15:05 - INFO - __main__ -   Batch Number = 125
01/06/2022 17:15:05 - INFO - __main__ -   Batch Number = 126
01/06/2022 17:15:05 - INFO - __main__ -   Batch Number = 127
Evaluating:  49%|████▉     | 127/260 [00:02<00:03, 43.50it/s]01/06/2022 17:15:05 - INFO - __main__ -   Batch Number = 128
01/06/2022 17:15:05 - INFO - __main__ -   Batch Number = 129
01/06/2022 17:15:05 - INFO - __main__ -   Batch Number = 130
01/06/2022 17:15:05 - INFO - __main__ -   Batch Number = 131
01/06/2022 17:15:05 - INFO - __main__ -   Batch Number = 132
Evaluating:  51%|█████     | 132/260 [00:03<00:02, 43.75it/s]01/06/2022 17:15:05 - INFO - __main__ -   Batch Number = 133
01/06/2022 17:15:05 - INFO - __main__ -   Batch Number = 134
01/06/2022 17:15:05 - INFO - __main__ -   Batch Number = 135
01/06/2022 17:15:05 - INFO - __main__ -   Batch Number = 136
01/06/2022 17:15:05 - INFO - __main__ -   Batch Number = 137
Evaluating:  53%|█████▎    | 137/260 [00:03<00:02, 43.81it/s]01/06/2022 17:15:05 - INFO - __main__ -   Batch Number = 138
01/06/2022 17:15:05 - INFO - __main__ -   Batch Number = 139
01/06/2022 17:15:05 - INFO - __main__ -   Batch Number = 140
01/06/2022 17:15:05 - INFO - __main__ -   Batch Number = 141
01/06/2022 17:15:05 - INFO - __main__ -   Batch Number = 142
Evaluating:  55%|█████▍    | 142/260 [00:03<00:02, 43.97it/s]01/06/2022 17:15:05 - INFO - __main__ -   Batch Number = 143
01/06/2022 17:15:05 - INFO - __main__ -   Batch Number = 144
01/06/2022 17:15:05 - INFO - __main__ -   Batch Number = 145
01/06/2022 17:15:05 - INFO - __main__ -   Batch Number = 146
01/06/2022 17:15:05 - INFO - __main__ -   Batch Number = 147
Evaluating:  57%|█████▋    | 147/260 [00:03<00:02, 44.28it/s]01/06/2022 17:15:05 - INFO - __main__ -   Batch Number = 148
01/06/2022 17:15:05 - INFO - __main__ -   Batch Number = 149
01/06/2022 17:15:05 - INFO - __main__ -   Batch Number = 150
01/06/2022 17:15:05 - INFO - __main__ -   Batch Number = 151
01/06/2022 17:15:05 - INFO - __main__ -   Batch Number = 152
Evaluating:  58%|█████▊    | 152/260 [00:03<00:02, 44.24it/s]01/06/2022 17:15:05 - INFO - __main__ -   Batch Number = 153
01/06/2022 17:15:05 - INFO - __main__ -   Batch Number = 154
01/06/2022 17:15:05 - INFO - __main__ -   Batch Number = 155
01/06/2022 17:15:05 - INFO - __main__ -   Batch Number = 156
01/06/2022 17:15:05 - INFO - __main__ -   Batch Number = 157
Evaluating:  60%|██████    | 157/260 [00:03<00:02, 44.45it/s]01/06/2022 17:15:05 - INFO - __main__ -   Batch Number = 158
01/06/2022 17:15:05 - INFO - __main__ -   Batch Number = 159
01/06/2022 17:15:05 - INFO - __main__ -   Batch Number = 160
01/06/2022 17:15:05 - INFO - __main__ -   Batch Number = 161
01/06/2022 17:15:05 - INFO - __main__ -   Batch Number = 162
Evaluating:  62%|██████▏   | 162/260 [00:03<00:02, 44.49it/s]01/06/2022 17:15:05 - INFO - __main__ -   Batch Number = 163
01/06/2022 17:15:05 - INFO - __main__ -   Batch Number = 164
01/06/2022 17:15:05 - INFO - __main__ -   Batch Number = 165
01/06/2022 17:15:05 - INFO - __main__ -   Batch Number = 166
01/06/2022 17:15:05 - INFO - __main__ -   Batch Number = 167
Evaluating:  64%|██████▍   | 167/260 [00:03<00:02, 44.43it/s]01/06/2022 17:15:05 - INFO - __main__ -   Batch Number = 168
01/06/2022 17:15:06 - INFO - __main__ -   Batch Number = 169
01/06/2022 17:15:06 - INFO - __main__ -   Batch Number = 170
01/06/2022 17:15:06 - INFO - __main__ -   Batch Number = 171
01/06/2022 17:15:06 - INFO - __main__ -   Batch Number = 172
Evaluating:  66%|██████▌   | 172/260 [00:03<00:01, 44.48it/s]01/06/2022 17:15:06 - INFO - __main__ -   Batch Number = 173
01/06/2022 17:15:06 - INFO - __main__ -   Batch Number = 174
01/06/2022 17:15:06 - INFO - __main__ -   Batch Number = 175
01/06/2022 17:15:06 - INFO - __main__ -   Batch Number = 176
01/06/2022 17:15:06 - INFO - __main__ -   Batch Number = 177
Evaluating:  68%|██████▊   | 177/260 [00:04<00:01, 44.59it/s]01/06/2022 17:15:06 - INFO - __main__ -   Batch Number = 178
01/06/2022 17:15:06 - INFO - __main__ -   Batch Number = 179
01/06/2022 17:15:06 - INFO - __main__ -   Batch Number = 180
01/06/2022 17:15:06 - INFO - __main__ -   Batch Number = 181
01/06/2022 17:15:06 - INFO - __main__ -   Batch Number = 182
Evaluating:  70%|███████   | 182/260 [00:04<00:01, 44.53it/s]01/06/2022 17:15:06 - INFO - __main__ -   Batch Number = 183
01/06/2022 17:15:06 - INFO - __main__ -   Batch Number = 184
01/06/2022 17:15:06 - INFO - __main__ -   Batch Number = 185
01/06/2022 17:15:06 - INFO - __main__ -   Batch Number = 186
01/06/2022 17:15:06 - INFO - __main__ -   Batch Number = 187
Evaluating:  72%|███████▏  | 187/260 [00:04<00:01, 44.55it/s]01/06/2022 17:15:06 - INFO - __main__ -   Batch Number = 188
01/06/2022 17:15:06 - INFO - __main__ -   Batch Number = 189
01/06/2022 17:15:06 - INFO - __main__ -   Batch Number = 190
01/06/2022 17:15:06 - INFO - __main__ -   Batch Number = 191
01/06/2022 17:15:06 - INFO - __main__ -   Batch Number = 192
Evaluating:  74%|███████▍  | 192/260 [00:04<00:01, 44.62it/s]01/06/2022 17:15:06 - INFO - __main__ -   Batch Number = 193
01/06/2022 17:15:06 - INFO - __main__ -   Batch Number = 194
01/06/2022 17:15:06 - INFO - __main__ -   Batch Number = 195
01/06/2022 17:15:06 - INFO - __main__ -   Batch Number = 196
01/06/2022 17:15:06 - INFO - __main__ -   Batch Number = 197
Evaluating:  76%|███████▌  | 197/260 [00:04<00:01, 44.51it/s]01/06/2022 17:15:06 - INFO - __main__ -   Batch Number = 198
01/06/2022 17:15:06 - INFO - __main__ -   Batch Number = 199
01/06/2022 17:15:06 - INFO - __main__ -   Batch Number = 200
01/06/2022 17:15:06 - INFO - __main__ -   Batch Number = 201
01/06/2022 17:15:06 - INFO - __main__ -   Batch Number = 202
Evaluating:  78%|███████▊  | 202/260 [00:04<00:01, 44.52it/s]01/06/2022 17:15:06 - INFO - __main__ -   Batch Number = 203
01/06/2022 17:15:06 - INFO - __main__ -   Batch Number = 204
01/06/2022 17:15:06 - INFO - __main__ -   Batch Number = 205
01/06/2022 17:15:06 - INFO - __main__ -   Batch Number = 206
01/06/2022 17:15:06 - INFO - __main__ -   Batch Number = 207
Evaluating:  80%|███████▉  | 207/260 [00:04<00:01, 44.58it/s]01/06/2022 17:15:06 - INFO - __main__ -   Batch Number = 208
01/06/2022 17:15:06 - INFO - __main__ -   Batch Number = 209
01/06/2022 17:15:06 - INFO - __main__ -   Batch Number = 210
01/06/2022 17:15:06 - INFO - __main__ -   Batch Number = 211
01/06/2022 17:15:06 - INFO - __main__ -   Batch Number = 212
Evaluating:  82%|████████▏ | 212/260 [00:04<00:01, 44.54it/s]01/06/2022 17:15:06 - INFO - __main__ -   Batch Number = 213
01/06/2022 17:15:07 - INFO - __main__ -   Batch Number = 214
01/06/2022 17:15:07 - INFO - __main__ -   Batch Number = 215
01/06/2022 17:15:07 - INFO - __main__ -   Batch Number = 216
01/06/2022 17:15:07 - INFO - __main__ -   Batch Number = 217
Evaluating:  83%|████████▎ | 217/260 [00:04<00:00, 44.40it/s]01/06/2022 17:15:07 - INFO - __main__ -   Batch Number = 218
01/06/2022 17:15:07 - INFO - __main__ -   Batch Number = 219
01/06/2022 17:15:07 - INFO - __main__ -   Batch Number = 220
01/06/2022 17:15:07 - INFO - __main__ -   Batch Number = 221
01/06/2022 17:15:07 - INFO - __main__ -   Batch Number = 222
Evaluating:  85%|████████▌ | 222/260 [00:05<00:00, 44.49it/s]01/06/2022 17:15:07 - INFO - __main__ -   Batch Number = 223
01/06/2022 17:15:07 - INFO - __main__ -   Batch Number = 224
01/06/2022 17:15:07 - INFO - __main__ -   Batch Number = 225
01/06/2022 17:15:07 - INFO - __main__ -   Batch Number = 226
01/06/2022 17:15:07 - INFO - __main__ -   Batch Number = 227
Evaluating:  87%|████████▋ | 227/260 [00:05<00:00, 44.39it/s]01/06/2022 17:15:07 - INFO - __main__ -   Batch Number = 228
01/06/2022 17:15:07 - INFO - __main__ -   Batch Number = 229
01/06/2022 17:15:07 - INFO - __main__ -   Batch Number = 230
01/06/2022 17:15:07 - INFO - __main__ -   Batch Number = 231
01/06/2022 17:15:07 - INFO - __main__ -   Batch Number = 232
Evaluating:  89%|████████▉ | 232/260 [00:05<00:00, 44.41it/s]01/06/2022 17:15:07 - INFO - __main__ -   Batch Number = 233
01/06/2022 17:15:07 - INFO - __main__ -   Batch Number = 234
01/06/2022 17:15:07 - INFO - __main__ -   Batch Number = 235
01/06/2022 17:15:07 - INFO - __main__ -   Batch Number = 236
01/06/2022 17:15:07 - INFO - __main__ -   Batch Number = 237
Evaluating:  91%|█████████ | 237/260 [00:05<00:00, 44.42it/s]01/06/2022 17:15:07 - INFO - __main__ -   Batch Number = 238
01/06/2022 17:15:07 - INFO - __main__ -   Batch Number = 239
01/06/2022 17:15:07 - INFO - __main__ -   Batch Number = 240
01/06/2022 17:15:07 - INFO - __main__ -   Batch Number = 241
01/06/2022 17:15:07 - INFO - __main__ -   Batch Number = 242
Evaluating:  93%|█████████▎| 242/260 [00:05<00:00, 44.46it/s]01/06/2022 17:15:07 - INFO - __main__ -   Batch Number = 243
01/06/2022 17:15:07 - INFO - __main__ -   Batch Number = 244
01/06/2022 17:15:07 - INFO - __main__ -   Batch Number = 245
01/06/2022 17:15:07 - INFO - __main__ -   Batch Number = 246
01/06/2022 17:15:07 - INFO - __main__ -   Batch Number = 247
Evaluating:  95%|█████████▌| 247/260 [00:05<00:00, 44.33it/s]01/06/2022 17:15:07 - INFO - __main__ -   Batch Number = 248
01/06/2022 17:15:07 - INFO - __main__ -   Batch Number = 249
01/06/2022 17:15:07 - INFO - __main__ -   Batch Number = 250
01/06/2022 17:15:07 - INFO - __main__ -   Batch Number = 251
01/06/2022 17:15:07 - INFO - __main__ -   Batch Number = 252
Evaluating:  97%|█████████▋| 252/260 [00:05<00:00, 44.47it/s]01/06/2022 17:15:07 - INFO - __main__ -   Batch Number = 253
01/06/2022 17:15:07 - INFO - __main__ -   Batch Number = 254
01/06/2022 17:15:07 - INFO - __main__ -   Batch Number = 255
01/06/2022 17:15:07 - INFO - __main__ -   Batch Number = 256
01/06/2022 17:15:07 - INFO - __main__ -   Batch Number = 257
Evaluating:  99%|█████████▉| 257/260 [00:05<00:00, 44.55it/s]01/06/2022 17:15:08 - INFO - __main__ -   Batch Number = 258
01/06/2022 17:15:08 - INFO - __main__ -   Batch Number = 259
01/06/2022 17:15:08 - INFO - __main__ -   Batch Number = 260
Evaluating: 100%|██████████| 260/260 [00:05<00:00, 43.88it/s]
01/06/2022 17:15:08 - INFO - __main__ -     Evaluation done in total 5.925841 secs (0.002853 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/predictions_el_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/nbest_predictions_el_.json
01/06/2022 17:15:11 - INFO - __main__ -   Results = OrderedDict([('exact', 32.6890756302521), ('f1', 45.16151993379042), ('total', 1190), ('HasAns_exact', 32.6890756302521), ('HasAns_f1', 45.16151993379042), ('HasAns_total', 1190), ('best_exact', 32.6890756302521), ('best_exact_thresh', 0.0), ('best_f1', 45.16151993379042), ('best_f1_thresh', 0.0)])
01/06/2022 17:15:11 - INFO - __main__ -   Language adapter for es not found, using en instead
01/06/2022 17:15:11 - INFO - __main__ -   Set active language adapter to en
01/06/2022 17:15:11 - INFO - __main__ -   Args Adapter Weight = None
01/06/2022 17:15:11 - INFO - __main__ -   Adapter Languages = ['en']
01/06/2022 17:15:11 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
  0%|          | 0/48 [00:00<?, ?it/s] 33%|███▎      | 16/48 [00:00<00:00, 124.32it/s] 60%|██████    | 29/48 [00:00<00:00, 117.82it/s] 90%|████████▉ | 43/48 [00:00<00:00, 125.73it/s]100%|██████████| 48/48 [00:00<00:00, 126.52it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<06:06,  3.24it/s]convert squad examples to features:  22%|██▏       | 257/1190 [00:00<00:01, 655.52it/s]convert squad examples to features:  32%|███▏      | 385/1190 [00:01<00:02, 370.10it/s]convert squad examples to features:  83%|████████▎ | 993/1190 [00:01<00:00, 1108.21it/s]convert squad examples to features:  99%|█████████▉| 1177/1190 [00:01<00:00, 1193.74it/s]convert squad examples to features: 100%|██████████| 1190/1190 [00:01<00:00, 901.69it/s] /root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|██████████| 1190/1190 [00:00<00:00, 713031.68it/s]
01/06/2022 17:15:13 - INFO - __main__ -   Local Rank = -1
01/06/2022 17:15:13 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.es.json_bert-base-multilingual-cased_384_es
01/06/2022 17:15:14 - INFO - __main__ -   ***** Running evaluation  *****
01/06/2022 17:15:14 - INFO - __main__ -     Num examples = 1307
01/06/2022 17:15:14 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/164 [00:00<?, ?it/s]01/06/2022 17:15:14 - INFO - __main__ -   Batch Number = 1
01/06/2022 17:15:14 - INFO - __main__ -   Batch Number = 2
01/06/2022 17:15:14 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|▏         | 3/164 [00:00<00:05, 29.70it/s]01/06/2022 17:15:14 - INFO - __main__ -   Batch Number = 4
01/06/2022 17:15:14 - INFO - __main__ -   Batch Number = 5
01/06/2022 17:15:14 - INFO - __main__ -   Batch Number = 6
01/06/2022 17:15:14 - INFO - __main__ -   Batch Number = 7
Evaluating:   4%|▍         | 7/164 [00:00<00:04, 32.24it/s]01/06/2022 17:15:15 - INFO - __main__ -   Batch Number = 8
01/06/2022 17:15:15 - INFO - __main__ -   Batch Number = 9
01/06/2022 17:15:15 - INFO - __main__ -   Batch Number = 10
01/06/2022 17:15:15 - INFO - __main__ -   Batch Number = 11
Evaluating:   7%|▋         | 11/164 [00:00<00:04, 34.03it/s]01/06/2022 17:15:15 - INFO - __main__ -   Batch Number = 12
01/06/2022 17:15:15 - INFO - __main__ -   Batch Number = 13
01/06/2022 17:15:15 - INFO - __main__ -   Batch Number = 14
01/06/2022 17:15:15 - INFO - __main__ -   Batch Number = 15
01/06/2022 17:15:15 - INFO - __main__ -   Batch Number = 16
Evaluating:  10%|▉         | 16/164 [00:00<00:03, 38.27it/s]01/06/2022 17:15:15 - INFO - __main__ -   Batch Number = 17
01/06/2022 17:15:15 - INFO - __main__ -   Batch Number = 18
01/06/2022 17:15:15 - INFO - __main__ -   Batch Number = 19
01/06/2022 17:15:15 - INFO - __main__ -   Batch Number = 20
01/06/2022 17:15:15 - INFO - __main__ -   Batch Number = 21
Evaluating:  13%|█▎        | 21/164 [00:00<00:03, 40.56it/s]01/06/2022 17:15:15 - INFO - __main__ -   Batch Number = 22
01/06/2022 17:15:15 - INFO - __main__ -   Batch Number = 23
01/06/2022 17:15:15 - INFO - __main__ -   Batch Number = 24
01/06/2022 17:15:15 - INFO - __main__ -   Batch Number = 25
01/06/2022 17:15:15 - INFO - __main__ -   Batch Number = 26
Evaluating:  16%|█▌        | 26/164 [00:00<00:03, 42.00it/s]01/06/2022 17:15:15 - INFO - __main__ -   Batch Number = 27
01/06/2022 17:15:15 - INFO - __main__ -   Batch Number = 28
01/06/2022 17:15:15 - INFO - __main__ -   Batch Number = 29
01/06/2022 17:15:15 - INFO - __main__ -   Batch Number = 30
01/06/2022 17:15:15 - INFO - __main__ -   Batch Number = 31
Evaluating:  19%|█▉        | 31/164 [00:00<00:03, 43.04it/s]01/06/2022 17:15:15 - INFO - __main__ -   Batch Number = 32
01/06/2022 17:15:15 - INFO - __main__ -   Batch Number = 33
01/06/2022 17:15:15 - INFO - __main__ -   Batch Number = 34
01/06/2022 17:15:15 - INFO - __main__ -   Batch Number = 35
01/06/2022 17:15:15 - INFO - __main__ -   Batch Number = 36
Evaluating:  22%|██▏       | 36/164 [00:00<00:02, 43.53it/s]01/06/2022 17:15:15 - INFO - __main__ -   Batch Number = 37
01/06/2022 17:15:15 - INFO - __main__ -   Batch Number = 38
01/06/2022 17:15:15 - INFO - __main__ -   Batch Number = 39
01/06/2022 17:15:15 - INFO - __main__ -   Batch Number = 40
01/06/2022 17:15:15 - INFO - __main__ -   Batch Number = 41
Evaluating:  25%|██▌       | 41/164 [00:01<00:02, 43.91it/s]01/06/2022 17:15:15 - INFO - __main__ -   Batch Number = 42
01/06/2022 17:15:15 - INFO - __main__ -   Batch Number = 43
01/06/2022 17:15:15 - INFO - __main__ -   Batch Number = 44
01/06/2022 17:15:15 - INFO - __main__ -   Batch Number = 45
01/06/2022 17:15:15 - INFO - __main__ -   Batch Number = 46
Evaluating:  28%|██▊       | 46/164 [00:01<00:02, 43.94it/s]01/06/2022 17:15:15 - INFO - __main__ -   Batch Number = 47
01/06/2022 17:15:15 - INFO - __main__ -   Batch Number = 48
01/06/2022 17:15:15 - INFO - __main__ -   Batch Number = 49
01/06/2022 17:15:15 - INFO - __main__ -   Batch Number = 50
01/06/2022 17:15:16 - INFO - __main__ -   Batch Number = 51
Evaluating:  31%|███       | 51/164 [00:01<00:02, 44.12it/s]01/06/2022 17:15:16 - INFO - __main__ -   Batch Number = 52
01/06/2022 17:15:16 - INFO - __main__ -   Batch Number = 53
01/06/2022 17:15:16 - INFO - __main__ -   Batch Number = 54
01/06/2022 17:15:16 - INFO - __main__ -   Batch Number = 55
01/06/2022 17:15:16 - INFO - __main__ -   Batch Number = 56
Evaluating:  34%|███▍      | 56/164 [00:01<00:02, 44.28it/s]01/06/2022 17:15:16 - INFO - __main__ -   Batch Number = 57
01/06/2022 17:15:16 - INFO - __main__ -   Batch Number = 58
01/06/2022 17:15:16 - INFO - __main__ -   Batch Number = 59
01/06/2022 17:15:16 - INFO - __main__ -   Batch Number = 60
01/06/2022 17:15:16 - INFO - __main__ -   Batch Number = 61
Evaluating:  37%|███▋      | 61/164 [00:01<00:02, 44.50it/s]01/06/2022 17:15:16 - INFO - __main__ -   Batch Number = 62
01/06/2022 17:15:16 - INFO - __main__ -   Batch Number = 63
01/06/2022 17:15:16 - INFO - __main__ -   Batch Number = 64
01/06/2022 17:15:16 - INFO - __main__ -   Batch Number = 65
01/06/2022 17:15:16 - INFO - __main__ -   Batch Number = 66
Evaluating:  40%|████      | 66/164 [00:01<00:02, 43.11it/s]01/06/2022 17:15:16 - INFO - __main__ -   Batch Number = 67
01/06/2022 17:15:16 - INFO - __main__ -   Batch Number = 68
01/06/2022 17:15:16 - INFO - __main__ -   Batch Number = 69
01/06/2022 17:15:16 - INFO - __main__ -   Batch Number = 70
01/06/2022 17:15:16 - INFO - __main__ -   Batch Number = 71
Evaluating:  43%|████▎     | 71/164 [00:01<00:02, 43.56it/s]01/06/2022 17:15:16 - INFO - __main__ -   Batch Number = 72
01/06/2022 17:15:16 - INFO - __main__ -   Batch Number = 73
01/06/2022 17:15:16 - INFO - __main__ -   Batch Number = 74
01/06/2022 17:15:16 - INFO - __main__ -   Batch Number = 75
01/06/2022 17:15:16 - INFO - __main__ -   Batch Number = 76
Evaluating:  46%|████▋     | 76/164 [00:01<00:01, 44.02it/s]01/06/2022 17:15:16 - INFO - __main__ -   Batch Number = 77
01/06/2022 17:15:16 - INFO - __main__ -   Batch Number = 78
01/06/2022 17:15:16 - INFO - __main__ -   Batch Number = 79
01/06/2022 17:15:16 - INFO - __main__ -   Batch Number = 80
01/06/2022 17:15:16 - INFO - __main__ -   Batch Number = 81
Evaluating:  49%|████▉     | 81/164 [00:01<00:01, 44.08it/s]01/06/2022 17:15:16 - INFO - __main__ -   Batch Number = 82
01/06/2022 17:15:16 - INFO - __main__ -   Batch Number = 83
01/06/2022 17:15:16 - INFO - __main__ -   Batch Number = 84
01/06/2022 17:15:16 - INFO - __main__ -   Batch Number = 85
01/06/2022 17:15:16 - INFO - __main__ -   Batch Number = 86
Evaluating:  52%|█████▏    | 86/164 [00:02<00:01, 44.37it/s]01/06/2022 17:15:16 - INFO - __main__ -   Batch Number = 87
01/06/2022 17:15:16 - INFO - __main__ -   Batch Number = 88
01/06/2022 17:15:16 - INFO - __main__ -   Batch Number = 89
01/06/2022 17:15:16 - INFO - __main__ -   Batch Number = 90
01/06/2022 17:15:16 - INFO - __main__ -   Batch Number = 91
Evaluating:  55%|█████▌    | 91/164 [00:02<00:01, 44.49it/s]01/06/2022 17:15:16 - INFO - __main__ -   Batch Number = 92
01/06/2022 17:15:16 - INFO - __main__ -   Batch Number = 93
01/06/2022 17:15:16 - INFO - __main__ -   Batch Number = 94
01/06/2022 17:15:17 - INFO - __main__ -   Batch Number = 95
01/06/2022 17:15:17 - INFO - __main__ -   Batch Number = 96
Evaluating:  59%|█████▊    | 96/164 [00:02<00:01, 44.42it/s]01/06/2022 17:15:17 - INFO - __main__ -   Batch Number = 97
01/06/2022 17:15:17 - INFO - __main__ -   Batch Number = 98
01/06/2022 17:15:17 - INFO - __main__ -   Batch Number = 99
01/06/2022 17:15:17 - INFO - __main__ -   Batch Number = 100
01/06/2022 17:15:17 - INFO - __main__ -   Batch Number = 101
Evaluating:  62%|██████▏   | 101/164 [00:02<00:01, 44.43it/s]01/06/2022 17:15:17 - INFO - __main__ -   Batch Number = 102
01/06/2022 17:15:17 - INFO - __main__ -   Batch Number = 103
01/06/2022 17:15:17 - INFO - __main__ -   Batch Number = 104
01/06/2022 17:15:17 - INFO - __main__ -   Batch Number = 105
01/06/2022 17:15:17 - INFO - __main__ -   Batch Number = 106
Evaluating:  65%|██████▍   | 106/164 [00:02<00:01, 44.61it/s]01/06/2022 17:15:17 - INFO - __main__ -   Batch Number = 107
01/06/2022 17:15:17 - INFO - __main__ -   Batch Number = 108
01/06/2022 17:15:17 - INFO - __main__ -   Batch Number = 109
01/06/2022 17:15:17 - INFO - __main__ -   Batch Number = 110
01/06/2022 17:15:17 - INFO - __main__ -   Batch Number = 111
Evaluating:  68%|██████▊   | 111/164 [00:02<00:01, 44.51it/s]01/06/2022 17:15:17 - INFO - __main__ -   Batch Number = 112
01/06/2022 17:15:17 - INFO - __main__ -   Batch Number = 113
01/06/2022 17:15:17 - INFO - __main__ -   Batch Number = 114
01/06/2022 17:15:17 - INFO - __main__ -   Batch Number = 115
01/06/2022 17:15:17 - INFO - __main__ -   Batch Number = 116
Evaluating:  71%|███████   | 116/164 [00:02<00:01, 44.67it/s]01/06/2022 17:15:17 - INFO - __main__ -   Batch Number = 117
01/06/2022 17:15:17 - INFO - __main__ -   Batch Number = 118
01/06/2022 17:15:17 - INFO - __main__ -   Batch Number = 119
01/06/2022 17:15:17 - INFO - __main__ -   Batch Number = 120
01/06/2022 17:15:17 - INFO - __main__ -   Batch Number = 121
Evaluating:  74%|███████▍  | 121/164 [00:02<00:00, 44.62it/s]01/06/2022 17:15:17 - INFO - __main__ -   Batch Number = 122
01/06/2022 17:15:17 - INFO - __main__ -   Batch Number = 123
01/06/2022 17:15:17 - INFO - __main__ -   Batch Number = 124
01/06/2022 17:15:17 - INFO - __main__ -   Batch Number = 125
01/06/2022 17:15:17 - INFO - __main__ -   Batch Number = 126
Evaluating:  77%|███████▋  | 126/164 [00:02<00:00, 44.60it/s]01/06/2022 17:15:17 - INFO - __main__ -   Batch Number = 127
01/06/2022 17:15:17 - INFO - __main__ -   Batch Number = 128
01/06/2022 17:15:17 - INFO - __main__ -   Batch Number = 129
01/06/2022 17:15:17 - INFO - __main__ -   Batch Number = 130
01/06/2022 17:15:17 - INFO - __main__ -   Batch Number = 131
Evaluating:  80%|███████▉  | 131/164 [00:03<00:00, 44.34it/s]01/06/2022 17:15:17 - INFO - __main__ -   Batch Number = 132
01/06/2022 17:15:17 - INFO - __main__ -   Batch Number = 133
01/06/2022 17:15:17 - INFO - __main__ -   Batch Number = 134
01/06/2022 17:15:17 - INFO - __main__ -   Batch Number = 135
01/06/2022 17:15:17 - INFO - __main__ -   Batch Number = 136
Evaluating:  83%|████████▎ | 136/164 [00:03<00:00, 44.11it/s]01/06/2022 17:15:17 - INFO - __main__ -   Batch Number = 137
01/06/2022 17:15:17 - INFO - __main__ -   Batch Number = 138
01/06/2022 17:15:17 - INFO - __main__ -   Batch Number = 139
01/06/2022 17:15:18 - INFO - __main__ -   Batch Number = 140
01/06/2022 17:15:18 - INFO - __main__ -   Batch Number = 141
Evaluating:  86%|████████▌ | 141/164 [00:03<00:00, 44.26it/s]01/06/2022 17:15:18 - INFO - __main__ -   Batch Number = 142
01/06/2022 17:15:18 - INFO - __main__ -   Batch Number = 143
01/06/2022 17:15:18 - INFO - __main__ -   Batch Number = 144
01/06/2022 17:15:18 - INFO - __main__ -   Batch Number = 145
01/06/2022 17:15:18 - INFO - __main__ -   Batch Number = 146
Evaluating:  89%|████████▉ | 146/164 [00:03<00:00, 44.30it/s]01/06/2022 17:15:18 - INFO - __main__ -   Batch Number = 147
01/06/2022 17:15:18 - INFO - __main__ -   Batch Number = 148
01/06/2022 17:15:18 - INFO - __main__ -   Batch Number = 149
01/06/2022 17:15:18 - INFO - __main__ -   Batch Number = 150
01/06/2022 17:15:18 - INFO - __main__ -   Batch Number = 151
Evaluating:  92%|█████████▏| 151/164 [00:03<00:00, 44.57it/s]01/06/2022 17:15:18 - INFO - __main__ -   Batch Number = 152
01/06/2022 17:15:18 - INFO - __main__ -   Batch Number = 153
01/06/2022 17:15:18 - INFO - __main__ -   Batch Number = 154
01/06/2022 17:15:18 - INFO - __main__ -   Batch Number = 155
01/06/2022 17:15:18 - INFO - __main__ -   Batch Number = 156
Evaluating:  95%|█████████▌| 156/164 [00:03<00:00, 44.44it/s]01/06/2022 17:15:18 - INFO - __main__ -   Batch Number = 157
01/06/2022 17:15:18 - INFO - __main__ -   Batch Number = 158
01/06/2022 17:15:18 - INFO - __main__ -   Batch Number = 159
01/06/2022 17:15:18 - INFO - __main__ -   Batch Number = 160
01/06/2022 17:15:18 - INFO - __main__ -   Batch Number = 161
Evaluating:  98%|█████████▊| 161/164 [00:03<00:00, 44.62it/s]01/06/2022 17:15:18 - INFO - __main__ -   Batch Number = 162
01/06/2022 17:15:18 - INFO - __main__ -   Batch Number = 163
01/06/2022 17:15:18 - INFO - __main__ -   Batch Number = 164
Evaluating: 100%|██████████| 164/164 [00:03<00:00, 43.54it/s]
01/06/2022 17:15:18 - INFO - __main__ -     Evaluation done in total 3.767249 secs (0.002882 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/predictions_es_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/nbest_predictions_es_.json
01/06/2022 17:15:22 - INFO - __main__ -   Results = OrderedDict([('exact', 52.857142857142854), ('f1', 71.44926151857138), ('total', 1190), ('HasAns_exact', 52.857142857142854), ('HasAns_f1', 71.44926151857138), ('HasAns_total', 1190), ('best_exact', 52.857142857142854), ('best_exact_thresh', 0.0), ('best_f1', 71.44926151857138), ('best_f1_thresh', 0.0)])
01/06/2022 17:15:22 - INFO - __main__ -   Language adapter for hi not found, using en instead
01/06/2022 17:15:22 - INFO - __main__ -   Set active language adapter to en
01/06/2022 17:15:22 - INFO - __main__ -   Args Adapter Weight = None
01/06/2022 17:15:22 - INFO - __main__ -   Adapter Languages = ['en']
01/06/2022 17:15:22 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
  0%|          | 0/48 [00:00<?, ?it/s] 33%|███▎      | 16/48 [00:00<00:00, 133.25it/s] 62%|██████▎   | 30/48 [00:00<00:00, 127.14it/s] 94%|█████████▍| 45/48 [00:00<00:00, 136.54it/s]100%|██████████| 48/48 [00:00<00:00, 136.31it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<08:07,  2.44it/s]convert squad examples to features:  27%|██▋       | 321/1190 [00:00<00:01, 766.91it/s]convert squad examples to features:  39%|███▉      | 465/1190 [00:01<00:01, 395.11it/s]convert squad examples to features:  75%|███████▌  | 897/1190 [00:01<00:00, 856.32it/s]convert squad examples to features:  94%|█████████▍| 1121/1190 [00:01<00:00, 994.05it/s]convert squad examples to features: 100%|██████████| 1190/1190 [00:01<00:00, 722.80it/s]/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|██████████| 1190/1190 [00:00<00:00, 404770.23it/s]
01/06/2022 17:15:24 - INFO - __main__ -   Local Rank = -1
01/06/2022 17:15:24 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.hi.json_bert-base-multilingual-cased_384_hi
01/06/2022 17:15:26 - INFO - __main__ -   ***** Running evaluation  *****
01/06/2022 17:15:26 - INFO - __main__ -     Num examples = 1629
01/06/2022 17:15:26 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/204 [00:00<?, ?it/s]01/06/2022 17:15:26 - INFO - __main__ -   Batch Number = 1
01/06/2022 17:15:26 - INFO - __main__ -   Batch Number = 2
01/06/2022 17:15:26 - INFO - __main__ -   Batch Number = 3
Evaluating:   1%|▏         | 3/204 [00:00<00:06, 29.73it/s]01/06/2022 17:15:26 - INFO - __main__ -   Batch Number = 4
01/06/2022 17:15:26 - INFO - __main__ -   Batch Number = 5
01/06/2022 17:15:26 - INFO - __main__ -   Batch Number = 6
Evaluating:   3%|▎         | 6/204 [00:00<00:06, 29.75it/s]01/06/2022 17:15:26 - INFO - __main__ -   Batch Number = 7
01/06/2022 17:15:26 - INFO - __main__ -   Batch Number = 8
01/06/2022 17:15:26 - INFO - __main__ -   Batch Number = 9
01/06/2022 17:15:26 - INFO - __main__ -   Batch Number = 10
Evaluating:   5%|▍         | 10/204 [00:00<00:06, 32.26it/s]01/06/2022 17:15:26 - INFO - __main__ -   Batch Number = 11
01/06/2022 17:15:26 - INFO - __main__ -   Batch Number = 12
01/06/2022 17:15:26 - INFO - __main__ -   Batch Number = 13
01/06/2022 17:15:26 - INFO - __main__ -   Batch Number = 14
01/06/2022 17:15:26 - INFO - __main__ -   Batch Number = 15
Evaluating:   7%|▋         | 15/204 [00:00<00:05, 37.34it/s]01/06/2022 17:15:26 - INFO - __main__ -   Batch Number = 16
01/06/2022 17:15:26 - INFO - __main__ -   Batch Number = 17
01/06/2022 17:15:26 - INFO - __main__ -   Batch Number = 18
01/06/2022 17:15:26 - INFO - __main__ -   Batch Number = 19
01/06/2022 17:15:26 - INFO - __main__ -   Batch Number = 20
Evaluating:  10%|▉         | 20/204 [00:00<00:04, 39.75it/s]01/06/2022 17:15:26 - INFO - __main__ -   Batch Number = 21
01/06/2022 17:15:26 - INFO - __main__ -   Batch Number = 22
01/06/2022 17:15:26 - INFO - __main__ -   Batch Number = 23
01/06/2022 17:15:26 - INFO - __main__ -   Batch Number = 24
01/06/2022 17:15:26 - INFO - __main__ -   Batch Number = 25
Evaluating:  12%|█▏        | 25/204 [00:00<00:04, 41.48it/s]01/06/2022 17:15:26 - INFO - __main__ -   Batch Number = 26
01/06/2022 17:15:26 - INFO - __main__ -   Batch Number = 27
01/06/2022 17:15:26 - INFO - __main__ -   Batch Number = 28
01/06/2022 17:15:26 - INFO - __main__ -   Batch Number = 29
01/06/2022 17:15:26 - INFO - __main__ -   Batch Number = 30
Evaluating:  15%|█▍        | 30/204 [00:00<00:04, 42.51it/s]01/06/2022 17:15:26 - INFO - __main__ -   Batch Number = 31
01/06/2022 17:15:26 - INFO - __main__ -   Batch Number = 32
01/06/2022 17:15:26 - INFO - __main__ -   Batch Number = 33
01/06/2022 17:15:26 - INFO - __main__ -   Batch Number = 34
01/06/2022 17:15:26 - INFO - __main__ -   Batch Number = 35
Evaluating:  17%|█▋        | 35/204 [00:00<00:03, 43.10it/s]01/06/2022 17:15:27 - INFO - __main__ -   Batch Number = 36
01/06/2022 17:15:27 - INFO - __main__ -   Batch Number = 37
01/06/2022 17:15:27 - INFO - __main__ -   Batch Number = 38
01/06/2022 17:15:27 - INFO - __main__ -   Batch Number = 39
01/06/2022 17:15:27 - INFO - __main__ -   Batch Number = 40
Evaluating:  20%|█▉        | 40/204 [00:00<00:03, 43.25it/s]01/06/2022 17:15:27 - INFO - __main__ -   Batch Number = 41
01/06/2022 17:15:27 - INFO - __main__ -   Batch Number = 42
01/06/2022 17:15:27 - INFO - __main__ -   Batch Number = 43
01/06/2022 17:15:27 - INFO - __main__ -   Batch Number = 44
01/06/2022 17:15:27 - INFO - __main__ -   Batch Number = 45
Evaluating:  22%|██▏       | 45/204 [00:01<00:03, 43.46it/s]01/06/2022 17:15:27 - INFO - __main__ -   Batch Number = 46
01/06/2022 17:15:27 - INFO - __main__ -   Batch Number = 47
01/06/2022 17:15:27 - INFO - __main__ -   Batch Number = 48
01/06/2022 17:15:27 - INFO - __main__ -   Batch Number = 49
01/06/2022 17:15:27 - INFO - __main__ -   Batch Number = 50
Evaluating:  25%|██▍       | 50/204 [00:01<00:03, 43.67it/s]01/06/2022 17:15:27 - INFO - __main__ -   Batch Number = 51
01/06/2022 17:15:27 - INFO - __main__ -   Batch Number = 52
01/06/2022 17:15:27 - INFO - __main__ -   Batch Number = 53
01/06/2022 17:15:27 - INFO - __main__ -   Batch Number = 54
01/06/2022 17:15:27 - INFO - __main__ -   Batch Number = 55
Evaluating:  27%|██▋       | 55/204 [00:01<00:03, 43.76it/s]01/06/2022 17:15:27 - INFO - __main__ -   Batch Number = 56
01/06/2022 17:15:27 - INFO - __main__ -   Batch Number = 57
01/06/2022 17:15:27 - INFO - __main__ -   Batch Number = 58
01/06/2022 17:15:27 - INFO - __main__ -   Batch Number = 59
01/06/2022 17:15:27 - INFO - __main__ -   Batch Number = 60
Evaluating:  29%|██▉       | 60/204 [00:01<00:03, 43.95it/s]01/06/2022 17:15:27 - INFO - __main__ -   Batch Number = 61
01/06/2022 17:15:27 - INFO - __main__ -   Batch Number = 62
01/06/2022 17:15:27 - INFO - __main__ -   Batch Number = 63
01/06/2022 17:15:27 - INFO - __main__ -   Batch Number = 64
01/06/2022 17:15:27 - INFO - __main__ -   Batch Number = 65
Evaluating:  32%|███▏      | 65/204 [00:01<00:03, 43.97it/s]01/06/2022 17:15:27 - INFO - __main__ -   Batch Number = 66
01/06/2022 17:15:27 - INFO - __main__ -   Batch Number = 67
01/06/2022 17:15:27 - INFO - __main__ -   Batch Number = 68
01/06/2022 17:15:27 - INFO - __main__ -   Batch Number = 69
01/06/2022 17:15:27 - INFO - __main__ -   Batch Number = 70
Evaluating:  34%|███▍      | 70/204 [00:01<00:03, 44.04it/s]01/06/2022 17:15:27 - INFO - __main__ -   Batch Number = 71
01/06/2022 17:15:27 - INFO - __main__ -   Batch Number = 72
01/06/2022 17:15:27 - INFO - __main__ -   Batch Number = 73
01/06/2022 17:15:27 - INFO - __main__ -   Batch Number = 74
01/06/2022 17:15:27 - INFO - __main__ -   Batch Number = 75
Evaluating:  37%|███▋      | 75/204 [00:01<00:02, 44.04it/s]01/06/2022 17:15:27 - INFO - __main__ -   Batch Number = 76
01/06/2022 17:15:27 - INFO - __main__ -   Batch Number = 77
01/06/2022 17:15:27 - INFO - __main__ -   Batch Number = 78
01/06/2022 17:15:27 - INFO - __main__ -   Batch Number = 79
01/06/2022 17:15:28 - INFO - __main__ -   Batch Number = 80
Evaluating:  39%|███▉      | 80/204 [00:01<00:02, 43.84it/s]01/06/2022 17:15:28 - INFO - __main__ -   Batch Number = 81
01/06/2022 17:15:28 - INFO - __main__ -   Batch Number = 82
01/06/2022 17:15:28 - INFO - __main__ -   Batch Number = 83
01/06/2022 17:15:28 - INFO - __main__ -   Batch Number = 84
01/06/2022 17:15:28 - INFO - __main__ -   Batch Number = 85
Evaluating:  42%|████▏     | 85/204 [00:02<00:02, 44.14it/s]01/06/2022 17:15:28 - INFO - __main__ -   Batch Number = 86
01/06/2022 17:15:28 - INFO - __main__ -   Batch Number = 87
01/06/2022 17:15:28 - INFO - __main__ -   Batch Number = 88
01/06/2022 17:15:28 - INFO - __main__ -   Batch Number = 89
01/06/2022 17:15:28 - INFO - __main__ -   Batch Number = 90
Evaluating:  44%|████▍     | 90/204 [00:02<00:02, 44.23it/s]01/06/2022 17:15:28 - INFO - __main__ -   Batch Number = 91
01/06/2022 17:15:28 - INFO - __main__ -   Batch Number = 92
01/06/2022 17:15:28 - INFO - __main__ -   Batch Number = 93
01/06/2022 17:15:28 - INFO - __main__ -   Batch Number = 94
01/06/2022 17:15:28 - INFO - __main__ -   Batch Number = 95
Evaluating:  47%|████▋     | 95/204 [00:02<00:02, 44.27it/s]01/06/2022 17:15:28 - INFO - __main__ -   Batch Number = 96
01/06/2022 17:15:28 - INFO - __main__ -   Batch Number = 97
01/06/2022 17:15:28 - INFO - __main__ -   Batch Number = 98
01/06/2022 17:15:28 - INFO - __main__ -   Batch Number = 99
01/06/2022 17:15:28 - INFO - __main__ -   Batch Number = 100
Evaluating:  49%|████▉     | 100/204 [00:02<00:02, 44.48it/s]01/06/2022 17:15:28 - INFO - __main__ -   Batch Number = 101
01/06/2022 17:15:28 - INFO - __main__ -   Batch Number = 102
01/06/2022 17:15:28 - INFO - __main__ -   Batch Number = 103
01/06/2022 17:15:28 - INFO - __main__ -   Batch Number = 104
01/06/2022 17:15:28 - INFO - __main__ -   Batch Number = 105
Evaluating:  51%|█████▏    | 105/204 [00:02<00:02, 42.61it/s]01/06/2022 17:15:28 - INFO - __main__ -   Batch Number = 106
01/06/2022 17:15:28 - INFO - __main__ -   Batch Number = 107
01/06/2022 17:15:28 - INFO - __main__ -   Batch Number = 108
01/06/2022 17:15:28 - INFO - __main__ -   Batch Number = 109
01/06/2022 17:15:28 - INFO - __main__ -   Batch Number = 110
Evaluating:  54%|█████▍    | 110/204 [00:02<00:02, 39.78it/s]01/06/2022 17:15:28 - INFO - __main__ -   Batch Number = 111
01/06/2022 17:15:28 - INFO - __main__ -   Batch Number = 112
01/06/2022 17:15:28 - INFO - __main__ -   Batch Number = 113
01/06/2022 17:15:28 - INFO - __main__ -   Batch Number = 114
01/06/2022 17:15:28 - INFO - __main__ -   Batch Number = 115
Evaluating:  56%|█████▋    | 115/204 [00:02<00:02, 41.22it/s]01/06/2022 17:15:28 - INFO - __main__ -   Batch Number = 116
01/06/2022 17:15:28 - INFO - __main__ -   Batch Number = 117
01/06/2022 17:15:28 - INFO - __main__ -   Batch Number = 118
01/06/2022 17:15:28 - INFO - __main__ -   Batch Number = 119
01/06/2022 17:15:28 - INFO - __main__ -   Batch Number = 120
Evaluating:  59%|█████▉    | 120/204 [00:02<00:01, 42.21it/s]01/06/2022 17:15:28 - INFO - __main__ -   Batch Number = 121
01/06/2022 17:15:29 - INFO - __main__ -   Batch Number = 122
01/06/2022 17:15:29 - INFO - __main__ -   Batch Number = 123
01/06/2022 17:15:29 - INFO - __main__ -   Batch Number = 124
01/06/2022 17:15:29 - INFO - __main__ -   Batch Number = 125
Evaluating:  61%|██████▏   | 125/204 [00:02<00:01, 41.34it/s]01/06/2022 17:15:29 - INFO - __main__ -   Batch Number = 126
01/06/2022 17:15:29 - INFO - __main__ -   Batch Number = 127
01/06/2022 17:15:29 - INFO - __main__ -   Batch Number = 128
01/06/2022 17:15:29 - INFO - __main__ -   Batch Number = 129
01/06/2022 17:15:29 - INFO - __main__ -   Batch Number = 130
Evaluating:  64%|██████▎   | 130/204 [00:03<00:01, 42.13it/s]01/06/2022 17:15:29 - INFO - __main__ -   Batch Number = 131
01/06/2022 17:15:29 - INFO - __main__ -   Batch Number = 132
01/06/2022 17:15:29 - INFO - __main__ -   Batch Number = 133
01/06/2022 17:15:29 - INFO - __main__ -   Batch Number = 134
01/06/2022 17:15:29 - INFO - __main__ -   Batch Number = 135
Evaluating:  66%|██████▌   | 135/204 [00:03<00:01, 42.92it/s]01/06/2022 17:15:29 - INFO - __main__ -   Batch Number = 136
01/06/2022 17:15:29 - INFO - __main__ -   Batch Number = 137
01/06/2022 17:15:29 - INFO - __main__ -   Batch Number = 138
01/06/2022 17:15:29 - INFO - __main__ -   Batch Number = 139
01/06/2022 17:15:29 - INFO - __main__ -   Batch Number = 140
Evaluating:  69%|██████▊   | 140/204 [00:03<00:01, 43.26it/s]01/06/2022 17:15:29 - INFO - __main__ -   Batch Number = 141
01/06/2022 17:15:29 - INFO - __main__ -   Batch Number = 142
01/06/2022 17:15:29 - INFO - __main__ -   Batch Number = 143
01/06/2022 17:15:29 - INFO - __main__ -   Batch Number = 144
01/06/2022 17:15:29 - INFO - __main__ -   Batch Number = 145
Evaluating:  71%|███████   | 145/204 [00:03<00:01, 43.75it/s]01/06/2022 17:15:29 - INFO - __main__ -   Batch Number = 146
01/06/2022 17:15:29 - INFO - __main__ -   Batch Number = 147
01/06/2022 17:15:29 - INFO - __main__ -   Batch Number = 148
01/06/2022 17:15:29 - INFO - __main__ -   Batch Number = 149
01/06/2022 17:15:29 - INFO - __main__ -   Batch Number = 150
Evaluating:  74%|███████▎  | 150/204 [00:03<00:01, 43.88it/s]01/06/2022 17:15:29 - INFO - __main__ -   Batch Number = 151
01/06/2022 17:15:29 - INFO - __main__ -   Batch Number = 152
01/06/2022 17:15:29 - INFO - __main__ -   Batch Number = 153
01/06/2022 17:15:29 - INFO - __main__ -   Batch Number = 154
01/06/2022 17:15:29 - INFO - __main__ -   Batch Number = 155
Evaluating:  76%|███████▌  | 155/204 [00:03<00:01, 44.01it/s]01/06/2022 17:15:29 - INFO - __main__ -   Batch Number = 156
01/06/2022 17:15:29 - INFO - __main__ -   Batch Number = 157
01/06/2022 17:15:29 - INFO - __main__ -   Batch Number = 158
01/06/2022 17:15:29 - INFO - __main__ -   Batch Number = 159
01/06/2022 17:15:29 - INFO - __main__ -   Batch Number = 160
Evaluating:  78%|███████▊  | 160/204 [00:03<00:00, 44.18it/s]01/06/2022 17:15:29 - INFO - __main__ -   Batch Number = 161
01/06/2022 17:15:29 - INFO - __main__ -   Batch Number = 162
01/06/2022 17:15:29 - INFO - __main__ -   Batch Number = 163
01/06/2022 17:15:29 - INFO - __main__ -   Batch Number = 164
01/06/2022 17:15:29 - INFO - __main__ -   Batch Number = 165
Evaluating:  81%|████████  | 165/204 [00:03<00:00, 44.29it/s]01/06/2022 17:15:30 - INFO - __main__ -   Batch Number = 166
01/06/2022 17:15:30 - INFO - __main__ -   Batch Number = 167
01/06/2022 17:15:30 - INFO - __main__ -   Batch Number = 168
01/06/2022 17:15:30 - INFO - __main__ -   Batch Number = 169
01/06/2022 17:15:30 - INFO - __main__ -   Batch Number = 170
Evaluating:  83%|████████▎ | 170/204 [00:03<00:00, 44.06it/s]01/06/2022 17:15:30 - INFO - __main__ -   Batch Number = 171
01/06/2022 17:15:30 - INFO - __main__ -   Batch Number = 172
01/06/2022 17:15:30 - INFO - __main__ -   Batch Number = 173
01/06/2022 17:15:30 - INFO - __main__ -   Batch Number = 174
01/06/2022 17:15:30 - INFO - __main__ -   Batch Number = 175
Evaluating:  86%|████████▌ | 175/204 [00:04<00:00, 44.12it/s]01/06/2022 17:15:30 - INFO - __main__ -   Batch Number = 176
01/06/2022 17:15:30 - INFO - __main__ -   Batch Number = 177
01/06/2022 17:15:30 - INFO - __main__ -   Batch Number = 178
01/06/2022 17:15:30 - INFO - __main__ -   Batch Number = 179
01/06/2022 17:15:30 - INFO - __main__ -   Batch Number = 180
Evaluating:  88%|████████▊ | 180/204 [00:04<00:00, 44.26it/s]01/06/2022 17:15:30 - INFO - __main__ -   Batch Number = 181
01/06/2022 17:15:30 - INFO - __main__ -   Batch Number = 182
01/06/2022 17:15:30 - INFO - __main__ -   Batch Number = 183
01/06/2022 17:15:30 - INFO - __main__ -   Batch Number = 184
01/06/2022 17:15:30 - INFO - __main__ -   Batch Number = 185
Evaluating:  91%|█████████ | 185/204 [00:04<00:00, 44.15it/s]01/06/2022 17:15:30 - INFO - __main__ -   Batch Number = 186
01/06/2022 17:15:30 - INFO - __main__ -   Batch Number = 187
01/06/2022 17:15:30 - INFO - __main__ -   Batch Number = 188
01/06/2022 17:15:30 - INFO - __main__ -   Batch Number = 189
01/06/2022 17:15:30 - INFO - __main__ -   Batch Number = 190
Evaluating:  93%|█████████▎| 190/204 [00:04<00:00, 44.28it/s]01/06/2022 17:15:30 - INFO - __main__ -   Batch Number = 191
01/06/2022 17:15:30 - INFO - __main__ -   Batch Number = 192
01/06/2022 17:15:30 - INFO - __main__ -   Batch Number = 193
01/06/2022 17:15:30 - INFO - __main__ -   Batch Number = 194
01/06/2022 17:15:30 - INFO - __main__ -   Batch Number = 195
Evaluating:  96%|█████████▌| 195/204 [00:04<00:00, 44.33it/s]01/06/2022 17:15:30 - INFO - __main__ -   Batch Number = 196
01/06/2022 17:15:30 - INFO - __main__ -   Batch Number = 197
01/06/2022 17:15:30 - INFO - __main__ -   Batch Number = 198
01/06/2022 17:15:30 - INFO - __main__ -   Batch Number = 199
01/06/2022 17:15:30 - INFO - __main__ -   Batch Number = 200
Evaluating:  98%|█████████▊| 200/204 [00:04<00:00, 44.22it/s]01/06/2022 17:15:30 - INFO - __main__ -   Batch Number = 201
01/06/2022 17:15:30 - INFO - __main__ -   Batch Number = 202
01/06/2022 17:15:30 - INFO - __main__ -   Batch Number = 203
01/06/2022 17:15:30 - INFO - __main__ -   Batch Number = 204
Evaluating: 100%|██████████| 204/204 [00:04<00:00, 42.95it/s]
01/06/2022 17:15:30 - INFO - __main__ -     Evaluation done in total 4.750057 secs (0.002916 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/predictions_hi_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/nbest_predictions_hi_.json
01/06/2022 17:15:33 - INFO - __main__ -   Results = OrderedDict([('exact', 33.02521008403362), ('f1', 47.07234623908813), ('total', 1190), ('HasAns_exact', 33.02521008403362), ('HasAns_f1', 47.07234623908813), ('HasAns_total', 1190), ('best_exact', 33.02521008403362), ('best_exact_thresh', 0.0), ('best_f1', 47.07234623908813), ('best_f1_thresh', 0.0)])
01/06/2022 17:15:34 - INFO - __main__ -   Language adapter for ru not found, using en instead
01/06/2022 17:15:34 - INFO - __main__ -   Set active language adapter to en
01/06/2022 17:15:34 - INFO - __main__ -   Args Adapter Weight = None
01/06/2022 17:15:34 - INFO - __main__ -   Adapter Languages = ['en']
01/06/2022 17:15:34 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
  0%|          | 0/48 [00:00<?, ?it/s] 31%|███▏      | 15/48 [00:00<00:00, 144.39it/s] 62%|██████▎   | 30/48 [00:00<00:00, 107.80it/s] 92%|█████████▏| 44/48 [00:00<00:00, 114.19it/s]100%|██████████| 48/48 [00:00<00:00, 117.33it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<07:34,  2.62it/s]convert squad examples to features:  24%|██▍       | 289/1190 [00:00<00:01, 777.58it/s]convert squad examples to features:  38%|███▊      | 453/1190 [00:01<00:01, 410.15it/s]convert squad examples to features:  89%|████████▉ | 1057/1190 [00:01<00:00, 1136.38it/s]convert squad examples to features: 100%|██████████| 1190/1190 [00:01<00:00, 855.55it/s] /root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|██████████| 1190/1190 [00:00<00:00, 462878.77it/s]
01/06/2022 17:15:36 - INFO - __main__ -   Local Rank = -1
01/06/2022 17:15:36 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.ru.json_bert-base-multilingual-cased_384_ru
01/06/2022 17:15:37 - INFO - __main__ -   ***** Running evaluation  *****
01/06/2022 17:15:37 - INFO - __main__ -     Num examples = 1410
01/06/2022 17:15:37 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/177 [00:00<?, ?it/s]01/06/2022 17:15:37 - INFO - __main__ -   Batch Number = 1
01/06/2022 17:15:37 - INFO - __main__ -   Batch Number = 2
01/06/2022 17:15:37 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|▏         | 3/177 [00:00<00:05, 29.92it/s]01/06/2022 17:15:37 - INFO - __main__ -   Batch Number = 4
01/06/2022 17:15:37 - INFO - __main__ -   Batch Number = 5
01/06/2022 17:15:37 - INFO - __main__ -   Batch Number = 6
01/06/2022 17:15:37 - INFO - __main__ -   Batch Number = 7
Evaluating:   4%|▍         | 7/177 [00:00<00:05, 31.70it/s]01/06/2022 17:15:37 - INFO - __main__ -   Batch Number = 8
01/06/2022 17:15:37 - INFO - __main__ -   Batch Number = 9
01/06/2022 17:15:37 - INFO - __main__ -   Batch Number = 10
01/06/2022 17:15:37 - INFO - __main__ -   Batch Number = 11
01/06/2022 17:15:38 - INFO - __main__ -   Batch Number = 12
Evaluating:   7%|▋         | 12/177 [00:00<00:04, 37.57it/s]01/06/2022 17:15:38 - INFO - __main__ -   Batch Number = 13
01/06/2022 17:15:38 - INFO - __main__ -   Batch Number = 14
01/06/2022 17:15:38 - INFO - __main__ -   Batch Number = 15
01/06/2022 17:15:38 - INFO - __main__ -   Batch Number = 16
01/06/2022 17:15:38 - INFO - __main__ -   Batch Number = 17
Evaluating:  10%|▉         | 17/177 [00:00<00:03, 40.21it/s]01/06/2022 17:15:38 - INFO - __main__ -   Batch Number = 18
01/06/2022 17:15:38 - INFO - __main__ -   Batch Number = 19
01/06/2022 17:15:38 - INFO - __main__ -   Batch Number = 20
01/06/2022 17:15:38 - INFO - __main__ -   Batch Number = 21
01/06/2022 17:15:38 - INFO - __main__ -   Batch Number = 22
Evaluating:  12%|█▏        | 22/177 [00:00<00:03, 41.90it/s]01/06/2022 17:15:38 - INFO - __main__ -   Batch Number = 23
01/06/2022 17:15:38 - INFO - __main__ -   Batch Number = 24
01/06/2022 17:15:38 - INFO - __main__ -   Batch Number = 25
01/06/2022 17:15:38 - INFO - __main__ -   Batch Number = 26
01/06/2022 17:15:38 - INFO - __main__ -   Batch Number = 27
Evaluating:  15%|█▌        | 27/177 [00:00<00:03, 42.89it/s]01/06/2022 17:15:38 - INFO - __main__ -   Batch Number = 28
01/06/2022 17:15:38 - INFO - __main__ -   Batch Number = 29
01/06/2022 17:15:38 - INFO - __main__ -   Batch Number = 30
01/06/2022 17:15:38 - INFO - __main__ -   Batch Number = 31
01/06/2022 17:15:38 - INFO - __main__ -   Batch Number = 32
Evaluating:  18%|█▊        | 32/177 [00:00<00:03, 43.44it/s]01/06/2022 17:15:38 - INFO - __main__ -   Batch Number = 33
01/06/2022 17:15:38 - INFO - __main__ -   Batch Number = 34
01/06/2022 17:15:38 - INFO - __main__ -   Batch Number = 35
01/06/2022 17:15:38 - INFO - __main__ -   Batch Number = 36
01/06/2022 17:15:38 - INFO - __main__ -   Batch Number = 37
Evaluating:  21%|██        | 37/177 [00:00<00:03, 43.82it/s]01/06/2022 17:15:38 - INFO - __main__ -   Batch Number = 38
01/06/2022 17:15:38 - INFO - __main__ -   Batch Number = 39
01/06/2022 17:15:38 - INFO - __main__ -   Batch Number = 40
01/06/2022 17:15:38 - INFO - __main__ -   Batch Number = 41
01/06/2022 17:15:38 - INFO - __main__ -   Batch Number = 42
Evaluating:  24%|██▎       | 42/177 [00:01<00:03, 44.15it/s]01/06/2022 17:15:38 - INFO - __main__ -   Batch Number = 43
01/06/2022 17:15:38 - INFO - __main__ -   Batch Number = 44
01/06/2022 17:15:38 - INFO - __main__ -   Batch Number = 45
01/06/2022 17:15:38 - INFO - __main__ -   Batch Number = 46
01/06/2022 17:15:38 - INFO - __main__ -   Batch Number = 47
Evaluating:  27%|██▋       | 47/177 [00:01<00:03, 42.92it/s]01/06/2022 17:15:38 - INFO - __main__ -   Batch Number = 48
01/06/2022 17:15:38 - INFO - __main__ -   Batch Number = 49
01/06/2022 17:15:38 - INFO - __main__ -   Batch Number = 50
01/06/2022 17:15:38 - INFO - __main__ -   Batch Number = 51
01/06/2022 17:15:38 - INFO - __main__ -   Batch Number = 52
Evaluating:  29%|██▉       | 52/177 [00:01<00:02, 43.46it/s]01/06/2022 17:15:38 - INFO - __main__ -   Batch Number = 53
01/06/2022 17:15:38 - INFO - __main__ -   Batch Number = 54
01/06/2022 17:15:38 - INFO - __main__ -   Batch Number = 55
01/06/2022 17:15:39 - INFO - __main__ -   Batch Number = 56
01/06/2022 17:15:39 - INFO - __main__ -   Batch Number = 57
Evaluating:  32%|███▏      | 57/177 [00:01<00:02, 43.93it/s]01/06/2022 17:15:39 - INFO - __main__ -   Batch Number = 58
01/06/2022 17:15:39 - INFO - __main__ -   Batch Number = 59
01/06/2022 17:15:39 - INFO - __main__ -   Batch Number = 60
01/06/2022 17:15:39 - INFO - __main__ -   Batch Number = 61
01/06/2022 17:15:39 - INFO - __main__ -   Batch Number = 62
Evaluating:  35%|███▌      | 62/177 [00:01<00:02, 43.97it/s]01/06/2022 17:15:39 - INFO - __main__ -   Batch Number = 63
01/06/2022 17:15:39 - INFO - __main__ -   Batch Number = 64
01/06/2022 17:15:39 - INFO - __main__ -   Batch Number = 65
01/06/2022 17:15:39 - INFO - __main__ -   Batch Number = 66
01/06/2022 17:15:39 - INFO - __main__ -   Batch Number = 67
Evaluating:  38%|███▊      | 67/177 [00:01<00:02, 44.30it/s]01/06/2022 17:15:39 - INFO - __main__ -   Batch Number = 68
01/06/2022 17:15:39 - INFO - __main__ -   Batch Number = 69
01/06/2022 17:15:39 - INFO - __main__ -   Batch Number = 70
01/06/2022 17:15:39 - INFO - __main__ -   Batch Number = 71
01/06/2022 17:15:39 - INFO - __main__ -   Batch Number = 72
Evaluating:  41%|████      | 72/177 [00:01<00:02, 44.36it/s]01/06/2022 17:15:39 - INFO - __main__ -   Batch Number = 73
01/06/2022 17:15:39 - INFO - __main__ -   Batch Number = 74
01/06/2022 17:15:39 - INFO - __main__ -   Batch Number = 75
01/06/2022 17:15:39 - INFO - __main__ -   Batch Number = 76
01/06/2022 17:15:39 - INFO - __main__ -   Batch Number = 77
Evaluating:  44%|████▎     | 77/177 [00:01<00:02, 44.48it/s]01/06/2022 17:15:39 - INFO - __main__ -   Batch Number = 78
01/06/2022 17:15:39 - INFO - __main__ -   Batch Number = 79
01/06/2022 17:15:39 - INFO - __main__ -   Batch Number = 80
01/06/2022 17:15:39 - INFO - __main__ -   Batch Number = 81
01/06/2022 17:15:39 - INFO - __main__ -   Batch Number = 82
Evaluating:  46%|████▋     | 82/177 [00:01<00:02, 44.59it/s]01/06/2022 17:15:39 - INFO - __main__ -   Batch Number = 83
01/06/2022 17:15:39 - INFO - __main__ -   Batch Number = 84
01/06/2022 17:15:39 - INFO - __main__ -   Batch Number = 85
01/06/2022 17:15:39 - INFO - __main__ -   Batch Number = 86
01/06/2022 17:15:39 - INFO - __main__ -   Batch Number = 87
Evaluating:  49%|████▉     | 87/177 [00:02<00:02, 44.79it/s]01/06/2022 17:15:39 - INFO - __main__ -   Batch Number = 88
01/06/2022 17:15:39 - INFO - __main__ -   Batch Number = 89
01/06/2022 17:15:39 - INFO - __main__ -   Batch Number = 90
01/06/2022 17:15:39 - INFO - __main__ -   Batch Number = 91
01/06/2022 17:15:39 - INFO - __main__ -   Batch Number = 92
Evaluating:  52%|█████▏    | 92/177 [00:02<00:01, 44.70it/s]01/06/2022 17:15:39 - INFO - __main__ -   Batch Number = 93
01/06/2022 17:15:39 - INFO - __main__ -   Batch Number = 94
01/06/2022 17:15:39 - INFO - __main__ -   Batch Number = 95
01/06/2022 17:15:39 - INFO - __main__ -   Batch Number = 96
01/06/2022 17:15:39 - INFO - __main__ -   Batch Number = 97
Evaluating:  55%|█████▍    | 97/177 [00:02<00:01, 44.82it/s]01/06/2022 17:15:39 - INFO - __main__ -   Batch Number = 98
01/06/2022 17:15:39 - INFO - __main__ -   Batch Number = 99
01/06/2022 17:15:39 - INFO - __main__ -   Batch Number = 100
01/06/2022 17:15:40 - INFO - __main__ -   Batch Number = 101
01/06/2022 17:15:40 - INFO - __main__ -   Batch Number = 102
Evaluating:  58%|█████▊    | 102/177 [00:02<00:01, 44.71it/s]01/06/2022 17:15:40 - INFO - __main__ -   Batch Number = 103
01/06/2022 17:15:40 - INFO - __main__ -   Batch Number = 104
01/06/2022 17:15:40 - INFO - __main__ -   Batch Number = 105
01/06/2022 17:15:40 - INFO - __main__ -   Batch Number = 106
01/06/2022 17:15:40 - INFO - __main__ -   Batch Number = 107
Evaluating:  60%|██████    | 107/177 [00:02<00:01, 44.64it/s]01/06/2022 17:15:40 - INFO - __main__ -   Batch Number = 108
01/06/2022 17:15:40 - INFO - __main__ -   Batch Number = 109
01/06/2022 17:15:40 - INFO - __main__ -   Batch Number = 110
01/06/2022 17:15:40 - INFO - __main__ -   Batch Number = 111
01/06/2022 17:15:40 - INFO - __main__ -   Batch Number = 112
Evaluating:  63%|██████▎   | 112/177 [00:02<00:01, 44.67it/s]01/06/2022 17:15:40 - INFO - __main__ -   Batch Number = 113
01/06/2022 17:15:40 - INFO - __main__ -   Batch Number = 114
01/06/2022 17:15:40 - INFO - __main__ -   Batch Number = 115
01/06/2022 17:15:40 - INFO - __main__ -   Batch Number = 116
01/06/2022 17:15:40 - INFO - __main__ -   Batch Number = 117
Evaluating:  66%|██████▌   | 117/177 [00:02<00:01, 44.68it/s]01/06/2022 17:15:40 - INFO - __main__ -   Batch Number = 118
01/06/2022 17:15:40 - INFO - __main__ -   Batch Number = 119
01/06/2022 17:15:40 - INFO - __main__ -   Batch Number = 120
01/06/2022 17:15:40 - INFO - __main__ -   Batch Number = 121
01/06/2022 17:15:40 - INFO - __main__ -   Batch Number = 122
Evaluating:  69%|██████▉   | 122/177 [00:02<00:01, 44.65it/s]01/06/2022 17:15:40 - INFO - __main__ -   Batch Number = 123
01/06/2022 17:15:40 - INFO - __main__ -   Batch Number = 124
01/06/2022 17:15:40 - INFO - __main__ -   Batch Number = 125
01/06/2022 17:15:40 - INFO - __main__ -   Batch Number = 126
01/06/2022 17:15:40 - INFO - __main__ -   Batch Number = 127
Evaluating:  72%|███████▏  | 127/177 [00:02<00:01, 44.63it/s]01/06/2022 17:15:40 - INFO - __main__ -   Batch Number = 128
01/06/2022 17:15:40 - INFO - __main__ -   Batch Number = 129
01/06/2022 17:15:40 - INFO - __main__ -   Batch Number = 130
01/06/2022 17:15:40 - INFO - __main__ -   Batch Number = 131
01/06/2022 17:15:40 - INFO - __main__ -   Batch Number = 132
Evaluating:  75%|███████▍  | 132/177 [00:03<00:01, 44.76it/s]01/06/2022 17:15:40 - INFO - __main__ -   Batch Number = 133
01/06/2022 17:15:40 - INFO - __main__ -   Batch Number = 134
01/06/2022 17:15:40 - INFO - __main__ -   Batch Number = 135
01/06/2022 17:15:40 - INFO - __main__ -   Batch Number = 136
01/06/2022 17:15:40 - INFO - __main__ -   Batch Number = 137
Evaluating:  77%|███████▋  | 137/177 [00:03<00:00, 44.55it/s]01/06/2022 17:15:40 - INFO - __main__ -   Batch Number = 138
01/06/2022 17:15:40 - INFO - __main__ -   Batch Number = 139
01/06/2022 17:15:40 - INFO - __main__ -   Batch Number = 140
01/06/2022 17:15:40 - INFO - __main__ -   Batch Number = 141
01/06/2022 17:15:40 - INFO - __main__ -   Batch Number = 142
Evaluating:  80%|████████  | 142/177 [00:03<00:00, 44.73it/s]01/06/2022 17:15:40 - INFO - __main__ -   Batch Number = 143
01/06/2022 17:15:40 - INFO - __main__ -   Batch Number = 144
01/06/2022 17:15:40 - INFO - __main__ -   Batch Number = 145
01/06/2022 17:15:41 - INFO - __main__ -   Batch Number = 146
01/06/2022 17:15:41 - INFO - __main__ -   Batch Number = 147
Evaluating:  83%|████████▎ | 147/177 [00:03<00:00, 44.66it/s]01/06/2022 17:15:41 - INFO - __main__ -   Batch Number = 148
01/06/2022 17:15:41 - INFO - __main__ -   Batch Number = 149
01/06/2022 17:15:41 - INFO - __main__ -   Batch Number = 150
01/06/2022 17:15:41 - INFO - __main__ -   Batch Number = 151
01/06/2022 17:15:41 - INFO - __main__ -   Batch Number = 152
Evaluating:  86%|████████▌ | 152/177 [00:03<00:00, 44.58it/s]01/06/2022 17:15:41 - INFO - __main__ -   Batch Number = 153
01/06/2022 17:15:41 - INFO - __main__ -   Batch Number = 154
01/06/2022 17:15:41 - INFO - __main__ -   Batch Number = 155
01/06/2022 17:15:41 - INFO - __main__ -   Batch Number = 156
01/06/2022 17:15:41 - INFO - __main__ -   Batch Number = 157
Evaluating:  89%|████████▊ | 157/177 [00:03<00:00, 44.65it/s]01/06/2022 17:15:41 - INFO - __main__ -   Batch Number = 158
01/06/2022 17:15:41 - INFO - __main__ -   Batch Number = 159
01/06/2022 17:15:41 - INFO - __main__ -   Batch Number = 160
01/06/2022 17:15:41 - INFO - __main__ -   Batch Number = 161
01/06/2022 17:15:41 - INFO - __main__ -   Batch Number = 162
Evaluating:  92%|█████████▏| 162/177 [00:03<00:00, 44.74it/s]01/06/2022 17:15:41 - INFO - __main__ -   Batch Number = 163
01/06/2022 17:15:41 - INFO - __main__ -   Batch Number = 164
01/06/2022 17:15:41 - INFO - __main__ -   Batch Number = 165
01/06/2022 17:15:41 - INFO - __main__ -   Batch Number = 166
01/06/2022 17:15:41 - INFO - __main__ -   Batch Number = 167
Evaluating:  94%|█████████▍| 167/177 [00:03<00:00, 44.54it/s]01/06/2022 17:15:41 - INFO - __main__ -   Batch Number = 168
01/06/2022 17:15:41 - INFO - __main__ -   Batch Number = 169
01/06/2022 17:15:41 - INFO - __main__ -   Batch Number = 170
01/06/2022 17:15:41 - INFO - __main__ -   Batch Number = 171
01/06/2022 17:15:41 - INFO - __main__ -   Batch Number = 172
Evaluating:  97%|█████████▋| 172/177 [00:03<00:00, 43.85it/s]01/06/2022 17:15:41 - INFO - __main__ -   Batch Number = 173
01/06/2022 17:15:41 - INFO - __main__ -   Batch Number = 174
01/06/2022 17:15:41 - INFO - __main__ -   Batch Number = 175
01/06/2022 17:15:41 - INFO - __main__ -   Batch Number = 176
01/06/2022 17:15:41 - INFO - __main__ -   Batch Number = 177
Evaluating: 100%|██████████| 177/177 [00:04<00:00, 44.94it/s]Evaluating: 100%|██████████| 177/177 [00:04<00:00, 43.83it/s]
01/06/2022 17:15:41 - INFO - __main__ -     Evaluation done in total 4.038844 secs (0.002864 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/predictions_ru_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/nbest_predictions_ru_.json
01/06/2022 17:15:45 - INFO - __main__ -   Results = OrderedDict([('exact', 49.747899159663866), ('f1', 66.11399752909622), ('total', 1190), ('HasAns_exact', 49.747899159663866), ('HasAns_f1', 66.11399752909622), ('HasAns_total', 1190), ('best_exact', 49.747899159663866), ('best_exact_thresh', 0.0), ('best_f1', 66.11399752909622), ('best_f1_thresh', 0.0)])
01/06/2022 17:15:45 - INFO - __main__ -   Language adapter for th not found, using en instead
01/06/2022 17:15:45 - INFO - __main__ -   Set active language adapter to en
01/06/2022 17:15:45 - INFO - __main__ -   Args Adapter Weight = None
01/06/2022 17:15:45 - INFO - __main__ -   Adapter Languages = ['en']
01/06/2022 17:15:45 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
  0%|          | 0/48 [00:00<?, ?it/s] 33%|███▎      | 16/48 [00:00<00:00, 127.16it/s] 60%|██████    | 29/48 [00:00<00:00, 92.34it/s]  92%|█████████▏| 44/48 [00:00<00:00, 111.96it/s]100%|██████████| 48/48 [00:00<00:00, 111.79it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<12:37,  1.57it/s]convert squad examples to features:  24%|██▍       | 289/1190 [00:00<00:01, 509.86it/s]convert squad examples to features:  36%|███▌      | 430/1190 [00:02<00:03, 192.46it/s]convert squad examples to features:  89%|████████▉ | 1057/1190 [00:02<00:00, 640.36it/s]convert squad examples to features: 100%|██████████| 1190/1190 [00:02<00:00, 455.68it/s]/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|██████████| 1190/1190 [00:00<00:00, 311309.28it/s]
01/06/2022 17:15:49 - INFO - __main__ -   Local Rank = -1
01/06/2022 17:15:49 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.th.json_bert-base-multilingual-cased_384_th
01/06/2022 17:15:51 - INFO - __main__ -   ***** Running evaluation  *****
01/06/2022 17:15:51 - INFO - __main__ -     Num examples = 3123
01/06/2022 17:15:51 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/391 [00:00<?, ?it/s]01/06/2022 17:15:51 - INFO - __main__ -   Batch Number = 1
01/06/2022 17:15:51 - INFO - __main__ -   Batch Number = 2
01/06/2022 17:15:51 - INFO - __main__ -   Batch Number = 3
Evaluating:   1%|          | 3/391 [00:00<00:13, 29.59it/s]01/06/2022 17:15:51 - INFO - __main__ -   Batch Number = 4
01/06/2022 17:15:51 - INFO - __main__ -   Batch Number = 5
01/06/2022 17:15:51 - INFO - __main__ -   Batch Number = 6
Evaluating:   2%|▏         | 6/391 [00:00<00:12, 29.76it/s]01/06/2022 17:15:51 - INFO - __main__ -   Batch Number = 7
01/06/2022 17:15:51 - INFO - __main__ -   Batch Number = 8
01/06/2022 17:15:52 - INFO - __main__ -   Batch Number = 9
01/06/2022 17:15:52 - INFO - __main__ -   Batch Number = 10
Evaluating:   3%|▎         | 10/391 [00:00<00:11, 34.30it/s]01/06/2022 17:15:52 - INFO - __main__ -   Batch Number = 11
01/06/2022 17:15:52 - INFO - __main__ -   Batch Number = 12
01/06/2022 17:15:52 - INFO - __main__ -   Batch Number = 13
01/06/2022 17:15:52 - INFO - __main__ -   Batch Number = 14
01/06/2022 17:15:52 - INFO - __main__ -   Batch Number = 15
Evaluating:   4%|▍         | 15/391 [00:00<00:09, 38.91it/s]01/06/2022 17:15:52 - INFO - __main__ -   Batch Number = 16
01/06/2022 17:15:52 - INFO - __main__ -   Batch Number = 17
01/06/2022 17:15:52 - INFO - __main__ -   Batch Number = 18
01/06/2022 17:15:52 - INFO - __main__ -   Batch Number = 19
01/06/2022 17:15:52 - INFO - __main__ -   Batch Number = 20
Evaluating:   5%|▌         | 20/391 [00:00<00:09, 40.78it/s]01/06/2022 17:15:52 - INFO - __main__ -   Batch Number = 21
01/06/2022 17:15:52 - INFO - __main__ -   Batch Number = 22
01/06/2022 17:15:52 - INFO - __main__ -   Batch Number = 23
01/06/2022 17:15:52 - INFO - __main__ -   Batch Number = 24
01/06/2022 17:15:52 - INFO - __main__ -   Batch Number = 25
Evaluating:   6%|▋         | 25/391 [00:00<00:08, 42.20it/s]01/06/2022 17:15:52 - INFO - __main__ -   Batch Number = 26
01/06/2022 17:15:52 - INFO - __main__ -   Batch Number = 27
01/06/2022 17:15:52 - INFO - __main__ -   Batch Number = 28
01/06/2022 17:15:52 - INFO - __main__ -   Batch Number = 29
01/06/2022 17:15:52 - INFO - __main__ -   Batch Number = 30
Evaluating:   8%|▊         | 30/391 [00:00<00:08, 43.02it/s]01/06/2022 17:15:52 - INFO - __main__ -   Batch Number = 31
01/06/2022 17:15:52 - INFO - __main__ -   Batch Number = 32
01/06/2022 17:15:52 - INFO - __main__ -   Batch Number = 33
01/06/2022 17:15:52 - INFO - __main__ -   Batch Number = 34
01/06/2022 17:15:52 - INFO - __main__ -   Batch Number = 35
Evaluating:   9%|▉         | 35/391 [00:00<00:08, 43.50it/s]01/06/2022 17:15:52 - INFO - __main__ -   Batch Number = 36
01/06/2022 17:15:52 - INFO - __main__ -   Batch Number = 37
01/06/2022 17:15:52 - INFO - __main__ -   Batch Number = 38
01/06/2022 17:15:52 - INFO - __main__ -   Batch Number = 39
01/06/2022 17:15:52 - INFO - __main__ -   Batch Number = 40
Evaluating:  10%|█         | 40/391 [00:00<00:08, 43.78it/s]01/06/2022 17:15:52 - INFO - __main__ -   Batch Number = 41
01/06/2022 17:15:52 - INFO - __main__ -   Batch Number = 42
01/06/2022 17:15:52 - INFO - __main__ -   Batch Number = 43
01/06/2022 17:15:52 - INFO - __main__ -   Batch Number = 44
01/06/2022 17:15:52 - INFO - __main__ -   Batch Number = 45
Evaluating:  12%|█▏        | 45/391 [00:01<00:07, 44.12it/s]01/06/2022 17:15:52 - INFO - __main__ -   Batch Number = 46
01/06/2022 17:15:52 - INFO - __main__ -   Batch Number = 47
01/06/2022 17:15:52 - INFO - __main__ -   Batch Number = 48
01/06/2022 17:15:52 - INFO - __main__ -   Batch Number = 49
01/06/2022 17:15:52 - INFO - __main__ -   Batch Number = 50
Evaluating:  13%|█▎        | 50/391 [00:01<00:07, 44.08it/s]01/06/2022 17:15:52 - INFO - __main__ -   Batch Number = 51
01/06/2022 17:15:52 - INFO - __main__ -   Batch Number = 52
01/06/2022 17:15:53 - INFO - __main__ -   Batch Number = 53
01/06/2022 17:15:53 - INFO - __main__ -   Batch Number = 54
01/06/2022 17:15:53 - INFO - __main__ -   Batch Number = 55
Evaluating:  14%|█▍        | 55/391 [00:01<00:07, 44.02it/s]01/06/2022 17:15:53 - INFO - __main__ -   Batch Number = 56
01/06/2022 17:15:53 - INFO - __main__ -   Batch Number = 57
01/06/2022 17:15:53 - INFO - __main__ -   Batch Number = 58
01/06/2022 17:15:53 - INFO - __main__ -   Batch Number = 59
01/06/2022 17:15:53 - INFO - __main__ -   Batch Number = 60
Evaluating:  15%|█▌        | 60/391 [00:01<00:07, 44.14it/s]01/06/2022 17:15:53 - INFO - __main__ -   Batch Number = 61
01/06/2022 17:15:53 - INFO - __main__ -   Batch Number = 62
01/06/2022 17:15:53 - INFO - __main__ -   Batch Number = 63
01/06/2022 17:15:53 - INFO - __main__ -   Batch Number = 64
01/06/2022 17:15:53 - INFO - __main__ -   Batch Number = 65
Evaluating:  17%|█▋        | 65/391 [00:01<00:07, 44.01it/s]01/06/2022 17:15:53 - INFO - __main__ -   Batch Number = 66
01/06/2022 17:15:53 - INFO - __main__ -   Batch Number = 67
01/06/2022 17:15:53 - INFO - __main__ -   Batch Number = 68
01/06/2022 17:15:53 - INFO - __main__ -   Batch Number = 69
01/06/2022 17:15:53 - INFO - __main__ -   Batch Number = 70
Evaluating:  18%|█▊        | 70/391 [00:01<00:07, 44.21it/s]01/06/2022 17:15:53 - INFO - __main__ -   Batch Number = 71
01/06/2022 17:15:53 - INFO - __main__ -   Batch Number = 72
01/06/2022 17:15:53 - INFO - __main__ -   Batch Number = 73
01/06/2022 17:15:53 - INFO - __main__ -   Batch Number = 74
01/06/2022 17:15:53 - INFO - __main__ -   Batch Number = 75
Evaluating:  19%|█▉        | 75/391 [00:01<00:07, 44.33it/s]01/06/2022 17:15:53 - INFO - __main__ -   Batch Number = 76
01/06/2022 17:15:53 - INFO - __main__ -   Batch Number = 77
01/06/2022 17:15:53 - INFO - __main__ -   Batch Number = 78
01/06/2022 17:15:53 - INFO - __main__ -   Batch Number = 79
01/06/2022 17:15:53 - INFO - __main__ -   Batch Number = 80
Evaluating:  20%|██        | 80/391 [00:01<00:07, 44.37it/s]01/06/2022 17:15:53 - INFO - __main__ -   Batch Number = 81
01/06/2022 17:15:53 - INFO - __main__ -   Batch Number = 82
01/06/2022 17:15:53 - INFO - __main__ -   Batch Number = 83
01/06/2022 17:15:53 - INFO - __main__ -   Batch Number = 84
01/06/2022 17:15:53 - INFO - __main__ -   Batch Number = 85
Evaluating:  22%|██▏       | 85/391 [00:01<00:06, 44.48it/s]01/06/2022 17:15:53 - INFO - __main__ -   Batch Number = 86
01/06/2022 17:15:53 - INFO - __main__ -   Batch Number = 87
01/06/2022 17:15:53 - INFO - __main__ -   Batch Number = 88
01/06/2022 17:15:53 - INFO - __main__ -   Batch Number = 89
01/06/2022 17:15:53 - INFO - __main__ -   Batch Number = 90
Evaluating:  23%|██▎       | 90/391 [00:02<00:06, 44.54it/s]01/06/2022 17:15:53 - INFO - __main__ -   Batch Number = 91
01/06/2022 17:15:53 - INFO - __main__ -   Batch Number = 92
01/06/2022 17:15:53 - INFO - __main__ -   Batch Number = 93
01/06/2022 17:15:53 - INFO - __main__ -   Batch Number = 94
01/06/2022 17:15:53 - INFO - __main__ -   Batch Number = 95
Evaluating:  24%|██▍       | 95/391 [00:02<00:06, 44.54it/s]01/06/2022 17:15:53 - INFO - __main__ -   Batch Number = 96
01/06/2022 17:15:53 - INFO - __main__ -   Batch Number = 97
01/06/2022 17:15:54 - INFO - __main__ -   Batch Number = 98
01/06/2022 17:15:54 - INFO - __main__ -   Batch Number = 99
01/06/2022 17:15:54 - INFO - __main__ -   Batch Number = 100
Evaluating:  26%|██▌       | 100/391 [00:02<00:06, 44.37it/s]01/06/2022 17:15:54 - INFO - __main__ -   Batch Number = 101
01/06/2022 17:15:54 - INFO - __main__ -   Batch Number = 102
01/06/2022 17:15:54 - INFO - __main__ -   Batch Number = 103
01/06/2022 17:15:54 - INFO - __main__ -   Batch Number = 104
01/06/2022 17:15:54 - INFO - __main__ -   Batch Number = 105
Evaluating:  27%|██▋       | 105/391 [00:02<00:06, 44.46it/s]01/06/2022 17:15:54 - INFO - __main__ -   Batch Number = 106
01/06/2022 17:15:54 - INFO - __main__ -   Batch Number = 107
01/06/2022 17:15:54 - INFO - __main__ -   Batch Number = 108
01/06/2022 17:15:54 - INFO - __main__ -   Batch Number = 109
01/06/2022 17:15:54 - INFO - __main__ -   Batch Number = 110
Evaluating:  28%|██▊       | 110/391 [00:02<00:06, 44.30it/s]01/06/2022 17:15:54 - INFO - __main__ -   Batch Number = 111
01/06/2022 17:15:54 - INFO - __main__ -   Batch Number = 112
01/06/2022 17:15:54 - INFO - __main__ -   Batch Number = 113
01/06/2022 17:15:54 - INFO - __main__ -   Batch Number = 114
01/06/2022 17:15:54 - INFO - __main__ -   Batch Number = 115
Evaluating:  29%|██▉       | 115/391 [00:02<00:06, 44.47it/s]01/06/2022 17:15:54 - INFO - __main__ -   Batch Number = 116
01/06/2022 17:15:54 - INFO - __main__ -   Batch Number = 117
01/06/2022 17:15:54 - INFO - __main__ -   Batch Number = 118
01/06/2022 17:15:54 - INFO - __main__ -   Batch Number = 119
01/06/2022 17:15:54 - INFO - __main__ -   Batch Number = 120
Evaluating:  31%|███       | 120/391 [00:02<00:06, 44.48it/s]01/06/2022 17:15:54 - INFO - __main__ -   Batch Number = 121
01/06/2022 17:15:54 - INFO - __main__ -   Batch Number = 122
01/06/2022 17:15:54 - INFO - __main__ -   Batch Number = 123
01/06/2022 17:15:54 - INFO - __main__ -   Batch Number = 124
01/06/2022 17:15:54 - INFO - __main__ -   Batch Number = 125
Evaluating:  32%|███▏      | 125/391 [00:02<00:05, 44.42it/s]01/06/2022 17:15:54 - INFO - __main__ -   Batch Number = 126
01/06/2022 17:15:54 - INFO - __main__ -   Batch Number = 127
01/06/2022 17:15:54 - INFO - __main__ -   Batch Number = 128
01/06/2022 17:15:54 - INFO - __main__ -   Batch Number = 129
01/06/2022 17:15:54 - INFO - __main__ -   Batch Number = 130
Evaluating:  33%|███▎      | 130/391 [00:03<00:05, 44.52it/s]01/06/2022 17:15:54 - INFO - __main__ -   Batch Number = 131
01/06/2022 17:15:54 - INFO - __main__ -   Batch Number = 132
01/06/2022 17:15:54 - INFO - __main__ -   Batch Number = 133
01/06/2022 17:15:54 - INFO - __main__ -   Batch Number = 134
01/06/2022 17:15:54 - INFO - __main__ -   Batch Number = 135
Evaluating:  35%|███▍      | 135/391 [00:03<00:05, 44.59it/s]01/06/2022 17:15:54 - INFO - __main__ -   Batch Number = 136
01/06/2022 17:15:54 - INFO - __main__ -   Batch Number = 137
01/06/2022 17:15:54 - INFO - __main__ -   Batch Number = 138
01/06/2022 17:15:54 - INFO - __main__ -   Batch Number = 139
01/06/2022 17:15:54 - INFO - __main__ -   Batch Number = 140
Evaluating:  36%|███▌      | 140/391 [00:03<00:05, 44.58it/s]01/06/2022 17:15:54 - INFO - __main__ -   Batch Number = 141
01/06/2022 17:15:55 - INFO - __main__ -   Batch Number = 142
01/06/2022 17:15:55 - INFO - __main__ -   Batch Number = 143
01/06/2022 17:15:55 - INFO - __main__ -   Batch Number = 144
01/06/2022 17:15:55 - INFO - __main__ -   Batch Number = 145
Evaluating:  37%|███▋      | 145/391 [00:03<00:05, 44.52it/s]01/06/2022 17:15:55 - INFO - __main__ -   Batch Number = 146
01/06/2022 17:15:55 - INFO - __main__ -   Batch Number = 147
01/06/2022 17:15:55 - INFO - __main__ -   Batch Number = 148
01/06/2022 17:15:55 - INFO - __main__ -   Batch Number = 149
01/06/2022 17:15:55 - INFO - __main__ -   Batch Number = 150
Evaluating:  38%|███▊      | 150/391 [00:03<00:05, 44.39it/s]01/06/2022 17:15:55 - INFO - __main__ -   Batch Number = 151
01/06/2022 17:15:55 - INFO - __main__ -   Batch Number = 152
01/06/2022 17:15:55 - INFO - __main__ -   Batch Number = 153
01/06/2022 17:15:55 - INFO - __main__ -   Batch Number = 154
01/06/2022 17:15:55 - INFO - __main__ -   Batch Number = 155
Evaluating:  40%|███▉      | 155/391 [00:03<00:05, 44.13it/s]01/06/2022 17:15:55 - INFO - __main__ -   Batch Number = 156
01/06/2022 17:15:55 - INFO - __main__ -   Batch Number = 157
01/06/2022 17:15:55 - INFO - __main__ -   Batch Number = 158
01/06/2022 17:15:55 - INFO - __main__ -   Batch Number = 159
01/06/2022 17:15:55 - INFO - __main__ -   Batch Number = 160
Evaluating:  41%|████      | 160/391 [00:03<00:05, 44.21it/s]01/06/2022 17:15:55 - INFO - __main__ -   Batch Number = 161
01/06/2022 17:15:55 - INFO - __main__ -   Batch Number = 162
01/06/2022 17:15:55 - INFO - __main__ -   Batch Number = 163
01/06/2022 17:15:55 - INFO - __main__ -   Batch Number = 164
01/06/2022 17:15:55 - INFO - __main__ -   Batch Number = 165
Evaluating:  42%|████▏     | 165/391 [00:03<00:05, 44.22it/s]01/06/2022 17:15:55 - INFO - __main__ -   Batch Number = 166
01/06/2022 17:15:55 - INFO - __main__ -   Batch Number = 167
01/06/2022 17:15:55 - INFO - __main__ -   Batch Number = 168
01/06/2022 17:15:55 - INFO - __main__ -   Batch Number = 169
01/06/2022 17:15:55 - INFO - __main__ -   Batch Number = 170
Evaluating:  43%|████▎     | 170/391 [00:03<00:05, 42.61it/s]01/06/2022 17:15:55 - INFO - __main__ -   Batch Number = 171
01/06/2022 17:15:55 - INFO - __main__ -   Batch Number = 172
01/06/2022 17:15:55 - INFO - __main__ -   Batch Number = 173
01/06/2022 17:15:55 - INFO - __main__ -   Batch Number = 174
01/06/2022 17:15:55 - INFO - __main__ -   Batch Number = 175
Evaluating:  45%|████▍     | 175/391 [00:04<00:05, 43.12it/s]01/06/2022 17:15:55 - INFO - __main__ -   Batch Number = 176
01/06/2022 17:15:55 - INFO - __main__ -   Batch Number = 177
01/06/2022 17:15:55 - INFO - __main__ -   Batch Number = 178
01/06/2022 17:15:55 - INFO - __main__ -   Batch Number = 179
01/06/2022 17:15:55 - INFO - __main__ -   Batch Number = 180
Evaluating:  46%|████▌     | 180/391 [00:04<00:04, 43.47it/s]01/06/2022 17:15:55 - INFO - __main__ -   Batch Number = 181
01/06/2022 17:15:55 - INFO - __main__ -   Batch Number = 182
01/06/2022 17:15:55 - INFO - __main__ -   Batch Number = 183
01/06/2022 17:15:55 - INFO - __main__ -   Batch Number = 184
01/06/2022 17:15:55 - INFO - __main__ -   Batch Number = 185
Evaluating:  47%|████▋     | 185/391 [00:04<00:04, 43.68it/s]01/06/2022 17:15:56 - INFO - __main__ -   Batch Number = 186
01/06/2022 17:15:56 - INFO - __main__ -   Batch Number = 187
01/06/2022 17:15:56 - INFO - __main__ -   Batch Number = 188
01/06/2022 17:15:56 - INFO - __main__ -   Batch Number = 189
01/06/2022 17:15:56 - INFO - __main__ -   Batch Number = 190
Evaluating:  49%|████▊     | 190/391 [00:04<00:04, 43.88it/s]01/06/2022 17:15:56 - INFO - __main__ -   Batch Number = 191
01/06/2022 17:15:56 - INFO - __main__ -   Batch Number = 192
01/06/2022 17:15:56 - INFO - __main__ -   Batch Number = 193
01/06/2022 17:15:56 - INFO - __main__ -   Batch Number = 194
01/06/2022 17:15:56 - INFO - __main__ -   Batch Number = 195
Evaluating:  50%|████▉     | 195/391 [00:04<00:04, 43.99it/s]01/06/2022 17:15:56 - INFO - __main__ -   Batch Number = 196
01/06/2022 17:15:56 - INFO - __main__ -   Batch Number = 197
01/06/2022 17:15:56 - INFO - __main__ -   Batch Number = 198
01/06/2022 17:15:56 - INFO - __main__ -   Batch Number = 199
01/06/2022 17:15:56 - INFO - __main__ -   Batch Number = 200
Evaluating:  51%|█████     | 200/391 [00:04<00:04, 44.01it/s]01/06/2022 17:15:56 - INFO - __main__ -   Batch Number = 201
01/06/2022 17:15:56 - INFO - __main__ -   Batch Number = 202
01/06/2022 17:15:56 - INFO - __main__ -   Batch Number = 203
01/06/2022 17:15:56 - INFO - __main__ -   Batch Number = 204
01/06/2022 17:15:56 - INFO - __main__ -   Batch Number = 205
Evaluating:  52%|█████▏    | 205/391 [00:04<00:04, 44.14it/s]01/06/2022 17:15:56 - INFO - __main__ -   Batch Number = 206
01/06/2022 17:15:56 - INFO - __main__ -   Batch Number = 207
01/06/2022 17:15:56 - INFO - __main__ -   Batch Number = 208
01/06/2022 17:15:56 - INFO - __main__ -   Batch Number = 209
01/06/2022 17:15:56 - INFO - __main__ -   Batch Number = 210
Evaluating:  54%|█████▎    | 210/391 [00:04<00:04, 44.34it/s]01/06/2022 17:15:56 - INFO - __main__ -   Batch Number = 211
01/06/2022 17:15:56 - INFO - __main__ -   Batch Number = 212
01/06/2022 17:15:56 - INFO - __main__ -   Batch Number = 213
01/06/2022 17:15:56 - INFO - __main__ -   Batch Number = 214
01/06/2022 17:15:56 - INFO - __main__ -   Batch Number = 215
Evaluating:  55%|█████▍    | 215/391 [00:04<00:03, 44.31it/s]01/06/2022 17:15:56 - INFO - __main__ -   Batch Number = 216
01/06/2022 17:15:56 - INFO - __main__ -   Batch Number = 217
01/06/2022 17:15:56 - INFO - __main__ -   Batch Number = 218
01/06/2022 17:15:56 - INFO - __main__ -   Batch Number = 219
01/06/2022 17:15:56 - INFO - __main__ -   Batch Number = 220
Evaluating:  56%|█████▋    | 220/391 [00:05<00:03, 44.43it/s]01/06/2022 17:15:56 - INFO - __main__ -   Batch Number = 221
01/06/2022 17:15:56 - INFO - __main__ -   Batch Number = 222
01/06/2022 17:15:56 - INFO - __main__ -   Batch Number = 223
01/06/2022 17:15:56 - INFO - __main__ -   Batch Number = 224
01/06/2022 17:15:56 - INFO - __main__ -   Batch Number = 225
Evaluating:  58%|█████▊    | 225/391 [00:05<00:03, 44.50it/s]01/06/2022 17:15:56 - INFO - __main__ -   Batch Number = 226
01/06/2022 17:15:56 - INFO - __main__ -   Batch Number = 227
01/06/2022 17:15:56 - INFO - __main__ -   Batch Number = 228
01/06/2022 17:15:56 - INFO - __main__ -   Batch Number = 229
01/06/2022 17:15:57 - INFO - __main__ -   Batch Number = 230
Evaluating:  59%|█████▉    | 230/391 [00:05<00:03, 44.51it/s]01/06/2022 17:15:57 - INFO - __main__ -   Batch Number = 231
01/06/2022 17:15:57 - INFO - __main__ -   Batch Number = 232
01/06/2022 17:15:57 - INFO - __main__ -   Batch Number = 233
01/06/2022 17:15:57 - INFO - __main__ -   Batch Number = 234
01/06/2022 17:15:57 - INFO - __main__ -   Batch Number = 235
Evaluating:  60%|██████    | 235/391 [00:05<00:03, 44.48it/s]01/06/2022 17:15:57 - INFO - __main__ -   Batch Number = 236
01/06/2022 17:15:57 - INFO - __main__ -   Batch Number = 237
01/06/2022 17:15:57 - INFO - __main__ -   Batch Number = 238
01/06/2022 17:15:57 - INFO - __main__ -   Batch Number = 239
01/06/2022 17:15:57 - INFO - __main__ -   Batch Number = 240
Evaluating:  61%|██████▏   | 240/391 [00:05<00:03, 44.59it/s]01/06/2022 17:15:57 - INFO - __main__ -   Batch Number = 241
01/06/2022 17:15:57 - INFO - __main__ -   Batch Number = 242
01/06/2022 17:15:57 - INFO - __main__ -   Batch Number = 243
01/06/2022 17:15:57 - INFO - __main__ -   Batch Number = 244
01/06/2022 17:15:57 - INFO - __main__ -   Batch Number = 245
Evaluating:  63%|██████▎   | 245/391 [00:05<00:03, 44.63it/s]01/06/2022 17:15:57 - INFO - __main__ -   Batch Number = 246
01/06/2022 17:15:57 - INFO - __main__ -   Batch Number = 247
01/06/2022 17:15:57 - INFO - __main__ -   Batch Number = 248
01/06/2022 17:15:57 - INFO - __main__ -   Batch Number = 249
01/06/2022 17:15:57 - INFO - __main__ -   Batch Number = 250
Evaluating:  64%|██████▍   | 250/391 [00:05<00:03, 44.24it/s]01/06/2022 17:15:57 - INFO - __main__ -   Batch Number = 251
01/06/2022 17:15:57 - INFO - __main__ -   Batch Number = 252
01/06/2022 17:15:57 - INFO - __main__ -   Batch Number = 253
01/06/2022 17:15:57 - INFO - __main__ -   Batch Number = 254
01/06/2022 17:15:57 - INFO - __main__ -   Batch Number = 255
Evaluating:  65%|██████▌   | 255/391 [00:05<00:03, 44.30it/s]01/06/2022 17:15:57 - INFO - __main__ -   Batch Number = 256
01/06/2022 17:15:57 - INFO - __main__ -   Batch Number = 257
01/06/2022 17:15:57 - INFO - __main__ -   Batch Number = 258
01/06/2022 17:15:57 - INFO - __main__ -   Batch Number = 259
01/06/2022 17:15:57 - INFO - __main__ -   Batch Number = 260
Evaluating:  66%|██████▋   | 260/391 [00:05<00:02, 44.31it/s]01/06/2022 17:15:57 - INFO - __main__ -   Batch Number = 261
01/06/2022 17:15:57 - INFO - __main__ -   Batch Number = 262
01/06/2022 17:15:57 - INFO - __main__ -   Batch Number = 263
01/06/2022 17:15:57 - INFO - __main__ -   Batch Number = 264
01/06/2022 17:15:57 - INFO - __main__ -   Batch Number = 265
Evaluating:  68%|██████▊   | 265/391 [00:06<00:02, 44.39it/s]01/06/2022 17:15:57 - INFO - __main__ -   Batch Number = 266
01/06/2022 17:15:57 - INFO - __main__ -   Batch Number = 267
01/06/2022 17:15:57 - INFO - __main__ -   Batch Number = 268
01/06/2022 17:15:57 - INFO - __main__ -   Batch Number = 269
01/06/2022 17:15:57 - INFO - __main__ -   Batch Number = 270
Evaluating:  69%|██████▉   | 270/391 [00:06<00:02, 44.19it/s]01/06/2022 17:15:57 - INFO - __main__ -   Batch Number = 271
01/06/2022 17:15:57 - INFO - __main__ -   Batch Number = 272
01/06/2022 17:15:57 - INFO - __main__ -   Batch Number = 273
01/06/2022 17:15:58 - INFO - __main__ -   Batch Number = 274
01/06/2022 17:15:58 - INFO - __main__ -   Batch Number = 275
Evaluating:  70%|███████   | 275/391 [00:06<00:02, 44.32it/s]01/06/2022 17:15:58 - INFO - __main__ -   Batch Number = 276
01/06/2022 17:15:58 - INFO - __main__ -   Batch Number = 277
01/06/2022 17:15:58 - INFO - __main__ -   Batch Number = 278
01/06/2022 17:15:58 - INFO - __main__ -   Batch Number = 279
01/06/2022 17:15:58 - INFO - __main__ -   Batch Number = 280
Evaluating:  72%|███████▏  | 280/391 [00:06<00:02, 44.22it/s]01/06/2022 17:15:58 - INFO - __main__ -   Batch Number = 281
01/06/2022 17:15:58 - INFO - __main__ -   Batch Number = 282
01/06/2022 17:15:58 - INFO - __main__ -   Batch Number = 283
01/06/2022 17:15:58 - INFO - __main__ -   Batch Number = 284
01/06/2022 17:15:58 - INFO - __main__ -   Batch Number = 285
Evaluating:  73%|███████▎  | 285/391 [00:06<00:02, 44.15it/s]01/06/2022 17:15:58 - INFO - __main__ -   Batch Number = 286
01/06/2022 17:15:58 - INFO - __main__ -   Batch Number = 287
01/06/2022 17:15:58 - INFO - __main__ -   Batch Number = 288
01/06/2022 17:15:58 - INFO - __main__ -   Batch Number = 289
01/06/2022 17:15:58 - INFO - __main__ -   Batch Number = 290
Evaluating:  74%|███████▍  | 290/391 [00:06<00:02, 44.36it/s]01/06/2022 17:15:58 - INFO - __main__ -   Batch Number = 291
01/06/2022 17:15:58 - INFO - __main__ -   Batch Number = 292
01/06/2022 17:15:58 - INFO - __main__ -   Batch Number = 293
01/06/2022 17:15:58 - INFO - __main__ -   Batch Number = 294
01/06/2022 17:15:58 - INFO - __main__ -   Batch Number = 295
Evaluating:  75%|███████▌  | 295/391 [00:06<00:02, 44.21it/s]01/06/2022 17:15:58 - INFO - __main__ -   Batch Number = 296
01/06/2022 17:15:58 - INFO - __main__ -   Batch Number = 297
01/06/2022 17:15:58 - INFO - __main__ -   Batch Number = 298
01/06/2022 17:15:58 - INFO - __main__ -   Batch Number = 299
01/06/2022 17:15:58 - INFO - __main__ -   Batch Number = 300
Evaluating:  77%|███████▋  | 300/391 [00:06<00:02, 44.34it/s]01/06/2022 17:15:58 - INFO - __main__ -   Batch Number = 301
01/06/2022 17:15:58 - INFO - __main__ -   Batch Number = 302
01/06/2022 17:15:58 - INFO - __main__ -   Batch Number = 303
01/06/2022 17:15:58 - INFO - __main__ -   Batch Number = 304
01/06/2022 17:15:58 - INFO - __main__ -   Batch Number = 305
Evaluating:  78%|███████▊  | 305/391 [00:06<00:01, 44.12it/s]01/06/2022 17:15:58 - INFO - __main__ -   Batch Number = 306
01/06/2022 17:15:58 - INFO - __main__ -   Batch Number = 307
01/06/2022 17:15:58 - INFO - __main__ -   Batch Number = 308
01/06/2022 17:15:58 - INFO - __main__ -   Batch Number = 309
01/06/2022 17:15:58 - INFO - __main__ -   Batch Number = 310
Evaluating:  79%|███████▉  | 310/391 [00:07<00:01, 44.25it/s]01/06/2022 17:15:58 - INFO - __main__ -   Batch Number = 311
01/06/2022 17:15:58 - INFO - __main__ -   Batch Number = 312
01/06/2022 17:15:58 - INFO - __main__ -   Batch Number = 313
01/06/2022 17:15:58 - INFO - __main__ -   Batch Number = 314
01/06/2022 17:15:58 - INFO - __main__ -   Batch Number = 315
Evaluating:  81%|████████  | 315/391 [00:07<00:01, 44.38it/s]01/06/2022 17:15:58 - INFO - __main__ -   Batch Number = 316
01/06/2022 17:15:58 - INFO - __main__ -   Batch Number = 317
01/06/2022 17:15:58 - INFO - __main__ -   Batch Number = 318
01/06/2022 17:15:59 - INFO - __main__ -   Batch Number = 319
01/06/2022 17:15:59 - INFO - __main__ -   Batch Number = 320
Evaluating:  82%|████████▏ | 320/391 [00:07<00:01, 42.89it/s]01/06/2022 17:15:59 - INFO - __main__ -   Batch Number = 321
01/06/2022 17:15:59 - INFO - __main__ -   Batch Number = 322
01/06/2022 17:15:59 - INFO - __main__ -   Batch Number = 323
01/06/2022 17:15:59 - INFO - __main__ -   Batch Number = 324
01/06/2022 17:15:59 - INFO - __main__ -   Batch Number = 325
Evaluating:  83%|████████▎ | 325/391 [00:07<00:01, 40.16it/s]01/06/2022 17:15:59 - INFO - __main__ -   Batch Number = 326
01/06/2022 17:15:59 - INFO - __main__ -   Batch Number = 327
01/06/2022 17:15:59 - INFO - __main__ -   Batch Number = 328
01/06/2022 17:15:59 - INFO - __main__ -   Batch Number = 329
01/06/2022 17:15:59 - INFO - __main__ -   Batch Number = 330
Evaluating:  84%|████████▍ | 330/391 [00:07<00:01, 41.41it/s]01/06/2022 17:15:59 - INFO - __main__ -   Batch Number = 331
01/06/2022 17:15:59 - INFO - __main__ -   Batch Number = 332
01/06/2022 17:15:59 - INFO - __main__ -   Batch Number = 333
01/06/2022 17:15:59 - INFO - __main__ -   Batch Number = 334
01/06/2022 17:15:59 - INFO - __main__ -   Batch Number = 335
Evaluating:  86%|████████▌ | 335/391 [00:07<00:01, 42.41it/s]01/06/2022 17:15:59 - INFO - __main__ -   Batch Number = 336
01/06/2022 17:15:59 - INFO - __main__ -   Batch Number = 337
01/06/2022 17:15:59 - INFO - __main__ -   Batch Number = 338
01/06/2022 17:15:59 - INFO - __main__ -   Batch Number = 339
01/06/2022 17:15:59 - INFO - __main__ -   Batch Number = 340
Evaluating:  87%|████████▋ | 340/391 [00:07<00:01, 43.05it/s]01/06/2022 17:15:59 - INFO - __main__ -   Batch Number = 341
01/06/2022 17:15:59 - INFO - __main__ -   Batch Number = 342
01/06/2022 17:15:59 - INFO - __main__ -   Batch Number = 343
01/06/2022 17:15:59 - INFO - __main__ -   Batch Number = 344
01/06/2022 17:15:59 - INFO - __main__ -   Batch Number = 345
Evaluating:  88%|████████▊ | 345/391 [00:07<00:01, 43.25it/s]01/06/2022 17:15:59 - INFO - __main__ -   Batch Number = 346
01/06/2022 17:15:59 - INFO - __main__ -   Batch Number = 347
01/06/2022 17:15:59 - INFO - __main__ -   Batch Number = 348
01/06/2022 17:15:59 - INFO - __main__ -   Batch Number = 349
01/06/2022 17:15:59 - INFO - __main__ -   Batch Number = 350
Evaluating:  90%|████████▉ | 350/391 [00:08<00:00, 43.60it/s]01/06/2022 17:15:59 - INFO - __main__ -   Batch Number = 351
01/06/2022 17:15:59 - INFO - __main__ -   Batch Number = 352
01/06/2022 17:15:59 - INFO - __main__ -   Batch Number = 353
01/06/2022 17:15:59 - INFO - __main__ -   Batch Number = 354
01/06/2022 17:15:59 - INFO - __main__ -   Batch Number = 355
Evaluating:  91%|█████████ | 355/391 [00:08<00:00, 43.77it/s]01/06/2022 17:15:59 - INFO - __main__ -   Batch Number = 356
01/06/2022 17:15:59 - INFO - __main__ -   Batch Number = 357
01/06/2022 17:15:59 - INFO - __main__ -   Batch Number = 358
01/06/2022 17:15:59 - INFO - __main__ -   Batch Number = 359
01/06/2022 17:15:59 - INFO - __main__ -   Batch Number = 360
Evaluating:  92%|█████████▏| 360/391 [00:08<00:00, 43.98it/s]01/06/2022 17:16:00 - INFO - __main__ -   Batch Number = 361
01/06/2022 17:16:00 - INFO - __main__ -   Batch Number = 362
01/06/2022 17:16:00 - INFO - __main__ -   Batch Number = 363
01/06/2022 17:16:00 - INFO - __main__ -   Batch Number = 364
01/06/2022 17:16:00 - INFO - __main__ -   Batch Number = 365
Evaluating:  93%|█████████▎| 365/391 [00:08<00:00, 43.99it/s]01/06/2022 17:16:00 - INFO - __main__ -   Batch Number = 366
01/06/2022 17:16:00 - INFO - __main__ -   Batch Number = 367
01/06/2022 17:16:00 - INFO - __main__ -   Batch Number = 368
01/06/2022 17:16:00 - INFO - __main__ -   Batch Number = 369
01/06/2022 17:16:00 - INFO - __main__ -   Batch Number = 370
Evaluating:  95%|█████████▍| 370/391 [00:08<00:00, 44.25it/s]01/06/2022 17:16:00 - INFO - __main__ -   Batch Number = 371
01/06/2022 17:16:00 - INFO - __main__ -   Batch Number = 372
01/06/2022 17:16:00 - INFO - __main__ -   Batch Number = 373
01/06/2022 17:16:00 - INFO - __main__ -   Batch Number = 374
01/06/2022 17:16:00 - INFO - __main__ -   Batch Number = 375
Evaluating:  96%|█████████▌| 375/391 [00:08<00:00, 44.37it/s]01/06/2022 17:16:00 - INFO - __main__ -   Batch Number = 376
01/06/2022 17:16:00 - INFO - __main__ -   Batch Number = 377
01/06/2022 17:16:00 - INFO - __main__ -   Batch Number = 378
01/06/2022 17:16:00 - INFO - __main__ -   Batch Number = 379
01/06/2022 17:16:00 - INFO - __main__ -   Batch Number = 380
Evaluating:  97%|█████████▋| 380/391 [00:08<00:00, 42.80it/s]01/06/2022 17:16:00 - INFO - __main__ -   Batch Number = 381
01/06/2022 17:16:00 - INFO - __main__ -   Batch Number = 382
01/06/2022 17:16:00 - INFO - __main__ -   Batch Number = 383
01/06/2022 17:16:00 - INFO - __main__ -   Batch Number = 384
01/06/2022 17:16:00 - INFO - __main__ -   Batch Number = 385
Evaluating:  98%|█████████▊| 385/391 [00:08<00:00, 43.36it/s]01/06/2022 17:16:00 - INFO - __main__ -   Batch Number = 386
01/06/2022 17:16:00 - INFO - __main__ -   Batch Number = 387
01/06/2022 17:16:00 - INFO - __main__ -   Batch Number = 388
01/06/2022 17:16:00 - INFO - __main__ -   Batch Number = 389
01/06/2022 17:16:00 - INFO - __main__ -   Batch Number = 390
Evaluating: 100%|█████████▉| 390/391 [00:08<00:00, 43.59it/s]01/06/2022 17:16:00 - INFO - __main__ -   Batch Number = 391
Evaluating: 100%|██████████| 391/391 [00:08<00:00, 43.69it/s]
01/06/2022 17:16:00 - INFO - __main__ -     Evaluation done in total 8.950820 secs (0.002866 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/predictions_th_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/nbest_predictions_th_.json
01/06/2022 17:16:11 - INFO - __main__ -   Results = OrderedDict([('exact', 11.932773109243698), ('f1', 17.047788812494694), ('total', 1190), ('HasAns_exact', 11.932773109243698), ('HasAns_f1', 17.047788812494694), ('HasAns_total', 1190), ('best_exact', 11.932773109243698), ('best_exact_thresh', 0.0), ('best_f1', 17.047788812494694), ('best_f1_thresh', 0.0)])
01/06/2022 17:16:11 - INFO - __main__ -   Language adapter for tr not found, using en instead
01/06/2022 17:16:11 - INFO - __main__ -   Set active language adapter to en
01/06/2022 17:16:11 - INFO - __main__ -   Args Adapter Weight = None
01/06/2022 17:16:11 - INFO - __main__ -   Adapter Languages = ['en']
01/06/2022 17:16:11 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
  0%|          | 0/48 [00:00<?, ?it/s] 33%|███▎      | 16/48 [00:00<00:00, 129.92it/s] 60%|██████    | 29/48 [00:00<00:00, 124.56it/s] 94%|█████████▍| 45/48 [00:00<00:00, 137.51it/s]100%|██████████| 48/48 [00:00<00:00, 136.56it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<07:21,  2.70it/s]convert squad examples to features:  32%|███▏      | 385/1190 [00:01<00:01, 430.36it/s]convert squad examples to features:  94%|█████████▍| 1121/1190 [00:01<00:00, 1348.58it/s]convert squad examples to features: 100%|██████████| 1190/1190 [00:01<00:00, 1021.02it/s]/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|██████████| 1190/1190 [00:00<00:00, 682140.46it/s]
01/06/2022 17:16:14 - INFO - __main__ -   Local Rank = -1
01/06/2022 17:16:14 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.tr.json_bert-base-multilingual-cased_384_tr
01/06/2022 17:16:15 - INFO - __main__ -   ***** Running evaluation  *****
01/06/2022 17:16:15 - INFO - __main__ -     Num examples = 1360
01/06/2022 17:16:15 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/170 [00:00<?, ?it/s]01/06/2022 17:16:15 - INFO - __main__ -   Batch Number = 1
01/06/2022 17:16:15 - INFO - __main__ -   Batch Number = 2
01/06/2022 17:16:15 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|▏         | 3/170 [00:00<00:05, 29.52it/s]01/06/2022 17:16:15 - INFO - __main__ -   Batch Number = 4
01/06/2022 17:16:15 - INFO - __main__ -   Batch Number = 5
01/06/2022 17:16:15 - INFO - __main__ -   Batch Number = 6
Evaluating:   4%|▎         | 6/170 [00:00<00:05, 29.62it/s]01/06/2022 17:16:15 - INFO - __main__ -   Batch Number = 7
01/06/2022 17:16:15 - INFO - __main__ -   Batch Number = 8
01/06/2022 17:16:15 - INFO - __main__ -   Batch Number = 9
01/06/2022 17:16:15 - INFO - __main__ -   Batch Number = 10
01/06/2022 17:16:15 - INFO - __main__ -   Batch Number = 11
Evaluating:   6%|▋         | 11/170 [00:00<00:04, 36.96it/s]01/06/2022 17:16:15 - INFO - __main__ -   Batch Number = 12
01/06/2022 17:16:15 - INFO - __main__ -   Batch Number = 13
01/06/2022 17:16:15 - INFO - __main__ -   Batch Number = 14
01/06/2022 17:16:15 - INFO - __main__ -   Batch Number = 15
01/06/2022 17:16:15 - INFO - __main__ -   Batch Number = 16
Evaluating:   9%|▉         | 16/170 [00:00<00:03, 40.17it/s]01/06/2022 17:16:15 - INFO - __main__ -   Batch Number = 17
01/06/2022 17:16:15 - INFO - __main__ -   Batch Number = 18
01/06/2022 17:16:15 - INFO - __main__ -   Batch Number = 19
01/06/2022 17:16:15 - INFO - __main__ -   Batch Number = 20
01/06/2022 17:16:15 - INFO - __main__ -   Batch Number = 21
Evaluating:  12%|█▏        | 21/170 [00:00<00:03, 41.76it/s]01/06/2022 17:16:15 - INFO - __main__ -   Batch Number = 22
01/06/2022 17:16:15 - INFO - __main__ -   Batch Number = 23
01/06/2022 17:16:15 - INFO - __main__ -   Batch Number = 24
01/06/2022 17:16:15 - INFO - __main__ -   Batch Number = 25
01/06/2022 17:16:15 - INFO - __main__ -   Batch Number = 26
Evaluating:  15%|█▌        | 26/170 [00:00<00:03, 42.70it/s]01/06/2022 17:16:15 - INFO - __main__ -   Batch Number = 27
01/06/2022 17:16:15 - INFO - __main__ -   Batch Number = 28
01/06/2022 17:16:15 - INFO - __main__ -   Batch Number = 29
01/06/2022 17:16:15 - INFO - __main__ -   Batch Number = 30
01/06/2022 17:16:15 - INFO - __main__ -   Batch Number = 31
Evaluating:  18%|█▊        | 31/170 [00:00<00:03, 43.51it/s]01/06/2022 17:16:15 - INFO - __main__ -   Batch Number = 32
01/06/2022 17:16:15 - INFO - __main__ -   Batch Number = 33
01/06/2022 17:16:15 - INFO - __main__ -   Batch Number = 34
01/06/2022 17:16:15 - INFO - __main__ -   Batch Number = 35
01/06/2022 17:16:16 - INFO - __main__ -   Batch Number = 36
Evaluating:  21%|██        | 36/170 [00:00<00:03, 43.51it/s]01/06/2022 17:16:16 - INFO - __main__ -   Batch Number = 37
01/06/2022 17:16:16 - INFO - __main__ -   Batch Number = 38
01/06/2022 17:16:16 - INFO - __main__ -   Batch Number = 39
01/06/2022 17:16:16 - INFO - __main__ -   Batch Number = 40
01/06/2022 17:16:16 - INFO - __main__ -   Batch Number = 41
Evaluating:  24%|██▍       | 41/170 [00:00<00:02, 43.88it/s]01/06/2022 17:16:16 - INFO - __main__ -   Batch Number = 42
01/06/2022 17:16:16 - INFO - __main__ -   Batch Number = 43
01/06/2022 17:16:16 - INFO - __main__ -   Batch Number = 44
01/06/2022 17:16:16 - INFO - __main__ -   Batch Number = 45
01/06/2022 17:16:16 - INFO - __main__ -   Batch Number = 46
Evaluating:  27%|██▋       | 46/170 [00:01<00:02, 43.88it/s]01/06/2022 17:16:16 - INFO - __main__ -   Batch Number = 47
01/06/2022 17:16:16 - INFO - __main__ -   Batch Number = 48
01/06/2022 17:16:16 - INFO - __main__ -   Batch Number = 49
01/06/2022 17:16:16 - INFO - __main__ -   Batch Number = 50
01/06/2022 17:16:16 - INFO - __main__ -   Batch Number = 51
Evaluating:  30%|███       | 51/170 [00:01<00:02, 44.01it/s]01/06/2022 17:16:16 - INFO - __main__ -   Batch Number = 52
01/06/2022 17:16:16 - INFO - __main__ -   Batch Number = 53
01/06/2022 17:16:16 - INFO - __main__ -   Batch Number = 54
01/06/2022 17:16:16 - INFO - __main__ -   Batch Number = 55
01/06/2022 17:16:16 - INFO - __main__ -   Batch Number = 56
Evaluating:  33%|███▎      | 56/170 [00:01<00:02, 44.17it/s]01/06/2022 17:16:16 - INFO - __main__ -   Batch Number = 57
01/06/2022 17:16:16 - INFO - __main__ -   Batch Number = 58
01/06/2022 17:16:16 - INFO - __main__ -   Batch Number = 59
01/06/2022 17:16:16 - INFO - __main__ -   Batch Number = 60
01/06/2022 17:16:16 - INFO - __main__ -   Batch Number = 61
Evaluating:  36%|███▌      | 61/170 [00:01<00:02, 44.34it/s]01/06/2022 17:16:16 - INFO - __main__ -   Batch Number = 62
01/06/2022 17:16:16 - INFO - __main__ -   Batch Number = 63
01/06/2022 17:16:16 - INFO - __main__ -   Batch Number = 64
01/06/2022 17:16:16 - INFO - __main__ -   Batch Number = 65
01/06/2022 17:16:16 - INFO - __main__ -   Batch Number = 66
Evaluating:  39%|███▉      | 66/170 [00:01<00:02, 44.26it/s]01/06/2022 17:16:16 - INFO - __main__ -   Batch Number = 67
01/06/2022 17:16:16 - INFO - __main__ -   Batch Number = 68
01/06/2022 17:16:16 - INFO - __main__ -   Batch Number = 69
01/06/2022 17:16:16 - INFO - __main__ -   Batch Number = 70
01/06/2022 17:16:16 - INFO - __main__ -   Batch Number = 71
Evaluating:  42%|████▏     | 71/170 [00:01<00:02, 44.44it/s]01/06/2022 17:16:16 - INFO - __main__ -   Batch Number = 72
01/06/2022 17:16:16 - INFO - __main__ -   Batch Number = 73
01/06/2022 17:16:16 - INFO - __main__ -   Batch Number = 74
01/06/2022 17:16:16 - INFO - __main__ -   Batch Number = 75
01/06/2022 17:16:16 - INFO - __main__ -   Batch Number = 76
Evaluating:  45%|████▍     | 76/170 [00:01<00:02, 44.55it/s]01/06/2022 17:16:16 - INFO - __main__ -   Batch Number = 77
01/06/2022 17:16:16 - INFO - __main__ -   Batch Number = 78
01/06/2022 17:16:16 - INFO - __main__ -   Batch Number = 79
01/06/2022 17:16:16 - INFO - __main__ -   Batch Number = 80
01/06/2022 17:16:17 - INFO - __main__ -   Batch Number = 81
Evaluating:  48%|████▊     | 81/170 [00:01<00:02, 44.47it/s]01/06/2022 17:16:17 - INFO - __main__ -   Batch Number = 82
01/06/2022 17:16:17 - INFO - __main__ -   Batch Number = 83
01/06/2022 17:16:17 - INFO - __main__ -   Batch Number = 84
01/06/2022 17:16:17 - INFO - __main__ -   Batch Number = 85
01/06/2022 17:16:17 - INFO - __main__ -   Batch Number = 86
Evaluating:  51%|█████     | 86/170 [00:02<00:01, 44.52it/s]01/06/2022 17:16:17 - INFO - __main__ -   Batch Number = 87
01/06/2022 17:16:17 - INFO - __main__ -   Batch Number = 88
01/06/2022 17:16:17 - INFO - __main__ -   Batch Number = 89
01/06/2022 17:16:17 - INFO - __main__ -   Batch Number = 90
01/06/2022 17:16:17 - INFO - __main__ -   Batch Number = 91
Evaluating:  54%|█████▎    | 91/170 [00:02<00:01, 44.47it/s]01/06/2022 17:16:17 - INFO - __main__ -   Batch Number = 92
01/06/2022 17:16:17 - INFO - __main__ -   Batch Number = 93
01/06/2022 17:16:17 - INFO - __main__ -   Batch Number = 94
01/06/2022 17:16:17 - INFO - __main__ -   Batch Number = 95
01/06/2022 17:16:17 - INFO - __main__ -   Batch Number = 96
Evaluating:  56%|█████▋    | 96/170 [00:02<00:01, 44.02it/s]01/06/2022 17:16:17 - INFO - __main__ -   Batch Number = 97
01/06/2022 17:16:17 - INFO - __main__ -   Batch Number = 98
01/06/2022 17:16:17 - INFO - __main__ -   Batch Number = 99
01/06/2022 17:16:17 - INFO - __main__ -   Batch Number = 100
01/06/2022 17:16:17 - INFO - __main__ -   Batch Number = 101
Evaluating:  59%|█████▉    | 101/170 [00:02<00:01, 44.00it/s]01/06/2022 17:16:17 - INFO - __main__ -   Batch Number = 102
01/06/2022 17:16:17 - INFO - __main__ -   Batch Number = 103
01/06/2022 17:16:17 - INFO - __main__ -   Batch Number = 104
01/06/2022 17:16:17 - INFO - __main__ -   Batch Number = 105
01/06/2022 17:16:17 - INFO - __main__ -   Batch Number = 106
Evaluating:  62%|██████▏   | 106/170 [00:02<00:01, 44.14it/s]01/06/2022 17:16:17 - INFO - __main__ -   Batch Number = 107
01/06/2022 17:16:17 - INFO - __main__ -   Batch Number = 108
01/06/2022 17:16:17 - INFO - __main__ -   Batch Number = 109
01/06/2022 17:16:17 - INFO - __main__ -   Batch Number = 110
01/06/2022 17:16:17 - INFO - __main__ -   Batch Number = 111
Evaluating:  65%|██████▌   | 111/170 [00:02<00:01, 44.20it/s]01/06/2022 17:16:17 - INFO - __main__ -   Batch Number = 112
01/06/2022 17:16:17 - INFO - __main__ -   Batch Number = 113
01/06/2022 17:16:17 - INFO - __main__ -   Batch Number = 114
01/06/2022 17:16:17 - INFO - __main__ -   Batch Number = 115
01/06/2022 17:16:17 - INFO - __main__ -   Batch Number = 116
Evaluating:  68%|██████▊   | 116/170 [00:02<00:01, 44.37it/s]01/06/2022 17:16:17 - INFO - __main__ -   Batch Number = 117
01/06/2022 17:16:17 - INFO - __main__ -   Batch Number = 118
01/06/2022 17:16:17 - INFO - __main__ -   Batch Number = 119
01/06/2022 17:16:17 - INFO - __main__ -   Batch Number = 120
01/06/2022 17:16:17 - INFO - __main__ -   Batch Number = 121
Evaluating:  71%|███████   | 121/170 [00:02<00:01, 44.50it/s]01/06/2022 17:16:17 - INFO - __main__ -   Batch Number = 122
01/06/2022 17:16:17 - INFO - __main__ -   Batch Number = 123
01/06/2022 17:16:17 - INFO - __main__ -   Batch Number = 124
01/06/2022 17:16:18 - INFO - __main__ -   Batch Number = 125
01/06/2022 17:16:18 - INFO - __main__ -   Batch Number = 126
Evaluating:  74%|███████▍  | 126/170 [00:02<00:00, 44.42it/s]01/06/2022 17:16:18 - INFO - __main__ -   Batch Number = 127
01/06/2022 17:16:18 - INFO - __main__ -   Batch Number = 128
01/06/2022 17:16:18 - INFO - __main__ -   Batch Number = 129
01/06/2022 17:16:18 - INFO - __main__ -   Batch Number = 130
01/06/2022 17:16:18 - INFO - __main__ -   Batch Number = 131
Evaluating:  77%|███████▋  | 131/170 [00:03<00:00, 44.51it/s]01/06/2022 17:16:18 - INFO - __main__ -   Batch Number = 132
01/06/2022 17:16:18 - INFO - __main__ -   Batch Number = 133
01/06/2022 17:16:18 - INFO - __main__ -   Batch Number = 134
01/06/2022 17:16:18 - INFO - __main__ -   Batch Number = 135
01/06/2022 17:16:18 - INFO - __main__ -   Batch Number = 136
Evaluating:  80%|████████  | 136/170 [00:03<00:00, 44.33it/s]01/06/2022 17:16:18 - INFO - __main__ -   Batch Number = 137
01/06/2022 17:16:18 - INFO - __main__ -   Batch Number = 138
01/06/2022 17:16:18 - INFO - __main__ -   Batch Number = 139
01/06/2022 17:16:18 - INFO - __main__ -   Batch Number = 140
01/06/2022 17:16:18 - INFO - __main__ -   Batch Number = 141
Evaluating:  83%|████████▎ | 141/170 [00:03<00:00, 44.24it/s]01/06/2022 17:16:18 - INFO - __main__ -   Batch Number = 142
01/06/2022 17:16:18 - INFO - __main__ -   Batch Number = 143
01/06/2022 17:16:18 - INFO - __main__ -   Batch Number = 144
01/06/2022 17:16:18 - INFO - __main__ -   Batch Number = 145
01/06/2022 17:16:18 - INFO - __main__ -   Batch Number = 146
Evaluating:  86%|████████▌ | 146/170 [00:03<00:00, 44.31it/s]01/06/2022 17:16:18 - INFO - __main__ -   Batch Number = 147
01/06/2022 17:16:18 - INFO - __main__ -   Batch Number = 148
01/06/2022 17:16:18 - INFO - __main__ -   Batch Number = 149
01/06/2022 17:16:18 - INFO - __main__ -   Batch Number = 150
01/06/2022 17:16:18 - INFO - __main__ -   Batch Number = 151
Evaluating:  89%|████████▉ | 151/170 [00:03<00:00, 44.46it/s]01/06/2022 17:16:18 - INFO - __main__ -   Batch Number = 152
01/06/2022 17:16:18 - INFO - __main__ -   Batch Number = 153
01/06/2022 17:16:18 - INFO - __main__ -   Batch Number = 154
01/06/2022 17:16:18 - INFO - __main__ -   Batch Number = 155
01/06/2022 17:16:18 - INFO - __main__ -   Batch Number = 156
Evaluating:  92%|█████████▏| 156/170 [00:03<00:00, 42.89it/s]01/06/2022 17:16:18 - INFO - __main__ -   Batch Number = 157
01/06/2022 17:16:18 - INFO - __main__ -   Batch Number = 158
01/06/2022 17:16:18 - INFO - __main__ -   Batch Number = 159
01/06/2022 17:16:18 - INFO - __main__ -   Batch Number = 160
01/06/2022 17:16:18 - INFO - __main__ -   Batch Number = 161
Evaluating:  95%|█████████▍| 161/170 [00:03<00:00, 43.37it/s]01/06/2022 17:16:18 - INFO - __main__ -   Batch Number = 162
01/06/2022 17:16:18 - INFO - __main__ -   Batch Number = 163
01/06/2022 17:16:18 - INFO - __main__ -   Batch Number = 164
01/06/2022 17:16:18 - INFO - __main__ -   Batch Number = 165
01/06/2022 17:16:18 - INFO - __main__ -   Batch Number = 166
Evaluating:  98%|█████████▊| 166/170 [00:03<00:00, 43.68it/s]01/06/2022 17:16:18 - INFO - __main__ -   Batch Number = 167
01/06/2022 17:16:18 - INFO - __main__ -   Batch Number = 168
01/06/2022 17:16:19 - INFO - __main__ -   Batch Number = 169
01/06/2022 17:16:19 - INFO - __main__ -   Batch Number = 170
Evaluating: 100%|██████████| 170/170 [00:03<00:00, 43.49it/s]
01/06/2022 17:16:19 - INFO - __main__ -     Evaluation done in total 3.908947 secs (0.002874 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/predictions_tr_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/nbest_predictions_tr_.json
01/06/2022 17:16:22 - INFO - __main__ -   Results = OrderedDict([('exact', 31.51260504201681), ('f1', 47.91087920317416), ('total', 1190), ('HasAns_exact', 31.51260504201681), ('HasAns_f1', 47.91087920317416), ('HasAns_total', 1190), ('best_exact', 31.51260504201681), ('best_exact_thresh', 0.0), ('best_f1', 47.91087920317416), ('best_f1_thresh', 0.0)])
01/06/2022 17:16:22 - INFO - __main__ -   Language adapter for vi not found, using en instead
01/06/2022 17:16:22 - INFO - __main__ -   Set active language adapter to en
01/06/2022 17:16:22 - INFO - __main__ -   Args Adapter Weight = None
01/06/2022 17:16:22 - INFO - __main__ -   Adapter Languages = ['en']
01/06/2022 17:16:22 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
  0%|          | 0/48 [00:00<?, ?it/s] 33%|███▎      | 16/48 [00:00<00:00, 136.92it/s] 62%|██████▎   | 30/48 [00:00<00:00, 130.56it/s] 96%|█████████▌| 46/48 [00:00<00:00, 141.82it/s]100%|██████████| 48/48 [00:00<00:00, 140.52it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<06:00,  3.30it/s]convert squad examples to features:  22%|██▏       | 257/1190 [00:00<00:01, 771.12it/s]convert squad examples to features:  32%|███▏      | 385/1190 [00:01<00:02, 345.32it/s]convert squad examples to features:  78%|███████▊  | 929/1190 [00:01<00:00, 951.08it/s]convert squad examples to features:  92%|█████████▏| 1099/1190 [00:01<00:00, 1056.70it/s]convert squad examples to features: 100%|██████████| 1190/1190 [00:01<00:00, 811.03it/s] /root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|██████████| 1190/1190 [00:00<00:00, 496638.98it/s]
01/06/2022 17:16:25 - INFO - __main__ -   Local Rank = -1
01/06/2022 17:16:25 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.vi.json_bert-base-multilingual-cased_384_vi
01/06/2022 17:16:26 - INFO - __main__ -   ***** Running evaluation  *****
01/06/2022 17:16:26 - INFO - __main__ -     Num examples = 1325
01/06/2022 17:16:26 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/166 [00:00<?, ?it/s]01/06/2022 17:16:26 - INFO - __main__ -   Batch Number = 1
01/06/2022 17:16:26 - INFO - __main__ -   Batch Number = 2
01/06/2022 17:16:26 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|▏         | 3/166 [00:00<00:05, 29.69it/s]01/06/2022 17:16:26 - INFO - __main__ -   Batch Number = 4
01/06/2022 17:16:26 - INFO - __main__ -   Batch Number = 5
01/06/2022 17:16:26 - INFO - __main__ -   Batch Number = 6
01/06/2022 17:16:26 - INFO - __main__ -   Batch Number = 7
Evaluating:   4%|▍         | 7/166 [00:00<00:05, 29.98it/s]01/06/2022 17:16:26 - INFO - __main__ -   Batch Number = 8
01/06/2022 17:16:26 - INFO - __main__ -   Batch Number = 9
01/06/2022 17:16:26 - INFO - __main__ -   Batch Number = 10
01/06/2022 17:16:26 - INFO - __main__ -   Batch Number = 11
Evaluating:   7%|▋         | 11/166 [00:00<00:04, 33.80it/s]01/06/2022 17:16:26 - INFO - __main__ -   Batch Number = 12
01/06/2022 17:16:26 - INFO - __main__ -   Batch Number = 13
01/06/2022 17:16:26 - INFO - __main__ -   Batch Number = 14
01/06/2022 17:16:26 - INFO - __main__ -   Batch Number = 15
01/06/2022 17:16:26 - INFO - __main__ -   Batch Number = 16
Evaluating:  10%|▉         | 16/166 [00:00<00:03, 38.04it/s]01/06/2022 17:16:26 - INFO - __main__ -   Batch Number = 17
01/06/2022 17:16:26 - INFO - __main__ -   Batch Number = 18
01/06/2022 17:16:26 - INFO - __main__ -   Batch Number = 19
01/06/2022 17:16:26 - INFO - __main__ -   Batch Number = 20
01/06/2022 17:16:26 - INFO - __main__ -   Batch Number = 21
Evaluating:  13%|█▎        | 21/166 [00:00<00:03, 40.01it/s]01/06/2022 17:16:26 - INFO - __main__ -   Batch Number = 22
01/06/2022 17:16:26 - INFO - __main__ -   Batch Number = 23
01/06/2022 17:16:26 - INFO - __main__ -   Batch Number = 24
01/06/2022 17:16:26 - INFO - __main__ -   Batch Number = 25
01/06/2022 17:16:26 - INFO - __main__ -   Batch Number = 26
Evaluating:  16%|█▌        | 26/166 [00:00<00:03, 41.59it/s]01/06/2022 17:16:26 - INFO - __main__ -   Batch Number = 27
01/06/2022 17:16:26 - INFO - __main__ -   Batch Number = 28
01/06/2022 17:16:26 - INFO - __main__ -   Batch Number = 29
01/06/2022 17:16:26 - INFO - __main__ -   Batch Number = 30
01/06/2022 17:16:26 - INFO - __main__ -   Batch Number = 31
Evaluating:  19%|█▊        | 31/166 [00:00<00:03, 42.52it/s]01/06/2022 17:16:26 - INFO - __main__ -   Batch Number = 32
01/06/2022 17:16:26 - INFO - __main__ -   Batch Number = 33
01/06/2022 17:16:27 - INFO - __main__ -   Batch Number = 34
01/06/2022 17:16:27 - INFO - __main__ -   Batch Number = 35
01/06/2022 17:16:27 - INFO - __main__ -   Batch Number = 36
Evaluating:  22%|██▏       | 36/166 [00:00<00:03, 42.91it/s]01/06/2022 17:16:27 - INFO - __main__ -   Batch Number = 37
01/06/2022 17:16:27 - INFO - __main__ -   Batch Number = 38
01/06/2022 17:16:27 - INFO - __main__ -   Batch Number = 39
01/06/2022 17:16:27 - INFO - __main__ -   Batch Number = 40
01/06/2022 17:16:27 - INFO - __main__ -   Batch Number = 41
Evaluating:  25%|██▍       | 41/166 [00:01<00:02, 43.45it/s]01/06/2022 17:16:27 - INFO - __main__ -   Batch Number = 42
01/06/2022 17:16:27 - INFO - __main__ -   Batch Number = 43
01/06/2022 17:16:27 - INFO - __main__ -   Batch Number = 44
01/06/2022 17:16:27 - INFO - __main__ -   Batch Number = 45
01/06/2022 17:16:27 - INFO - __main__ -   Batch Number = 46
Evaluating:  28%|██▊       | 46/166 [00:01<00:02, 43.83it/s]01/06/2022 17:16:27 - INFO - __main__ -   Batch Number = 47
01/06/2022 17:16:27 - INFO - __main__ -   Batch Number = 48
01/06/2022 17:16:27 - INFO - __main__ -   Batch Number = 49
01/06/2022 17:16:27 - INFO - __main__ -   Batch Number = 50
01/06/2022 17:16:27 - INFO - __main__ -   Batch Number = 51
Evaluating:  31%|███       | 51/166 [00:01<00:02, 43.99it/s]01/06/2022 17:16:27 - INFO - __main__ -   Batch Number = 52
01/06/2022 17:16:27 - INFO - __main__ -   Batch Number = 53
01/06/2022 17:16:27 - INFO - __main__ -   Batch Number = 54
01/06/2022 17:16:27 - INFO - __main__ -   Batch Number = 55
01/06/2022 17:16:27 - INFO - __main__ -   Batch Number = 56
Evaluating:  34%|███▎      | 56/166 [00:01<00:02, 44.11it/s]01/06/2022 17:16:27 - INFO - __main__ -   Batch Number = 57
01/06/2022 17:16:27 - INFO - __main__ -   Batch Number = 58
01/06/2022 17:16:27 - INFO - __main__ -   Batch Number = 59
01/06/2022 17:16:27 - INFO - __main__ -   Batch Number = 60
01/06/2022 17:16:27 - INFO - __main__ -   Batch Number = 61
Evaluating:  37%|███▋      | 61/166 [00:01<00:02, 44.34it/s]01/06/2022 17:16:27 - INFO - __main__ -   Batch Number = 62
01/06/2022 17:16:27 - INFO - __main__ -   Batch Number = 63
01/06/2022 17:16:27 - INFO - __main__ -   Batch Number = 64
01/06/2022 17:16:27 - INFO - __main__ -   Batch Number = 65
01/06/2022 17:16:27 - INFO - __main__ -   Batch Number = 66
Evaluating:  40%|███▉      | 66/166 [00:01<00:02, 44.17it/s]01/06/2022 17:16:27 - INFO - __main__ -   Batch Number = 67
01/06/2022 17:16:27 - INFO - __main__ -   Batch Number = 68
01/06/2022 17:16:27 - INFO - __main__ -   Batch Number = 69
01/06/2022 17:16:27 - INFO - __main__ -   Batch Number = 70
01/06/2022 17:16:27 - INFO - __main__ -   Batch Number = 71
Evaluating:  43%|████▎     | 71/166 [00:01<00:02, 44.36it/s]01/06/2022 17:16:27 - INFO - __main__ -   Batch Number = 72
01/06/2022 17:16:27 - INFO - __main__ -   Batch Number = 73
01/06/2022 17:16:27 - INFO - __main__ -   Batch Number = 74
01/06/2022 17:16:27 - INFO - __main__ -   Batch Number = 75
01/06/2022 17:16:27 - INFO - __main__ -   Batch Number = 76
Evaluating:  46%|████▌     | 76/166 [00:01<00:02, 44.43it/s]01/06/2022 17:16:27 - INFO - __main__ -   Batch Number = 77
01/06/2022 17:16:28 - INFO - __main__ -   Batch Number = 78
01/06/2022 17:16:28 - INFO - __main__ -   Batch Number = 79
01/06/2022 17:16:28 - INFO - __main__ -   Batch Number = 80
01/06/2022 17:16:28 - INFO - __main__ -   Batch Number = 81
Evaluating:  49%|████▉     | 81/166 [00:01<00:01, 44.32it/s]01/06/2022 17:16:28 - INFO - __main__ -   Batch Number = 82
01/06/2022 17:16:28 - INFO - __main__ -   Batch Number = 83
01/06/2022 17:16:28 - INFO - __main__ -   Batch Number = 84
01/06/2022 17:16:28 - INFO - __main__ -   Batch Number = 85
01/06/2022 17:16:28 - INFO - __main__ -   Batch Number = 86
Evaluating:  52%|█████▏    | 86/166 [00:02<00:01, 44.41it/s]01/06/2022 17:16:28 - INFO - __main__ -   Batch Number = 87
01/06/2022 17:16:28 - INFO - __main__ -   Batch Number = 88
01/06/2022 17:16:28 - INFO - __main__ -   Batch Number = 89
01/06/2022 17:16:28 - INFO - __main__ -   Batch Number = 90
01/06/2022 17:16:28 - INFO - __main__ -   Batch Number = 91
Evaluating:  55%|█████▍    | 91/166 [00:02<00:01, 44.53it/s]01/06/2022 17:16:28 - INFO - __main__ -   Batch Number = 92
01/06/2022 17:16:28 - INFO - __main__ -   Batch Number = 93
01/06/2022 17:16:28 - INFO - __main__ -   Batch Number = 94
01/06/2022 17:16:28 - INFO - __main__ -   Batch Number = 95
01/06/2022 17:16:28 - INFO - __main__ -   Batch Number = 96
Evaluating:  58%|█████▊    | 96/166 [00:02<00:01, 44.44it/s]01/06/2022 17:16:28 - INFO - __main__ -   Batch Number = 97
01/06/2022 17:16:28 - INFO - __main__ -   Batch Number = 98
01/06/2022 17:16:28 - INFO - __main__ -   Batch Number = 99
01/06/2022 17:16:28 - INFO - __main__ -   Batch Number = 100
01/06/2022 17:16:28 - INFO - __main__ -   Batch Number = 101
Evaluating:  61%|██████    | 101/166 [00:02<00:01, 44.44it/s]01/06/2022 17:16:28 - INFO - __main__ -   Batch Number = 102
01/06/2022 17:16:28 - INFO - __main__ -   Batch Number = 103
01/06/2022 17:16:28 - INFO - __main__ -   Batch Number = 104
01/06/2022 17:16:28 - INFO - __main__ -   Batch Number = 105
01/06/2022 17:16:28 - INFO - __main__ -   Batch Number = 106
Evaluating:  64%|██████▍   | 106/166 [00:02<00:01, 44.57it/s]01/06/2022 17:16:28 - INFO - __main__ -   Batch Number = 107
01/06/2022 17:16:28 - INFO - __main__ -   Batch Number = 108
01/06/2022 17:16:28 - INFO - __main__ -   Batch Number = 109
01/06/2022 17:16:28 - INFO - __main__ -   Batch Number = 110
01/06/2022 17:16:28 - INFO - __main__ -   Batch Number = 111
Evaluating:  67%|██████▋   | 111/166 [00:02<00:01, 44.37it/s]01/06/2022 17:16:28 - INFO - __main__ -   Batch Number = 112
01/06/2022 17:16:28 - INFO - __main__ -   Batch Number = 113
01/06/2022 17:16:28 - INFO - __main__ -   Batch Number = 114
01/06/2022 17:16:28 - INFO - __main__ -   Batch Number = 115
01/06/2022 17:16:28 - INFO - __main__ -   Batch Number = 116
Evaluating:  70%|██████▉   | 116/166 [00:02<00:01, 44.46it/s]01/06/2022 17:16:28 - INFO - __main__ -   Batch Number = 117
01/06/2022 17:16:28 - INFO - __main__ -   Batch Number = 118
01/06/2022 17:16:28 - INFO - __main__ -   Batch Number = 119
01/06/2022 17:16:28 - INFO - __main__ -   Batch Number = 120
01/06/2022 17:16:28 - INFO - __main__ -   Batch Number = 121
Evaluating:  73%|███████▎  | 121/166 [00:02<00:01, 44.45it/s]01/06/2022 17:16:28 - INFO - __main__ -   Batch Number = 122
01/06/2022 17:16:29 - INFO - __main__ -   Batch Number = 123
01/06/2022 17:16:29 - INFO - __main__ -   Batch Number = 124
01/06/2022 17:16:29 - INFO - __main__ -   Batch Number = 125
01/06/2022 17:16:29 - INFO - __main__ -   Batch Number = 126
Evaluating:  76%|███████▌  | 126/166 [00:02<00:00, 42.97it/s]01/06/2022 17:16:29 - INFO - __main__ -   Batch Number = 127
01/06/2022 17:16:29 - INFO - __main__ -   Batch Number = 128
01/06/2022 17:16:29 - INFO - __main__ -   Batch Number = 129
01/06/2022 17:16:29 - INFO - __main__ -   Batch Number = 130
01/06/2022 17:16:29 - INFO - __main__ -   Batch Number = 131
Evaluating:  79%|███████▉  | 131/166 [00:03<00:00, 43.34it/s]01/06/2022 17:16:29 - INFO - __main__ -   Batch Number = 132
01/06/2022 17:16:29 - INFO - __main__ -   Batch Number = 133
01/06/2022 17:16:29 - INFO - __main__ -   Batch Number = 134
01/06/2022 17:16:29 - INFO - __main__ -   Batch Number = 135
01/06/2022 17:16:29 - INFO - __main__ -   Batch Number = 136
Evaluating:  82%|████████▏ | 136/166 [00:03<00:00, 43.77it/s]01/06/2022 17:16:29 - INFO - __main__ -   Batch Number = 137
01/06/2022 17:16:29 - INFO - __main__ -   Batch Number = 138
01/06/2022 17:16:29 - INFO - __main__ -   Batch Number = 139
01/06/2022 17:16:29 - INFO - __main__ -   Batch Number = 140
01/06/2022 17:16:29 - INFO - __main__ -   Batch Number = 141
Evaluating:  85%|████████▍ | 141/166 [00:03<00:00, 43.86it/s]01/06/2022 17:16:29 - INFO - __main__ -   Batch Number = 142
01/06/2022 17:16:29 - INFO - __main__ -   Batch Number = 143
01/06/2022 17:16:29 - INFO - __main__ -   Batch Number = 144
01/06/2022 17:16:29 - INFO - __main__ -   Batch Number = 145
01/06/2022 17:16:29 - INFO - __main__ -   Batch Number = 146
Evaluating:  88%|████████▊ | 146/166 [00:03<00:00, 44.12it/s]01/06/2022 17:16:29 - INFO - __main__ -   Batch Number = 147
01/06/2022 17:16:29 - INFO - __main__ -   Batch Number = 148
01/06/2022 17:16:29 - INFO - __main__ -   Batch Number = 149
01/06/2022 17:16:29 - INFO - __main__ -   Batch Number = 150
01/06/2022 17:16:29 - INFO - __main__ -   Batch Number = 151
Evaluating:  91%|█████████ | 151/166 [00:03<00:00, 44.16it/s]01/06/2022 17:16:29 - INFO - __main__ -   Batch Number = 152
01/06/2022 17:16:29 - INFO - __main__ -   Batch Number = 153
01/06/2022 17:16:29 - INFO - __main__ -   Batch Number = 154
01/06/2022 17:16:29 - INFO - __main__ -   Batch Number = 155
01/06/2022 17:16:29 - INFO - __main__ -   Batch Number = 156
Evaluating:  94%|█████████▍| 156/166 [00:03<00:00, 44.19it/s]01/06/2022 17:16:29 - INFO - __main__ -   Batch Number = 157
01/06/2022 17:16:29 - INFO - __main__ -   Batch Number = 158
01/06/2022 17:16:29 - INFO - __main__ -   Batch Number = 159
01/06/2022 17:16:29 - INFO - __main__ -   Batch Number = 160
01/06/2022 17:16:29 - INFO - __main__ -   Batch Number = 161
Evaluating:  97%|█████████▋| 161/166 [00:03<00:00, 44.31it/s]01/06/2022 17:16:29 - INFO - __main__ -   Batch Number = 162
01/06/2022 17:16:29 - INFO - __main__ -   Batch Number = 163
01/06/2022 17:16:29 - INFO - __main__ -   Batch Number = 164
01/06/2022 17:16:29 - INFO - __main__ -   Batch Number = 165
01/06/2022 17:16:30 - INFO - __main__ -   Batch Number = 166
Evaluating: 100%|██████████| 166/166 [00:03<00:00, 44.86it/s]Evaluating: 100%|██████████| 166/166 [00:03<00:00, 43.28it/s]
01/06/2022 17:16:30 - INFO - __main__ -     Evaluation done in total 3.835653 secs (0.002895 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/predictions_vi_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/nbest_predictions_vi_.json
01/06/2022 17:16:33 - INFO - __main__ -   Results = OrderedDict([('exact', 50.08403361344538), ('f1', 68.8913627156456), ('total', 1190), ('HasAns_exact', 50.08403361344538), ('HasAns_f1', 68.8913627156456), ('HasAns_total', 1190), ('best_exact', 50.08403361344538), ('best_exact_thresh', 0.0), ('best_f1', 68.8913627156456), ('best_f1_thresh', 0.0)])
01/06/2022 17:16:33 - INFO - __main__ -   Language adapter for zh not found, using en instead
01/06/2022 17:16:33 - INFO - __main__ -   Set active language adapter to en
01/06/2022 17:16:33 - INFO - __main__ -   Args Adapter Weight = None
01/06/2022 17:16:33 - INFO - __main__ -   Adapter Languages = ['en']
01/06/2022 17:16:33 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
  0%|          | 0/48 [00:00<?, ?it/s] 75%|███████▌  | 36/48 [00:00<00:00, 350.70it/s]100%|██████████| 48/48 [00:00<00:00, 371.36it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<03:50,  5.16it/s]convert squad examples to features:  32%|███▏      | 385/1190 [00:00<00:01, 727.34it/s]convert squad examples to features: 100%|██████████| 1190/1190 [00:00<00:00, 2007.05it/s]/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|██████████| 1190/1190 [00:00<00:00, 731850.70it/s]
01/06/2022 17:16:34 - INFO - __main__ -   Local Rank = -1
01/06/2022 17:16:34 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.zh.json_bert-base-multilingual-cased_384_zh
01/06/2022 17:16:35 - INFO - __main__ -   ***** Running evaluation  *****
01/06/2022 17:16:35 - INFO - __main__ -     Num examples = 1374
01/06/2022 17:16:35 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/172 [00:00<?, ?it/s]01/06/2022 17:16:35 - INFO - __main__ -   Batch Number = 1
01/06/2022 17:16:36 - INFO - __main__ -   Batch Number = 2
01/06/2022 17:16:36 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|▏         | 3/172 [00:00<00:05, 29.66it/s]01/06/2022 17:16:36 - INFO - __main__ -   Batch Number = 4
01/06/2022 17:16:36 - INFO - __main__ -   Batch Number = 5
01/06/2022 17:16:36 - INFO - __main__ -   Batch Number = 6
01/06/2022 17:16:36 - INFO - __main__ -   Batch Number = 7
Evaluating:   4%|▍         | 7/172 [00:00<00:04, 33.95it/s]01/06/2022 17:16:36 - INFO - __main__ -   Batch Number = 8
01/06/2022 17:16:36 - INFO - __main__ -   Batch Number = 9
01/06/2022 17:16:36 - INFO - __main__ -   Batch Number = 10
01/06/2022 17:16:36 - INFO - __main__ -   Batch Number = 11
01/06/2022 17:16:36 - INFO - __main__ -   Batch Number = 12
Evaluating:   7%|▋         | 12/172 [00:00<00:04, 37.92it/s]01/06/2022 17:16:36 - INFO - __main__ -   Batch Number = 13
01/06/2022 17:16:36 - INFO - __main__ -   Batch Number = 14
01/06/2022 17:16:36 - INFO - __main__ -   Batch Number = 15
01/06/2022 17:16:36 - INFO - __main__ -   Batch Number = 16
01/06/2022 17:16:36 - INFO - __main__ -   Batch Number = 17
Evaluating:  10%|▉         | 17/172 [00:00<00:03, 40.27it/s]01/06/2022 17:16:36 - INFO - __main__ -   Batch Number = 18
01/06/2022 17:16:36 - INFO - __main__ -   Batch Number = 19
01/06/2022 17:16:36 - INFO - __main__ -   Batch Number = 20
01/06/2022 17:16:36 - INFO - __main__ -   Batch Number = 21
01/06/2022 17:16:36 - INFO - __main__ -   Batch Number = 22
Evaluating:  13%|█▎        | 22/172 [00:00<00:03, 41.58it/s]01/06/2022 17:16:36 - INFO - __main__ -   Batch Number = 23
01/06/2022 17:16:36 - INFO - __main__ -   Batch Number = 24
01/06/2022 17:16:36 - INFO - __main__ -   Batch Number = 25
01/06/2022 17:16:36 - INFO - __main__ -   Batch Number = 26
01/06/2022 17:16:36 - INFO - __main__ -   Batch Number = 27
Evaluating:  16%|█▌        | 27/172 [00:00<00:03, 42.66it/s]01/06/2022 17:16:36 - INFO - __main__ -   Batch Number = 28
01/06/2022 17:16:36 - INFO - __main__ -   Batch Number = 29
01/06/2022 17:16:36 - INFO - __main__ -   Batch Number = 30
01/06/2022 17:16:36 - INFO - __main__ -   Batch Number = 31
01/06/2022 17:16:36 - INFO - __main__ -   Batch Number = 32
Evaluating:  19%|█▊        | 32/172 [00:00<00:03, 43.34it/s]01/06/2022 17:16:36 - INFO - __main__ -   Batch Number = 33
01/06/2022 17:16:36 - INFO - __main__ -   Batch Number = 34
01/06/2022 17:16:36 - INFO - __main__ -   Batch Number = 35
01/06/2022 17:16:36 - INFO - __main__ -   Batch Number = 36
01/06/2022 17:16:36 - INFO - __main__ -   Batch Number = 37
Evaluating:  22%|██▏       | 37/172 [00:00<00:03, 43.73it/s]01/06/2022 17:16:36 - INFO - __main__ -   Batch Number = 38
01/06/2022 17:16:36 - INFO - __main__ -   Batch Number = 39
01/06/2022 17:16:36 - INFO - __main__ -   Batch Number = 40
01/06/2022 17:16:36 - INFO - __main__ -   Batch Number = 41
01/06/2022 17:16:36 - INFO - __main__ -   Batch Number = 42
Evaluating:  24%|██▍       | 42/172 [00:01<00:02, 43.56it/s]01/06/2022 17:16:36 - INFO - __main__ -   Batch Number = 43
01/06/2022 17:16:37 - INFO - __main__ -   Batch Number = 44
01/06/2022 17:16:37 - INFO - __main__ -   Batch Number = 45
01/06/2022 17:16:37 - INFO - __main__ -   Batch Number = 46
01/06/2022 17:16:37 - INFO - __main__ -   Batch Number = 47
Evaluating:  27%|██▋       | 47/172 [00:01<00:02, 43.78it/s]01/06/2022 17:16:37 - INFO - __main__ -   Batch Number = 48
01/06/2022 17:16:37 - INFO - __main__ -   Batch Number = 49
01/06/2022 17:16:37 - INFO - __main__ -   Batch Number = 50
01/06/2022 17:16:37 - INFO - __main__ -   Batch Number = 51
01/06/2022 17:16:37 - INFO - __main__ -   Batch Number = 52
Evaluating:  30%|███       | 52/172 [00:01<00:02, 43.74it/s]01/06/2022 17:16:37 - INFO - __main__ -   Batch Number = 53
01/06/2022 17:16:37 - INFO - __main__ -   Batch Number = 54
01/06/2022 17:16:37 - INFO - __main__ -   Batch Number = 55
01/06/2022 17:16:37 - INFO - __main__ -   Batch Number = 56
01/06/2022 17:16:37 - INFO - __main__ -   Batch Number = 57
Evaluating:  33%|███▎      | 57/172 [00:01<00:02, 43.95it/s]01/06/2022 17:16:37 - INFO - __main__ -   Batch Number = 58
01/06/2022 17:16:37 - INFO - __main__ -   Batch Number = 59
01/06/2022 17:16:37 - INFO - __main__ -   Batch Number = 60
01/06/2022 17:16:37 - INFO - __main__ -   Batch Number = 61
01/06/2022 17:16:37 - INFO - __main__ -   Batch Number = 62
Evaluating:  36%|███▌      | 62/172 [00:01<00:02, 44.21it/s]01/06/2022 17:16:37 - INFO - __main__ -   Batch Number = 63
01/06/2022 17:16:37 - INFO - __main__ -   Batch Number = 64
01/06/2022 17:16:37 - INFO - __main__ -   Batch Number = 65
01/06/2022 17:16:37 - INFO - __main__ -   Batch Number = 66
01/06/2022 17:16:37 - INFO - __main__ -   Batch Number = 67
Evaluating:  39%|███▉      | 67/172 [00:01<00:02, 42.89it/s]01/06/2022 17:16:37 - INFO - __main__ -   Batch Number = 68
01/06/2022 17:16:37 - INFO - __main__ -   Batch Number = 69
01/06/2022 17:16:37 - INFO - __main__ -   Batch Number = 70
01/06/2022 17:16:37 - INFO - __main__ -   Batch Number = 71
01/06/2022 17:16:37 - INFO - __main__ -   Batch Number = 72
Evaluating:  42%|████▏     | 72/172 [00:01<00:02, 43.42it/s]01/06/2022 17:16:37 - INFO - __main__ -   Batch Number = 73
01/06/2022 17:16:37 - INFO - __main__ -   Batch Number = 74
01/06/2022 17:16:37 - INFO - __main__ -   Batch Number = 75
01/06/2022 17:16:37 - INFO - __main__ -   Batch Number = 76
01/06/2022 17:16:37 - INFO - __main__ -   Batch Number = 77
Evaluating:  45%|████▍     | 77/172 [00:01<00:02, 43.88it/s]01/06/2022 17:16:37 - INFO - __main__ -   Batch Number = 78
01/06/2022 17:16:37 - INFO - __main__ -   Batch Number = 79
01/06/2022 17:16:37 - INFO - __main__ -   Batch Number = 80
01/06/2022 17:16:37 - INFO - __main__ -   Batch Number = 81
01/06/2022 17:16:37 - INFO - __main__ -   Batch Number = 82
Evaluating:  48%|████▊     | 82/172 [00:01<00:02, 43.96it/s]01/06/2022 17:16:37 - INFO - __main__ -   Batch Number = 83
01/06/2022 17:16:37 - INFO - __main__ -   Batch Number = 84
01/06/2022 17:16:37 - INFO - __main__ -   Batch Number = 85
01/06/2022 17:16:37 - INFO - __main__ -   Batch Number = 86
01/06/2022 17:16:37 - INFO - __main__ -   Batch Number = 87
Evaluating:  51%|█████     | 87/172 [00:02<00:01, 44.19it/s]01/06/2022 17:16:38 - INFO - __main__ -   Batch Number = 88
01/06/2022 17:16:38 - INFO - __main__ -   Batch Number = 89
01/06/2022 17:16:38 - INFO - __main__ -   Batch Number = 90
01/06/2022 17:16:38 - INFO - __main__ -   Batch Number = 91
01/06/2022 17:16:38 - INFO - __main__ -   Batch Number = 92
Evaluating:  53%|█████▎    | 92/172 [00:02<00:01, 44.10it/s]01/06/2022 17:16:38 - INFO - __main__ -   Batch Number = 93
01/06/2022 17:16:38 - INFO - __main__ -   Batch Number = 94
01/06/2022 17:16:38 - INFO - __main__ -   Batch Number = 95
01/06/2022 17:16:38 - INFO - __main__ -   Batch Number = 96
01/06/2022 17:16:38 - INFO - __main__ -   Batch Number = 97
Evaluating:  56%|█████▋    | 97/172 [00:02<00:01, 43.99it/s]01/06/2022 17:16:38 - INFO - __main__ -   Batch Number = 98
01/06/2022 17:16:38 - INFO - __main__ -   Batch Number = 99
01/06/2022 17:16:38 - INFO - __main__ -   Batch Number = 100
01/06/2022 17:16:38 - INFO - __main__ -   Batch Number = 101
01/06/2022 17:16:38 - INFO - __main__ -   Batch Number = 102
Evaluating:  59%|█████▉    | 102/172 [00:02<00:01, 44.25it/s]01/06/2022 17:16:38 - INFO - __main__ -   Batch Number = 103
01/06/2022 17:16:38 - INFO - __main__ -   Batch Number = 104
01/06/2022 17:16:38 - INFO - __main__ -   Batch Number = 105
01/06/2022 17:16:38 - INFO - __main__ -   Batch Number = 106
01/06/2022 17:16:38 - INFO - __main__ -   Batch Number = 107
Evaluating:  62%|██████▏   | 107/172 [00:02<00:01, 44.34it/s]01/06/2022 17:16:38 - INFO - __main__ -   Batch Number = 108
01/06/2022 17:16:38 - INFO - __main__ -   Batch Number = 109
01/06/2022 17:16:38 - INFO - __main__ -   Batch Number = 110
01/06/2022 17:16:38 - INFO - __main__ -   Batch Number = 111
01/06/2022 17:16:38 - INFO - __main__ -   Batch Number = 112
Evaluating:  65%|██████▌   | 112/172 [00:02<00:01, 44.41it/s]01/06/2022 17:16:38 - INFO - __main__ -   Batch Number = 113
01/06/2022 17:16:38 - INFO - __main__ -   Batch Number = 114
01/06/2022 17:16:38 - INFO - __main__ -   Batch Number = 115
01/06/2022 17:16:38 - INFO - __main__ -   Batch Number = 116
01/06/2022 17:16:38 - INFO - __main__ -   Batch Number = 117
Evaluating:  68%|██████▊   | 117/172 [00:02<00:01, 44.36it/s]01/06/2022 17:16:38 - INFO - __main__ -   Batch Number = 118
01/06/2022 17:16:38 - INFO - __main__ -   Batch Number = 119
01/06/2022 17:16:38 - INFO - __main__ -   Batch Number = 120
01/06/2022 17:16:38 - INFO - __main__ -   Batch Number = 121
01/06/2022 17:16:38 - INFO - __main__ -   Batch Number = 122
Evaluating:  71%|███████   | 122/172 [00:02<00:01, 44.47it/s]01/06/2022 17:16:38 - INFO - __main__ -   Batch Number = 123
01/06/2022 17:16:38 - INFO - __main__ -   Batch Number = 124
01/06/2022 17:16:38 - INFO - __main__ -   Batch Number = 125
01/06/2022 17:16:38 - INFO - __main__ -   Batch Number = 126
01/06/2022 17:16:38 - INFO - __main__ -   Batch Number = 127
Evaluating:  74%|███████▍  | 127/172 [00:02<00:01, 44.41it/s]01/06/2022 17:16:38 - INFO - __main__ -   Batch Number = 128
01/06/2022 17:16:38 - INFO - __main__ -   Batch Number = 129
01/06/2022 17:16:38 - INFO - __main__ -   Batch Number = 130
01/06/2022 17:16:38 - INFO - __main__ -   Batch Number = 131
01/06/2022 17:16:39 - INFO - __main__ -   Batch Number = 132
Evaluating:  77%|███████▋  | 132/172 [00:03<00:00, 44.41it/s]01/06/2022 17:16:39 - INFO - __main__ -   Batch Number = 133
01/06/2022 17:16:39 - INFO - __main__ -   Batch Number = 134
01/06/2022 17:16:39 - INFO - __main__ -   Batch Number = 135
01/06/2022 17:16:39 - INFO - __main__ -   Batch Number = 136
01/06/2022 17:16:39 - INFO - __main__ -   Batch Number = 137
Evaluating:  80%|███████▉  | 137/172 [00:03<00:00, 44.30it/s]01/06/2022 17:16:39 - INFO - __main__ -   Batch Number = 138
01/06/2022 17:16:39 - INFO - __main__ -   Batch Number = 139
01/06/2022 17:16:39 - INFO - __main__ -   Batch Number = 140
01/06/2022 17:16:39 - INFO - __main__ -   Batch Number = 141
01/06/2022 17:16:39 - INFO - __main__ -   Batch Number = 142
Evaluating:  83%|████████▎ | 142/172 [00:03<00:00, 44.09it/s]01/06/2022 17:16:39 - INFO - __main__ -   Batch Number = 143
01/06/2022 17:16:39 - INFO - __main__ -   Batch Number = 144
01/06/2022 17:16:39 - INFO - __main__ -   Batch Number = 145
01/06/2022 17:16:39 - INFO - __main__ -   Batch Number = 146
01/06/2022 17:16:39 - INFO - __main__ -   Batch Number = 147
Evaluating:  85%|████████▌ | 147/172 [00:03<00:00, 44.22it/s]01/06/2022 17:16:39 - INFO - __main__ -   Batch Number = 148
01/06/2022 17:16:39 - INFO - __main__ -   Batch Number = 149
01/06/2022 17:16:39 - INFO - __main__ -   Batch Number = 150
01/06/2022 17:16:39 - INFO - __main__ -   Batch Number = 151
01/06/2022 17:16:39 - INFO - __main__ -   Batch Number = 152
Evaluating:  88%|████████▊ | 152/172 [00:03<00:00, 44.32it/s]01/06/2022 17:16:39 - INFO - __main__ -   Batch Number = 153
01/06/2022 17:16:39 - INFO - __main__ -   Batch Number = 154
01/06/2022 17:16:39 - INFO - __main__ -   Batch Number = 155
01/06/2022 17:16:39 - INFO - __main__ -   Batch Number = 156
01/06/2022 17:16:39 - INFO - __main__ -   Batch Number = 157
Evaluating:  91%|█████████▏| 157/172 [00:03<00:00, 44.26it/s]01/06/2022 17:16:39 - INFO - __main__ -   Batch Number = 158
01/06/2022 17:16:39 - INFO - __main__ -   Batch Number = 159
01/06/2022 17:16:39 - INFO - __main__ -   Batch Number = 160
01/06/2022 17:16:39 - INFO - __main__ -   Batch Number = 161
01/06/2022 17:16:39 - INFO - __main__ -   Batch Number = 162
Evaluating:  94%|█████████▍| 162/172 [00:03<00:00, 44.46it/s]01/06/2022 17:16:39 - INFO - __main__ -   Batch Number = 163
01/06/2022 17:16:39 - INFO - __main__ -   Batch Number = 164
01/06/2022 17:16:39 - INFO - __main__ -   Batch Number = 165
01/06/2022 17:16:39 - INFO - __main__ -   Batch Number = 166
01/06/2022 17:16:39 - INFO - __main__ -   Batch Number = 167
Evaluating:  97%|█████████▋| 167/172 [00:03<00:00, 44.47it/s]01/06/2022 17:16:39 - INFO - __main__ -   Batch Number = 168
01/06/2022 17:16:39 - INFO - __main__ -   Batch Number = 169
01/06/2022 17:16:39 - INFO - __main__ -   Batch Number = 170
01/06/2022 17:16:39 - INFO - __main__ -   Batch Number = 171
01/06/2022 17:16:39 - INFO - __main__ -   Batch Number = 172
Evaluating: 100%|██████████| 172/172 [00:03<00:00, 44.99it/s]Evaluating: 100%|██████████| 172/172 [00:03<00:00, 43.60it/s]
01/06/2022 17:16:39 - INFO - __main__ -     Evaluation done in total 3.945216 secs (0.002871 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/predictions_zh_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/nbest_predictions_zh_.json
01/06/2022 17:16:45 - INFO - __main__ -   Results = OrderedDict([('exact', 47.05882352941177), ('f1', 56.50813658796847), ('total', 1190), ('HasAns_exact', 47.05882352941177), ('HasAns_f1', 56.50813658796847), ('HasAns_total', 1190), ('best_exact', 47.05882352941177), ('best_exact_thresh', 0.0), ('best_f1', 56.50813658796847), ('best_f1_thresh', 0.0)])
PyTorch version 1.10.0+cu111 available.
01/06/2022 17:16:48 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/06/2022 17:16:48 - INFO - root -   save model
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /root/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/06/2022 17:16:54 - INFO - __main__ -   lang2id = None
01/06/2022 17:16:57 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, data_dir='/root/Desktop/cloud-emea-copy/data//xquad', output_dir='/root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8', max_seq_length=384, train_file='/root/Desktop/cloud-emea-copy/data//xquad/train-v1.1.json', predict_file='/root/Desktop/cloud-emea-copy/data//xquad/dev-v1.1.json', eval_test_set=False, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=4, per_gpu_eval_batch_size=8, gradient_accumulation_steps=8, learning_rate=0.0003, weight_decay=0.0001, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=15.0, max_steps=-1, save_steps=1000, warmup_steps=500, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', eval_lang='en', predict_langs='ar,de,el,es,hi,ru,th,tr,vi,zh', train_lang='en', log_file='/root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='qna', threads=8, version_2_with_negative=False, verbose_logging=False, n_best_size=20, max_query_length=64, max_answer_length=30, doc_stride=128, null_score_diff_threshold=0.0, predict_task_adapter='output/squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8_s2/checkpoint-best/qna', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix=None, en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/06/2022 17:16:57 - INFO - __main__ -   Results: {}
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /root/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/06/2022 17:17:04 - INFO - __main__ -   lang2id = None
01/06/2022 17:17:04 - INFO - __main__ -   Evaluating the model on the test set of all languages specified
01/06/2022 17:17:04 - INFO - __main__ -   Adapter will be loaded from this path: output/squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8_s2/checkpoint-best/qna
01/06/2022 17:17:04 - INFO - root -   Trying to decide if add adapter
01/06/2022 17:17:04 - INFO - root -   loading task adapter
Loading module configuration from output/squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8_s2/checkpoint-best/qna/adapter_config.json
Adding adapter 'qna' of type 'text_task'.
Loading module weights from output/squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8_s2/checkpoint-best/qna/pytorch_adapter.bin
Loading module configuration from output/squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8_s2/checkpoint-best/qna/head_config.json
Loading module weights from output/squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8_s2/checkpoint-best/qna/pytorch_model_head.bin
01/06/2022 17:17:04 - INFO - root -   loading lang adpater en/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip.
Loading module configuration from /root/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
Loading module weights from /root/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/pytorch_adapter.bin
No matching prediction head found in '/root/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted'
01/06/2022 17:17:05 - INFO - __main__ -   Language adapter for ar not found, using en instead
01/06/2022 17:17:05 - INFO - __main__ -   Set active language adapter to en
01/06/2022 17:17:05 - INFO - __main__ -   Args Adapter Weight = None
01/06/2022 17:17:05 - INFO - __main__ -   Adapter Languages = ['en']
01/06/2022 17:17:05 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
en en/wiki@ukp
  0%|          | 0/48 [00:00<?, ?it/s] 33%|███▎      | 16/48 [00:00<00:00, 140.88it/s] 65%|██████▍   | 31/48 [00:00<00:00, 135.70it/s] 94%|█████████▍| 45/48 [00:00<00:00, 109.86it/s]100%|██████████| 48/48 [00:00<00:00, 119.71it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<07:56,  2.49it/s]convert squad examples to features:  32%|███▏      | 385/1190 [00:00<00:01, 449.75it/s]convert squad examples to features:  89%|████████▉ | 1057/1190 [00:01<00:00, 1269.67it/s]convert squad examples to features: 100%|██████████| 1190/1190 [00:01<00:00, 952.04it/s] /root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|██████████| 1190/1190 [00:00<00:00, 440687.07it/s]
01/06/2022 17:17:07 - INFO - __main__ -   Local Rank = -1
01/06/2022 17:17:07 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.ar.json_bert-base-multilingual-cased_384_ar
01/06/2022 17:17:09 - INFO - __main__ -   ***** Running evaluation  *****
01/06/2022 17:17:09 - INFO - __main__ -     Num examples = 1455
01/06/2022 17:17:09 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/182 [00:00<?, ?it/s]01/06/2022 17:17:09 - INFO - __main__ -   Batch Number = 1
01/06/2022 17:17:09 - INFO - __main__ -   Batch Number = 2
01/06/2022 17:17:09 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|▏         | 3/182 [00:00<00:06, 27.18it/s]01/06/2022 17:17:09 - INFO - __main__ -   Batch Number = 4
01/06/2022 17:17:09 - INFO - __main__ -   Batch Number = 5
01/06/2022 17:17:09 - INFO - __main__ -   Batch Number = 6
01/06/2022 17:17:09 - INFO - __main__ -   Batch Number = 7
Evaluating:   4%|▍         | 7/182 [00:00<00:05, 33.14it/s]01/06/2022 17:17:09 - INFO - __main__ -   Batch Number = 8
01/06/2022 17:17:09 - INFO - __main__ -   Batch Number = 9
01/06/2022 17:17:09 - INFO - __main__ -   Batch Number = 10
01/06/2022 17:17:09 - INFO - __main__ -   Batch Number = 11
01/06/2022 17:17:09 - INFO - __main__ -   Batch Number = 12
Evaluating:   7%|▋         | 12/182 [00:00<00:04, 37.20it/s]01/06/2022 17:17:09 - INFO - __main__ -   Batch Number = 13
01/06/2022 17:17:09 - INFO - __main__ -   Batch Number = 14
01/06/2022 17:17:09 - INFO - __main__ -   Batch Number = 15
01/06/2022 17:17:09 - INFO - __main__ -   Batch Number = 16
Evaluating:   9%|▉         | 16/182 [00:00<00:04, 37.81it/s]01/06/2022 17:17:09 - INFO - __main__ -   Batch Number = 17
01/06/2022 17:17:09 - INFO - __main__ -   Batch Number = 18
01/06/2022 17:17:09 - INFO - __main__ -   Batch Number = 19
01/06/2022 17:17:09 - INFO - __main__ -   Batch Number = 20
01/06/2022 17:17:09 - INFO - __main__ -   Batch Number = 21
Evaluating:  12%|█▏        | 21/182 [00:00<00:04, 40.04it/s]01/06/2022 17:17:09 - INFO - __main__ -   Batch Number = 22
01/06/2022 17:17:09 - INFO - __main__ -   Batch Number = 23
01/06/2022 17:17:09 - INFO - __main__ -   Batch Number = 24
01/06/2022 17:17:09 - INFO - __main__ -   Batch Number = 25
01/06/2022 17:17:09 - INFO - __main__ -   Batch Number = 26
Evaluating:  14%|█▍        | 26/182 [00:00<00:03, 41.82it/s]01/06/2022 17:17:09 - INFO - __main__ -   Batch Number = 27
01/06/2022 17:17:09 - INFO - __main__ -   Batch Number = 28
01/06/2022 17:17:09 - INFO - __main__ -   Batch Number = 29
01/06/2022 17:17:09 - INFO - __main__ -   Batch Number = 30
01/06/2022 17:17:09 - INFO - __main__ -   Batch Number = 31
Evaluating:  17%|█▋        | 31/182 [00:00<00:03, 42.62it/s]01/06/2022 17:17:09 - INFO - __main__ -   Batch Number = 32
01/06/2022 17:17:09 - INFO - __main__ -   Batch Number = 33
01/06/2022 17:17:10 - INFO - __main__ -   Batch Number = 34
01/06/2022 17:17:10 - INFO - __main__ -   Batch Number = 35
01/06/2022 17:17:10 - INFO - __main__ -   Batch Number = 36
Evaluating:  20%|█▉        | 36/182 [00:00<00:03, 43.16it/s]01/06/2022 17:17:10 - INFO - __main__ -   Batch Number = 37
01/06/2022 17:17:10 - INFO - __main__ -   Batch Number = 38
01/06/2022 17:17:10 - INFO - __main__ -   Batch Number = 39
01/06/2022 17:17:10 - INFO - __main__ -   Batch Number = 40
01/06/2022 17:17:10 - INFO - __main__ -   Batch Number = 41
Evaluating:  23%|██▎       | 41/182 [00:01<00:03, 43.62it/s]01/06/2022 17:17:10 - INFO - __main__ -   Batch Number = 42
01/06/2022 17:17:10 - INFO - __main__ -   Batch Number = 43
01/06/2022 17:17:10 - INFO - __main__ -   Batch Number = 44
01/06/2022 17:17:10 - INFO - __main__ -   Batch Number = 45
01/06/2022 17:17:10 - INFO - __main__ -   Batch Number = 46
Evaluating:  25%|██▌       | 46/182 [00:01<00:03, 43.89it/s]01/06/2022 17:17:10 - INFO - __main__ -   Batch Number = 47
01/06/2022 17:17:10 - INFO - __main__ -   Batch Number = 48
01/06/2022 17:17:10 - INFO - __main__ -   Batch Number = 49
01/06/2022 17:17:10 - INFO - __main__ -   Batch Number = 50
01/06/2022 17:17:10 - INFO - __main__ -   Batch Number = 51
Evaluating:  28%|██▊       | 51/182 [00:01<00:02, 44.13it/s]01/06/2022 17:17:10 - INFO - __main__ -   Batch Number = 52
01/06/2022 17:17:10 - INFO - __main__ -   Batch Number = 53
01/06/2022 17:17:10 - INFO - __main__ -   Batch Number = 54
01/06/2022 17:17:10 - INFO - __main__ -   Batch Number = 55
01/06/2022 17:17:10 - INFO - __main__ -   Batch Number = 56
Evaluating:  31%|███       | 56/182 [00:01<00:02, 44.28it/s]01/06/2022 17:17:10 - INFO - __main__ -   Batch Number = 57
01/06/2022 17:17:10 - INFO - __main__ -   Batch Number = 58
01/06/2022 17:17:10 - INFO - __main__ -   Batch Number = 59
01/06/2022 17:17:10 - INFO - __main__ -   Batch Number = 60
01/06/2022 17:17:10 - INFO - __main__ -   Batch Number = 61
Evaluating:  34%|███▎      | 61/182 [00:01<00:02, 44.24it/s]01/06/2022 17:17:10 - INFO - __main__ -   Batch Number = 62
01/06/2022 17:17:10 - INFO - __main__ -   Batch Number = 63
01/06/2022 17:17:10 - INFO - __main__ -   Batch Number = 64
01/06/2022 17:17:10 - INFO - __main__ -   Batch Number = 65
01/06/2022 17:17:10 - INFO - __main__ -   Batch Number = 66
Evaluating:  36%|███▋      | 66/182 [00:01<00:02, 44.27it/s]01/06/2022 17:17:10 - INFO - __main__ -   Batch Number = 67
01/06/2022 17:17:10 - INFO - __main__ -   Batch Number = 68
01/06/2022 17:17:10 - INFO - __main__ -   Batch Number = 69
01/06/2022 17:17:10 - INFO - __main__ -   Batch Number = 70
01/06/2022 17:17:10 - INFO - __main__ -   Batch Number = 71
Evaluating:  39%|███▉      | 71/182 [00:01<00:02, 44.40it/s]01/06/2022 17:17:10 - INFO - __main__ -   Batch Number = 72
01/06/2022 17:17:10 - INFO - __main__ -   Batch Number = 73
01/06/2022 17:17:10 - INFO - __main__ -   Batch Number = 74
01/06/2022 17:17:10 - INFO - __main__ -   Batch Number = 75
01/06/2022 17:17:10 - INFO - __main__ -   Batch Number = 76
Evaluating:  42%|████▏     | 76/182 [00:01<00:02, 44.35it/s]01/06/2022 17:17:10 - INFO - __main__ -   Batch Number = 77
01/06/2022 17:17:10 - INFO - __main__ -   Batch Number = 78
01/06/2022 17:17:11 - INFO - __main__ -   Batch Number = 79
01/06/2022 17:17:11 - INFO - __main__ -   Batch Number = 80
01/06/2022 17:17:11 - INFO - __main__ -   Batch Number = 81
Evaluating:  45%|████▍     | 81/182 [00:01<00:02, 44.41it/s]01/06/2022 17:17:11 - INFO - __main__ -   Batch Number = 82
01/06/2022 17:17:11 - INFO - __main__ -   Batch Number = 83
01/06/2022 17:17:11 - INFO - __main__ -   Batch Number = 84
01/06/2022 17:17:11 - INFO - __main__ -   Batch Number = 85
01/06/2022 17:17:11 - INFO - __main__ -   Batch Number = 86
Evaluating:  47%|████▋     | 86/182 [00:02<00:02, 44.39it/s]01/06/2022 17:17:11 - INFO - __main__ -   Batch Number = 87
01/06/2022 17:17:11 - INFO - __main__ -   Batch Number = 88
01/06/2022 17:17:11 - INFO - __main__ -   Batch Number = 89
01/06/2022 17:17:11 - INFO - __main__ -   Batch Number = 90
01/06/2022 17:17:11 - INFO - __main__ -   Batch Number = 91
Evaluating:  50%|█████     | 91/182 [00:02<00:02, 44.30it/s]01/06/2022 17:17:11 - INFO - __main__ -   Batch Number = 92
01/06/2022 17:17:11 - INFO - __main__ -   Batch Number = 93
01/06/2022 17:17:11 - INFO - __main__ -   Batch Number = 94
01/06/2022 17:17:11 - INFO - __main__ -   Batch Number = 95
01/06/2022 17:17:11 - INFO - __main__ -   Batch Number = 96
Evaluating:  53%|█████▎    | 96/182 [00:02<00:01, 44.41it/s]01/06/2022 17:17:11 - INFO - __main__ -   Batch Number = 97
01/06/2022 17:17:11 - INFO - __main__ -   Batch Number = 98
01/06/2022 17:17:11 - INFO - __main__ -   Batch Number = 99
01/06/2022 17:17:11 - INFO - __main__ -   Batch Number = 100
01/06/2022 17:17:11 - INFO - __main__ -   Batch Number = 101
Evaluating:  55%|█████▌    | 101/182 [00:02<00:01, 44.50it/s]01/06/2022 17:17:11 - INFO - __main__ -   Batch Number = 102
01/06/2022 17:17:11 - INFO - __main__ -   Batch Number = 103
01/06/2022 17:17:11 - INFO - __main__ -   Batch Number = 104
01/06/2022 17:17:11 - INFO - __main__ -   Batch Number = 105
01/06/2022 17:17:11 - INFO - __main__ -   Batch Number = 106
Evaluating:  58%|█████▊    | 106/182 [00:02<00:01, 44.48it/s]01/06/2022 17:17:11 - INFO - __main__ -   Batch Number = 107
01/06/2022 17:17:11 - INFO - __main__ -   Batch Number = 108
01/06/2022 17:17:11 - INFO - __main__ -   Batch Number = 109
01/06/2022 17:17:11 - INFO - __main__ -   Batch Number = 110
01/06/2022 17:17:11 - INFO - __main__ -   Batch Number = 111
Evaluating:  61%|██████    | 111/182 [00:02<00:01, 44.33it/s]01/06/2022 17:17:11 - INFO - __main__ -   Batch Number = 112
01/06/2022 17:17:11 - INFO - __main__ -   Batch Number = 113
01/06/2022 17:17:11 - INFO - __main__ -   Batch Number = 114
01/06/2022 17:17:11 - INFO - __main__ -   Batch Number = 115
01/06/2022 17:17:11 - INFO - __main__ -   Batch Number = 116
Evaluating:  64%|██████▎   | 116/182 [00:02<00:01, 44.44it/s]01/06/2022 17:17:11 - INFO - __main__ -   Batch Number = 117
01/06/2022 17:17:11 - INFO - __main__ -   Batch Number = 118
01/06/2022 17:17:11 - INFO - __main__ -   Batch Number = 119
01/06/2022 17:17:11 - INFO - __main__ -   Batch Number = 120
01/06/2022 17:17:11 - INFO - __main__ -   Batch Number = 121
Evaluating:  66%|██████▋   | 121/182 [00:02<00:01, 44.20it/s]01/06/2022 17:17:11 - INFO - __main__ -   Batch Number = 122
01/06/2022 17:17:12 - INFO - __main__ -   Batch Number = 123
01/06/2022 17:17:12 - INFO - __main__ -   Batch Number = 124
01/06/2022 17:17:12 - INFO - __main__ -   Batch Number = 125
01/06/2022 17:17:12 - INFO - __main__ -   Batch Number = 126
Evaluating:  69%|██████▉   | 126/182 [00:02<00:01, 44.34it/s]01/06/2022 17:17:12 - INFO - __main__ -   Batch Number = 127
01/06/2022 17:17:12 - INFO - __main__ -   Batch Number = 128
01/06/2022 17:17:12 - INFO - __main__ -   Batch Number = 129
01/06/2022 17:17:12 - INFO - __main__ -   Batch Number = 130
01/06/2022 17:17:12 - INFO - __main__ -   Batch Number = 131
Evaluating:  72%|███████▏  | 131/182 [00:03<00:01, 44.43it/s]01/06/2022 17:17:12 - INFO - __main__ -   Batch Number = 132
01/06/2022 17:17:12 - INFO - __main__ -   Batch Number = 133
01/06/2022 17:17:12 - INFO - __main__ -   Batch Number = 134
01/06/2022 17:17:12 - INFO - __main__ -   Batch Number = 135
01/06/2022 17:17:12 - INFO - __main__ -   Batch Number = 136
Evaluating:  75%|███████▍  | 136/182 [00:03<00:01, 44.21it/s]01/06/2022 17:17:12 - INFO - __main__ -   Batch Number = 137
01/06/2022 17:17:12 - INFO - __main__ -   Batch Number = 138
01/06/2022 17:17:12 - INFO - __main__ -   Batch Number = 139
01/06/2022 17:17:12 - INFO - __main__ -   Batch Number = 140
01/06/2022 17:17:12 - INFO - __main__ -   Batch Number = 141
Evaluating:  77%|███████▋  | 141/182 [00:03<00:00, 44.40it/s]01/06/2022 17:17:12 - INFO - __main__ -   Batch Number = 142
01/06/2022 17:17:12 - INFO - __main__ -   Batch Number = 143
01/06/2022 17:17:12 - INFO - __main__ -   Batch Number = 144
01/06/2022 17:17:12 - INFO - __main__ -   Batch Number = 145
01/06/2022 17:17:12 - INFO - __main__ -   Batch Number = 146
Evaluating:  80%|████████  | 146/182 [00:03<00:00, 44.41it/s]01/06/2022 17:17:12 - INFO - __main__ -   Batch Number = 147
01/06/2022 17:17:12 - INFO - __main__ -   Batch Number = 148
01/06/2022 17:17:12 - INFO - __main__ -   Batch Number = 149
01/06/2022 17:17:12 - INFO - __main__ -   Batch Number = 150
01/06/2022 17:17:12 - INFO - __main__ -   Batch Number = 151
Evaluating:  83%|████████▎ | 151/182 [00:03<00:00, 44.30it/s]01/06/2022 17:17:12 - INFO - __main__ -   Batch Number = 152
01/06/2022 17:17:12 - INFO - __main__ -   Batch Number = 153
01/06/2022 17:17:12 - INFO - __main__ -   Batch Number = 154
01/06/2022 17:17:12 - INFO - __main__ -   Batch Number = 155
01/06/2022 17:17:12 - INFO - __main__ -   Batch Number = 156
Evaluating:  86%|████████▌ | 156/182 [00:03<00:00, 44.37it/s]01/06/2022 17:17:12 - INFO - __main__ -   Batch Number = 157
01/06/2022 17:17:12 - INFO - __main__ -   Batch Number = 158
01/06/2022 17:17:12 - INFO - __main__ -   Batch Number = 159
01/06/2022 17:17:12 - INFO - __main__ -   Batch Number = 160
01/06/2022 17:17:12 - INFO - __main__ -   Batch Number = 161
Evaluating:  88%|████████▊ | 161/182 [00:03<00:00, 44.45it/s]01/06/2022 17:17:12 - INFO - __main__ -   Batch Number = 162
01/06/2022 17:17:12 - INFO - __main__ -   Batch Number = 163
01/06/2022 17:17:12 - INFO - __main__ -   Batch Number = 164
01/06/2022 17:17:12 - INFO - __main__ -   Batch Number = 165
01/06/2022 17:17:12 - INFO - __main__ -   Batch Number = 166
Evaluating:  91%|█████████ | 166/182 [00:03<00:00, 44.41it/s]01/06/2022 17:17:12 - INFO - __main__ -   Batch Number = 167
01/06/2022 17:17:13 - INFO - __main__ -   Batch Number = 168
01/06/2022 17:17:13 - INFO - __main__ -   Batch Number = 169
01/06/2022 17:17:13 - INFO - __main__ -   Batch Number = 170
01/06/2022 17:17:13 - INFO - __main__ -   Batch Number = 171
Evaluating:  94%|█████████▍| 171/182 [00:03<00:00, 44.35it/s]01/06/2022 17:17:13 - INFO - __main__ -   Batch Number = 172
01/06/2022 17:17:13 - INFO - __main__ -   Batch Number = 173
01/06/2022 17:17:13 - INFO - __main__ -   Batch Number = 174
01/06/2022 17:17:13 - INFO - __main__ -   Batch Number = 175
01/06/2022 17:17:13 - INFO - __main__ -   Batch Number = 176
Evaluating:  97%|█████████▋| 176/182 [00:04<00:00, 44.48it/s]01/06/2022 17:17:13 - INFO - __main__ -   Batch Number = 177
01/06/2022 17:17:13 - INFO - __main__ -   Batch Number = 178
01/06/2022 17:17:13 - INFO - __main__ -   Batch Number = 179
01/06/2022 17:17:13 - INFO - __main__ -   Batch Number = 180
01/06/2022 17:17:13 - INFO - __main__ -   Batch Number = 181
Evaluating:  99%|█████████▉| 181/182 [00:04<00:00, 44.34it/s]01/06/2022 17:17:13 - INFO - __main__ -   Batch Number = 182
Evaluating: 100%|██████████| 182/182 [00:04<00:00, 43.57it/s]
01/06/2022 17:17:13 - INFO - __main__ -     Evaluation done in total 4.177201 secs (0.002871 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/predictions_ar_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/nbest_predictions_ar_.json
01/06/2022 17:17:16 - INFO - __main__ -   Results = OrderedDict([('exact', 40.33613445378151), ('f1', 56.311890675125575), ('total', 1190), ('HasAns_exact', 40.33613445378151), ('HasAns_f1', 56.311890675125575), ('HasAns_total', 1190), ('best_exact', 40.33613445378151), ('best_exact_thresh', 0.0), ('best_f1', 56.311890675125575), ('best_f1_thresh', 0.0)])
01/06/2022 17:17:16 - INFO - __main__ -   Language adapter for de not found, using en instead
01/06/2022 17:17:16 - INFO - __main__ -   Set active language adapter to en
01/06/2022 17:17:16 - INFO - __main__ -   Args Adapter Weight = None
01/06/2022 17:17:16 - INFO - __main__ -   Adapter Languages = ['en']
01/06/2022 17:17:16 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
  0%|          | 0/48 [00:00<?, ?it/s] 31%|███▏      | 15/48 [00:00<00:00, 148.20it/s] 62%|██████▎   | 30/48 [00:00<00:00, 112.36it/s] 92%|█████████▏| 44/48 [00:00<00:00, 122.82it/s]100%|██████████| 48/48 [00:00<00:00, 124.45it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<05:53,  3.36it/s]convert squad examples to features:  22%|██▏       | 257/1190 [00:00<00:01, 648.37it/s]convert squad examples to features:  32%|███▏      | 385/1190 [00:00<00:01, 455.27it/s]convert squad examples to features:  78%|███████▊  | 929/1190 [00:00<00:00, 1317.42it/s]convert squad examples to features:  99%|█████████▊| 1173/1190 [00:01<00:00, 1248.57it/s]convert squad examples to features: 100%|██████████| 1190/1190 [00:01<00:00, 987.32it/s] /root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|██████████| 1190/1190 [00:00<00:00, 577620.85it/s]
01/06/2022 17:17:18 - INFO - __main__ -   Local Rank = -1
01/06/2022 17:17:18 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.de.json_bert-base-multilingual-cased_384_de
01/06/2022 17:17:20 - INFO - __main__ -   ***** Running evaluation  *****
01/06/2022 17:17:20 - INFO - __main__ -     Num examples = 1296
01/06/2022 17:17:20 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/162 [00:00<?, ?it/s]01/06/2022 17:17:20 - INFO - __main__ -   Batch Number = 1
01/06/2022 17:17:20 - INFO - __main__ -   Batch Number = 2
01/06/2022 17:17:20 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|▏         | 3/162 [00:00<00:06, 24.93it/s]01/06/2022 17:17:20 - INFO - __main__ -   Batch Number = 4
01/06/2022 17:17:20 - INFO - __main__ -   Batch Number = 5
01/06/2022 17:17:20 - INFO - __main__ -   Batch Number = 6
01/06/2022 17:17:20 - INFO - __main__ -   Batch Number = 7
Evaluating:   4%|▍         | 7/162 [00:00<00:04, 31.21it/s]01/06/2022 17:17:20 - INFO - __main__ -   Batch Number = 8
01/06/2022 17:17:20 - INFO - __main__ -   Batch Number = 9
01/06/2022 17:17:20 - INFO - __main__ -   Batch Number = 10
01/06/2022 17:17:20 - INFO - __main__ -   Batch Number = 11
01/06/2022 17:17:20 - INFO - __main__ -   Batch Number = 12
Evaluating:   7%|▋         | 12/162 [00:00<00:04, 37.24it/s]01/06/2022 17:17:20 - INFO - __main__ -   Batch Number = 13
01/06/2022 17:17:20 - INFO - __main__ -   Batch Number = 14
01/06/2022 17:17:20 - INFO - __main__ -   Batch Number = 15
01/06/2022 17:17:20 - INFO - __main__ -   Batch Number = 16
01/06/2022 17:17:20 - INFO - __main__ -   Batch Number = 17
Evaluating:  10%|█         | 17/162 [00:00<00:03, 40.19it/s]01/06/2022 17:17:20 - INFO - __main__ -   Batch Number = 18
01/06/2022 17:17:20 - INFO - __main__ -   Batch Number = 19
01/06/2022 17:17:20 - INFO - __main__ -   Batch Number = 20
01/06/2022 17:17:20 - INFO - __main__ -   Batch Number = 21
01/06/2022 17:17:20 - INFO - __main__ -   Batch Number = 22
Evaluating:  14%|█▎        | 22/162 [00:00<00:03, 41.48it/s]01/06/2022 17:17:20 - INFO - __main__ -   Batch Number = 23
01/06/2022 17:17:20 - INFO - __main__ -   Batch Number = 24
01/06/2022 17:17:20 - INFO - __main__ -   Batch Number = 25
01/06/2022 17:17:20 - INFO - __main__ -   Batch Number = 26
01/06/2022 17:17:20 - INFO - __main__ -   Batch Number = 27
Evaluating:  17%|█▋        | 27/162 [00:00<00:03, 42.48it/s]01/06/2022 17:17:20 - INFO - __main__ -   Batch Number = 28
01/06/2022 17:17:20 - INFO - __main__ -   Batch Number = 29
01/06/2022 17:17:20 - INFO - __main__ -   Batch Number = 30
01/06/2022 17:17:20 - INFO - __main__ -   Batch Number = 31
01/06/2022 17:17:20 - INFO - __main__ -   Batch Number = 32
Evaluating:  20%|█▉        | 32/162 [00:00<00:03, 43.11it/s]01/06/2022 17:17:20 - INFO - __main__ -   Batch Number = 33
01/06/2022 17:17:20 - INFO - __main__ -   Batch Number = 34
01/06/2022 17:17:20 - INFO - __main__ -   Batch Number = 35
01/06/2022 17:17:20 - INFO - __main__ -   Batch Number = 36
01/06/2022 17:17:20 - INFO - __main__ -   Batch Number = 37
Evaluating:  23%|██▎       | 37/162 [00:00<00:02, 43.41it/s]01/06/2022 17:17:20 - INFO - __main__ -   Batch Number = 38
01/06/2022 17:17:20 - INFO - __main__ -   Batch Number = 39
01/06/2022 17:17:20 - INFO - __main__ -   Batch Number = 40
01/06/2022 17:17:21 - INFO - __main__ -   Batch Number = 41
01/06/2022 17:17:21 - INFO - __main__ -   Batch Number = 42
Evaluating:  26%|██▌       | 42/162 [00:01<00:02, 43.84it/s]01/06/2022 17:17:21 - INFO - __main__ -   Batch Number = 43
01/06/2022 17:17:21 - INFO - __main__ -   Batch Number = 44
01/06/2022 17:17:21 - INFO - __main__ -   Batch Number = 45
01/06/2022 17:17:21 - INFO - __main__ -   Batch Number = 46
01/06/2022 17:17:21 - INFO - __main__ -   Batch Number = 47
Evaluating:  29%|██▉       | 47/162 [00:01<00:02, 43.97it/s]01/06/2022 17:17:21 - INFO - __main__ -   Batch Number = 48
01/06/2022 17:17:21 - INFO - __main__ -   Batch Number = 49
01/06/2022 17:17:21 - INFO - __main__ -   Batch Number = 50
01/06/2022 17:17:21 - INFO - __main__ -   Batch Number = 51
01/06/2022 17:17:21 - INFO - __main__ -   Batch Number = 52
Evaluating:  32%|███▏      | 52/162 [00:01<00:02, 44.12it/s]01/06/2022 17:17:21 - INFO - __main__ -   Batch Number = 53
01/06/2022 17:17:21 - INFO - __main__ -   Batch Number = 54
01/06/2022 17:17:21 - INFO - __main__ -   Batch Number = 55
01/06/2022 17:17:21 - INFO - __main__ -   Batch Number = 56
01/06/2022 17:17:21 - INFO - __main__ -   Batch Number = 57
Evaluating:  35%|███▌      | 57/162 [00:01<00:02, 44.21it/s]01/06/2022 17:17:21 - INFO - __main__ -   Batch Number = 58
01/06/2022 17:17:21 - INFO - __main__ -   Batch Number = 59
01/06/2022 17:17:21 - INFO - __main__ -   Batch Number = 60
01/06/2022 17:17:21 - INFO - __main__ -   Batch Number = 61
01/06/2022 17:17:21 - INFO - __main__ -   Batch Number = 62
Evaluating:  38%|███▊      | 62/162 [00:01<00:02, 44.36it/s]01/06/2022 17:17:21 - INFO - __main__ -   Batch Number = 63
01/06/2022 17:17:21 - INFO - __main__ -   Batch Number = 64
01/06/2022 17:17:21 - INFO - __main__ -   Batch Number = 65
01/06/2022 17:17:21 - INFO - __main__ -   Batch Number = 66
01/06/2022 17:17:21 - INFO - __main__ -   Batch Number = 67
Evaluating:  41%|████▏     | 67/162 [00:01<00:02, 44.22it/s]01/06/2022 17:17:21 - INFO - __main__ -   Batch Number = 68
01/06/2022 17:17:21 - INFO - __main__ -   Batch Number = 69
01/06/2022 17:17:21 - INFO - __main__ -   Batch Number = 70
01/06/2022 17:17:21 - INFO - __main__ -   Batch Number = 71
01/06/2022 17:17:21 - INFO - __main__ -   Batch Number = 72
Evaluating:  44%|████▍     | 72/162 [00:01<00:02, 44.13it/s]01/06/2022 17:17:21 - INFO - __main__ -   Batch Number = 73
01/06/2022 17:17:21 - INFO - __main__ -   Batch Number = 74
01/06/2022 17:17:21 - INFO - __main__ -   Batch Number = 75
01/06/2022 17:17:21 - INFO - __main__ -   Batch Number = 76
01/06/2022 17:17:21 - INFO - __main__ -   Batch Number = 77
Evaluating:  48%|████▊     | 77/162 [00:01<00:01, 44.23it/s]01/06/2022 17:17:21 - INFO - __main__ -   Batch Number = 78
01/06/2022 17:17:21 - INFO - __main__ -   Batch Number = 79
01/06/2022 17:17:21 - INFO - __main__ -   Batch Number = 80
01/06/2022 17:17:21 - INFO - __main__ -   Batch Number = 81
01/06/2022 17:17:21 - INFO - __main__ -   Batch Number = 82
Evaluating:  51%|█████     | 82/162 [00:01<00:01, 44.07it/s]01/06/2022 17:17:21 - INFO - __main__ -   Batch Number = 83
01/06/2022 17:17:21 - INFO - __main__ -   Batch Number = 84
01/06/2022 17:17:22 - INFO - __main__ -   Batch Number = 85
01/06/2022 17:17:22 - INFO - __main__ -   Batch Number = 86
01/06/2022 17:17:22 - INFO - __main__ -   Batch Number = 87
Evaluating:  54%|█████▎    | 87/162 [00:02<00:01, 44.25it/s]01/06/2022 17:17:22 - INFO - __main__ -   Batch Number = 88
01/06/2022 17:17:22 - INFO - __main__ -   Batch Number = 89
01/06/2022 17:17:22 - INFO - __main__ -   Batch Number = 90
01/06/2022 17:17:22 - INFO - __main__ -   Batch Number = 91
01/06/2022 17:17:22 - INFO - __main__ -   Batch Number = 92
Evaluating:  57%|█████▋    | 92/162 [00:02<00:01, 44.15it/s]01/06/2022 17:17:22 - INFO - __main__ -   Batch Number = 93
01/06/2022 17:17:22 - INFO - __main__ -   Batch Number = 94
01/06/2022 17:17:22 - INFO - __main__ -   Batch Number = 95
01/06/2022 17:17:22 - INFO - __main__ -   Batch Number = 96
01/06/2022 17:17:22 - INFO - __main__ -   Batch Number = 97
Evaluating:  60%|█████▉    | 97/162 [00:02<00:01, 44.12it/s]01/06/2022 17:17:22 - INFO - __main__ -   Batch Number = 98
01/06/2022 17:17:22 - INFO - __main__ -   Batch Number = 99
01/06/2022 17:17:22 - INFO - __main__ -   Batch Number = 100
01/06/2022 17:17:22 - INFO - __main__ -   Batch Number = 101
01/06/2022 17:17:22 - INFO - __main__ -   Batch Number = 102
Evaluating:  63%|██████▎   | 102/162 [00:02<00:01, 44.16it/s]01/06/2022 17:17:22 - INFO - __main__ -   Batch Number = 103
01/06/2022 17:17:22 - INFO - __main__ -   Batch Number = 104
01/06/2022 17:17:22 - INFO - __main__ -   Batch Number = 105
01/06/2022 17:17:22 - INFO - __main__ -   Batch Number = 106
01/06/2022 17:17:22 - INFO - __main__ -   Batch Number = 107
Evaluating:  66%|██████▌   | 107/162 [00:02<00:01, 44.18it/s]01/06/2022 17:17:22 - INFO - __main__ -   Batch Number = 108
01/06/2022 17:17:22 - INFO - __main__ -   Batch Number = 109
01/06/2022 17:17:22 - INFO - __main__ -   Batch Number = 110
01/06/2022 17:17:22 - INFO - __main__ -   Batch Number = 111
01/06/2022 17:17:22 - INFO - __main__ -   Batch Number = 112
Evaluating:  69%|██████▉   | 112/162 [00:02<00:01, 44.02it/s]01/06/2022 17:17:22 - INFO - __main__ -   Batch Number = 113
01/06/2022 17:17:22 - INFO - __main__ -   Batch Number = 114
01/06/2022 17:17:22 - INFO - __main__ -   Batch Number = 115
01/06/2022 17:17:22 - INFO - __main__ -   Batch Number = 116
01/06/2022 17:17:22 - INFO - __main__ -   Batch Number = 117
Evaluating:  72%|███████▏  | 117/162 [00:02<00:01, 44.10it/s]01/06/2022 17:17:22 - INFO - __main__ -   Batch Number = 118
01/06/2022 17:17:22 - INFO - __main__ -   Batch Number = 119
01/06/2022 17:17:22 - INFO - __main__ -   Batch Number = 120
01/06/2022 17:17:22 - INFO - __main__ -   Batch Number = 121
01/06/2022 17:17:22 - INFO - __main__ -   Batch Number = 122
Evaluating:  75%|███████▌  | 122/162 [00:02<00:00, 44.21it/s]01/06/2022 17:17:22 - INFO - __main__ -   Batch Number = 123
01/06/2022 17:17:22 - INFO - __main__ -   Batch Number = 124
01/06/2022 17:17:22 - INFO - __main__ -   Batch Number = 125
01/06/2022 17:17:22 - INFO - __main__ -   Batch Number = 126
01/06/2022 17:17:22 - INFO - __main__ -   Batch Number = 127
Evaluating:  78%|███████▊  | 127/162 [00:02<00:00, 44.15it/s]01/06/2022 17:17:22 - INFO - __main__ -   Batch Number = 128
01/06/2022 17:17:22 - INFO - __main__ -   Batch Number = 129
01/06/2022 17:17:23 - INFO - __main__ -   Batch Number = 130
01/06/2022 17:17:23 - INFO - __main__ -   Batch Number = 131
01/06/2022 17:17:23 - INFO - __main__ -   Batch Number = 132
Evaluating:  81%|████████▏ | 132/162 [00:03<00:00, 43.67it/s]01/06/2022 17:17:23 - INFO - __main__ -   Batch Number = 133
01/06/2022 17:17:23 - INFO - __main__ -   Batch Number = 134
01/06/2022 17:17:23 - INFO - __main__ -   Batch Number = 135
01/06/2022 17:17:23 - INFO - __main__ -   Batch Number = 136
01/06/2022 17:17:23 - INFO - __main__ -   Batch Number = 137
Evaluating:  85%|████████▍ | 137/162 [00:03<00:00, 43.69it/s]01/06/2022 17:17:23 - INFO - __main__ -   Batch Number = 138
01/06/2022 17:17:23 - INFO - __main__ -   Batch Number = 139
01/06/2022 17:17:23 - INFO - __main__ -   Batch Number = 140
01/06/2022 17:17:23 - INFO - __main__ -   Batch Number = 141
01/06/2022 17:17:23 - INFO - __main__ -   Batch Number = 142
Evaluating:  88%|████████▊ | 142/162 [00:03<00:00, 43.87it/s]01/06/2022 17:17:23 - INFO - __main__ -   Batch Number = 143
01/06/2022 17:17:23 - INFO - __main__ -   Batch Number = 144
01/06/2022 17:17:23 - INFO - __main__ -   Batch Number = 145
01/06/2022 17:17:23 - INFO - __main__ -   Batch Number = 146
01/06/2022 17:17:23 - INFO - __main__ -   Batch Number = 147
Evaluating:  91%|█████████ | 147/162 [00:03<00:00, 43.89it/s]01/06/2022 17:17:23 - INFO - __main__ -   Batch Number = 148
01/06/2022 17:17:23 - INFO - __main__ -   Batch Number = 149
01/06/2022 17:17:23 - INFO - __main__ -   Batch Number = 150
01/06/2022 17:17:23 - INFO - __main__ -   Batch Number = 151
01/06/2022 17:17:23 - INFO - __main__ -   Batch Number = 152
Evaluating:  94%|█████████▍| 152/162 [00:03<00:00, 44.11it/s]01/06/2022 17:17:23 - INFO - __main__ -   Batch Number = 153
01/06/2022 17:17:23 - INFO - __main__ -   Batch Number = 154
01/06/2022 17:17:23 - INFO - __main__ -   Batch Number = 155
01/06/2022 17:17:23 - INFO - __main__ -   Batch Number = 156
01/06/2022 17:17:23 - INFO - __main__ -   Batch Number = 157
Evaluating:  97%|█████████▋| 157/162 [00:03<00:00, 44.14it/s]01/06/2022 17:17:23 - INFO - __main__ -   Batch Number = 158
01/06/2022 17:17:23 - INFO - __main__ -   Batch Number = 159
01/06/2022 17:17:23 - INFO - __main__ -   Batch Number = 160
01/06/2022 17:17:23 - INFO - __main__ -   Batch Number = 161
01/06/2022 17:17:23 - INFO - __main__ -   Batch Number = 162
Evaluating: 100%|██████████| 162/162 [00:03<00:00, 44.18it/s]Evaluating: 100%|██████████| 162/162 [00:03<00:00, 43.32it/s]
01/06/2022 17:17:23 - INFO - __main__ -     Evaluation done in total 3.740347 secs (0.002886 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/predictions_de_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/nbest_predictions_de_.json
01/06/2022 17:17:27 - INFO - __main__ -   Results = OrderedDict([('exact', 55.21008403361345), ('f1', 70.30444319332756), ('total', 1190), ('HasAns_exact', 55.21008403361345), ('HasAns_f1', 70.30444319332756), ('HasAns_total', 1190), ('best_exact', 55.21008403361345), ('best_exact_thresh', 0.0), ('best_f1', 70.30444319332756), ('best_f1_thresh', 0.0)])
01/06/2022 17:17:27 - INFO - __main__ -   Language adapter for el not found, using en instead
01/06/2022 17:17:27 - INFO - __main__ -   Set active language adapter to en
01/06/2022 17:17:27 - INFO - __main__ -   Args Adapter Weight = None
01/06/2022 17:17:27 - INFO - __main__ -   Adapter Languages = ['en']
01/06/2022 17:17:27 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
  0%|          | 0/48 [00:00<?, ?it/s] 29%|██▉       | 14/48 [00:00<00:00, 138.78it/s] 58%|█████▊    | 28/48 [00:00<00:00, 104.89it/s] 83%|████████▎ | 40/48 [00:00<00:00, 109.64it/s]100%|██████████| 48/48 [00:00<00:00, 114.63it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<06:54,  2.87it/s]convert squad examples to features:  22%|██▏       | 257/1190 [00:00<00:01, 636.97it/s]convert squad examples to features:  32%|███▏      | 385/1190 [00:01<00:03, 213.51it/s]convert squad examples to features: 100%|██████████| 1190/1190 [00:01<00:00, 694.09it/s]/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|██████████| 1190/1190 [00:00<00:00, 604996.58it/s]
01/06/2022 17:17:30 - INFO - __main__ -   Local Rank = -1
01/06/2022 17:17:30 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.el.json_bert-base-multilingual-cased_384_el
01/06/2022 17:17:32 - INFO - __main__ -   ***** Running evaluation  *****
01/06/2022 17:17:32 - INFO - __main__ -     Num examples = 2077
01/06/2022 17:17:32 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/260 [00:00<?, ?it/s]01/06/2022 17:17:32 - INFO - __main__ -   Batch Number = 1
01/06/2022 17:17:32 - INFO - __main__ -   Batch Number = 2
01/06/2022 17:17:32 - INFO - __main__ -   Batch Number = 3
Evaluating:   1%|          | 3/260 [00:00<00:08, 29.49it/s]01/06/2022 17:17:32 - INFO - __main__ -   Batch Number = 4
01/06/2022 17:17:32 - INFO - __main__ -   Batch Number = 5
01/06/2022 17:17:32 - INFO - __main__ -   Batch Number = 6
Evaluating:   2%|▏         | 6/260 [00:00<00:08, 29.69it/s]01/06/2022 17:17:32 - INFO - __main__ -   Batch Number = 7
01/06/2022 17:17:32 - INFO - __main__ -   Batch Number = 8
01/06/2022 17:17:32 - INFO - __main__ -   Batch Number = 9
01/06/2022 17:17:32 - INFO - __main__ -   Batch Number = 10
Evaluating:   4%|▍         | 10/260 [00:00<00:07, 33.03it/s]01/06/2022 17:17:32 - INFO - __main__ -   Batch Number = 11
01/06/2022 17:17:32 - INFO - __main__ -   Batch Number = 12
01/06/2022 17:17:32 - INFO - __main__ -   Batch Number = 13
01/06/2022 17:17:32 - INFO - __main__ -   Batch Number = 14
01/06/2022 17:17:32 - INFO - __main__ -   Batch Number = 15
Evaluating:   6%|▌         | 15/260 [00:00<00:06, 37.75it/s]01/06/2022 17:17:32 - INFO - __main__ -   Batch Number = 16
01/06/2022 17:17:32 - INFO - __main__ -   Batch Number = 17
01/06/2022 17:17:32 - INFO - __main__ -   Batch Number = 18
01/06/2022 17:17:32 - INFO - __main__ -   Batch Number = 19
01/06/2022 17:17:32 - INFO - __main__ -   Batch Number = 20
Evaluating:   8%|▊         | 20/260 [00:00<00:05, 40.04it/s]01/06/2022 17:17:32 - INFO - __main__ -   Batch Number = 21
01/06/2022 17:17:32 - INFO - __main__ -   Batch Number = 22
01/06/2022 17:17:32 - INFO - __main__ -   Batch Number = 23
01/06/2022 17:17:32 - INFO - __main__ -   Batch Number = 24
01/06/2022 17:17:32 - INFO - __main__ -   Batch Number = 25
Evaluating:  10%|▉         | 25/260 [00:00<00:05, 41.57it/s]01/06/2022 17:17:32 - INFO - __main__ -   Batch Number = 26
01/06/2022 17:17:32 - INFO - __main__ -   Batch Number = 27
01/06/2022 17:17:32 - INFO - __main__ -   Batch Number = 28
01/06/2022 17:17:32 - INFO - __main__ -   Batch Number = 29
01/06/2022 17:17:32 - INFO - __main__ -   Batch Number = 30
Evaluating:  12%|█▏        | 30/260 [00:00<00:05, 42.62it/s]01/06/2022 17:17:32 - INFO - __main__ -   Batch Number = 31
01/06/2022 17:17:32 - INFO - __main__ -   Batch Number = 32
01/06/2022 17:17:32 - INFO - __main__ -   Batch Number = 33
01/06/2022 17:17:32 - INFO - __main__ -   Batch Number = 34
01/06/2022 17:17:32 - INFO - __main__ -   Batch Number = 35
Evaluating:  13%|█▎        | 35/260 [00:00<00:05, 42.91it/s]01/06/2022 17:17:32 - INFO - __main__ -   Batch Number = 36
01/06/2022 17:17:32 - INFO - __main__ -   Batch Number = 37
01/06/2022 17:17:32 - INFO - __main__ -   Batch Number = 38
01/06/2022 17:17:32 - INFO - __main__ -   Batch Number = 39
01/06/2022 17:17:33 - INFO - __main__ -   Batch Number = 40
Evaluating:  15%|█▌        | 40/260 [00:00<00:05, 43.28it/s]01/06/2022 17:17:33 - INFO - __main__ -   Batch Number = 41
01/06/2022 17:17:33 - INFO - __main__ -   Batch Number = 42
01/06/2022 17:17:33 - INFO - __main__ -   Batch Number = 43
01/06/2022 17:17:33 - INFO - __main__ -   Batch Number = 44
01/06/2022 17:17:33 - INFO - __main__ -   Batch Number = 45
Evaluating:  17%|█▋        | 45/260 [00:01<00:04, 43.39it/s]01/06/2022 17:17:33 - INFO - __main__ -   Batch Number = 46
01/06/2022 17:17:33 - INFO - __main__ -   Batch Number = 47
01/06/2022 17:17:33 - INFO - __main__ -   Batch Number = 48
01/06/2022 17:17:33 - INFO - __main__ -   Batch Number = 49
01/06/2022 17:17:33 - INFO - __main__ -   Batch Number = 50
Evaluating:  19%|█▉        | 50/260 [00:01<00:04, 43.47it/s]01/06/2022 17:17:33 - INFO - __main__ -   Batch Number = 51
01/06/2022 17:17:33 - INFO - __main__ -   Batch Number = 52
01/06/2022 17:17:33 - INFO - __main__ -   Batch Number = 53
01/06/2022 17:17:33 - INFO - __main__ -   Batch Number = 54
01/06/2022 17:17:33 - INFO - __main__ -   Batch Number = 55
Evaluating:  21%|██        | 55/260 [00:01<00:04, 43.73it/s]01/06/2022 17:17:33 - INFO - __main__ -   Batch Number = 56
01/06/2022 17:17:33 - INFO - __main__ -   Batch Number = 57
01/06/2022 17:17:33 - INFO - __main__ -   Batch Number = 58
01/06/2022 17:17:33 - INFO - __main__ -   Batch Number = 59
01/06/2022 17:17:33 - INFO - __main__ -   Batch Number = 60
Evaluating:  23%|██▎       | 60/260 [00:01<00:04, 43.80it/s]01/06/2022 17:17:33 - INFO - __main__ -   Batch Number = 61
01/06/2022 17:17:33 - INFO - __main__ -   Batch Number = 62
01/06/2022 17:17:33 - INFO - __main__ -   Batch Number = 63
01/06/2022 17:17:33 - INFO - __main__ -   Batch Number = 64
01/06/2022 17:17:33 - INFO - __main__ -   Batch Number = 65
Evaluating:  25%|██▌       | 65/260 [00:01<00:04, 43.83it/s]01/06/2022 17:17:33 - INFO - __main__ -   Batch Number = 66
01/06/2022 17:17:33 - INFO - __main__ -   Batch Number = 67
01/06/2022 17:17:33 - INFO - __main__ -   Batch Number = 68
01/06/2022 17:17:33 - INFO - __main__ -   Batch Number = 69
01/06/2022 17:17:33 - INFO - __main__ -   Batch Number = 70
Evaluating:  27%|██▋       | 70/260 [00:01<00:04, 43.98it/s]01/06/2022 17:17:33 - INFO - __main__ -   Batch Number = 71
01/06/2022 17:17:33 - INFO - __main__ -   Batch Number = 72
01/06/2022 17:17:33 - INFO - __main__ -   Batch Number = 73
01/06/2022 17:17:33 - INFO - __main__ -   Batch Number = 74
01/06/2022 17:17:33 - INFO - __main__ -   Batch Number = 75
Evaluating:  29%|██▉       | 75/260 [00:01<00:04, 44.08it/s]01/06/2022 17:17:33 - INFO - __main__ -   Batch Number = 76
01/06/2022 17:17:33 - INFO - __main__ -   Batch Number = 77
01/06/2022 17:17:33 - INFO - __main__ -   Batch Number = 78
01/06/2022 17:17:33 - INFO - __main__ -   Batch Number = 79
01/06/2022 17:17:33 - INFO - __main__ -   Batch Number = 80
Evaluating:  31%|███       | 80/260 [00:01<00:04, 44.15it/s]01/06/2022 17:17:33 - INFO - __main__ -   Batch Number = 81
01/06/2022 17:17:33 - INFO - __main__ -   Batch Number = 82
01/06/2022 17:17:33 - INFO - __main__ -   Batch Number = 83
01/06/2022 17:17:34 - INFO - __main__ -   Batch Number = 84
01/06/2022 17:17:34 - INFO - __main__ -   Batch Number = 85
Evaluating:  33%|███▎      | 85/260 [00:02<00:03, 44.23it/s]01/06/2022 17:17:34 - INFO - __main__ -   Batch Number = 86
01/06/2022 17:17:34 - INFO - __main__ -   Batch Number = 87
01/06/2022 17:17:34 - INFO - __main__ -   Batch Number = 88
01/06/2022 17:17:34 - INFO - __main__ -   Batch Number = 89
01/06/2022 17:17:34 - INFO - __main__ -   Batch Number = 90
Evaluating:  35%|███▍      | 90/260 [00:02<00:03, 44.37it/s]01/06/2022 17:17:34 - INFO - __main__ -   Batch Number = 91
01/06/2022 17:17:34 - INFO - __main__ -   Batch Number = 92
01/06/2022 17:17:34 - INFO - __main__ -   Batch Number = 93
01/06/2022 17:17:34 - INFO - __main__ -   Batch Number = 94
01/06/2022 17:17:34 - INFO - __main__ -   Batch Number = 95
Evaluating:  37%|███▋      | 95/260 [00:02<00:03, 44.15it/s]01/06/2022 17:17:34 - INFO - __main__ -   Batch Number = 96
01/06/2022 17:17:34 - INFO - __main__ -   Batch Number = 97
01/06/2022 17:17:34 - INFO - __main__ -   Batch Number = 98
01/06/2022 17:17:34 - INFO - __main__ -   Batch Number = 99
01/06/2022 17:17:34 - INFO - __main__ -   Batch Number = 100
Evaluating:  38%|███▊      | 100/260 [00:02<00:03, 44.26it/s]01/06/2022 17:17:34 - INFO - __main__ -   Batch Number = 101
01/06/2022 17:17:34 - INFO - __main__ -   Batch Number = 102
01/06/2022 17:17:34 - INFO - __main__ -   Batch Number = 103
01/06/2022 17:17:34 - INFO - __main__ -   Batch Number = 104
01/06/2022 17:17:34 - INFO - __main__ -   Batch Number = 105
Evaluating:  40%|████      | 105/260 [00:02<00:03, 44.23it/s]01/06/2022 17:17:34 - INFO - __main__ -   Batch Number = 106
01/06/2022 17:17:34 - INFO - __main__ -   Batch Number = 107
01/06/2022 17:17:34 - INFO - __main__ -   Batch Number = 108
01/06/2022 17:17:34 - INFO - __main__ -   Batch Number = 109
01/06/2022 17:17:34 - INFO - __main__ -   Batch Number = 110
Evaluating:  42%|████▏     | 110/260 [00:02<00:03, 44.04it/s]01/06/2022 17:17:34 - INFO - __main__ -   Batch Number = 111
01/06/2022 17:17:34 - INFO - __main__ -   Batch Number = 112
01/06/2022 17:17:34 - INFO - __main__ -   Batch Number = 113
01/06/2022 17:17:34 - INFO - __main__ -   Batch Number = 114
01/06/2022 17:17:34 - INFO - __main__ -   Batch Number = 115
Evaluating:  44%|████▍     | 115/260 [00:02<00:03, 44.16it/s]01/06/2022 17:17:34 - INFO - __main__ -   Batch Number = 116
01/06/2022 17:17:34 - INFO - __main__ -   Batch Number = 117
01/06/2022 17:17:34 - INFO - __main__ -   Batch Number = 118
01/06/2022 17:17:34 - INFO - __main__ -   Batch Number = 119
01/06/2022 17:17:34 - INFO - __main__ -   Batch Number = 120
Evaluating:  46%|████▌     | 120/260 [00:02<00:03, 44.10it/s]01/06/2022 17:17:34 - INFO - __main__ -   Batch Number = 121
01/06/2022 17:17:34 - INFO - __main__ -   Batch Number = 122
01/06/2022 17:17:34 - INFO - __main__ -   Batch Number = 123
01/06/2022 17:17:34 - INFO - __main__ -   Batch Number = 124
01/06/2022 17:17:34 - INFO - __main__ -   Batch Number = 125
Evaluating:  48%|████▊     | 125/260 [00:02<00:03, 42.49it/s]01/06/2022 17:17:34 - INFO - __main__ -   Batch Number = 126
01/06/2022 17:17:35 - INFO - __main__ -   Batch Number = 127
01/06/2022 17:17:35 - INFO - __main__ -   Batch Number = 128
01/06/2022 17:17:35 - INFO - __main__ -   Batch Number = 129
01/06/2022 17:17:35 - INFO - __main__ -   Batch Number = 130
Evaluating:  50%|█████     | 130/260 [00:03<00:03, 43.02it/s]01/06/2022 17:17:35 - INFO - __main__ -   Batch Number = 131
01/06/2022 17:17:35 - INFO - __main__ -   Batch Number = 132
01/06/2022 17:17:35 - INFO - __main__ -   Batch Number = 133
01/06/2022 17:17:35 - INFO - __main__ -   Batch Number = 134
01/06/2022 17:17:35 - INFO - __main__ -   Batch Number = 135
Evaluating:  52%|█████▏    | 135/260 [00:03<00:02, 43.41it/s]01/06/2022 17:17:35 - INFO - __main__ -   Batch Number = 136
01/06/2022 17:17:35 - INFO - __main__ -   Batch Number = 137
01/06/2022 17:17:35 - INFO - __main__ -   Batch Number = 138
01/06/2022 17:17:35 - INFO - __main__ -   Batch Number = 139
01/06/2022 17:17:35 - INFO - __main__ -   Batch Number = 140
Evaluating:  54%|█████▍    | 140/260 [00:03<00:02, 43.55it/s]01/06/2022 17:17:35 - INFO - __main__ -   Batch Number = 141
01/06/2022 17:17:35 - INFO - __main__ -   Batch Number = 142
01/06/2022 17:17:35 - INFO - __main__ -   Batch Number = 143
01/06/2022 17:17:35 - INFO - __main__ -   Batch Number = 144
01/06/2022 17:17:35 - INFO - __main__ -   Batch Number = 145
Evaluating:  56%|█████▌    | 145/260 [00:03<00:02, 43.84it/s]01/06/2022 17:17:35 - INFO - __main__ -   Batch Number = 146
01/06/2022 17:17:35 - INFO - __main__ -   Batch Number = 147
01/06/2022 17:17:35 - INFO - __main__ -   Batch Number = 148
01/06/2022 17:17:35 - INFO - __main__ -   Batch Number = 149
01/06/2022 17:17:35 - INFO - __main__ -   Batch Number = 150
Evaluating:  58%|█████▊    | 150/260 [00:03<00:02, 43.91it/s]01/06/2022 17:17:35 - INFO - __main__ -   Batch Number = 151
01/06/2022 17:17:35 - INFO - __main__ -   Batch Number = 152
01/06/2022 17:17:35 - INFO - __main__ -   Batch Number = 153
01/06/2022 17:17:35 - INFO - __main__ -   Batch Number = 154
01/06/2022 17:17:35 - INFO - __main__ -   Batch Number = 155
Evaluating:  60%|█████▉    | 155/260 [00:03<00:02, 43.99it/s]01/06/2022 17:17:35 - INFO - __main__ -   Batch Number = 156
01/06/2022 17:17:35 - INFO - __main__ -   Batch Number = 157
01/06/2022 17:17:35 - INFO - __main__ -   Batch Number = 158
01/06/2022 17:17:35 - INFO - __main__ -   Batch Number = 159
01/06/2022 17:17:35 - INFO - __main__ -   Batch Number = 160
Evaluating:  62%|██████▏   | 160/260 [00:03<00:02, 43.99it/s]01/06/2022 17:17:35 - INFO - __main__ -   Batch Number = 161
01/06/2022 17:17:35 - INFO - __main__ -   Batch Number = 162
01/06/2022 17:17:35 - INFO - __main__ -   Batch Number = 163
01/06/2022 17:17:35 - INFO - __main__ -   Batch Number = 164
01/06/2022 17:17:35 - INFO - __main__ -   Batch Number = 165
Evaluating:  63%|██████▎   | 165/260 [00:03<00:02, 44.14it/s]01/06/2022 17:17:35 - INFO - __main__ -   Batch Number = 166
01/06/2022 17:17:35 - INFO - __main__ -   Batch Number = 167
01/06/2022 17:17:35 - INFO - __main__ -   Batch Number = 168
01/06/2022 17:17:35 - INFO - __main__ -   Batch Number = 169
01/06/2022 17:17:35 - INFO - __main__ -   Batch Number = 170
Evaluating:  65%|██████▌   | 170/260 [00:03<00:02, 43.96it/s]01/06/2022 17:17:35 - INFO - __main__ -   Batch Number = 171
01/06/2022 17:17:36 - INFO - __main__ -   Batch Number = 172
01/06/2022 17:17:36 - INFO - __main__ -   Batch Number = 173
01/06/2022 17:17:36 - INFO - __main__ -   Batch Number = 174
01/06/2022 17:17:36 - INFO - __main__ -   Batch Number = 175
Evaluating:  67%|██████▋   | 175/260 [00:04<00:01, 44.03it/s]01/06/2022 17:17:36 - INFO - __main__ -   Batch Number = 176
01/06/2022 17:17:36 - INFO - __main__ -   Batch Number = 177
01/06/2022 17:17:36 - INFO - __main__ -   Batch Number = 178
01/06/2022 17:17:36 - INFO - __main__ -   Batch Number = 179
01/06/2022 17:17:36 - INFO - __main__ -   Batch Number = 180
Evaluating:  69%|██████▉   | 180/260 [00:04<00:01, 44.17it/s]01/06/2022 17:17:36 - INFO - __main__ -   Batch Number = 181
01/06/2022 17:17:36 - INFO - __main__ -   Batch Number = 182
01/06/2022 17:17:36 - INFO - __main__ -   Batch Number = 183
01/06/2022 17:17:36 - INFO - __main__ -   Batch Number = 184
01/06/2022 17:17:36 - INFO - __main__ -   Batch Number = 185
Evaluating:  71%|███████   | 185/260 [00:04<00:01, 44.04it/s]01/06/2022 17:17:36 - INFO - __main__ -   Batch Number = 186
01/06/2022 17:17:36 - INFO - __main__ -   Batch Number = 187
01/06/2022 17:17:36 - INFO - __main__ -   Batch Number = 188
01/06/2022 17:17:36 - INFO - __main__ -   Batch Number = 189
01/06/2022 17:17:36 - INFO - __main__ -   Batch Number = 190
Evaluating:  73%|███████▎  | 190/260 [00:04<00:01, 44.17it/s]01/06/2022 17:17:36 - INFO - __main__ -   Batch Number = 191
01/06/2022 17:17:36 - INFO - __main__ -   Batch Number = 192
01/06/2022 17:17:36 - INFO - __main__ -   Batch Number = 193
01/06/2022 17:17:36 - INFO - __main__ -   Batch Number = 194
01/06/2022 17:17:36 - INFO - __main__ -   Batch Number = 195
Evaluating:  75%|███████▌  | 195/260 [00:04<00:01, 44.12it/s]01/06/2022 17:17:36 - INFO - __main__ -   Batch Number = 196
01/06/2022 17:17:36 - INFO - __main__ -   Batch Number = 197
01/06/2022 17:17:36 - INFO - __main__ -   Batch Number = 198
01/06/2022 17:17:36 - INFO - __main__ -   Batch Number = 199
01/06/2022 17:17:36 - INFO - __main__ -   Batch Number = 200
Evaluating:  77%|███████▋  | 200/260 [00:04<00:01, 44.02it/s]01/06/2022 17:17:36 - INFO - __main__ -   Batch Number = 201
01/06/2022 17:17:36 - INFO - __main__ -   Batch Number = 202
01/06/2022 17:17:36 - INFO - __main__ -   Batch Number = 203
01/06/2022 17:17:36 - INFO - __main__ -   Batch Number = 204
01/06/2022 17:17:36 - INFO - __main__ -   Batch Number = 205
Evaluating:  79%|███████▉  | 205/260 [00:04<00:01, 43.97it/s]01/06/2022 17:17:36 - INFO - __main__ -   Batch Number = 206
01/06/2022 17:17:36 - INFO - __main__ -   Batch Number = 207
01/06/2022 17:17:36 - INFO - __main__ -   Batch Number = 208
01/06/2022 17:17:36 - INFO - __main__ -   Batch Number = 209
01/06/2022 17:17:36 - INFO - __main__ -   Batch Number = 210
Evaluating:  81%|████████  | 210/260 [00:04<00:01, 43.81it/s]01/06/2022 17:17:36 - INFO - __main__ -   Batch Number = 211
01/06/2022 17:17:36 - INFO - __main__ -   Batch Number = 212
01/06/2022 17:17:36 - INFO - __main__ -   Batch Number = 213
01/06/2022 17:17:36 - INFO - __main__ -   Batch Number = 214
01/06/2022 17:17:36 - INFO - __main__ -   Batch Number = 215
Evaluating:  83%|████████▎ | 215/260 [00:04<00:01, 43.86it/s]01/06/2022 17:17:37 - INFO - __main__ -   Batch Number = 216
01/06/2022 17:17:37 - INFO - __main__ -   Batch Number = 217
01/06/2022 17:17:37 - INFO - __main__ -   Batch Number = 218
01/06/2022 17:17:37 - INFO - __main__ -   Batch Number = 219
01/06/2022 17:17:37 - INFO - __main__ -   Batch Number = 220
Evaluating:  85%|████████▍ | 220/260 [00:05<00:00, 43.83it/s]01/06/2022 17:17:37 - INFO - __main__ -   Batch Number = 221
01/06/2022 17:17:37 - INFO - __main__ -   Batch Number = 222
01/06/2022 17:17:37 - INFO - __main__ -   Batch Number = 223
01/06/2022 17:17:37 - INFO - __main__ -   Batch Number = 224
01/06/2022 17:17:37 - INFO - __main__ -   Batch Number = 225
Evaluating:  87%|████████▋ | 225/260 [00:05<00:00, 43.90it/s]01/06/2022 17:17:37 - INFO - __main__ -   Batch Number = 226
01/06/2022 17:17:37 - INFO - __main__ -   Batch Number = 227
01/06/2022 17:17:37 - INFO - __main__ -   Batch Number = 228
01/06/2022 17:17:37 - INFO - __main__ -   Batch Number = 229
01/06/2022 17:17:37 - INFO - __main__ -   Batch Number = 230
Evaluating:  88%|████████▊ | 230/260 [00:05<00:00, 43.76it/s]01/06/2022 17:17:37 - INFO - __main__ -   Batch Number = 231
01/06/2022 17:17:37 - INFO - __main__ -   Batch Number = 232
01/06/2022 17:17:37 - INFO - __main__ -   Batch Number = 233
01/06/2022 17:17:37 - INFO - __main__ -   Batch Number = 234
01/06/2022 17:17:37 - INFO - __main__ -   Batch Number = 235
Evaluating:  90%|█████████ | 235/260 [00:05<00:00, 43.75it/s]01/06/2022 17:17:37 - INFO - __main__ -   Batch Number = 236
01/06/2022 17:17:37 - INFO - __main__ -   Batch Number = 237
01/06/2022 17:17:37 - INFO - __main__ -   Batch Number = 238
01/06/2022 17:17:37 - INFO - __main__ -   Batch Number = 239
01/06/2022 17:17:37 - INFO - __main__ -   Batch Number = 240
Evaluating:  92%|█████████▏| 240/260 [00:05<00:00, 43.84it/s]01/06/2022 17:17:37 - INFO - __main__ -   Batch Number = 241
01/06/2022 17:17:37 - INFO - __main__ -   Batch Number = 242
01/06/2022 17:17:37 - INFO - __main__ -   Batch Number = 243
01/06/2022 17:17:37 - INFO - __main__ -   Batch Number = 244
01/06/2022 17:17:37 - INFO - __main__ -   Batch Number = 245
Evaluating:  94%|█████████▍| 245/260 [00:05<00:00, 43.89it/s]01/06/2022 17:17:37 - INFO - __main__ -   Batch Number = 246
01/06/2022 17:17:37 - INFO - __main__ -   Batch Number = 247
01/06/2022 17:17:37 - INFO - __main__ -   Batch Number = 248
01/06/2022 17:17:37 - INFO - __main__ -   Batch Number = 249
01/06/2022 17:17:37 - INFO - __main__ -   Batch Number = 250
Evaluating:  96%|█████████▌| 250/260 [00:05<00:00, 43.87it/s]01/06/2022 17:17:37 - INFO - __main__ -   Batch Number = 251
01/06/2022 17:17:37 - INFO - __main__ -   Batch Number = 252
01/06/2022 17:17:37 - INFO - __main__ -   Batch Number = 253
01/06/2022 17:17:37 - INFO - __main__ -   Batch Number = 254
01/06/2022 17:17:37 - INFO - __main__ -   Batch Number = 255
Evaluating:  98%|█████████▊| 255/260 [00:05<00:00, 44.01it/s]01/06/2022 17:17:37 - INFO - __main__ -   Batch Number = 256
01/06/2022 17:17:37 - INFO - __main__ -   Batch Number = 257
01/06/2022 17:17:37 - INFO - __main__ -   Batch Number = 258
01/06/2022 17:17:37 - INFO - __main__ -   Batch Number = 259
01/06/2022 17:17:38 - INFO - __main__ -   Batch Number = 260
Evaluating: 100%|██████████| 260/260 [00:05<00:00, 44.67it/s]Evaluating: 100%|██████████| 260/260 [00:05<00:00, 43.38it/s]
01/06/2022 17:17:38 - INFO - __main__ -     Evaluation done in total 5.993340 secs (0.002886 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/predictions_el_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/nbest_predictions_el_.json
01/06/2022 17:17:41 - INFO - __main__ -   Results = OrderedDict([('exact', 32.857142857142854), ('f1', 44.08174634801323), ('total', 1190), ('HasAns_exact', 32.857142857142854), ('HasAns_f1', 44.08174634801323), ('HasAns_total', 1190), ('best_exact', 32.857142857142854), ('best_exact_thresh', 0.0), ('best_f1', 44.08174634801323), ('best_f1_thresh', 0.0)])
01/06/2022 17:17:41 - INFO - __main__ -   Language adapter for es not found, using en instead
01/06/2022 17:17:41 - INFO - __main__ -   Set active language adapter to en
01/06/2022 17:17:41 - INFO - __main__ -   Args Adapter Weight = None
01/06/2022 17:17:41 - INFO - __main__ -   Adapter Languages = ['en']
01/06/2022 17:17:41 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
  0%|          | 0/48 [00:00<?, ?it/s] 33%|███▎      | 16/48 [00:00<00:00, 123.06it/s] 60%|██████    | 29/48 [00:00<00:00, 116.98it/s] 92%|█████████▏| 44/48 [00:00<00:00, 124.83it/s]100%|██████████| 48/48 [00:00<00:00, 125.22it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<07:35,  2.61it/s]convert squad examples to features:  24%|██▍       | 289/1190 [00:00<00:01, 734.38it/s]convert squad examples to features:  36%|███▌      | 425/1190 [00:01<00:01, 432.21it/s]convert squad examples to features:  86%|████████▌ | 1025/1190 [00:01<00:00, 1073.87it/s]convert squad examples to features: 100%|██████████| 1190/1190 [00:01<00:00, 927.05it/s] /root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|██████████| 1190/1190 [00:00<00:00, 754416.83it/s]
01/06/2022 17:17:43 - INFO - __main__ -   Local Rank = -1
01/06/2022 17:17:43 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.es.json_bert-base-multilingual-cased_384_es
01/06/2022 17:17:44 - INFO - __main__ -   ***** Running evaluation  *****
01/06/2022 17:17:44 - INFO - __main__ -     Num examples = 1307
01/06/2022 17:17:44 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/164 [00:00<?, ?it/s]01/06/2022 17:17:44 - INFO - __main__ -   Batch Number = 1
01/06/2022 17:17:44 - INFO - __main__ -   Batch Number = 2
01/06/2022 17:17:44 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|▏         | 3/164 [00:00<00:05, 29.62it/s]01/06/2022 17:17:44 - INFO - __main__ -   Batch Number = 4
01/06/2022 17:17:44 - INFO - __main__ -   Batch Number = 5
01/06/2022 17:17:44 - INFO - __main__ -   Batch Number = 6
Evaluating:   4%|▎         | 6/164 [00:00<00:05, 29.83it/s]01/06/2022 17:17:44 - INFO - __main__ -   Batch Number = 7
01/06/2022 17:17:45 - INFO - __main__ -   Batch Number = 8
01/06/2022 17:17:45 - INFO - __main__ -   Batch Number = 9
01/06/2022 17:17:45 - INFO - __main__ -   Batch Number = 10
Evaluating:   6%|▌         | 10/164 [00:00<00:04, 33.83it/s]01/06/2022 17:17:45 - INFO - __main__ -   Batch Number = 11
01/06/2022 17:17:45 - INFO - __main__ -   Batch Number = 12
01/06/2022 17:17:45 - INFO - __main__ -   Batch Number = 13
01/06/2022 17:17:45 - INFO - __main__ -   Batch Number = 14
01/06/2022 17:17:45 - INFO - __main__ -   Batch Number = 15
Evaluating:   9%|▉         | 15/164 [00:00<00:03, 38.41it/s]01/06/2022 17:17:45 - INFO - __main__ -   Batch Number = 16
01/06/2022 17:17:45 - INFO - __main__ -   Batch Number = 17
01/06/2022 17:17:45 - INFO - __main__ -   Batch Number = 18
01/06/2022 17:17:45 - INFO - __main__ -   Batch Number = 19
01/06/2022 17:17:45 - INFO - __main__ -   Batch Number = 20
Evaluating:  12%|█▏        | 20/164 [00:00<00:03, 40.44it/s]01/06/2022 17:17:45 - INFO - __main__ -   Batch Number = 21
01/06/2022 17:17:45 - INFO - __main__ -   Batch Number = 22
01/06/2022 17:17:45 - INFO - __main__ -   Batch Number = 23
01/06/2022 17:17:45 - INFO - __main__ -   Batch Number = 24
01/06/2022 17:17:45 - INFO - __main__ -   Batch Number = 25
Evaluating:  15%|█▌        | 25/164 [00:00<00:03, 41.93it/s]01/06/2022 17:17:45 - INFO - __main__ -   Batch Number = 26
01/06/2022 17:17:45 - INFO - __main__ -   Batch Number = 27
01/06/2022 17:17:45 - INFO - __main__ -   Batch Number = 28
01/06/2022 17:17:45 - INFO - __main__ -   Batch Number = 29
01/06/2022 17:17:45 - INFO - __main__ -   Batch Number = 30
Evaluating:  18%|█▊        | 30/164 [00:00<00:03, 42.81it/s]01/06/2022 17:17:45 - INFO - __main__ -   Batch Number = 31
01/06/2022 17:17:45 - INFO - __main__ -   Batch Number = 32
01/06/2022 17:17:45 - INFO - __main__ -   Batch Number = 33
01/06/2022 17:17:45 - INFO - __main__ -   Batch Number = 34
01/06/2022 17:17:45 - INFO - __main__ -   Batch Number = 35
Evaluating:  21%|██▏       | 35/164 [00:00<00:02, 43.20it/s]01/06/2022 17:17:45 - INFO - __main__ -   Batch Number = 36
01/06/2022 17:17:45 - INFO - __main__ -   Batch Number = 37
01/06/2022 17:17:45 - INFO - __main__ -   Batch Number = 38
01/06/2022 17:17:45 - INFO - __main__ -   Batch Number = 39
01/06/2022 17:17:45 - INFO - __main__ -   Batch Number = 40
Evaluating:  24%|██▍       | 40/164 [00:00<00:02, 43.65it/s]01/06/2022 17:17:45 - INFO - __main__ -   Batch Number = 41
01/06/2022 17:17:45 - INFO - __main__ -   Batch Number = 42
01/06/2022 17:17:45 - INFO - __main__ -   Batch Number = 43
01/06/2022 17:17:45 - INFO - __main__ -   Batch Number = 44
01/06/2022 17:17:45 - INFO - __main__ -   Batch Number = 45
Evaluating:  27%|██▋       | 45/164 [00:01<00:02, 43.78it/s]01/06/2022 17:17:45 - INFO - __main__ -   Batch Number = 46
01/06/2022 17:17:45 - INFO - __main__ -   Batch Number = 47
01/06/2022 17:17:45 - INFO - __main__ -   Batch Number = 48
01/06/2022 17:17:45 - INFO - __main__ -   Batch Number = 49
01/06/2022 17:17:45 - INFO - __main__ -   Batch Number = 50
Evaluating:  30%|███       | 50/164 [00:01<00:02, 43.93it/s]01/06/2022 17:17:46 - INFO - __main__ -   Batch Number = 51
01/06/2022 17:17:46 - INFO - __main__ -   Batch Number = 52
01/06/2022 17:17:46 - INFO - __main__ -   Batch Number = 53
01/06/2022 17:17:46 - INFO - __main__ -   Batch Number = 54
01/06/2022 17:17:46 - INFO - __main__ -   Batch Number = 55
Evaluating:  34%|███▎      | 55/164 [00:01<00:02, 44.04it/s]01/06/2022 17:17:46 - INFO - __main__ -   Batch Number = 56
01/06/2022 17:17:46 - INFO - __main__ -   Batch Number = 57
01/06/2022 17:17:46 - INFO - __main__ -   Batch Number = 58
01/06/2022 17:17:46 - INFO - __main__ -   Batch Number = 59
01/06/2022 17:17:46 - INFO - __main__ -   Batch Number = 60
Evaluating:  37%|███▋      | 60/164 [00:01<00:02, 44.17it/s]01/06/2022 17:17:46 - INFO - __main__ -   Batch Number = 61
01/06/2022 17:17:46 - INFO - __main__ -   Batch Number = 62
01/06/2022 17:17:46 - INFO - __main__ -   Batch Number = 63
01/06/2022 17:17:46 - INFO - __main__ -   Batch Number = 64
01/06/2022 17:17:46 - INFO - __main__ -   Batch Number = 65
Evaluating:  40%|███▉      | 65/164 [00:01<00:02, 44.16it/s]01/06/2022 17:17:46 - INFO - __main__ -   Batch Number = 66
01/06/2022 17:17:46 - INFO - __main__ -   Batch Number = 67
01/06/2022 17:17:46 - INFO - __main__ -   Batch Number = 68
01/06/2022 17:17:46 - INFO - __main__ -   Batch Number = 69
01/06/2022 17:17:46 - INFO - __main__ -   Batch Number = 70
Evaluating:  43%|████▎     | 70/164 [00:01<00:02, 44.21it/s]01/06/2022 17:17:46 - INFO - __main__ -   Batch Number = 71
01/06/2022 17:17:46 - INFO - __main__ -   Batch Number = 72
01/06/2022 17:17:46 - INFO - __main__ -   Batch Number = 73
01/06/2022 17:17:46 - INFO - __main__ -   Batch Number = 74
01/06/2022 17:17:46 - INFO - __main__ -   Batch Number = 75
Evaluating:  46%|████▌     | 75/164 [00:01<00:02, 44.29it/s]01/06/2022 17:17:46 - INFO - __main__ -   Batch Number = 76
01/06/2022 17:17:46 - INFO - __main__ -   Batch Number = 77
01/06/2022 17:17:46 - INFO - __main__ -   Batch Number = 78
01/06/2022 17:17:46 - INFO - __main__ -   Batch Number = 79
01/06/2022 17:17:46 - INFO - __main__ -   Batch Number = 80
Evaluating:  49%|████▉     | 80/164 [00:01<00:01, 42.68it/s]01/06/2022 17:17:46 - INFO - __main__ -   Batch Number = 81
01/06/2022 17:17:46 - INFO - __main__ -   Batch Number = 82
01/06/2022 17:17:46 - INFO - __main__ -   Batch Number = 83
01/06/2022 17:17:46 - INFO - __main__ -   Batch Number = 84
01/06/2022 17:17:46 - INFO - __main__ -   Batch Number = 85
Evaluating:  52%|█████▏    | 85/164 [00:02<00:01, 43.23it/s]01/06/2022 17:17:46 - INFO - __main__ -   Batch Number = 86
01/06/2022 17:17:46 - INFO - __main__ -   Batch Number = 87
01/06/2022 17:17:46 - INFO - __main__ -   Batch Number = 88
01/06/2022 17:17:46 - INFO - __main__ -   Batch Number = 89
01/06/2022 17:17:46 - INFO - __main__ -   Batch Number = 90
Evaluating:  55%|█████▍    | 90/164 [00:02<00:01, 43.61it/s]01/06/2022 17:17:46 - INFO - __main__ -   Batch Number = 91
01/06/2022 17:17:46 - INFO - __main__ -   Batch Number = 92
01/06/2022 17:17:46 - INFO - __main__ -   Batch Number = 93
01/06/2022 17:17:46 - INFO - __main__ -   Batch Number = 94
01/06/2022 17:17:47 - INFO - __main__ -   Batch Number = 95
Evaluating:  58%|█████▊    | 95/164 [00:02<00:01, 43.83it/s]01/06/2022 17:17:47 - INFO - __main__ -   Batch Number = 96
01/06/2022 17:17:47 - INFO - __main__ -   Batch Number = 97
01/06/2022 17:17:47 - INFO - __main__ -   Batch Number = 98
01/06/2022 17:17:47 - INFO - __main__ -   Batch Number = 99
01/06/2022 17:17:47 - INFO - __main__ -   Batch Number = 100
Evaluating:  61%|██████    | 100/164 [00:02<00:01, 44.02it/s]01/06/2022 17:17:47 - INFO - __main__ -   Batch Number = 101
01/06/2022 17:17:47 - INFO - __main__ -   Batch Number = 102
01/06/2022 17:17:47 - INFO - __main__ -   Batch Number = 103
01/06/2022 17:17:47 - INFO - __main__ -   Batch Number = 104
01/06/2022 17:17:47 - INFO - __main__ -   Batch Number = 105
Evaluating:  64%|██████▍   | 105/164 [00:02<00:01, 44.28it/s]01/06/2022 17:17:47 - INFO - __main__ -   Batch Number = 106
01/06/2022 17:17:47 - INFO - __main__ -   Batch Number = 107
01/06/2022 17:17:47 - INFO - __main__ -   Batch Number = 108
01/06/2022 17:17:47 - INFO - __main__ -   Batch Number = 109
01/06/2022 17:17:47 - INFO - __main__ -   Batch Number = 110
Evaluating:  67%|██████▋   | 110/164 [00:02<00:01, 44.27it/s]01/06/2022 17:17:47 - INFO - __main__ -   Batch Number = 111
01/06/2022 17:17:47 - INFO - __main__ -   Batch Number = 112
01/06/2022 17:17:47 - INFO - __main__ -   Batch Number = 113
01/06/2022 17:17:47 - INFO - __main__ -   Batch Number = 114
01/06/2022 17:17:47 - INFO - __main__ -   Batch Number = 115
Evaluating:  70%|███████   | 115/164 [00:02<00:01, 44.46it/s]01/06/2022 17:17:47 - INFO - __main__ -   Batch Number = 116
01/06/2022 17:17:47 - INFO - __main__ -   Batch Number = 117
01/06/2022 17:17:47 - INFO - __main__ -   Batch Number = 118
01/06/2022 17:17:47 - INFO - __main__ -   Batch Number = 119
01/06/2022 17:17:47 - INFO - __main__ -   Batch Number = 120
Evaluating:  73%|███████▎  | 120/164 [00:02<00:00, 44.51it/s]01/06/2022 17:17:47 - INFO - __main__ -   Batch Number = 121
01/06/2022 17:17:47 - INFO - __main__ -   Batch Number = 122
01/06/2022 17:17:47 - INFO - __main__ -   Batch Number = 123
01/06/2022 17:17:47 - INFO - __main__ -   Batch Number = 124
01/06/2022 17:17:47 - INFO - __main__ -   Batch Number = 125
Evaluating:  76%|███████▌  | 125/164 [00:02<00:00, 44.52it/s]01/06/2022 17:17:47 - INFO - __main__ -   Batch Number = 126
01/06/2022 17:17:47 - INFO - __main__ -   Batch Number = 127
01/06/2022 17:17:47 - INFO - __main__ -   Batch Number = 128
01/06/2022 17:17:47 - INFO - __main__ -   Batch Number = 129
01/06/2022 17:17:47 - INFO - __main__ -   Batch Number = 130
Evaluating:  79%|███████▉  | 130/164 [00:03<00:00, 44.55it/s]01/06/2022 17:17:47 - INFO - __main__ -   Batch Number = 131
01/06/2022 17:17:47 - INFO - __main__ -   Batch Number = 132
01/06/2022 17:17:47 - INFO - __main__ -   Batch Number = 133
01/06/2022 17:17:47 - INFO - __main__ -   Batch Number = 134
01/06/2022 17:17:47 - INFO - __main__ -   Batch Number = 135
Evaluating:  82%|████████▏ | 135/164 [00:03<00:00, 44.60it/s]01/06/2022 17:17:47 - INFO - __main__ -   Batch Number = 136
01/06/2022 17:17:47 - INFO - __main__ -   Batch Number = 137
01/06/2022 17:17:47 - INFO - __main__ -   Batch Number = 138
01/06/2022 17:17:47 - INFO - __main__ -   Batch Number = 139
01/06/2022 17:17:48 - INFO - __main__ -   Batch Number = 140
Evaluating:  85%|████████▌ | 140/164 [00:03<00:00, 44.44it/s]01/06/2022 17:17:48 - INFO - __main__ -   Batch Number = 141
01/06/2022 17:17:48 - INFO - __main__ -   Batch Number = 142
01/06/2022 17:17:48 - INFO - __main__ -   Batch Number = 143
01/06/2022 17:17:48 - INFO - __main__ -   Batch Number = 144
01/06/2022 17:17:48 - INFO - __main__ -   Batch Number = 145
Evaluating:  88%|████████▊ | 145/164 [00:03<00:00, 44.50it/s]01/06/2022 17:17:48 - INFO - __main__ -   Batch Number = 146
01/06/2022 17:17:48 - INFO - __main__ -   Batch Number = 147
01/06/2022 17:17:48 - INFO - __main__ -   Batch Number = 148
01/06/2022 17:17:48 - INFO - __main__ -   Batch Number = 149
01/06/2022 17:17:48 - INFO - __main__ -   Batch Number = 150
Evaluating:  91%|█████████▏| 150/164 [00:03<00:00, 44.52it/s]01/06/2022 17:17:48 - INFO - __main__ -   Batch Number = 151
01/06/2022 17:17:48 - INFO - __main__ -   Batch Number = 152
01/06/2022 17:17:48 - INFO - __main__ -   Batch Number = 153
01/06/2022 17:17:48 - INFO - __main__ -   Batch Number = 154
01/06/2022 17:17:48 - INFO - __main__ -   Batch Number = 155
Evaluating:  95%|█████████▍| 155/164 [00:03<00:00, 44.37it/s]01/06/2022 17:17:48 - INFO - __main__ -   Batch Number = 156
01/06/2022 17:17:48 - INFO - __main__ -   Batch Number = 157
01/06/2022 17:17:48 - INFO - __main__ -   Batch Number = 158
01/06/2022 17:17:48 - INFO - __main__ -   Batch Number = 159
01/06/2022 17:17:48 - INFO - __main__ -   Batch Number = 160
Evaluating:  98%|█████████▊| 160/164 [00:03<00:00, 44.22it/s]01/06/2022 17:17:48 - INFO - __main__ -   Batch Number = 161
01/06/2022 17:17:48 - INFO - __main__ -   Batch Number = 162
01/06/2022 17:17:48 - INFO - __main__ -   Batch Number = 163
01/06/2022 17:17:48 - INFO - __main__ -   Batch Number = 164
Evaluating: 100%|██████████| 164/164 [00:03<00:00, 43.37it/s]
01/06/2022 17:17:48 - INFO - __main__ -     Evaluation done in total 3.782345 secs (0.002894 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/predictions_es_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/nbest_predictions_es_.json
01/06/2022 17:17:52 - INFO - __main__ -   Results = OrderedDict([('exact', 53.69747899159664), ('f1', 71.58981202906249), ('total', 1190), ('HasAns_exact', 53.69747899159664), ('HasAns_f1', 71.58981202906249), ('HasAns_total', 1190), ('best_exact', 53.69747899159664), ('best_exact_thresh', 0.0), ('best_f1', 71.58981202906249), ('best_f1_thresh', 0.0)])
01/06/2022 17:17:52 - INFO - __main__ -   Language adapter for hi not found, using en instead
01/06/2022 17:17:52 - INFO - __main__ -   Set active language adapter to en
01/06/2022 17:17:52 - INFO - __main__ -   Args Adapter Weight = None
01/06/2022 17:17:52 - INFO - __main__ -   Adapter Languages = ['en']
01/06/2022 17:17:52 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
  0%|          | 0/48 [00:00<?, ?it/s] 33%|███▎      | 16/48 [00:00<00:00, 134.54it/s] 62%|██████▎   | 30/48 [00:00<00:00, 125.97it/s] 96%|█████████▌| 46/48 [00:00<00:00, 137.73it/s]100%|██████████| 48/48 [00:00<00:00, 136.30it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<06:59,  2.84it/s]convert squad examples to features:  27%|██▋       | 321/1190 [00:00<00:01, 750.89it/s]convert squad examples to features:  36%|███▌      | 426/1190 [00:01<00:02, 371.51it/s]convert squad examples to features:  78%|███████▊  | 929/1190 [00:01<00:00, 1013.65it/s]convert squad examples to features:  98%|█████████▊| 1162/1190 [00:01<00:00, 916.16it/s]convert squad examples to features: 100%|██████████| 1190/1190 [00:01<00:00, 776.32it/s]/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|██████████| 1190/1190 [00:00<00:00, 459385.34it/s]
01/06/2022 17:17:54 - INFO - __main__ -   Local Rank = -1
01/06/2022 17:17:54 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.hi.json_bert-base-multilingual-cased_384_hi
01/06/2022 17:17:56 - INFO - __main__ -   ***** Running evaluation  *****
01/06/2022 17:17:56 - INFO - __main__ -     Num examples = 1629
01/06/2022 17:17:56 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/204 [00:00<?, ?it/s]01/06/2022 17:17:56 - INFO - __main__ -   Batch Number = 1
01/06/2022 17:17:56 - INFO - __main__ -   Batch Number = 2
01/06/2022 17:17:56 - INFO - __main__ -   Batch Number = 3
Evaluating:   1%|▏         | 3/204 [00:00<00:07, 25.34it/s]01/06/2022 17:17:56 - INFO - __main__ -   Batch Number = 4
01/06/2022 17:17:56 - INFO - __main__ -   Batch Number = 5
01/06/2022 17:17:56 - INFO - __main__ -   Batch Number = 6
Evaluating:   3%|▎         | 6/204 [00:00<00:07, 27.14it/s]01/06/2022 17:17:56 - INFO - __main__ -   Batch Number = 7
01/06/2022 17:17:56 - INFO - __main__ -   Batch Number = 8
01/06/2022 17:17:56 - INFO - __main__ -   Batch Number = 9
01/06/2022 17:17:56 - INFO - __main__ -   Batch Number = 10
01/06/2022 17:17:56 - INFO - __main__ -   Batch Number = 11
Evaluating:   5%|▌         | 11/204 [00:00<00:05, 35.11it/s]01/06/2022 17:17:56 - INFO - __main__ -   Batch Number = 12
01/06/2022 17:17:56 - INFO - __main__ -   Batch Number = 13
01/06/2022 17:17:56 - INFO - __main__ -   Batch Number = 14
01/06/2022 17:17:56 - INFO - __main__ -   Batch Number = 15
01/06/2022 17:17:56 - INFO - __main__ -   Batch Number = 16
Evaluating:   8%|▊         | 16/204 [00:00<00:04, 38.86it/s]01/06/2022 17:17:56 - INFO - __main__ -   Batch Number = 17
01/06/2022 17:17:56 - INFO - __main__ -   Batch Number = 18
01/06/2022 17:17:56 - INFO - __main__ -   Batch Number = 19
01/06/2022 17:17:56 - INFO - __main__ -   Batch Number = 20
01/06/2022 17:17:56 - INFO - __main__ -   Batch Number = 21
Evaluating:  10%|█         | 21/204 [00:00<00:04, 40.84it/s]01/06/2022 17:17:56 - INFO - __main__ -   Batch Number = 22
01/06/2022 17:17:56 - INFO - __main__ -   Batch Number = 23
01/06/2022 17:17:56 - INFO - __main__ -   Batch Number = 24
01/06/2022 17:17:56 - INFO - __main__ -   Batch Number = 25
01/06/2022 17:17:56 - INFO - __main__ -   Batch Number = 26
Evaluating:  13%|█▎        | 26/204 [00:00<00:04, 42.02it/s]01/06/2022 17:17:56 - INFO - __main__ -   Batch Number = 27
01/06/2022 17:17:56 - INFO - __main__ -   Batch Number = 28
01/06/2022 17:17:56 - INFO - __main__ -   Batch Number = 29
01/06/2022 17:17:56 - INFO - __main__ -   Batch Number = 30
01/06/2022 17:17:56 - INFO - __main__ -   Batch Number = 31
Evaluating:  15%|█▌        | 31/204 [00:00<00:04, 42.86it/s]01/06/2022 17:17:56 - INFO - __main__ -   Batch Number = 32
01/06/2022 17:17:56 - INFO - __main__ -   Batch Number = 33
01/06/2022 17:17:56 - INFO - __main__ -   Batch Number = 34
01/06/2022 17:17:56 - INFO - __main__ -   Batch Number = 35
01/06/2022 17:17:56 - INFO - __main__ -   Batch Number = 36
Evaluating:  18%|█▊        | 36/204 [00:00<00:03, 43.25it/s]01/06/2022 17:17:57 - INFO - __main__ -   Batch Number = 37
01/06/2022 17:17:57 - INFO - __main__ -   Batch Number = 38
01/06/2022 17:17:57 - INFO - __main__ -   Batch Number = 39
01/06/2022 17:17:57 - INFO - __main__ -   Batch Number = 40
01/06/2022 17:17:57 - INFO - __main__ -   Batch Number = 41
Evaluating:  20%|██        | 41/204 [00:01<00:03, 43.62it/s]01/06/2022 17:17:57 - INFO - __main__ -   Batch Number = 42
01/06/2022 17:17:57 - INFO - __main__ -   Batch Number = 43
01/06/2022 17:17:57 - INFO - __main__ -   Batch Number = 44
01/06/2022 17:17:57 - INFO - __main__ -   Batch Number = 45
01/06/2022 17:17:57 - INFO - __main__ -   Batch Number = 46
Evaluating:  23%|██▎       | 46/204 [00:01<00:03, 43.91it/s]01/06/2022 17:17:57 - INFO - __main__ -   Batch Number = 47
01/06/2022 17:17:57 - INFO - __main__ -   Batch Number = 48
01/06/2022 17:17:57 - INFO - __main__ -   Batch Number = 49
01/06/2022 17:17:57 - INFO - __main__ -   Batch Number = 50
01/06/2022 17:17:57 - INFO - __main__ -   Batch Number = 51
Evaluating:  25%|██▌       | 51/204 [00:01<00:03, 43.90it/s]01/06/2022 17:17:57 - INFO - __main__ -   Batch Number = 52
01/06/2022 17:17:57 - INFO - __main__ -   Batch Number = 53
01/06/2022 17:17:57 - INFO - __main__ -   Batch Number = 54
01/06/2022 17:17:57 - INFO - __main__ -   Batch Number = 55
01/06/2022 17:17:57 - INFO - __main__ -   Batch Number = 56
Evaluating:  27%|██▋       | 56/204 [00:01<00:03, 44.13it/s]01/06/2022 17:17:57 - INFO - __main__ -   Batch Number = 57
01/06/2022 17:17:57 - INFO - __main__ -   Batch Number = 58
01/06/2022 17:17:57 - INFO - __main__ -   Batch Number = 59
01/06/2022 17:17:57 - INFO - __main__ -   Batch Number = 60
01/06/2022 17:17:57 - INFO - __main__ -   Batch Number = 61
Evaluating:  30%|██▉       | 61/204 [00:01<00:03, 44.11it/s]01/06/2022 17:17:57 - INFO - __main__ -   Batch Number = 62
01/06/2022 17:17:57 - INFO - __main__ -   Batch Number = 63
01/06/2022 17:17:57 - INFO - __main__ -   Batch Number = 64
01/06/2022 17:17:57 - INFO - __main__ -   Batch Number = 65
01/06/2022 17:17:57 - INFO - __main__ -   Batch Number = 66
Evaluating:  32%|███▏      | 66/204 [00:01<00:03, 44.14it/s]01/06/2022 17:17:57 - INFO - __main__ -   Batch Number = 67
01/06/2022 17:17:57 - INFO - __main__ -   Batch Number = 68
01/06/2022 17:17:57 - INFO - __main__ -   Batch Number = 69
01/06/2022 17:17:57 - INFO - __main__ -   Batch Number = 70
01/06/2022 17:17:57 - INFO - __main__ -   Batch Number = 71
Evaluating:  35%|███▍      | 71/204 [00:01<00:03, 44.17it/s]01/06/2022 17:17:57 - INFO - __main__ -   Batch Number = 72
01/06/2022 17:17:57 - INFO - __main__ -   Batch Number = 73
01/06/2022 17:17:57 - INFO - __main__ -   Batch Number = 74
01/06/2022 17:17:57 - INFO - __main__ -   Batch Number = 75
01/06/2022 17:17:57 - INFO - __main__ -   Batch Number = 76
Evaluating:  37%|███▋      | 76/204 [00:01<00:02, 44.25it/s]01/06/2022 17:17:57 - INFO - __main__ -   Batch Number = 77
01/06/2022 17:17:57 - INFO - __main__ -   Batch Number = 78
01/06/2022 17:17:57 - INFO - __main__ -   Batch Number = 79
01/06/2022 17:17:57 - INFO - __main__ -   Batch Number = 80
01/06/2022 17:17:57 - INFO - __main__ -   Batch Number = 81
Evaluating:  40%|███▉      | 81/204 [00:01<00:02, 44.29it/s]01/06/2022 17:17:58 - INFO - __main__ -   Batch Number = 82
01/06/2022 17:17:58 - INFO - __main__ -   Batch Number = 83
01/06/2022 17:17:58 - INFO - __main__ -   Batch Number = 84
01/06/2022 17:17:58 - INFO - __main__ -   Batch Number = 85
01/06/2022 17:17:58 - INFO - __main__ -   Batch Number = 86
Evaluating:  42%|████▏     | 86/204 [00:02<00:02, 44.25it/s]01/06/2022 17:17:58 - INFO - __main__ -   Batch Number = 87
01/06/2022 17:17:58 - INFO - __main__ -   Batch Number = 88
01/06/2022 17:17:58 - INFO - __main__ -   Batch Number = 89
01/06/2022 17:17:58 - INFO - __main__ -   Batch Number = 90
01/06/2022 17:17:58 - INFO - __main__ -   Batch Number = 91
Evaluating:  45%|████▍     | 91/204 [00:02<00:02, 44.33it/s]01/06/2022 17:17:58 - INFO - __main__ -   Batch Number = 92
01/06/2022 17:17:58 - INFO - __main__ -   Batch Number = 93
01/06/2022 17:17:58 - INFO - __main__ -   Batch Number = 94
01/06/2022 17:17:58 - INFO - __main__ -   Batch Number = 95
01/06/2022 17:17:58 - INFO - __main__ -   Batch Number = 96
Evaluating:  47%|████▋     | 96/204 [00:02<00:02, 44.22it/s]01/06/2022 17:17:58 - INFO - __main__ -   Batch Number = 97
01/06/2022 17:17:58 - INFO - __main__ -   Batch Number = 98
01/06/2022 17:17:58 - INFO - __main__ -   Batch Number = 99
01/06/2022 17:17:58 - INFO - __main__ -   Batch Number = 100
01/06/2022 17:17:58 - INFO - __main__ -   Batch Number = 101
Evaluating:  50%|████▉     | 101/204 [00:02<00:02, 44.24it/s]01/06/2022 17:17:58 - INFO - __main__ -   Batch Number = 102
01/06/2022 17:17:58 - INFO - __main__ -   Batch Number = 103
01/06/2022 17:17:58 - INFO - __main__ -   Batch Number = 104
01/06/2022 17:17:58 - INFO - __main__ -   Batch Number = 105
01/06/2022 17:17:58 - INFO - __main__ -   Batch Number = 106
Evaluating:  52%|█████▏    | 106/204 [00:02<00:02, 44.30it/s]01/06/2022 17:17:58 - INFO - __main__ -   Batch Number = 107
01/06/2022 17:17:58 - INFO - __main__ -   Batch Number = 108
01/06/2022 17:17:58 - INFO - __main__ -   Batch Number = 109
01/06/2022 17:17:58 - INFO - __main__ -   Batch Number = 110
01/06/2022 17:17:58 - INFO - __main__ -   Batch Number = 111
Evaluating:  54%|█████▍    | 111/204 [00:02<00:02, 44.18it/s]01/06/2022 17:17:58 - INFO - __main__ -   Batch Number = 112
01/06/2022 17:17:58 - INFO - __main__ -   Batch Number = 113
01/06/2022 17:17:58 - INFO - __main__ -   Batch Number = 114
01/06/2022 17:17:58 - INFO - __main__ -   Batch Number = 115
01/06/2022 17:17:58 - INFO - __main__ -   Batch Number = 116
Evaluating:  57%|█████▋    | 116/204 [00:02<00:01, 44.33it/s]01/06/2022 17:17:58 - INFO - __main__ -   Batch Number = 117
01/06/2022 17:17:58 - INFO - __main__ -   Batch Number = 118
01/06/2022 17:17:58 - INFO - __main__ -   Batch Number = 119
01/06/2022 17:17:58 - INFO - __main__ -   Batch Number = 120
01/06/2022 17:17:58 - INFO - __main__ -   Batch Number = 121
Evaluating:  59%|█████▉    | 121/204 [00:02<00:01, 44.22it/s]01/06/2022 17:17:58 - INFO - __main__ -   Batch Number = 122
01/06/2022 17:17:58 - INFO - __main__ -   Batch Number = 123
01/06/2022 17:17:58 - INFO - __main__ -   Batch Number = 124
01/06/2022 17:17:58 - INFO - __main__ -   Batch Number = 125
01/06/2022 17:17:59 - INFO - __main__ -   Batch Number = 126
Evaluating:  62%|██████▏   | 126/204 [00:02<00:01, 44.16it/s]01/06/2022 17:17:59 - INFO - __main__ -   Batch Number = 127
01/06/2022 17:17:59 - INFO - __main__ -   Batch Number = 128
01/06/2022 17:17:59 - INFO - __main__ -   Batch Number = 129
01/06/2022 17:17:59 - INFO - __main__ -   Batch Number = 130
01/06/2022 17:17:59 - INFO - __main__ -   Batch Number = 131
Evaluating:  64%|██████▍   | 131/204 [00:03<00:01, 44.15it/s]01/06/2022 17:17:59 - INFO - __main__ -   Batch Number = 132
01/06/2022 17:17:59 - INFO - __main__ -   Batch Number = 133
01/06/2022 17:17:59 - INFO - __main__ -   Batch Number = 134
01/06/2022 17:17:59 - INFO - __main__ -   Batch Number = 135
01/06/2022 17:17:59 - INFO - __main__ -   Batch Number = 136
Evaluating:  67%|██████▋   | 136/204 [00:03<00:01, 44.21it/s]01/06/2022 17:17:59 - INFO - __main__ -   Batch Number = 137
01/06/2022 17:17:59 - INFO - __main__ -   Batch Number = 138
01/06/2022 17:17:59 - INFO - __main__ -   Batch Number = 139
01/06/2022 17:17:59 - INFO - __main__ -   Batch Number = 140
01/06/2022 17:17:59 - INFO - __main__ -   Batch Number = 141
Evaluating:  69%|██████▉   | 141/204 [00:03<00:01, 42.71it/s]01/06/2022 17:17:59 - INFO - __main__ -   Batch Number = 142
01/06/2022 17:17:59 - INFO - __main__ -   Batch Number = 143
01/06/2022 17:17:59 - INFO - __main__ -   Batch Number = 144
01/06/2022 17:17:59 - INFO - __main__ -   Batch Number = 145
01/06/2022 17:17:59 - INFO - __main__ -   Batch Number = 146
Evaluating:  72%|███████▏  | 146/204 [00:03<00:01, 43.22it/s]01/06/2022 17:17:59 - INFO - __main__ -   Batch Number = 147
01/06/2022 17:17:59 - INFO - __main__ -   Batch Number = 148
01/06/2022 17:17:59 - INFO - __main__ -   Batch Number = 149
01/06/2022 17:17:59 - INFO - __main__ -   Batch Number = 150
01/06/2022 17:17:59 - INFO - __main__ -   Batch Number = 151
Evaluating:  74%|███████▍  | 151/204 [00:03<00:01, 43.52it/s]01/06/2022 17:17:59 - INFO - __main__ -   Batch Number = 152
01/06/2022 17:17:59 - INFO - __main__ -   Batch Number = 153
01/06/2022 17:17:59 - INFO - __main__ -   Batch Number = 154
01/06/2022 17:17:59 - INFO - __main__ -   Batch Number = 155
01/06/2022 17:17:59 - INFO - __main__ -   Batch Number = 156
Evaluating:  76%|███████▋  | 156/204 [00:03<00:01, 43.60it/s]01/06/2022 17:17:59 - INFO - __main__ -   Batch Number = 157
01/06/2022 17:17:59 - INFO - __main__ -   Batch Number = 158
01/06/2022 17:17:59 - INFO - __main__ -   Batch Number = 159
01/06/2022 17:17:59 - INFO - __main__ -   Batch Number = 160
01/06/2022 17:17:59 - INFO - __main__ -   Batch Number = 161
Evaluating:  79%|███████▉  | 161/204 [00:03<00:00, 43.87it/s]01/06/2022 17:17:59 - INFO - __main__ -   Batch Number = 162
01/06/2022 17:17:59 - INFO - __main__ -   Batch Number = 163
01/06/2022 17:17:59 - INFO - __main__ -   Batch Number = 164
01/06/2022 17:17:59 - INFO - __main__ -   Batch Number = 165
01/06/2022 17:17:59 - INFO - __main__ -   Batch Number = 166
Evaluating:  81%|████████▏ | 166/204 [00:03<00:00, 43.93it/s]01/06/2022 17:17:59 - INFO - __main__ -   Batch Number = 167
01/06/2022 17:17:59 - INFO - __main__ -   Batch Number = 168
01/06/2022 17:17:59 - INFO - __main__ -   Batch Number = 169
01/06/2022 17:18:00 - INFO - __main__ -   Batch Number = 170
01/06/2022 17:18:00 - INFO - __main__ -   Batch Number = 171
Evaluating:  84%|████████▍ | 171/204 [00:03<00:00, 43.91it/s]01/06/2022 17:18:00 - INFO - __main__ -   Batch Number = 172
01/06/2022 17:18:00 - INFO - __main__ -   Batch Number = 173
01/06/2022 17:18:00 - INFO - __main__ -   Batch Number = 174
01/06/2022 17:18:00 - INFO - __main__ -   Batch Number = 175
01/06/2022 17:18:00 - INFO - __main__ -   Batch Number = 176
Evaluating:  86%|████████▋ | 176/204 [00:04<00:00, 44.02it/s]01/06/2022 17:18:00 - INFO - __main__ -   Batch Number = 177
01/06/2022 17:18:00 - INFO - __main__ -   Batch Number = 178
01/06/2022 17:18:00 - INFO - __main__ -   Batch Number = 179
01/06/2022 17:18:00 - INFO - __main__ -   Batch Number = 180
01/06/2022 17:18:00 - INFO - __main__ -   Batch Number = 181
Evaluating:  89%|████████▊ | 181/204 [00:04<00:00, 44.03it/s]01/06/2022 17:18:00 - INFO - __main__ -   Batch Number = 182
01/06/2022 17:18:00 - INFO - __main__ -   Batch Number = 183
01/06/2022 17:18:00 - INFO - __main__ -   Batch Number = 184
01/06/2022 17:18:00 - INFO - __main__ -   Batch Number = 185
01/06/2022 17:18:00 - INFO - __main__ -   Batch Number = 186
Evaluating:  91%|█████████ | 186/204 [00:04<00:00, 44.08it/s]01/06/2022 17:18:00 - INFO - __main__ -   Batch Number = 187
01/06/2022 17:18:00 - INFO - __main__ -   Batch Number = 188
01/06/2022 17:18:00 - INFO - __main__ -   Batch Number = 189
01/06/2022 17:18:00 - INFO - __main__ -   Batch Number = 190
01/06/2022 17:18:00 - INFO - __main__ -   Batch Number = 191
Evaluating:  94%|█████████▎| 191/204 [00:04<00:00, 44.08it/s]01/06/2022 17:18:00 - INFO - __main__ -   Batch Number = 192
01/06/2022 17:18:00 - INFO - __main__ -   Batch Number = 193
01/06/2022 17:18:00 - INFO - __main__ -   Batch Number = 194
01/06/2022 17:18:00 - INFO - __main__ -   Batch Number = 195
01/06/2022 17:18:00 - INFO - __main__ -   Batch Number = 196
Evaluating:  96%|█████████▌| 196/204 [00:04<00:00, 44.13it/s]01/06/2022 17:18:00 - INFO - __main__ -   Batch Number = 197
01/06/2022 17:18:00 - INFO - __main__ -   Batch Number = 198
01/06/2022 17:18:00 - INFO - __main__ -   Batch Number = 199
01/06/2022 17:18:00 - INFO - __main__ -   Batch Number = 200
01/06/2022 17:18:00 - INFO - __main__ -   Batch Number = 201
Evaluating:  99%|█████████▊| 201/204 [00:04<00:00, 44.06it/s]01/06/2022 17:18:00 - INFO - __main__ -   Batch Number = 202
01/06/2022 17:18:00 - INFO - __main__ -   Batch Number = 203
01/06/2022 17:18:00 - INFO - __main__ -   Batch Number = 204
Evaluating: 100%|██████████| 204/204 [00:04<00:00, 43.35it/s]
01/06/2022 17:18:00 - INFO - __main__ -     Evaluation done in total 4.706551 secs (0.002889 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/predictions_hi_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/nbest_predictions_hi_.json
01/06/2022 17:18:03 - INFO - __main__ -   Results = OrderedDict([('exact', 36.38655462184874), ('f1', 48.586344228565494), ('total', 1190), ('HasAns_exact', 36.38655462184874), ('HasAns_f1', 48.586344228565494), ('HasAns_total', 1190), ('best_exact', 36.38655462184874), ('best_exact_thresh', 0.0), ('best_f1', 48.586344228565494), ('best_f1_thresh', 0.0)])
01/06/2022 17:18:03 - INFO - __main__ -   Language adapter for ru not found, using en instead
01/06/2022 17:18:03 - INFO - __main__ -   Set active language adapter to en
01/06/2022 17:18:03 - INFO - __main__ -   Args Adapter Weight = None
01/06/2022 17:18:03 - INFO - __main__ -   Adapter Languages = ['en']
01/06/2022 17:18:03 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
  0%|          | 0/48 [00:00<?, ?it/s] 31%|███▏      | 15/48 [00:00<00:00, 147.57it/s] 62%|██████▎   | 30/48 [00:00<00:00, 108.81it/s] 92%|█████████▏| 44/48 [00:00<00:00, 118.50it/s]100%|██████████| 48/48 [00:00<00:00, 120.53it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<08:23,  2.36it/s]convert squad examples to features:  32%|███▏      | 385/1190 [00:01<00:02, 352.36it/s]convert squad examples to features:  94%|█████████▍| 1121/1190 [00:01<00:00, 1097.53it/s]convert squad examples to features: 100%|██████████| 1190/1190 [00:01<00:00, 847.25it/s] /root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|██████████| 1190/1190 [00:00<00:00, 696806.05it/s]
01/06/2022 17:18:06 - INFO - __main__ -   Local Rank = -1
01/06/2022 17:18:06 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.ru.json_bert-base-multilingual-cased_384_ru
01/06/2022 17:18:07 - INFO - __main__ -   ***** Running evaluation  *****
01/06/2022 17:18:07 - INFO - __main__ -     Num examples = 1410
01/06/2022 17:18:07 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/177 [00:00<?, ?it/s]01/06/2022 17:18:07 - INFO - __main__ -   Batch Number = 1
01/06/2022 17:18:07 - INFO - __main__ -   Batch Number = 2
01/06/2022 17:18:07 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|▏         | 3/177 [00:00<00:05, 29.88it/s]01/06/2022 17:18:07 - INFO - __main__ -   Batch Number = 4
01/06/2022 17:18:07 - INFO - __main__ -   Batch Number = 5
01/06/2022 17:18:07 - INFO - __main__ -   Batch Number = 6
01/06/2022 17:18:07 - INFO - __main__ -   Batch Number = 7
Evaluating:   4%|▍         | 7/177 [00:00<00:05, 32.61it/s]01/06/2022 17:18:07 - INFO - __main__ -   Batch Number = 8
01/06/2022 17:18:07 - INFO - __main__ -   Batch Number = 9
01/06/2022 17:18:07 - INFO - __main__ -   Batch Number = 10
01/06/2022 17:18:07 - INFO - __main__ -   Batch Number = 11
01/06/2022 17:18:07 - INFO - __main__ -   Batch Number = 12
Evaluating:   7%|▋         | 12/177 [00:00<00:04, 38.19it/s]01/06/2022 17:18:07 - INFO - __main__ -   Batch Number = 13
01/06/2022 17:18:07 - INFO - __main__ -   Batch Number = 14
01/06/2022 17:18:07 - INFO - __main__ -   Batch Number = 15
01/06/2022 17:18:07 - INFO - __main__ -   Batch Number = 16
01/06/2022 17:18:07 - INFO - __main__ -   Batch Number = 17
Evaluating:  10%|▉         | 17/177 [00:00<00:03, 40.78it/s]01/06/2022 17:18:07 - INFO - __main__ -   Batch Number = 18
01/06/2022 17:18:07 - INFO - __main__ -   Batch Number = 19
01/06/2022 17:18:07 - INFO - __main__ -   Batch Number = 20
01/06/2022 17:18:07 - INFO - __main__ -   Batch Number = 21
01/06/2022 17:18:07 - INFO - __main__ -   Batch Number = 22
Evaluating:  12%|█▏        | 22/177 [00:00<00:03, 42.07it/s]01/06/2022 17:18:07 - INFO - __main__ -   Batch Number = 23
01/06/2022 17:18:08 - INFO - __main__ -   Batch Number = 24
01/06/2022 17:18:08 - INFO - __main__ -   Batch Number = 25
01/06/2022 17:18:08 - INFO - __main__ -   Batch Number = 26
01/06/2022 17:18:08 - INFO - __main__ -   Batch Number = 27
Evaluating:  15%|█▌        | 27/177 [00:00<00:03, 43.09it/s]01/06/2022 17:18:08 - INFO - __main__ -   Batch Number = 28
01/06/2022 17:18:08 - INFO - __main__ -   Batch Number = 29
01/06/2022 17:18:08 - INFO - __main__ -   Batch Number = 30
01/06/2022 17:18:08 - INFO - __main__ -   Batch Number = 31
01/06/2022 17:18:08 - INFO - __main__ -   Batch Number = 32
Evaluating:  18%|█▊        | 32/177 [00:00<00:03, 43.42it/s]01/06/2022 17:18:08 - INFO - __main__ -   Batch Number = 33
01/06/2022 17:18:08 - INFO - __main__ -   Batch Number = 34
01/06/2022 17:18:08 - INFO - __main__ -   Batch Number = 35
01/06/2022 17:18:08 - INFO - __main__ -   Batch Number = 36
01/06/2022 17:18:08 - INFO - __main__ -   Batch Number = 37
Evaluating:  21%|██        | 37/177 [00:00<00:03, 43.84it/s]01/06/2022 17:18:08 - INFO - __main__ -   Batch Number = 38
01/06/2022 17:18:08 - INFO - __main__ -   Batch Number = 39
01/06/2022 17:18:08 - INFO - __main__ -   Batch Number = 40
01/06/2022 17:18:08 - INFO - __main__ -   Batch Number = 41
01/06/2022 17:18:08 - INFO - __main__ -   Batch Number = 42
Evaluating:  24%|██▎       | 42/177 [00:01<00:03, 44.02it/s]01/06/2022 17:18:08 - INFO - __main__ -   Batch Number = 43
01/06/2022 17:18:08 - INFO - __main__ -   Batch Number = 44
01/06/2022 17:18:08 - INFO - __main__ -   Batch Number = 45
01/06/2022 17:18:08 - INFO - __main__ -   Batch Number = 46
01/06/2022 17:18:08 - INFO - __main__ -   Batch Number = 47
Evaluating:  27%|██▋       | 47/177 [00:01<00:02, 44.19it/s]01/06/2022 17:18:08 - INFO - __main__ -   Batch Number = 48
01/06/2022 17:18:08 - INFO - __main__ -   Batch Number = 49
01/06/2022 17:18:08 - INFO - __main__ -   Batch Number = 50
01/06/2022 17:18:08 - INFO - __main__ -   Batch Number = 51
01/06/2022 17:18:08 - INFO - __main__ -   Batch Number = 52
Evaluating:  29%|██▉       | 52/177 [00:01<00:02, 43.99it/s]01/06/2022 17:18:08 - INFO - __main__ -   Batch Number = 53
01/06/2022 17:18:08 - INFO - __main__ -   Batch Number = 54
01/06/2022 17:18:08 - INFO - __main__ -   Batch Number = 55
01/06/2022 17:18:08 - INFO - __main__ -   Batch Number = 56
01/06/2022 17:18:08 - INFO - __main__ -   Batch Number = 57
Evaluating:  32%|███▏      | 57/177 [00:01<00:02, 44.19it/s]01/06/2022 17:18:08 - INFO - __main__ -   Batch Number = 58
01/06/2022 17:18:08 - INFO - __main__ -   Batch Number = 59
01/06/2022 17:18:08 - INFO - __main__ -   Batch Number = 60
01/06/2022 17:18:08 - INFO - __main__ -   Batch Number = 61
01/06/2022 17:18:08 - INFO - __main__ -   Batch Number = 62
Evaluating:  35%|███▌      | 62/177 [00:01<00:02, 42.87it/s]01/06/2022 17:18:08 - INFO - __main__ -   Batch Number = 63
01/06/2022 17:18:08 - INFO - __main__ -   Batch Number = 64
01/06/2022 17:18:08 - INFO - __main__ -   Batch Number = 65
01/06/2022 17:18:08 - INFO - __main__ -   Batch Number = 66
01/06/2022 17:18:08 - INFO - __main__ -   Batch Number = 67
Evaluating:  38%|███▊      | 67/177 [00:01<00:02, 43.54it/s]01/06/2022 17:18:09 - INFO - __main__ -   Batch Number = 68
01/06/2022 17:18:09 - INFO - __main__ -   Batch Number = 69
01/06/2022 17:18:09 - INFO - __main__ -   Batch Number = 70
01/06/2022 17:18:09 - INFO - __main__ -   Batch Number = 71
01/06/2022 17:18:09 - INFO - __main__ -   Batch Number = 72
Evaluating:  41%|████      | 72/177 [00:01<00:02, 43.80it/s]01/06/2022 17:18:09 - INFO - __main__ -   Batch Number = 73
01/06/2022 17:18:09 - INFO - __main__ -   Batch Number = 74
01/06/2022 17:18:09 - INFO - __main__ -   Batch Number = 75
01/06/2022 17:18:09 - INFO - __main__ -   Batch Number = 76
01/06/2022 17:18:09 - INFO - __main__ -   Batch Number = 77
Evaluating:  44%|████▎     | 77/177 [00:01<00:02, 44.00it/s]01/06/2022 17:18:09 - INFO - __main__ -   Batch Number = 78
01/06/2022 17:18:09 - INFO - __main__ -   Batch Number = 79
01/06/2022 17:18:09 - INFO - __main__ -   Batch Number = 80
01/06/2022 17:18:09 - INFO - __main__ -   Batch Number = 81
01/06/2022 17:18:09 - INFO - __main__ -   Batch Number = 82
Evaluating:  46%|████▋     | 82/177 [00:01<00:02, 44.17it/s]01/06/2022 17:18:09 - INFO - __main__ -   Batch Number = 83
01/06/2022 17:18:09 - INFO - __main__ -   Batch Number = 84
01/06/2022 17:18:09 - INFO - __main__ -   Batch Number = 85
01/06/2022 17:18:09 - INFO - __main__ -   Batch Number = 86
01/06/2022 17:18:09 - INFO - __main__ -   Batch Number = 87
Evaluating:  49%|████▉     | 87/177 [00:02<00:02, 44.36it/s]01/06/2022 17:18:09 - INFO - __main__ -   Batch Number = 88
01/06/2022 17:18:09 - INFO - __main__ -   Batch Number = 89
01/06/2022 17:18:09 - INFO - __main__ -   Batch Number = 90
01/06/2022 17:18:09 - INFO - __main__ -   Batch Number = 91
01/06/2022 17:18:09 - INFO - __main__ -   Batch Number = 92
Evaluating:  52%|█████▏    | 92/177 [00:02<00:01, 44.33it/s]01/06/2022 17:18:09 - INFO - __main__ -   Batch Number = 93
01/06/2022 17:18:09 - INFO - __main__ -   Batch Number = 94
01/06/2022 17:18:09 - INFO - __main__ -   Batch Number = 95
01/06/2022 17:18:09 - INFO - __main__ -   Batch Number = 96
01/06/2022 17:18:09 - INFO - __main__ -   Batch Number = 97
Evaluating:  55%|█████▍    | 97/177 [00:02<00:01, 44.35it/s]01/06/2022 17:18:09 - INFO - __main__ -   Batch Number = 98
01/06/2022 17:18:09 - INFO - __main__ -   Batch Number = 99
01/06/2022 17:18:09 - INFO - __main__ -   Batch Number = 100
01/06/2022 17:18:09 - INFO - __main__ -   Batch Number = 101
01/06/2022 17:18:09 - INFO - __main__ -   Batch Number = 102
Evaluating:  58%|█████▊    | 102/177 [00:02<00:01, 44.39it/s]01/06/2022 17:18:09 - INFO - __main__ -   Batch Number = 103
01/06/2022 17:18:09 - INFO - __main__ -   Batch Number = 104
01/06/2022 17:18:09 - INFO - __main__ -   Batch Number = 105
01/06/2022 17:18:09 - INFO - __main__ -   Batch Number = 106
01/06/2022 17:18:09 - INFO - __main__ -   Batch Number = 107
Evaluating:  60%|██████    | 107/177 [00:02<00:01, 44.00it/s]01/06/2022 17:18:09 - INFO - __main__ -   Batch Number = 108
01/06/2022 17:18:09 - INFO - __main__ -   Batch Number = 109
01/06/2022 17:18:09 - INFO - __main__ -   Batch Number = 110
01/06/2022 17:18:09 - INFO - __main__ -   Batch Number = 111
01/06/2022 17:18:10 - INFO - __main__ -   Batch Number = 112
Evaluating:  63%|██████▎   | 112/177 [00:02<00:01, 44.17it/s]01/06/2022 17:18:10 - INFO - __main__ -   Batch Number = 113
01/06/2022 17:18:10 - INFO - __main__ -   Batch Number = 114
01/06/2022 17:18:10 - INFO - __main__ -   Batch Number = 115
01/06/2022 17:18:10 - INFO - __main__ -   Batch Number = 116
01/06/2022 17:18:10 - INFO - __main__ -   Batch Number = 117
Evaluating:  66%|██████▌   | 117/177 [00:02<00:01, 44.21it/s]01/06/2022 17:18:10 - INFO - __main__ -   Batch Number = 118
01/06/2022 17:18:10 - INFO - __main__ -   Batch Number = 119
01/06/2022 17:18:10 - INFO - __main__ -   Batch Number = 120
01/06/2022 17:18:10 - INFO - __main__ -   Batch Number = 121
01/06/2022 17:18:10 - INFO - __main__ -   Batch Number = 122
Evaluating:  69%|██████▉   | 122/177 [00:02<00:01, 44.14it/s]01/06/2022 17:18:10 - INFO - __main__ -   Batch Number = 123
01/06/2022 17:18:10 - INFO - __main__ -   Batch Number = 124
01/06/2022 17:18:10 - INFO - __main__ -   Batch Number = 125
01/06/2022 17:18:10 - INFO - __main__ -   Batch Number = 126
01/06/2022 17:18:10 - INFO - __main__ -   Batch Number = 127
Evaluating:  72%|███████▏  | 127/177 [00:02<00:01, 44.31it/s]01/06/2022 17:18:10 - INFO - __main__ -   Batch Number = 128
01/06/2022 17:18:10 - INFO - __main__ -   Batch Number = 129
01/06/2022 17:18:10 - INFO - __main__ -   Batch Number = 130
01/06/2022 17:18:10 - INFO - __main__ -   Batch Number = 131
01/06/2022 17:18:10 - INFO - __main__ -   Batch Number = 132
Evaluating:  75%|███████▍  | 132/177 [00:03<00:01, 44.19it/s]01/06/2022 17:18:10 - INFO - __main__ -   Batch Number = 133
01/06/2022 17:18:10 - INFO - __main__ -   Batch Number = 134
01/06/2022 17:18:10 - INFO - __main__ -   Batch Number = 135
01/06/2022 17:18:10 - INFO - __main__ -   Batch Number = 136
01/06/2022 17:18:10 - INFO - __main__ -   Batch Number = 137
Evaluating:  77%|███████▋  | 137/177 [00:03<00:00, 44.19it/s]01/06/2022 17:18:10 - INFO - __main__ -   Batch Number = 138
01/06/2022 17:18:10 - INFO - __main__ -   Batch Number = 139
01/06/2022 17:18:10 - INFO - __main__ -   Batch Number = 140
01/06/2022 17:18:10 - INFO - __main__ -   Batch Number = 141
01/06/2022 17:18:10 - INFO - __main__ -   Batch Number = 142
Evaluating:  80%|████████  | 142/177 [00:03<00:00, 44.27it/s]01/06/2022 17:18:10 - INFO - __main__ -   Batch Number = 143
01/06/2022 17:18:10 - INFO - __main__ -   Batch Number = 144
01/06/2022 17:18:10 - INFO - __main__ -   Batch Number = 145
01/06/2022 17:18:10 - INFO - __main__ -   Batch Number = 146
01/06/2022 17:18:10 - INFO - __main__ -   Batch Number = 147
Evaluating:  83%|████████▎ | 147/177 [00:03<00:00, 44.29it/s]01/06/2022 17:18:10 - INFO - __main__ -   Batch Number = 148
01/06/2022 17:18:10 - INFO - __main__ -   Batch Number = 149
01/06/2022 17:18:10 - INFO - __main__ -   Batch Number = 150
01/06/2022 17:18:10 - INFO - __main__ -   Batch Number = 151
01/06/2022 17:18:10 - INFO - __main__ -   Batch Number = 152
Evaluating:  86%|████████▌ | 152/177 [00:03<00:00, 44.27it/s]01/06/2022 17:18:10 - INFO - __main__ -   Batch Number = 153
01/06/2022 17:18:10 - INFO - __main__ -   Batch Number = 154
01/06/2022 17:18:10 - INFO - __main__ -   Batch Number = 155
01/06/2022 17:18:10 - INFO - __main__ -   Batch Number = 156
01/06/2022 17:18:11 - INFO - __main__ -   Batch Number = 157
Evaluating:  89%|████████▊ | 157/177 [00:03<00:00, 44.24it/s]01/06/2022 17:18:11 - INFO - __main__ -   Batch Number = 158
01/06/2022 17:18:11 - INFO - __main__ -   Batch Number = 159
01/06/2022 17:18:11 - INFO - __main__ -   Batch Number = 160
01/06/2022 17:18:11 - INFO - __main__ -   Batch Number = 161
01/06/2022 17:18:11 - INFO - __main__ -   Batch Number = 162
Evaluating:  92%|█████████▏| 162/177 [00:03<00:00, 44.36it/s]01/06/2022 17:18:11 - INFO - __main__ -   Batch Number = 163
01/06/2022 17:18:11 - INFO - __main__ -   Batch Number = 164
01/06/2022 17:18:11 - INFO - __main__ -   Batch Number = 165
01/06/2022 17:18:11 - INFO - __main__ -   Batch Number = 166
01/06/2022 17:18:11 - INFO - __main__ -   Batch Number = 167
Evaluating:  94%|█████████▍| 167/177 [00:03<00:00, 44.14it/s]01/06/2022 17:18:11 - INFO - __main__ -   Batch Number = 168
01/06/2022 17:18:11 - INFO - __main__ -   Batch Number = 169
01/06/2022 17:18:11 - INFO - __main__ -   Batch Number = 170
01/06/2022 17:18:11 - INFO - __main__ -   Batch Number = 171
01/06/2022 17:18:11 - INFO - __main__ -   Batch Number = 172
Evaluating:  97%|█████████▋| 172/177 [00:03<00:00, 44.18it/s]01/06/2022 17:18:11 - INFO - __main__ -   Batch Number = 173
01/06/2022 17:18:11 - INFO - __main__ -   Batch Number = 174
01/06/2022 17:18:11 - INFO - __main__ -   Batch Number = 175
01/06/2022 17:18:11 - INFO - __main__ -   Batch Number = 176
01/06/2022 17:18:11 - INFO - __main__ -   Batch Number = 177
Evaluating: 100%|██████████| 177/177 [00:04<00:00, 44.92it/s]Evaluating: 100%|██████████| 177/177 [00:04<00:00, 43.65it/s]
01/06/2022 17:18:11 - INFO - __main__ -     Evaluation done in total 4.055630 secs (0.002876 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/predictions_ru_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/nbest_predictions_ru_.json
01/06/2022 17:18:14 - INFO - __main__ -   Results = OrderedDict([('exact', 49.91596638655462), ('f1', 67.00065141305423), ('total', 1190), ('HasAns_exact', 49.91596638655462), ('HasAns_f1', 67.00065141305423), ('HasAns_total', 1190), ('best_exact', 49.91596638655462), ('best_exact_thresh', 0.0), ('best_f1', 67.00065141305423), ('best_f1_thresh', 0.0)])
01/06/2022 17:18:14 - INFO - __main__ -   Language adapter for th not found, using en instead
01/06/2022 17:18:14 - INFO - __main__ -   Set active language adapter to en
01/06/2022 17:18:14 - INFO - __main__ -   Args Adapter Weight = None
01/06/2022 17:18:14 - INFO - __main__ -   Adapter Languages = ['en']
01/06/2022 17:18:14 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
  0%|          | 0/48 [00:00<?, ?it/s] 33%|███▎      | 16/48 [00:00<00:00, 126.22it/s] 60%|██████    | 29/48 [00:00<00:00, 89.76it/s]  92%|█████████▏| 44/48 [00:00<00:00, 110.18it/s]100%|██████████| 48/48 [00:00<00:00, 110.05it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<12:55,  1.53it/s]convert squad examples to features:  22%|██▏       | 257/1190 [00:00<00:02, 446.32it/s]convert squad examples to features:  32%|███▏      | 385/1190 [00:02<00:04, 172.01it/s]convert squad examples to features:  89%|████████▉ | 1057/1190 [00:02<00:00, 640.55it/s]convert squad examples to features: 100%|██████████| 1190/1190 [00:02<00:00, 451.63it/s]/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|██████████| 1190/1190 [00:00<00:00, 453376.49it/s]
01/06/2022 17:18:18 - INFO - __main__ -   Local Rank = -1
01/06/2022 17:18:18 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.th.json_bert-base-multilingual-cased_384_th
01/06/2022 17:18:21 - INFO - __main__ -   ***** Running evaluation  *****
01/06/2022 17:18:21 - INFO - __main__ -     Num examples = 3123
01/06/2022 17:18:21 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/391 [00:00<?, ?it/s]01/06/2022 17:18:21 - INFO - __main__ -   Batch Number = 1
01/06/2022 17:18:21 - INFO - __main__ -   Batch Number = 2
01/06/2022 17:18:21 - INFO - __main__ -   Batch Number = 3
Evaluating:   1%|          | 3/391 [00:00<00:13, 29.48it/s]01/06/2022 17:18:21 - INFO - __main__ -   Batch Number = 4
01/06/2022 17:18:21 - INFO - __main__ -   Batch Number = 5
01/06/2022 17:18:21 - INFO - __main__ -   Batch Number = 6
01/06/2022 17:18:21 - INFO - __main__ -   Batch Number = 7
Evaluating:   2%|▏         | 7/391 [00:00<00:11, 33.86it/s]01/06/2022 17:18:21 - INFO - __main__ -   Batch Number = 8
01/06/2022 17:18:21 - INFO - __main__ -   Batch Number = 9
01/06/2022 17:18:21 - INFO - __main__ -   Batch Number = 10
01/06/2022 17:18:21 - INFO - __main__ -   Batch Number = 11
Evaluating:   3%|▎         | 11/391 [00:00<00:10, 36.25it/s]01/06/2022 17:18:21 - INFO - __main__ -   Batch Number = 12
01/06/2022 17:18:21 - INFO - __main__ -   Batch Number = 13
01/06/2022 17:18:21 - INFO - __main__ -   Batch Number = 14
01/06/2022 17:18:21 - INFO - __main__ -   Batch Number = 15
01/06/2022 17:18:21 - INFO - __main__ -   Batch Number = 16
Evaluating:   4%|▍         | 16/391 [00:00<00:09, 39.29it/s]01/06/2022 17:18:22 - INFO - __main__ -   Batch Number = 17
01/06/2022 17:18:22 - INFO - __main__ -   Batch Number = 18
01/06/2022 17:18:22 - INFO - __main__ -   Batch Number = 19
01/06/2022 17:18:22 - INFO - __main__ -   Batch Number = 20
01/06/2022 17:18:22 - INFO - __main__ -   Batch Number = 21
Evaluating:   5%|▌         | 21/391 [00:00<00:09, 40.97it/s]01/06/2022 17:18:22 - INFO - __main__ -   Batch Number = 22
01/06/2022 17:18:22 - INFO - __main__ -   Batch Number = 23
01/06/2022 17:18:22 - INFO - __main__ -   Batch Number = 24
01/06/2022 17:18:22 - INFO - __main__ -   Batch Number = 25
01/06/2022 17:18:22 - INFO - __main__ -   Batch Number = 26
Evaluating:   7%|▋         | 26/391 [00:00<00:08, 42.34it/s]01/06/2022 17:18:22 - INFO - __main__ -   Batch Number = 27
01/06/2022 17:18:22 - INFO - __main__ -   Batch Number = 28
01/06/2022 17:18:22 - INFO - __main__ -   Batch Number = 29
01/06/2022 17:18:22 - INFO - __main__ -   Batch Number = 30
01/06/2022 17:18:22 - INFO - __main__ -   Batch Number = 31
Evaluating:   8%|▊         | 31/391 [00:00<00:08, 42.91it/s]01/06/2022 17:18:22 - INFO - __main__ -   Batch Number = 32
01/06/2022 17:18:22 - INFO - __main__ -   Batch Number = 33
01/06/2022 17:18:22 - INFO - __main__ -   Batch Number = 34
01/06/2022 17:18:22 - INFO - __main__ -   Batch Number = 35
01/06/2022 17:18:22 - INFO - __main__ -   Batch Number = 36
Evaluating:   9%|▉         | 36/391 [00:00<00:08, 43.42it/s]01/06/2022 17:18:22 - INFO - __main__ -   Batch Number = 37
01/06/2022 17:18:22 - INFO - __main__ -   Batch Number = 38
01/06/2022 17:18:22 - INFO - __main__ -   Batch Number = 39
01/06/2022 17:18:22 - INFO - __main__ -   Batch Number = 40
01/06/2022 17:18:22 - INFO - __main__ -   Batch Number = 41
Evaluating:  10%|█         | 41/391 [00:00<00:08, 43.66it/s]01/06/2022 17:18:22 - INFO - __main__ -   Batch Number = 42
01/06/2022 17:18:22 - INFO - __main__ -   Batch Number = 43
01/06/2022 17:18:22 - INFO - __main__ -   Batch Number = 44
01/06/2022 17:18:22 - INFO - __main__ -   Batch Number = 45
01/06/2022 17:18:22 - INFO - __main__ -   Batch Number = 46
Evaluating:  12%|█▏        | 46/391 [00:01<00:07, 43.93it/s]01/06/2022 17:18:22 - INFO - __main__ -   Batch Number = 47
01/06/2022 17:18:22 - INFO - __main__ -   Batch Number = 48
01/06/2022 17:18:22 - INFO - __main__ -   Batch Number = 49
01/06/2022 17:18:22 - INFO - __main__ -   Batch Number = 50
01/06/2022 17:18:22 - INFO - __main__ -   Batch Number = 51
Evaluating:  13%|█▎        | 51/391 [00:01<00:07, 43.92it/s]01/06/2022 17:18:22 - INFO - __main__ -   Batch Number = 52
01/06/2022 17:18:22 - INFO - __main__ -   Batch Number = 53
01/06/2022 17:18:22 - INFO - __main__ -   Batch Number = 54
01/06/2022 17:18:22 - INFO - __main__ -   Batch Number = 55
01/06/2022 17:18:22 - INFO - __main__ -   Batch Number = 56
Evaluating:  14%|█▍        | 56/391 [00:01<00:07, 44.07it/s]01/06/2022 17:18:22 - INFO - __main__ -   Batch Number = 57
01/06/2022 17:18:22 - INFO - __main__ -   Batch Number = 58
01/06/2022 17:18:22 - INFO - __main__ -   Batch Number = 59
01/06/2022 17:18:22 - INFO - __main__ -   Batch Number = 60
01/06/2022 17:18:23 - INFO - __main__ -   Batch Number = 61
Evaluating:  16%|█▌        | 61/391 [00:01<00:07, 44.22it/s]01/06/2022 17:18:23 - INFO - __main__ -   Batch Number = 62
01/06/2022 17:18:23 - INFO - __main__ -   Batch Number = 63
01/06/2022 17:18:23 - INFO - __main__ -   Batch Number = 64
01/06/2022 17:18:23 - INFO - __main__ -   Batch Number = 65
01/06/2022 17:18:23 - INFO - __main__ -   Batch Number = 66
Evaluating:  17%|█▋        | 66/391 [00:01<00:07, 44.28it/s]01/06/2022 17:18:23 - INFO - __main__ -   Batch Number = 67
01/06/2022 17:18:23 - INFO - __main__ -   Batch Number = 68
01/06/2022 17:18:23 - INFO - __main__ -   Batch Number = 69
01/06/2022 17:18:23 - INFO - __main__ -   Batch Number = 70
01/06/2022 17:18:23 - INFO - __main__ -   Batch Number = 71
Evaluating:  18%|█▊        | 71/391 [00:01<00:07, 44.37it/s]01/06/2022 17:18:23 - INFO - __main__ -   Batch Number = 72
01/06/2022 17:18:23 - INFO - __main__ -   Batch Number = 73
01/06/2022 17:18:23 - INFO - __main__ -   Batch Number = 74
01/06/2022 17:18:23 - INFO - __main__ -   Batch Number = 75
01/06/2022 17:18:23 - INFO - __main__ -   Batch Number = 76
Evaluating:  19%|█▉        | 76/391 [00:01<00:07, 44.29it/s]01/06/2022 17:18:23 - INFO - __main__ -   Batch Number = 77
01/06/2022 17:18:23 - INFO - __main__ -   Batch Number = 78
01/06/2022 17:18:23 - INFO - __main__ -   Batch Number = 79
01/06/2022 17:18:23 - INFO - __main__ -   Batch Number = 80
01/06/2022 17:18:23 - INFO - __main__ -   Batch Number = 81
Evaluating:  21%|██        | 81/391 [00:01<00:06, 44.38it/s]01/06/2022 17:18:23 - INFO - __main__ -   Batch Number = 82
01/06/2022 17:18:23 - INFO - __main__ -   Batch Number = 83
01/06/2022 17:18:23 - INFO - __main__ -   Batch Number = 84
01/06/2022 17:18:23 - INFO - __main__ -   Batch Number = 85
01/06/2022 17:18:23 - INFO - __main__ -   Batch Number = 86
Evaluating:  22%|██▏       | 86/391 [00:02<00:06, 44.44it/s]01/06/2022 17:18:23 - INFO - __main__ -   Batch Number = 87
01/06/2022 17:18:23 - INFO - __main__ -   Batch Number = 88
01/06/2022 17:18:23 - INFO - __main__ -   Batch Number = 89
01/06/2022 17:18:23 - INFO - __main__ -   Batch Number = 90
01/06/2022 17:18:23 - INFO - __main__ -   Batch Number = 91
Evaluating:  23%|██▎       | 91/391 [00:02<00:06, 44.56it/s]01/06/2022 17:18:23 - INFO - __main__ -   Batch Number = 92
01/06/2022 17:18:23 - INFO - __main__ -   Batch Number = 93
01/06/2022 17:18:23 - INFO - __main__ -   Batch Number = 94
01/06/2022 17:18:23 - INFO - __main__ -   Batch Number = 95
01/06/2022 17:18:23 - INFO - __main__ -   Batch Number = 96
Evaluating:  25%|██▍       | 96/391 [00:02<00:06, 44.45it/s]01/06/2022 17:18:23 - INFO - __main__ -   Batch Number = 97
01/06/2022 17:18:23 - INFO - __main__ -   Batch Number = 98
01/06/2022 17:18:23 - INFO - __main__ -   Batch Number = 99
01/06/2022 17:18:23 - INFO - __main__ -   Batch Number = 100
01/06/2022 17:18:23 - INFO - __main__ -   Batch Number = 101
Evaluating:  26%|██▌       | 101/391 [00:02<00:06, 44.46it/s]01/06/2022 17:18:23 - INFO - __main__ -   Batch Number = 102
01/06/2022 17:18:23 - INFO - __main__ -   Batch Number = 103
01/06/2022 17:18:23 - INFO - __main__ -   Batch Number = 104
01/06/2022 17:18:23 - INFO - __main__ -   Batch Number = 105
01/06/2022 17:18:24 - INFO - __main__ -   Batch Number = 106
Evaluating:  27%|██▋       | 106/391 [00:02<00:06, 44.52it/s]01/06/2022 17:18:24 - INFO - __main__ -   Batch Number = 107
01/06/2022 17:18:24 - INFO - __main__ -   Batch Number = 108
01/06/2022 17:18:24 - INFO - __main__ -   Batch Number = 109
01/06/2022 17:18:24 - INFO - __main__ -   Batch Number = 110
01/06/2022 17:18:24 - INFO - __main__ -   Batch Number = 111
Evaluating:  28%|██▊       | 111/391 [00:02<00:06, 44.33it/s]01/06/2022 17:18:24 - INFO - __main__ -   Batch Number = 112
01/06/2022 17:18:24 - INFO - __main__ -   Batch Number = 113
01/06/2022 17:18:24 - INFO - __main__ -   Batch Number = 114
01/06/2022 17:18:24 - INFO - __main__ -   Batch Number = 115
01/06/2022 17:18:24 - INFO - __main__ -   Batch Number = 116
Evaluating:  30%|██▉       | 116/391 [00:02<00:06, 44.26it/s]01/06/2022 17:18:24 - INFO - __main__ -   Batch Number = 117
01/06/2022 17:18:24 - INFO - __main__ -   Batch Number = 118
01/06/2022 17:18:24 - INFO - __main__ -   Batch Number = 119
01/06/2022 17:18:24 - INFO - __main__ -   Batch Number = 120
01/06/2022 17:18:24 - INFO - __main__ -   Batch Number = 121
Evaluating:  31%|███       | 121/391 [00:02<00:06, 44.15it/s]01/06/2022 17:18:24 - INFO - __main__ -   Batch Number = 122
01/06/2022 17:18:24 - INFO - __main__ -   Batch Number = 123
01/06/2022 17:18:24 - INFO - __main__ -   Batch Number = 124
01/06/2022 17:18:24 - INFO - __main__ -   Batch Number = 125
01/06/2022 17:18:24 - INFO - __main__ -   Batch Number = 126
Evaluating:  32%|███▏      | 126/391 [00:02<00:06, 44.11it/s]01/06/2022 17:18:24 - INFO - __main__ -   Batch Number = 127
01/06/2022 17:18:24 - INFO - __main__ -   Batch Number = 128
01/06/2022 17:18:24 - INFO - __main__ -   Batch Number = 129
01/06/2022 17:18:24 - INFO - __main__ -   Batch Number = 130
01/06/2022 17:18:24 - INFO - __main__ -   Batch Number = 131
Evaluating:  34%|███▎      | 131/391 [00:03<00:05, 44.30it/s]01/06/2022 17:18:24 - INFO - __main__ -   Batch Number = 132
01/06/2022 17:18:24 - INFO - __main__ -   Batch Number = 133
01/06/2022 17:18:24 - INFO - __main__ -   Batch Number = 134
01/06/2022 17:18:24 - INFO - __main__ -   Batch Number = 135
01/06/2022 17:18:24 - INFO - __main__ -   Batch Number = 136
Evaluating:  35%|███▍      | 136/391 [00:03<00:05, 44.35it/s]01/06/2022 17:18:24 - INFO - __main__ -   Batch Number = 137
01/06/2022 17:18:24 - INFO - __main__ -   Batch Number = 138
01/06/2022 17:18:24 - INFO - __main__ -   Batch Number = 139
01/06/2022 17:18:24 - INFO - __main__ -   Batch Number = 140
01/06/2022 17:18:24 - INFO - __main__ -   Batch Number = 141
Evaluating:  36%|███▌      | 141/391 [00:03<00:05, 44.33it/s]01/06/2022 17:18:24 - INFO - __main__ -   Batch Number = 142
01/06/2022 17:18:24 - INFO - __main__ -   Batch Number = 143
01/06/2022 17:18:24 - INFO - __main__ -   Batch Number = 144
01/06/2022 17:18:24 - INFO - __main__ -   Batch Number = 145
01/06/2022 17:18:24 - INFO - __main__ -   Batch Number = 146
Evaluating:  37%|███▋      | 146/391 [00:03<00:05, 44.27it/s]01/06/2022 17:18:24 - INFO - __main__ -   Batch Number = 147
01/06/2022 17:18:24 - INFO - __main__ -   Batch Number = 148
01/06/2022 17:18:24 - INFO - __main__ -   Batch Number = 149
01/06/2022 17:18:25 - INFO - __main__ -   Batch Number = 150
01/06/2022 17:18:25 - INFO - __main__ -   Batch Number = 151
Evaluating:  39%|███▊      | 151/391 [00:03<00:05, 44.32it/s]01/06/2022 17:18:25 - INFO - __main__ -   Batch Number = 152
01/06/2022 17:18:25 - INFO - __main__ -   Batch Number = 153
01/06/2022 17:18:25 - INFO - __main__ -   Batch Number = 154
01/06/2022 17:18:25 - INFO - __main__ -   Batch Number = 155
01/06/2022 17:18:25 - INFO - __main__ -   Batch Number = 156
Evaluating:  40%|███▉      | 156/391 [00:03<00:05, 44.26it/s]01/06/2022 17:18:25 - INFO - __main__ -   Batch Number = 157
01/06/2022 17:18:25 - INFO - __main__ -   Batch Number = 158
01/06/2022 17:18:25 - INFO - __main__ -   Batch Number = 159
01/06/2022 17:18:25 - INFO - __main__ -   Batch Number = 160
01/06/2022 17:18:25 - INFO - __main__ -   Batch Number = 161
Evaluating:  41%|████      | 161/391 [00:03<00:05, 44.14it/s]01/06/2022 17:18:25 - INFO - __main__ -   Batch Number = 162
01/06/2022 17:18:25 - INFO - __main__ -   Batch Number = 163
01/06/2022 17:18:25 - INFO - __main__ -   Batch Number = 164
01/06/2022 17:18:25 - INFO - __main__ -   Batch Number = 165
01/06/2022 17:18:25 - INFO - __main__ -   Batch Number = 166
Evaluating:  42%|████▏     | 166/391 [00:03<00:05, 44.29it/s]01/06/2022 17:18:25 - INFO - __main__ -   Batch Number = 167
01/06/2022 17:18:25 - INFO - __main__ -   Batch Number = 168
01/06/2022 17:18:25 - INFO - __main__ -   Batch Number = 169
01/06/2022 17:18:25 - INFO - __main__ -   Batch Number = 170
01/06/2022 17:18:25 - INFO - __main__ -   Batch Number = 171
Evaluating:  44%|████▎     | 171/391 [00:03<00:04, 44.22it/s]01/06/2022 17:18:25 - INFO - __main__ -   Batch Number = 172
01/06/2022 17:18:25 - INFO - __main__ -   Batch Number = 173
01/06/2022 17:18:25 - INFO - __main__ -   Batch Number = 174
01/06/2022 17:18:25 - INFO - __main__ -   Batch Number = 175
01/06/2022 17:18:25 - INFO - __main__ -   Batch Number = 176
Evaluating:  45%|████▌     | 176/391 [00:04<00:04, 44.45it/s]01/06/2022 17:18:25 - INFO - __main__ -   Batch Number = 177
01/06/2022 17:18:25 - INFO - __main__ -   Batch Number = 178
01/06/2022 17:18:25 - INFO - __main__ -   Batch Number = 179
01/06/2022 17:18:25 - INFO - __main__ -   Batch Number = 180
01/06/2022 17:18:25 - INFO - __main__ -   Batch Number = 181
Evaluating:  46%|████▋     | 181/391 [00:04<00:04, 44.48it/s]01/06/2022 17:18:25 - INFO - __main__ -   Batch Number = 182
01/06/2022 17:18:25 - INFO - __main__ -   Batch Number = 183
01/06/2022 17:18:25 - INFO - __main__ -   Batch Number = 184
01/06/2022 17:18:25 - INFO - __main__ -   Batch Number = 185
01/06/2022 17:18:25 - INFO - __main__ -   Batch Number = 186
Evaluating:  48%|████▊     | 186/391 [00:04<00:04, 43.09it/s]01/06/2022 17:18:25 - INFO - __main__ -   Batch Number = 187
01/06/2022 17:18:25 - INFO - __main__ -   Batch Number = 188
01/06/2022 17:18:25 - INFO - __main__ -   Batch Number = 189
01/06/2022 17:18:25 - INFO - __main__ -   Batch Number = 190
01/06/2022 17:18:25 - INFO - __main__ -   Batch Number = 191
Evaluating:  49%|████▉     | 191/391 [00:04<00:04, 43.53it/s]01/06/2022 17:18:25 - INFO - __main__ -   Batch Number = 192
01/06/2022 17:18:25 - INFO - __main__ -   Batch Number = 193
01/06/2022 17:18:26 - INFO - __main__ -   Batch Number = 194
01/06/2022 17:18:26 - INFO - __main__ -   Batch Number = 195
01/06/2022 17:18:26 - INFO - __main__ -   Batch Number = 196
Evaluating:  50%|█████     | 196/391 [00:04<00:04, 43.90it/s]01/06/2022 17:18:26 - INFO - __main__ -   Batch Number = 197
01/06/2022 17:18:26 - INFO - __main__ -   Batch Number = 198
01/06/2022 17:18:26 - INFO - __main__ -   Batch Number = 199
01/06/2022 17:18:26 - INFO - __main__ -   Batch Number = 200
01/06/2022 17:18:26 - INFO - __main__ -   Batch Number = 201
Evaluating:  51%|█████▏    | 201/391 [00:04<00:04, 43.93it/s]01/06/2022 17:18:26 - INFO - __main__ -   Batch Number = 202
01/06/2022 17:18:26 - INFO - __main__ -   Batch Number = 203
01/06/2022 17:18:26 - INFO - __main__ -   Batch Number = 204
01/06/2022 17:18:26 - INFO - __main__ -   Batch Number = 205
01/06/2022 17:18:26 - INFO - __main__ -   Batch Number = 206
Evaluating:  53%|█████▎    | 206/391 [00:04<00:04, 43.99it/s]01/06/2022 17:18:26 - INFO - __main__ -   Batch Number = 207
01/06/2022 17:18:26 - INFO - __main__ -   Batch Number = 208
01/06/2022 17:18:26 - INFO - __main__ -   Batch Number = 209
01/06/2022 17:18:26 - INFO - __main__ -   Batch Number = 210
01/06/2022 17:18:26 - INFO - __main__ -   Batch Number = 211
Evaluating:  54%|█████▍    | 211/391 [00:04<00:04, 44.08it/s]01/06/2022 17:18:26 - INFO - __main__ -   Batch Number = 212
01/06/2022 17:18:26 - INFO - __main__ -   Batch Number = 213
01/06/2022 17:18:26 - INFO - __main__ -   Batch Number = 214
01/06/2022 17:18:26 - INFO - __main__ -   Batch Number = 215
01/06/2022 17:18:26 - INFO - __main__ -   Batch Number = 216
Evaluating:  55%|█████▌    | 216/391 [00:04<00:03, 44.15it/s]01/06/2022 17:18:26 - INFO - __main__ -   Batch Number = 217
01/06/2022 17:18:26 - INFO - __main__ -   Batch Number = 218
01/06/2022 17:18:26 - INFO - __main__ -   Batch Number = 219
01/06/2022 17:18:26 - INFO - __main__ -   Batch Number = 220
01/06/2022 17:18:26 - INFO - __main__ -   Batch Number = 221
Evaluating:  57%|█████▋    | 221/391 [00:05<00:03, 44.24it/s]01/06/2022 17:18:26 - INFO - __main__ -   Batch Number = 222
01/06/2022 17:18:26 - INFO - __main__ -   Batch Number = 223
01/06/2022 17:18:26 - INFO - __main__ -   Batch Number = 224
01/06/2022 17:18:26 - INFO - __main__ -   Batch Number = 225
01/06/2022 17:18:26 - INFO - __main__ -   Batch Number = 226
Evaluating:  58%|█████▊    | 226/391 [00:05<00:03, 44.26it/s]01/06/2022 17:18:26 - INFO - __main__ -   Batch Number = 227
01/06/2022 17:18:26 - INFO - __main__ -   Batch Number = 228
01/06/2022 17:18:26 - INFO - __main__ -   Batch Number = 229
01/06/2022 17:18:26 - INFO - __main__ -   Batch Number = 230
01/06/2022 17:18:26 - INFO - __main__ -   Batch Number = 231
Evaluating:  59%|█████▉    | 231/391 [00:05<00:03, 44.24it/s]01/06/2022 17:18:26 - INFO - __main__ -   Batch Number = 232
01/06/2022 17:18:26 - INFO - __main__ -   Batch Number = 233
01/06/2022 17:18:26 - INFO - __main__ -   Batch Number = 234
01/06/2022 17:18:26 - INFO - __main__ -   Batch Number = 235
01/06/2022 17:18:26 - INFO - __main__ -   Batch Number = 236
Evaluating:  60%|██████    | 236/391 [00:05<00:03, 44.26it/s]01/06/2022 17:18:26 - INFO - __main__ -   Batch Number = 237
01/06/2022 17:18:27 - INFO - __main__ -   Batch Number = 238
01/06/2022 17:18:27 - INFO - __main__ -   Batch Number = 239
01/06/2022 17:18:27 - INFO - __main__ -   Batch Number = 240
01/06/2022 17:18:27 - INFO - __main__ -   Batch Number = 241
Evaluating:  62%|██████▏   | 241/391 [00:05<00:03, 44.41it/s]01/06/2022 17:18:27 - INFO - __main__ -   Batch Number = 242
01/06/2022 17:18:27 - INFO - __main__ -   Batch Number = 243
01/06/2022 17:18:27 - INFO - __main__ -   Batch Number = 244
01/06/2022 17:18:27 - INFO - __main__ -   Batch Number = 245
01/06/2022 17:18:27 - INFO - __main__ -   Batch Number = 246
Evaluating:  63%|██████▎   | 246/391 [00:05<00:03, 44.40it/s]01/06/2022 17:18:27 - INFO - __main__ -   Batch Number = 247
01/06/2022 17:18:27 - INFO - __main__ -   Batch Number = 248
01/06/2022 17:18:27 - INFO - __main__ -   Batch Number = 249
01/06/2022 17:18:27 - INFO - __main__ -   Batch Number = 250
01/06/2022 17:18:27 - INFO - __main__ -   Batch Number = 251
Evaluating:  64%|██████▍   | 251/391 [00:05<00:03, 44.15it/s]01/06/2022 17:18:27 - INFO - __main__ -   Batch Number = 252
01/06/2022 17:18:27 - INFO - __main__ -   Batch Number = 253
01/06/2022 17:18:27 - INFO - __main__ -   Batch Number = 254
01/06/2022 17:18:27 - INFO - __main__ -   Batch Number = 255
01/06/2022 17:18:27 - INFO - __main__ -   Batch Number = 256
Evaluating:  65%|██████▌   | 256/391 [00:05<00:03, 44.22it/s]01/06/2022 17:18:27 - INFO - __main__ -   Batch Number = 257
01/06/2022 17:18:27 - INFO - __main__ -   Batch Number = 258
01/06/2022 17:18:27 - INFO - __main__ -   Batch Number = 259
01/06/2022 17:18:27 - INFO - __main__ -   Batch Number = 260
01/06/2022 17:18:27 - INFO - __main__ -   Batch Number = 261
Evaluating:  67%|██████▋   | 261/391 [00:05<00:02, 44.21it/s]01/06/2022 17:18:27 - INFO - __main__ -   Batch Number = 262
01/06/2022 17:18:27 - INFO - __main__ -   Batch Number = 263
01/06/2022 17:18:27 - INFO - __main__ -   Batch Number = 264
01/06/2022 17:18:27 - INFO - __main__ -   Batch Number = 265
01/06/2022 17:18:27 - INFO - __main__ -   Batch Number = 266
Evaluating:  68%|██████▊   | 266/391 [00:06<00:02, 44.32it/s]01/06/2022 17:18:27 - INFO - __main__ -   Batch Number = 267
01/06/2022 17:18:27 - INFO - __main__ -   Batch Number = 268
01/06/2022 17:18:27 - INFO - __main__ -   Batch Number = 269
01/06/2022 17:18:27 - INFO - __main__ -   Batch Number = 270
01/06/2022 17:18:27 - INFO - __main__ -   Batch Number = 271
Evaluating:  69%|██████▉   | 271/391 [00:06<00:02, 44.37it/s]01/06/2022 17:18:27 - INFO - __main__ -   Batch Number = 272
01/06/2022 17:18:27 - INFO - __main__ -   Batch Number = 273
01/06/2022 17:18:27 - INFO - __main__ -   Batch Number = 274
01/06/2022 17:18:27 - INFO - __main__ -   Batch Number = 275
01/06/2022 17:18:27 - INFO - __main__ -   Batch Number = 276
Evaluating:  71%|███████   | 276/391 [00:06<00:02, 44.44it/s]01/06/2022 17:18:27 - INFO - __main__ -   Batch Number = 277
01/06/2022 17:18:27 - INFO - __main__ -   Batch Number = 278
01/06/2022 17:18:27 - INFO - __main__ -   Batch Number = 279
01/06/2022 17:18:27 - INFO - __main__ -   Batch Number = 280
01/06/2022 17:18:27 - INFO - __main__ -   Batch Number = 281
Evaluating:  72%|███████▏  | 281/391 [00:06<00:02, 44.41it/s]01/06/2022 17:18:28 - INFO - __main__ -   Batch Number = 282
01/06/2022 17:18:28 - INFO - __main__ -   Batch Number = 283
01/06/2022 17:18:28 - INFO - __main__ -   Batch Number = 284
01/06/2022 17:18:28 - INFO - __main__ -   Batch Number = 285
01/06/2022 17:18:28 - INFO - __main__ -   Batch Number = 286
Evaluating:  73%|███████▎  | 286/391 [00:06<00:02, 44.34it/s]01/06/2022 17:18:28 - INFO - __main__ -   Batch Number = 287
01/06/2022 17:18:28 - INFO - __main__ -   Batch Number = 288
01/06/2022 17:18:28 - INFO - __main__ -   Batch Number = 289
01/06/2022 17:18:28 - INFO - __main__ -   Batch Number = 290
01/06/2022 17:18:28 - INFO - __main__ -   Batch Number = 291
Evaluating:  74%|███████▍  | 291/391 [00:06<00:02, 44.48it/s]01/06/2022 17:18:28 - INFO - __main__ -   Batch Number = 292
01/06/2022 17:18:28 - INFO - __main__ -   Batch Number = 293
01/06/2022 17:18:28 - INFO - __main__ -   Batch Number = 294
01/06/2022 17:18:28 - INFO - __main__ -   Batch Number = 295
01/06/2022 17:18:28 - INFO - __main__ -   Batch Number = 296
Evaluating:  76%|███████▌  | 296/391 [00:06<00:02, 44.13it/s]01/06/2022 17:18:28 - INFO - __main__ -   Batch Number = 297
01/06/2022 17:18:28 - INFO - __main__ -   Batch Number = 298
01/06/2022 17:18:28 - INFO - __main__ -   Batch Number = 299
01/06/2022 17:18:28 - INFO - __main__ -   Batch Number = 300
01/06/2022 17:18:28 - INFO - __main__ -   Batch Number = 301
Evaluating:  77%|███████▋  | 301/391 [00:06<00:02, 44.22it/s]01/06/2022 17:18:28 - INFO - __main__ -   Batch Number = 302
01/06/2022 17:18:28 - INFO - __main__ -   Batch Number = 303
01/06/2022 17:18:28 - INFO - __main__ -   Batch Number = 304
01/06/2022 17:18:28 - INFO - __main__ -   Batch Number = 305
01/06/2022 17:18:28 - INFO - __main__ -   Batch Number = 306
Evaluating:  78%|███████▊  | 306/391 [00:06<00:01, 44.00it/s]01/06/2022 17:18:28 - INFO - __main__ -   Batch Number = 307
01/06/2022 17:18:28 - INFO - __main__ -   Batch Number = 308
01/06/2022 17:18:28 - INFO - __main__ -   Batch Number = 309
01/06/2022 17:18:28 - INFO - __main__ -   Batch Number = 310
01/06/2022 17:18:28 - INFO - __main__ -   Batch Number = 311
Evaluating:  80%|███████▉  | 311/391 [00:07<00:01, 44.15it/s]01/06/2022 17:18:28 - INFO - __main__ -   Batch Number = 312
01/06/2022 17:18:28 - INFO - __main__ -   Batch Number = 313
01/06/2022 17:18:28 - INFO - __main__ -   Batch Number = 314
01/06/2022 17:18:28 - INFO - __main__ -   Batch Number = 315
01/06/2022 17:18:28 - INFO - __main__ -   Batch Number = 316
Evaluating:  81%|████████  | 316/391 [00:07<00:01, 44.25it/s]01/06/2022 17:18:28 - INFO - __main__ -   Batch Number = 317
01/06/2022 17:18:28 - INFO - __main__ -   Batch Number = 318
01/06/2022 17:18:28 - INFO - __main__ -   Batch Number = 319
01/06/2022 17:18:28 - INFO - __main__ -   Batch Number = 320
01/06/2022 17:18:28 - INFO - __main__ -   Batch Number = 321
Evaluating:  82%|████████▏ | 321/391 [00:07<00:01, 44.23it/s]01/06/2022 17:18:28 - INFO - __main__ -   Batch Number = 322
01/06/2022 17:18:28 - INFO - __main__ -   Batch Number = 323
01/06/2022 17:18:28 - INFO - __main__ -   Batch Number = 324
01/06/2022 17:18:28 - INFO - __main__ -   Batch Number = 325
01/06/2022 17:18:28 - INFO - __main__ -   Batch Number = 326
Evaluating:  83%|████████▎ | 326/391 [00:07<00:01, 44.20it/s]01/06/2022 17:18:29 - INFO - __main__ -   Batch Number = 327
01/06/2022 17:18:29 - INFO - __main__ -   Batch Number = 328
01/06/2022 17:18:29 - INFO - __main__ -   Batch Number = 329
01/06/2022 17:18:29 - INFO - __main__ -   Batch Number = 330
01/06/2022 17:18:29 - INFO - __main__ -   Batch Number = 331
Evaluating:  85%|████████▍ | 331/391 [00:07<00:01, 44.20it/s]01/06/2022 17:18:29 - INFO - __main__ -   Batch Number = 332
01/06/2022 17:18:29 - INFO - __main__ -   Batch Number = 333
01/06/2022 17:18:29 - INFO - __main__ -   Batch Number = 334
01/06/2022 17:18:29 - INFO - __main__ -   Batch Number = 335
01/06/2022 17:18:29 - INFO - __main__ -   Batch Number = 336
Evaluating:  86%|████████▌ | 336/391 [00:07<00:01, 44.27it/s]01/06/2022 17:18:29 - INFO - __main__ -   Batch Number = 337
01/06/2022 17:18:29 - INFO - __main__ -   Batch Number = 338
01/06/2022 17:18:29 - INFO - __main__ -   Batch Number = 339
01/06/2022 17:18:29 - INFO - __main__ -   Batch Number = 340
01/06/2022 17:18:29 - INFO - __main__ -   Batch Number = 341
Evaluating:  87%|████████▋ | 341/391 [00:07<00:01, 44.23it/s]01/06/2022 17:18:29 - INFO - __main__ -   Batch Number = 342
01/06/2022 17:18:29 - INFO - __main__ -   Batch Number = 343
01/06/2022 17:18:29 - INFO - __main__ -   Batch Number = 344
01/06/2022 17:18:29 - INFO - __main__ -   Batch Number = 345
01/06/2022 17:18:29 - INFO - __main__ -   Batch Number = 346
Evaluating:  88%|████████▊ | 346/391 [00:07<00:01, 44.09it/s]01/06/2022 17:18:29 - INFO - __main__ -   Batch Number = 347
01/06/2022 17:18:29 - INFO - __main__ -   Batch Number = 348
01/06/2022 17:18:29 - INFO - __main__ -   Batch Number = 349
01/06/2022 17:18:29 - INFO - __main__ -   Batch Number = 350
01/06/2022 17:18:29 - INFO - __main__ -   Batch Number = 351
Evaluating:  90%|████████▉ | 351/391 [00:07<00:00, 44.24it/s]01/06/2022 17:18:29 - INFO - __main__ -   Batch Number = 352
01/06/2022 17:18:29 - INFO - __main__ -   Batch Number = 353
01/06/2022 17:18:29 - INFO - __main__ -   Batch Number = 354
01/06/2022 17:18:29 - INFO - __main__ -   Batch Number = 355
01/06/2022 17:18:29 - INFO - __main__ -   Batch Number = 356
Evaluating:  91%|█████████ | 356/391 [00:08<00:00, 44.22it/s]01/06/2022 17:18:29 - INFO - __main__ -   Batch Number = 357
01/06/2022 17:18:29 - INFO - __main__ -   Batch Number = 358
01/06/2022 17:18:29 - INFO - __main__ -   Batch Number = 359
01/06/2022 17:18:29 - INFO - __main__ -   Batch Number = 360
01/06/2022 17:18:29 - INFO - __main__ -   Batch Number = 361
Evaluating:  92%|█████████▏| 361/391 [00:08<00:00, 44.16it/s]01/06/2022 17:18:29 - INFO - __main__ -   Batch Number = 362
01/06/2022 17:18:29 - INFO - __main__ -   Batch Number = 363
01/06/2022 17:18:29 - INFO - __main__ -   Batch Number = 364
01/06/2022 17:18:29 - INFO - __main__ -   Batch Number = 365
01/06/2022 17:18:29 - INFO - __main__ -   Batch Number = 366
Evaluating:  94%|█████████▎| 366/391 [00:08<00:00, 44.16it/s]01/06/2022 17:18:29 - INFO - __main__ -   Batch Number = 367
01/06/2022 17:18:29 - INFO - __main__ -   Batch Number = 368
01/06/2022 17:18:29 - INFO - __main__ -   Batch Number = 369
01/06/2022 17:18:29 - INFO - __main__ -   Batch Number = 370
01/06/2022 17:18:30 - INFO - __main__ -   Batch Number = 371
Evaluating:  95%|█████████▍| 371/391 [00:08<00:00, 44.20it/s]01/06/2022 17:18:30 - INFO - __main__ -   Batch Number = 372
01/06/2022 17:18:30 - INFO - __main__ -   Batch Number = 373
01/06/2022 17:18:30 - INFO - __main__ -   Batch Number = 374
01/06/2022 17:18:30 - INFO - __main__ -   Batch Number = 375
01/06/2022 17:18:30 - INFO - __main__ -   Batch Number = 376
Evaluating:  96%|█████████▌| 376/391 [00:08<00:00, 44.33it/s]01/06/2022 17:18:30 - INFO - __main__ -   Batch Number = 377
01/06/2022 17:18:30 - INFO - __main__ -   Batch Number = 378
01/06/2022 17:18:30 - INFO - __main__ -   Batch Number = 379
01/06/2022 17:18:30 - INFO - __main__ -   Batch Number = 380
01/06/2022 17:18:30 - INFO - __main__ -   Batch Number = 381
Evaluating:  97%|█████████▋| 381/391 [00:08<00:00, 44.14it/s]01/06/2022 17:18:30 - INFO - __main__ -   Batch Number = 382
01/06/2022 17:18:30 - INFO - __main__ -   Batch Number = 383
01/06/2022 17:18:30 - INFO - __main__ -   Batch Number = 384
01/06/2022 17:18:30 - INFO - __main__ -   Batch Number = 385
01/06/2022 17:18:30 - INFO - __main__ -   Batch Number = 386
Evaluating:  99%|█████████▊| 386/391 [00:08<00:00, 44.19it/s]01/06/2022 17:18:30 - INFO - __main__ -   Batch Number = 387
01/06/2022 17:18:30 - INFO - __main__ -   Batch Number = 388
01/06/2022 17:18:30 - INFO - __main__ -   Batch Number = 389
01/06/2022 17:18:30 - INFO - __main__ -   Batch Number = 390
01/06/2022 17:18:30 - INFO - __main__ -   Batch Number = 391
Evaluating: 100%|██████████| 391/391 [00:08<00:00, 44.93it/s]Evaluating: 100%|██████████| 391/391 [00:08<00:00, 43.94it/s]
01/06/2022 17:18:30 - INFO - __main__ -     Evaluation done in total 8.898455 secs (0.002849 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/predictions_th_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/nbest_predictions_th_.json
01/06/2022 17:18:40 - INFO - __main__ -   Results = OrderedDict([('exact', 13.193277310924369), ('f1', 17.831178625296264), ('total', 1190), ('HasAns_exact', 13.193277310924369), ('HasAns_f1', 17.831178625296264), ('HasAns_total', 1190), ('best_exact', 13.193277310924369), ('best_exact_thresh', 0.0), ('best_f1', 17.831178625296264), ('best_f1_thresh', 0.0)])
01/06/2022 17:18:40 - INFO - __main__ -   Language adapter for tr not found, using en instead
01/06/2022 17:18:40 - INFO - __main__ -   Set active language adapter to en
01/06/2022 17:18:40 - INFO - __main__ -   Args Adapter Weight = None
01/06/2022 17:18:40 - INFO - __main__ -   Adapter Languages = ['en']
01/06/2022 17:18:40 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
  0%|          | 0/48 [00:00<?, ?it/s] 33%|███▎      | 16/48 [00:00<00:00, 135.84it/s] 62%|██████▎   | 30/48 [00:00<00:00, 125.32it/s] 96%|█████████▌| 46/48 [00:00<00:00, 138.05it/s]100%|██████████| 48/48 [00:00<00:00, 136.78it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<06:17,  3.15it/s]convert squad examples to features:  32%|███▏      | 385/1190 [00:00<00:01, 476.31it/s]convert squad examples to features:  89%|████████▉ | 1057/1190 [00:01<00:00, 1374.20it/s]convert squad examples to features: 100%|██████████| 1190/1190 [00:01<00:00, 1030.22it/s]/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|██████████| 1190/1190 [00:00<00:00, 662317.11it/s]
01/06/2022 17:18:43 - INFO - __main__ -   Local Rank = -1
01/06/2022 17:18:43 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.tr.json_bert-base-multilingual-cased_384_tr
01/06/2022 17:18:44 - INFO - __main__ -   ***** Running evaluation  *****
01/06/2022 17:18:44 - INFO - __main__ -     Num examples = 1360
01/06/2022 17:18:44 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/170 [00:00<?, ?it/s]01/06/2022 17:18:44 - INFO - __main__ -   Batch Number = 1
01/06/2022 17:18:44 - INFO - __main__ -   Batch Number = 2
01/06/2022 17:18:44 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|▏         | 3/170 [00:00<00:05, 29.62it/s]01/06/2022 17:18:44 - INFO - __main__ -   Batch Number = 4
01/06/2022 17:18:44 - INFO - __main__ -   Batch Number = 5
01/06/2022 17:18:44 - INFO - __main__ -   Batch Number = 6
01/06/2022 17:18:44 - INFO - __main__ -   Batch Number = 7
Evaluating:   4%|▍         | 7/170 [00:00<00:04, 32.61it/s]01/06/2022 17:18:44 - INFO - __main__ -   Batch Number = 8
01/06/2022 17:18:44 - INFO - __main__ -   Batch Number = 9
01/06/2022 17:18:44 - INFO - __main__ -   Batch Number = 10
01/06/2022 17:18:44 - INFO - __main__ -   Batch Number = 11
01/06/2022 17:18:44 - INFO - __main__ -   Batch Number = 12
Evaluating:   7%|▋         | 12/170 [00:00<00:04, 38.10it/s]01/06/2022 17:18:44 - INFO - __main__ -   Batch Number = 13
01/06/2022 17:18:44 - INFO - __main__ -   Batch Number = 14
01/06/2022 17:18:44 - INFO - __main__ -   Batch Number = 15
01/06/2022 17:18:44 - INFO - __main__ -   Batch Number = 16
01/06/2022 17:18:44 - INFO - __main__ -   Batch Number = 17
Evaluating:  10%|█         | 17/170 [00:00<00:03, 40.63it/s]01/06/2022 17:18:44 - INFO - __main__ -   Batch Number = 18
01/06/2022 17:18:44 - INFO - __main__ -   Batch Number = 19
01/06/2022 17:18:44 - INFO - __main__ -   Batch Number = 20
01/06/2022 17:18:44 - INFO - __main__ -   Batch Number = 21
01/06/2022 17:18:44 - INFO - __main__ -   Batch Number = 22
Evaluating:  13%|█▎        | 22/170 [00:00<00:03, 40.44it/s]01/06/2022 17:18:44 - INFO - __main__ -   Batch Number = 23
01/06/2022 17:18:44 - INFO - __main__ -   Batch Number = 24
01/06/2022 17:18:44 - INFO - __main__ -   Batch Number = 25
01/06/2022 17:18:44 - INFO - __main__ -   Batch Number = 26
01/06/2022 17:18:44 - INFO - __main__ -   Batch Number = 27
Evaluating:  16%|█▌        | 27/170 [00:00<00:03, 41.92it/s]01/06/2022 17:18:44 - INFO - __main__ -   Batch Number = 28
01/06/2022 17:18:44 - INFO - __main__ -   Batch Number = 29
01/06/2022 17:18:44 - INFO - __main__ -   Batch Number = 30
01/06/2022 17:18:44 - INFO - __main__ -   Batch Number = 31
01/06/2022 17:18:44 - INFO - __main__ -   Batch Number = 32
Evaluating:  19%|█▉        | 32/170 [00:00<00:03, 42.81it/s]01/06/2022 17:18:44 - INFO - __main__ -   Batch Number = 33
01/06/2022 17:18:44 - INFO - __main__ -   Batch Number = 34
01/06/2022 17:18:44 - INFO - __main__ -   Batch Number = 35
01/06/2022 17:18:44 - INFO - __main__ -   Batch Number = 36
01/06/2022 17:18:44 - INFO - __main__ -   Batch Number = 37
Evaluating:  22%|██▏       | 37/170 [00:00<00:03, 43.34it/s]01/06/2022 17:18:45 - INFO - __main__ -   Batch Number = 38
01/06/2022 17:18:45 - INFO - __main__ -   Batch Number = 39
01/06/2022 17:18:45 - INFO - __main__ -   Batch Number = 40
01/06/2022 17:18:45 - INFO - __main__ -   Batch Number = 41
01/06/2022 17:18:45 - INFO - __main__ -   Batch Number = 42
Evaluating:  25%|██▍       | 42/170 [00:01<00:02, 43.75it/s]01/06/2022 17:18:45 - INFO - __main__ -   Batch Number = 43
01/06/2022 17:18:45 - INFO - __main__ -   Batch Number = 44
01/06/2022 17:18:45 - INFO - __main__ -   Batch Number = 45
01/06/2022 17:18:45 - INFO - __main__ -   Batch Number = 46
01/06/2022 17:18:45 - INFO - __main__ -   Batch Number = 47
Evaluating:  28%|██▊       | 47/170 [00:01<00:02, 44.08it/s]01/06/2022 17:18:45 - INFO - __main__ -   Batch Number = 48
01/06/2022 17:18:45 - INFO - __main__ -   Batch Number = 49
01/06/2022 17:18:45 - INFO - __main__ -   Batch Number = 50
01/06/2022 17:18:45 - INFO - __main__ -   Batch Number = 51
01/06/2022 17:18:45 - INFO - __main__ -   Batch Number = 52
Evaluating:  31%|███       | 52/170 [00:01<00:02, 43.98it/s]01/06/2022 17:18:45 - INFO - __main__ -   Batch Number = 53
01/06/2022 17:18:45 - INFO - __main__ -   Batch Number = 54
01/06/2022 17:18:45 - INFO - __main__ -   Batch Number = 55
01/06/2022 17:18:45 - INFO - __main__ -   Batch Number = 56
01/06/2022 17:18:45 - INFO - __main__ -   Batch Number = 57
Evaluating:  34%|███▎      | 57/170 [00:01<00:02, 43.98it/s]01/06/2022 17:18:45 - INFO - __main__ -   Batch Number = 58
01/06/2022 17:18:45 - INFO - __main__ -   Batch Number = 59
01/06/2022 17:18:45 - INFO - __main__ -   Batch Number = 60
01/06/2022 17:18:45 - INFO - __main__ -   Batch Number = 61
01/06/2022 17:18:45 - INFO - __main__ -   Batch Number = 62
Evaluating:  36%|███▋      | 62/170 [00:01<00:02, 43.97it/s]01/06/2022 17:18:45 - INFO - __main__ -   Batch Number = 63
01/06/2022 17:18:45 - INFO - __main__ -   Batch Number = 64
01/06/2022 17:18:45 - INFO - __main__ -   Batch Number = 65
01/06/2022 17:18:45 - INFO - __main__ -   Batch Number = 66
01/06/2022 17:18:45 - INFO - __main__ -   Batch Number = 67
Evaluating:  39%|███▉      | 67/170 [00:01<00:02, 43.88it/s]01/06/2022 17:18:45 - INFO - __main__ -   Batch Number = 68
01/06/2022 17:18:45 - INFO - __main__ -   Batch Number = 69
01/06/2022 17:18:45 - INFO - __main__ -   Batch Number = 70
01/06/2022 17:18:45 - INFO - __main__ -   Batch Number = 71
01/06/2022 17:18:45 - INFO - __main__ -   Batch Number = 72
Evaluating:  42%|████▏     | 72/170 [00:01<00:02, 44.06it/s]01/06/2022 17:18:45 - INFO - __main__ -   Batch Number = 73
01/06/2022 17:18:45 - INFO - __main__ -   Batch Number = 74
01/06/2022 17:18:45 - INFO - __main__ -   Batch Number = 75
01/06/2022 17:18:45 - INFO - __main__ -   Batch Number = 76
01/06/2022 17:18:45 - INFO - __main__ -   Batch Number = 77
Evaluating:  45%|████▌     | 77/170 [00:01<00:02, 44.20it/s]01/06/2022 17:18:45 - INFO - __main__ -   Batch Number = 78
01/06/2022 17:18:45 - INFO - __main__ -   Batch Number = 79
01/06/2022 17:18:45 - INFO - __main__ -   Batch Number = 80
01/06/2022 17:18:45 - INFO - __main__ -   Batch Number = 81
01/06/2022 17:18:45 - INFO - __main__ -   Batch Number = 82
Evaluating:  48%|████▊     | 82/170 [00:01<00:01, 44.33it/s]01/06/2022 17:18:46 - INFO - __main__ -   Batch Number = 83
01/06/2022 17:18:46 - INFO - __main__ -   Batch Number = 84
01/06/2022 17:18:46 - INFO - __main__ -   Batch Number = 85
01/06/2022 17:18:46 - INFO - __main__ -   Batch Number = 86
01/06/2022 17:18:46 - INFO - __main__ -   Batch Number = 87
Evaluating:  51%|█████     | 87/170 [00:02<00:01, 44.40it/s]01/06/2022 17:18:46 - INFO - __main__ -   Batch Number = 88
01/06/2022 17:18:46 - INFO - __main__ -   Batch Number = 89
01/06/2022 17:18:46 - INFO - __main__ -   Batch Number = 90
01/06/2022 17:18:46 - INFO - __main__ -   Batch Number = 91
01/06/2022 17:18:46 - INFO - __main__ -   Batch Number = 92
Evaluating:  54%|█████▍    | 92/170 [00:02<00:01, 44.55it/s]01/06/2022 17:18:46 - INFO - __main__ -   Batch Number = 93
01/06/2022 17:18:46 - INFO - __main__ -   Batch Number = 94
01/06/2022 17:18:46 - INFO - __main__ -   Batch Number = 95
01/06/2022 17:18:46 - INFO - __main__ -   Batch Number = 96
01/06/2022 17:18:46 - INFO - __main__ -   Batch Number = 97
Evaluating:  57%|█████▋    | 97/170 [00:02<00:01, 44.45it/s]01/06/2022 17:18:46 - INFO - __main__ -   Batch Number = 98
01/06/2022 17:18:46 - INFO - __main__ -   Batch Number = 99
01/06/2022 17:18:46 - INFO - __main__ -   Batch Number = 100
01/06/2022 17:18:46 - INFO - __main__ -   Batch Number = 101
01/06/2022 17:18:46 - INFO - __main__ -   Batch Number = 102
Evaluating:  60%|██████    | 102/170 [00:02<00:01, 44.42it/s]01/06/2022 17:18:46 - INFO - __main__ -   Batch Number = 103
01/06/2022 17:18:46 - INFO - __main__ -   Batch Number = 104
01/06/2022 17:18:46 - INFO - __main__ -   Batch Number = 105
01/06/2022 17:18:46 - INFO - __main__ -   Batch Number = 106
01/06/2022 17:18:46 - INFO - __main__ -   Batch Number = 107
Evaluating:  63%|██████▎   | 107/170 [00:02<00:01, 44.40it/s]01/06/2022 17:18:46 - INFO - __main__ -   Batch Number = 108
01/06/2022 17:18:46 - INFO - __main__ -   Batch Number = 109
01/06/2022 17:18:46 - INFO - __main__ -   Batch Number = 110
01/06/2022 17:18:46 - INFO - __main__ -   Batch Number = 111
01/06/2022 17:18:46 - INFO - __main__ -   Batch Number = 112
Evaluating:  66%|██████▌   | 112/170 [00:02<00:01, 44.36it/s]01/06/2022 17:18:46 - INFO - __main__ -   Batch Number = 113
01/06/2022 17:18:46 - INFO - __main__ -   Batch Number = 114
01/06/2022 17:18:46 - INFO - __main__ -   Batch Number = 115
01/06/2022 17:18:46 - INFO - __main__ -   Batch Number = 116
01/06/2022 17:18:46 - INFO - __main__ -   Batch Number = 117
Evaluating:  69%|██████▉   | 117/170 [00:02<00:01, 44.39it/s]01/06/2022 17:18:46 - INFO - __main__ -   Batch Number = 118
01/06/2022 17:18:46 - INFO - __main__ -   Batch Number = 119
01/06/2022 17:18:46 - INFO - __main__ -   Batch Number = 120
01/06/2022 17:18:46 - INFO - __main__ -   Batch Number = 121
01/06/2022 17:18:46 - INFO - __main__ -   Batch Number = 122
Evaluating:  72%|███████▏  | 122/170 [00:02<00:01, 44.56it/s]01/06/2022 17:18:46 - INFO - __main__ -   Batch Number = 123
01/06/2022 17:18:46 - INFO - __main__ -   Batch Number = 124
01/06/2022 17:18:46 - INFO - __main__ -   Batch Number = 125
01/06/2022 17:18:46 - INFO - __main__ -   Batch Number = 126
01/06/2022 17:18:47 - INFO - __main__ -   Batch Number = 127
Evaluating:  75%|███████▍  | 127/170 [00:02<00:00, 44.44it/s]01/06/2022 17:18:47 - INFO - __main__ -   Batch Number = 128
01/06/2022 17:18:47 - INFO - __main__ -   Batch Number = 129
01/06/2022 17:18:47 - INFO - __main__ -   Batch Number = 130
01/06/2022 17:18:47 - INFO - __main__ -   Batch Number = 131
01/06/2022 17:18:47 - INFO - __main__ -   Batch Number = 132
Evaluating:  78%|███████▊  | 132/170 [00:03<00:00, 44.58it/s]01/06/2022 17:18:47 - INFO - __main__ -   Batch Number = 133
01/06/2022 17:18:47 - INFO - __main__ -   Batch Number = 134
01/06/2022 17:18:47 - INFO - __main__ -   Batch Number = 135
01/06/2022 17:18:47 - INFO - __main__ -   Batch Number = 136
01/06/2022 17:18:47 - INFO - __main__ -   Batch Number = 137
Evaluating:  81%|████████  | 137/170 [00:03<00:00, 44.57it/s]01/06/2022 17:18:47 - INFO - __main__ -   Batch Number = 138
01/06/2022 17:18:47 - INFO - __main__ -   Batch Number = 139
01/06/2022 17:18:47 - INFO - __main__ -   Batch Number = 140
01/06/2022 17:18:47 - INFO - __main__ -   Batch Number = 141
01/06/2022 17:18:47 - INFO - __main__ -   Batch Number = 142
Evaluating:  84%|████████▎ | 142/170 [00:03<00:00, 44.57it/s]01/06/2022 17:18:47 - INFO - __main__ -   Batch Number = 143
01/06/2022 17:18:47 - INFO - __main__ -   Batch Number = 144
01/06/2022 17:18:47 - INFO - __main__ -   Batch Number = 145
01/06/2022 17:18:47 - INFO - __main__ -   Batch Number = 146
01/06/2022 17:18:47 - INFO - __main__ -   Batch Number = 147
Evaluating:  86%|████████▋ | 147/170 [00:03<00:00, 44.57it/s]01/06/2022 17:18:47 - INFO - __main__ -   Batch Number = 148
01/06/2022 17:18:47 - INFO - __main__ -   Batch Number = 149
01/06/2022 17:18:47 - INFO - __main__ -   Batch Number = 150
01/06/2022 17:18:47 - INFO - __main__ -   Batch Number = 151
01/06/2022 17:18:47 - INFO - __main__ -   Batch Number = 152
Evaluating:  89%|████████▉ | 152/170 [00:03<00:00, 44.56it/s]01/06/2022 17:18:47 - INFO - __main__ -   Batch Number = 153
01/06/2022 17:18:47 - INFO - __main__ -   Batch Number = 154
01/06/2022 17:18:47 - INFO - __main__ -   Batch Number = 155
01/06/2022 17:18:47 - INFO - __main__ -   Batch Number = 156
01/06/2022 17:18:47 - INFO - __main__ -   Batch Number = 157
Evaluating:  92%|█████████▏| 157/170 [00:03<00:00, 42.59it/s]01/06/2022 17:18:47 - INFO - __main__ -   Batch Number = 158
01/06/2022 17:18:47 - INFO - __main__ -   Batch Number = 159
01/06/2022 17:18:47 - INFO - __main__ -   Batch Number = 160
01/06/2022 17:18:47 - INFO - __main__ -   Batch Number = 161
01/06/2022 17:18:47 - INFO - __main__ -   Batch Number = 162
Evaluating:  95%|█████████▌| 162/170 [00:03<00:00, 43.28it/s]01/06/2022 17:18:47 - INFO - __main__ -   Batch Number = 163
01/06/2022 17:18:47 - INFO - __main__ -   Batch Number = 164
01/06/2022 17:18:47 - INFO - __main__ -   Batch Number = 165
01/06/2022 17:18:47 - INFO - __main__ -   Batch Number = 166
01/06/2022 17:18:47 - INFO - __main__ -   Batch Number = 167
Evaluating:  98%|█████████▊| 167/170 [00:03<00:00, 43.54it/s]01/06/2022 17:18:47 - INFO - __main__ -   Batch Number = 168
01/06/2022 17:18:47 - INFO - __main__ -   Batch Number = 169
01/06/2022 17:18:47 - INFO - __main__ -   Batch Number = 170
Evaluating: 100%|██████████| 170/170 [00:03<00:00, 43.45it/s]
01/06/2022 17:18:48 - INFO - __main__ -     Evaluation done in total 3.912744 secs (0.002877 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/predictions_tr_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/nbest_predictions_tr_.json
01/06/2022 17:18:51 - INFO - __main__ -   Results = OrderedDict([('exact', 32.52100840336134), ('f1', 47.71934586244814), ('total', 1190), ('HasAns_exact', 32.52100840336134), ('HasAns_f1', 47.71934586244814), ('HasAns_total', 1190), ('best_exact', 32.52100840336134), ('best_exact_thresh', 0.0), ('best_f1', 47.71934586244814), ('best_f1_thresh', 0.0)])
01/06/2022 17:18:51 - INFO - __main__ -   Language adapter for vi not found, using en instead
01/06/2022 17:18:51 - INFO - __main__ -   Set active language adapter to en
01/06/2022 17:18:51 - INFO - __main__ -   Args Adapter Weight = None
01/06/2022 17:18:51 - INFO - __main__ -   Adapter Languages = ['en']
01/06/2022 17:18:51 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
  0%|          | 0/48 [00:00<?, ?it/s] 33%|███▎      | 16/48 [00:00<00:00, 133.56it/s] 62%|██████▎   | 30/48 [00:00<00:00, 128.39it/s] 96%|█████████▌| 46/48 [00:00<00:00, 139.85it/s]100%|██████████| 48/48 [00:00<00:00, 138.37it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<08:10,  2.43it/s]convert squad examples to features:  32%|███▏      | 385/1190 [00:01<00:02, 389.43it/s]convert squad examples to features:  89%|████████▉ | 1057/1190 [00:01<00:00, 1115.84it/s]convert squad examples to features: 100%|██████████| 1190/1190 [00:01<00:00, 850.33it/s] /root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|██████████| 1190/1190 [00:00<00:00, 756474.96it/s]
01/06/2022 17:18:54 - INFO - __main__ -   Local Rank = -1
01/06/2022 17:18:54 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.vi.json_bert-base-multilingual-cased_384_vi
01/06/2022 17:18:54 - INFO - __main__ -   ***** Running evaluation  *****
01/06/2022 17:18:54 - INFO - __main__ -     Num examples = 1325
01/06/2022 17:18:54 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/166 [00:00<?, ?it/s]01/06/2022 17:18:54 - INFO - __main__ -   Batch Number = 1
01/06/2022 17:18:55 - INFO - __main__ -   Batch Number = 2
01/06/2022 17:18:55 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|▏         | 3/166 [00:00<00:06, 24.45it/s]01/06/2022 17:18:55 - INFO - __main__ -   Batch Number = 4
01/06/2022 17:18:55 - INFO - __main__ -   Batch Number = 5
01/06/2022 17:18:55 - INFO - __main__ -   Batch Number = 6
01/06/2022 17:18:55 - INFO - __main__ -   Batch Number = 7
Evaluating:   4%|▍         | 7/166 [00:00<00:05, 30.50it/s]01/06/2022 17:18:55 - INFO - __main__ -   Batch Number = 8
01/06/2022 17:18:55 - INFO - __main__ -   Batch Number = 9
01/06/2022 17:18:55 - INFO - __main__ -   Batch Number = 10
01/06/2022 17:18:55 - INFO - __main__ -   Batch Number = 11
01/06/2022 17:18:55 - INFO - __main__ -   Batch Number = 12
Evaluating:   7%|▋         | 12/166 [00:00<00:04, 36.57it/s]01/06/2022 17:18:55 - INFO - __main__ -   Batch Number = 13
01/06/2022 17:18:55 - INFO - __main__ -   Batch Number = 14
01/06/2022 17:18:55 - INFO - __main__ -   Batch Number = 15
01/06/2022 17:18:55 - INFO - __main__ -   Batch Number = 16
01/06/2022 17:18:55 - INFO - __main__ -   Batch Number = 17
Evaluating:  10%|█         | 17/166 [00:00<00:03, 39.74it/s]01/06/2022 17:18:55 - INFO - __main__ -   Batch Number = 18
01/06/2022 17:18:55 - INFO - __main__ -   Batch Number = 19
01/06/2022 17:18:55 - INFO - __main__ -   Batch Number = 20
01/06/2022 17:18:55 - INFO - __main__ -   Batch Number = 21
01/06/2022 17:18:55 - INFO - __main__ -   Batch Number = 22
Evaluating:  13%|█▎        | 22/166 [00:00<00:03, 41.14it/s]01/06/2022 17:18:55 - INFO - __main__ -   Batch Number = 23
01/06/2022 17:18:55 - INFO - __main__ -   Batch Number = 24
01/06/2022 17:18:55 - INFO - __main__ -   Batch Number = 25
01/06/2022 17:18:55 - INFO - __main__ -   Batch Number = 26
01/06/2022 17:18:55 - INFO - __main__ -   Batch Number = 27
Evaluating:  16%|█▋        | 27/166 [00:00<00:03, 42.42it/s]01/06/2022 17:18:55 - INFO - __main__ -   Batch Number = 28
01/06/2022 17:18:55 - INFO - __main__ -   Batch Number = 29
01/06/2022 17:18:55 - INFO - __main__ -   Batch Number = 30
01/06/2022 17:18:55 - INFO - __main__ -   Batch Number = 31
01/06/2022 17:18:55 - INFO - __main__ -   Batch Number = 32
Evaluating:  19%|█▉        | 32/166 [00:00<00:03, 43.09it/s]01/06/2022 17:18:55 - INFO - __main__ -   Batch Number = 33
01/06/2022 17:18:55 - INFO - __main__ -   Batch Number = 34
01/06/2022 17:18:55 - INFO - __main__ -   Batch Number = 35
01/06/2022 17:18:55 - INFO - __main__ -   Batch Number = 36
01/06/2022 17:18:55 - INFO - __main__ -   Batch Number = 37
Evaluating:  22%|██▏       | 37/166 [00:00<00:02, 43.41it/s]01/06/2022 17:18:55 - INFO - __main__ -   Batch Number = 38
01/06/2022 17:18:55 - INFO - __main__ -   Batch Number = 39
01/06/2022 17:18:55 - INFO - __main__ -   Batch Number = 40
01/06/2022 17:18:55 - INFO - __main__ -   Batch Number = 41
01/06/2022 17:18:55 - INFO - __main__ -   Batch Number = 42
Evaluating:  25%|██▌       | 42/166 [00:01<00:02, 43.69it/s]01/06/2022 17:18:56 - INFO - __main__ -   Batch Number = 43
01/06/2022 17:18:56 - INFO - __main__ -   Batch Number = 44
01/06/2022 17:18:56 - INFO - __main__ -   Batch Number = 45
01/06/2022 17:18:56 - INFO - __main__ -   Batch Number = 46
01/06/2022 17:18:56 - INFO - __main__ -   Batch Number = 47
Evaluating:  28%|██▊       | 47/166 [00:01<00:02, 44.08it/s]01/06/2022 17:18:56 - INFO - __main__ -   Batch Number = 48
01/06/2022 17:18:56 - INFO - __main__ -   Batch Number = 49
01/06/2022 17:18:56 - INFO - __main__ -   Batch Number = 50
01/06/2022 17:18:56 - INFO - __main__ -   Batch Number = 51
01/06/2022 17:18:56 - INFO - __main__ -   Batch Number = 52
Evaluating:  31%|███▏      | 52/166 [00:01<00:02, 44.08it/s]01/06/2022 17:18:56 - INFO - __main__ -   Batch Number = 53
01/06/2022 17:18:56 - INFO - __main__ -   Batch Number = 54
01/06/2022 17:18:56 - INFO - __main__ -   Batch Number = 55
01/06/2022 17:18:56 - INFO - __main__ -   Batch Number = 56
01/06/2022 17:18:56 - INFO - __main__ -   Batch Number = 57
Evaluating:  34%|███▍      | 57/166 [00:01<00:02, 44.06it/s]01/06/2022 17:18:56 - INFO - __main__ -   Batch Number = 58
01/06/2022 17:18:56 - INFO - __main__ -   Batch Number = 59
01/06/2022 17:18:56 - INFO - __main__ -   Batch Number = 60
01/06/2022 17:18:56 - INFO - __main__ -   Batch Number = 61
01/06/2022 17:18:56 - INFO - __main__ -   Batch Number = 62
Evaluating:  37%|███▋      | 62/166 [00:01<00:02, 44.18it/s]01/06/2022 17:18:56 - INFO - __main__ -   Batch Number = 63
01/06/2022 17:18:56 - INFO - __main__ -   Batch Number = 64
01/06/2022 17:18:56 - INFO - __main__ -   Batch Number = 65
01/06/2022 17:18:56 - INFO - __main__ -   Batch Number = 66
01/06/2022 17:18:56 - INFO - __main__ -   Batch Number = 67
Evaluating:  40%|████      | 67/166 [00:01<00:02, 44.12it/s]01/06/2022 17:18:56 - INFO - __main__ -   Batch Number = 68
01/06/2022 17:18:56 - INFO - __main__ -   Batch Number = 69
01/06/2022 17:18:56 - INFO - __main__ -   Batch Number = 70
01/06/2022 17:18:56 - INFO - __main__ -   Batch Number = 71
01/06/2022 17:18:56 - INFO - __main__ -   Batch Number = 72
Evaluating:  43%|████▎     | 72/166 [00:01<00:02, 44.23it/s]01/06/2022 17:18:56 - INFO - __main__ -   Batch Number = 73
01/06/2022 17:18:56 - INFO - __main__ -   Batch Number = 74
01/06/2022 17:18:56 - INFO - __main__ -   Batch Number = 75
01/06/2022 17:18:56 - INFO - __main__ -   Batch Number = 76
01/06/2022 17:18:56 - INFO - __main__ -   Batch Number = 77
Evaluating:  46%|████▋     | 77/166 [00:01<00:02, 44.18it/s]01/06/2022 17:18:56 - INFO - __main__ -   Batch Number = 78
01/06/2022 17:18:56 - INFO - __main__ -   Batch Number = 79
01/06/2022 17:18:56 - INFO - __main__ -   Batch Number = 80
01/06/2022 17:18:56 - INFO - __main__ -   Batch Number = 81
01/06/2022 17:18:56 - INFO - __main__ -   Batch Number = 82
Evaluating:  49%|████▉     | 82/166 [00:01<00:01, 44.12it/s]01/06/2022 17:18:56 - INFO - __main__ -   Batch Number = 83
01/06/2022 17:18:56 - INFO - __main__ -   Batch Number = 84
01/06/2022 17:18:56 - INFO - __main__ -   Batch Number = 85
01/06/2022 17:18:56 - INFO - __main__ -   Batch Number = 86
01/06/2022 17:18:57 - INFO - __main__ -   Batch Number = 87
Evaluating:  52%|█████▏    | 87/166 [00:02<00:01, 44.20it/s]01/06/2022 17:18:57 - INFO - __main__ -   Batch Number = 88
01/06/2022 17:18:57 - INFO - __main__ -   Batch Number = 89
01/06/2022 17:18:57 - INFO - __main__ -   Batch Number = 90
01/06/2022 17:18:57 - INFO - __main__ -   Batch Number = 91
01/06/2022 17:18:57 - INFO - __main__ -   Batch Number = 92
Evaluating:  55%|█████▌    | 92/166 [00:02<00:01, 44.08it/s]01/06/2022 17:18:57 - INFO - __main__ -   Batch Number = 93
01/06/2022 17:18:57 - INFO - __main__ -   Batch Number = 94
01/06/2022 17:18:57 - INFO - __main__ -   Batch Number = 95
01/06/2022 17:18:57 - INFO - __main__ -   Batch Number = 96
01/06/2022 17:18:57 - INFO - __main__ -   Batch Number = 97
Evaluating:  58%|█████▊    | 97/166 [00:02<00:01, 44.07it/s]01/06/2022 17:18:57 - INFO - __main__ -   Batch Number = 98
01/06/2022 17:18:57 - INFO - __main__ -   Batch Number = 99
01/06/2022 17:18:57 - INFO - __main__ -   Batch Number = 100
01/06/2022 17:18:57 - INFO - __main__ -   Batch Number = 101
01/06/2022 17:18:57 - INFO - __main__ -   Batch Number = 102
Evaluating:  61%|██████▏   | 102/166 [00:02<00:01, 43.93it/s]01/06/2022 17:18:57 - INFO - __main__ -   Batch Number = 103
01/06/2022 17:18:57 - INFO - __main__ -   Batch Number = 104
01/06/2022 17:18:57 - INFO - __main__ -   Batch Number = 105
01/06/2022 17:18:57 - INFO - __main__ -   Batch Number = 106
01/06/2022 17:18:57 - INFO - __main__ -   Batch Number = 107
Evaluating:  64%|██████▍   | 107/166 [00:02<00:01, 44.07it/s]01/06/2022 17:18:57 - INFO - __main__ -   Batch Number = 108
01/06/2022 17:18:57 - INFO - __main__ -   Batch Number = 109
01/06/2022 17:18:57 - INFO - __main__ -   Batch Number = 110
01/06/2022 17:18:57 - INFO - __main__ -   Batch Number = 111
01/06/2022 17:18:57 - INFO - __main__ -   Batch Number = 112
Evaluating:  67%|██████▋   | 112/166 [00:02<00:01, 44.11it/s]01/06/2022 17:18:57 - INFO - __main__ -   Batch Number = 113
01/06/2022 17:18:57 - INFO - __main__ -   Batch Number = 114
01/06/2022 17:18:57 - INFO - __main__ -   Batch Number = 115
01/06/2022 17:18:57 - INFO - __main__ -   Batch Number = 116
01/06/2022 17:18:57 - INFO - __main__ -   Batch Number = 117
Evaluating:  70%|███████   | 117/166 [00:02<00:01, 44.13it/s]01/06/2022 17:18:57 - INFO - __main__ -   Batch Number = 118
01/06/2022 17:18:57 - INFO - __main__ -   Batch Number = 119
01/06/2022 17:18:57 - INFO - __main__ -   Batch Number = 120
01/06/2022 17:18:57 - INFO - __main__ -   Batch Number = 121
01/06/2022 17:18:57 - INFO - __main__ -   Batch Number = 122
Evaluating:  73%|███████▎  | 122/166 [00:02<00:00, 44.27it/s]01/06/2022 17:18:57 - INFO - __main__ -   Batch Number = 123
01/06/2022 17:18:57 - INFO - __main__ -   Batch Number = 124
01/06/2022 17:18:57 - INFO - __main__ -   Batch Number = 125
01/06/2022 17:18:57 - INFO - __main__ -   Batch Number = 126
01/06/2022 17:18:57 - INFO - __main__ -   Batch Number = 127
Evaluating:  77%|███████▋  | 127/166 [00:02<00:00, 44.12it/s]01/06/2022 17:18:57 - INFO - __main__ -   Batch Number = 128
01/06/2022 17:18:57 - INFO - __main__ -   Batch Number = 129
01/06/2022 17:18:57 - INFO - __main__ -   Batch Number = 130
01/06/2022 17:18:58 - INFO - __main__ -   Batch Number = 131
01/06/2022 17:18:58 - INFO - __main__ -   Batch Number = 132
Evaluating:  80%|███████▉  | 132/166 [00:03<00:00, 44.22it/s]01/06/2022 17:18:58 - INFO - __main__ -   Batch Number = 133
01/06/2022 17:18:58 - INFO - __main__ -   Batch Number = 134
01/06/2022 17:18:58 - INFO - __main__ -   Batch Number = 135
01/06/2022 17:18:58 - INFO - __main__ -   Batch Number = 136
01/06/2022 17:18:58 - INFO - __main__ -   Batch Number = 137
Evaluating:  83%|████████▎ | 137/166 [00:03<00:00, 44.16it/s]01/06/2022 17:18:58 - INFO - __main__ -   Batch Number = 138
01/06/2022 17:18:58 - INFO - __main__ -   Batch Number = 139
01/06/2022 17:18:58 - INFO - __main__ -   Batch Number = 140
01/06/2022 17:18:58 - INFO - __main__ -   Batch Number = 141
01/06/2022 17:18:58 - INFO - __main__ -   Batch Number = 142
Evaluating:  86%|████████▌ | 142/166 [00:03<00:00, 44.17it/s]01/06/2022 17:18:58 - INFO - __main__ -   Batch Number = 143
01/06/2022 17:18:58 - INFO - __main__ -   Batch Number = 144
01/06/2022 17:18:58 - INFO - __main__ -   Batch Number = 145
01/06/2022 17:18:58 - INFO - __main__ -   Batch Number = 146
01/06/2022 17:18:58 - INFO - __main__ -   Batch Number = 147
Evaluating:  89%|████████▊ | 147/166 [00:03<00:00, 44.04it/s]01/06/2022 17:18:58 - INFO - __main__ -   Batch Number = 148
01/06/2022 17:18:58 - INFO - __main__ -   Batch Number = 149
01/06/2022 17:18:58 - INFO - __main__ -   Batch Number = 150
01/06/2022 17:18:58 - INFO - __main__ -   Batch Number = 151
01/06/2022 17:18:58 - INFO - __main__ -   Batch Number = 152
Evaluating:  92%|█████████▏| 152/166 [00:03<00:00, 44.00it/s]01/06/2022 17:18:58 - INFO - __main__ -   Batch Number = 153
01/06/2022 17:18:58 - INFO - __main__ -   Batch Number = 154
01/06/2022 17:18:58 - INFO - __main__ -   Batch Number = 155
01/06/2022 17:18:58 - INFO - __main__ -   Batch Number = 156
01/06/2022 17:18:58 - INFO - __main__ -   Batch Number = 157
Evaluating:  95%|█████████▍| 157/166 [00:03<00:00, 44.04it/s]01/06/2022 17:18:58 - INFO - __main__ -   Batch Number = 158
01/06/2022 17:18:58 - INFO - __main__ -   Batch Number = 159
01/06/2022 17:18:58 - INFO - __main__ -   Batch Number = 160
01/06/2022 17:18:58 - INFO - __main__ -   Batch Number = 161
01/06/2022 17:18:58 - INFO - __main__ -   Batch Number = 162
Evaluating:  98%|█████████▊| 162/166 [00:03<00:00, 44.15it/s]01/06/2022 17:18:58 - INFO - __main__ -   Batch Number = 163
01/06/2022 17:18:58 - INFO - __main__ -   Batch Number = 164
01/06/2022 17:18:58 - INFO - __main__ -   Batch Number = 165
01/06/2022 17:18:58 - INFO - __main__ -   Batch Number = 166
Evaluating: 100%|██████████| 166/166 [00:03<00:00, 43.36it/s]
01/06/2022 17:18:58 - INFO - __main__ -     Evaluation done in total 3.829079 secs (0.002890 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/predictions_vi_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/nbest_predictions_vi_.json
01/06/2022 17:19:02 - INFO - __main__ -   Results = OrderedDict([('exact', 47.39495798319328), ('f1', 67.01604077310803), ('total', 1190), ('HasAns_exact', 47.39495798319328), ('HasAns_f1', 67.01604077310803), ('HasAns_total', 1190), ('best_exact', 47.39495798319328), ('best_exact_thresh', 0.0), ('best_f1', 67.01604077310803), ('best_f1_thresh', 0.0)])
01/06/2022 17:19:02 - INFO - __main__ -   Language adapter for zh not found, using en instead
01/06/2022 17:19:02 - INFO - __main__ -   Set active language adapter to en
01/06/2022 17:19:02 - INFO - __main__ -   Args Adapter Weight = None
01/06/2022 17:19:02 - INFO - __main__ -   Adapter Languages = ['en']
01/06/2022 17:19:02 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
  0%|          | 0/48 [00:00<?, ?it/s] 75%|███████▌  | 36/48 [00:00<00:00, 354.30it/s]100%|██████████| 48/48 [00:00<00:00, 374.45it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<04:25,  4.47it/s]convert squad examples to features:  32%|███▏      | 385/1190 [00:00<00:01, 794.39it/s]convert squad examples to features: 100%|██████████| 1190/1190 [00:00<00:00, 2086.31it/s]/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|██████████| 1190/1190 [00:00<00:00, 627037.91it/s]
01/06/2022 17:19:03 - INFO - __main__ -   Local Rank = -1
01/06/2022 17:19:03 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.zh.json_bert-base-multilingual-cased_384_zh
01/06/2022 17:19:04 - INFO - __main__ -   ***** Running evaluation  *****
01/06/2022 17:19:04 - INFO - __main__ -     Num examples = 1374
01/06/2022 17:19:04 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/172 [00:00<?, ?it/s]01/06/2022 17:19:04 - INFO - __main__ -   Batch Number = 1
01/06/2022 17:19:04 - INFO - __main__ -   Batch Number = 2
01/06/2022 17:19:04 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|▏         | 3/172 [00:00<00:05, 29.59it/s]01/06/2022 17:19:04 - INFO - __main__ -   Batch Number = 4
01/06/2022 17:19:04 - INFO - __main__ -   Batch Number = 5
01/06/2022 17:19:04 - INFO - __main__ -   Batch Number = 6
Evaluating:   3%|▎         | 6/172 [00:00<00:05, 29.79it/s]01/06/2022 17:19:05 - INFO - __main__ -   Batch Number = 7
01/06/2022 17:19:05 - INFO - __main__ -   Batch Number = 8
01/06/2022 17:19:05 - INFO - __main__ -   Batch Number = 9
01/06/2022 17:19:05 - INFO - __main__ -   Batch Number = 10
Evaluating:   6%|▌         | 10/172 [00:00<00:04, 33.06it/s]01/06/2022 17:19:05 - INFO - __main__ -   Batch Number = 11
01/06/2022 17:19:05 - INFO - __main__ -   Batch Number = 12
01/06/2022 17:19:05 - INFO - __main__ -   Batch Number = 13
01/06/2022 17:19:05 - INFO - __main__ -   Batch Number = 14
01/06/2022 17:19:05 - INFO - __main__ -   Batch Number = 15
Evaluating:   9%|▊         | 15/172 [00:00<00:04, 37.96it/s]01/06/2022 17:19:05 - INFO - __main__ -   Batch Number = 16
01/06/2022 17:19:05 - INFO - __main__ -   Batch Number = 17
01/06/2022 17:19:05 - INFO - __main__ -   Batch Number = 18
01/06/2022 17:19:05 - INFO - __main__ -   Batch Number = 19
Evaluating:  11%|█         | 19/172 [00:00<00:04, 36.88it/s]01/06/2022 17:19:05 - INFO - __main__ -   Batch Number = 20
01/06/2022 17:19:05 - INFO - __main__ -   Batch Number = 21
01/06/2022 17:19:05 - INFO - __main__ -   Batch Number = 22
01/06/2022 17:19:05 - INFO - __main__ -   Batch Number = 23
01/06/2022 17:19:05 - INFO - __main__ -   Batch Number = 24
Evaluating:  14%|█▍        | 24/172 [00:00<00:03, 39.48it/s]01/06/2022 17:19:05 - INFO - __main__ -   Batch Number = 25
01/06/2022 17:19:05 - INFO - __main__ -   Batch Number = 26
01/06/2022 17:19:05 - INFO - __main__ -   Batch Number = 27
01/06/2022 17:19:05 - INFO - __main__ -   Batch Number = 28
01/06/2022 17:19:05 - INFO - __main__ -   Batch Number = 29
Evaluating:  17%|█▋        | 29/172 [00:00<00:03, 41.11it/s]01/06/2022 17:19:05 - INFO - __main__ -   Batch Number = 30
01/06/2022 17:19:05 - INFO - __main__ -   Batch Number = 31
01/06/2022 17:19:05 - INFO - __main__ -   Batch Number = 32
01/06/2022 17:19:05 - INFO - __main__ -   Batch Number = 33
01/06/2022 17:19:05 - INFO - __main__ -   Batch Number = 34
Evaluating:  20%|█▉        | 34/172 [00:00<00:03, 42.04it/s]01/06/2022 17:19:05 - INFO - __main__ -   Batch Number = 35
01/06/2022 17:19:05 - INFO - __main__ -   Batch Number = 36
01/06/2022 17:19:05 - INFO - __main__ -   Batch Number = 37
01/06/2022 17:19:05 - INFO - __main__ -   Batch Number = 38
01/06/2022 17:19:05 - INFO - __main__ -   Batch Number = 39
Evaluating:  23%|██▎       | 39/172 [00:00<00:03, 42.89it/s]01/06/2022 17:19:05 - INFO - __main__ -   Batch Number = 40
01/06/2022 17:19:05 - INFO - __main__ -   Batch Number = 41
01/06/2022 17:19:05 - INFO - __main__ -   Batch Number = 42
01/06/2022 17:19:05 - INFO - __main__ -   Batch Number = 43
01/06/2022 17:19:05 - INFO - __main__ -   Batch Number = 44
Evaluating:  26%|██▌       | 44/172 [00:01<00:02, 43.26it/s]01/06/2022 17:19:05 - INFO - __main__ -   Batch Number = 45
01/06/2022 17:19:05 - INFO - __main__ -   Batch Number = 46
01/06/2022 17:19:05 - INFO - __main__ -   Batch Number = 47
01/06/2022 17:19:05 - INFO - __main__ -   Batch Number = 48
01/06/2022 17:19:05 - INFO - __main__ -   Batch Number = 49
Evaluating:  28%|██▊       | 49/172 [00:01<00:02, 43.54it/s]01/06/2022 17:19:06 - INFO - __main__ -   Batch Number = 50
01/06/2022 17:19:06 - INFO - __main__ -   Batch Number = 51
01/06/2022 17:19:06 - INFO - __main__ -   Batch Number = 52
01/06/2022 17:19:06 - INFO - __main__ -   Batch Number = 53
01/06/2022 17:19:06 - INFO - __main__ -   Batch Number = 54
Evaluating:  31%|███▏      | 54/172 [00:01<00:02, 43.76it/s]01/06/2022 17:19:06 - INFO - __main__ -   Batch Number = 55
01/06/2022 17:19:06 - INFO - __main__ -   Batch Number = 56
01/06/2022 17:19:06 - INFO - __main__ -   Batch Number = 57
01/06/2022 17:19:06 - INFO - __main__ -   Batch Number = 58
01/06/2022 17:19:06 - INFO - __main__ -   Batch Number = 59
Evaluating:  34%|███▍      | 59/172 [00:01<00:02, 43.94it/s]01/06/2022 17:19:06 - INFO - __main__ -   Batch Number = 60
01/06/2022 17:19:06 - INFO - __main__ -   Batch Number = 61
01/06/2022 17:19:06 - INFO - __main__ -   Batch Number = 62
01/06/2022 17:19:06 - INFO - __main__ -   Batch Number = 63
01/06/2022 17:19:06 - INFO - __main__ -   Batch Number = 64
Evaluating:  37%|███▋      | 64/172 [00:01<00:02, 43.92it/s]01/06/2022 17:19:06 - INFO - __main__ -   Batch Number = 65
01/06/2022 17:19:06 - INFO - __main__ -   Batch Number = 66
01/06/2022 17:19:06 - INFO - __main__ -   Batch Number = 67
01/06/2022 17:19:06 - INFO - __main__ -   Batch Number = 68
01/06/2022 17:19:06 - INFO - __main__ -   Batch Number = 69
Evaluating:  40%|████      | 69/172 [00:01<00:02, 44.00it/s]01/06/2022 17:19:06 - INFO - __main__ -   Batch Number = 70
01/06/2022 17:19:06 - INFO - __main__ -   Batch Number = 71
01/06/2022 17:19:06 - INFO - __main__ -   Batch Number = 72
01/06/2022 17:19:06 - INFO - __main__ -   Batch Number = 73
01/06/2022 17:19:06 - INFO - __main__ -   Batch Number = 74
Evaluating:  43%|████▎     | 74/172 [00:01<00:02, 44.18it/s]01/06/2022 17:19:06 - INFO - __main__ -   Batch Number = 75
01/06/2022 17:19:06 - INFO - __main__ -   Batch Number = 76
01/06/2022 17:19:06 - INFO - __main__ -   Batch Number = 77
01/06/2022 17:19:06 - INFO - __main__ -   Batch Number = 78
01/06/2022 17:19:06 - INFO - __main__ -   Batch Number = 79
Evaluating:  46%|████▌     | 79/172 [00:01<00:02, 44.03it/s]01/06/2022 17:19:06 - INFO - __main__ -   Batch Number = 80
01/06/2022 17:19:06 - INFO - __main__ -   Batch Number = 81
01/06/2022 17:19:06 - INFO - __main__ -   Batch Number = 82
01/06/2022 17:19:06 - INFO - __main__ -   Batch Number = 83
01/06/2022 17:19:06 - INFO - __main__ -   Batch Number = 84
Evaluating:  49%|████▉     | 84/172 [00:02<00:01, 44.11it/s]01/06/2022 17:19:06 - INFO - __main__ -   Batch Number = 85
01/06/2022 17:19:06 - INFO - __main__ -   Batch Number = 86
01/06/2022 17:19:06 - INFO - __main__ -   Batch Number = 87
01/06/2022 17:19:06 - INFO - __main__ -   Batch Number = 88
01/06/2022 17:19:06 - INFO - __main__ -   Batch Number = 89
Evaluating:  52%|█████▏    | 89/172 [00:02<00:01, 44.24it/s]01/06/2022 17:19:06 - INFO - __main__ -   Batch Number = 90
01/06/2022 17:19:06 - INFO - __main__ -   Batch Number = 91
01/06/2022 17:19:06 - INFO - __main__ -   Batch Number = 92
01/06/2022 17:19:06 - INFO - __main__ -   Batch Number = 93
01/06/2022 17:19:07 - INFO - __main__ -   Batch Number = 94
Evaluating:  55%|█████▍    | 94/172 [00:02<00:01, 44.03it/s]01/06/2022 17:19:07 - INFO - __main__ -   Batch Number = 95
01/06/2022 17:19:07 - INFO - __main__ -   Batch Number = 96
01/06/2022 17:19:07 - INFO - __main__ -   Batch Number = 97
01/06/2022 17:19:07 - INFO - __main__ -   Batch Number = 98
01/06/2022 17:19:07 - INFO - __main__ -   Batch Number = 99
Evaluating:  58%|█████▊    | 99/172 [00:02<00:01, 44.04it/s]01/06/2022 17:19:07 - INFO - __main__ -   Batch Number = 100
01/06/2022 17:19:07 - INFO - __main__ -   Batch Number = 101
01/06/2022 17:19:07 - INFO - __main__ -   Batch Number = 102
01/06/2022 17:19:07 - INFO - __main__ -   Batch Number = 103
01/06/2022 17:19:07 - INFO - __main__ -   Batch Number = 104
Evaluating:  60%|██████    | 104/172 [00:02<00:01, 44.09it/s]01/06/2022 17:19:07 - INFO - __main__ -   Batch Number = 105
01/06/2022 17:19:07 - INFO - __main__ -   Batch Number = 106
01/06/2022 17:19:07 - INFO - __main__ -   Batch Number = 107
01/06/2022 17:19:07 - INFO - __main__ -   Batch Number = 108
01/06/2022 17:19:07 - INFO - __main__ -   Batch Number = 109
Evaluating:  63%|██████▎   | 109/172 [00:02<00:01, 44.03it/s]01/06/2022 17:19:07 - INFO - __main__ -   Batch Number = 110
01/06/2022 17:19:07 - INFO - __main__ -   Batch Number = 111
01/06/2022 17:19:07 - INFO - __main__ -   Batch Number = 112
01/06/2022 17:19:07 - INFO - __main__ -   Batch Number = 113
01/06/2022 17:19:07 - INFO - __main__ -   Batch Number = 114
Evaluating:  66%|██████▋   | 114/172 [00:02<00:01, 44.21it/s]01/06/2022 17:19:07 - INFO - __main__ -   Batch Number = 115
01/06/2022 17:19:07 - INFO - __main__ -   Batch Number = 116
01/06/2022 17:19:07 - INFO - __main__ -   Batch Number = 117
01/06/2022 17:19:07 - INFO - __main__ -   Batch Number = 118
01/06/2022 17:19:07 - INFO - __main__ -   Batch Number = 119
Evaluating:  69%|██████▉   | 119/172 [00:02<00:01, 44.15it/s]01/06/2022 17:19:07 - INFO - __main__ -   Batch Number = 120
01/06/2022 17:19:07 - INFO - __main__ -   Batch Number = 121
01/06/2022 17:19:07 - INFO - __main__ -   Batch Number = 122
01/06/2022 17:19:07 - INFO - __main__ -   Batch Number = 123
01/06/2022 17:19:07 - INFO - __main__ -   Batch Number = 124
Evaluating:  72%|███████▏  | 124/172 [00:02<00:01, 42.65it/s]01/06/2022 17:19:07 - INFO - __main__ -   Batch Number = 125
01/06/2022 17:19:07 - INFO - __main__ -   Batch Number = 126
01/06/2022 17:19:07 - INFO - __main__ -   Batch Number = 127
01/06/2022 17:19:07 - INFO - __main__ -   Batch Number = 128
01/06/2022 17:19:07 - INFO - __main__ -   Batch Number = 129
Evaluating:  75%|███████▌  | 129/172 [00:03<00:00, 43.15it/s]01/06/2022 17:19:07 - INFO - __main__ -   Batch Number = 130
01/06/2022 17:19:07 - INFO - __main__ -   Batch Number = 131
01/06/2022 17:19:07 - INFO - __main__ -   Batch Number = 132
01/06/2022 17:19:07 - INFO - __main__ -   Batch Number = 133
01/06/2022 17:19:07 - INFO - __main__ -   Batch Number = 134
Evaluating:  78%|███████▊  | 134/172 [00:03<00:00, 43.57it/s]01/06/2022 17:19:07 - INFO - __main__ -   Batch Number = 135
01/06/2022 17:19:07 - INFO - __main__ -   Batch Number = 136
01/06/2022 17:19:07 - INFO - __main__ -   Batch Number = 137
01/06/2022 17:19:08 - INFO - __main__ -   Batch Number = 138
01/06/2022 17:19:08 - INFO - __main__ -   Batch Number = 139
Evaluating:  81%|████████  | 139/172 [00:03<00:00, 43.63it/s]01/06/2022 17:19:08 - INFO - __main__ -   Batch Number = 140
01/06/2022 17:19:08 - INFO - __main__ -   Batch Number = 141
01/06/2022 17:19:08 - INFO - __main__ -   Batch Number = 142
01/06/2022 17:19:08 - INFO - __main__ -   Batch Number = 143
01/06/2022 17:19:08 - INFO - __main__ -   Batch Number = 144
Evaluating:  84%|████████▎ | 144/172 [00:03<00:00, 43.98it/s]01/06/2022 17:19:08 - INFO - __main__ -   Batch Number = 145
01/06/2022 17:19:08 - INFO - __main__ -   Batch Number = 146
01/06/2022 17:19:08 - INFO - __main__ -   Batch Number = 147
01/06/2022 17:19:08 - INFO - __main__ -   Batch Number = 148
01/06/2022 17:19:08 - INFO - __main__ -   Batch Number = 149
Evaluating:  87%|████████▋ | 149/172 [00:03<00:00, 44.04it/s]01/06/2022 17:19:08 - INFO - __main__ -   Batch Number = 150
01/06/2022 17:19:08 - INFO - __main__ -   Batch Number = 151
01/06/2022 17:19:08 - INFO - __main__ -   Batch Number = 152
01/06/2022 17:19:08 - INFO - __main__ -   Batch Number = 153
01/06/2022 17:19:08 - INFO - __main__ -   Batch Number = 154
Evaluating:  90%|████████▉ | 154/172 [00:03<00:00, 43.82it/s]01/06/2022 17:19:08 - INFO - __main__ -   Batch Number = 155
01/06/2022 17:19:08 - INFO - __main__ -   Batch Number = 156
01/06/2022 17:19:08 - INFO - __main__ -   Batch Number = 157
01/06/2022 17:19:08 - INFO - __main__ -   Batch Number = 158
01/06/2022 17:19:08 - INFO - __main__ -   Batch Number = 159
Evaluating:  92%|█████████▏| 159/172 [00:03<00:00, 44.05it/s]01/06/2022 17:19:08 - INFO - __main__ -   Batch Number = 160
01/06/2022 17:19:08 - INFO - __main__ -   Batch Number = 161
01/06/2022 17:19:08 - INFO - __main__ -   Batch Number = 162
01/06/2022 17:19:08 - INFO - __main__ -   Batch Number = 163
01/06/2022 17:19:08 - INFO - __main__ -   Batch Number = 164
Evaluating:  95%|█████████▌| 164/172 [00:03<00:00, 43.96it/s]01/06/2022 17:19:08 - INFO - __main__ -   Batch Number = 165
01/06/2022 17:19:08 - INFO - __main__ -   Batch Number = 166
01/06/2022 17:19:08 - INFO - __main__ -   Batch Number = 167
01/06/2022 17:19:08 - INFO - __main__ -   Batch Number = 168
01/06/2022 17:19:08 - INFO - __main__ -   Batch Number = 169
Evaluating:  98%|█████████▊| 169/172 [00:03<00:00, 44.04it/s]01/06/2022 17:19:08 - INFO - __main__ -   Batch Number = 170
01/06/2022 17:19:08 - INFO - __main__ -   Batch Number = 171
01/06/2022 17:19:08 - INFO - __main__ -   Batch Number = 172
Evaluating: 100%|██████████| 172/172 [00:04<00:00, 42.92it/s]
01/06/2022 17:19:08 - INFO - __main__ -     Evaluation done in total 4.007405 secs (0.002917 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/predictions_zh_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/nbest_predictions_zh_.json
01/06/2022 17:19:14 - INFO - __main__ -   Results = OrderedDict([('exact', 45.88235294117647), ('f1', 55.7271575296785), ('total', 1190), ('HasAns_exact', 45.88235294117647), ('HasAns_f1', 55.7271575296785), ('HasAns_total', 1190), ('best_exact', 45.88235294117647), ('best_exact_thresh', 0.0), ('best_f1', 55.7271575296785), ('best_f1_thresh', 0.0)])
PyTorch version 1.10.0+cu111 available.
01/06/2022 17:19:17 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/06/2022 17:19:17 - INFO - root -   save model
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /root/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/06/2022 17:19:23 - INFO - __main__ -   lang2id = None
01/06/2022 17:19:26 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, data_dir='/root/Desktop/cloud-emea-copy/data//xquad', output_dir='/root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8', max_seq_length=384, train_file='/root/Desktop/cloud-emea-copy/data//xquad/train-v1.1.json', predict_file='/root/Desktop/cloud-emea-copy/data//xquad/dev-v1.1.json', eval_test_set=False, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=4, per_gpu_eval_batch_size=8, gradient_accumulation_steps=8, learning_rate=0.0003, weight_decay=0.0001, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=15.0, max_steps=-1, save_steps=1000, warmup_steps=500, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', eval_lang='en', predict_langs='ar,de,el,es,hi,ru,th,tr,vi,zh', train_lang='en', log_file='/root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='qna', threads=8, version_2_with_negative=False, verbose_logging=False, n_best_size=20, max_query_length=64, max_answer_length=30, doc_stride=128, null_score_diff_threshold=0.0, predict_task_adapter='output/squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8_s3/checkpoint-best/qna', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix=None, en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/06/2022 17:19:26 - INFO - __main__ -   Results: {}
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /root/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/06/2022 17:19:33 - INFO - __main__ -   lang2id = None
01/06/2022 17:19:33 - INFO - __main__ -   Evaluating the model on the test set of all languages specified
01/06/2022 17:19:33 - INFO - __main__ -   Adapter will be loaded from this path: output/squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8_s3/checkpoint-best/qna
01/06/2022 17:19:33 - INFO - root -   Trying to decide if add adapter
01/06/2022 17:19:33 - INFO - root -   loading task adapter
Loading module configuration from output/squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8_s3/checkpoint-best/qna/adapter_config.json
Adding adapter 'qna' of type 'text_task'.
Loading module weights from output/squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8_s3/checkpoint-best/qna/pytorch_adapter.bin
Loading module configuration from output/squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8_s3/checkpoint-best/qna/head_config.json
Loading module weights from output/squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8_s3/checkpoint-best/qna/pytorch_model_head.bin
01/06/2022 17:19:33 - INFO - root -   loading lang adpater en/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip.
Loading module configuration from /root/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
Loading module weights from /root/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/pytorch_adapter.bin
No matching prediction head found in '/root/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted'
01/06/2022 17:19:34 - INFO - __main__ -   Language adapter for ar not found, using en instead
01/06/2022 17:19:34 - INFO - __main__ -   Set active language adapter to en
01/06/2022 17:19:34 - INFO - __main__ -   Args Adapter Weight = None
01/06/2022 17:19:34 - INFO - __main__ -   Adapter Languages = ['en']
01/06/2022 17:19:34 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
en en/wiki@ukp
  0%|          | 0/48 [00:00<?, ?it/s] 33%|███▎      | 16/48 [00:00<00:00, 135.92it/s] 62%|██████▎   | 30/48 [00:00<00:00, 131.74it/s] 96%|█████████▌| 46/48 [00:00<00:00, 142.35it/s]100%|██████████| 48/48 [00:00<00:00, 141.46it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<06:55,  2.86it/s]convert squad examples to features:  32%|███▏      | 385/1190 [00:01<00:01, 404.17it/s]convert squad examples to features:  94%|█████████▍| 1121/1190 [00:01<00:00, 1311.12it/s]convert squad examples to features: 100%|██████████| 1190/1190 [00:01<00:00, 986.28it/s] /root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|██████████| 1190/1190 [00:00<00:00, 501680.75it/s]
01/06/2022 17:19:36 - INFO - __main__ -   Local Rank = -1
01/06/2022 17:19:36 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.ar.json_bert-base-multilingual-cased_384_ar
01/06/2022 17:19:38 - INFO - __main__ -   ***** Running evaluation  *****
01/06/2022 17:19:38 - INFO - __main__ -     Num examples = 1455
01/06/2022 17:19:38 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/182 [00:00<?, ?it/s]01/06/2022 17:19:38 - INFO - __main__ -   Batch Number = 1
01/06/2022 17:19:38 - INFO - __main__ -   Batch Number = 2
01/06/2022 17:19:38 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|▏         | 3/182 [00:00<00:07, 23.02it/s]01/06/2022 17:19:38 - INFO - __main__ -   Batch Number = 4
01/06/2022 17:19:38 - INFO - __main__ -   Batch Number = 5
01/06/2022 17:19:38 - INFO - __main__ -   Batch Number = 6
Evaluating:   3%|▎         | 6/182 [00:00<00:06, 26.38it/s]01/06/2022 17:19:38 - INFO - __main__ -   Batch Number = 7
01/06/2022 17:19:38 - INFO - __main__ -   Batch Number = 8
01/06/2022 17:19:38 - INFO - __main__ -   Batch Number = 9
01/06/2022 17:19:38 - INFO - __main__ -   Batch Number = 10
01/06/2022 17:19:38 - INFO - __main__ -   Batch Number = 11
Evaluating:   6%|▌         | 11/182 [00:00<00:04, 34.59it/s]01/06/2022 17:19:38 - INFO - __main__ -   Batch Number = 12
01/06/2022 17:19:38 - INFO - __main__ -   Batch Number = 13
01/06/2022 17:19:38 - INFO - __main__ -   Batch Number = 14
01/06/2022 17:19:38 - INFO - __main__ -   Batch Number = 15
Evaluating:   8%|▊         | 15/182 [00:00<00:04, 36.20it/s]01/06/2022 17:19:38 - INFO - __main__ -   Batch Number = 16
01/06/2022 17:19:38 - INFO - __main__ -   Batch Number = 17
01/06/2022 17:19:38 - INFO - __main__ -   Batch Number = 18
01/06/2022 17:19:38 - INFO - __main__ -   Batch Number = 19
01/06/2022 17:19:38 - INFO - __main__ -   Batch Number = 20
Evaluating:  11%|█         | 20/182 [00:00<00:04, 39.26it/s]01/06/2022 17:19:38 - INFO - __main__ -   Batch Number = 21
01/06/2022 17:19:38 - INFO - __main__ -   Batch Number = 22
01/06/2022 17:19:38 - INFO - __main__ -   Batch Number = 23
01/06/2022 17:19:38 - INFO - __main__ -   Batch Number = 24
01/06/2022 17:19:38 - INFO - __main__ -   Batch Number = 25
Evaluating:  14%|█▎        | 25/182 [00:00<00:03, 41.14it/s]01/06/2022 17:19:38 - INFO - __main__ -   Batch Number = 26
01/06/2022 17:19:38 - INFO - __main__ -   Batch Number = 27
01/06/2022 17:19:38 - INFO - __main__ -   Batch Number = 28
01/06/2022 17:19:38 - INFO - __main__ -   Batch Number = 29
01/06/2022 17:19:38 - INFO - __main__ -   Batch Number = 30
Evaluating:  16%|█▋        | 30/182 [00:00<00:03, 42.30it/s]01/06/2022 17:19:38 - INFO - __main__ -   Batch Number = 31
01/06/2022 17:19:38 - INFO - __main__ -   Batch Number = 32
01/06/2022 17:19:38 - INFO - __main__ -   Batch Number = 33
01/06/2022 17:19:38 - INFO - __main__ -   Batch Number = 34
01/06/2022 17:19:38 - INFO - __main__ -   Batch Number = 35
Evaluating:  19%|█▉        | 35/182 [00:00<00:03, 42.95it/s]01/06/2022 17:19:38 - INFO - __main__ -   Batch Number = 36
01/06/2022 17:19:38 - INFO - __main__ -   Batch Number = 37
01/06/2022 17:19:38 - INFO - __main__ -   Batch Number = 38
01/06/2022 17:19:39 - INFO - __main__ -   Batch Number = 39
01/06/2022 17:19:39 - INFO - __main__ -   Batch Number = 40
Evaluating:  22%|██▏       | 40/182 [00:01<00:03, 43.51it/s]01/06/2022 17:19:39 - INFO - __main__ -   Batch Number = 41
01/06/2022 17:19:39 - INFO - __main__ -   Batch Number = 42
01/06/2022 17:19:39 - INFO - __main__ -   Batch Number = 43
01/06/2022 17:19:39 - INFO - __main__ -   Batch Number = 44
01/06/2022 17:19:39 - INFO - __main__ -   Batch Number = 45
Evaluating:  25%|██▍       | 45/182 [00:01<00:03, 43.71it/s]01/06/2022 17:19:39 - INFO - __main__ -   Batch Number = 46
01/06/2022 17:19:39 - INFO - __main__ -   Batch Number = 47
01/06/2022 17:19:39 - INFO - __main__ -   Batch Number = 48
01/06/2022 17:19:39 - INFO - __main__ -   Batch Number = 49
01/06/2022 17:19:39 - INFO - __main__ -   Batch Number = 50
Evaluating:  27%|██▋       | 50/182 [00:01<00:03, 43.97it/s]01/06/2022 17:19:39 - INFO - __main__ -   Batch Number = 51
01/06/2022 17:19:39 - INFO - __main__ -   Batch Number = 52
01/06/2022 17:19:39 - INFO - __main__ -   Batch Number = 53
01/06/2022 17:19:39 - INFO - __main__ -   Batch Number = 54
01/06/2022 17:19:39 - INFO - __main__ -   Batch Number = 55
Evaluating:  30%|███       | 55/182 [00:01<00:02, 44.10it/s]01/06/2022 17:19:39 - INFO - __main__ -   Batch Number = 56
01/06/2022 17:19:39 - INFO - __main__ -   Batch Number = 57
01/06/2022 17:19:39 - INFO - __main__ -   Batch Number = 58
01/06/2022 17:19:39 - INFO - __main__ -   Batch Number = 59
01/06/2022 17:19:39 - INFO - __main__ -   Batch Number = 60
Evaluating:  33%|███▎      | 60/182 [00:01<00:02, 44.01it/s]01/06/2022 17:19:39 - INFO - __main__ -   Batch Number = 61
01/06/2022 17:19:39 - INFO - __main__ -   Batch Number = 62
01/06/2022 17:19:39 - INFO - __main__ -   Batch Number = 63
01/06/2022 17:19:39 - INFO - __main__ -   Batch Number = 64
01/06/2022 17:19:39 - INFO - __main__ -   Batch Number = 65
Evaluating:  36%|███▌      | 65/182 [00:01<00:02, 44.32it/s]01/06/2022 17:19:39 - INFO - __main__ -   Batch Number = 66
01/06/2022 17:19:39 - INFO - __main__ -   Batch Number = 67
01/06/2022 17:19:39 - INFO - __main__ -   Batch Number = 68
01/06/2022 17:19:39 - INFO - __main__ -   Batch Number = 69
01/06/2022 17:19:39 - INFO - __main__ -   Batch Number = 70
Evaluating:  38%|███▊      | 70/182 [00:01<00:02, 44.28it/s]01/06/2022 17:19:39 - INFO - __main__ -   Batch Number = 71
01/06/2022 17:19:39 - INFO - __main__ -   Batch Number = 72
01/06/2022 17:19:39 - INFO - __main__ -   Batch Number = 73
01/06/2022 17:19:39 - INFO - __main__ -   Batch Number = 74
01/06/2022 17:19:39 - INFO - __main__ -   Batch Number = 75
Evaluating:  41%|████      | 75/182 [00:01<00:02, 44.20it/s]01/06/2022 17:19:39 - INFO - __main__ -   Batch Number = 76
01/06/2022 17:19:39 - INFO - __main__ -   Batch Number = 77
01/06/2022 17:19:39 - INFO - __main__ -   Batch Number = 78
01/06/2022 17:19:39 - INFO - __main__ -   Batch Number = 79
01/06/2022 17:19:39 - INFO - __main__ -   Batch Number = 80
Evaluating:  44%|████▍     | 80/182 [00:01<00:02, 44.23it/s]01/06/2022 17:19:39 - INFO - __main__ -   Batch Number = 81
01/06/2022 17:19:39 - INFO - __main__ -   Batch Number = 82
01/06/2022 17:19:40 - INFO - __main__ -   Batch Number = 83
01/06/2022 17:19:40 - INFO - __main__ -   Batch Number = 84
01/06/2022 17:19:40 - INFO - __main__ -   Batch Number = 85
Evaluating:  47%|████▋     | 85/182 [00:02<00:02, 44.25it/s]01/06/2022 17:19:40 - INFO - __main__ -   Batch Number = 86
01/06/2022 17:19:40 - INFO - __main__ -   Batch Number = 87
01/06/2022 17:19:40 - INFO - __main__ -   Batch Number = 88
01/06/2022 17:19:40 - INFO - __main__ -   Batch Number = 89
01/06/2022 17:19:40 - INFO - __main__ -   Batch Number = 90
Evaluating:  49%|████▉     | 90/182 [00:02<00:02, 44.10it/s]01/06/2022 17:19:40 - INFO - __main__ -   Batch Number = 91
01/06/2022 17:19:40 - INFO - __main__ -   Batch Number = 92
01/06/2022 17:19:40 - INFO - __main__ -   Batch Number = 93
01/06/2022 17:19:40 - INFO - __main__ -   Batch Number = 94
01/06/2022 17:19:40 - INFO - __main__ -   Batch Number = 95
Evaluating:  52%|█████▏    | 95/182 [00:02<00:01, 44.20it/s]01/06/2022 17:19:40 - INFO - __main__ -   Batch Number = 96
01/06/2022 17:19:40 - INFO - __main__ -   Batch Number = 97
01/06/2022 17:19:40 - INFO - __main__ -   Batch Number = 98
01/06/2022 17:19:40 - INFO - __main__ -   Batch Number = 99
01/06/2022 17:19:40 - INFO - __main__ -   Batch Number = 100
Evaluating:  55%|█████▍    | 100/182 [00:02<00:01, 44.39it/s]01/06/2022 17:19:40 - INFO - __main__ -   Batch Number = 101
01/06/2022 17:19:40 - INFO - __main__ -   Batch Number = 102
01/06/2022 17:19:40 - INFO - __main__ -   Batch Number = 103
01/06/2022 17:19:40 - INFO - __main__ -   Batch Number = 104
01/06/2022 17:19:40 - INFO - __main__ -   Batch Number = 105
Evaluating:  58%|█████▊    | 105/182 [00:02<00:01, 44.27it/s]01/06/2022 17:19:40 - INFO - __main__ -   Batch Number = 106
01/06/2022 17:19:40 - INFO - __main__ -   Batch Number = 107
01/06/2022 17:19:40 - INFO - __main__ -   Batch Number = 108
01/06/2022 17:19:40 - INFO - __main__ -   Batch Number = 109
01/06/2022 17:19:40 - INFO - __main__ -   Batch Number = 110
Evaluating:  60%|██████    | 110/182 [00:02<00:01, 44.39it/s]01/06/2022 17:19:40 - INFO - __main__ -   Batch Number = 111
01/06/2022 17:19:40 - INFO - __main__ -   Batch Number = 112
01/06/2022 17:19:40 - INFO - __main__ -   Batch Number = 113
01/06/2022 17:19:40 - INFO - __main__ -   Batch Number = 114
01/06/2022 17:19:40 - INFO - __main__ -   Batch Number = 115
Evaluating:  63%|██████▎   | 115/182 [00:02<00:01, 44.44it/s]01/06/2022 17:19:40 - INFO - __main__ -   Batch Number = 116
01/06/2022 17:19:40 - INFO - __main__ -   Batch Number = 117
01/06/2022 17:19:40 - INFO - __main__ -   Batch Number = 118
01/06/2022 17:19:40 - INFO - __main__ -   Batch Number = 119
01/06/2022 17:19:40 - INFO - __main__ -   Batch Number = 120
Evaluating:  66%|██████▌   | 120/182 [00:02<00:01, 44.38it/s]01/06/2022 17:19:40 - INFO - __main__ -   Batch Number = 121
01/06/2022 17:19:40 - INFO - __main__ -   Batch Number = 122
01/06/2022 17:19:40 - INFO - __main__ -   Batch Number = 123
01/06/2022 17:19:40 - INFO - __main__ -   Batch Number = 124
01/06/2022 17:19:40 - INFO - __main__ -   Batch Number = 125
Evaluating:  69%|██████▊   | 125/182 [00:02<00:01, 44.50it/s]01/06/2022 17:19:40 - INFO - __main__ -   Batch Number = 126
01/06/2022 17:19:40 - INFO - __main__ -   Batch Number = 127
01/06/2022 17:19:41 - INFO - __main__ -   Batch Number = 128
01/06/2022 17:19:41 - INFO - __main__ -   Batch Number = 129
01/06/2022 17:19:41 - INFO - __main__ -   Batch Number = 130
Evaluating:  71%|███████▏  | 130/182 [00:03<00:01, 44.54it/s]01/06/2022 17:19:41 - INFO - __main__ -   Batch Number = 131
01/06/2022 17:19:41 - INFO - __main__ -   Batch Number = 132
01/06/2022 17:19:41 - INFO - __main__ -   Batch Number = 133
01/06/2022 17:19:41 - INFO - __main__ -   Batch Number = 134
01/06/2022 17:19:41 - INFO - __main__ -   Batch Number = 135
Evaluating:  74%|███████▍  | 135/182 [00:03<00:01, 44.54it/s]01/06/2022 17:19:41 - INFO - __main__ -   Batch Number = 136
01/06/2022 17:19:41 - INFO - __main__ -   Batch Number = 137
01/06/2022 17:19:41 - INFO - __main__ -   Batch Number = 138
01/06/2022 17:19:41 - INFO - __main__ -   Batch Number = 139
01/06/2022 17:19:41 - INFO - __main__ -   Batch Number = 140
Evaluating:  77%|███████▋  | 140/182 [00:03<00:00, 44.53it/s]01/06/2022 17:19:41 - INFO - __main__ -   Batch Number = 141
01/06/2022 17:19:41 - INFO - __main__ -   Batch Number = 142
01/06/2022 17:19:41 - INFO - __main__ -   Batch Number = 143
01/06/2022 17:19:41 - INFO - __main__ -   Batch Number = 144
01/06/2022 17:19:41 - INFO - __main__ -   Batch Number = 145
Evaluating:  80%|███████▉  | 145/182 [00:03<00:00, 44.44it/s]01/06/2022 17:19:41 - INFO - __main__ -   Batch Number = 146
01/06/2022 17:19:41 - INFO - __main__ -   Batch Number = 147
01/06/2022 17:19:41 - INFO - __main__ -   Batch Number = 148
01/06/2022 17:19:41 - INFO - __main__ -   Batch Number = 149
01/06/2022 17:19:41 - INFO - __main__ -   Batch Number = 150
Evaluating:  82%|████████▏ | 150/182 [00:03<00:00, 44.25it/s]01/06/2022 17:19:41 - INFO - __main__ -   Batch Number = 151
01/06/2022 17:19:41 - INFO - __main__ -   Batch Number = 152
01/06/2022 17:19:41 - INFO - __main__ -   Batch Number = 153
01/06/2022 17:19:41 - INFO - __main__ -   Batch Number = 154
01/06/2022 17:19:41 - INFO - __main__ -   Batch Number = 155
Evaluating:  85%|████████▌ | 155/182 [00:03<00:00, 44.40it/s]01/06/2022 17:19:41 - INFO - __main__ -   Batch Number = 156
01/06/2022 17:19:41 - INFO - __main__ -   Batch Number = 157
01/06/2022 17:19:41 - INFO - __main__ -   Batch Number = 158
01/06/2022 17:19:41 - INFO - __main__ -   Batch Number = 159
01/06/2022 17:19:41 - INFO - __main__ -   Batch Number = 160
Evaluating:  88%|████████▊ | 160/182 [00:03<00:00, 44.47it/s]01/06/2022 17:19:41 - INFO - __main__ -   Batch Number = 161
01/06/2022 17:19:41 - INFO - __main__ -   Batch Number = 162
01/06/2022 17:19:41 - INFO - __main__ -   Batch Number = 163
01/06/2022 17:19:41 - INFO - __main__ -   Batch Number = 164
01/06/2022 17:19:41 - INFO - __main__ -   Batch Number = 165
Evaluating:  91%|█████████ | 165/182 [00:03<00:00, 44.29it/s]01/06/2022 17:19:41 - INFO - __main__ -   Batch Number = 166
01/06/2022 17:19:41 - INFO - __main__ -   Batch Number = 167
01/06/2022 17:19:41 - INFO - __main__ -   Batch Number = 168
01/06/2022 17:19:41 - INFO - __main__ -   Batch Number = 169
01/06/2022 17:19:41 - INFO - __main__ -   Batch Number = 170
Evaluating:  93%|█████████▎| 170/182 [00:03<00:00, 44.44it/s]01/06/2022 17:19:41 - INFO - __main__ -   Batch Number = 171
01/06/2022 17:19:42 - INFO - __main__ -   Batch Number = 172
01/06/2022 17:19:42 - INFO - __main__ -   Batch Number = 173
01/06/2022 17:19:42 - INFO - __main__ -   Batch Number = 174
01/06/2022 17:19:42 - INFO - __main__ -   Batch Number = 175
Evaluating:  96%|█████████▌| 175/182 [00:04<00:00, 44.31it/s]01/06/2022 17:19:42 - INFO - __main__ -   Batch Number = 176
01/06/2022 17:19:42 - INFO - __main__ -   Batch Number = 177
01/06/2022 17:19:42 - INFO - __main__ -   Batch Number = 178
01/06/2022 17:19:42 - INFO - __main__ -   Batch Number = 179
01/06/2022 17:19:42 - INFO - __main__ -   Batch Number = 180
Evaluating:  99%|█████████▉| 180/182 [00:04<00:00, 44.29it/s]01/06/2022 17:19:42 - INFO - __main__ -   Batch Number = 181
01/06/2022 17:19:42 - INFO - __main__ -   Batch Number = 182
Evaluating: 100%|██████████| 182/182 [00:04<00:00, 43.25it/s]
01/06/2022 17:19:42 - INFO - __main__ -     Evaluation done in total 4.208667 secs (0.002893 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/predictions_ar_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/nbest_predictions_ar_.json
01/06/2022 17:19:45 - INFO - __main__ -   Results = OrderedDict([('exact', 33.02521008403362), ('f1', 50.5483163717577), ('total', 1190), ('HasAns_exact', 33.02521008403362), ('HasAns_f1', 50.5483163717577), ('HasAns_total', 1190), ('best_exact', 33.02521008403362), ('best_exact_thresh', 0.0), ('best_f1', 50.5483163717577), ('best_f1_thresh', 0.0)])
01/06/2022 17:19:45 - INFO - __main__ -   Language adapter for de not found, using en instead
01/06/2022 17:19:45 - INFO - __main__ -   Set active language adapter to en
01/06/2022 17:19:45 - INFO - __main__ -   Args Adapter Weight = None
01/06/2022 17:19:45 - INFO - __main__ -   Adapter Languages = ['en']
01/06/2022 17:19:45 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
  0%|          | 0/48 [00:00<?, ?it/s] 33%|███▎      | 16/48 [00:00<00:00, 123.31it/s] 60%|██████    | 29/48 [00:00<00:00, 115.20it/s] 92%|█████████▏| 44/48 [00:00<00:00, 126.01it/s]100%|██████████| 48/48 [00:00<00:00, 125.33it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<05:41,  3.48it/s]convert squad examples to features:  27%|██▋       | 321/1190 [00:00<00:00, 916.68it/s]convert squad examples to features:  38%|███▊      | 448/1190 [00:01<00:01, 410.05it/s]convert squad examples to features:  97%|█████████▋| 1153/1190 [00:01<00:00, 1252.41it/s]convert squad examples to features: 100%|██████████| 1190/1190 [00:01<00:00, 984.95it/s] /root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|██████████| 1190/1190 [00:00<00:00, 780366.13it/s]
01/06/2022 17:19:47 - INFO - __main__ -   Local Rank = -1
01/06/2022 17:19:47 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.de.json_bert-base-multilingual-cased_384_de
01/06/2022 17:19:48 - INFO - __main__ -   ***** Running evaluation  *****
01/06/2022 17:19:48 - INFO - __main__ -     Num examples = 1296
01/06/2022 17:19:48 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/162 [00:00<?, ?it/s]01/06/2022 17:19:48 - INFO - __main__ -   Batch Number = 1
01/06/2022 17:19:48 - INFO - __main__ -   Batch Number = 2
01/06/2022 17:19:48 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|▏         | 3/162 [00:00<00:05, 29.48it/s]01/06/2022 17:19:48 - INFO - __main__ -   Batch Number = 4
01/06/2022 17:19:48 - INFO - __main__ -   Batch Number = 5
01/06/2022 17:19:48 - INFO - __main__ -   Batch Number = 6
01/06/2022 17:19:48 - INFO - __main__ -   Batch Number = 7
Evaluating:   4%|▍         | 7/162 [00:00<00:04, 34.27it/s]01/06/2022 17:19:49 - INFO - __main__ -   Batch Number = 8
01/06/2022 17:19:49 - INFO - __main__ -   Batch Number = 9
01/06/2022 17:19:49 - INFO - __main__ -   Batch Number = 10
01/06/2022 17:19:49 - INFO - __main__ -   Batch Number = 11
01/06/2022 17:19:49 - INFO - __main__ -   Batch Number = 12
Evaluating:   7%|▋         | 12/162 [00:00<00:03, 38.54it/s]01/06/2022 17:19:49 - INFO - __main__ -   Batch Number = 13
01/06/2022 17:19:49 - INFO - __main__ -   Batch Number = 14
01/06/2022 17:19:49 - INFO - __main__ -   Batch Number = 15
01/06/2022 17:19:49 - INFO - __main__ -   Batch Number = 16
01/06/2022 17:19:49 - INFO - __main__ -   Batch Number = 17
Evaluating:  10%|█         | 17/162 [00:00<00:03, 40.74it/s]01/06/2022 17:19:49 - INFO - __main__ -   Batch Number = 18
01/06/2022 17:19:49 - INFO - __main__ -   Batch Number = 19
01/06/2022 17:19:49 - INFO - __main__ -   Batch Number = 20
01/06/2022 17:19:49 - INFO - __main__ -   Batch Number = 21
01/06/2022 17:19:49 - INFO - __main__ -   Batch Number = 22
Evaluating:  14%|█▎        | 22/162 [00:00<00:03, 41.96it/s]01/06/2022 17:19:49 - INFO - __main__ -   Batch Number = 23
01/06/2022 17:19:49 - INFO - __main__ -   Batch Number = 24
01/06/2022 17:19:49 - INFO - __main__ -   Batch Number = 25
01/06/2022 17:19:49 - INFO - __main__ -   Batch Number = 26
01/06/2022 17:19:49 - INFO - __main__ -   Batch Number = 27
Evaluating:  17%|█▋        | 27/162 [00:00<00:03, 42.81it/s]01/06/2022 17:19:49 - INFO - __main__ -   Batch Number = 28
01/06/2022 17:19:49 - INFO - __main__ -   Batch Number = 29
01/06/2022 17:19:49 - INFO - __main__ -   Batch Number = 30
01/06/2022 17:19:49 - INFO - __main__ -   Batch Number = 31
01/06/2022 17:19:49 - INFO - __main__ -   Batch Number = 32
Evaluating:  20%|█▉        | 32/162 [00:00<00:02, 43.38it/s]01/06/2022 17:19:49 - INFO - __main__ -   Batch Number = 33
01/06/2022 17:19:49 - INFO - __main__ -   Batch Number = 34
01/06/2022 17:19:49 - INFO - __main__ -   Batch Number = 35
01/06/2022 17:19:49 - INFO - __main__ -   Batch Number = 36
01/06/2022 17:19:49 - INFO - __main__ -   Batch Number = 37
Evaluating:  23%|██▎       | 37/162 [00:00<00:02, 43.70it/s]01/06/2022 17:19:49 - INFO - __main__ -   Batch Number = 38
01/06/2022 17:19:49 - INFO - __main__ -   Batch Number = 39
01/06/2022 17:19:49 - INFO - __main__ -   Batch Number = 40
01/06/2022 17:19:49 - INFO - __main__ -   Batch Number = 41
01/06/2022 17:19:49 - INFO - __main__ -   Batch Number = 42
Evaluating:  26%|██▌       | 42/162 [00:01<00:02, 43.95it/s]01/06/2022 17:19:49 - INFO - __main__ -   Batch Number = 43
01/06/2022 17:19:49 - INFO - __main__ -   Batch Number = 44
01/06/2022 17:19:49 - INFO - __main__ -   Batch Number = 45
01/06/2022 17:19:49 - INFO - __main__ -   Batch Number = 46
01/06/2022 17:19:49 - INFO - __main__ -   Batch Number = 47
Evaluating:  29%|██▉       | 47/162 [00:01<00:02, 44.25it/s]01/06/2022 17:19:49 - INFO - __main__ -   Batch Number = 48
01/06/2022 17:19:49 - INFO - __main__ -   Batch Number = 49
01/06/2022 17:19:49 - INFO - __main__ -   Batch Number = 50
01/06/2022 17:19:49 - INFO - __main__ -   Batch Number = 51
01/06/2022 17:19:50 - INFO - __main__ -   Batch Number = 52
Evaluating:  32%|███▏      | 52/162 [00:01<00:02, 44.18it/s]01/06/2022 17:19:50 - INFO - __main__ -   Batch Number = 53
01/06/2022 17:19:50 - INFO - __main__ -   Batch Number = 54
01/06/2022 17:19:50 - INFO - __main__ -   Batch Number = 55
01/06/2022 17:19:50 - INFO - __main__ -   Batch Number = 56
01/06/2022 17:19:50 - INFO - __main__ -   Batch Number = 57
Evaluating:  35%|███▌      | 57/162 [00:01<00:02, 44.33it/s]01/06/2022 17:19:50 - INFO - __main__ -   Batch Number = 58
01/06/2022 17:19:50 - INFO - __main__ -   Batch Number = 59
01/06/2022 17:19:50 - INFO - __main__ -   Batch Number = 60
01/06/2022 17:19:50 - INFO - __main__ -   Batch Number = 61
01/06/2022 17:19:50 - INFO - __main__ -   Batch Number = 62
Evaluating:  38%|███▊      | 62/162 [00:01<00:02, 44.31it/s]01/06/2022 17:19:50 - INFO - __main__ -   Batch Number = 63
01/06/2022 17:19:50 - INFO - __main__ -   Batch Number = 64
01/06/2022 17:19:50 - INFO - __main__ -   Batch Number = 65
01/06/2022 17:19:50 - INFO - __main__ -   Batch Number = 66
01/06/2022 17:19:50 - INFO - __main__ -   Batch Number = 67
Evaluating:  41%|████▏     | 67/162 [00:01<00:02, 44.06it/s]01/06/2022 17:19:50 - INFO - __main__ -   Batch Number = 68
01/06/2022 17:19:50 - INFO - __main__ -   Batch Number = 69
01/06/2022 17:19:50 - INFO - __main__ -   Batch Number = 70
01/06/2022 17:19:50 - INFO - __main__ -   Batch Number = 71
01/06/2022 17:19:50 - INFO - __main__ -   Batch Number = 72
Evaluating:  44%|████▍     | 72/162 [00:01<00:02, 44.15it/s]01/06/2022 17:19:50 - INFO - __main__ -   Batch Number = 73
01/06/2022 17:19:50 - INFO - __main__ -   Batch Number = 74
01/06/2022 17:19:50 - INFO - __main__ -   Batch Number = 75
01/06/2022 17:19:50 - INFO - __main__ -   Batch Number = 76
01/06/2022 17:19:50 - INFO - __main__ -   Batch Number = 77
Evaluating:  48%|████▊     | 77/162 [00:01<00:01, 44.13it/s]01/06/2022 17:19:50 - INFO - __main__ -   Batch Number = 78
01/06/2022 17:19:50 - INFO - __main__ -   Batch Number = 79
01/06/2022 17:19:50 - INFO - __main__ -   Batch Number = 80
01/06/2022 17:19:50 - INFO - __main__ -   Batch Number = 81
01/06/2022 17:19:50 - INFO - __main__ -   Batch Number = 82
Evaluating:  51%|█████     | 82/162 [00:01<00:01, 44.16it/s]01/06/2022 17:19:50 - INFO - __main__ -   Batch Number = 83
01/06/2022 17:19:50 - INFO - __main__ -   Batch Number = 84
01/06/2022 17:19:50 - INFO - __main__ -   Batch Number = 85
01/06/2022 17:19:50 - INFO - __main__ -   Batch Number = 86
01/06/2022 17:19:50 - INFO - __main__ -   Batch Number = 87
Evaluating:  54%|█████▎    | 87/162 [00:02<00:01, 44.13it/s]01/06/2022 17:19:50 - INFO - __main__ -   Batch Number = 88
01/06/2022 17:19:50 - INFO - __main__ -   Batch Number = 89
01/06/2022 17:19:50 - INFO - __main__ -   Batch Number = 90
01/06/2022 17:19:50 - INFO - __main__ -   Batch Number = 91
01/06/2022 17:19:50 - INFO - __main__ -   Batch Number = 92
Evaluating:  57%|█████▋    | 92/162 [00:02<00:01, 44.18it/s]01/06/2022 17:19:50 - INFO - __main__ -   Batch Number = 93
01/06/2022 17:19:50 - INFO - __main__ -   Batch Number = 94
01/06/2022 17:19:50 - INFO - __main__ -   Batch Number = 95
01/06/2022 17:19:51 - INFO - __main__ -   Batch Number = 96
01/06/2022 17:19:51 - INFO - __main__ -   Batch Number = 97
Evaluating:  60%|█████▉    | 97/162 [00:02<00:01, 44.16it/s]01/06/2022 17:19:51 - INFO - __main__ -   Batch Number = 98
01/06/2022 17:19:51 - INFO - __main__ -   Batch Number = 99
01/06/2022 17:19:51 - INFO - __main__ -   Batch Number = 100
01/06/2022 17:19:51 - INFO - __main__ -   Batch Number = 101
01/06/2022 17:19:51 - INFO - __main__ -   Batch Number = 102
Evaluating:  63%|██████▎   | 102/162 [00:02<00:01, 44.11it/s]01/06/2022 17:19:51 - INFO - __main__ -   Batch Number = 103
01/06/2022 17:19:51 - INFO - __main__ -   Batch Number = 104
01/06/2022 17:19:51 - INFO - __main__ -   Batch Number = 105
01/06/2022 17:19:51 - INFO - __main__ -   Batch Number = 106
01/06/2022 17:19:51 - INFO - __main__ -   Batch Number = 107
Evaluating:  66%|██████▌   | 107/162 [00:02<00:01, 44.25it/s]01/06/2022 17:19:51 - INFO - __main__ -   Batch Number = 108
01/06/2022 17:19:51 - INFO - __main__ -   Batch Number = 109
01/06/2022 17:19:51 - INFO - __main__ -   Batch Number = 110
01/06/2022 17:19:51 - INFO - __main__ -   Batch Number = 111
01/06/2022 17:19:51 - INFO - __main__ -   Batch Number = 112
Evaluating:  69%|██████▉   | 112/162 [00:02<00:01, 42.46it/s]01/06/2022 17:19:51 - INFO - __main__ -   Batch Number = 113
01/06/2022 17:19:51 - INFO - __main__ -   Batch Number = 114
01/06/2022 17:19:51 - INFO - __main__ -   Batch Number = 115
01/06/2022 17:19:51 - INFO - __main__ -   Batch Number = 116
01/06/2022 17:19:51 - INFO - __main__ -   Batch Number = 117
Evaluating:  72%|███████▏  | 117/162 [00:02<00:01, 43.07it/s]01/06/2022 17:19:51 - INFO - __main__ -   Batch Number = 118
01/06/2022 17:19:51 - INFO - __main__ -   Batch Number = 119
01/06/2022 17:19:51 - INFO - __main__ -   Batch Number = 120
01/06/2022 17:19:51 - INFO - __main__ -   Batch Number = 121
01/06/2022 17:19:51 - INFO - __main__ -   Batch Number = 122
Evaluating:  75%|███████▌  | 122/162 [00:02<00:00, 43.37it/s]01/06/2022 17:19:51 - INFO - __main__ -   Batch Number = 123
01/06/2022 17:19:51 - INFO - __main__ -   Batch Number = 124
01/06/2022 17:19:51 - INFO - __main__ -   Batch Number = 125
01/06/2022 17:19:51 - INFO - __main__ -   Batch Number = 126
01/06/2022 17:19:51 - INFO - __main__ -   Batch Number = 127
Evaluating:  78%|███████▊  | 127/162 [00:03<00:01, 34.41it/s]01/06/2022 17:19:51 - INFO - __main__ -   Batch Number = 128
01/06/2022 17:19:51 - INFO - __main__ -   Batch Number = 129
01/06/2022 17:19:51 - INFO - __main__ -   Batch Number = 130
01/06/2022 17:19:51 - INFO - __main__ -   Batch Number = 131
01/06/2022 17:19:51 - INFO - __main__ -   Batch Number = 132
Evaluating:  81%|████████▏ | 132/162 [00:03<00:00, 36.89it/s]01/06/2022 17:19:51 - INFO - __main__ -   Batch Number = 133
01/06/2022 17:19:51 - INFO - __main__ -   Batch Number = 134
01/06/2022 17:19:52 - INFO - __main__ -   Batch Number = 135
01/06/2022 17:19:52 - INFO - __main__ -   Batch Number = 136
01/06/2022 17:19:52 - INFO - __main__ -   Batch Number = 137
Evaluating:  85%|████████▍ | 137/162 [00:03<00:00, 38.81it/s]01/06/2022 17:19:52 - INFO - __main__ -   Batch Number = 138
01/06/2022 17:19:52 - INFO - __main__ -   Batch Number = 139
01/06/2022 17:19:52 - INFO - __main__ -   Batch Number = 140
01/06/2022 17:19:52 - INFO - __main__ -   Batch Number = 141
01/06/2022 17:19:52 - INFO - __main__ -   Batch Number = 142
Evaluating:  88%|████████▊ | 142/162 [00:03<00:00, 40.37it/s]01/06/2022 17:19:52 - INFO - __main__ -   Batch Number = 143
01/06/2022 17:19:52 - INFO - __main__ -   Batch Number = 144
01/06/2022 17:19:52 - INFO - __main__ -   Batch Number = 145
01/06/2022 17:19:52 - INFO - __main__ -   Batch Number = 146
01/06/2022 17:19:52 - INFO - __main__ -   Batch Number = 147
Evaluating:  91%|█████████ | 147/162 [00:03<00:00, 41.45it/s]01/06/2022 17:19:52 - INFO - __main__ -   Batch Number = 148
01/06/2022 17:19:52 - INFO - __main__ -   Batch Number = 149
01/06/2022 17:19:52 - INFO - __main__ -   Batch Number = 150
01/06/2022 17:19:52 - INFO - __main__ -   Batch Number = 151
01/06/2022 17:19:52 - INFO - __main__ -   Batch Number = 152
Evaluating:  94%|█████████▍| 152/162 [00:03<00:00, 42.16it/s]01/06/2022 17:19:52 - INFO - __main__ -   Batch Number = 153
01/06/2022 17:19:52 - INFO - __main__ -   Batch Number = 154
01/06/2022 17:19:52 - INFO - __main__ -   Batch Number = 155
01/06/2022 17:19:52 - INFO - __main__ -   Batch Number = 156
01/06/2022 17:19:52 - INFO - __main__ -   Batch Number = 157
Evaluating:  97%|█████████▋| 157/162 [00:03<00:00, 42.63it/s]01/06/2022 17:19:52 - INFO - __main__ -   Batch Number = 158
01/06/2022 17:19:52 - INFO - __main__ -   Batch Number = 159
01/06/2022 17:19:52 - INFO - __main__ -   Batch Number = 160
01/06/2022 17:19:52 - INFO - __main__ -   Batch Number = 161
01/06/2022 17:19:52 - INFO - __main__ -   Batch Number = 162
Evaluating: 100%|██████████| 162/162 [00:03<00:00, 43.16it/s]Evaluating: 100%|██████████| 162/162 [00:03<00:00, 42.27it/s]
01/06/2022 17:19:52 - INFO - __main__ -     Evaluation done in total 3.832935 secs (0.002958 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/predictions_de_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/nbest_predictions_de_.json
01/06/2022 17:19:56 - INFO - __main__ -   Results = OrderedDict([('exact', 53.109243697478995), ('f1', 69.92205290492534), ('total', 1190), ('HasAns_exact', 53.109243697478995), ('HasAns_f1', 69.92205290492534), ('HasAns_total', 1190), ('best_exact', 53.109243697478995), ('best_exact_thresh', 0.0), ('best_f1', 69.92205290492534), ('best_f1_thresh', 0.0)])
01/06/2022 17:19:56 - INFO - __main__ -   Language adapter for el not found, using en instead
01/06/2022 17:19:56 - INFO - __main__ -   Set active language adapter to en
01/06/2022 17:19:56 - INFO - __main__ -   Args Adapter Weight = None
01/06/2022 17:19:56 - INFO - __main__ -   Adapter Languages = ['en']
01/06/2022 17:19:56 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
  0%|          | 0/48 [00:00<?, ?it/s] 29%|██▉       | 14/48 [00:00<00:00, 139.67it/s] 58%|█████▊    | 28/48 [00:00<00:00, 104.75it/s] 83%|████████▎ | 40/48 [00:00<00:00, 109.86it/s]100%|██████████| 48/48 [00:00<00:00, 114.66it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<06:17,  3.15it/s]convert squad examples to features:   5%|▌         | 65/1190 [00:00<00:05, 189.38it/s]convert squad examples to features:  27%|██▋       | 321/1190 [00:00<00:01, 802.14it/s]convert squad examples to features:  36%|███▌      | 424/1190 [00:01<00:03, 233.76it/s]convert squad examples to features:  94%|█████████▍| 1121/1190 [00:01<00:00, 826.58it/s]convert squad examples to features: 100%|██████████| 1190/1190 [00:01<00:00, 652.68it/s]/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|██████████| 1190/1190 [00:00<00:00, 378984.19it/s]
01/06/2022 17:19:59 - INFO - __main__ -   Local Rank = -1
01/06/2022 17:19:59 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.el.json_bert-base-multilingual-cased_384_el
01/06/2022 17:20:00 - INFO - __main__ -   ***** Running evaluation  *****
01/06/2022 17:20:00 - INFO - __main__ -     Num examples = 2077
01/06/2022 17:20:00 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/260 [00:00<?, ?it/s]01/06/2022 17:20:00 - INFO - __main__ -   Batch Number = 1
01/06/2022 17:20:00 - INFO - __main__ -   Batch Number = 2
01/06/2022 17:20:01 - INFO - __main__ -   Batch Number = 3
Evaluating:   1%|          | 3/260 [00:00<00:08, 29.74it/s]01/06/2022 17:20:01 - INFO - __main__ -   Batch Number = 4
01/06/2022 17:20:01 - INFO - __main__ -   Batch Number = 5
01/06/2022 17:20:01 - INFO - __main__ -   Batch Number = 6
01/06/2022 17:20:01 - INFO - __main__ -   Batch Number = 7
Evaluating:   3%|▎         | 7/260 [00:00<00:08, 30.02it/s]01/06/2022 17:20:01 - INFO - __main__ -   Batch Number = 8
01/06/2022 17:20:01 - INFO - __main__ -   Batch Number = 9
01/06/2022 17:20:01 - INFO - __main__ -   Batch Number = 10
01/06/2022 17:20:01 - INFO - __main__ -   Batch Number = 11
Evaluating:   4%|▍         | 11/260 [00:00<00:07, 32.82it/s]01/06/2022 17:20:01 - INFO - __main__ -   Batch Number = 12
01/06/2022 17:20:01 - INFO - __main__ -   Batch Number = 13
01/06/2022 17:20:01 - INFO - __main__ -   Batch Number = 14
01/06/2022 17:20:01 - INFO - __main__ -   Batch Number = 15
01/06/2022 17:20:01 - INFO - __main__ -   Batch Number = 16
Evaluating:   6%|▌         | 16/260 [00:00<00:06, 37.27it/s]01/06/2022 17:20:01 - INFO - __main__ -   Batch Number = 17
01/06/2022 17:20:01 - INFO - __main__ -   Batch Number = 18
01/06/2022 17:20:01 - INFO - __main__ -   Batch Number = 19
01/06/2022 17:20:01 - INFO - __main__ -   Batch Number = 20
01/06/2022 17:20:01 - INFO - __main__ -   Batch Number = 21
Evaluating:   8%|▊         | 21/260 [00:00<00:06, 39.46it/s]01/06/2022 17:20:01 - INFO - __main__ -   Batch Number = 22
01/06/2022 17:20:01 - INFO - __main__ -   Batch Number = 23
01/06/2022 17:20:01 - INFO - __main__ -   Batch Number = 24
01/06/2022 17:20:01 - INFO - __main__ -   Batch Number = 25
01/06/2022 17:20:01 - INFO - __main__ -   Batch Number = 26
Evaluating:  10%|█         | 26/260 [00:00<00:05, 41.16it/s]01/06/2022 17:20:01 - INFO - __main__ -   Batch Number = 27
01/06/2022 17:20:01 - INFO - __main__ -   Batch Number = 28
01/06/2022 17:20:01 - INFO - __main__ -   Batch Number = 29
01/06/2022 17:20:01 - INFO - __main__ -   Batch Number = 30
01/06/2022 17:20:01 - INFO - __main__ -   Batch Number = 31
Evaluating:  12%|█▏        | 31/260 [00:00<00:05, 42.23it/s]01/06/2022 17:20:01 - INFO - __main__ -   Batch Number = 32
01/06/2022 17:20:01 - INFO - __main__ -   Batch Number = 33
01/06/2022 17:20:01 - INFO - __main__ -   Batch Number = 34
01/06/2022 17:20:01 - INFO - __main__ -   Batch Number = 35
01/06/2022 17:20:01 - INFO - __main__ -   Batch Number = 36
Evaluating:  14%|█▍        | 36/260 [00:00<00:05, 41.13it/s]01/06/2022 17:20:01 - INFO - __main__ -   Batch Number = 37
01/06/2022 17:20:01 - INFO - __main__ -   Batch Number = 38
01/06/2022 17:20:01 - INFO - __main__ -   Batch Number = 39
01/06/2022 17:20:01 - INFO - __main__ -   Batch Number = 40
01/06/2022 17:20:01 - INFO - __main__ -   Batch Number = 41
Evaluating:  16%|█▌        | 41/260 [00:01<00:05, 42.09it/s]01/06/2022 17:20:01 - INFO - __main__ -   Batch Number = 42
01/06/2022 17:20:02 - INFO - __main__ -   Batch Number = 43
01/06/2022 17:20:02 - INFO - __main__ -   Batch Number = 44
01/06/2022 17:20:02 - INFO - __main__ -   Batch Number = 45
01/06/2022 17:20:02 - INFO - __main__ -   Batch Number = 46
Evaluating:  18%|█▊        | 46/260 [00:01<00:05, 42.77it/s]01/06/2022 17:20:02 - INFO - __main__ -   Batch Number = 47
01/06/2022 17:20:02 - INFO - __main__ -   Batch Number = 48
01/06/2022 17:20:02 - INFO - __main__ -   Batch Number = 49
01/06/2022 17:20:02 - INFO - __main__ -   Batch Number = 50
01/06/2022 17:20:02 - INFO - __main__ -   Batch Number = 51
Evaluating:  20%|█▉        | 51/260 [00:01<00:04, 43.10it/s]01/06/2022 17:20:02 - INFO - __main__ -   Batch Number = 52
01/06/2022 17:20:02 - INFO - __main__ -   Batch Number = 53
01/06/2022 17:20:02 - INFO - __main__ -   Batch Number = 54
01/06/2022 17:20:02 - INFO - __main__ -   Batch Number = 55
01/06/2022 17:20:02 - INFO - __main__ -   Batch Number = 56
Evaluating:  22%|██▏       | 56/260 [00:01<00:04, 43.44it/s]01/06/2022 17:20:02 - INFO - __main__ -   Batch Number = 57
01/06/2022 17:20:02 - INFO - __main__ -   Batch Number = 58
01/06/2022 17:20:02 - INFO - __main__ -   Batch Number = 59
01/06/2022 17:20:02 - INFO - __main__ -   Batch Number = 60
01/06/2022 17:20:02 - INFO - __main__ -   Batch Number = 61
Evaluating:  23%|██▎       | 61/260 [00:01<00:04, 43.76it/s]01/06/2022 17:20:02 - INFO - __main__ -   Batch Number = 62
01/06/2022 17:20:02 - INFO - __main__ -   Batch Number = 63
01/06/2022 17:20:02 - INFO - __main__ -   Batch Number = 64
01/06/2022 17:20:02 - INFO - __main__ -   Batch Number = 65
01/06/2022 17:20:02 - INFO - __main__ -   Batch Number = 66
Evaluating:  25%|██▌       | 66/260 [00:01<00:04, 43.65it/s]01/06/2022 17:20:02 - INFO - __main__ -   Batch Number = 67
01/06/2022 17:20:02 - INFO - __main__ -   Batch Number = 68
01/06/2022 17:20:02 - INFO - __main__ -   Batch Number = 69
01/06/2022 17:20:02 - INFO - __main__ -   Batch Number = 70
01/06/2022 17:20:02 - INFO - __main__ -   Batch Number = 71
Evaluating:  27%|██▋       | 71/260 [00:01<00:04, 43.70it/s]01/06/2022 17:20:02 - INFO - __main__ -   Batch Number = 72
01/06/2022 17:20:02 - INFO - __main__ -   Batch Number = 73
01/06/2022 17:20:02 - INFO - __main__ -   Batch Number = 74
01/06/2022 17:20:02 - INFO - __main__ -   Batch Number = 75
01/06/2022 17:20:02 - INFO - __main__ -   Batch Number = 76
Evaluating:  29%|██▉       | 76/260 [00:01<00:04, 44.04it/s]01/06/2022 17:20:02 - INFO - __main__ -   Batch Number = 77
01/06/2022 17:20:02 - INFO - __main__ -   Batch Number = 78
01/06/2022 17:20:02 - INFO - __main__ -   Batch Number = 79
01/06/2022 17:20:02 - INFO - __main__ -   Batch Number = 80
01/06/2022 17:20:02 - INFO - __main__ -   Batch Number = 81
Evaluating:  31%|███       | 81/260 [00:01<00:04, 44.12it/s]01/06/2022 17:20:02 - INFO - __main__ -   Batch Number = 82
01/06/2022 17:20:02 - INFO - __main__ -   Batch Number = 83
01/06/2022 17:20:02 - INFO - __main__ -   Batch Number = 84
01/06/2022 17:20:02 - INFO - __main__ -   Batch Number = 85
01/06/2022 17:20:02 - INFO - __main__ -   Batch Number = 86
Evaluating:  33%|███▎      | 86/260 [00:02<00:03, 44.32it/s]01/06/2022 17:20:03 - INFO - __main__ -   Batch Number = 87
01/06/2022 17:20:03 - INFO - __main__ -   Batch Number = 88
01/06/2022 17:20:03 - INFO - __main__ -   Batch Number = 89
01/06/2022 17:20:03 - INFO - __main__ -   Batch Number = 90
01/06/2022 17:20:03 - INFO - __main__ -   Batch Number = 91
Evaluating:  35%|███▌      | 91/260 [00:02<00:03, 44.29it/s]01/06/2022 17:20:03 - INFO - __main__ -   Batch Number = 92
01/06/2022 17:20:03 - INFO - __main__ -   Batch Number = 93
01/06/2022 17:20:03 - INFO - __main__ -   Batch Number = 94
01/06/2022 17:20:03 - INFO - __main__ -   Batch Number = 95
01/06/2022 17:20:03 - INFO - __main__ -   Batch Number = 96
Evaluating:  37%|███▋      | 96/260 [00:02<00:03, 44.29it/s]01/06/2022 17:20:03 - INFO - __main__ -   Batch Number = 97
01/06/2022 17:20:03 - INFO - __main__ -   Batch Number = 98
01/06/2022 17:20:03 - INFO - __main__ -   Batch Number = 99
01/06/2022 17:20:03 - INFO - __main__ -   Batch Number = 100
01/06/2022 17:20:03 - INFO - __main__ -   Batch Number = 101
Evaluating:  39%|███▉      | 101/260 [00:02<00:03, 44.30it/s]01/06/2022 17:20:03 - INFO - __main__ -   Batch Number = 102
01/06/2022 17:20:03 - INFO - __main__ -   Batch Number = 103
01/06/2022 17:20:03 - INFO - __main__ -   Batch Number = 104
01/06/2022 17:20:03 - INFO - __main__ -   Batch Number = 105
01/06/2022 17:20:03 - INFO - __main__ -   Batch Number = 106
Evaluating:  41%|████      | 106/260 [00:02<00:03, 44.16it/s]01/06/2022 17:20:03 - INFO - __main__ -   Batch Number = 107
01/06/2022 17:20:03 - INFO - __main__ -   Batch Number = 108
01/06/2022 17:20:03 - INFO - __main__ -   Batch Number = 109
01/06/2022 17:20:03 - INFO - __main__ -   Batch Number = 110
01/06/2022 17:20:03 - INFO - __main__ -   Batch Number = 111
Evaluating:  43%|████▎     | 111/260 [00:02<00:03, 44.13it/s]01/06/2022 17:20:03 - INFO - __main__ -   Batch Number = 112
01/06/2022 17:20:03 - INFO - __main__ -   Batch Number = 113
01/06/2022 17:20:03 - INFO - __main__ -   Batch Number = 114
01/06/2022 17:20:03 - INFO - __main__ -   Batch Number = 115
01/06/2022 17:20:03 - INFO - __main__ -   Batch Number = 116
Evaluating:  45%|████▍     | 116/260 [00:02<00:03, 44.17it/s]01/06/2022 17:20:03 - INFO - __main__ -   Batch Number = 117
01/06/2022 17:20:03 - INFO - __main__ -   Batch Number = 118
01/06/2022 17:20:03 - INFO - __main__ -   Batch Number = 119
01/06/2022 17:20:03 - INFO - __main__ -   Batch Number = 120
01/06/2022 17:20:03 - INFO - __main__ -   Batch Number = 121
Evaluating:  47%|████▋     | 121/260 [00:02<00:03, 44.35it/s]01/06/2022 17:20:03 - INFO - __main__ -   Batch Number = 122
01/06/2022 17:20:03 - INFO - __main__ -   Batch Number = 123
01/06/2022 17:20:03 - INFO - __main__ -   Batch Number = 124
01/06/2022 17:20:03 - INFO - __main__ -   Batch Number = 125
01/06/2022 17:20:03 - INFO - __main__ -   Batch Number = 126
Evaluating:  48%|████▊     | 126/260 [00:02<00:03, 44.19it/s]01/06/2022 17:20:03 - INFO - __main__ -   Batch Number = 127
01/06/2022 17:20:03 - INFO - __main__ -   Batch Number = 128
01/06/2022 17:20:03 - INFO - __main__ -   Batch Number = 129
01/06/2022 17:20:03 - INFO - __main__ -   Batch Number = 130
01/06/2022 17:20:04 - INFO - __main__ -   Batch Number = 131
Evaluating:  50%|█████     | 131/260 [00:03<00:02, 44.32it/s]01/06/2022 17:20:04 - INFO - __main__ -   Batch Number = 132
01/06/2022 17:20:04 - INFO - __main__ -   Batch Number = 133
01/06/2022 17:20:04 - INFO - __main__ -   Batch Number = 134
01/06/2022 17:20:04 - INFO - __main__ -   Batch Number = 135
01/06/2022 17:20:04 - INFO - __main__ -   Batch Number = 136
Evaluating:  52%|█████▏    | 136/260 [00:03<00:02, 44.28it/s]01/06/2022 17:20:04 - INFO - __main__ -   Batch Number = 137
01/06/2022 17:20:04 - INFO - __main__ -   Batch Number = 138
01/06/2022 17:20:04 - INFO - __main__ -   Batch Number = 139
01/06/2022 17:20:04 - INFO - __main__ -   Batch Number = 140
01/06/2022 17:20:04 - INFO - __main__ -   Batch Number = 141
Evaluating:  54%|█████▍    | 141/260 [00:03<00:02, 44.21it/s]01/06/2022 17:20:04 - INFO - __main__ -   Batch Number = 142
01/06/2022 17:20:04 - INFO - __main__ -   Batch Number = 143
01/06/2022 17:20:04 - INFO - __main__ -   Batch Number = 144
01/06/2022 17:20:04 - INFO - __main__ -   Batch Number = 145
01/06/2022 17:20:04 - INFO - __main__ -   Batch Number = 146
Evaluating:  56%|█████▌    | 146/260 [00:03<00:02, 44.26it/s]01/06/2022 17:20:04 - INFO - __main__ -   Batch Number = 147
01/06/2022 17:20:04 - INFO - __main__ -   Batch Number = 148
01/06/2022 17:20:04 - INFO - __main__ -   Batch Number = 149
01/06/2022 17:20:04 - INFO - __main__ -   Batch Number = 150
01/06/2022 17:20:04 - INFO - __main__ -   Batch Number = 151
Evaluating:  58%|█████▊    | 151/260 [00:03<00:02, 44.01it/s]01/06/2022 17:20:04 - INFO - __main__ -   Batch Number = 152
01/06/2022 17:20:04 - INFO - __main__ -   Batch Number = 153
01/06/2022 17:20:04 - INFO - __main__ -   Batch Number = 154
01/06/2022 17:20:04 - INFO - __main__ -   Batch Number = 155
01/06/2022 17:20:04 - INFO - __main__ -   Batch Number = 156
Evaluating:  60%|██████    | 156/260 [00:03<00:02, 44.10it/s]01/06/2022 17:20:04 - INFO - __main__ -   Batch Number = 157
01/06/2022 17:20:04 - INFO - __main__ -   Batch Number = 158
01/06/2022 17:20:04 - INFO - __main__ -   Batch Number = 159
01/06/2022 17:20:04 - INFO - __main__ -   Batch Number = 160
01/06/2022 17:20:04 - INFO - __main__ -   Batch Number = 161
Evaluating:  62%|██████▏   | 161/260 [00:03<00:02, 44.12it/s]01/06/2022 17:20:04 - INFO - __main__ -   Batch Number = 162
01/06/2022 17:20:04 - INFO - __main__ -   Batch Number = 163
01/06/2022 17:20:04 - INFO - __main__ -   Batch Number = 164
01/06/2022 17:20:04 - INFO - __main__ -   Batch Number = 165
01/06/2022 17:20:04 - INFO - __main__ -   Batch Number = 166
Evaluating:  64%|██████▍   | 166/260 [00:03<00:02, 44.10it/s]01/06/2022 17:20:04 - INFO - __main__ -   Batch Number = 167
01/06/2022 17:20:04 - INFO - __main__ -   Batch Number = 168
01/06/2022 17:20:04 - INFO - __main__ -   Batch Number = 169
01/06/2022 17:20:04 - INFO - __main__ -   Batch Number = 170
01/06/2022 17:20:04 - INFO - __main__ -   Batch Number = 171
Evaluating:  66%|██████▌   | 171/260 [00:03<00:02, 44.03it/s]01/06/2022 17:20:04 - INFO - __main__ -   Batch Number = 172
01/06/2022 17:20:04 - INFO - __main__ -   Batch Number = 173
01/06/2022 17:20:04 - INFO - __main__ -   Batch Number = 174
01/06/2022 17:20:05 - INFO - __main__ -   Batch Number = 175
01/06/2022 17:20:05 - INFO - __main__ -   Batch Number = 176
Evaluating:  68%|██████▊   | 176/260 [00:04<00:01, 44.20it/s]01/06/2022 17:20:05 - INFO - __main__ -   Batch Number = 177
01/06/2022 17:20:05 - INFO - __main__ -   Batch Number = 178
01/06/2022 17:20:05 - INFO - __main__ -   Batch Number = 179
01/06/2022 17:20:05 - INFO - __main__ -   Batch Number = 180
01/06/2022 17:20:05 - INFO - __main__ -   Batch Number = 181
Evaluating:  70%|██████▉   | 181/260 [00:04<00:01, 44.26it/s]01/06/2022 17:20:05 - INFO - __main__ -   Batch Number = 182
01/06/2022 17:20:05 - INFO - __main__ -   Batch Number = 183
01/06/2022 17:20:05 - INFO - __main__ -   Batch Number = 184
01/06/2022 17:20:05 - INFO - __main__ -   Batch Number = 185
01/06/2022 17:20:05 - INFO - __main__ -   Batch Number = 186
Evaluating:  72%|███████▏  | 186/260 [00:04<00:01, 44.09it/s]01/06/2022 17:20:05 - INFO - __main__ -   Batch Number = 187
01/06/2022 17:20:05 - INFO - __main__ -   Batch Number = 188
01/06/2022 17:20:05 - INFO - __main__ -   Batch Number = 189
01/06/2022 17:20:05 - INFO - __main__ -   Batch Number = 190
01/06/2022 17:20:05 - INFO - __main__ -   Batch Number = 191
Evaluating:  73%|███████▎  | 191/260 [00:04<00:01, 44.21it/s]01/06/2022 17:20:05 - INFO - __main__ -   Batch Number = 192
01/06/2022 17:20:05 - INFO - __main__ -   Batch Number = 193
01/06/2022 17:20:05 - INFO - __main__ -   Batch Number = 194
01/06/2022 17:20:05 - INFO - __main__ -   Batch Number = 195
01/06/2022 17:20:05 - INFO - __main__ -   Batch Number = 196
Evaluating:  75%|███████▌  | 196/260 [00:04<00:01, 44.03it/s]01/06/2022 17:20:05 - INFO - __main__ -   Batch Number = 197
01/06/2022 17:20:05 - INFO - __main__ -   Batch Number = 198
01/06/2022 17:20:05 - INFO - __main__ -   Batch Number = 199
01/06/2022 17:20:05 - INFO - __main__ -   Batch Number = 200
01/06/2022 17:20:05 - INFO - __main__ -   Batch Number = 201
Evaluating:  77%|███████▋  | 201/260 [00:04<00:01, 44.07it/s]01/06/2022 17:20:05 - INFO - __main__ -   Batch Number = 202
01/06/2022 17:20:05 - INFO - __main__ -   Batch Number = 203
01/06/2022 17:20:05 - INFO - __main__ -   Batch Number = 204
01/06/2022 17:20:05 - INFO - __main__ -   Batch Number = 205
01/06/2022 17:20:05 - INFO - __main__ -   Batch Number = 206
Evaluating:  79%|███████▉  | 206/260 [00:04<00:01, 44.12it/s]01/06/2022 17:20:05 - INFO - __main__ -   Batch Number = 207
01/06/2022 17:20:05 - INFO - __main__ -   Batch Number = 208
01/06/2022 17:20:05 - INFO - __main__ -   Batch Number = 209
01/06/2022 17:20:05 - INFO - __main__ -   Batch Number = 210
01/06/2022 17:20:05 - INFO - __main__ -   Batch Number = 211
Evaluating:  81%|████████  | 211/260 [00:04<00:01, 44.22it/s]01/06/2022 17:20:05 - INFO - __main__ -   Batch Number = 212
01/06/2022 17:20:05 - INFO - __main__ -   Batch Number = 213
01/06/2022 17:20:05 - INFO - __main__ -   Batch Number = 214
01/06/2022 17:20:05 - INFO - __main__ -   Batch Number = 215
01/06/2022 17:20:05 - INFO - __main__ -   Batch Number = 216
Evaluating:  83%|████████▎ | 216/260 [00:05<00:01, 42.78it/s]01/06/2022 17:20:05 - INFO - __main__ -   Batch Number = 217
01/06/2022 17:20:05 - INFO - __main__ -   Batch Number = 218
01/06/2022 17:20:06 - INFO - __main__ -   Batch Number = 219
01/06/2022 17:20:06 - INFO - __main__ -   Batch Number = 220
01/06/2022 17:20:06 - INFO - __main__ -   Batch Number = 221
Evaluating:  85%|████████▌ | 221/260 [00:05<00:00, 43.31it/s]01/06/2022 17:20:06 - INFO - __main__ -   Batch Number = 222
01/06/2022 17:20:06 - INFO - __main__ -   Batch Number = 223
01/06/2022 17:20:06 - INFO - __main__ -   Batch Number = 224
01/06/2022 17:20:06 - INFO - __main__ -   Batch Number = 225
01/06/2022 17:20:06 - INFO - __main__ -   Batch Number = 226
Evaluating:  87%|████████▋ | 226/260 [00:05<00:00, 43.57it/s]01/06/2022 17:20:06 - INFO - __main__ -   Batch Number = 227
01/06/2022 17:20:06 - INFO - __main__ -   Batch Number = 228
01/06/2022 17:20:06 - INFO - __main__ -   Batch Number = 229
01/06/2022 17:20:06 - INFO - __main__ -   Batch Number = 230
01/06/2022 17:20:06 - INFO - __main__ -   Batch Number = 231
Evaluating:  89%|████████▉ | 231/260 [00:05<00:00, 43.72it/s]01/06/2022 17:20:06 - INFO - __main__ -   Batch Number = 232
01/06/2022 17:20:06 - INFO - __main__ -   Batch Number = 233
01/06/2022 17:20:06 - INFO - __main__ -   Batch Number = 234
01/06/2022 17:20:06 - INFO - __main__ -   Batch Number = 235
01/06/2022 17:20:06 - INFO - __main__ -   Batch Number = 236
Evaluating:  91%|█████████ | 236/260 [00:05<00:00, 43.71it/s]01/06/2022 17:20:06 - INFO - __main__ -   Batch Number = 237
01/06/2022 17:20:06 - INFO - __main__ -   Batch Number = 238
01/06/2022 17:20:06 - INFO - __main__ -   Batch Number = 239
01/06/2022 17:20:06 - INFO - __main__ -   Batch Number = 240
01/06/2022 17:20:06 - INFO - __main__ -   Batch Number = 241
Evaluating:  93%|█████████▎| 241/260 [00:05<00:00, 43.70it/s]01/06/2022 17:20:06 - INFO - __main__ -   Batch Number = 242
01/06/2022 17:20:06 - INFO - __main__ -   Batch Number = 243
01/06/2022 17:20:06 - INFO - __main__ -   Batch Number = 244
01/06/2022 17:20:06 - INFO - __main__ -   Batch Number = 245
01/06/2022 17:20:06 - INFO - __main__ -   Batch Number = 246
Evaluating:  95%|█████████▍| 246/260 [00:05<00:00, 43.91it/s]01/06/2022 17:20:06 - INFO - __main__ -   Batch Number = 247
01/06/2022 17:20:06 - INFO - __main__ -   Batch Number = 248
01/06/2022 17:20:06 - INFO - __main__ -   Batch Number = 249
01/06/2022 17:20:06 - INFO - __main__ -   Batch Number = 250
01/06/2022 17:20:06 - INFO - __main__ -   Batch Number = 251
Evaluating:  97%|█████████▋| 251/260 [00:05<00:00, 43.88it/s]01/06/2022 17:20:06 - INFO - __main__ -   Batch Number = 252
01/06/2022 17:20:06 - INFO - __main__ -   Batch Number = 253
01/06/2022 17:20:06 - INFO - __main__ -   Batch Number = 254
01/06/2022 17:20:06 - INFO - __main__ -   Batch Number = 255
01/06/2022 17:20:06 - INFO - __main__ -   Batch Number = 256
Evaluating:  98%|█████████▊| 256/260 [00:05<00:00, 44.09it/s]01/06/2022 17:20:06 - INFO - __main__ -   Batch Number = 257
01/06/2022 17:20:06 - INFO - __main__ -   Batch Number = 258
01/06/2022 17:20:06 - INFO - __main__ -   Batch Number = 259
01/06/2022 17:20:06 - INFO - __main__ -   Batch Number = 260
Evaluating: 100%|██████████| 260/260 [00:06<00:00, 43.32it/s]
01/06/2022 17:20:06 - INFO - __main__ -     Evaluation done in total 6.002487 secs (0.002890 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/predictions_el_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/nbest_predictions_el_.json
01/06/2022 17:20:10 - INFO - __main__ -   Results = OrderedDict([('exact', 28.571428571428573), ('f1', 43.471296291567995), ('total', 1190), ('HasAns_exact', 28.571428571428573), ('HasAns_f1', 43.471296291567995), ('HasAns_total', 1190), ('best_exact', 28.571428571428573), ('best_exact_thresh', 0.0), ('best_f1', 43.471296291567995), ('best_f1_thresh', 0.0)])
01/06/2022 17:20:10 - INFO - __main__ -   Language adapter for es not found, using en instead
01/06/2022 17:20:10 - INFO - __main__ -   Set active language adapter to en
01/06/2022 17:20:10 - INFO - __main__ -   Args Adapter Weight = None
01/06/2022 17:20:10 - INFO - __main__ -   Adapter Languages = ['en']
01/06/2022 17:20:10 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
  0%|          | 0/48 [00:00<?, ?it/s] 33%|███▎      | 16/48 [00:00<00:00, 124.12it/s] 60%|██████    | 29/48 [00:00<00:00, 96.22it/s]  81%|████████▏ | 39/48 [00:00<00:00, 89.65it/s]100%|██████████| 48/48 [00:00<00:00, 101.73it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<07:44,  2.56it/s]convert squad examples to features:  32%|███▏      | 385/1190 [00:01<00:01, 407.53it/s]convert squad examples to features:  92%|█████████▏| 1089/1190 [00:01<00:00, 1258.23it/s]convert squad examples to features: 100%|██████████| 1190/1190 [00:01<00:00, 907.33it/s] /root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|██████████| 1190/1190 [00:00<00:00, 485197.02it/s]
01/06/2022 17:20:13 - INFO - __main__ -   Local Rank = -1
01/06/2022 17:20:13 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.es.json_bert-base-multilingual-cased_384_es
01/06/2022 17:20:14 - INFO - __main__ -   ***** Running evaluation  *****
01/06/2022 17:20:14 - INFO - __main__ -     Num examples = 1307
01/06/2022 17:20:14 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/164 [00:00<?, ?it/s]01/06/2022 17:20:14 - INFO - __main__ -   Batch Number = 1
01/06/2022 17:20:14 - INFO - __main__ -   Batch Number = 2
01/06/2022 17:20:14 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|▏         | 3/164 [00:00<00:05, 29.49it/s]01/06/2022 17:20:14 - INFO - __main__ -   Batch Number = 4
01/06/2022 17:20:14 - INFO - __main__ -   Batch Number = 5
01/06/2022 17:20:14 - INFO - __main__ -   Batch Number = 6
01/06/2022 17:20:14 - INFO - __main__ -   Batch Number = 7
Evaluating:   4%|▍         | 7/164 [00:00<00:04, 33.59it/s]01/06/2022 17:20:14 - INFO - __main__ -   Batch Number = 8
01/06/2022 17:20:14 - INFO - __main__ -   Batch Number = 9
01/06/2022 17:20:14 - INFO - __main__ -   Batch Number = 10
01/06/2022 17:20:14 - INFO - __main__ -   Batch Number = 11
Evaluating:   7%|▋         | 11/164 [00:00<00:04, 35.46it/s]01/06/2022 17:20:14 - INFO - __main__ -   Batch Number = 12
01/06/2022 17:20:14 - INFO - __main__ -   Batch Number = 13
01/06/2022 17:20:14 - INFO - __main__ -   Batch Number = 14
01/06/2022 17:20:14 - INFO - __main__ -   Batch Number = 15
01/06/2022 17:20:14 - INFO - __main__ -   Batch Number = 16
Evaluating:  10%|▉         | 16/164 [00:00<00:03, 38.75it/s]01/06/2022 17:20:14 - INFO - __main__ -   Batch Number = 17
01/06/2022 17:20:14 - INFO - __main__ -   Batch Number = 18
01/06/2022 17:20:14 - INFO - __main__ -   Batch Number = 19
01/06/2022 17:20:14 - INFO - __main__ -   Batch Number = 20
01/06/2022 17:20:14 - INFO - __main__ -   Batch Number = 21
Evaluating:  13%|█▎        | 21/164 [00:00<00:03, 40.52it/s]01/06/2022 17:20:14 - INFO - __main__ -   Batch Number = 22
01/06/2022 17:20:14 - INFO - __main__ -   Batch Number = 23
01/06/2022 17:20:14 - INFO - __main__ -   Batch Number = 24
01/06/2022 17:20:14 - INFO - __main__ -   Batch Number = 25
01/06/2022 17:20:14 - INFO - __main__ -   Batch Number = 26
Evaluating:  16%|█▌        | 26/164 [00:00<00:03, 41.92it/s]01/06/2022 17:20:14 - INFO - __main__ -   Batch Number = 27
01/06/2022 17:20:14 - INFO - __main__ -   Batch Number = 28
01/06/2022 17:20:14 - INFO - __main__ -   Batch Number = 29
01/06/2022 17:20:14 - INFO - __main__ -   Batch Number = 30
01/06/2022 17:20:14 - INFO - __main__ -   Batch Number = 31
Evaluating:  19%|█▉        | 31/164 [00:00<00:03, 42.80it/s]01/06/2022 17:20:14 - INFO - __main__ -   Batch Number = 32
01/06/2022 17:20:14 - INFO - __main__ -   Batch Number = 33
01/06/2022 17:20:14 - INFO - __main__ -   Batch Number = 34
01/06/2022 17:20:14 - INFO - __main__ -   Batch Number = 35
01/06/2022 17:20:14 - INFO - __main__ -   Batch Number = 36
Evaluating:  22%|██▏       | 36/164 [00:00<00:02, 43.20it/s]01/06/2022 17:20:14 - INFO - __main__ -   Batch Number = 37
01/06/2022 17:20:14 - INFO - __main__ -   Batch Number = 38
01/06/2022 17:20:14 - INFO - __main__ -   Batch Number = 39
01/06/2022 17:20:15 - INFO - __main__ -   Batch Number = 40
01/06/2022 17:20:15 - INFO - __main__ -   Batch Number = 41
Evaluating:  25%|██▌       | 41/164 [00:00<00:02, 43.70it/s]01/06/2022 17:20:15 - INFO - __main__ -   Batch Number = 42
01/06/2022 17:20:15 - INFO - __main__ -   Batch Number = 43
01/06/2022 17:20:15 - INFO - __main__ -   Batch Number = 44
01/06/2022 17:20:15 - INFO - __main__ -   Batch Number = 45
01/06/2022 17:20:15 - INFO - __main__ -   Batch Number = 46
Evaluating:  28%|██▊       | 46/164 [00:01<00:02, 43.90it/s]01/06/2022 17:20:15 - INFO - __main__ -   Batch Number = 47
01/06/2022 17:20:15 - INFO - __main__ -   Batch Number = 48
01/06/2022 17:20:15 - INFO - __main__ -   Batch Number = 49
01/06/2022 17:20:15 - INFO - __main__ -   Batch Number = 50
01/06/2022 17:20:15 - INFO - __main__ -   Batch Number = 51
Evaluating:  31%|███       | 51/164 [00:01<00:02, 43.97it/s]01/06/2022 17:20:15 - INFO - __main__ -   Batch Number = 52
01/06/2022 17:20:15 - INFO - __main__ -   Batch Number = 53
01/06/2022 17:20:15 - INFO - __main__ -   Batch Number = 54
01/06/2022 17:20:15 - INFO - __main__ -   Batch Number = 55
01/06/2022 17:20:15 - INFO - __main__ -   Batch Number = 56
Evaluating:  34%|███▍      | 56/164 [00:01<00:02, 44.00it/s]01/06/2022 17:20:15 - INFO - __main__ -   Batch Number = 57
01/06/2022 17:20:15 - INFO - __main__ -   Batch Number = 58
01/06/2022 17:20:15 - INFO - __main__ -   Batch Number = 59
01/06/2022 17:20:15 - INFO - __main__ -   Batch Number = 60
01/06/2022 17:20:15 - INFO - __main__ -   Batch Number = 61
Evaluating:  37%|███▋      | 61/164 [00:01<00:02, 43.82it/s]01/06/2022 17:20:15 - INFO - __main__ -   Batch Number = 62
01/06/2022 17:20:15 - INFO - __main__ -   Batch Number = 63
01/06/2022 17:20:15 - INFO - __main__ -   Batch Number = 64
01/06/2022 17:20:15 - INFO - __main__ -   Batch Number = 65
01/06/2022 17:20:15 - INFO - __main__ -   Batch Number = 66
Evaluating:  40%|████      | 66/164 [00:01<00:02, 43.88it/s]01/06/2022 17:20:15 - INFO - __main__ -   Batch Number = 67
01/06/2022 17:20:15 - INFO - __main__ -   Batch Number = 68
01/06/2022 17:20:15 - INFO - __main__ -   Batch Number = 69
01/06/2022 17:20:15 - INFO - __main__ -   Batch Number = 70
01/06/2022 17:20:15 - INFO - __main__ -   Batch Number = 71
Evaluating:  43%|████▎     | 71/164 [00:01<00:02, 43.92it/s]01/06/2022 17:20:15 - INFO - __main__ -   Batch Number = 72
01/06/2022 17:20:15 - INFO - __main__ -   Batch Number = 73
01/06/2022 17:20:15 - INFO - __main__ -   Batch Number = 74
01/06/2022 17:20:15 - INFO - __main__ -   Batch Number = 75
01/06/2022 17:20:15 - INFO - __main__ -   Batch Number = 76
Evaluating:  46%|████▋     | 76/164 [00:01<00:01, 44.14it/s]01/06/2022 17:20:15 - INFO - __main__ -   Batch Number = 77
01/06/2022 17:20:15 - INFO - __main__ -   Batch Number = 78
01/06/2022 17:20:15 - INFO - __main__ -   Batch Number = 79
01/06/2022 17:20:15 - INFO - __main__ -   Batch Number = 80
01/06/2022 17:20:15 - INFO - __main__ -   Batch Number = 81
Evaluating:  49%|████▉     | 81/164 [00:01<00:01, 44.07it/s]01/06/2022 17:20:15 - INFO - __main__ -   Batch Number = 82
01/06/2022 17:20:15 - INFO - __main__ -   Batch Number = 83
01/06/2022 17:20:16 - INFO - __main__ -   Batch Number = 84
01/06/2022 17:20:16 - INFO - __main__ -   Batch Number = 85
01/06/2022 17:20:16 - INFO - __main__ -   Batch Number = 86
Evaluating:  52%|█████▏    | 86/164 [00:02<00:01, 44.20it/s]01/06/2022 17:20:16 - INFO - __main__ -   Batch Number = 87
01/06/2022 17:20:16 - INFO - __main__ -   Batch Number = 88
01/06/2022 17:20:16 - INFO - __main__ -   Batch Number = 89
01/06/2022 17:20:16 - INFO - __main__ -   Batch Number = 90
01/06/2022 17:20:16 - INFO - __main__ -   Batch Number = 91
Evaluating:  55%|█████▌    | 91/164 [00:02<00:01, 44.31it/s]01/06/2022 17:20:16 - INFO - __main__ -   Batch Number = 92
01/06/2022 17:20:16 - INFO - __main__ -   Batch Number = 93
01/06/2022 17:20:16 - INFO - __main__ -   Batch Number = 94
01/06/2022 17:20:16 - INFO - __main__ -   Batch Number = 95
01/06/2022 17:20:16 - INFO - __main__ -   Batch Number = 96
Evaluating:  59%|█████▊    | 96/164 [00:02<00:01, 44.31it/s]01/06/2022 17:20:16 - INFO - __main__ -   Batch Number = 97
01/06/2022 17:20:16 - INFO - __main__ -   Batch Number = 98
01/06/2022 17:20:16 - INFO - __main__ -   Batch Number = 99
01/06/2022 17:20:16 - INFO - __main__ -   Batch Number = 100
01/06/2022 17:20:16 - INFO - __main__ -   Batch Number = 101
Evaluating:  62%|██████▏   | 101/164 [00:02<00:01, 44.41it/s]01/06/2022 17:20:16 - INFO - __main__ -   Batch Number = 102
01/06/2022 17:20:16 - INFO - __main__ -   Batch Number = 103
01/06/2022 17:20:16 - INFO - __main__ -   Batch Number = 104
01/06/2022 17:20:16 - INFO - __main__ -   Batch Number = 105
01/06/2022 17:20:16 - INFO - __main__ -   Batch Number = 106
Evaluating:  65%|██████▍   | 106/164 [00:02<00:01, 44.15it/s]01/06/2022 17:20:16 - INFO - __main__ -   Batch Number = 107
01/06/2022 17:20:16 - INFO - __main__ -   Batch Number = 108
01/06/2022 17:20:16 - INFO - __main__ -   Batch Number = 109
01/06/2022 17:20:16 - INFO - __main__ -   Batch Number = 110
01/06/2022 17:20:16 - INFO - __main__ -   Batch Number = 111
Evaluating:  68%|██████▊   | 111/164 [00:02<00:01, 42.68it/s]01/06/2022 17:20:16 - INFO - __main__ -   Batch Number = 112
01/06/2022 17:20:16 - INFO - __main__ -   Batch Number = 113
01/06/2022 17:20:16 - INFO - __main__ -   Batch Number = 114
01/06/2022 17:20:16 - INFO - __main__ -   Batch Number = 115
01/06/2022 17:20:16 - INFO - __main__ -   Batch Number = 116
Evaluating:  71%|███████   | 116/164 [00:02<00:01, 43.11it/s]01/06/2022 17:20:16 - INFO - __main__ -   Batch Number = 117
01/06/2022 17:20:16 - INFO - __main__ -   Batch Number = 118
01/06/2022 17:20:16 - INFO - __main__ -   Batch Number = 119
01/06/2022 17:20:16 - INFO - __main__ -   Batch Number = 120
01/06/2022 17:20:16 - INFO - __main__ -   Batch Number = 121
Evaluating:  74%|███████▍  | 121/164 [00:02<00:00, 43.56it/s]01/06/2022 17:20:16 - INFO - __main__ -   Batch Number = 122
01/06/2022 17:20:16 - INFO - __main__ -   Batch Number = 123
01/06/2022 17:20:16 - INFO - __main__ -   Batch Number = 124
01/06/2022 17:20:16 - INFO - __main__ -   Batch Number = 125
01/06/2022 17:20:16 - INFO - __main__ -   Batch Number = 126
Evaluating:  77%|███████▋  | 126/164 [00:02<00:00, 43.61it/s]01/06/2022 17:20:16 - INFO - __main__ -   Batch Number = 127
01/06/2022 17:20:17 - INFO - __main__ -   Batch Number = 128
01/06/2022 17:20:17 - INFO - __main__ -   Batch Number = 129
01/06/2022 17:20:17 - INFO - __main__ -   Batch Number = 130
01/06/2022 17:20:17 - INFO - __main__ -   Batch Number = 131
Evaluating:  80%|███████▉  | 131/164 [00:03<00:00, 43.87it/s]01/06/2022 17:20:17 - INFO - __main__ -   Batch Number = 132
01/06/2022 17:20:17 - INFO - __main__ -   Batch Number = 133
01/06/2022 17:20:17 - INFO - __main__ -   Batch Number = 134
01/06/2022 17:20:17 - INFO - __main__ -   Batch Number = 135
01/06/2022 17:20:17 - INFO - __main__ -   Batch Number = 136
Evaluating:  83%|████████▎ | 136/164 [00:03<00:00, 43.87it/s]01/06/2022 17:20:17 - INFO - __main__ -   Batch Number = 137
01/06/2022 17:20:17 - INFO - __main__ -   Batch Number = 138
01/06/2022 17:20:17 - INFO - __main__ -   Batch Number = 139
01/06/2022 17:20:17 - INFO - __main__ -   Batch Number = 140
01/06/2022 17:20:17 - INFO - __main__ -   Batch Number = 141
Evaluating:  86%|████████▌ | 141/164 [00:03<00:00, 43.79it/s]01/06/2022 17:20:17 - INFO - __main__ -   Batch Number = 142
01/06/2022 17:20:17 - INFO - __main__ -   Batch Number = 143
01/06/2022 17:20:17 - INFO - __main__ -   Batch Number = 144
01/06/2022 17:20:17 - INFO - __main__ -   Batch Number = 145
01/06/2022 17:20:17 - INFO - __main__ -   Batch Number = 146
Evaluating:  89%|████████▉ | 146/164 [00:03<00:00, 37.62it/s]01/06/2022 17:20:17 - INFO - __main__ -   Batch Number = 147
01/06/2022 17:20:17 - INFO - __main__ -   Batch Number = 148
01/06/2022 17:20:17 - INFO - __main__ -   Batch Number = 149
01/06/2022 17:20:17 - INFO - __main__ -   Batch Number = 150
01/06/2022 17:20:17 - INFO - __main__ -   Batch Number = 151
Evaluating:  92%|█████████▏| 151/164 [00:03<00:00, 39.35it/s]01/06/2022 17:20:17 - INFO - __main__ -   Batch Number = 152
01/06/2022 17:20:17 - INFO - __main__ -   Batch Number = 153
01/06/2022 17:20:17 - INFO - __main__ -   Batch Number = 154
01/06/2022 17:20:17 - INFO - __main__ -   Batch Number = 155
01/06/2022 17:20:17 - INFO - __main__ -   Batch Number = 156
Evaluating:  95%|█████████▌| 156/164 [00:03<00:00, 40.66it/s]01/06/2022 17:20:17 - INFO - __main__ -   Batch Number = 157
01/06/2022 17:20:17 - INFO - __main__ -   Batch Number = 158
01/06/2022 17:20:17 - INFO - __main__ -   Batch Number = 159
01/06/2022 17:20:17 - INFO - __main__ -   Batch Number = 160
01/06/2022 17:20:17 - INFO - __main__ -   Batch Number = 161
Evaluating:  98%|█████████▊| 161/164 [00:03<00:00, 41.71it/s]01/06/2022 17:20:17 - INFO - __main__ -   Batch Number = 162
01/06/2022 17:20:17 - INFO - __main__ -   Batch Number = 163
01/06/2022 17:20:17 - INFO - __main__ -   Batch Number = 164
Evaluating: 100%|██████████| 164/164 [00:03<00:00, 42.56it/s]
01/06/2022 17:20:17 - INFO - __main__ -     Evaluation done in total 3.854028 secs (0.002949 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/predictions_es_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/nbest_predictions_es_.json
01/06/2022 17:20:21 - INFO - __main__ -   Results = OrderedDict([('exact', 50.67226890756302), ('f1', 70.8562906056144), ('total', 1190), ('HasAns_exact', 50.67226890756302), ('HasAns_f1', 70.8562906056144), ('HasAns_total', 1190), ('best_exact', 50.67226890756302), ('best_exact_thresh', 0.0), ('best_f1', 70.8562906056144), ('best_f1_thresh', 0.0)])
01/06/2022 17:20:21 - INFO - __main__ -   Language adapter for hi not found, using en instead
01/06/2022 17:20:21 - INFO - __main__ -   Set active language adapter to en
01/06/2022 17:20:21 - INFO - __main__ -   Args Adapter Weight = None
01/06/2022 17:20:21 - INFO - __main__ -   Adapter Languages = ['en']
01/06/2022 17:20:21 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
  0%|          | 0/48 [00:00<?, ?it/s] 33%|███▎      | 16/48 [00:00<00:00, 134.20it/s] 62%|██████▎   | 30/48 [00:00<00:00, 124.32it/s] 90%|████████▉ | 43/48 [00:00<00:00, 105.67it/s]100%|██████████| 48/48 [00:00<00:00, 114.66it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<06:34,  3.01it/s]convert squad examples to features:  22%|██▏       | 257/1190 [00:00<00:01, 677.75it/s]convert squad examples to features:  32%|███▏      | 385/1190 [00:01<00:02, 321.91it/s]convert squad examples to features:  75%|███████▌  | 897/1190 [00:01<00:00, 922.20it/s]convert squad examples to features:  94%|█████████▍| 1121/1190 [00:01<00:00, 1031.63it/s]convert squad examples to features: 100%|██████████| 1190/1190 [00:01<00:00, 787.37it/s] /root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|██████████| 1190/1190 [00:00<00:00, 590327.82it/s]
01/06/2022 17:20:24 - INFO - __main__ -   Local Rank = -1
01/06/2022 17:20:24 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.hi.json_bert-base-multilingual-cased_384_hi
01/06/2022 17:20:25 - INFO - __main__ -   ***** Running evaluation  *****
01/06/2022 17:20:25 - INFO - __main__ -     Num examples = 1629
01/06/2022 17:20:25 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/204 [00:00<?, ?it/s]01/06/2022 17:20:25 - INFO - __main__ -   Batch Number = 1
01/06/2022 17:20:25 - INFO - __main__ -   Batch Number = 2
01/06/2022 17:20:25 - INFO - __main__ -   Batch Number = 3
Evaluating:   1%|▏         | 3/204 [00:00<00:06, 29.53it/s]01/06/2022 17:20:25 - INFO - __main__ -   Batch Number = 4
01/06/2022 17:20:25 - INFO - __main__ -   Batch Number = 5
01/06/2022 17:20:25 - INFO - __main__ -   Batch Number = 6
Evaluating:   3%|▎         | 6/204 [00:00<00:06, 29.76it/s]01/06/2022 17:20:25 - INFO - __main__ -   Batch Number = 7
01/06/2022 17:20:25 - INFO - __main__ -   Batch Number = 8
01/06/2022 17:20:25 - INFO - __main__ -   Batch Number = 9
01/06/2022 17:20:25 - INFO - __main__ -   Batch Number = 10
01/06/2022 17:20:25 - INFO - __main__ -   Batch Number = 11
Evaluating:   5%|▌         | 11/204 [00:00<00:05, 36.59it/s]01/06/2022 17:20:25 - INFO - __main__ -   Batch Number = 12
01/06/2022 17:20:25 - INFO - __main__ -   Batch Number = 13
01/06/2022 17:20:25 - INFO - __main__ -   Batch Number = 14
01/06/2022 17:20:25 - INFO - __main__ -   Batch Number = 15
01/06/2022 17:20:25 - INFO - __main__ -   Batch Number = 16
Evaluating:   8%|▊         | 16/204 [00:00<00:04, 39.65it/s]01/06/2022 17:20:25 - INFO - __main__ -   Batch Number = 17
01/06/2022 17:20:25 - INFO - __main__ -   Batch Number = 18
01/06/2022 17:20:25 - INFO - __main__ -   Batch Number = 19
01/06/2022 17:20:25 - INFO - __main__ -   Batch Number = 20
01/06/2022 17:20:25 - INFO - __main__ -   Batch Number = 21
Evaluating:  10%|█         | 21/204 [00:00<00:04, 40.79it/s]01/06/2022 17:20:26 - INFO - __main__ -   Batch Number = 22
01/06/2022 17:20:26 - INFO - __main__ -   Batch Number = 23
01/06/2022 17:20:26 - INFO - __main__ -   Batch Number = 24
01/06/2022 17:20:26 - INFO - __main__ -   Batch Number = 25
01/06/2022 17:20:26 - INFO - __main__ -   Batch Number = 26
Evaluating:  13%|█▎        | 26/204 [00:00<00:04, 41.95it/s]01/06/2022 17:20:26 - INFO - __main__ -   Batch Number = 27
01/06/2022 17:20:26 - INFO - __main__ -   Batch Number = 28
01/06/2022 17:20:26 - INFO - __main__ -   Batch Number = 29
01/06/2022 17:20:26 - INFO - __main__ -   Batch Number = 30
01/06/2022 17:20:26 - INFO - __main__ -   Batch Number = 31
Evaluating:  15%|█▌        | 31/204 [00:00<00:04, 42.60it/s]01/06/2022 17:20:26 - INFO - __main__ -   Batch Number = 32
01/06/2022 17:20:26 - INFO - __main__ -   Batch Number = 33
01/06/2022 17:20:26 - INFO - __main__ -   Batch Number = 34
01/06/2022 17:20:26 - INFO - __main__ -   Batch Number = 35
01/06/2022 17:20:26 - INFO - __main__ -   Batch Number = 36
Evaluating:  18%|█▊        | 36/204 [00:00<00:03, 42.85it/s]01/06/2022 17:20:26 - INFO - __main__ -   Batch Number = 37
01/06/2022 17:20:26 - INFO - __main__ -   Batch Number = 38
01/06/2022 17:20:26 - INFO - __main__ -   Batch Number = 39
01/06/2022 17:20:26 - INFO - __main__ -   Batch Number = 40
01/06/2022 17:20:26 - INFO - __main__ -   Batch Number = 41
Evaluating:  20%|██        | 41/204 [00:01<00:03, 43.25it/s]01/06/2022 17:20:26 - INFO - __main__ -   Batch Number = 42
01/06/2022 17:20:26 - INFO - __main__ -   Batch Number = 43
01/06/2022 17:20:26 - INFO - __main__ -   Batch Number = 44
01/06/2022 17:20:26 - INFO - __main__ -   Batch Number = 45
01/06/2022 17:20:26 - INFO - __main__ -   Batch Number = 46
Evaluating:  23%|██▎       | 46/204 [00:01<00:03, 43.41it/s]01/06/2022 17:20:26 - INFO - __main__ -   Batch Number = 47
01/06/2022 17:20:26 - INFO - __main__ -   Batch Number = 48
01/06/2022 17:20:26 - INFO - __main__ -   Batch Number = 49
01/06/2022 17:20:26 - INFO - __main__ -   Batch Number = 50
01/06/2022 17:20:26 - INFO - __main__ -   Batch Number = 51
Evaluating:  25%|██▌       | 51/204 [00:01<00:03, 43.56it/s]01/06/2022 17:20:26 - INFO - __main__ -   Batch Number = 52
01/06/2022 17:20:26 - INFO - __main__ -   Batch Number = 53
01/06/2022 17:20:26 - INFO - __main__ -   Batch Number = 54
01/06/2022 17:20:26 - INFO - __main__ -   Batch Number = 55
01/06/2022 17:20:26 - INFO - __main__ -   Batch Number = 56
Evaluating:  27%|██▋       | 56/204 [00:01<00:03, 43.81it/s]01/06/2022 17:20:26 - INFO - __main__ -   Batch Number = 57
01/06/2022 17:20:26 - INFO - __main__ -   Batch Number = 58
01/06/2022 17:20:26 - INFO - __main__ -   Batch Number = 59
01/06/2022 17:20:26 - INFO - __main__ -   Batch Number = 60
01/06/2022 17:20:26 - INFO - __main__ -   Batch Number = 61
Evaluating:  30%|██▉       | 61/204 [00:01<00:03, 43.85it/s]01/06/2022 17:20:26 - INFO - __main__ -   Batch Number = 62
01/06/2022 17:20:26 - INFO - __main__ -   Batch Number = 63
01/06/2022 17:20:26 - INFO - __main__ -   Batch Number = 64
01/06/2022 17:20:26 - INFO - __main__ -   Batch Number = 65
01/06/2022 17:20:27 - INFO - __main__ -   Batch Number = 66
Evaluating:  32%|███▏      | 66/204 [00:01<00:03, 43.76it/s]01/06/2022 17:20:27 - INFO - __main__ -   Batch Number = 67
01/06/2022 17:20:27 - INFO - __main__ -   Batch Number = 68
01/06/2022 17:20:27 - INFO - __main__ -   Batch Number = 69
01/06/2022 17:20:27 - INFO - __main__ -   Batch Number = 70
01/06/2022 17:20:27 - INFO - __main__ -   Batch Number = 71
Evaluating:  35%|███▍      | 71/204 [00:01<00:03, 43.88it/s]01/06/2022 17:20:27 - INFO - __main__ -   Batch Number = 72
01/06/2022 17:20:27 - INFO - __main__ -   Batch Number = 73
01/06/2022 17:20:27 - INFO - __main__ -   Batch Number = 74
01/06/2022 17:20:27 - INFO - __main__ -   Batch Number = 75
01/06/2022 17:20:27 - INFO - __main__ -   Batch Number = 76
Evaluating:  37%|███▋      | 76/204 [00:01<00:02, 43.99it/s]01/06/2022 17:20:27 - INFO - __main__ -   Batch Number = 77
01/06/2022 17:20:27 - INFO - __main__ -   Batch Number = 78
01/06/2022 17:20:27 - INFO - __main__ -   Batch Number = 79
01/06/2022 17:20:27 - INFO - __main__ -   Batch Number = 80
01/06/2022 17:20:27 - INFO - __main__ -   Batch Number = 81
Evaluating:  40%|███▉      | 81/204 [00:01<00:02, 44.04it/s]01/06/2022 17:20:27 - INFO - __main__ -   Batch Number = 82
01/06/2022 17:20:27 - INFO - __main__ -   Batch Number = 83
01/06/2022 17:20:27 - INFO - __main__ -   Batch Number = 84
01/06/2022 17:20:27 - INFO - __main__ -   Batch Number = 85
01/06/2022 17:20:27 - INFO - __main__ -   Batch Number = 86
Evaluating:  42%|████▏     | 86/204 [00:02<00:02, 44.13it/s]01/06/2022 17:20:27 - INFO - __main__ -   Batch Number = 87
01/06/2022 17:20:27 - INFO - __main__ -   Batch Number = 88
01/06/2022 17:20:27 - INFO - __main__ -   Batch Number = 89
01/06/2022 17:20:27 - INFO - __main__ -   Batch Number = 90
01/06/2022 17:20:27 - INFO - __main__ -   Batch Number = 91
Evaluating:  45%|████▍     | 91/204 [00:02<00:02, 44.24it/s]01/06/2022 17:20:27 - INFO - __main__ -   Batch Number = 92
01/06/2022 17:20:27 - INFO - __main__ -   Batch Number = 93
01/06/2022 17:20:27 - INFO - __main__ -   Batch Number = 94
01/06/2022 17:20:27 - INFO - __main__ -   Batch Number = 95
01/06/2022 17:20:27 - INFO - __main__ -   Batch Number = 96
Evaluating:  47%|████▋     | 96/204 [00:02<00:02, 44.02it/s]01/06/2022 17:20:27 - INFO - __main__ -   Batch Number = 97
01/06/2022 17:20:27 - INFO - __main__ -   Batch Number = 98
01/06/2022 17:20:27 - INFO - __main__ -   Batch Number = 99
01/06/2022 17:20:27 - INFO - __main__ -   Batch Number = 100
01/06/2022 17:20:27 - INFO - __main__ -   Batch Number = 101
Evaluating:  50%|████▉     | 101/204 [00:02<00:02, 44.03it/s]01/06/2022 17:20:27 - INFO - __main__ -   Batch Number = 102
01/06/2022 17:20:27 - INFO - __main__ -   Batch Number = 103
01/06/2022 17:20:27 - INFO - __main__ -   Batch Number = 104
01/06/2022 17:20:27 - INFO - __main__ -   Batch Number = 105
01/06/2022 17:20:27 - INFO - __main__ -   Batch Number = 106
Evaluating:  52%|█████▏    | 106/204 [00:02<00:02, 44.05it/s]01/06/2022 17:20:27 - INFO - __main__ -   Batch Number = 107
01/06/2022 17:20:27 - INFO - __main__ -   Batch Number = 108
01/06/2022 17:20:27 - INFO - __main__ -   Batch Number = 109
01/06/2022 17:20:28 - INFO - __main__ -   Batch Number = 110
01/06/2022 17:20:28 - INFO - __main__ -   Batch Number = 111
Evaluating:  54%|█████▍    | 111/204 [00:02<00:02, 43.92it/s]01/06/2022 17:20:28 - INFO - __main__ -   Batch Number = 112
01/06/2022 17:20:28 - INFO - __main__ -   Batch Number = 113
01/06/2022 17:20:28 - INFO - __main__ -   Batch Number = 114
01/06/2022 17:20:28 - INFO - __main__ -   Batch Number = 115
01/06/2022 17:20:28 - INFO - __main__ -   Batch Number = 116
Evaluating:  57%|█████▋    | 116/204 [00:02<00:02, 43.93it/s]01/06/2022 17:20:28 - INFO - __main__ -   Batch Number = 117
01/06/2022 17:20:28 - INFO - __main__ -   Batch Number = 118
01/06/2022 17:20:28 - INFO - __main__ -   Batch Number = 119
01/06/2022 17:20:28 - INFO - __main__ -   Batch Number = 120
01/06/2022 17:20:28 - INFO - __main__ -   Batch Number = 121
Evaluating:  59%|█████▉    | 121/204 [00:02<00:01, 43.97it/s]01/06/2022 17:20:28 - INFO - __main__ -   Batch Number = 122
01/06/2022 17:20:28 - INFO - __main__ -   Batch Number = 123
01/06/2022 17:20:28 - INFO - __main__ -   Batch Number = 124
01/06/2022 17:20:28 - INFO - __main__ -   Batch Number = 125
01/06/2022 17:20:28 - INFO - __main__ -   Batch Number = 126
Evaluating:  62%|██████▏   | 126/204 [00:02<00:01, 43.90it/s]01/06/2022 17:20:28 - INFO - __main__ -   Batch Number = 127
01/06/2022 17:20:28 - INFO - __main__ -   Batch Number = 128
01/06/2022 17:20:28 - INFO - __main__ -   Batch Number = 129
01/06/2022 17:20:28 - INFO - __main__ -   Batch Number = 130
01/06/2022 17:20:28 - INFO - __main__ -   Batch Number = 131
Evaluating:  64%|██████▍   | 131/204 [00:03<00:01, 44.03it/s]01/06/2022 17:20:28 - INFO - __main__ -   Batch Number = 132
01/06/2022 17:20:28 - INFO - __main__ -   Batch Number = 133
01/06/2022 17:20:28 - INFO - __main__ -   Batch Number = 134
01/06/2022 17:20:28 - INFO - __main__ -   Batch Number = 135
01/06/2022 17:20:28 - INFO - __main__ -   Batch Number = 136
Evaluating:  67%|██████▋   | 136/204 [00:03<00:01, 44.04it/s]01/06/2022 17:20:28 - INFO - __main__ -   Batch Number = 137
01/06/2022 17:20:28 - INFO - __main__ -   Batch Number = 138
01/06/2022 17:20:28 - INFO - __main__ -   Batch Number = 139
01/06/2022 17:20:28 - INFO - __main__ -   Batch Number = 140
01/06/2022 17:20:28 - INFO - __main__ -   Batch Number = 141
Evaluating:  69%|██████▉   | 141/204 [00:03<00:01, 44.00it/s]01/06/2022 17:20:28 - INFO - __main__ -   Batch Number = 142
01/06/2022 17:20:28 - INFO - __main__ -   Batch Number = 143
01/06/2022 17:20:28 - INFO - __main__ -   Batch Number = 144
01/06/2022 17:20:28 - INFO - __main__ -   Batch Number = 145
01/06/2022 17:20:28 - INFO - __main__ -   Batch Number = 146
Evaluating:  72%|███████▏  | 146/204 [00:03<00:01, 44.08it/s]01/06/2022 17:20:28 - INFO - __main__ -   Batch Number = 147
01/06/2022 17:20:28 - INFO - __main__ -   Batch Number = 148
01/06/2022 17:20:28 - INFO - __main__ -   Batch Number = 149
01/06/2022 17:20:28 - INFO - __main__ -   Batch Number = 150
01/06/2022 17:20:28 - INFO - __main__ -   Batch Number = 151
Evaluating:  74%|███████▍  | 151/204 [00:03<00:01, 44.16it/s]01/06/2022 17:20:28 - INFO - __main__ -   Batch Number = 152
01/06/2022 17:20:29 - INFO - __main__ -   Batch Number = 153
01/06/2022 17:20:29 - INFO - __main__ -   Batch Number = 154
01/06/2022 17:20:29 - INFO - __main__ -   Batch Number = 155
01/06/2022 17:20:29 - INFO - __main__ -   Batch Number = 156
Evaluating:  76%|███████▋  | 156/204 [00:03<00:01, 42.69it/s]01/06/2022 17:20:29 - INFO - __main__ -   Batch Number = 157
01/06/2022 17:20:29 - INFO - __main__ -   Batch Number = 158
01/06/2022 17:20:29 - INFO - __main__ -   Batch Number = 159
01/06/2022 17:20:29 - INFO - __main__ -   Batch Number = 160
01/06/2022 17:20:29 - INFO - __main__ -   Batch Number = 161
Evaluating:  79%|███████▉  | 161/204 [00:03<00:00, 43.12it/s]01/06/2022 17:20:29 - INFO - __main__ -   Batch Number = 162
01/06/2022 17:20:29 - INFO - __main__ -   Batch Number = 163
01/06/2022 17:20:29 - INFO - __main__ -   Batch Number = 164
01/06/2022 17:20:29 - INFO - __main__ -   Batch Number = 165
01/06/2022 17:20:29 - INFO - __main__ -   Batch Number = 166
Evaluating:  81%|████████▏ | 166/204 [00:03<00:00, 43.49it/s]01/06/2022 17:20:29 - INFO - __main__ -   Batch Number = 167
01/06/2022 17:20:29 - INFO - __main__ -   Batch Number = 168
01/06/2022 17:20:29 - INFO - __main__ -   Batch Number = 169
01/06/2022 17:20:29 - INFO - __main__ -   Batch Number = 170
01/06/2022 17:20:29 - INFO - __main__ -   Batch Number = 171
Evaluating:  84%|████████▍ | 171/204 [00:03<00:00, 43.60it/s]01/06/2022 17:20:29 - INFO - __main__ -   Batch Number = 172
01/06/2022 17:20:29 - INFO - __main__ -   Batch Number = 173
01/06/2022 17:20:29 - INFO - __main__ -   Batch Number = 174
01/06/2022 17:20:29 - INFO - __main__ -   Batch Number = 175
01/06/2022 17:20:29 - INFO - __main__ -   Batch Number = 176
Evaluating:  86%|████████▋ | 176/204 [00:04<00:00, 40.14it/s]01/06/2022 17:20:29 - INFO - __main__ -   Batch Number = 177
01/06/2022 17:20:29 - INFO - __main__ -   Batch Number = 178
01/06/2022 17:20:29 - INFO - __main__ -   Batch Number = 179
01/06/2022 17:20:29 - INFO - __main__ -   Batch Number = 180
01/06/2022 17:20:29 - INFO - __main__ -   Batch Number = 181
Evaluating:  89%|████████▊ | 181/204 [00:04<00:00, 41.17it/s]01/06/2022 17:20:29 - INFO - __main__ -   Batch Number = 182
01/06/2022 17:20:29 - INFO - __main__ -   Batch Number = 183
01/06/2022 17:20:29 - INFO - __main__ -   Batch Number = 184
01/06/2022 17:20:29 - INFO - __main__ -   Batch Number = 185
01/06/2022 17:20:29 - INFO - __main__ -   Batch Number = 186
Evaluating:  91%|█████████ | 186/204 [00:04<00:00, 42.02it/s]01/06/2022 17:20:29 - INFO - __main__ -   Batch Number = 187
01/06/2022 17:20:29 - INFO - __main__ -   Batch Number = 188
01/06/2022 17:20:29 - INFO - __main__ -   Batch Number = 189
01/06/2022 17:20:29 - INFO - __main__ -   Batch Number = 190
01/06/2022 17:20:29 - INFO - __main__ -   Batch Number = 191
Evaluating:  94%|█████████▎| 191/204 [00:04<00:00, 42.62it/s]01/06/2022 17:20:29 - INFO - __main__ -   Batch Number = 192
01/06/2022 17:20:29 - INFO - __main__ -   Batch Number = 193
01/06/2022 17:20:29 - INFO - __main__ -   Batch Number = 194
01/06/2022 17:20:29 - INFO - __main__ -   Batch Number = 195
01/06/2022 17:20:30 - INFO - __main__ -   Batch Number = 196
Evaluating:  96%|█████████▌| 196/204 [00:04<00:00, 43.11it/s]01/06/2022 17:20:30 - INFO - __main__ -   Batch Number = 197
01/06/2022 17:20:30 - INFO - __main__ -   Batch Number = 198
01/06/2022 17:20:30 - INFO - __main__ -   Batch Number = 199
01/06/2022 17:20:30 - INFO - __main__ -   Batch Number = 200
01/06/2022 17:20:30 - INFO - __main__ -   Batch Number = 201
Evaluating:  99%|█████████▊| 201/204 [00:04<00:00, 43.37it/s]01/06/2022 17:20:30 - INFO - __main__ -   Batch Number = 202
01/06/2022 17:20:30 - INFO - __main__ -   Batch Number = 203
01/06/2022 17:20:30 - INFO - __main__ -   Batch Number = 204
Evaluating: 100%|██████████| 204/204 [00:04<00:00, 43.01it/s]
01/06/2022 17:20:30 - INFO - __main__ -     Evaluation done in total 4.743132 secs (0.002912 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/predictions_hi_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/nbest_predictions_hi_.json
01/06/2022 17:20:33 - INFO - __main__ -   Results = OrderedDict([('exact', 33.445378151260506), ('f1', 47.82692550523264), ('total', 1190), ('HasAns_exact', 33.445378151260506), ('HasAns_f1', 47.82692550523264), ('HasAns_total', 1190), ('best_exact', 33.445378151260506), ('best_exact_thresh', 0.0), ('best_f1', 47.82692550523264), ('best_f1_thresh', 0.0)])
01/06/2022 17:20:33 - INFO - __main__ -   Language adapter for ru not found, using en instead
01/06/2022 17:20:33 - INFO - __main__ -   Set active language adapter to en
01/06/2022 17:20:33 - INFO - __main__ -   Args Adapter Weight = None
01/06/2022 17:20:33 - INFO - __main__ -   Adapter Languages = ['en']
01/06/2022 17:20:33 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
  0%|          | 0/48 [00:00<?, ?it/s] 31%|███▏      | 15/48 [00:00<00:00, 144.85it/s] 62%|██████▎   | 30/48 [00:00<00:00, 107.49it/s] 90%|████████▉ | 43/48 [00:00<00:00, 113.39it/s]100%|██████████| 48/48 [00:00<00:00, 116.91it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<05:13,  3.79it/s]convert squad examples to features:  22%|██▏       | 257/1190 [00:00<00:01, 895.82it/s]convert squad examples to features:  34%|███▍      | 406/1190 [00:01<00:02, 374.45it/s]convert squad examples to features:  92%|█████████▏| 1089/1190 [00:01<00:00, 1250.49it/s]convert squad examples to features: 100%|██████████| 1190/1190 [00:01<00:00, 918.91it/s] /root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|██████████| 1190/1190 [00:00<00:00, 433152.98it/s]
01/06/2022 17:20:36 - INFO - __main__ -   Local Rank = -1
01/06/2022 17:20:36 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.ru.json_bert-base-multilingual-cased_384_ru
01/06/2022 17:20:37 - INFO - __main__ -   ***** Running evaluation  *****
01/06/2022 17:20:37 - INFO - __main__ -     Num examples = 1410
01/06/2022 17:20:37 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/177 [00:00<?, ?it/s]01/06/2022 17:20:37 - INFO - __main__ -   Batch Number = 1
01/06/2022 17:20:37 - INFO - __main__ -   Batch Number = 2
01/06/2022 17:20:37 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|▏         | 3/177 [00:00<00:06, 25.41it/s]01/06/2022 17:20:37 - INFO - __main__ -   Batch Number = 4
01/06/2022 17:20:37 - INFO - __main__ -   Batch Number = 5
01/06/2022 17:20:37 - INFO - __main__ -   Batch Number = 6
01/06/2022 17:20:37 - INFO - __main__ -   Batch Number = 7
Evaluating:   4%|▍         | 7/177 [00:00<00:05, 31.15it/s]01/06/2022 17:20:37 - INFO - __main__ -   Batch Number = 8
01/06/2022 17:20:37 - INFO - __main__ -   Batch Number = 9
01/06/2022 17:20:37 - INFO - __main__ -   Batch Number = 10
01/06/2022 17:20:37 - INFO - __main__ -   Batch Number = 11
01/06/2022 17:20:37 - INFO - __main__ -   Batch Number = 12
Evaluating:   7%|▋         | 12/177 [00:00<00:04, 35.52it/s]01/06/2022 17:20:37 - INFO - __main__ -   Batch Number = 13
01/06/2022 17:20:37 - INFO - __main__ -   Batch Number = 14
01/06/2022 17:20:37 - INFO - __main__ -   Batch Number = 15
01/06/2022 17:20:37 - INFO - __main__ -   Batch Number = 16
01/06/2022 17:20:37 - INFO - __main__ -   Batch Number = 17
Evaluating:  10%|▉         | 17/177 [00:00<00:04, 38.76it/s]01/06/2022 17:20:37 - INFO - __main__ -   Batch Number = 18
01/06/2022 17:20:37 - INFO - __main__ -   Batch Number = 19
01/06/2022 17:20:37 - INFO - __main__ -   Batch Number = 20
01/06/2022 17:20:37 - INFO - __main__ -   Batch Number = 21
Evaluating:  12%|█▏        | 21/177 [00:00<00:04, 38.96it/s]01/06/2022 17:20:37 - INFO - __main__ -   Batch Number = 22
01/06/2022 17:20:37 - INFO - __main__ -   Batch Number = 23
01/06/2022 17:20:37 - INFO - __main__ -   Batch Number = 24
01/06/2022 17:20:37 - INFO - __main__ -   Batch Number = 25
01/06/2022 17:20:37 - INFO - __main__ -   Batch Number = 26
Evaluating:  15%|█▍        | 26/177 [00:00<00:03, 40.96it/s]01/06/2022 17:20:37 - INFO - __main__ -   Batch Number = 27
01/06/2022 17:20:37 - INFO - __main__ -   Batch Number = 28
01/06/2022 17:20:37 - INFO - __main__ -   Batch Number = 29
01/06/2022 17:20:37 - INFO - __main__ -   Batch Number = 30
01/06/2022 17:20:37 - INFO - __main__ -   Batch Number = 31
Evaluating:  18%|█▊        | 31/177 [00:00<00:03, 42.09it/s]01/06/2022 17:20:38 - INFO - __main__ -   Batch Number = 32
01/06/2022 17:20:38 - INFO - __main__ -   Batch Number = 33
01/06/2022 17:20:38 - INFO - __main__ -   Batch Number = 34
01/06/2022 17:20:38 - INFO - __main__ -   Batch Number = 35
01/06/2022 17:20:38 - INFO - __main__ -   Batch Number = 36
Evaluating:  20%|██        | 36/177 [00:00<00:03, 42.69it/s]01/06/2022 17:20:38 - INFO - __main__ -   Batch Number = 37
01/06/2022 17:20:38 - INFO - __main__ -   Batch Number = 38
01/06/2022 17:20:38 - INFO - __main__ -   Batch Number = 39
01/06/2022 17:20:38 - INFO - __main__ -   Batch Number = 40
01/06/2022 17:20:38 - INFO - __main__ -   Batch Number = 41
Evaluating:  23%|██▎       | 41/177 [00:01<00:03, 43.19it/s]01/06/2022 17:20:38 - INFO - __main__ -   Batch Number = 42
01/06/2022 17:20:38 - INFO - __main__ -   Batch Number = 43
01/06/2022 17:20:38 - INFO - __main__ -   Batch Number = 44
01/06/2022 17:20:38 - INFO - __main__ -   Batch Number = 45
01/06/2022 17:20:38 - INFO - __main__ -   Batch Number = 46
Evaluating:  26%|██▌       | 46/177 [00:01<00:03, 43.60it/s]01/06/2022 17:20:38 - INFO - __main__ -   Batch Number = 47
01/06/2022 17:20:38 - INFO - __main__ -   Batch Number = 48
01/06/2022 17:20:38 - INFO - __main__ -   Batch Number = 49
01/06/2022 17:20:38 - INFO - __main__ -   Batch Number = 50
01/06/2022 17:20:38 - INFO - __main__ -   Batch Number = 51
Evaluating:  29%|██▉       | 51/177 [00:01<00:02, 43.78it/s]01/06/2022 17:20:38 - INFO - __main__ -   Batch Number = 52
01/06/2022 17:20:38 - INFO - __main__ -   Batch Number = 53
01/06/2022 17:20:38 - INFO - __main__ -   Batch Number = 54
01/06/2022 17:20:38 - INFO - __main__ -   Batch Number = 55
01/06/2022 17:20:38 - INFO - __main__ -   Batch Number = 56
Evaluating:  32%|███▏      | 56/177 [00:01<00:02, 43.85it/s]01/06/2022 17:20:38 - INFO - __main__ -   Batch Number = 57
01/06/2022 17:20:38 - INFO - __main__ -   Batch Number = 58
01/06/2022 17:20:38 - INFO - __main__ -   Batch Number = 59
01/06/2022 17:20:38 - INFO - __main__ -   Batch Number = 60
01/06/2022 17:20:38 - INFO - __main__ -   Batch Number = 61
Evaluating:  34%|███▍      | 61/177 [00:01<00:02, 44.05it/s]01/06/2022 17:20:38 - INFO - __main__ -   Batch Number = 62
01/06/2022 17:20:38 - INFO - __main__ -   Batch Number = 63
01/06/2022 17:20:38 - INFO - __main__ -   Batch Number = 64
01/06/2022 17:20:38 - INFO - __main__ -   Batch Number = 65
01/06/2022 17:20:38 - INFO - __main__ -   Batch Number = 66
Evaluating:  37%|███▋      | 66/177 [00:01<00:02, 43.92it/s]01/06/2022 17:20:38 - INFO - __main__ -   Batch Number = 67
01/06/2022 17:20:38 - INFO - __main__ -   Batch Number = 68
01/06/2022 17:20:38 - INFO - __main__ -   Batch Number = 69
01/06/2022 17:20:38 - INFO - __main__ -   Batch Number = 70
01/06/2022 17:20:38 - INFO - __main__ -   Batch Number = 71
Evaluating:  40%|████      | 71/177 [00:01<00:02, 44.02it/s]01/06/2022 17:20:38 - INFO - __main__ -   Batch Number = 72
01/06/2022 17:20:38 - INFO - __main__ -   Batch Number = 73
01/06/2022 17:20:38 - INFO - __main__ -   Batch Number = 74
01/06/2022 17:20:38 - INFO - __main__ -   Batch Number = 75
01/06/2022 17:20:38 - INFO - __main__ -   Batch Number = 76
Evaluating:  43%|████▎     | 76/177 [00:01<00:02, 44.16it/s]01/06/2022 17:20:39 - INFO - __main__ -   Batch Number = 77
01/06/2022 17:20:39 - INFO - __main__ -   Batch Number = 78
01/06/2022 17:20:39 - INFO - __main__ -   Batch Number = 79
01/06/2022 17:20:39 - INFO - __main__ -   Batch Number = 80
01/06/2022 17:20:39 - INFO - __main__ -   Batch Number = 81
Evaluating:  46%|████▌     | 81/177 [00:01<00:02, 43.96it/s]01/06/2022 17:20:39 - INFO - __main__ -   Batch Number = 82
01/06/2022 17:20:39 - INFO - __main__ -   Batch Number = 83
01/06/2022 17:20:39 - INFO - __main__ -   Batch Number = 84
01/06/2022 17:20:39 - INFO - __main__ -   Batch Number = 85
01/06/2022 17:20:39 - INFO - __main__ -   Batch Number = 86
Evaluating:  49%|████▊     | 86/177 [00:02<00:02, 44.04it/s]01/06/2022 17:20:39 - INFO - __main__ -   Batch Number = 87
01/06/2022 17:20:39 - INFO - __main__ -   Batch Number = 88
01/06/2022 17:20:39 - INFO - __main__ -   Batch Number = 89
01/06/2022 17:20:39 - INFO - __main__ -   Batch Number = 90
01/06/2022 17:20:39 - INFO - __main__ -   Batch Number = 91
Evaluating:  51%|█████▏    | 91/177 [00:02<00:01, 44.06it/s]01/06/2022 17:20:39 - INFO - __main__ -   Batch Number = 92
01/06/2022 17:20:39 - INFO - __main__ -   Batch Number = 93
01/06/2022 17:20:39 - INFO - __main__ -   Batch Number = 94
01/06/2022 17:20:39 - INFO - __main__ -   Batch Number = 95
01/06/2022 17:20:39 - INFO - __main__ -   Batch Number = 96
Evaluating:  54%|█████▍    | 96/177 [00:02<00:01, 43.90it/s]01/06/2022 17:20:39 - INFO - __main__ -   Batch Number = 97
01/06/2022 17:20:39 - INFO - __main__ -   Batch Number = 98
01/06/2022 17:20:39 - INFO - __main__ -   Batch Number = 99
01/06/2022 17:20:39 - INFO - __main__ -   Batch Number = 100
01/06/2022 17:20:39 - INFO - __main__ -   Batch Number = 101
Evaluating:  57%|█████▋    | 101/177 [00:02<00:01, 44.04it/s]01/06/2022 17:20:39 - INFO - __main__ -   Batch Number = 102
01/06/2022 17:20:39 - INFO - __main__ -   Batch Number = 103
01/06/2022 17:20:39 - INFO - __main__ -   Batch Number = 104
01/06/2022 17:20:39 - INFO - __main__ -   Batch Number = 105
01/06/2022 17:20:39 - INFO - __main__ -   Batch Number = 106
Evaluating:  60%|█████▉    | 106/177 [00:02<00:01, 44.02it/s]01/06/2022 17:20:39 - INFO - __main__ -   Batch Number = 107
01/06/2022 17:20:39 - INFO - __main__ -   Batch Number = 108
01/06/2022 17:20:39 - INFO - __main__ -   Batch Number = 109
01/06/2022 17:20:39 - INFO - __main__ -   Batch Number = 110
01/06/2022 17:20:39 - INFO - __main__ -   Batch Number = 111
Evaluating:  63%|██████▎   | 111/177 [00:02<00:01, 43.27it/s]01/06/2022 17:20:39 - INFO - __main__ -   Batch Number = 112
01/06/2022 17:20:39 - INFO - __main__ -   Batch Number = 113
01/06/2022 17:20:39 - INFO - __main__ -   Batch Number = 114
01/06/2022 17:20:39 - INFO - __main__ -   Batch Number = 115
01/06/2022 17:20:39 - INFO - __main__ -   Batch Number = 116
Evaluating:  66%|██████▌   | 116/177 [00:02<00:01, 43.52it/s]01/06/2022 17:20:39 - INFO - __main__ -   Batch Number = 117
01/06/2022 17:20:39 - INFO - __main__ -   Batch Number = 118
01/06/2022 17:20:39 - INFO - __main__ -   Batch Number = 119
01/06/2022 17:20:40 - INFO - __main__ -   Batch Number = 120
01/06/2022 17:20:40 - INFO - __main__ -   Batch Number = 121
Evaluating:  68%|██████▊   | 121/177 [00:02<00:01, 43.60it/s]01/06/2022 17:20:40 - INFO - __main__ -   Batch Number = 122
01/06/2022 17:20:40 - INFO - __main__ -   Batch Number = 123
01/06/2022 17:20:40 - INFO - __main__ -   Batch Number = 124
01/06/2022 17:20:40 - INFO - __main__ -   Batch Number = 125
01/06/2022 17:20:40 - INFO - __main__ -   Batch Number = 126
Evaluating:  71%|███████   | 126/177 [00:02<00:01, 43.64it/s]01/06/2022 17:20:40 - INFO - __main__ -   Batch Number = 127
01/06/2022 17:20:40 - INFO - __main__ -   Batch Number = 128
01/06/2022 17:20:40 - INFO - __main__ -   Batch Number = 129
01/06/2022 17:20:40 - INFO - __main__ -   Batch Number = 130
01/06/2022 17:20:40 - INFO - __main__ -   Batch Number = 131
Evaluating:  74%|███████▍  | 131/177 [00:03<00:01, 43.78it/s]01/06/2022 17:20:40 - INFO - __main__ -   Batch Number = 132
01/06/2022 17:20:40 - INFO - __main__ -   Batch Number = 133
01/06/2022 17:20:40 - INFO - __main__ -   Batch Number = 134
01/06/2022 17:20:40 - INFO - __main__ -   Batch Number = 135
01/06/2022 17:20:40 - INFO - __main__ -   Batch Number = 136
Evaluating:  77%|███████▋  | 136/177 [00:03<00:00, 43.99it/s]01/06/2022 17:20:40 - INFO - __main__ -   Batch Number = 137
01/06/2022 17:20:40 - INFO - __main__ -   Batch Number = 138
01/06/2022 17:20:40 - INFO - __main__ -   Batch Number = 139
01/06/2022 17:20:40 - INFO - __main__ -   Batch Number = 140
01/06/2022 17:20:40 - INFO - __main__ -   Batch Number = 141
Evaluating:  80%|███████▉  | 141/177 [00:03<00:00, 43.94it/s]01/06/2022 17:20:40 - INFO - __main__ -   Batch Number = 142
01/06/2022 17:20:40 - INFO - __main__ -   Batch Number = 143
01/06/2022 17:20:40 - INFO - __main__ -   Batch Number = 144
01/06/2022 17:20:40 - INFO - __main__ -   Batch Number = 145
01/06/2022 17:20:40 - INFO - __main__ -   Batch Number = 146
Evaluating:  82%|████████▏ | 146/177 [00:03<00:00, 44.04it/s]01/06/2022 17:20:40 - INFO - __main__ -   Batch Number = 147
01/06/2022 17:20:40 - INFO - __main__ -   Batch Number = 148
01/06/2022 17:20:40 - INFO - __main__ -   Batch Number = 149
01/06/2022 17:20:40 - INFO - __main__ -   Batch Number = 150
01/06/2022 17:20:40 - INFO - __main__ -   Batch Number = 151
Evaluating:  85%|████████▌ | 151/177 [00:03<00:00, 44.08it/s]01/06/2022 17:20:40 - INFO - __main__ -   Batch Number = 152
01/06/2022 17:20:40 - INFO - __main__ -   Batch Number = 153
01/06/2022 17:20:40 - INFO - __main__ -   Batch Number = 154
01/06/2022 17:20:40 - INFO - __main__ -   Batch Number = 155
01/06/2022 17:20:40 - INFO - __main__ -   Batch Number = 156
Evaluating:  88%|████████▊ | 156/177 [00:03<00:00, 44.07it/s]01/06/2022 17:20:40 - INFO - __main__ -   Batch Number = 157
01/06/2022 17:20:40 - INFO - __main__ -   Batch Number = 158
01/06/2022 17:20:40 - INFO - __main__ -   Batch Number = 159
01/06/2022 17:20:40 - INFO - __main__ -   Batch Number = 160
01/06/2022 17:20:40 - INFO - __main__ -   Batch Number = 161
Evaluating:  91%|█████████ | 161/177 [00:03<00:00, 44.08it/s]01/06/2022 17:20:40 - INFO - __main__ -   Batch Number = 162
01/06/2022 17:20:40 - INFO - __main__ -   Batch Number = 163
01/06/2022 17:20:41 - INFO - __main__ -   Batch Number = 164
01/06/2022 17:20:41 - INFO - __main__ -   Batch Number = 165
01/06/2022 17:20:41 - INFO - __main__ -   Batch Number = 166
Evaluating:  94%|█████████▍| 166/177 [00:03<00:00, 44.12it/s]01/06/2022 17:20:41 - INFO - __main__ -   Batch Number = 167
01/06/2022 17:20:41 - INFO - __main__ -   Batch Number = 168
01/06/2022 17:20:41 - INFO - __main__ -   Batch Number = 169
01/06/2022 17:20:41 - INFO - __main__ -   Batch Number = 170
01/06/2022 17:20:41 - INFO - __main__ -   Batch Number = 171
Evaluating:  97%|█████████▋| 171/177 [00:03<00:00, 44.05it/s]01/06/2022 17:20:41 - INFO - __main__ -   Batch Number = 172
01/06/2022 17:20:41 - INFO - __main__ -   Batch Number = 173
01/06/2022 17:20:41 - INFO - __main__ -   Batch Number = 174
01/06/2022 17:20:41 - INFO - __main__ -   Batch Number = 175
01/06/2022 17:20:41 - INFO - __main__ -   Batch Number = 176
Evaluating:  99%|█████████▉| 176/177 [00:04<00:00, 44.16it/s]01/06/2022 17:20:41 - INFO - __main__ -   Batch Number = 177
Evaluating: 100%|██████████| 177/177 [00:04<00:00, 43.11it/s]
01/06/2022 17:20:41 - INFO - __main__ -     Evaluation done in total 4.107131 secs (0.002913 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/predictions_ru_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/nbest_predictions_ru_.json
01/06/2022 17:20:44 - INFO - __main__ -   Results = OrderedDict([('exact', 49.747899159663866), ('f1', 67.25468242407638), ('total', 1190), ('HasAns_exact', 49.747899159663866), ('HasAns_f1', 67.25468242407638), ('HasAns_total', 1190), ('best_exact', 49.747899159663866), ('best_exact_thresh', 0.0), ('best_f1', 67.25468242407638), ('best_f1_thresh', 0.0)])
01/06/2022 17:20:44 - INFO - __main__ -   Language adapter for th not found, using en instead
01/06/2022 17:20:44 - INFO - __main__ -   Set active language adapter to en
01/06/2022 17:20:44 - INFO - __main__ -   Args Adapter Weight = None
01/06/2022 17:20:44 - INFO - __main__ -   Adapter Languages = ['en']
01/06/2022 17:20:44 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
  0%|          | 0/48 [00:00<?, ?it/s] 33%|███▎      | 16/48 [00:00<00:00, 127.79it/s] 60%|██████    | 29/48 [00:00<00:00, 118.66it/s] 92%|█████████▏| 44/48 [00:00<00:00, 130.03it/s]100%|██████████| 48/48 [00:00<00:00, 129.06it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<13:59,  1.42it/s]convert squad examples to features:  27%|██▋       | 321/1190 [00:00<00:01, 450.55it/s]convert squad examples to features:  34%|███▍      | 408/1190 [00:02<00:04, 165.05it/s]convert squad examples to features:  94%|█████████▍| 1121/1190 [00:02<00:00, 590.88it/s]convert squad examples to features: 100%|██████████| 1190/1190 [00:02<00:00, 449.30it/s]/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|██████████| 1190/1190 [00:00<00:00, 477537.48it/s]
01/06/2022 17:20:48 - INFO - __main__ -   Local Rank = -1
01/06/2022 17:20:48 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.th.json_bert-base-multilingual-cased_384_th
01/06/2022 17:20:51 - INFO - __main__ -   ***** Running evaluation  *****
01/06/2022 17:20:51 - INFO - __main__ -     Num examples = 3123
01/06/2022 17:20:51 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/391 [00:00<?, ?it/s]01/06/2022 17:20:51 - INFO - __main__ -   Batch Number = 1
01/06/2022 17:20:51 - INFO - __main__ -   Batch Number = 2
01/06/2022 17:20:51 - INFO - __main__ -   Batch Number = 3
Evaluating:   1%|          | 3/391 [00:00<00:14, 26.43it/s]01/06/2022 17:20:51 - INFO - __main__ -   Batch Number = 4
01/06/2022 17:20:51 - INFO - __main__ -   Batch Number = 5
01/06/2022 17:20:51 - INFO - __main__ -   Batch Number = 6
Evaluating:   2%|▏         | 6/391 [00:00<00:13, 27.72it/s]01/06/2022 17:20:51 - INFO - __main__ -   Batch Number = 7
01/06/2022 17:20:51 - INFO - __main__ -   Batch Number = 8
01/06/2022 17:20:51 - INFO - __main__ -   Batch Number = 9
01/06/2022 17:20:51 - INFO - __main__ -   Batch Number = 10
Evaluating:   3%|▎         | 10/391 [00:00<00:11, 32.60it/s]01/06/2022 17:20:51 - INFO - __main__ -   Batch Number = 11
01/06/2022 17:20:51 - INFO - __main__ -   Batch Number = 12
01/06/2022 17:20:51 - INFO - __main__ -   Batch Number = 13
01/06/2022 17:20:51 - INFO - __main__ -   Batch Number = 14
01/06/2022 17:20:51 - INFO - __main__ -   Batch Number = 15
Evaluating:   4%|▍         | 15/391 [00:00<00:10, 37.36it/s]01/06/2022 17:20:51 - INFO - __main__ -   Batch Number = 16
01/06/2022 17:20:51 - INFO - __main__ -   Batch Number = 17
01/06/2022 17:20:51 - INFO - __main__ -   Batch Number = 18
01/06/2022 17:20:51 - INFO - __main__ -   Batch Number = 19
01/06/2022 17:20:51 - INFO - __main__ -   Batch Number = 20
Evaluating:   5%|▌         | 20/391 [00:00<00:09, 39.79it/s]01/06/2022 17:20:51 - INFO - __main__ -   Batch Number = 21
01/06/2022 17:20:51 - INFO - __main__ -   Batch Number = 22
01/06/2022 17:20:51 - INFO - __main__ -   Batch Number = 23
01/06/2022 17:20:52 - INFO - __main__ -   Batch Number = 24
01/06/2022 17:20:52 - INFO - __main__ -   Batch Number = 25
Evaluating:   6%|▋         | 25/391 [00:00<00:08, 41.52it/s]01/06/2022 17:20:52 - INFO - __main__ -   Batch Number = 26
01/06/2022 17:20:52 - INFO - __main__ -   Batch Number = 27
01/06/2022 17:20:52 - INFO - __main__ -   Batch Number = 28
01/06/2022 17:20:52 - INFO - __main__ -   Batch Number = 29
01/06/2022 17:20:52 - INFO - __main__ -   Batch Number = 30
Evaluating:   8%|▊         | 30/391 [00:00<00:08, 42.51it/s]01/06/2022 17:20:52 - INFO - __main__ -   Batch Number = 31
01/06/2022 17:20:52 - INFO - __main__ -   Batch Number = 32
01/06/2022 17:20:52 - INFO - __main__ -   Batch Number = 33
01/06/2022 17:20:52 - INFO - __main__ -   Batch Number = 34
01/06/2022 17:20:52 - INFO - __main__ -   Batch Number = 35
Evaluating:   9%|▉         | 35/391 [00:00<00:08, 43.17it/s]01/06/2022 17:20:52 - INFO - __main__ -   Batch Number = 36
01/06/2022 17:20:52 - INFO - __main__ -   Batch Number = 37
01/06/2022 17:20:52 - INFO - __main__ -   Batch Number = 38
01/06/2022 17:20:52 - INFO - __main__ -   Batch Number = 39
01/06/2022 17:20:52 - INFO - __main__ -   Batch Number = 40
Evaluating:  10%|█         | 40/391 [00:00<00:08, 43.62it/s]01/06/2022 17:20:52 - INFO - __main__ -   Batch Number = 41
01/06/2022 17:20:52 - INFO - __main__ -   Batch Number = 42
01/06/2022 17:20:52 - INFO - __main__ -   Batch Number = 43
01/06/2022 17:20:52 - INFO - __main__ -   Batch Number = 44
01/06/2022 17:20:52 - INFO - __main__ -   Batch Number = 45
Evaluating:  12%|█▏        | 45/391 [00:01<00:07, 43.96it/s]01/06/2022 17:20:52 - INFO - __main__ -   Batch Number = 46
01/06/2022 17:20:52 - INFO - __main__ -   Batch Number = 47
01/06/2022 17:20:52 - INFO - __main__ -   Batch Number = 48
01/06/2022 17:20:52 - INFO - __main__ -   Batch Number = 49
01/06/2022 17:20:52 - INFO - __main__ -   Batch Number = 50
Evaluating:  13%|█▎        | 50/391 [00:01<00:07, 44.06it/s]01/06/2022 17:20:52 - INFO - __main__ -   Batch Number = 51
01/06/2022 17:20:52 - INFO - __main__ -   Batch Number = 52
01/06/2022 17:20:52 - INFO - __main__ -   Batch Number = 53
01/06/2022 17:20:52 - INFO - __main__ -   Batch Number = 54
01/06/2022 17:20:52 - INFO - __main__ -   Batch Number = 55
Evaluating:  14%|█▍        | 55/391 [00:01<00:07, 44.20it/s]01/06/2022 17:20:52 - INFO - __main__ -   Batch Number = 56
01/06/2022 17:20:52 - INFO - __main__ -   Batch Number = 57
01/06/2022 17:20:52 - INFO - __main__ -   Batch Number = 58
01/06/2022 17:20:52 - INFO - __main__ -   Batch Number = 59
01/06/2022 17:20:52 - INFO - __main__ -   Batch Number = 60
Evaluating:  15%|█▌        | 60/391 [00:01<00:07, 44.22it/s]01/06/2022 17:20:52 - INFO - __main__ -   Batch Number = 61
01/06/2022 17:20:52 - INFO - __main__ -   Batch Number = 62
01/06/2022 17:20:52 - INFO - __main__ -   Batch Number = 63
01/06/2022 17:20:52 - INFO - __main__ -   Batch Number = 64
01/06/2022 17:20:52 - INFO - __main__ -   Batch Number = 65
Evaluating:  17%|█▋        | 65/391 [00:01<00:07, 44.21it/s]01/06/2022 17:20:52 - INFO - __main__ -   Batch Number = 66
01/06/2022 17:20:52 - INFO - __main__ -   Batch Number = 67
01/06/2022 17:20:53 - INFO - __main__ -   Batch Number = 68
01/06/2022 17:20:53 - INFO - __main__ -   Batch Number = 69
01/06/2022 17:20:53 - INFO - __main__ -   Batch Number = 70
Evaluating:  18%|█▊        | 70/391 [00:01<00:07, 44.12it/s]01/06/2022 17:20:53 - INFO - __main__ -   Batch Number = 71
01/06/2022 17:20:53 - INFO - __main__ -   Batch Number = 72
01/06/2022 17:20:53 - INFO - __main__ -   Batch Number = 73
01/06/2022 17:20:53 - INFO - __main__ -   Batch Number = 74
01/06/2022 17:20:53 - INFO - __main__ -   Batch Number = 75
Evaluating:  19%|█▉        | 75/391 [00:01<00:07, 43.99it/s]01/06/2022 17:20:53 - INFO - __main__ -   Batch Number = 76
01/06/2022 17:20:53 - INFO - __main__ -   Batch Number = 77
01/06/2022 17:20:53 - INFO - __main__ -   Batch Number = 78
01/06/2022 17:20:53 - INFO - __main__ -   Batch Number = 79
01/06/2022 17:20:53 - INFO - __main__ -   Batch Number = 80
Evaluating:  20%|██        | 80/391 [00:01<00:07, 43.92it/s]01/06/2022 17:20:53 - INFO - __main__ -   Batch Number = 81
01/06/2022 17:20:53 - INFO - __main__ -   Batch Number = 82
01/06/2022 17:20:53 - INFO - __main__ -   Batch Number = 83
01/06/2022 17:20:53 - INFO - __main__ -   Batch Number = 84
01/06/2022 17:20:53 - INFO - __main__ -   Batch Number = 85
Evaluating:  22%|██▏       | 85/391 [00:02<00:06, 44.04it/s]01/06/2022 17:20:53 - INFO - __main__ -   Batch Number = 86
01/06/2022 17:20:53 - INFO - __main__ -   Batch Number = 87
01/06/2022 17:20:53 - INFO - __main__ -   Batch Number = 88
01/06/2022 17:20:53 - INFO - __main__ -   Batch Number = 89
01/06/2022 17:20:53 - INFO - __main__ -   Batch Number = 90
Evaluating:  23%|██▎       | 90/391 [00:02<00:06, 44.17it/s]01/06/2022 17:20:53 - INFO - __main__ -   Batch Number = 91
01/06/2022 17:20:53 - INFO - __main__ -   Batch Number = 92
01/06/2022 17:20:53 - INFO - __main__ -   Batch Number = 93
01/06/2022 17:20:53 - INFO - __main__ -   Batch Number = 94
01/06/2022 17:20:53 - INFO - __main__ -   Batch Number = 95
Evaluating:  24%|██▍       | 95/391 [00:02<00:06, 42.59it/s]01/06/2022 17:20:53 - INFO - __main__ -   Batch Number = 96
01/06/2022 17:20:53 - INFO - __main__ -   Batch Number = 97
01/06/2022 17:20:53 - INFO - __main__ -   Batch Number = 98
01/06/2022 17:20:53 - INFO - __main__ -   Batch Number = 99
01/06/2022 17:20:53 - INFO - __main__ -   Batch Number = 100
Evaluating:  26%|██▌       | 100/391 [00:02<00:06, 43.24it/s]01/06/2022 17:20:53 - INFO - __main__ -   Batch Number = 101
01/06/2022 17:20:53 - INFO - __main__ -   Batch Number = 102
01/06/2022 17:20:53 - INFO - __main__ -   Batch Number = 103
01/06/2022 17:20:53 - INFO - __main__ -   Batch Number = 104
01/06/2022 17:20:53 - INFO - __main__ -   Batch Number = 105
Evaluating:  27%|██▋       | 105/391 [00:02<00:06, 43.62it/s]01/06/2022 17:20:53 - INFO - __main__ -   Batch Number = 106
01/06/2022 17:20:53 - INFO - __main__ -   Batch Number = 107
01/06/2022 17:20:53 - INFO - __main__ -   Batch Number = 108
01/06/2022 17:20:53 - INFO - __main__ -   Batch Number = 109
01/06/2022 17:20:53 - INFO - __main__ -   Batch Number = 110
Evaluating:  28%|██▊       | 110/391 [00:02<00:06, 43.79it/s]01/06/2022 17:20:53 - INFO - __main__ -   Batch Number = 111
01/06/2022 17:20:54 - INFO - __main__ -   Batch Number = 112
01/06/2022 17:20:54 - INFO - __main__ -   Batch Number = 113
01/06/2022 17:20:54 - INFO - __main__ -   Batch Number = 114
01/06/2022 17:20:54 - INFO - __main__ -   Batch Number = 115
Evaluating:  29%|██▉       | 115/391 [00:02<00:06, 44.03it/s]01/06/2022 17:20:54 - INFO - __main__ -   Batch Number = 116
01/06/2022 17:20:54 - INFO - __main__ -   Batch Number = 117
01/06/2022 17:20:54 - INFO - __main__ -   Batch Number = 118
01/06/2022 17:20:54 - INFO - __main__ -   Batch Number = 119
01/06/2022 17:20:54 - INFO - __main__ -   Batch Number = 120
Evaluating:  31%|███       | 120/391 [00:02<00:06, 44.18it/s]01/06/2022 17:20:54 - INFO - __main__ -   Batch Number = 121
01/06/2022 17:20:54 - INFO - __main__ -   Batch Number = 122
01/06/2022 17:20:54 - INFO - __main__ -   Batch Number = 123
01/06/2022 17:20:54 - INFO - __main__ -   Batch Number = 124
01/06/2022 17:20:54 - INFO - __main__ -   Batch Number = 125
Evaluating:  32%|███▏      | 125/391 [00:02<00:06, 44.23it/s]01/06/2022 17:20:54 - INFO - __main__ -   Batch Number = 126
01/06/2022 17:20:54 - INFO - __main__ -   Batch Number = 127
01/06/2022 17:20:54 - INFO - __main__ -   Batch Number = 128
01/06/2022 17:20:54 - INFO - __main__ -   Batch Number = 129
01/06/2022 17:20:54 - INFO - __main__ -   Batch Number = 130
Evaluating:  33%|███▎      | 130/391 [00:03<00:05, 44.27it/s]01/06/2022 17:20:54 - INFO - __main__ -   Batch Number = 131
01/06/2022 17:20:54 - INFO - __main__ -   Batch Number = 132
01/06/2022 17:20:54 - INFO - __main__ -   Batch Number = 133
01/06/2022 17:20:54 - INFO - __main__ -   Batch Number = 134
01/06/2022 17:20:54 - INFO - __main__ -   Batch Number = 135
Evaluating:  35%|███▍      | 135/391 [00:03<00:05, 44.37it/s]01/06/2022 17:20:54 - INFO - __main__ -   Batch Number = 136
01/06/2022 17:20:54 - INFO - __main__ -   Batch Number = 137
01/06/2022 17:20:54 - INFO - __main__ -   Batch Number = 138
01/06/2022 17:20:54 - INFO - __main__ -   Batch Number = 139
01/06/2022 17:20:54 - INFO - __main__ -   Batch Number = 140
Evaluating:  36%|███▌      | 140/391 [00:03<00:05, 44.18it/s]01/06/2022 17:20:54 - INFO - __main__ -   Batch Number = 141
01/06/2022 17:20:54 - INFO - __main__ -   Batch Number = 142
01/06/2022 17:20:54 - INFO - __main__ -   Batch Number = 143
01/06/2022 17:20:54 - INFO - __main__ -   Batch Number = 144
01/06/2022 17:20:54 - INFO - __main__ -   Batch Number = 145
Evaluating:  37%|███▋      | 145/391 [00:03<00:05, 44.30it/s]01/06/2022 17:20:54 - INFO - __main__ -   Batch Number = 146
01/06/2022 17:20:54 - INFO - __main__ -   Batch Number = 147
01/06/2022 17:20:54 - INFO - __main__ -   Batch Number = 148
01/06/2022 17:20:54 - INFO - __main__ -   Batch Number = 149
01/06/2022 17:20:54 - INFO - __main__ -   Batch Number = 150
Evaluating:  38%|███▊      | 150/391 [00:03<00:05, 44.36it/s]01/06/2022 17:20:54 - INFO - __main__ -   Batch Number = 151
01/06/2022 17:20:54 - INFO - __main__ -   Batch Number = 152
01/06/2022 17:20:54 - INFO - __main__ -   Batch Number = 153
01/06/2022 17:20:54 - INFO - __main__ -   Batch Number = 154
01/06/2022 17:20:54 - INFO - __main__ -   Batch Number = 155
Evaluating:  40%|███▉      | 155/391 [00:03<00:05, 44.27it/s]01/06/2022 17:20:55 - INFO - __main__ -   Batch Number = 156
01/06/2022 17:20:55 - INFO - __main__ -   Batch Number = 157
01/06/2022 17:20:55 - INFO - __main__ -   Batch Number = 158
01/06/2022 17:20:55 - INFO - __main__ -   Batch Number = 159
01/06/2022 17:20:55 - INFO - __main__ -   Batch Number = 160
Evaluating:  41%|████      | 160/391 [00:03<00:05, 44.40it/s]01/06/2022 17:20:55 - INFO - __main__ -   Batch Number = 161
01/06/2022 17:20:55 - INFO - __main__ -   Batch Number = 162
01/06/2022 17:20:55 - INFO - __main__ -   Batch Number = 163
01/06/2022 17:20:55 - INFO - __main__ -   Batch Number = 164
01/06/2022 17:20:55 - INFO - __main__ -   Batch Number = 165
Evaluating:  42%|████▏     | 165/391 [00:03<00:05, 44.34it/s]01/06/2022 17:20:55 - INFO - __main__ -   Batch Number = 166
01/06/2022 17:20:55 - INFO - __main__ -   Batch Number = 167
01/06/2022 17:20:55 - INFO - __main__ -   Batch Number = 168
01/06/2022 17:20:55 - INFO - __main__ -   Batch Number = 169
01/06/2022 17:20:55 - INFO - __main__ -   Batch Number = 170
Evaluating:  43%|████▎     | 170/391 [00:03<00:04, 44.29it/s]01/06/2022 17:20:55 - INFO - __main__ -   Batch Number = 171
01/06/2022 17:20:55 - INFO - __main__ -   Batch Number = 172
01/06/2022 17:20:55 - INFO - __main__ -   Batch Number = 173
01/06/2022 17:20:55 - INFO - __main__ -   Batch Number = 174
01/06/2022 17:20:55 - INFO - __main__ -   Batch Number = 175
Evaluating:  45%|████▍     | 175/391 [00:04<00:04, 44.25it/s]01/06/2022 17:20:55 - INFO - __main__ -   Batch Number = 176
01/06/2022 17:20:55 - INFO - __main__ -   Batch Number = 177
01/06/2022 17:20:55 - INFO - __main__ -   Batch Number = 178
01/06/2022 17:20:55 - INFO - __main__ -   Batch Number = 179
01/06/2022 17:20:55 - INFO - __main__ -   Batch Number = 180
Evaluating:  46%|████▌     | 180/391 [00:04<00:04, 44.36it/s]01/06/2022 17:20:55 - INFO - __main__ -   Batch Number = 181
01/06/2022 17:20:55 - INFO - __main__ -   Batch Number = 182
01/06/2022 17:20:55 - INFO - __main__ -   Batch Number = 183
01/06/2022 17:20:55 - INFO - __main__ -   Batch Number = 184
01/06/2022 17:20:55 - INFO - __main__ -   Batch Number = 185
Evaluating:  47%|████▋     | 185/391 [00:04<00:04, 44.33it/s]01/06/2022 17:20:55 - INFO - __main__ -   Batch Number = 186
01/06/2022 17:20:55 - INFO - __main__ -   Batch Number = 187
01/06/2022 17:20:55 - INFO - __main__ -   Batch Number = 188
01/06/2022 17:20:55 - INFO - __main__ -   Batch Number = 189
01/06/2022 17:20:55 - INFO - __main__ -   Batch Number = 190
Evaluating:  49%|████▊     | 190/391 [00:04<00:04, 44.32it/s]01/06/2022 17:20:55 - INFO - __main__ -   Batch Number = 191
01/06/2022 17:20:55 - INFO - __main__ -   Batch Number = 192
01/06/2022 17:20:55 - INFO - __main__ -   Batch Number = 193
01/06/2022 17:20:55 - INFO - __main__ -   Batch Number = 194
01/06/2022 17:20:55 - INFO - __main__ -   Batch Number = 195
Evaluating:  50%|████▉     | 195/391 [00:04<00:04, 44.44it/s]01/06/2022 17:20:55 - INFO - __main__ -   Batch Number = 196
01/06/2022 17:20:55 - INFO - __main__ -   Batch Number = 197
01/06/2022 17:20:55 - INFO - __main__ -   Batch Number = 198
01/06/2022 17:20:55 - INFO - __main__ -   Batch Number = 199
01/06/2022 17:20:56 - INFO - __main__ -   Batch Number = 200
Evaluating:  51%|█████     | 200/391 [00:04<00:04, 44.29it/s]01/06/2022 17:20:56 - INFO - __main__ -   Batch Number = 201
01/06/2022 17:20:56 - INFO - __main__ -   Batch Number = 202
01/06/2022 17:20:56 - INFO - __main__ -   Batch Number = 203
01/06/2022 17:20:56 - INFO - __main__ -   Batch Number = 204
01/06/2022 17:20:56 - INFO - __main__ -   Batch Number = 205
Evaluating:  52%|█████▏    | 205/391 [00:04<00:04, 44.42it/s]01/06/2022 17:20:56 - INFO - __main__ -   Batch Number = 206
01/06/2022 17:20:56 - INFO - __main__ -   Batch Number = 207
01/06/2022 17:20:56 - INFO - __main__ -   Batch Number = 208
01/06/2022 17:20:56 - INFO - __main__ -   Batch Number = 209
01/06/2022 17:20:56 - INFO - __main__ -   Batch Number = 210
Evaluating:  54%|█████▎    | 210/391 [00:04<00:04, 44.36it/s]01/06/2022 17:20:56 - INFO - __main__ -   Batch Number = 211
01/06/2022 17:20:56 - INFO - __main__ -   Batch Number = 212
01/06/2022 17:20:56 - INFO - __main__ -   Batch Number = 213
01/06/2022 17:20:56 - INFO - __main__ -   Batch Number = 214
01/06/2022 17:20:56 - INFO - __main__ -   Batch Number = 215
Evaluating:  55%|█████▍    | 215/391 [00:04<00:03, 44.26it/s]01/06/2022 17:20:56 - INFO - __main__ -   Batch Number = 216
01/06/2022 17:20:56 - INFO - __main__ -   Batch Number = 217
01/06/2022 17:20:56 - INFO - __main__ -   Batch Number = 218
01/06/2022 17:20:56 - INFO - __main__ -   Batch Number = 219
01/06/2022 17:20:56 - INFO - __main__ -   Batch Number = 220
Evaluating:  56%|█████▋    | 220/391 [00:05<00:03, 44.30it/s]01/06/2022 17:20:56 - INFO - __main__ -   Batch Number = 221
01/06/2022 17:20:56 - INFO - __main__ -   Batch Number = 222
01/06/2022 17:20:56 - INFO - __main__ -   Batch Number = 223
01/06/2022 17:20:56 - INFO - __main__ -   Batch Number = 224
01/06/2022 17:20:56 - INFO - __main__ -   Batch Number = 225
Evaluating:  58%|█████▊    | 225/391 [00:05<00:03, 44.31it/s]01/06/2022 17:20:56 - INFO - __main__ -   Batch Number = 226
01/06/2022 17:20:56 - INFO - __main__ -   Batch Number = 227
01/06/2022 17:20:56 - INFO - __main__ -   Batch Number = 228
01/06/2022 17:20:56 - INFO - __main__ -   Batch Number = 229
01/06/2022 17:20:56 - INFO - __main__ -   Batch Number = 230
Evaluating:  59%|█████▉    | 230/391 [00:05<00:03, 44.26it/s]01/06/2022 17:20:56 - INFO - __main__ -   Batch Number = 231
01/06/2022 17:20:56 - INFO - __main__ -   Batch Number = 232
01/06/2022 17:20:56 - INFO - __main__ -   Batch Number = 233
01/06/2022 17:20:56 - INFO - __main__ -   Batch Number = 234
01/06/2022 17:20:56 - INFO - __main__ -   Batch Number = 235
Evaluating:  60%|██████    | 235/391 [00:05<00:03, 44.28it/s]01/06/2022 17:20:56 - INFO - __main__ -   Batch Number = 236
01/06/2022 17:20:56 - INFO - __main__ -   Batch Number = 237
01/06/2022 17:20:56 - INFO - __main__ -   Batch Number = 238
01/06/2022 17:20:56 - INFO - __main__ -   Batch Number = 239
01/06/2022 17:20:56 - INFO - __main__ -   Batch Number = 240
Evaluating:  61%|██████▏   | 240/391 [00:05<00:03, 44.33it/s]01/06/2022 17:20:56 - INFO - __main__ -   Batch Number = 241
01/06/2022 17:20:56 - INFO - __main__ -   Batch Number = 242
01/06/2022 17:20:56 - INFO - __main__ -   Batch Number = 243
01/06/2022 17:20:56 - INFO - __main__ -   Batch Number = 244
01/06/2022 17:20:57 - INFO - __main__ -   Batch Number = 245
Evaluating:  63%|██████▎   | 245/391 [00:05<00:03, 44.28it/s]01/06/2022 17:20:57 - INFO - __main__ -   Batch Number = 246
01/06/2022 17:20:57 - INFO - __main__ -   Batch Number = 247
01/06/2022 17:20:57 - INFO - __main__ -   Batch Number = 248
01/06/2022 17:20:57 - INFO - __main__ -   Batch Number = 249
01/06/2022 17:20:57 - INFO - __main__ -   Batch Number = 250
Evaluating:  64%|██████▍   | 250/391 [00:05<00:03, 44.17it/s]01/06/2022 17:20:57 - INFO - __main__ -   Batch Number = 251
01/06/2022 17:20:57 - INFO - __main__ -   Batch Number = 252
01/06/2022 17:20:57 - INFO - __main__ -   Batch Number = 253
01/06/2022 17:20:57 - INFO - __main__ -   Batch Number = 254
01/06/2022 17:20:57 - INFO - __main__ -   Batch Number = 255
Evaluating:  65%|██████▌   | 255/391 [00:05<00:03, 44.30it/s]01/06/2022 17:20:57 - INFO - __main__ -   Batch Number = 256
01/06/2022 17:20:57 - INFO - __main__ -   Batch Number = 257
01/06/2022 17:20:57 - INFO - __main__ -   Batch Number = 258
01/06/2022 17:20:57 - INFO - __main__ -   Batch Number = 259
01/06/2022 17:20:57 - INFO - __main__ -   Batch Number = 260
Evaluating:  66%|██████▋   | 260/391 [00:05<00:02, 44.22it/s]01/06/2022 17:20:57 - INFO - __main__ -   Batch Number = 261
01/06/2022 17:20:57 - INFO - __main__ -   Batch Number = 262
01/06/2022 17:20:57 - INFO - __main__ -   Batch Number = 263
01/06/2022 17:20:57 - INFO - __main__ -   Batch Number = 264
01/06/2022 17:20:57 - INFO - __main__ -   Batch Number = 265
Evaluating:  68%|██████▊   | 265/391 [00:06<00:02, 44.32it/s]01/06/2022 17:20:57 - INFO - __main__ -   Batch Number = 266
01/06/2022 17:20:57 - INFO - __main__ -   Batch Number = 267
01/06/2022 17:20:57 - INFO - __main__ -   Batch Number = 268
01/06/2022 17:20:57 - INFO - __main__ -   Batch Number = 269
01/06/2022 17:20:57 - INFO - __main__ -   Batch Number = 270
Evaluating:  69%|██████▉   | 270/391 [00:06<00:02, 44.12it/s]01/06/2022 17:20:57 - INFO - __main__ -   Batch Number = 271
01/06/2022 17:20:57 - INFO - __main__ -   Batch Number = 272
01/06/2022 17:20:57 - INFO - __main__ -   Batch Number = 273
01/06/2022 17:20:57 - INFO - __main__ -   Batch Number = 274
01/06/2022 17:20:57 - INFO - __main__ -   Batch Number = 275
Evaluating:  70%|███████   | 275/391 [00:06<00:02, 44.21it/s]01/06/2022 17:20:57 - INFO - __main__ -   Batch Number = 276
01/06/2022 17:20:57 - INFO - __main__ -   Batch Number = 277
01/06/2022 17:20:57 - INFO - __main__ -   Batch Number = 278
01/06/2022 17:20:57 - INFO - __main__ -   Batch Number = 279
01/06/2022 17:20:57 - INFO - __main__ -   Batch Number = 280
Evaluating:  72%|███████▏  | 280/391 [00:06<00:02, 44.36it/s]01/06/2022 17:20:57 - INFO - __main__ -   Batch Number = 281
01/06/2022 17:20:57 - INFO - __main__ -   Batch Number = 282
01/06/2022 17:20:57 - INFO - __main__ -   Batch Number = 283
01/06/2022 17:20:57 - INFO - __main__ -   Batch Number = 284
01/06/2022 17:20:57 - INFO - __main__ -   Batch Number = 285
Evaluating:  73%|███████▎  | 285/391 [00:06<00:02, 44.36it/s]01/06/2022 17:20:57 - INFO - __main__ -   Batch Number = 286
01/06/2022 17:20:57 - INFO - __main__ -   Batch Number = 287
01/06/2022 17:20:58 - INFO - __main__ -   Batch Number = 288
01/06/2022 17:20:58 - INFO - __main__ -   Batch Number = 289
01/06/2022 17:20:58 - INFO - __main__ -   Batch Number = 290
Evaluating:  74%|███████▍  | 290/391 [00:06<00:02, 42.78it/s]01/06/2022 17:20:58 - INFO - __main__ -   Batch Number = 291
01/06/2022 17:20:58 - INFO - __main__ -   Batch Number = 292
01/06/2022 17:20:58 - INFO - __main__ -   Batch Number = 293
01/06/2022 17:20:58 - INFO - __main__ -   Batch Number = 294
01/06/2022 17:20:58 - INFO - __main__ -   Batch Number = 295
Evaluating:  75%|███████▌  | 295/391 [00:06<00:02, 43.19it/s]01/06/2022 17:20:58 - INFO - __main__ -   Batch Number = 296
01/06/2022 17:20:58 - INFO - __main__ -   Batch Number = 297
01/06/2022 17:20:58 - INFO - __main__ -   Batch Number = 298
01/06/2022 17:20:58 - INFO - __main__ -   Batch Number = 299
01/06/2022 17:20:58 - INFO - __main__ -   Batch Number = 300
Evaluating:  77%|███████▋  | 300/391 [00:06<00:02, 43.59it/s]01/06/2022 17:20:58 - INFO - __main__ -   Batch Number = 301
01/06/2022 17:20:58 - INFO - __main__ -   Batch Number = 302
01/06/2022 17:20:58 - INFO - __main__ -   Batch Number = 303
01/06/2022 17:20:58 - INFO - __main__ -   Batch Number = 304
01/06/2022 17:20:58 - INFO - __main__ -   Batch Number = 305
Evaluating:  78%|███████▊  | 305/391 [00:07<00:01, 43.57it/s]01/06/2022 17:20:58 - INFO - __main__ -   Batch Number = 306
01/06/2022 17:20:58 - INFO - __main__ -   Batch Number = 307
01/06/2022 17:20:58 - INFO - __main__ -   Batch Number = 308
01/06/2022 17:20:58 - INFO - __main__ -   Batch Number = 309
01/06/2022 17:20:58 - INFO - __main__ -   Batch Number = 310
Evaluating:  79%|███████▉  | 310/391 [00:07<00:01, 43.87it/s]01/06/2022 17:20:58 - INFO - __main__ -   Batch Number = 311
01/06/2022 17:20:58 - INFO - __main__ -   Batch Number = 312
01/06/2022 17:20:58 - INFO - __main__ -   Batch Number = 313
01/06/2022 17:20:58 - INFO - __main__ -   Batch Number = 314
01/06/2022 17:20:58 - INFO - __main__ -   Batch Number = 315
Evaluating:  81%|████████  | 315/391 [00:07<00:01, 43.98it/s]01/06/2022 17:20:58 - INFO - __main__ -   Batch Number = 316
01/06/2022 17:20:58 - INFO - __main__ -   Batch Number = 317
01/06/2022 17:20:58 - INFO - __main__ -   Batch Number = 318
01/06/2022 17:20:58 - INFO - __main__ -   Batch Number = 319
01/06/2022 17:20:58 - INFO - __main__ -   Batch Number = 320
Evaluating:  82%|████████▏ | 320/391 [00:07<00:01, 44.05it/s]01/06/2022 17:20:58 - INFO - __main__ -   Batch Number = 321
01/06/2022 17:20:58 - INFO - __main__ -   Batch Number = 322
01/06/2022 17:20:58 - INFO - __main__ -   Batch Number = 323
01/06/2022 17:20:58 - INFO - __main__ -   Batch Number = 324
01/06/2022 17:20:58 - INFO - __main__ -   Batch Number = 325
Evaluating:  83%|████████▎ | 325/391 [00:07<00:01, 43.89it/s]01/06/2022 17:20:58 - INFO - __main__ -   Batch Number = 326
01/06/2022 17:20:58 - INFO - __main__ -   Batch Number = 327
01/06/2022 17:20:58 - INFO - __main__ -   Batch Number = 328
01/06/2022 17:20:58 - INFO - __main__ -   Batch Number = 329
01/06/2022 17:20:58 - INFO - __main__ -   Batch Number = 330
Evaluating:  84%|████████▍ | 330/391 [00:07<00:01, 43.92it/s]01/06/2022 17:20:58 - INFO - __main__ -   Batch Number = 331
01/06/2022 17:20:59 - INFO - __main__ -   Batch Number = 332
01/06/2022 17:20:59 - INFO - __main__ -   Batch Number = 333
01/06/2022 17:20:59 - INFO - __main__ -   Batch Number = 334
01/06/2022 17:20:59 - INFO - __main__ -   Batch Number = 335
Evaluating:  86%|████████▌ | 335/391 [00:07<00:01, 44.14it/s]01/06/2022 17:20:59 - INFO - __main__ -   Batch Number = 336
01/06/2022 17:20:59 - INFO - __main__ -   Batch Number = 337
01/06/2022 17:20:59 - INFO - __main__ -   Batch Number = 338
01/06/2022 17:20:59 - INFO - __main__ -   Batch Number = 339
01/06/2022 17:20:59 - INFO - __main__ -   Batch Number = 340
Evaluating:  87%|████████▋ | 340/391 [00:07<00:01, 44.15it/s]01/06/2022 17:20:59 - INFO - __main__ -   Batch Number = 341
01/06/2022 17:20:59 - INFO - __main__ -   Batch Number = 342
01/06/2022 17:20:59 - INFO - __main__ -   Batch Number = 343
01/06/2022 17:20:59 - INFO - __main__ -   Batch Number = 344
01/06/2022 17:20:59 - INFO - __main__ -   Batch Number = 345
Evaluating:  88%|████████▊ | 345/391 [00:07<00:01, 44.08it/s]01/06/2022 17:20:59 - INFO - __main__ -   Batch Number = 346
01/06/2022 17:20:59 - INFO - __main__ -   Batch Number = 347
01/06/2022 17:20:59 - INFO - __main__ -   Batch Number = 348
01/06/2022 17:20:59 - INFO - __main__ -   Batch Number = 349
01/06/2022 17:20:59 - INFO - __main__ -   Batch Number = 350
Evaluating:  90%|████████▉ | 350/391 [00:08<00:00, 44.22it/s]01/06/2022 17:20:59 - INFO - __main__ -   Batch Number = 351
01/06/2022 17:20:59 - INFO - __main__ -   Batch Number = 352
01/06/2022 17:20:59 - INFO - __main__ -   Batch Number = 353
01/06/2022 17:20:59 - INFO - __main__ -   Batch Number = 354
01/06/2022 17:20:59 - INFO - __main__ -   Batch Number = 355
Evaluating:  91%|█████████ | 355/391 [00:08<00:00, 44.23it/s]01/06/2022 17:20:59 - INFO - __main__ -   Batch Number = 356
01/06/2022 17:20:59 - INFO - __main__ -   Batch Number = 357
01/06/2022 17:20:59 - INFO - __main__ -   Batch Number = 358
01/06/2022 17:20:59 - INFO - __main__ -   Batch Number = 359
01/06/2022 17:20:59 - INFO - __main__ -   Batch Number = 360
Evaluating:  92%|█████████▏| 360/391 [00:08<00:00, 44.28it/s]01/06/2022 17:20:59 - INFO - __main__ -   Batch Number = 361
01/06/2022 17:20:59 - INFO - __main__ -   Batch Number = 362
01/06/2022 17:20:59 - INFO - __main__ -   Batch Number = 363
01/06/2022 17:20:59 - INFO - __main__ -   Batch Number = 364
01/06/2022 17:20:59 - INFO - __main__ -   Batch Number = 365
Evaluating:  93%|█████████▎| 365/391 [00:08<00:00, 44.07it/s]01/06/2022 17:20:59 - INFO - __main__ -   Batch Number = 366
01/06/2022 17:20:59 - INFO - __main__ -   Batch Number = 367
01/06/2022 17:20:59 - INFO - __main__ -   Batch Number = 368
01/06/2022 17:20:59 - INFO - __main__ -   Batch Number = 369
01/06/2022 17:20:59 - INFO - __main__ -   Batch Number = 370
Evaluating:  95%|█████████▍| 370/391 [00:08<00:00, 44.20it/s]01/06/2022 17:20:59 - INFO - __main__ -   Batch Number = 371
01/06/2022 17:20:59 - INFO - __main__ -   Batch Number = 372
01/06/2022 17:20:59 - INFO - __main__ -   Batch Number = 373
01/06/2022 17:20:59 - INFO - __main__ -   Batch Number = 374
01/06/2022 17:20:59 - INFO - __main__ -   Batch Number = 375
Evaluating:  96%|█████████▌| 375/391 [00:08<00:00, 44.23it/s]01/06/2022 17:20:59 - INFO - __main__ -   Batch Number = 376
01/06/2022 17:21:00 - INFO - __main__ -   Batch Number = 377
01/06/2022 17:21:00 - INFO - __main__ -   Batch Number = 378
01/06/2022 17:21:00 - INFO - __main__ -   Batch Number = 379
01/06/2022 17:21:00 - INFO - __main__ -   Batch Number = 380
Evaluating:  97%|█████████▋| 380/391 [00:08<00:00, 44.28it/s]01/06/2022 17:21:00 - INFO - __main__ -   Batch Number = 381
01/06/2022 17:21:00 - INFO - __main__ -   Batch Number = 382
01/06/2022 17:21:00 - INFO - __main__ -   Batch Number = 383
01/06/2022 17:21:00 - INFO - __main__ -   Batch Number = 384
01/06/2022 17:21:00 - INFO - __main__ -   Batch Number = 385
Evaluating:  98%|█████████▊| 385/391 [00:08<00:00, 44.28it/s]01/06/2022 17:21:00 - INFO - __main__ -   Batch Number = 386
01/06/2022 17:21:00 - INFO - __main__ -   Batch Number = 387
01/06/2022 17:21:00 - INFO - __main__ -   Batch Number = 388
01/06/2022 17:21:00 - INFO - __main__ -   Batch Number = 389
01/06/2022 17:21:00 - INFO - __main__ -   Batch Number = 390
Evaluating: 100%|█████████▉| 390/391 [00:08<00:00, 44.22it/s]01/06/2022 17:21:00 - INFO - __main__ -   Batch Number = 391
Evaluating: 100%|██████████| 391/391 [00:08<00:00, 43.71it/s]
01/06/2022 17:21:00 - INFO - __main__ -     Evaluation done in total 8.946334 secs (0.002865 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/predictions_th_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/nbest_predictions_th_.json
01/06/2022 17:21:12 - INFO - __main__ -   Results = OrderedDict([('exact', 11.764705882352942), ('f1', 17.92694621708332), ('total', 1190), ('HasAns_exact', 11.764705882352942), ('HasAns_f1', 17.92694621708332), ('HasAns_total', 1190), ('best_exact', 11.764705882352942), ('best_exact_thresh', 0.0), ('best_f1', 17.92694621708332), ('best_f1_thresh', 0.0)])
01/06/2022 17:21:12 - INFO - __main__ -   Language adapter for tr not found, using en instead
01/06/2022 17:21:12 - INFO - __main__ -   Set active language adapter to en
01/06/2022 17:21:12 - INFO - __main__ -   Args Adapter Weight = None
01/06/2022 17:21:12 - INFO - __main__ -   Adapter Languages = ['en']
01/06/2022 17:21:12 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
  0%|          | 0/48 [00:00<?, ?it/s] 33%|███▎      | 16/48 [00:00<00:00, 136.00it/s] 62%|██████▎   | 30/48 [00:00<00:00, 128.81it/s] 96%|█████████▌| 46/48 [00:00<00:00, 140.40it/s]100%|██████████| 48/48 [00:00<00:00, 139.07it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<06:59,  2.84it/s]convert squad examples to features:  32%|███▏      | 385/1190 [00:00<00:01, 436.20it/s]convert squad examples to features:  94%|█████████▍| 1121/1190 [00:01<00:00, 1354.13it/s]convert squad examples to features: 100%|██████████| 1190/1190 [00:01<00:00, 1042.28it/s]/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|██████████| 1190/1190 [00:00<00:00, 455694.49it/s]
01/06/2022 17:21:14 - INFO - __main__ -   Local Rank = -1
01/06/2022 17:21:14 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.tr.json_bert-base-multilingual-cased_384_tr
01/06/2022 17:21:15 - INFO - __main__ -   ***** Running evaluation  *****
01/06/2022 17:21:15 - INFO - __main__ -     Num examples = 1360
01/06/2022 17:21:15 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/170 [00:00<?, ?it/s]01/06/2022 17:21:15 - INFO - __main__ -   Batch Number = 1
01/06/2022 17:21:15 - INFO - __main__ -   Batch Number = 2
01/06/2022 17:21:15 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|▏         | 3/170 [00:00<00:05, 29.11it/s]01/06/2022 17:21:15 - INFO - __main__ -   Batch Number = 4
01/06/2022 17:21:15 - INFO - __main__ -   Batch Number = 5
01/06/2022 17:21:16 - INFO - __main__ -   Batch Number = 6
01/06/2022 17:21:16 - INFO - __main__ -   Batch Number = 7
Evaluating:   4%|▍         | 7/170 [00:00<00:05, 31.26it/s]01/06/2022 17:21:16 - INFO - __main__ -   Batch Number = 8
01/06/2022 17:21:16 - INFO - __main__ -   Batch Number = 9
01/06/2022 17:21:16 - INFO - __main__ -   Batch Number = 10
01/06/2022 17:21:16 - INFO - __main__ -   Batch Number = 11
01/06/2022 17:21:16 - INFO - __main__ -   Batch Number = 12
Evaluating:   7%|▋         | 12/170 [00:00<00:04, 37.32it/s]01/06/2022 17:21:16 - INFO - __main__ -   Batch Number = 13
01/06/2022 17:21:16 - INFO - __main__ -   Batch Number = 14
01/06/2022 17:21:16 - INFO - __main__ -   Batch Number = 15
01/06/2022 17:21:16 - INFO - __main__ -   Batch Number = 16
01/06/2022 17:21:16 - INFO - __main__ -   Batch Number = 17
Evaluating:  10%|█         | 17/170 [00:00<00:03, 40.24it/s]01/06/2022 17:21:16 - INFO - __main__ -   Batch Number = 18
01/06/2022 17:21:16 - INFO - __main__ -   Batch Number = 19
01/06/2022 17:21:16 - INFO - __main__ -   Batch Number = 20
01/06/2022 17:21:16 - INFO - __main__ -   Batch Number = 21
01/06/2022 17:21:16 - INFO - __main__ -   Batch Number = 22
Evaluating:  13%|█▎        | 22/170 [00:00<00:03, 41.45it/s]01/06/2022 17:21:16 - INFO - __main__ -   Batch Number = 23
01/06/2022 17:21:16 - INFO - __main__ -   Batch Number = 24
01/06/2022 17:21:16 - INFO - __main__ -   Batch Number = 25
01/06/2022 17:21:16 - INFO - __main__ -   Batch Number = 26
01/06/2022 17:21:16 - INFO - __main__ -   Batch Number = 27
Evaluating:  16%|█▌        | 27/170 [00:00<00:03, 42.44it/s]01/06/2022 17:21:16 - INFO - __main__ -   Batch Number = 28
01/06/2022 17:21:16 - INFO - __main__ -   Batch Number = 29
01/06/2022 17:21:16 - INFO - __main__ -   Batch Number = 30
01/06/2022 17:21:16 - INFO - __main__ -   Batch Number = 31
01/06/2022 17:21:16 - INFO - __main__ -   Batch Number = 32
Evaluating:  19%|█▉        | 32/170 [00:00<00:03, 42.87it/s]01/06/2022 17:21:16 - INFO - __main__ -   Batch Number = 33
01/06/2022 17:21:16 - INFO - __main__ -   Batch Number = 34
01/06/2022 17:21:16 - INFO - __main__ -   Batch Number = 35
01/06/2022 17:21:16 - INFO - __main__ -   Batch Number = 36
01/06/2022 17:21:16 - INFO - __main__ -   Batch Number = 37
Evaluating:  22%|██▏       | 37/170 [00:00<00:03, 43.25it/s]01/06/2022 17:21:16 - INFO - __main__ -   Batch Number = 38
01/06/2022 17:21:16 - INFO - __main__ -   Batch Number = 39
01/06/2022 17:21:16 - INFO - __main__ -   Batch Number = 40
01/06/2022 17:21:16 - INFO - __main__ -   Batch Number = 41
01/06/2022 17:21:16 - INFO - __main__ -   Batch Number = 42
Evaluating:  25%|██▍       | 42/170 [00:01<00:02, 43.58it/s]01/06/2022 17:21:16 - INFO - __main__ -   Batch Number = 43
01/06/2022 17:21:16 - INFO - __main__ -   Batch Number = 44
01/06/2022 17:21:16 - INFO - __main__ -   Batch Number = 45
01/06/2022 17:21:16 - INFO - __main__ -   Batch Number = 46
01/06/2022 17:21:16 - INFO - __main__ -   Batch Number = 47
Evaluating:  28%|██▊       | 47/170 [00:01<00:02, 43.90it/s]01/06/2022 17:21:16 - INFO - __main__ -   Batch Number = 48
01/06/2022 17:21:16 - INFO - __main__ -   Batch Number = 49
01/06/2022 17:21:17 - INFO - __main__ -   Batch Number = 50
01/06/2022 17:21:17 - INFO - __main__ -   Batch Number = 51
01/06/2022 17:21:17 - INFO - __main__ -   Batch Number = 52
Evaluating:  31%|███       | 52/170 [00:01<00:02, 43.91it/s]01/06/2022 17:21:17 - INFO - __main__ -   Batch Number = 53
01/06/2022 17:21:17 - INFO - __main__ -   Batch Number = 54
01/06/2022 17:21:17 - INFO - __main__ -   Batch Number = 55
01/06/2022 17:21:17 - INFO - __main__ -   Batch Number = 56
01/06/2022 17:21:17 - INFO - __main__ -   Batch Number = 57
Evaluating:  34%|███▎      | 57/170 [00:01<00:02, 44.06it/s]01/06/2022 17:21:17 - INFO - __main__ -   Batch Number = 58
01/06/2022 17:21:17 - INFO - __main__ -   Batch Number = 59
01/06/2022 17:21:17 - INFO - __main__ -   Batch Number = 60
01/06/2022 17:21:17 - INFO - __main__ -   Batch Number = 61
01/06/2022 17:21:17 - INFO - __main__ -   Batch Number = 62
Evaluating:  36%|███▋      | 62/170 [00:01<00:02, 44.21it/s]01/06/2022 17:21:17 - INFO - __main__ -   Batch Number = 63
01/06/2022 17:21:17 - INFO - __main__ -   Batch Number = 64
01/06/2022 17:21:17 - INFO - __main__ -   Batch Number = 65
01/06/2022 17:21:17 - INFO - __main__ -   Batch Number = 66
01/06/2022 17:21:17 - INFO - __main__ -   Batch Number = 67
Evaluating:  39%|███▉      | 67/170 [00:01<00:02, 44.13it/s]01/06/2022 17:21:17 - INFO - __main__ -   Batch Number = 68
01/06/2022 17:21:17 - INFO - __main__ -   Batch Number = 69
01/06/2022 17:21:17 - INFO - __main__ -   Batch Number = 70
01/06/2022 17:21:17 - INFO - __main__ -   Batch Number = 71
01/06/2022 17:21:17 - INFO - __main__ -   Batch Number = 72
Evaluating:  42%|████▏     | 72/170 [00:01<00:02, 44.24it/s]01/06/2022 17:21:17 - INFO - __main__ -   Batch Number = 73
01/06/2022 17:21:17 - INFO - __main__ -   Batch Number = 74
01/06/2022 17:21:17 - INFO - __main__ -   Batch Number = 75
01/06/2022 17:21:17 - INFO - __main__ -   Batch Number = 76
01/06/2022 17:21:17 - INFO - __main__ -   Batch Number = 77
Evaluating:  45%|████▌     | 77/170 [00:01<00:02, 44.17it/s]01/06/2022 17:21:17 - INFO - __main__ -   Batch Number = 78
01/06/2022 17:21:17 - INFO - __main__ -   Batch Number = 79
01/06/2022 17:21:17 - INFO - __main__ -   Batch Number = 80
01/06/2022 17:21:17 - INFO - __main__ -   Batch Number = 81
01/06/2022 17:21:17 - INFO - __main__ -   Batch Number = 82
Evaluating:  48%|████▊     | 82/170 [00:01<00:01, 44.19it/s]01/06/2022 17:21:17 - INFO - __main__ -   Batch Number = 83
01/06/2022 17:21:17 - INFO - __main__ -   Batch Number = 84
01/06/2022 17:21:17 - INFO - __main__ -   Batch Number = 85
01/06/2022 17:21:17 - INFO - __main__ -   Batch Number = 86
01/06/2022 17:21:17 - INFO - __main__ -   Batch Number = 87
Evaluating:  51%|█████     | 87/170 [00:02<00:01, 44.23it/s]01/06/2022 17:21:17 - INFO - __main__ -   Batch Number = 88
01/06/2022 17:21:17 - INFO - __main__ -   Batch Number = 89
01/06/2022 17:21:17 - INFO - __main__ -   Batch Number = 90
01/06/2022 17:21:17 - INFO - __main__ -   Batch Number = 91
01/06/2022 17:21:17 - INFO - __main__ -   Batch Number = 92
Evaluating:  54%|█████▍    | 92/170 [00:02<00:01, 44.32it/s]01/06/2022 17:21:17 - INFO - __main__ -   Batch Number = 93
01/06/2022 17:21:18 - INFO - __main__ -   Batch Number = 94
01/06/2022 17:21:18 - INFO - __main__ -   Batch Number = 95
01/06/2022 17:21:18 - INFO - __main__ -   Batch Number = 96
01/06/2022 17:21:18 - INFO - __main__ -   Batch Number = 97
Evaluating:  57%|█████▋    | 97/170 [00:02<00:01, 44.23it/s]01/06/2022 17:21:18 - INFO - __main__ -   Batch Number = 98
01/06/2022 17:21:18 - INFO - __main__ -   Batch Number = 99
01/06/2022 17:21:18 - INFO - __main__ -   Batch Number = 100
01/06/2022 17:21:18 - INFO - __main__ -   Batch Number = 101
01/06/2022 17:21:18 - INFO - __main__ -   Batch Number = 102
Evaluating:  60%|██████    | 102/170 [00:02<00:01, 44.28it/s]01/06/2022 17:21:18 - INFO - __main__ -   Batch Number = 103
01/06/2022 17:21:18 - INFO - __main__ -   Batch Number = 104
01/06/2022 17:21:18 - INFO - __main__ -   Batch Number = 105
01/06/2022 17:21:18 - INFO - __main__ -   Batch Number = 106
01/06/2022 17:21:18 - INFO - __main__ -   Batch Number = 107
Evaluating:  63%|██████▎   | 107/170 [00:02<00:01, 44.35it/s]01/06/2022 17:21:18 - INFO - __main__ -   Batch Number = 108
01/06/2022 17:21:18 - INFO - __main__ -   Batch Number = 109
01/06/2022 17:21:18 - INFO - __main__ -   Batch Number = 110
01/06/2022 17:21:18 - INFO - __main__ -   Batch Number = 111
01/06/2022 17:21:18 - INFO - __main__ -   Batch Number = 112
Evaluating:  66%|██████▌   | 112/170 [00:02<00:01, 42.78it/s]01/06/2022 17:21:18 - INFO - __main__ -   Batch Number = 113
01/06/2022 17:21:18 - INFO - __main__ -   Batch Number = 114
01/06/2022 17:21:18 - INFO - __main__ -   Batch Number = 115
01/06/2022 17:21:18 - INFO - __main__ -   Batch Number = 116
01/06/2022 17:21:18 - INFO - __main__ -   Batch Number = 117
Evaluating:  69%|██████▉   | 117/170 [00:02<00:01, 43.02it/s]01/06/2022 17:21:18 - INFO - __main__ -   Batch Number = 118
01/06/2022 17:21:18 - INFO - __main__ -   Batch Number = 119
01/06/2022 17:21:18 - INFO - __main__ -   Batch Number = 120
01/06/2022 17:21:18 - INFO - __main__ -   Batch Number = 121
01/06/2022 17:21:18 - INFO - __main__ -   Batch Number = 122
Evaluating:  72%|███████▏  | 122/170 [00:02<00:01, 43.37it/s]01/06/2022 17:21:18 - INFO - __main__ -   Batch Number = 123
01/06/2022 17:21:18 - INFO - __main__ -   Batch Number = 124
01/06/2022 17:21:18 - INFO - __main__ -   Batch Number = 125
01/06/2022 17:21:18 - INFO - __main__ -   Batch Number = 126
01/06/2022 17:21:18 - INFO - __main__ -   Batch Number = 127
Evaluating:  75%|███████▍  | 127/170 [00:02<00:00, 43.55it/s]01/06/2022 17:21:18 - INFO - __main__ -   Batch Number = 128
01/06/2022 17:21:18 - INFO - __main__ -   Batch Number = 129
01/06/2022 17:21:18 - INFO - __main__ -   Batch Number = 130
01/06/2022 17:21:18 - INFO - __main__ -   Batch Number = 131
01/06/2022 17:21:18 - INFO - __main__ -   Batch Number = 132
Evaluating:  78%|███████▊  | 132/170 [00:03<00:00, 43.68it/s]01/06/2022 17:21:18 - INFO - __main__ -   Batch Number = 133
01/06/2022 17:21:18 - INFO - __main__ -   Batch Number = 134
01/06/2022 17:21:18 - INFO - __main__ -   Batch Number = 135
01/06/2022 17:21:18 - INFO - __main__ -   Batch Number = 136
01/06/2022 17:21:18 - INFO - __main__ -   Batch Number = 137
Evaluating:  81%|████████  | 137/170 [00:03<00:00, 44.00it/s]01/06/2022 17:21:19 - INFO - __main__ -   Batch Number = 138
01/06/2022 17:21:19 - INFO - __main__ -   Batch Number = 139
01/06/2022 17:21:19 - INFO - __main__ -   Batch Number = 140
01/06/2022 17:21:19 - INFO - __main__ -   Batch Number = 141
01/06/2022 17:21:19 - INFO - __main__ -   Batch Number = 142
Evaluating:  84%|████████▎ | 142/170 [00:03<00:00, 43.96it/s]01/06/2022 17:21:19 - INFO - __main__ -   Batch Number = 143
01/06/2022 17:21:19 - INFO - __main__ -   Batch Number = 144
01/06/2022 17:21:19 - INFO - __main__ -   Batch Number = 145
01/06/2022 17:21:19 - INFO - __main__ -   Batch Number = 146
01/06/2022 17:21:19 - INFO - __main__ -   Batch Number = 147
Evaluating:  86%|████████▋ | 147/170 [00:03<00:00, 44.15it/s]01/06/2022 17:21:19 - INFO - __main__ -   Batch Number = 148
01/06/2022 17:21:19 - INFO - __main__ -   Batch Number = 149
01/06/2022 17:21:19 - INFO - __main__ -   Batch Number = 150
01/06/2022 17:21:19 - INFO - __main__ -   Batch Number = 151
01/06/2022 17:21:19 - INFO - __main__ -   Batch Number = 152
Evaluating:  89%|████████▉ | 152/170 [00:03<00:00, 44.21it/s]01/06/2022 17:21:19 - INFO - __main__ -   Batch Number = 153
01/06/2022 17:21:19 - INFO - __main__ -   Batch Number = 154
01/06/2022 17:21:19 - INFO - __main__ -   Batch Number = 155
01/06/2022 17:21:19 - INFO - __main__ -   Batch Number = 156
01/06/2022 17:21:19 - INFO - __main__ -   Batch Number = 157
Evaluating:  92%|█████████▏| 157/170 [00:03<00:00, 44.16it/s]01/06/2022 17:21:19 - INFO - __main__ -   Batch Number = 158
01/06/2022 17:21:19 - INFO - __main__ -   Batch Number = 159
01/06/2022 17:21:19 - INFO - __main__ -   Batch Number = 160
01/06/2022 17:21:19 - INFO - __main__ -   Batch Number = 161
01/06/2022 17:21:19 - INFO - __main__ -   Batch Number = 162
Evaluating:  95%|█████████▌| 162/170 [00:03<00:00, 44.20it/s]01/06/2022 17:21:19 - INFO - __main__ -   Batch Number = 163
01/06/2022 17:21:19 - INFO - __main__ -   Batch Number = 164
01/06/2022 17:21:19 - INFO - __main__ -   Batch Number = 165
01/06/2022 17:21:19 - INFO - __main__ -   Batch Number = 166
01/06/2022 17:21:19 - INFO - __main__ -   Batch Number = 167
Evaluating:  98%|█████████▊| 167/170 [00:03<00:00, 44.26it/s]01/06/2022 17:21:19 - INFO - __main__ -   Batch Number = 168
01/06/2022 17:21:19 - INFO - __main__ -   Batch Number = 169
01/06/2022 17:21:19 - INFO - __main__ -   Batch Number = 170
Evaluating: 100%|██████████| 170/170 [00:03<00:00, 43.32it/s]
01/06/2022 17:21:19 - INFO - __main__ -     Evaluation done in total 3.924680 secs (0.002886 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/predictions_tr_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/nbest_predictions_tr_.json
01/06/2022 17:21:23 - INFO - __main__ -   Results = OrderedDict([('exact', 27.563025210084035), ('f1', 44.47064455206221), ('total', 1190), ('HasAns_exact', 27.563025210084035), ('HasAns_f1', 44.47064455206221), ('HasAns_total', 1190), ('best_exact', 27.563025210084035), ('best_exact_thresh', 0.0), ('best_f1', 44.47064455206221), ('best_f1_thresh', 0.0)])
01/06/2022 17:21:23 - INFO - __main__ -   Language adapter for vi not found, using en instead
01/06/2022 17:21:23 - INFO - __main__ -   Set active language adapter to en
01/06/2022 17:21:23 - INFO - __main__ -   Args Adapter Weight = None
01/06/2022 17:21:23 - INFO - __main__ -   Adapter Languages = ['en']
01/06/2022 17:21:23 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
  0%|          | 0/48 [00:00<?, ?it/s] 33%|███▎      | 16/48 [00:00<00:00, 137.53it/s] 62%|██████▎   | 30/48 [00:00<00:00, 130.99it/s] 94%|█████████▍| 45/48 [00:00<00:00, 135.17it/s]100%|██████████| 48/48 [00:00<00:00, 137.01it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<06:04,  3.26it/s]convert squad examples to features:  22%|██▏       | 257/1190 [00:00<00:01, 595.49it/s]convert squad examples to features:  32%|███▏      | 385/1190 [00:01<00:02, 341.77it/s]convert squad examples to features:  83%|████████▎ | 993/1190 [00:01<00:00, 1087.05it/s]convert squad examples to features: 100%|██████████| 1190/1190 [00:01<00:00, 846.85it/s]/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|██████████| 1190/1190 [00:00<00:00, 493642.74it/s]
01/06/2022 17:21:25 - INFO - __main__ -   Local Rank = -1
01/06/2022 17:21:25 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.vi.json_bert-base-multilingual-cased_384_vi
01/06/2022 17:21:26 - INFO - __main__ -   ***** Running evaluation  *****
01/06/2022 17:21:26 - INFO - __main__ -     Num examples = 1325
01/06/2022 17:21:26 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/166 [00:00<?, ?it/s]01/06/2022 17:21:26 - INFO - __main__ -   Batch Number = 1
01/06/2022 17:21:27 - INFO - __main__ -   Batch Number = 2
01/06/2022 17:21:27 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|▏         | 3/166 [00:00<00:05, 29.40it/s]01/06/2022 17:21:27 - INFO - __main__ -   Batch Number = 4
01/06/2022 17:21:27 - INFO - __main__ -   Batch Number = 5
01/06/2022 17:21:27 - INFO - __main__ -   Batch Number = 6
01/06/2022 17:21:27 - INFO - __main__ -   Batch Number = 7
Evaluating:   4%|▍         | 7/166 [00:00<00:04, 33.84it/s]01/06/2022 17:21:27 - INFO - __main__ -   Batch Number = 8
01/06/2022 17:21:27 - INFO - __main__ -   Batch Number = 9
01/06/2022 17:21:27 - INFO - __main__ -   Batch Number = 10
01/06/2022 17:21:27 - INFO - __main__ -   Batch Number = 11
01/06/2022 17:21:27 - INFO - __main__ -   Batch Number = 12
Evaluating:   7%|▋         | 12/166 [00:00<00:03, 38.67it/s]01/06/2022 17:21:27 - INFO - __main__ -   Batch Number = 13
01/06/2022 17:21:27 - INFO - __main__ -   Batch Number = 14
01/06/2022 17:21:27 - INFO - __main__ -   Batch Number = 15
01/06/2022 17:21:27 - INFO - __main__ -   Batch Number = 16
01/06/2022 17:21:27 - INFO - __main__ -   Batch Number = 17
Evaluating:  10%|█         | 17/166 [00:00<00:03, 41.05it/s]01/06/2022 17:21:27 - INFO - __main__ -   Batch Number = 18
01/06/2022 17:21:27 - INFO - __main__ -   Batch Number = 19
01/06/2022 17:21:27 - INFO - __main__ -   Batch Number = 20
01/06/2022 17:21:27 - INFO - __main__ -   Batch Number = 21
01/06/2022 17:21:27 - INFO - __main__ -   Batch Number = 22
Evaluating:  13%|█▎        | 22/166 [00:00<00:03, 41.94it/s]01/06/2022 17:21:27 - INFO - __main__ -   Batch Number = 23
01/06/2022 17:21:27 - INFO - __main__ -   Batch Number = 24
01/06/2022 17:21:27 - INFO - __main__ -   Batch Number = 25
01/06/2022 17:21:27 - INFO - __main__ -   Batch Number = 26
01/06/2022 17:21:27 - INFO - __main__ -   Batch Number = 27
Evaluating:  16%|█▋        | 27/166 [00:00<00:03, 42.81it/s]01/06/2022 17:21:27 - INFO - __main__ -   Batch Number = 28
01/06/2022 17:21:27 - INFO - __main__ -   Batch Number = 29
01/06/2022 17:21:27 - INFO - __main__ -   Batch Number = 30
01/06/2022 17:21:27 - INFO - __main__ -   Batch Number = 31
01/06/2022 17:21:27 - INFO - __main__ -   Batch Number = 32
Evaluating:  19%|█▉        | 32/166 [00:00<00:03, 43.22it/s]01/06/2022 17:21:27 - INFO - __main__ -   Batch Number = 33
01/06/2022 17:21:27 - INFO - __main__ -   Batch Number = 34
01/06/2022 17:21:27 - INFO - __main__ -   Batch Number = 35
01/06/2022 17:21:27 - INFO - __main__ -   Batch Number = 36
01/06/2022 17:21:27 - INFO - __main__ -   Batch Number = 37
Evaluating:  22%|██▏       | 37/166 [00:00<00:02, 43.49it/s]01/06/2022 17:21:27 - INFO - __main__ -   Batch Number = 38
01/06/2022 17:21:27 - INFO - __main__ -   Batch Number = 39
01/06/2022 17:21:27 - INFO - __main__ -   Batch Number = 40
01/06/2022 17:21:27 - INFO - __main__ -   Batch Number = 41
01/06/2022 17:21:27 - INFO - __main__ -   Batch Number = 42
Evaluating:  25%|██▌       | 42/166 [00:01<00:02, 43.82it/s]01/06/2022 17:21:27 - INFO - __main__ -   Batch Number = 43
01/06/2022 17:21:27 - INFO - __main__ -   Batch Number = 44
01/06/2022 17:21:28 - INFO - __main__ -   Batch Number = 45
01/06/2022 17:21:28 - INFO - __main__ -   Batch Number = 46
01/06/2022 17:21:28 - INFO - __main__ -   Batch Number = 47
Evaluating:  28%|██▊       | 47/166 [00:01<00:02, 43.93it/s]01/06/2022 17:21:28 - INFO - __main__ -   Batch Number = 48
01/06/2022 17:21:28 - INFO - __main__ -   Batch Number = 49
01/06/2022 17:21:28 - INFO - __main__ -   Batch Number = 50
01/06/2022 17:21:28 - INFO - __main__ -   Batch Number = 51
01/06/2022 17:21:28 - INFO - __main__ -   Batch Number = 52
Evaluating:  31%|███▏      | 52/166 [00:01<00:02, 44.01it/s]01/06/2022 17:21:28 - INFO - __main__ -   Batch Number = 53
01/06/2022 17:21:28 - INFO - __main__ -   Batch Number = 54
01/06/2022 17:21:28 - INFO - __main__ -   Batch Number = 55
01/06/2022 17:21:28 - INFO - __main__ -   Batch Number = 56
01/06/2022 17:21:28 - INFO - __main__ -   Batch Number = 57
Evaluating:  34%|███▍      | 57/166 [00:01<00:02, 44.05it/s]01/06/2022 17:21:28 - INFO - __main__ -   Batch Number = 58
01/06/2022 17:21:28 - INFO - __main__ -   Batch Number = 59
01/06/2022 17:21:28 - INFO - __main__ -   Batch Number = 60
01/06/2022 17:21:28 - INFO - __main__ -   Batch Number = 61
01/06/2022 17:21:28 - INFO - __main__ -   Batch Number = 62
Evaluating:  37%|███▋      | 62/166 [00:01<00:02, 44.21it/s]01/06/2022 17:21:28 - INFO - __main__ -   Batch Number = 63
01/06/2022 17:21:28 - INFO - __main__ -   Batch Number = 64
01/06/2022 17:21:28 - INFO - __main__ -   Batch Number = 65
01/06/2022 17:21:28 - INFO - __main__ -   Batch Number = 66
01/06/2022 17:21:28 - INFO - __main__ -   Batch Number = 67
Evaluating:  40%|████      | 67/166 [00:01<00:02, 42.59it/s]01/06/2022 17:21:28 - INFO - __main__ -   Batch Number = 68
01/06/2022 17:21:28 - INFO - __main__ -   Batch Number = 69
01/06/2022 17:21:28 - INFO - __main__ -   Batch Number = 70
01/06/2022 17:21:28 - INFO - __main__ -   Batch Number = 71
01/06/2022 17:21:28 - INFO - __main__ -   Batch Number = 72
Evaluating:  43%|████▎     | 72/166 [00:01<00:02, 43.18it/s]01/06/2022 17:21:28 - INFO - __main__ -   Batch Number = 73
01/06/2022 17:21:28 - INFO - __main__ -   Batch Number = 74
01/06/2022 17:21:28 - INFO - __main__ -   Batch Number = 75
01/06/2022 17:21:28 - INFO - __main__ -   Batch Number = 76
01/06/2022 17:21:28 - INFO - __main__ -   Batch Number = 77
Evaluating:  46%|████▋     | 77/166 [00:01<00:02, 43.53it/s]01/06/2022 17:21:28 - INFO - __main__ -   Batch Number = 78
01/06/2022 17:21:28 - INFO - __main__ -   Batch Number = 79
01/06/2022 17:21:28 - INFO - __main__ -   Batch Number = 80
01/06/2022 17:21:28 - INFO - __main__ -   Batch Number = 81
01/06/2022 17:21:28 - INFO - __main__ -   Batch Number = 82
Evaluating:  49%|████▉     | 82/166 [00:01<00:01, 43.57it/s]01/06/2022 17:21:28 - INFO - __main__ -   Batch Number = 83
01/06/2022 17:21:28 - INFO - __main__ -   Batch Number = 84
01/06/2022 17:21:28 - INFO - __main__ -   Batch Number = 85
01/06/2022 17:21:28 - INFO - __main__ -   Batch Number = 86
01/06/2022 17:21:28 - INFO - __main__ -   Batch Number = 87
Evaluating:  52%|█████▏    | 87/166 [00:02<00:01, 43.90it/s]01/06/2022 17:21:29 - INFO - __main__ -   Batch Number = 88
01/06/2022 17:21:29 - INFO - __main__ -   Batch Number = 89
01/06/2022 17:21:29 - INFO - __main__ -   Batch Number = 90
01/06/2022 17:21:29 - INFO - __main__ -   Batch Number = 91
01/06/2022 17:21:29 - INFO - __main__ -   Batch Number = 92
Evaluating:  55%|█████▌    | 92/166 [00:02<00:01, 44.04it/s]01/06/2022 17:21:29 - INFO - __main__ -   Batch Number = 93
01/06/2022 17:21:29 - INFO - __main__ -   Batch Number = 94
01/06/2022 17:21:29 - INFO - __main__ -   Batch Number = 95
01/06/2022 17:21:29 - INFO - __main__ -   Batch Number = 96
01/06/2022 17:21:29 - INFO - __main__ -   Batch Number = 97
Evaluating:  58%|█████▊    | 97/166 [00:02<00:01, 44.06it/s]01/06/2022 17:21:29 - INFO - __main__ -   Batch Number = 98
01/06/2022 17:21:29 - INFO - __main__ -   Batch Number = 99
01/06/2022 17:21:29 - INFO - __main__ -   Batch Number = 100
01/06/2022 17:21:29 - INFO - __main__ -   Batch Number = 101
01/06/2022 17:21:29 - INFO - __main__ -   Batch Number = 102
Evaluating:  61%|██████▏   | 102/166 [00:02<00:01, 44.07it/s]01/06/2022 17:21:29 - INFO - __main__ -   Batch Number = 103
01/06/2022 17:21:29 - INFO - __main__ -   Batch Number = 104
01/06/2022 17:21:29 - INFO - __main__ -   Batch Number = 105
01/06/2022 17:21:29 - INFO - __main__ -   Batch Number = 106
01/06/2022 17:21:29 - INFO - __main__ -   Batch Number = 107
Evaluating:  64%|██████▍   | 107/166 [00:02<00:01, 44.23it/s]01/06/2022 17:21:29 - INFO - __main__ -   Batch Number = 108
01/06/2022 17:21:29 - INFO - __main__ -   Batch Number = 109
01/06/2022 17:21:29 - INFO - __main__ -   Batch Number = 110
01/06/2022 17:21:29 - INFO - __main__ -   Batch Number = 111
01/06/2022 17:21:29 - INFO - __main__ -   Batch Number = 112
Evaluating:  67%|██████▋   | 112/166 [00:02<00:01, 44.06it/s]01/06/2022 17:21:29 - INFO - __main__ -   Batch Number = 113
01/06/2022 17:21:29 - INFO - __main__ -   Batch Number = 114
01/06/2022 17:21:29 - INFO - __main__ -   Batch Number = 115
01/06/2022 17:21:29 - INFO - __main__ -   Batch Number = 116
01/06/2022 17:21:29 - INFO - __main__ -   Batch Number = 117
Evaluating:  70%|███████   | 117/166 [00:02<00:01, 44.18it/s]01/06/2022 17:21:29 - INFO - __main__ -   Batch Number = 118
01/06/2022 17:21:29 - INFO - __main__ -   Batch Number = 119
01/06/2022 17:21:29 - INFO - __main__ -   Batch Number = 120
01/06/2022 17:21:29 - INFO - __main__ -   Batch Number = 121
01/06/2022 17:21:29 - INFO - __main__ -   Batch Number = 122
Evaluating:  73%|███████▎  | 122/166 [00:02<00:00, 44.29it/s]01/06/2022 17:21:29 - INFO - __main__ -   Batch Number = 123
01/06/2022 17:21:29 - INFO - __main__ -   Batch Number = 124
01/06/2022 17:21:29 - INFO - __main__ -   Batch Number = 125
01/06/2022 17:21:29 - INFO - __main__ -   Batch Number = 126
01/06/2022 17:21:29 - INFO - __main__ -   Batch Number = 127
Evaluating:  77%|███████▋  | 127/166 [00:02<00:00, 44.15it/s]01/06/2022 17:21:29 - INFO - __main__ -   Batch Number = 128
01/06/2022 17:21:29 - INFO - __main__ -   Batch Number = 129
01/06/2022 17:21:29 - INFO - __main__ -   Batch Number = 130
01/06/2022 17:21:29 - INFO - __main__ -   Batch Number = 131
01/06/2022 17:21:30 - INFO - __main__ -   Batch Number = 132
Evaluating:  80%|███████▉  | 132/166 [00:03<00:00, 44.24it/s]01/06/2022 17:21:30 - INFO - __main__ -   Batch Number = 133
01/06/2022 17:21:30 - INFO - __main__ -   Batch Number = 134
01/06/2022 17:21:30 - INFO - __main__ -   Batch Number = 135
01/06/2022 17:21:30 - INFO - __main__ -   Batch Number = 136
01/06/2022 17:21:30 - INFO - __main__ -   Batch Number = 137
Evaluating:  83%|████████▎ | 137/166 [00:03<00:00, 44.20it/s]01/06/2022 17:21:30 - INFO - __main__ -   Batch Number = 138
01/06/2022 17:21:30 - INFO - __main__ -   Batch Number = 139
01/06/2022 17:21:30 - INFO - __main__ -   Batch Number = 140
01/06/2022 17:21:30 - INFO - __main__ -   Batch Number = 141
01/06/2022 17:21:30 - INFO - __main__ -   Batch Number = 142
Evaluating:  86%|████████▌ | 142/166 [00:03<00:00, 44.11it/s]01/06/2022 17:21:30 - INFO - __main__ -   Batch Number = 143
01/06/2022 17:21:30 - INFO - __main__ -   Batch Number = 144
01/06/2022 17:21:30 - INFO - __main__ -   Batch Number = 145
01/06/2022 17:21:30 - INFO - __main__ -   Batch Number = 146
01/06/2022 17:21:30 - INFO - __main__ -   Batch Number = 147
Evaluating:  89%|████████▊ | 147/166 [00:03<00:00, 44.26it/s]01/06/2022 17:21:30 - INFO - __main__ -   Batch Number = 148
01/06/2022 17:21:30 - INFO - __main__ -   Batch Number = 149
01/06/2022 17:21:30 - INFO - __main__ -   Batch Number = 150
01/06/2022 17:21:30 - INFO - __main__ -   Batch Number = 151
01/06/2022 17:21:30 - INFO - __main__ -   Batch Number = 152
Evaluating:  92%|█████████▏| 152/166 [00:03<00:00, 44.21it/s]01/06/2022 17:21:30 - INFO - __main__ -   Batch Number = 153
01/06/2022 17:21:30 - INFO - __main__ -   Batch Number = 154
01/06/2022 17:21:30 - INFO - __main__ -   Batch Number = 155
01/06/2022 17:21:30 - INFO - __main__ -   Batch Number = 156
01/06/2022 17:21:30 - INFO - __main__ -   Batch Number = 157
Evaluating:  95%|█████████▍| 157/166 [00:03<00:00, 44.06it/s]01/06/2022 17:21:30 - INFO - __main__ -   Batch Number = 158
01/06/2022 17:21:30 - INFO - __main__ -   Batch Number = 159
01/06/2022 17:21:30 - INFO - __main__ -   Batch Number = 160
01/06/2022 17:21:30 - INFO - __main__ -   Batch Number = 161
01/06/2022 17:21:30 - INFO - __main__ -   Batch Number = 162
Evaluating:  98%|█████████▊| 162/166 [00:03<00:00, 44.09it/s]01/06/2022 17:21:30 - INFO - __main__ -   Batch Number = 163
01/06/2022 17:21:30 - INFO - __main__ -   Batch Number = 164
01/06/2022 17:21:30 - INFO - __main__ -   Batch Number = 165
01/06/2022 17:21:30 - INFO - __main__ -   Batch Number = 166
Evaluating: 100%|██████████| 166/166 [00:03<00:00, 43.51it/s]
01/06/2022 17:21:30 - INFO - __main__ -     Evaluation done in total 3.815699 secs (0.002880 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/predictions_vi_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/nbest_predictions_vi_.json
01/06/2022 17:21:34 - INFO - __main__ -   Results = OrderedDict([('exact', 43.529411764705884), ('f1', 63.88021528524543), ('total', 1190), ('HasAns_exact', 43.529411764705884), ('HasAns_f1', 63.88021528524543), ('HasAns_total', 1190), ('best_exact', 43.529411764705884), ('best_exact_thresh', 0.0), ('best_f1', 63.88021528524543), ('best_f1_thresh', 0.0)])
01/06/2022 17:21:34 - INFO - __main__ -   Language adapter for zh not found, using en instead
01/06/2022 17:21:34 - INFO - __main__ -   Set active language adapter to en
01/06/2022 17:21:34 - INFO - __main__ -   Args Adapter Weight = None
01/06/2022 17:21:34 - INFO - __main__ -   Adapter Languages = ['en']
01/06/2022 17:21:34 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
  0%|          | 0/48 [00:00<?, ?it/s] 75%|███████▌  | 36/48 [00:00<00:00, 350.70it/s]100%|██████████| 48/48 [00:00<00:00, 371.53it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<03:58,  4.99it/s]convert squad examples to features:  32%|███▏      | 385/1190 [00:00<00:01, 713.38it/s]convert squad examples to features: 100%|██████████| 1190/1190 [00:00<00:00, 1970.37it/s]/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|██████████| 1190/1190 [00:00<00:00, 592922.52it/s]
01/06/2022 17:21:35 - INFO - __main__ -   Local Rank = -1
01/06/2022 17:21:35 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.zh.json_bert-base-multilingual-cased_384_zh
01/06/2022 17:21:36 - INFO - __main__ -   ***** Running evaluation  *****
01/06/2022 17:21:36 - INFO - __main__ -     Num examples = 1374
01/06/2022 17:21:36 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/172 [00:00<?, ?it/s]01/06/2022 17:21:36 - INFO - __main__ -   Batch Number = 1
01/06/2022 17:21:36 - INFO - __main__ -   Batch Number = 2
01/06/2022 17:21:36 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|▏         | 3/172 [00:00<00:05, 29.49it/s]01/06/2022 17:21:36 - INFO - __main__ -   Batch Number = 4
01/06/2022 17:21:37 - INFO - __main__ -   Batch Number = 5
01/06/2022 17:21:37 - INFO - __main__ -   Batch Number = 6
01/06/2022 17:21:37 - INFO - __main__ -   Batch Number = 7
Evaluating:   4%|▍         | 7/172 [00:00<00:04, 33.91it/s]01/06/2022 17:21:37 - INFO - __main__ -   Batch Number = 8
01/06/2022 17:21:37 - INFO - __main__ -   Batch Number = 9
01/06/2022 17:21:37 - INFO - __main__ -   Batch Number = 10
01/06/2022 17:21:37 - INFO - __main__ -   Batch Number = 11
Evaluating:   6%|▋         | 11/172 [00:00<00:04, 36.17it/s]01/06/2022 17:21:37 - INFO - __main__ -   Batch Number = 12
01/06/2022 17:21:37 - INFO - __main__ -   Batch Number = 13
01/06/2022 17:21:37 - INFO - __main__ -   Batch Number = 14
01/06/2022 17:21:37 - INFO - __main__ -   Batch Number = 15
01/06/2022 17:21:37 - INFO - __main__ -   Batch Number = 16
Evaluating:   9%|▉         | 16/172 [00:00<00:03, 39.21it/s]01/06/2022 17:21:37 - INFO - __main__ -   Batch Number = 17
01/06/2022 17:21:37 - INFO - __main__ -   Batch Number = 18
01/06/2022 17:21:37 - INFO - __main__ -   Batch Number = 19
01/06/2022 17:21:37 - INFO - __main__ -   Batch Number = 20
01/06/2022 17:21:37 - INFO - __main__ -   Batch Number = 21
Evaluating:  12%|█▏        | 21/172 [00:00<00:03, 40.93it/s]01/06/2022 17:21:37 - INFO - __main__ -   Batch Number = 22
01/06/2022 17:21:37 - INFO - __main__ -   Batch Number = 23
01/06/2022 17:21:37 - INFO - __main__ -   Batch Number = 24
01/06/2022 17:21:37 - INFO - __main__ -   Batch Number = 25
01/06/2022 17:21:37 - INFO - __main__ -   Batch Number = 26
Evaluating:  15%|█▌        | 26/172 [00:00<00:03, 42.30it/s]01/06/2022 17:21:37 - INFO - __main__ -   Batch Number = 27
01/06/2022 17:21:37 - INFO - __main__ -   Batch Number = 28
01/06/2022 17:21:37 - INFO - __main__ -   Batch Number = 29
01/06/2022 17:21:37 - INFO - __main__ -   Batch Number = 30
01/06/2022 17:21:37 - INFO - __main__ -   Batch Number = 31
Evaluating:  18%|█▊        | 31/172 [00:00<00:03, 43.02it/s]01/06/2022 17:21:37 - INFO - __main__ -   Batch Number = 32
01/06/2022 17:21:37 - INFO - __main__ -   Batch Number = 33
01/06/2022 17:21:37 - INFO - __main__ -   Batch Number = 34
01/06/2022 17:21:37 - INFO - __main__ -   Batch Number = 35
01/06/2022 17:21:37 - INFO - __main__ -   Batch Number = 36
Evaluating:  21%|██        | 36/172 [00:00<00:03, 43.31it/s]01/06/2022 17:21:37 - INFO - __main__ -   Batch Number = 37
01/06/2022 17:21:37 - INFO - __main__ -   Batch Number = 38
01/06/2022 17:21:37 - INFO - __main__ -   Batch Number = 39
01/06/2022 17:21:37 - INFO - __main__ -   Batch Number = 40
01/06/2022 17:21:37 - INFO - __main__ -   Batch Number = 41
Evaluating:  24%|██▍       | 41/172 [00:00<00:03, 43.60it/s]01/06/2022 17:21:37 - INFO - __main__ -   Batch Number = 42
01/06/2022 17:21:37 - INFO - __main__ -   Batch Number = 43
01/06/2022 17:21:37 - INFO - __main__ -   Batch Number = 44
01/06/2022 17:21:37 - INFO - __main__ -   Batch Number = 45
01/06/2022 17:21:37 - INFO - __main__ -   Batch Number = 46
Evaluating:  27%|██▋       | 46/172 [00:01<00:02, 43.94it/s]01/06/2022 17:21:37 - INFO - __main__ -   Batch Number = 47
01/06/2022 17:21:38 - INFO - __main__ -   Batch Number = 48
01/06/2022 17:21:38 - INFO - __main__ -   Batch Number = 49
01/06/2022 17:21:38 - INFO - __main__ -   Batch Number = 50
01/06/2022 17:21:38 - INFO - __main__ -   Batch Number = 51
Evaluating:  30%|██▉       | 51/172 [00:01<00:02, 44.02it/s]01/06/2022 17:21:38 - INFO - __main__ -   Batch Number = 52
01/06/2022 17:21:38 - INFO - __main__ -   Batch Number = 53
01/06/2022 17:21:38 - INFO - __main__ -   Batch Number = 54
01/06/2022 17:21:38 - INFO - __main__ -   Batch Number = 55
01/06/2022 17:21:38 - INFO - __main__ -   Batch Number = 56
Evaluating:  33%|███▎      | 56/172 [00:01<00:02, 44.15it/s]01/06/2022 17:21:38 - INFO - __main__ -   Batch Number = 57
01/06/2022 17:21:38 - INFO - __main__ -   Batch Number = 58
01/06/2022 17:21:38 - INFO - __main__ -   Batch Number = 59
01/06/2022 17:21:38 - INFO - __main__ -   Batch Number = 60
01/06/2022 17:21:38 - INFO - __main__ -   Batch Number = 61
Evaluating:  35%|███▌      | 61/172 [00:01<00:02, 44.29it/s]01/06/2022 17:21:38 - INFO - __main__ -   Batch Number = 62
01/06/2022 17:21:38 - INFO - __main__ -   Batch Number = 63
01/06/2022 17:21:38 - INFO - __main__ -   Batch Number = 64
01/06/2022 17:21:38 - INFO - __main__ -   Batch Number = 65
01/06/2022 17:21:38 - INFO - __main__ -   Batch Number = 66
Evaluating:  38%|███▊      | 66/172 [00:01<00:02, 44.26it/s]01/06/2022 17:21:38 - INFO - __main__ -   Batch Number = 67
01/06/2022 17:21:38 - INFO - __main__ -   Batch Number = 68
01/06/2022 17:21:38 - INFO - __main__ -   Batch Number = 69
01/06/2022 17:21:38 - INFO - __main__ -   Batch Number = 70
01/06/2022 17:21:38 - INFO - __main__ -   Batch Number = 71
Evaluating:  41%|████▏     | 71/172 [00:01<00:02, 44.41it/s]01/06/2022 17:21:38 - INFO - __main__ -   Batch Number = 72
01/06/2022 17:21:38 - INFO - __main__ -   Batch Number = 73
01/06/2022 17:21:38 - INFO - __main__ -   Batch Number = 74
01/06/2022 17:21:38 - INFO - __main__ -   Batch Number = 75
01/06/2022 17:21:38 - INFO - __main__ -   Batch Number = 76
Evaluating:  44%|████▍     | 76/172 [00:01<00:02, 44.28it/s]01/06/2022 17:21:38 - INFO - __main__ -   Batch Number = 77
01/06/2022 17:21:38 - INFO - __main__ -   Batch Number = 78
01/06/2022 17:21:38 - INFO - __main__ -   Batch Number = 79
01/06/2022 17:21:38 - INFO - __main__ -   Batch Number = 80
01/06/2022 17:21:38 - INFO - __main__ -   Batch Number = 81
Evaluating:  47%|████▋     | 81/172 [00:01<00:02, 44.22it/s]01/06/2022 17:21:38 - INFO - __main__ -   Batch Number = 82
01/06/2022 17:21:38 - INFO - __main__ -   Batch Number = 83
01/06/2022 17:21:38 - INFO - __main__ -   Batch Number = 84
01/06/2022 17:21:38 - INFO - __main__ -   Batch Number = 85
01/06/2022 17:21:38 - INFO - __main__ -   Batch Number = 86
Evaluating:  50%|█████     | 86/172 [00:02<00:01, 44.08it/s]01/06/2022 17:21:38 - INFO - __main__ -   Batch Number = 87
01/06/2022 17:21:38 - INFO - __main__ -   Batch Number = 88
01/06/2022 17:21:38 - INFO - __main__ -   Batch Number = 89
01/06/2022 17:21:38 - INFO - __main__ -   Batch Number = 90
01/06/2022 17:21:38 - INFO - __main__ -   Batch Number = 91
Evaluating:  53%|█████▎    | 91/172 [00:02<00:01, 44.08it/s]01/06/2022 17:21:38 - INFO - __main__ -   Batch Number = 92
01/06/2022 17:21:39 - INFO - __main__ -   Batch Number = 93
01/06/2022 17:21:39 - INFO - __main__ -   Batch Number = 94
01/06/2022 17:21:39 - INFO - __main__ -   Batch Number = 95
01/06/2022 17:21:39 - INFO - __main__ -   Batch Number = 96
Evaluating:  56%|█████▌    | 96/172 [00:02<00:01, 44.10it/s]01/06/2022 17:21:39 - INFO - __main__ -   Batch Number = 97
01/06/2022 17:21:39 - INFO - __main__ -   Batch Number = 98
01/06/2022 17:21:39 - INFO - __main__ -   Batch Number = 99
01/06/2022 17:21:39 - INFO - __main__ -   Batch Number = 100
01/06/2022 17:21:39 - INFO - __main__ -   Batch Number = 101
Evaluating:  59%|█████▊    | 101/172 [00:02<00:01, 44.10it/s]01/06/2022 17:21:39 - INFO - __main__ -   Batch Number = 102
01/06/2022 17:21:39 - INFO - __main__ -   Batch Number = 103
01/06/2022 17:21:39 - INFO - __main__ -   Batch Number = 104
01/06/2022 17:21:39 - INFO - __main__ -   Batch Number = 105
01/06/2022 17:21:39 - INFO - __main__ -   Batch Number = 106
Evaluating:  62%|██████▏   | 106/172 [00:02<00:01, 44.19it/s]01/06/2022 17:21:39 - INFO - __main__ -   Batch Number = 107
01/06/2022 17:21:39 - INFO - __main__ -   Batch Number = 108
01/06/2022 17:21:39 - INFO - __main__ -   Batch Number = 109
01/06/2022 17:21:39 - INFO - __main__ -   Batch Number = 110
01/06/2022 17:21:39 - INFO - __main__ -   Batch Number = 111
Evaluating:  65%|██████▍   | 111/172 [00:02<00:01, 44.18it/s]01/06/2022 17:21:39 - INFO - __main__ -   Batch Number = 112
01/06/2022 17:21:39 - INFO - __main__ -   Batch Number = 113
01/06/2022 17:21:39 - INFO - __main__ -   Batch Number = 114
01/06/2022 17:21:39 - INFO - __main__ -   Batch Number = 115
01/06/2022 17:21:39 - INFO - __main__ -   Batch Number = 116
Evaluating:  67%|██████▋   | 116/172 [00:02<00:01, 44.16it/s]01/06/2022 17:21:39 - INFO - __main__ -   Batch Number = 117
01/06/2022 17:21:39 - INFO - __main__ -   Batch Number = 118
01/06/2022 17:21:39 - INFO - __main__ -   Batch Number = 119
01/06/2022 17:21:39 - INFO - __main__ -   Batch Number = 120
01/06/2022 17:21:39 - INFO - __main__ -   Batch Number = 121
Evaluating:  70%|███████   | 121/172 [00:02<00:01, 44.34it/s]01/06/2022 17:21:39 - INFO - __main__ -   Batch Number = 122
01/06/2022 17:21:39 - INFO - __main__ -   Batch Number = 123
01/06/2022 17:21:39 - INFO - __main__ -   Batch Number = 124
01/06/2022 17:21:39 - INFO - __main__ -   Batch Number = 125
01/06/2022 17:21:39 - INFO - __main__ -   Batch Number = 126
Evaluating:  73%|███████▎  | 126/172 [00:02<00:01, 44.16it/s]01/06/2022 17:21:39 - INFO - __main__ -   Batch Number = 127
01/06/2022 17:21:39 - INFO - __main__ -   Batch Number = 128
01/06/2022 17:21:39 - INFO - __main__ -   Batch Number = 129
01/06/2022 17:21:39 - INFO - __main__ -   Batch Number = 130
01/06/2022 17:21:39 - INFO - __main__ -   Batch Number = 131
Evaluating:  76%|███████▌  | 131/172 [00:03<00:00, 44.19it/s]01/06/2022 17:21:39 - INFO - __main__ -   Batch Number = 132
01/06/2022 17:21:39 - INFO - __main__ -   Batch Number = 133
01/06/2022 17:21:39 - INFO - __main__ -   Batch Number = 134
01/06/2022 17:21:39 - INFO - __main__ -   Batch Number = 135
01/06/2022 17:21:39 - INFO - __main__ -   Batch Number = 136
Evaluating:  79%|███████▉  | 136/172 [00:03<00:00, 44.12it/s]01/06/2022 17:21:40 - INFO - __main__ -   Batch Number = 137
01/06/2022 17:21:40 - INFO - __main__ -   Batch Number = 138
01/06/2022 17:21:40 - INFO - __main__ -   Batch Number = 139
01/06/2022 17:21:40 - INFO - __main__ -   Batch Number = 140
01/06/2022 17:21:40 - INFO - __main__ -   Batch Number = 141
Evaluating:  82%|████████▏ | 141/172 [00:03<00:00, 42.53it/s]01/06/2022 17:21:40 - INFO - __main__ -   Batch Number = 142
01/06/2022 17:21:40 - INFO - __main__ -   Batch Number = 143
01/06/2022 17:21:40 - INFO - __main__ -   Batch Number = 144
01/06/2022 17:21:40 - INFO - __main__ -   Batch Number = 145
01/06/2022 17:21:40 - INFO - __main__ -   Batch Number = 146
Evaluating:  85%|████████▍ | 146/172 [00:03<00:00, 43.03it/s]01/06/2022 17:21:40 - INFO - __main__ -   Batch Number = 147
01/06/2022 17:21:40 - INFO - __main__ -   Batch Number = 148
01/06/2022 17:21:40 - INFO - __main__ -   Batch Number = 149
01/06/2022 17:21:40 - INFO - __main__ -   Batch Number = 150
01/06/2022 17:21:40 - INFO - __main__ -   Batch Number = 151
Evaluating:  88%|████████▊ | 151/172 [00:03<00:00, 43.44it/s]01/06/2022 17:21:40 - INFO - __main__ -   Batch Number = 152
01/06/2022 17:21:40 - INFO - __main__ -   Batch Number = 153
01/06/2022 17:21:40 - INFO - __main__ -   Batch Number = 154
01/06/2022 17:21:40 - INFO - __main__ -   Batch Number = 155
01/06/2022 17:21:40 - INFO - __main__ -   Batch Number = 156
Evaluating:  91%|█████████ | 156/172 [00:03<00:00, 43.69it/s]01/06/2022 17:21:40 - INFO - __main__ -   Batch Number = 157
01/06/2022 17:21:40 - INFO - __main__ -   Batch Number = 158
01/06/2022 17:21:40 - INFO - __main__ -   Batch Number = 159
01/06/2022 17:21:40 - INFO - __main__ -   Batch Number = 160
01/06/2022 17:21:40 - INFO - __main__ -   Batch Number = 161
Evaluating:  94%|█████████▎| 161/172 [00:03<00:00, 43.77it/s]01/06/2022 17:21:40 - INFO - __main__ -   Batch Number = 162
01/06/2022 17:21:40 - INFO - __main__ -   Batch Number = 163
01/06/2022 17:21:40 - INFO - __main__ -   Batch Number = 164
01/06/2022 17:21:40 - INFO - __main__ -   Batch Number = 165
01/06/2022 17:21:40 - INFO - __main__ -   Batch Number = 166
Evaluating:  97%|█████████▋| 166/172 [00:03<00:00, 44.02it/s]01/06/2022 17:21:40 - INFO - __main__ -   Batch Number = 167
01/06/2022 17:21:40 - INFO - __main__ -   Batch Number = 168
01/06/2022 17:21:40 - INFO - __main__ -   Batch Number = 169
01/06/2022 17:21:40 - INFO - __main__ -   Batch Number = 170
01/06/2022 17:21:40 - INFO - __main__ -   Batch Number = 171
Evaluating:  99%|█████████▉| 171/172 [00:03<00:00, 43.96it/s]01/06/2022 17:21:40 - INFO - __main__ -   Batch Number = 172
Evaluating: 100%|██████████| 172/172 [00:03<00:00, 43.39it/s]
01/06/2022 17:21:40 - INFO - __main__ -     Evaluation done in total 3.964153 secs (0.002885 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/predictions_zh_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_bert-base-multilingual-cased_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8/nbest_predictions_zh_.json
01/06/2022 17:21:46 - INFO - __main__ -   Results = OrderedDict([('exact', 42.94117647058823), ('f1', 54.713333818375794), ('total', 1190), ('HasAns_exact', 42.94117647058823), ('HasAns_f1', 54.713333818375794), ('HasAns_total', 1190), ('best_exact', 42.94117647058823), ('best_exact_thresh', 0.0), ('best_f1', 54.713333818375794), ('best_f1_thresh', 0.0)])
