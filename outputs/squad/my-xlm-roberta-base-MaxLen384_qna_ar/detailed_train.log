PyTorch version 1.10.0+cu102 available.
01/08/2022 13:28:44 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/08/2022 13:28:44 - INFO - root -   save model
loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6
Model config XLMRobertaConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /home/abhijeet/.cache/torch/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8
loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/08/2022 13:28:54 - INFO - __main__ -   lang2id = None
01/08/2022 13:28:57 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='xlm-roberta-base', model_type='xlm-roberta', config_name=None, tokenizer_name=None, cache_dir=None, data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//xquad', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_ar/', max_seq_length=384, train_file='/home/abhijeet/rohan/cloud-emea-copy/data//xquad/train-v1.1.json', predict_file='/home/abhijeet/rohan/cloud-emea-copy/data//xquad/dev-v1.1.json', eval_test_set=False, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=4, per_gpu_eval_batch_size=8, gradient_accumulation_steps=4, learning_rate=0.0001, weight_decay=0.0001, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=15.0, max_steps=-1, save_steps=1000, warmup_steps=500, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', eval_lang='en', predict_langs='en', train_lang='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_ar//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='qna', threads=8, version_2_with_negative=False, verbose_logging=False, n_best_size=20, max_query_length=64, max_answer_length=30, doc_stride=128, null_score_diff_threshold=0.0, predict_task_adapter='output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s1/checkpoint-best/qna', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix=None, en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/08/2022 13:28:57 - INFO - __main__ -   Results: {}
loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6
Model config XLMRobertaConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /home/abhijeet/.cache/torch/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8
loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/08/2022 13:29:07 - INFO - __main__ -   lang2id = None
01/08/2022 13:29:07 - INFO - __main__ -   Evaluating the model on the test set of all languages specified
01/08/2022 13:29:07 - INFO - __main__ -   Adapter will be loaded from this path: output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s1/checkpoint-best/qna
01/08/2022 13:29:07 - INFO - root -   Trying to decide if add adapter
01/08/2022 13:29:07 - INFO - root -   loading task adapter
Loading module configuration from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s1/checkpoint-best/qna/adapter_config.json
Adding adapter 'qna' of type 'text_task'.
Loading module weights from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s1/checkpoint-best/qna/pytorch_adapter.bin
Loading module configuration from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s1/checkpoint-best/qna/head_config.json
Loading module weights from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s1/checkpoint-best/qna/pytorch_model_head.bin
01/08/2022 13:29:07 - INFO - root -   loading lang adpater ar/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/ar/xlm-roberta-base/pfeiffer/ar_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1f4018c6b187912fb6b47e664a34880506aef8c7320e0135939626a97e0bb137-4b1153cd3ebfdce09e1023c01e3ee723c32a0a57bea16e5ff13609f97eb2af0b-extracted/adapter_config.json
Adding adapter 'ar' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/1f4018c6b187912fb6b47e664a34880506aef8c7320e0135939626a97e0bb137-4b1153cd3ebfdce09e1023c01e3ee723c32a0a57bea16e5ff13609f97eb2af0b-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/1f4018c6b187912fb6b47e664a34880506aef8c7320e0135939626a97e0bb137-4b1153cd3ebfdce09e1023c01e3ee723c32a0a57bea16e5ff13609f97eb2af0b-extracted'
01/08/2022 13:29:09 - INFO - __main__ -   Language adapter for en not found, using ar instead
01/08/2022 13:29:09 - INFO - __main__ -   Set active language adapter to ar
01/08/2022 13:29:09 - INFO - __main__ -   Args Adapter Weight = None
01/08/2022 13:29:09 - INFO - __main__ -   Adapter Languages = ['ar']
01/08/2022 13:29:09 - INFO - __main__ -   Predict File = xquad.en.json
01/08/2022 13:29:09 - INFO - __main__ -   Creating features from dataset file at /home/abhijeet/rohan/cloud-emea-copy/data//xquad
ar ar/wiki@ukp
  0%|          | 0/48 [00:00<?, ?it/s] 31%|███▏      | 15/48 [00:00<00:00, 148.99it/s] 62%|██████▎   | 30/48 [00:00<00:00, 109.91it/s] 92%|█████████▏| 44/48 [00:00<00:00, 118.98it/s]100%|██████████| 48/48 [00:00<00:00, 121.51it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<03:52,  5.11it/s]convert squad examples to features:  27%|██▋       | 321/1190 [00:00<00:00, 1260.00it/s]convert squad examples to features:  41%|████      | 482/1190 [00:00<00:01, 587.23it/s] convert squad examples to features:  97%|█████████▋| 1153/1190 [00:00<00:00, 1642.81it/s]convert squad examples to features: 100%|██████████| 1190/1190 [00:00<00:00, 1314.68it/s]/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|██████████| 1190/1190 [00:00<00:00, 343275.22it/s]
01/08/2022 13:29:11 - INFO - __main__ -   Local Rank = -1
01/08/2022 13:29:11 - INFO - __main__ -   Saving features into cached file /home/abhijeet/rohan/cloud-emea-copy/data//xquad/cached_xquad.en.json_xlm-roberta-base_384_en
01/08/2022 13:29:12 - INFO - __main__ -   ***** Running evaluation  *****
01/08/2022 13:29:12 - INFO - __main__ -     Num examples = 1270
01/08/2022 13:29:12 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/159 [00:00<?, ?it/s]01/08/2022 13:29:12 - INFO - __main__ -   Batch Number = 1
Evaluating:   1%|          | 1/159 [00:00<00:24,  6.56it/s]01/08/2022 13:29:13 - INFO - __main__ -   Batch Number = 2
Evaluating:   1%|▏         | 2/159 [00:00<00:21,  7.22it/s]01/08/2022 13:29:13 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|▏         | 3/159 [00:00<00:20,  7.58it/s]01/08/2022 13:29:13 - INFO - __main__ -   Batch Number = 4
Evaluating:   3%|▎         | 4/159 [00:00<00:19,  7.84it/s]01/08/2022 13:29:13 - INFO - __main__ -   Batch Number = 5
Evaluating:   3%|▎         | 5/159 [00:00<00:19,  8.03it/s]01/08/2022 13:29:13 - INFO - __main__ -   Batch Number = 6
Evaluating:   4%|▍         | 6/159 [00:00<00:18,  8.13it/s]01/08/2022 13:29:13 - INFO - __main__ -   Batch Number = 7
Evaluating:   4%|▍         | 7/159 [00:00<00:18,  8.19it/s]01/08/2022 13:29:13 - INFO - __main__ -   Batch Number = 8
Evaluating:   5%|▌         | 8/159 [00:01<00:18,  8.24it/s]01/08/2022 13:29:13 - INFO - __main__ -   Batch Number = 9
Evaluating:   6%|▌         | 9/159 [00:01<00:18,  8.25it/s]01/08/2022 13:29:14 - INFO - __main__ -   Batch Number = 10
Evaluating:   6%|▋         | 10/159 [00:01<00:18,  8.24it/s]01/08/2022 13:29:14 - INFO - __main__ -   Batch Number = 11
Evaluating:   7%|▋         | 11/159 [00:01<00:17,  8.26it/s]01/08/2022 13:29:14 - INFO - __main__ -   Batch Number = 12
Evaluating:   8%|▊         | 12/159 [00:01<00:17,  8.28it/s]01/08/2022 13:29:14 - INFO - __main__ -   Batch Number = 13
Evaluating:   8%|▊         | 13/159 [00:01<00:17,  8.29it/s]01/08/2022 13:29:14 - INFO - __main__ -   Batch Number = 14
Evaluating:   9%|▉         | 14/159 [00:01<00:17,  8.23it/s]01/08/2022 13:29:14 - INFO - __main__ -   Batch Number = 15
Evaluating:   9%|▉         | 15/159 [00:01<00:17,  8.25it/s]01/08/2022 13:29:14 - INFO - __main__ -   Batch Number = 16
Evaluating:  10%|█         | 16/159 [00:01<00:17,  8.27it/s]01/08/2022 13:29:14 - INFO - __main__ -   Batch Number = 17
Evaluating:  11%|█         | 17/159 [00:02<00:17,  8.29it/s]01/08/2022 13:29:15 - INFO - __main__ -   Batch Number = 18
Evaluating:  11%|█▏        | 18/159 [00:02<00:17,  8.29it/s]01/08/2022 13:29:15 - INFO - __main__ -   Batch Number = 19
Evaluating:  12%|█▏        | 19/159 [00:02<00:16,  8.29it/s]01/08/2022 13:29:15 - INFO - __main__ -   Batch Number = 20
Evaluating:  13%|█▎        | 20/159 [00:02<00:16,  8.30it/s]01/08/2022 13:29:15 - INFO - __main__ -   Batch Number = 21
Evaluating:  13%|█▎        | 21/159 [00:02<00:16,  8.30it/s]01/08/2022 13:29:15 - INFO - __main__ -   Batch Number = 22
Evaluating:  14%|█▍        | 22/159 [00:02<00:16,  8.31it/s]01/08/2022 13:29:15 - INFO - __main__ -   Batch Number = 23
Evaluating:  14%|█▍        | 23/159 [00:02<00:16,  8.31it/s]01/08/2022 13:29:15 - INFO - __main__ -   Batch Number = 24
Evaluating:  15%|█▌        | 24/159 [00:02<00:16,  8.30it/s]01/08/2022 13:29:15 - INFO - __main__ -   Batch Number = 25
Evaluating:  16%|█▌        | 25/159 [00:03<00:16,  8.30it/s]01/08/2022 13:29:15 - INFO - __main__ -   Batch Number = 26
Evaluating:  16%|█▋        | 26/159 [00:03<00:16,  8.30it/s]01/08/2022 13:29:16 - INFO - __main__ -   Batch Number = 27
Evaluating:  17%|█▋        | 27/159 [00:03<00:18,  7.23it/s]01/08/2022 13:29:16 - INFO - __main__ -   Batch Number = 28
Evaluating:  18%|█▊        | 28/159 [00:03<00:17,  7.52it/s]01/08/2022 13:29:16 - INFO - __main__ -   Batch Number = 29
Evaluating:  18%|█▊        | 29/159 [00:03<00:17,  7.27it/s]01/08/2022 13:29:16 - INFO - __main__ -   Batch Number = 30
Evaluating:  19%|█▉        | 30/159 [00:03<00:17,  7.55it/s]01/08/2022 13:29:16 - INFO - __main__ -   Batch Number = 31
Evaluating:  19%|█▉        | 31/159 [00:03<00:16,  7.77it/s]01/08/2022 13:29:16 - INFO - __main__ -   Batch Number = 32
Evaluating:  20%|██        | 32/159 [00:03<00:16,  7.92it/s]01/08/2022 13:29:16 - INFO - __main__ -   Batch Number = 33
Evaluating:  21%|██        | 33/159 [00:04<00:15,  8.02it/s]01/08/2022 13:29:17 - INFO - __main__ -   Batch Number = 34
Evaluating:  21%|██▏       | 34/159 [00:04<00:15,  8.09it/s]01/08/2022 13:29:17 - INFO - __main__ -   Batch Number = 35
Evaluating:  22%|██▏       | 35/159 [00:04<00:15,  8.15it/s]01/08/2022 13:29:17 - INFO - __main__ -   Batch Number = 36
Evaluating:  23%|██▎       | 36/159 [00:04<00:15,  8.19it/s]01/08/2022 13:29:17 - INFO - __main__ -   Batch Number = 37
Evaluating:  23%|██▎       | 37/159 [00:04<00:14,  8.21it/s]01/08/2022 13:29:17 - INFO - __main__ -   Batch Number = 38
Evaluating:  24%|██▍       | 38/159 [00:04<00:14,  8.24it/s]01/08/2022 13:29:17 - INFO - __main__ -   Batch Number = 39
Evaluating:  25%|██▍       | 39/159 [00:04<00:14,  8.25it/s]01/08/2022 13:29:17 - INFO - __main__ -   Batch Number = 40
Evaluating:  25%|██▌       | 40/159 [00:04<00:14,  8.25it/s]01/08/2022 13:29:17 - INFO - __main__ -   Batch Number = 41
Evaluating:  26%|██▌       | 41/159 [00:05<00:14,  8.26it/s]01/08/2022 13:29:17 - INFO - __main__ -   Batch Number = 42
Evaluating:  26%|██▋       | 42/159 [00:05<00:14,  8.26it/s]01/08/2022 13:29:18 - INFO - __main__ -   Batch Number = 43
Evaluating:  27%|██▋       | 43/159 [00:05<00:14,  8.27it/s]01/08/2022 13:29:18 - INFO - __main__ -   Batch Number = 44
Evaluating:  28%|██▊       | 44/159 [00:05<00:13,  8.21it/s]01/08/2022 13:29:18 - INFO - __main__ -   Batch Number = 45
Evaluating:  28%|██▊       | 45/159 [00:05<00:13,  8.24it/s]01/08/2022 13:29:18 - INFO - __main__ -   Batch Number = 46
Evaluating:  29%|██▉       | 46/159 [00:05<00:13,  8.27it/s]01/08/2022 13:29:18 - INFO - __main__ -   Batch Number = 47
Evaluating:  30%|██▉       | 47/159 [00:05<00:13,  8.28it/s]01/08/2022 13:29:18 - INFO - __main__ -   Batch Number = 48
Evaluating:  30%|███       | 48/159 [00:05<00:13,  8.28it/s]01/08/2022 13:29:18 - INFO - __main__ -   Batch Number = 49
Evaluating:  31%|███       | 49/159 [00:06<00:13,  8.28it/s]01/08/2022 13:29:18 - INFO - __main__ -   Batch Number = 50
Evaluating:  31%|███▏      | 50/159 [00:06<00:13,  8.28it/s]01/08/2022 13:29:19 - INFO - __main__ -   Batch Number = 51
Evaluating:  32%|███▏      | 51/159 [00:06<00:13,  8.27it/s]01/08/2022 13:29:19 - INFO - __main__ -   Batch Number = 52
Evaluating:  33%|███▎      | 52/159 [00:06<00:12,  8.27it/s]01/08/2022 13:29:19 - INFO - __main__ -   Batch Number = 53
Evaluating:  33%|███▎      | 53/159 [00:06<00:12,  8.27it/s]01/08/2022 13:29:19 - INFO - __main__ -   Batch Number = 54
Evaluating:  34%|███▍      | 54/159 [00:06<00:12,  8.27it/s]01/08/2022 13:29:19 - INFO - __main__ -   Batch Number = 55
Evaluating:  35%|███▍      | 55/159 [00:06<00:12,  8.27it/s]01/08/2022 13:29:19 - INFO - __main__ -   Batch Number = 56
Evaluating:  35%|███▌      | 56/159 [00:06<00:12,  8.27it/s]01/08/2022 13:29:19 - INFO - __main__ -   Batch Number = 57
Evaluating:  36%|███▌      | 57/159 [00:07<00:12,  8.28it/s]01/08/2022 13:29:19 - INFO - __main__ -   Batch Number = 58
Evaluating:  36%|███▋      | 58/159 [00:07<00:12,  8.28it/s]01/08/2022 13:29:20 - INFO - __main__ -   Batch Number = 59
Evaluating:  37%|███▋      | 59/159 [00:07<00:12,  8.26it/s]01/08/2022 13:29:20 - INFO - __main__ -   Batch Number = 60
Evaluating:  38%|███▊      | 60/159 [00:07<00:11,  8.27it/s]01/08/2022 13:29:20 - INFO - __main__ -   Batch Number = 61
Evaluating:  38%|███▊      | 61/159 [00:07<00:11,  8.26it/s]01/08/2022 13:29:20 - INFO - __main__ -   Batch Number = 62
Evaluating:  39%|███▉      | 62/159 [00:07<00:11,  8.25it/s]01/08/2022 13:29:20 - INFO - __main__ -   Batch Number = 63
Evaluating:  40%|███▉      | 63/159 [00:07<00:11,  8.28it/s]01/08/2022 13:29:20 - INFO - __main__ -   Batch Number = 64
Evaluating:  40%|████      | 64/159 [00:07<00:11,  8.29it/s]01/08/2022 13:29:20 - INFO - __main__ -   Batch Number = 65
Evaluating:  41%|████      | 65/159 [00:07<00:11,  8.27it/s]01/08/2022 13:29:20 - INFO - __main__ -   Batch Number = 66
Evaluating:  42%|████▏     | 66/159 [00:08<00:11,  8.27it/s]01/08/2022 13:29:21 - INFO - __main__ -   Batch Number = 67
Evaluating:  42%|████▏     | 67/159 [00:08<00:11,  8.27it/s]01/08/2022 13:29:21 - INFO - __main__ -   Batch Number = 68
Evaluating:  43%|████▎     | 68/159 [00:08<00:11,  7.58it/s]01/08/2022 13:29:21 - INFO - __main__ -   Batch Number = 69
Evaluating:  43%|████▎     | 69/159 [00:08<00:11,  7.79it/s]01/08/2022 13:29:21 - INFO - __main__ -   Batch Number = 70
Evaluating:  44%|████▍     | 70/159 [00:08<00:11,  7.94it/s]01/08/2022 13:29:21 - INFO - __main__ -   Batch Number = 71
Evaluating:  45%|████▍     | 71/159 [00:08<00:10,  8.05it/s]01/08/2022 13:29:21 - INFO - __main__ -   Batch Number = 72
Evaluating:  45%|████▌     | 72/159 [00:08<00:10,  8.13it/s]01/08/2022 13:29:21 - INFO - __main__ -   Batch Number = 73
Evaluating:  46%|████▌     | 73/159 [00:08<00:10,  8.18it/s]01/08/2022 13:29:21 - INFO - __main__ -   Batch Number = 74
Evaluating:  47%|████▋     | 74/159 [00:09<00:10,  8.20it/s]01/08/2022 13:29:22 - INFO - __main__ -   Batch Number = 75
Evaluating:  47%|████▋     | 75/159 [00:09<00:10,  8.22it/s]01/08/2022 13:29:22 - INFO - __main__ -   Batch Number = 76
Evaluating:  48%|████▊     | 76/159 [00:09<00:10,  8.22it/s]01/08/2022 13:29:22 - INFO - __main__ -   Batch Number = 77
Evaluating:  48%|████▊     | 77/159 [00:09<00:09,  8.23it/s]01/08/2022 13:29:22 - INFO - __main__ -   Batch Number = 78
Evaluating:  49%|████▉     | 78/159 [00:09<00:09,  8.24it/s]01/08/2022 13:29:22 - INFO - __main__ -   Batch Number = 79
Evaluating:  50%|████▉     | 79/159 [00:09<00:09,  8.26it/s]01/08/2022 13:29:22 - INFO - __main__ -   Batch Number = 80
Evaluating:  50%|█████     | 80/159 [00:09<00:09,  8.28it/s]01/08/2022 13:29:22 - INFO - __main__ -   Batch Number = 81
Evaluating:  51%|█████     | 81/159 [00:09<00:09,  8.27it/s]01/08/2022 13:29:22 - INFO - __main__ -   Batch Number = 82
Evaluating:  52%|█████▏    | 82/159 [00:10<00:09,  8.25it/s]01/08/2022 13:29:22 - INFO - __main__ -   Batch Number = 83
Evaluating:  52%|█████▏    | 83/159 [00:10<00:09,  8.25it/s]01/08/2022 13:29:23 - INFO - __main__ -   Batch Number = 84
Evaluating:  53%|█████▎    | 84/159 [00:10<00:09,  8.25it/s]01/08/2022 13:29:23 - INFO - __main__ -   Batch Number = 85
Evaluating:  53%|█████▎    | 85/159 [00:10<00:08,  8.26it/s]01/08/2022 13:29:23 - INFO - __main__ -   Batch Number = 86
Evaluating:  54%|█████▍    | 86/159 [00:10<00:08,  8.25it/s]01/08/2022 13:29:23 - INFO - __main__ -   Batch Number = 87
Evaluating:  55%|█████▍    | 87/159 [00:10<00:08,  8.23it/s]01/08/2022 13:29:23 - INFO - __main__ -   Batch Number = 88
Evaluating:  55%|█████▌    | 88/159 [00:10<00:08,  8.22it/s]01/08/2022 13:29:23 - INFO - __main__ -   Batch Number = 89
Evaluating:  56%|█████▌    | 89/159 [00:10<00:08,  8.21it/s]01/08/2022 13:29:23 - INFO - __main__ -   Batch Number = 90
Evaluating:  57%|█████▋    | 90/159 [00:11<00:08,  8.20it/s]01/08/2022 13:29:23 - INFO - __main__ -   Batch Number = 91
Evaluating:  57%|█████▋    | 91/159 [00:11<00:08,  8.21it/s]01/08/2022 13:29:24 - INFO - __main__ -   Batch Number = 92
Evaluating:  58%|█████▊    | 92/159 [00:11<00:08,  8.22it/s]01/08/2022 13:29:24 - INFO - __main__ -   Batch Number = 93
Evaluating:  58%|█████▊    | 93/159 [00:11<00:08,  8.23it/s]01/08/2022 13:29:24 - INFO - __main__ -   Batch Number = 94
Evaluating:  59%|█████▉    | 94/159 [00:11<00:07,  8.23it/s]01/08/2022 13:29:24 - INFO - __main__ -   Batch Number = 95
Evaluating:  60%|█████▉    | 95/159 [00:11<00:07,  8.24it/s]01/08/2022 13:29:24 - INFO - __main__ -   Batch Number = 96
Evaluating:  60%|██████    | 96/159 [00:11<00:07,  8.25it/s]01/08/2022 13:29:24 - INFO - __main__ -   Batch Number = 97
Evaluating:  61%|██████    | 97/159 [00:11<00:07,  8.26it/s]01/08/2022 13:29:24 - INFO - __main__ -   Batch Number = 98
Evaluating:  62%|██████▏   | 98/159 [00:12<00:07,  8.26it/s]01/08/2022 13:29:24 - INFO - __main__ -   Batch Number = 99
Evaluating:  62%|██████▏   | 99/159 [00:12<00:07,  8.26it/s]01/08/2022 13:29:25 - INFO - __main__ -   Batch Number = 100
Evaluating:  63%|██████▎   | 100/159 [00:12<00:07,  8.27it/s]01/08/2022 13:29:25 - INFO - __main__ -   Batch Number = 101
Evaluating:  64%|██████▎   | 101/159 [00:12<00:07,  8.27it/s]01/08/2022 13:29:25 - INFO - __main__ -   Batch Number = 102
Evaluating:  64%|██████▍   | 102/159 [00:12<00:06,  8.26it/s]01/08/2022 13:29:25 - INFO - __main__ -   Batch Number = 103
Evaluating:  65%|██████▍   | 103/159 [00:12<00:06,  8.26it/s]01/08/2022 13:29:25 - INFO - __main__ -   Batch Number = 104
Evaluating:  65%|██████▌   | 104/159 [00:12<00:06,  8.25it/s]01/08/2022 13:29:25 - INFO - __main__ -   Batch Number = 105
Evaluating:  66%|██████▌   | 105/159 [00:12<00:06,  8.26it/s]01/08/2022 13:29:25 - INFO - __main__ -   Batch Number = 106
Evaluating:  67%|██████▋   | 106/159 [00:12<00:06,  8.26it/s]01/08/2022 13:29:25 - INFO - __main__ -   Batch Number = 107
Evaluating:  67%|██████▋   | 107/159 [00:13<00:06,  8.27it/s]01/08/2022 13:29:26 - INFO - __main__ -   Batch Number = 108
Evaluating:  68%|██████▊   | 108/159 [00:13<00:06,  8.26it/s]01/08/2022 13:29:26 - INFO - __main__ -   Batch Number = 109
Evaluating:  69%|██████▊   | 109/159 [00:13<00:07,  6.73it/s]01/08/2022 13:29:26 - INFO - __main__ -   Batch Number = 110
Evaluating:  69%|██████▉   | 110/159 [00:13<00:06,  7.13it/s]01/08/2022 13:29:26 - INFO - __main__ -   Batch Number = 111
Evaluating:  70%|██████▉   | 111/159 [00:13<00:06,  7.44it/s]01/08/2022 13:29:26 - INFO - __main__ -   Batch Number = 112
Evaluating:  70%|███████   | 112/159 [00:13<00:06,  7.68it/s]01/08/2022 13:29:26 - INFO - __main__ -   Batch Number = 113
Evaluating:  71%|███████   | 113/159 [00:13<00:05,  7.84it/s]01/08/2022 13:29:26 - INFO - __main__ -   Batch Number = 114
Evaluating:  72%|███████▏  | 114/159 [00:14<00:05,  7.97it/s]01/08/2022 13:29:26 - INFO - __main__ -   Batch Number = 115
Evaluating:  72%|███████▏  | 115/159 [00:14<00:05,  8.05it/s]01/08/2022 13:29:27 - INFO - __main__ -   Batch Number = 116
Evaluating:  73%|███████▎  | 116/159 [00:14<00:05,  8.09it/s]01/08/2022 13:29:27 - INFO - __main__ -   Batch Number = 117
Evaluating:  74%|███████▎  | 117/159 [00:14<00:05,  8.13it/s]01/08/2022 13:29:27 - INFO - __main__ -   Batch Number = 118
Evaluating:  74%|███████▍  | 118/159 [00:14<00:05,  8.17it/s]01/08/2022 13:29:27 - INFO - __main__ -   Batch Number = 119
Evaluating:  75%|███████▍  | 119/159 [00:14<00:04,  8.19it/s]01/08/2022 13:29:27 - INFO - __main__ -   Batch Number = 120
Evaluating:  75%|███████▌  | 120/159 [00:14<00:04,  8.22it/s]01/08/2022 13:29:27 - INFO - __main__ -   Batch Number = 121
Evaluating:  76%|███████▌  | 121/159 [00:14<00:04,  8.23it/s]01/08/2022 13:29:27 - INFO - __main__ -   Batch Number = 122
Evaluating:  77%|███████▋  | 122/159 [00:15<00:04,  8.25it/s]01/08/2022 13:29:27 - INFO - __main__ -   Batch Number = 123
Evaluating:  77%|███████▋  | 123/159 [00:15<00:04,  8.26it/s]01/08/2022 13:29:28 - INFO - __main__ -   Batch Number = 124
Evaluating:  78%|███████▊  | 124/159 [00:15<00:04,  8.26it/s]01/08/2022 13:29:28 - INFO - __main__ -   Batch Number = 125
Evaluating:  79%|███████▊  | 125/159 [00:15<00:04,  8.26it/s]01/08/2022 13:29:28 - INFO - __main__ -   Batch Number = 126
Evaluating:  79%|███████▉  | 126/159 [00:15<00:03,  8.26it/s]01/08/2022 13:29:28 - INFO - __main__ -   Batch Number = 127
Evaluating:  80%|███████▉  | 127/159 [00:15<00:03,  8.26it/s]01/08/2022 13:29:28 - INFO - __main__ -   Batch Number = 128
Evaluating:  81%|████████  | 128/159 [00:15<00:03,  8.25it/s]01/08/2022 13:29:28 - INFO - __main__ -   Batch Number = 129
Evaluating:  81%|████████  | 129/159 [00:15<00:03,  8.25it/s]01/08/2022 13:29:28 - INFO - __main__ -   Batch Number = 130
Evaluating:  82%|████████▏ | 130/159 [00:15<00:03,  8.25it/s]01/08/2022 13:29:28 - INFO - __main__ -   Batch Number = 131
Evaluating:  82%|████████▏ | 131/159 [00:16<00:03,  8.25it/s]01/08/2022 13:29:29 - INFO - __main__ -   Batch Number = 132
Evaluating:  83%|████████▎ | 132/159 [00:16<00:03,  8.25it/s]01/08/2022 13:29:29 - INFO - __main__ -   Batch Number = 133
Evaluating:  84%|████████▎ | 133/159 [00:16<00:03,  8.25it/s]01/08/2022 13:29:29 - INFO - __main__ -   Batch Number = 134
Evaluating:  84%|████████▍ | 134/159 [00:16<00:03,  8.23it/s]01/08/2022 13:29:29 - INFO - __main__ -   Batch Number = 135
Evaluating:  85%|████████▍ | 135/159 [00:16<00:02,  8.22it/s]01/08/2022 13:29:29 - INFO - __main__ -   Batch Number = 136
Evaluating:  86%|████████▌ | 136/159 [00:16<00:02,  8.21it/s]01/08/2022 13:29:29 - INFO - __main__ -   Batch Number = 137
Evaluating:  86%|████████▌ | 137/159 [00:16<00:02,  8.20it/s]01/08/2022 13:29:29 - INFO - __main__ -   Batch Number = 138
Evaluating:  87%|████████▋ | 138/159 [00:16<00:02,  8.21it/s]01/08/2022 13:29:29 - INFO - __main__ -   Batch Number = 139
Evaluating:  87%|████████▋ | 139/159 [00:17<00:02,  8.22it/s]01/08/2022 13:29:29 - INFO - __main__ -   Batch Number = 140
Evaluating:  88%|████████▊ | 140/159 [00:17<00:02,  8.24it/s]01/08/2022 13:29:30 - INFO - __main__ -   Batch Number = 141
Evaluating:  89%|████████▊ | 141/159 [00:17<00:02,  8.25it/s]01/08/2022 13:29:30 - INFO - __main__ -   Batch Number = 142
Evaluating:  89%|████████▉ | 142/159 [00:17<00:02,  8.25it/s]01/08/2022 13:29:30 - INFO - __main__ -   Batch Number = 143
Evaluating:  90%|████████▉ | 143/159 [00:17<00:01,  8.25it/s]01/08/2022 13:29:30 - INFO - __main__ -   Batch Number = 144
Evaluating:  91%|█████████ | 144/159 [00:17<00:01,  8.25it/s]01/08/2022 13:29:30 - INFO - __main__ -   Batch Number = 145
Evaluating:  91%|█████████ | 145/159 [00:17<00:01,  8.24it/s]01/08/2022 13:29:30 - INFO - __main__ -   Batch Number = 146
Evaluating:  92%|█████████▏| 146/159 [00:17<00:01,  8.22it/s]01/08/2022 13:29:30 - INFO - __main__ -   Batch Number = 147
Evaluating:  92%|█████████▏| 147/159 [00:18<00:01,  8.23it/s]01/08/2022 13:29:30 - INFO - __main__ -   Batch Number = 148
Evaluating:  93%|█████████▎| 148/159 [00:18<00:01,  8.24it/s]01/08/2022 13:29:31 - INFO - __main__ -   Batch Number = 149
Evaluating:  94%|█████████▎| 149/159 [00:18<00:01,  8.23it/s]01/08/2022 13:29:31 - INFO - __main__ -   Batch Number = 150
Evaluating:  94%|█████████▍| 150/159 [00:18<00:01,  5.80it/s]01/08/2022 13:29:31 - INFO - __main__ -   Batch Number = 151
Evaluating:  95%|█████████▍| 151/159 [00:18<00:01,  6.37it/s]01/08/2022 13:29:31 - INFO - __main__ -   Batch Number = 152
Evaluating:  96%|█████████▌| 152/159 [00:18<00:01,  6.84it/s]01/08/2022 13:29:31 - INFO - __main__ -   Batch Number = 153
Evaluating:  96%|█████████▌| 153/159 [00:18<00:00,  7.21it/s]01/08/2022 13:29:31 - INFO - __main__ -   Batch Number = 154
Evaluating:  97%|█████████▋| 154/159 [00:19<00:00,  7.50it/s]01/08/2022 13:29:31 - INFO - __main__ -   Batch Number = 155
Evaluating:  97%|█████████▋| 155/159 [00:19<00:00,  7.71it/s]01/08/2022 13:29:32 - INFO - __main__ -   Batch Number = 156
Evaluating:  98%|█████████▊| 156/159 [00:19<00:00,  7.87it/s]01/08/2022 13:29:32 - INFO - __main__ -   Batch Number = 157
Evaluating:  99%|█████████▊| 157/159 [00:19<00:00,  7.97it/s]01/08/2022 13:29:32 - INFO - __main__ -   Batch Number = 158
Evaluating:  99%|█████████▉| 158/159 [00:19<00:00,  8.03it/s]01/08/2022 13:29:32 - INFO - __main__ -   Batch Number = 159
Evaluating: 100%|██████████| 159/159 [00:19<00:00,  8.10it/s]
01/08/2022 13:29:32 - INFO - __main__ -     Evaluation done in total 19.635913 secs (0.015461 sec per example)
Writing predictions to: /home/abhijeet/rohan/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_ar/predictions_en_.json
Writing nbest to: /home/abhijeet/rohan/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_ar/nbest_predictions_en_.json
01/08/2022 13:29:36 - INFO - __main__ -   Results = OrderedDict([('exact', 70.25210084033614), ('f1', 81.61278652789485), ('total', 1190), ('HasAns_exact', 70.25210084033614), ('HasAns_f1', 81.61278652789485), ('HasAns_total', 1190), ('best_exact', 70.25210084033614), ('best_exact_thresh', 0.0), ('best_f1', 81.61278652789485), ('best_f1_thresh', 0.0)])
PyTorch version 1.10.0+cu102 available.
01/08/2022 13:29:38 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/08/2022 13:29:38 - INFO - root -   save model
loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6
Model config XLMRobertaConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /home/abhijeet/.cache/torch/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8
loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/08/2022 13:29:48 - INFO - __main__ -   lang2id = None
01/08/2022 13:29:51 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='xlm-roberta-base', model_type='xlm-roberta', config_name=None, tokenizer_name=None, cache_dir=None, data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//xquad', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_ar/', max_seq_length=384, train_file='/home/abhijeet/rohan/cloud-emea-copy/data//xquad/train-v1.1.json', predict_file='/home/abhijeet/rohan/cloud-emea-copy/data//xquad/dev-v1.1.json', eval_test_set=False, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=4, per_gpu_eval_batch_size=8, gradient_accumulation_steps=4, learning_rate=0.0001, weight_decay=0.0001, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=15.0, max_steps=-1, save_steps=1000, warmup_steps=500, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', eval_lang='en', predict_langs='en', train_lang='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_ar//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='qna', threads=8, version_2_with_negative=False, verbose_logging=False, n_best_size=20, max_query_length=64, max_answer_length=30, doc_stride=128, null_score_diff_threshold=0.0, predict_task_adapter='output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s2/checkpoint-best/qna', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix=None, en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/08/2022 13:29:51 - INFO - __main__ -   Results: {}
loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6
Model config XLMRobertaConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /home/abhijeet/.cache/torch/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8
loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/08/2022 13:30:01 - INFO - __main__ -   lang2id = None
01/08/2022 13:30:01 - INFO - __main__ -   Evaluating the model on the test set of all languages specified
01/08/2022 13:30:01 - INFO - __main__ -   Adapter will be loaded from this path: output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s2/checkpoint-best/qna
01/08/2022 13:30:01 - INFO - root -   Trying to decide if add adapter
01/08/2022 13:30:01 - INFO - root -   loading task adapter
Loading module configuration from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s2/checkpoint-best/qna/adapter_config.json
Adding adapter 'qna' of type 'text_task'.
Loading module weights from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s2/checkpoint-best/qna/pytorch_adapter.bin
Loading module configuration from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s2/checkpoint-best/qna/head_config.json
Loading module weights from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s2/checkpoint-best/qna/pytorch_model_head.bin
01/08/2022 13:30:01 - INFO - root -   loading lang adpater ar/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/ar/xlm-roberta-base/pfeiffer/ar_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1f4018c6b187912fb6b47e664a34880506aef8c7320e0135939626a97e0bb137-4b1153cd3ebfdce09e1023c01e3ee723c32a0a57bea16e5ff13609f97eb2af0b-extracted/adapter_config.json
Adding adapter 'ar' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/1f4018c6b187912fb6b47e664a34880506aef8c7320e0135939626a97e0bb137-4b1153cd3ebfdce09e1023c01e3ee723c32a0a57bea16e5ff13609f97eb2af0b-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/1f4018c6b187912fb6b47e664a34880506aef8c7320e0135939626a97e0bb137-4b1153cd3ebfdce09e1023c01e3ee723c32a0a57bea16e5ff13609f97eb2af0b-extracted'
01/08/2022 13:30:03 - INFO - __main__ -   Language adapter for en not found, using ar instead
01/08/2022 13:30:03 - INFO - __main__ -   Set active language adapter to ar
01/08/2022 13:30:03 - INFO - __main__ -   Args Adapter Weight = None
01/08/2022 13:30:03 - INFO - __main__ -   Adapter Languages = ['ar']
01/08/2022 13:30:03 - INFO - __main__ -   Predict File = xquad.en.json
01/08/2022 13:30:03 - INFO - __main__ -   Creating features from dataset file at /home/abhijeet/rohan/cloud-emea-copy/data//xquad
ar ar/wiki@ukp
  0%|          | 0/48 [00:00<?, ?it/s] 31%|███▏      | 15/48 [00:00<00:00, 146.05it/s] 62%|██████▎   | 30/48 [00:00<00:00, 108.76it/s] 92%|█████████▏| 44/48 [00:00<00:00, 119.58it/s]100%|██████████| 48/48 [00:00<00:00, 121.51it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<03:32,  5.60it/s]convert squad examples to features:  27%|██▋       | 321/1190 [00:00<00:00, 1347.36it/s]convert squad examples to features:  41%|████      | 488/1190 [00:00<00:01, 603.92it/s] convert squad examples to features: 100%|██████████| 1190/1190 [00:00<00:00, 1377.80it/s]/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|██████████| 1190/1190 [00:00<00:00, 418025.27it/s]
01/08/2022 13:30:05 - INFO - __main__ -   Local Rank = -1
01/08/2022 13:30:05 - INFO - __main__ -   Saving features into cached file /home/abhijeet/rohan/cloud-emea-copy/data//xquad/cached_xquad.en.json_xlm-roberta-base_384_en
01/08/2022 13:30:06 - INFO - __main__ -   ***** Running evaluation  *****
01/08/2022 13:30:06 - INFO - __main__ -     Num examples = 1270
01/08/2022 13:30:06 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/159 [00:00<?, ?it/s]01/08/2022 13:30:06 - INFO - __main__ -   Batch Number = 1
Evaluating:   1%|          | 1/159 [00:00<00:21,  7.36it/s]01/08/2022 13:30:06 - INFO - __main__ -   Batch Number = 2
Evaluating:   1%|▏         | 2/159 [00:00<00:20,  7.79it/s]01/08/2022 13:30:06 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|▏         | 3/159 [00:00<00:19,  7.99it/s]01/08/2022 13:30:06 - INFO - __main__ -   Batch Number = 4
Evaluating:   3%|▎         | 4/159 [00:00<00:19,  8.10it/s]01/08/2022 13:30:07 - INFO - __main__ -   Batch Number = 5
Evaluating:   3%|▎         | 5/159 [00:00<00:18,  8.16it/s]01/08/2022 13:30:07 - INFO - __main__ -   Batch Number = 6
Evaluating:   4%|▍         | 6/159 [00:00<00:18,  8.21it/s]01/08/2022 13:30:07 - INFO - __main__ -   Batch Number = 7
Evaluating:   4%|▍         | 7/159 [00:00<00:18,  8.23it/s]01/08/2022 13:30:07 - INFO - __main__ -   Batch Number = 8
Evaluating:   5%|▌         | 8/159 [00:00<00:18,  8.24it/s]01/08/2022 13:30:07 - INFO - __main__ -   Batch Number = 9
Evaluating:   6%|▌         | 9/159 [00:01<00:18,  8.24it/s]01/08/2022 13:30:07 - INFO - __main__ -   Batch Number = 10
Evaluating:   6%|▋         | 10/159 [00:01<00:18,  8.22it/s]01/08/2022 13:30:07 - INFO - __main__ -   Batch Number = 11
Evaluating:   7%|▋         | 11/159 [00:01<00:17,  8.23it/s]01/08/2022 13:30:07 - INFO - __main__ -   Batch Number = 12
Evaluating:   8%|▊         | 12/159 [00:01<00:17,  8.24it/s]01/08/2022 13:30:07 - INFO - __main__ -   Batch Number = 13
Evaluating:   8%|▊         | 13/159 [00:01<00:17,  8.24it/s]01/08/2022 13:30:08 - INFO - __main__ -   Batch Number = 14
Evaluating:   9%|▉         | 14/159 [00:01<00:17,  8.21it/s]01/08/2022 13:30:08 - INFO - __main__ -   Batch Number = 15
Evaluating:   9%|▉         | 15/159 [00:01<00:17,  8.20it/s]01/08/2022 13:30:08 - INFO - __main__ -   Batch Number = 16
Evaluating:  10%|█         | 16/159 [00:01<00:17,  8.21it/s]01/08/2022 13:30:08 - INFO - __main__ -   Batch Number = 17
Evaluating:  11%|█         | 17/159 [00:02<00:17,  8.23it/s]01/08/2022 13:30:08 - INFO - __main__ -   Batch Number = 18
Evaluating:  11%|█▏        | 18/159 [00:02<00:17,  8.24it/s]01/08/2022 13:30:08 - INFO - __main__ -   Batch Number = 19
Evaluating:  12%|█▏        | 19/159 [00:02<00:17,  8.23it/s]01/08/2022 13:30:08 - INFO - __main__ -   Batch Number = 20
Evaluating:  13%|█▎        | 20/159 [00:02<00:16,  8.23it/s]01/08/2022 13:30:08 - INFO - __main__ -   Batch Number = 21
Evaluating:  13%|█▎        | 21/159 [00:02<00:16,  8.23it/s]01/08/2022 13:30:09 - INFO - __main__ -   Batch Number = 22
Evaluating:  14%|█▍        | 22/159 [00:02<00:16,  8.23it/s]01/08/2022 13:30:09 - INFO - __main__ -   Batch Number = 23
Evaluating:  14%|█▍        | 23/159 [00:02<00:16,  8.22it/s]01/08/2022 13:30:09 - INFO - __main__ -   Batch Number = 24
Evaluating:  15%|█▌        | 24/159 [00:02<00:16,  8.21it/s]01/08/2022 13:30:09 - INFO - __main__ -   Batch Number = 25
Evaluating:  16%|█▌        | 25/159 [00:03<00:16,  8.22it/s]01/08/2022 13:30:09 - INFO - __main__ -   Batch Number = 26
Evaluating:  16%|█▋        | 26/159 [00:03<00:16,  8.22it/s]01/08/2022 13:30:09 - INFO - __main__ -   Batch Number = 27
Evaluating:  17%|█▋        | 27/159 [00:03<00:16,  8.21it/s]01/08/2022 13:30:09 - INFO - __main__ -   Batch Number = 28
Evaluating:  18%|█▊        | 28/159 [00:03<00:15,  8.21it/s]01/08/2022 13:30:09 - INFO - __main__ -   Batch Number = 29
Evaluating:  18%|█▊        | 29/159 [00:03<00:16,  7.79it/s]01/08/2022 13:30:10 - INFO - __main__ -   Batch Number = 30
Evaluating:  19%|█▉        | 30/159 [00:03<00:16,  7.92it/s]01/08/2022 13:30:10 - INFO - __main__ -   Batch Number = 31
Evaluating:  19%|█▉        | 31/159 [00:03<00:16,  8.00it/s]01/08/2022 13:30:10 - INFO - __main__ -   Batch Number = 32
Evaluating:  20%|██        | 32/159 [00:03<00:15,  8.06it/s]01/08/2022 13:30:10 - INFO - __main__ -   Batch Number = 33
Evaluating:  21%|██        | 33/159 [00:04<00:15,  8.11it/s]01/08/2022 13:30:10 - INFO - __main__ -   Batch Number = 34
Evaluating:  21%|██▏       | 34/159 [00:04<00:15,  8.13it/s]01/08/2022 13:30:10 - INFO - __main__ -   Batch Number = 35
Evaluating:  22%|██▏       | 35/159 [00:04<00:15,  8.15it/s]01/08/2022 13:30:10 - INFO - __main__ -   Batch Number = 36
Evaluating:  23%|██▎       | 36/159 [00:04<00:15,  8.17it/s]01/08/2022 13:30:10 - INFO - __main__ -   Batch Number = 37
Evaluating:  23%|██▎       | 37/159 [00:04<00:14,  8.18it/s]01/08/2022 13:30:11 - INFO - __main__ -   Batch Number = 38
Evaluating:  24%|██▍       | 38/159 [00:04<00:14,  8.18it/s]01/08/2022 13:30:11 - INFO - __main__ -   Batch Number = 39
Evaluating:  25%|██▍       | 39/159 [00:04<00:19,  6.19it/s]01/08/2022 13:30:11 - INFO - __main__ -   Batch Number = 40
Evaluating:  25%|██▌       | 40/159 [00:05<00:17,  6.69it/s]01/08/2022 13:30:11 - INFO - __main__ -   Batch Number = 41
Evaluating:  26%|██▌       | 41/159 [00:05<00:16,  7.08it/s]01/08/2022 13:30:11 - INFO - __main__ -   Batch Number = 42
Evaluating:  26%|██▋       | 42/159 [00:05<00:15,  7.39it/s]01/08/2022 13:30:11 - INFO - __main__ -   Batch Number = 43
Evaluating:  27%|██▋       | 43/159 [00:05<00:15,  7.63it/s]01/08/2022 13:30:11 - INFO - __main__ -   Batch Number = 44
Evaluating:  28%|██▊       | 44/159 [00:05<00:14,  7.78it/s]01/08/2022 13:30:12 - INFO - __main__ -   Batch Number = 45
Evaluating:  28%|██▊       | 45/159 [00:05<00:14,  7.91it/s]01/08/2022 13:30:12 - INFO - __main__ -   Batch Number = 46
Evaluating:  29%|██▉       | 46/159 [00:05<00:14,  8.00it/s]01/08/2022 13:30:12 - INFO - __main__ -   Batch Number = 47
Evaluating:  30%|██▉       | 47/159 [00:05<00:13,  8.07it/s]01/08/2022 13:30:12 - INFO - __main__ -   Batch Number = 48
Evaluating:  30%|███       | 48/159 [00:06<00:13,  8.11it/s]01/08/2022 13:30:12 - INFO - __main__ -   Batch Number = 49
Evaluating:  31%|███       | 49/159 [00:06<00:13,  8.12it/s]01/08/2022 13:30:12 - INFO - __main__ -   Batch Number = 50
Evaluating:  31%|███▏      | 50/159 [00:06<00:13,  8.12it/s]01/08/2022 13:30:12 - INFO - __main__ -   Batch Number = 51
Evaluating:  32%|███▏      | 51/159 [00:06<00:13,  8.14it/s]01/08/2022 13:30:12 - INFO - __main__ -   Batch Number = 52
Evaluating:  33%|███▎      | 52/159 [00:06<00:13,  8.16it/s]01/08/2022 13:30:13 - INFO - __main__ -   Batch Number = 53
Evaluating:  33%|███▎      | 53/159 [00:06<00:12,  8.17it/s]01/08/2022 13:30:13 - INFO - __main__ -   Batch Number = 54
Evaluating:  34%|███▍      | 54/159 [00:06<00:12,  8.18it/s]01/08/2022 13:30:13 - INFO - __main__ -   Batch Number = 55
Evaluating:  35%|███▍      | 55/159 [00:06<00:12,  8.16it/s]01/08/2022 13:30:13 - INFO - __main__ -   Batch Number = 56
Evaluating:  35%|███▌      | 56/159 [00:06<00:12,  8.17it/s]01/08/2022 13:30:13 - INFO - __main__ -   Batch Number = 57
Evaluating:  36%|███▌      | 57/159 [00:07<00:12,  8.18it/s]01/08/2022 13:30:13 - INFO - __main__ -   Batch Number = 58
Evaluating:  36%|███▋      | 58/159 [00:07<00:12,  8.18it/s]01/08/2022 13:30:13 - INFO - __main__ -   Batch Number = 59
Evaluating:  37%|███▋      | 59/159 [00:07<00:12,  8.13it/s]01/08/2022 13:30:13 - INFO - __main__ -   Batch Number = 60
Evaluating:  38%|███▊      | 60/159 [00:07<00:12,  8.13it/s]01/08/2022 13:30:13 - INFO - __main__ -   Batch Number = 61
Evaluating:  38%|███▊      | 61/159 [00:07<00:12,  8.15it/s]01/08/2022 13:30:14 - INFO - __main__ -   Batch Number = 62
Evaluating:  39%|███▉      | 62/159 [00:07<00:11,  8.17it/s]01/08/2022 13:30:14 - INFO - __main__ -   Batch Number = 63
Evaluating:  40%|███▉      | 63/159 [00:07<00:11,  8.18it/s]01/08/2022 13:30:14 - INFO - __main__ -   Batch Number = 64
Evaluating:  40%|████      | 64/159 [00:07<00:11,  8.19it/s]01/08/2022 13:30:14 - INFO - __main__ -   Batch Number = 65
Evaluating:  41%|████      | 65/159 [00:08<00:11,  8.19it/s]01/08/2022 13:30:14 - INFO - __main__ -   Batch Number = 66
Evaluating:  42%|████▏     | 66/159 [00:08<00:11,  8.20it/s]01/08/2022 13:30:14 - INFO - __main__ -   Batch Number = 67
Evaluating:  42%|████▏     | 67/159 [00:08<00:11,  8.20it/s]01/08/2022 13:30:14 - INFO - __main__ -   Batch Number = 68
Evaluating:  43%|████▎     | 68/159 [00:08<00:11,  8.21it/s]01/08/2022 13:30:14 - INFO - __main__ -   Batch Number = 69
Evaluating:  43%|████▎     | 69/159 [00:08<00:10,  8.20it/s]01/08/2022 13:30:15 - INFO - __main__ -   Batch Number = 70
Evaluating:  44%|████▍     | 70/159 [00:08<00:10,  8.21it/s]01/08/2022 13:30:15 - INFO - __main__ -   Batch Number = 71
Evaluating:  45%|████▍     | 71/159 [00:08<00:10,  8.21it/s]01/08/2022 13:30:15 - INFO - __main__ -   Batch Number = 72
Evaluating:  45%|████▌     | 72/159 [00:08<00:10,  8.22it/s]01/08/2022 13:30:15 - INFO - __main__ -   Batch Number = 73
Evaluating:  46%|████▌     | 73/159 [00:09<00:10,  8.22it/s]01/08/2022 13:30:15 - INFO - __main__ -   Batch Number = 74
Evaluating:  47%|████▋     | 74/159 [00:09<00:10,  8.21it/s]01/08/2022 13:30:15 - INFO - __main__ -   Batch Number = 75
Evaluating:  47%|████▋     | 75/159 [00:09<00:10,  8.21it/s]01/08/2022 13:30:15 - INFO - __main__ -   Batch Number = 76
Evaluating:  48%|████▊     | 76/159 [00:09<00:10,  8.20it/s]01/08/2022 13:30:15 - INFO - __main__ -   Batch Number = 77
Evaluating:  48%|████▊     | 77/159 [00:09<00:09,  8.21it/s]01/08/2022 13:30:16 - INFO - __main__ -   Batch Number = 78
Evaluating:  49%|████▉     | 78/159 [00:09<00:09,  8.21it/s]01/08/2022 13:30:16 - INFO - __main__ -   Batch Number = 79
Evaluating:  50%|████▉     | 79/159 [00:09<00:10,  7.93it/s]01/08/2022 13:30:16 - INFO - __main__ -   Batch Number = 80
Evaluating:  50%|█████     | 80/159 [00:09<00:09,  8.01it/s]01/08/2022 13:30:16 - INFO - __main__ -   Batch Number = 81
Evaluating:  51%|█████     | 81/159 [00:10<00:09,  8.07it/s]01/08/2022 13:30:16 - INFO - __main__ -   Batch Number = 82
Evaluating:  52%|█████▏    | 82/159 [00:10<00:09,  8.11it/s]01/08/2022 13:30:16 - INFO - __main__ -   Batch Number = 83
Evaluating:  52%|█████▏    | 83/159 [00:10<00:09,  8.15it/s]01/08/2022 13:30:16 - INFO - __main__ -   Batch Number = 84
Evaluating:  53%|█████▎    | 84/159 [00:10<00:09,  8.16it/s]01/08/2022 13:30:16 - INFO - __main__ -   Batch Number = 85
Evaluating:  53%|█████▎    | 85/159 [00:10<00:09,  8.17it/s]01/08/2022 13:30:17 - INFO - __main__ -   Batch Number = 86
Evaluating:  54%|█████▍    | 86/159 [00:10<00:08,  8.19it/s]01/08/2022 13:30:17 - INFO - __main__ -   Batch Number = 87
Evaluating:  55%|█████▍    | 87/159 [00:10<00:08,  8.18it/s]01/08/2022 13:30:17 - INFO - __main__ -   Batch Number = 88
Evaluating:  55%|█████▌    | 88/159 [00:10<00:08,  8.19it/s]01/08/2022 13:30:17 - INFO - __main__ -   Batch Number = 89
Evaluating:  56%|█████▌    | 89/159 [00:11<00:08,  8.17it/s]01/08/2022 13:30:17 - INFO - __main__ -   Batch Number = 90
Evaluating:  57%|█████▋    | 90/159 [00:11<00:08,  8.17it/s]01/08/2022 13:30:17 - INFO - __main__ -   Batch Number = 91
Evaluating:  57%|█████▋    | 91/159 [00:11<00:08,  8.16it/s]01/08/2022 13:30:17 - INFO - __main__ -   Batch Number = 92
Evaluating:  58%|█████▊    | 92/159 [00:11<00:08,  8.17it/s]01/08/2022 13:30:17 - INFO - __main__ -   Batch Number = 93
Evaluating:  58%|█████▊    | 93/159 [00:11<00:08,  8.18it/s]01/08/2022 13:30:18 - INFO - __main__ -   Batch Number = 94
Evaluating:  59%|█████▉    | 94/159 [00:11<00:07,  8.18it/s]01/08/2022 13:30:18 - INFO - __main__ -   Batch Number = 95
Evaluating:  60%|█████▉    | 95/159 [00:11<00:07,  8.18it/s]01/08/2022 13:30:18 - INFO - __main__ -   Batch Number = 96
Evaluating:  60%|██████    | 96/159 [00:11<00:07,  8.19it/s]01/08/2022 13:30:18 - INFO - __main__ -   Batch Number = 97
Evaluating:  61%|██████    | 97/159 [00:12<00:07,  8.19it/s]01/08/2022 13:30:18 - INFO - __main__ -   Batch Number = 98
Evaluating:  62%|██████▏   | 98/159 [00:12<00:07,  8.19it/s]01/08/2022 13:30:18 - INFO - __main__ -   Batch Number = 99
Evaluating:  62%|██████▏   | 99/159 [00:12<00:07,  8.19it/s]01/08/2022 13:30:18 - INFO - __main__ -   Batch Number = 100
Evaluating:  63%|██████▎   | 100/159 [00:12<00:07,  8.19it/s]01/08/2022 13:30:18 - INFO - __main__ -   Batch Number = 101
Evaluating:  64%|██████▎   | 101/159 [00:12<00:07,  8.18it/s]01/08/2022 13:30:18 - INFO - __main__ -   Batch Number = 102
Evaluating:  64%|██████▍   | 102/159 [00:12<00:06,  8.18it/s]01/08/2022 13:30:19 - INFO - __main__ -   Batch Number = 103
Evaluating:  65%|██████▍   | 103/159 [00:12<00:06,  8.20it/s]01/08/2022 13:30:19 - INFO - __main__ -   Batch Number = 104
Evaluating:  65%|██████▌   | 104/159 [00:12<00:06,  8.18it/s]01/08/2022 13:30:19 - INFO - __main__ -   Batch Number = 105
Evaluating:  66%|██████▌   | 105/159 [00:12<00:06,  8.18it/s]01/08/2022 13:30:19 - INFO - __main__ -   Batch Number = 106
Evaluating:  67%|██████▋   | 106/159 [00:13<00:06,  8.17it/s]01/08/2022 13:30:19 - INFO - __main__ -   Batch Number = 107
Evaluating:  67%|██████▋   | 107/159 [00:13<00:06,  8.17it/s]01/08/2022 13:30:19 - INFO - __main__ -   Batch Number = 108
Evaluating:  68%|██████▊   | 108/159 [00:13<00:06,  8.19it/s]01/08/2022 13:30:19 - INFO - __main__ -   Batch Number = 109
Evaluating:  69%|██████▊   | 109/159 [00:13<00:06,  8.18it/s]01/08/2022 13:30:19 - INFO - __main__ -   Batch Number = 110
Evaluating:  69%|██████▉   | 110/159 [00:13<00:05,  8.19it/s]01/08/2022 13:30:20 - INFO - __main__ -   Batch Number = 111
Evaluating:  70%|██████▉   | 111/159 [00:13<00:05,  8.18it/s]01/08/2022 13:30:20 - INFO - __main__ -   Batch Number = 112
Evaluating:  70%|███████   | 112/159 [00:13<00:05,  8.18it/s]01/08/2022 13:30:20 - INFO - __main__ -   Batch Number = 113
Evaluating:  71%|███████   | 113/159 [00:13<00:05,  8.18it/s]01/08/2022 13:30:20 - INFO - __main__ -   Batch Number = 114
Evaluating:  72%|███████▏  | 114/159 [00:14<00:05,  8.18it/s]01/08/2022 13:30:20 - INFO - __main__ -   Batch Number = 115
Evaluating:  72%|███████▏  | 115/159 [00:14<00:05,  8.17it/s]01/08/2022 13:30:20 - INFO - __main__ -   Batch Number = 116
Evaluating:  73%|███████▎  | 116/159 [00:14<00:05,  8.17it/s]01/08/2022 13:30:20 - INFO - __main__ -   Batch Number = 117
Evaluating:  74%|███████▎  | 117/159 [00:14<00:05,  8.11it/s]01/08/2022 13:30:20 - INFO - __main__ -   Batch Number = 118
Evaluating:  74%|███████▍  | 118/159 [00:14<00:05,  8.11it/s]01/08/2022 13:30:21 - INFO - __main__ -   Batch Number = 119
Evaluating:  75%|███████▍  | 119/159 [00:14<00:04,  8.11it/s]01/08/2022 13:30:21 - INFO - __main__ -   Batch Number = 120
Evaluating:  75%|███████▌  | 120/159 [00:14<00:05,  6.64it/s]01/08/2022 13:30:21 - INFO - __main__ -   Batch Number = 121
Evaluating:  76%|███████▌  | 121/159 [00:15<00:05,  7.04it/s]01/08/2022 13:30:21 - INFO - __main__ -   Batch Number = 122
Evaluating:  77%|███████▋  | 122/159 [00:15<00:05,  7.34it/s]01/08/2022 13:30:21 - INFO - __main__ -   Batch Number = 123
Evaluating:  77%|███████▋  | 123/159 [00:15<00:04,  7.58it/s]01/08/2022 13:30:21 - INFO - __main__ -   Batch Number = 124
Evaluating:  78%|███████▊  | 124/159 [00:15<00:04,  7.74it/s]01/08/2022 13:30:21 - INFO - __main__ -   Batch Number = 125
Evaluating:  79%|███████▊  | 125/159 [00:15<00:04,  7.87it/s]01/08/2022 13:30:22 - INFO - __main__ -   Batch Number = 126
Evaluating:  79%|███████▉  | 126/159 [00:15<00:04,  7.94it/s]01/08/2022 13:30:22 - INFO - __main__ -   Batch Number = 127
Evaluating:  80%|███████▉  | 127/159 [00:15<00:04,  7.99it/s]01/08/2022 13:30:22 - INFO - __main__ -   Batch Number = 128
Evaluating:  81%|████████  | 128/159 [00:15<00:03,  8.04it/s]01/08/2022 13:30:22 - INFO - __main__ -   Batch Number = 129
Evaluating:  81%|████████  | 129/159 [00:16<00:03,  8.08it/s]01/08/2022 13:30:22 - INFO - __main__ -   Batch Number = 130
Evaluating:  82%|████████▏ | 130/159 [00:16<00:03,  8.11it/s]01/08/2022 13:30:22 - INFO - __main__ -   Batch Number = 131
Evaluating:  82%|████████▏ | 131/159 [00:16<00:03,  8.13it/s]01/08/2022 13:30:22 - INFO - __main__ -   Batch Number = 132
Evaluating:  83%|████████▎ | 132/159 [00:16<00:03,  8.15it/s]01/08/2022 13:30:22 - INFO - __main__ -   Batch Number = 133
Evaluating:  84%|████████▎ | 133/159 [00:16<00:03,  8.15it/s]01/08/2022 13:30:23 - INFO - __main__ -   Batch Number = 134
Evaluating:  84%|████████▍ | 134/159 [00:16<00:03,  8.13it/s]01/08/2022 13:30:23 - INFO - __main__ -   Batch Number = 135
Evaluating:  85%|████████▍ | 135/159 [00:16<00:02,  8.14it/s]01/08/2022 13:30:23 - INFO - __main__ -   Batch Number = 136
Evaluating:  86%|████████▌ | 136/159 [00:16<00:02,  8.15it/s]01/08/2022 13:30:23 - INFO - __main__ -   Batch Number = 137
Evaluating:  86%|████████▌ | 137/159 [00:16<00:02,  8.15it/s]01/08/2022 13:30:23 - INFO - __main__ -   Batch Number = 138
Evaluating:  87%|████████▋ | 138/159 [00:17<00:02,  8.17it/s]01/08/2022 13:30:23 - INFO - __main__ -   Batch Number = 139
Evaluating:  87%|████████▋ | 139/159 [00:17<00:02,  8.17it/s]01/08/2022 13:30:23 - INFO - __main__ -   Batch Number = 140
Evaluating:  88%|████████▊ | 140/159 [00:17<00:02,  8.18it/s]01/08/2022 13:30:23 - INFO - __main__ -   Batch Number = 141
Evaluating:  89%|████████▊ | 141/159 [00:17<00:02,  8.17it/s]01/08/2022 13:30:23 - INFO - __main__ -   Batch Number = 142
Evaluating:  89%|████████▉ | 142/159 [00:17<00:02,  8.17it/s]01/08/2022 13:30:24 - INFO - __main__ -   Batch Number = 143
Evaluating:  90%|████████▉ | 143/159 [00:17<00:01,  8.17it/s]01/08/2022 13:30:24 - INFO - __main__ -   Batch Number = 144
Evaluating:  91%|█████████ | 144/159 [00:17<00:01,  8.17it/s]01/08/2022 13:30:24 - INFO - __main__ -   Batch Number = 145
Evaluating:  91%|█████████ | 145/159 [00:17<00:01,  8.18it/s]01/08/2022 13:30:24 - INFO - __main__ -   Batch Number = 146
Evaluating:  92%|█████████▏| 146/159 [00:18<00:01,  8.18it/s]01/08/2022 13:30:24 - INFO - __main__ -   Batch Number = 147
Evaluating:  92%|█████████▏| 147/159 [00:18<00:01,  8.18it/s]01/08/2022 13:30:24 - INFO - __main__ -   Batch Number = 148
Evaluating:  93%|█████████▎| 148/159 [00:18<00:01,  8.18it/s]01/08/2022 13:30:24 - INFO - __main__ -   Batch Number = 149
Evaluating:  94%|█████████▎| 149/159 [00:18<00:01,  8.16it/s]01/08/2022 13:30:24 - INFO - __main__ -   Batch Number = 150
Evaluating:  94%|█████████▍| 150/159 [00:18<00:01,  8.14it/s]01/08/2022 13:30:25 - INFO - __main__ -   Batch Number = 151
Evaluating:  95%|█████████▍| 151/159 [00:18<00:00,  8.15it/s]01/08/2022 13:30:25 - INFO - __main__ -   Batch Number = 152
Evaluating:  96%|█████████▌| 152/159 [00:18<00:00,  8.14it/s]01/08/2022 13:30:25 - INFO - __main__ -   Batch Number = 153
Evaluating:  96%|█████████▌| 153/159 [00:18<00:00,  8.14it/s]01/08/2022 13:30:25 - INFO - __main__ -   Batch Number = 154
Evaluating:  97%|█████████▋| 154/159 [00:19<00:00,  8.15it/s]01/08/2022 13:30:25 - INFO - __main__ -   Batch Number = 155
Evaluating:  97%|█████████▋| 155/159 [00:19<00:00,  8.15it/s]01/08/2022 13:30:25 - INFO - __main__ -   Batch Number = 156
Evaluating:  98%|█████████▊| 156/159 [00:19<00:00,  8.14it/s]01/08/2022 13:30:25 - INFO - __main__ -   Batch Number = 157
Evaluating:  99%|█████████▊| 157/159 [00:19<00:00,  8.14it/s]01/08/2022 13:30:25 - INFO - __main__ -   Batch Number = 158
Evaluating:  99%|█████████▉| 158/159 [00:19<00:00,  8.14it/s]01/08/2022 13:30:26 - INFO - __main__ -   Batch Number = 159
Evaluating: 100%|██████████| 159/159 [00:19<00:00,  8.09it/s]
01/08/2022 13:30:26 - INFO - __main__ -     Evaluation done in total 19.715298 secs (0.015524 sec per example)
Writing predictions to: /home/abhijeet/rohan/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_ar/predictions_en_.json
Writing nbest to: /home/abhijeet/rohan/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_ar/nbest_predictions_en_.json
01/08/2022 13:30:30 - INFO - __main__ -   Results = OrderedDict([('exact', 71.09243697478992), ('f1', 82.59472839738623), ('total', 1190), ('HasAns_exact', 71.09243697478992), ('HasAns_f1', 82.59472839738623), ('HasAns_total', 1190), ('best_exact', 71.09243697478992), ('best_exact_thresh', 0.0), ('best_f1', 82.59472839738623), ('best_f1_thresh', 0.0)])
PyTorch version 1.10.0+cu102 available.
01/08/2022 13:30:32 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/08/2022 13:30:32 - INFO - root -   save model
loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6
Model config XLMRobertaConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /home/abhijeet/.cache/torch/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8
loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/08/2022 13:30:42 - INFO - __main__ -   lang2id = None
01/08/2022 13:30:44 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='xlm-roberta-base', model_type='xlm-roberta', config_name=None, tokenizer_name=None, cache_dir=None, data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//xquad', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_ar/', max_seq_length=384, train_file='/home/abhijeet/rohan/cloud-emea-copy/data//xquad/train-v1.1.json', predict_file='/home/abhijeet/rohan/cloud-emea-copy/data//xquad/dev-v1.1.json', eval_test_set=False, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=4, per_gpu_eval_batch_size=8, gradient_accumulation_steps=4, learning_rate=0.0001, weight_decay=0.0001, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=15.0, max_steps=-1, save_steps=1000, warmup_steps=500, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', eval_lang='en', predict_langs='en', train_lang='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_ar//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='qna', threads=8, version_2_with_negative=False, verbose_logging=False, n_best_size=20, max_query_length=64, max_answer_length=30, doc_stride=128, null_score_diff_threshold=0.0, predict_task_adapter='output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s3/checkpoint-best/qna', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix=None, en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/08/2022 13:30:44 - INFO - __main__ -   Results: {}
loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6
Model config XLMRobertaConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /home/abhijeet/.cache/torch/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8
loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/08/2022 13:30:54 - INFO - __main__ -   lang2id = None
01/08/2022 13:30:54 - INFO - __main__ -   Evaluating the model on the test set of all languages specified
01/08/2022 13:30:54 - INFO - __main__ -   Adapter will be loaded from this path: output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s3/checkpoint-best/qna
01/08/2022 13:30:54 - INFO - root -   Trying to decide if add adapter
01/08/2022 13:30:54 - INFO - root -   loading task adapter
Loading module configuration from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s3/checkpoint-best/qna/adapter_config.json
Adding adapter 'qna' of type 'text_task'.
Loading module weights from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s3/checkpoint-best/qna/pytorch_adapter.bin
Loading module configuration from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s3/checkpoint-best/qna/head_config.json
Loading module weights from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s3/checkpoint-best/qna/pytorch_model_head.bin
01/08/2022 13:30:54 - INFO - root -   loading lang adpater ar/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/ar/xlm-roberta-base/pfeiffer/ar_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1f4018c6b187912fb6b47e664a34880506aef8c7320e0135939626a97e0bb137-4b1153cd3ebfdce09e1023c01e3ee723c32a0a57bea16e5ff13609f97eb2af0b-extracted/adapter_config.json
Adding adapter 'ar' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/1f4018c6b187912fb6b47e664a34880506aef8c7320e0135939626a97e0bb137-4b1153cd3ebfdce09e1023c01e3ee723c32a0a57bea16e5ff13609f97eb2af0b-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/1f4018c6b187912fb6b47e664a34880506aef8c7320e0135939626a97e0bb137-4b1153cd3ebfdce09e1023c01e3ee723c32a0a57bea16e5ff13609f97eb2af0b-extracted'
01/08/2022 13:30:56 - INFO - __main__ -   Language adapter for en not found, using ar instead
01/08/2022 13:30:56 - INFO - __main__ -   Set active language adapter to ar
01/08/2022 13:30:56 - INFO - __main__ -   Args Adapter Weight = None
01/08/2022 13:30:56 - INFO - __main__ -   Adapter Languages = ['ar']
01/08/2022 13:30:56 - INFO - __main__ -   Predict File = xquad.en.json
01/08/2022 13:30:56 - INFO - __main__ -   Creating features from dataset file at /home/abhijeet/rohan/cloud-emea-copy/data//xquad
ar ar/wiki@ukp
  0%|          | 0/48 [00:00<?, ?it/s] 33%|███▎      | 16/48 [00:00<00:00, 125.11it/s] 60%|██████    | 29/48 [00:00<00:00, 113.93it/s] 85%|████████▌ | 41/48 [00:00<00:00, 95.40it/s] 100%|██████████| 48/48 [00:00<00:00, 98.98it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<03:54,  5.07it/s]convert squad examples to features:  27%|██▋       | 321/1190 [00:00<00:00, 1289.48it/s]convert squad examples to features:  41%|████▏     | 492/1190 [00:00<00:01, 630.60it/s] convert squad examples to features:  97%|█████████▋| 1153/1190 [00:00<00:00, 1614.30it/s]convert squad examples to features: 100%|██████████| 1190/1190 [00:00<00:00, 1317.05it/s]/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|██████████| 1190/1190 [00:00<00:00, 416838.30it/s]
01/08/2022 13:30:58 - INFO - __main__ -   Local Rank = -1
01/08/2022 13:30:58 - INFO - __main__ -   Saving features into cached file /home/abhijeet/rohan/cloud-emea-copy/data//xquad/cached_xquad.en.json_xlm-roberta-base_384_en
01/08/2022 13:30:59 - INFO - __main__ -   ***** Running evaluation  *****
01/08/2022 13:30:59 - INFO - __main__ -     Num examples = 1270
01/08/2022 13:30:59 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/159 [00:00<?, ?it/s]01/08/2022 13:30:59 - INFO - __main__ -   Batch Number = 1
Evaluating:   1%|          | 1/159 [00:00<00:22,  7.02it/s]01/08/2022 13:30:59 - INFO - __main__ -   Batch Number = 2
Evaluating:   1%|▏         | 2/159 [00:00<00:20,  7.64it/s]01/08/2022 13:30:59 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|▏         | 3/159 [00:00<00:19,  7.90it/s]01/08/2022 13:30:59 - INFO - __main__ -   Batch Number = 4
Evaluating:   3%|▎         | 4/159 [00:00<00:19,  8.04it/s]01/08/2022 13:31:00 - INFO - __main__ -   Batch Number = 5
Evaluating:   3%|▎         | 5/159 [00:00<00:18,  8.11it/s]01/08/2022 13:31:00 - INFO - __main__ -   Batch Number = 6
Evaluating:   4%|▍         | 6/159 [00:00<00:18,  8.16it/s]01/08/2022 13:31:00 - INFO - __main__ -   Batch Number = 7
Evaluating:   4%|▍         | 7/159 [00:00<00:18,  8.20it/s]01/08/2022 13:31:00 - INFO - __main__ -   Batch Number = 8
Evaluating:   5%|▌         | 8/159 [00:00<00:18,  8.20it/s]01/08/2022 13:31:00 - INFO - __main__ -   Batch Number = 9
Evaluating:   6%|▌         | 9/159 [00:01<00:18,  8.20it/s]01/08/2022 13:31:00 - INFO - __main__ -   Batch Number = 10
Evaluating:   6%|▋         | 10/159 [00:01<00:18,  8.21it/s]01/08/2022 13:31:00 - INFO - __main__ -   Batch Number = 11
Evaluating:   7%|▋         | 11/159 [00:01<00:18,  8.20it/s]01/08/2022 13:31:00 - INFO - __main__ -   Batch Number = 12
Evaluating:   8%|▊         | 12/159 [00:01<00:17,  8.21it/s]01/08/2022 13:31:00 - INFO - __main__ -   Batch Number = 13
Evaluating:   8%|▊         | 13/159 [00:01<00:17,  8.21it/s]01/08/2022 13:31:01 - INFO - __main__ -   Batch Number = 14
Evaluating:   9%|▉         | 14/159 [00:01<00:17,  8.19it/s]01/08/2022 13:31:01 - INFO - __main__ -   Batch Number = 15
Evaluating:   9%|▉         | 15/159 [00:01<00:17,  8.02it/s]01/08/2022 13:31:01 - INFO - __main__ -   Batch Number = 16
Evaluating:  10%|█         | 16/159 [00:01<00:17,  8.08it/s]01/08/2022 13:31:01 - INFO - __main__ -   Batch Number = 17
Evaluating:  11%|█         | 17/159 [00:02<00:17,  8.13it/s]01/08/2022 13:31:01 - INFO - __main__ -   Batch Number = 18
Evaluating:  11%|█▏        | 18/159 [00:02<00:17,  8.16it/s]01/08/2022 13:31:01 - INFO - __main__ -   Batch Number = 19
Evaluating:  12%|█▏        | 19/159 [00:02<00:17,  8.18it/s]01/08/2022 13:31:01 - INFO - __main__ -   Batch Number = 20
Evaluating:  13%|█▎        | 20/159 [00:02<00:16,  8.20it/s]01/08/2022 13:31:01 - INFO - __main__ -   Batch Number = 21
Evaluating:  13%|█▎        | 21/159 [00:02<00:16,  8.21it/s]01/08/2022 13:31:02 - INFO - __main__ -   Batch Number = 22
Evaluating:  14%|█▍        | 22/159 [00:02<00:16,  8.22it/s]01/08/2022 13:31:02 - INFO - __main__ -   Batch Number = 23
Evaluating:  14%|█▍        | 23/159 [00:02<00:16,  8.24it/s]01/08/2022 13:31:02 - INFO - __main__ -   Batch Number = 24
Evaluating:  15%|█▌        | 24/159 [00:02<00:16,  8.24it/s]01/08/2022 13:31:02 - INFO - __main__ -   Batch Number = 25
Evaluating:  16%|█▌        | 25/159 [00:03<00:16,  8.24it/s]01/08/2022 13:31:02 - INFO - __main__ -   Batch Number = 26
Evaluating:  16%|█▋        | 26/159 [00:03<00:16,  8.24it/s]01/08/2022 13:31:02 - INFO - __main__ -   Batch Number = 27
Evaluating:  17%|█▋        | 27/159 [00:03<00:16,  8.24it/s]01/08/2022 13:31:02 - INFO - __main__ -   Batch Number = 28
Evaluating:  18%|█▊        | 28/159 [00:03<00:15,  8.24it/s]01/08/2022 13:31:02 - INFO - __main__ -   Batch Number = 29
Evaluating:  18%|█▊        | 29/159 [00:03<00:16,  7.98it/s]01/08/2022 13:31:03 - INFO - __main__ -   Batch Number = 30
Evaluating:  19%|█▉        | 30/159 [00:03<00:16,  8.05it/s]01/08/2022 13:31:03 - INFO - __main__ -   Batch Number = 31
Evaluating:  19%|█▉        | 31/159 [00:03<00:15,  8.12it/s]01/08/2022 13:31:03 - INFO - __main__ -   Batch Number = 32
Evaluating:  20%|██        | 32/159 [00:03<00:15,  8.16it/s]01/08/2022 13:31:03 - INFO - __main__ -   Batch Number = 33
Evaluating:  21%|██        | 33/159 [00:04<00:15,  8.18it/s]01/08/2022 13:31:03 - INFO - __main__ -   Batch Number = 34
Evaluating:  21%|██▏       | 34/159 [00:04<00:15,  8.19it/s]01/08/2022 13:31:03 - INFO - __main__ -   Batch Number = 35
Evaluating:  22%|██▏       | 35/159 [00:04<00:15,  8.20it/s]01/08/2022 13:31:03 - INFO - __main__ -   Batch Number = 36
Evaluating:  23%|██▎       | 36/159 [00:04<00:15,  8.20it/s]01/08/2022 13:31:03 - INFO - __main__ -   Batch Number = 37
Evaluating:  23%|██▎       | 37/159 [00:04<00:14,  8.17it/s]01/08/2022 13:31:04 - INFO - __main__ -   Batch Number = 38
Evaluating:  24%|██▍       | 38/159 [00:04<00:14,  8.20it/s]01/08/2022 13:31:04 - INFO - __main__ -   Batch Number = 39
Evaluating:  25%|██▍       | 39/159 [00:04<00:14,  8.21it/s]01/08/2022 13:31:04 - INFO - __main__ -   Batch Number = 40
Evaluating:  25%|██▌       | 40/159 [00:04<00:14,  8.21it/s]01/08/2022 13:31:04 - INFO - __main__ -   Batch Number = 41
Evaluating:  26%|██▌       | 41/159 [00:05<00:14,  8.22it/s]01/08/2022 13:31:04 - INFO - __main__ -   Batch Number = 42
Evaluating:  26%|██▋       | 42/159 [00:05<00:14,  8.16it/s]01/08/2022 13:31:04 - INFO - __main__ -   Batch Number = 43
Evaluating:  27%|██▋       | 43/159 [00:05<00:14,  8.18it/s]01/08/2022 13:31:04 - INFO - __main__ -   Batch Number = 44
Evaluating:  28%|██▊       | 44/159 [00:05<00:14,  8.16it/s]01/08/2022 13:31:04 - INFO - __main__ -   Batch Number = 45
Evaluating:  28%|██▊       | 45/159 [00:05<00:13,  8.18it/s]01/08/2022 13:31:05 - INFO - __main__ -   Batch Number = 46
Evaluating:  29%|██▉       | 46/159 [00:05<00:13,  8.20it/s]01/08/2022 13:31:05 - INFO - __main__ -   Batch Number = 47
Evaluating:  30%|██▉       | 47/159 [00:05<00:13,  8.21it/s]01/08/2022 13:31:05 - INFO - __main__ -   Batch Number = 48
Evaluating:  30%|███       | 48/159 [00:05<00:13,  8.21it/s]01/08/2022 13:31:05 - INFO - __main__ -   Batch Number = 49
Evaluating:  31%|███       | 49/159 [00:06<00:13,  8.22it/s]01/08/2022 13:31:05 - INFO - __main__ -   Batch Number = 50
Evaluating:  31%|███▏      | 50/159 [00:06<00:13,  8.21it/s]01/08/2022 13:31:05 - INFO - __main__ -   Batch Number = 51
Evaluating:  32%|███▏      | 51/159 [00:06<00:13,  8.20it/s]01/08/2022 13:31:05 - INFO - __main__ -   Batch Number = 52
Evaluating:  33%|███▎      | 52/159 [00:06<00:13,  8.19it/s]01/08/2022 13:31:05 - INFO - __main__ -   Batch Number = 53
Evaluating:  33%|███▎      | 53/159 [00:06<00:12,  8.20it/s]01/08/2022 13:31:05 - INFO - __main__ -   Batch Number = 54
Evaluating:  34%|███▍      | 54/159 [00:06<00:12,  8.20it/s]01/08/2022 13:31:06 - INFO - __main__ -   Batch Number = 55
Evaluating:  35%|███▍      | 55/159 [00:06<00:12,  8.20it/s]01/08/2022 13:31:06 - INFO - __main__ -   Batch Number = 56
Evaluating:  35%|███▌      | 56/159 [00:06<00:14,  7.08it/s]01/08/2022 13:31:06 - INFO - __main__ -   Batch Number = 57
Evaluating:  36%|███▌      | 57/159 [00:07<00:13,  7.38it/s]01/08/2022 13:31:06 - INFO - __main__ -   Batch Number = 58
Evaluating:  36%|███▋      | 58/159 [00:07<00:13,  7.60it/s]01/08/2022 13:31:06 - INFO - __main__ -   Batch Number = 59
Evaluating:  37%|███▋      | 59/159 [00:07<00:12,  7.76it/s]01/08/2022 13:31:06 - INFO - __main__ -   Batch Number = 60
Evaluating:  38%|███▊      | 60/159 [00:07<00:12,  7.88it/s]01/08/2022 13:31:06 - INFO - __main__ -   Batch Number = 61
Evaluating:  38%|███▊      | 61/159 [00:07<00:12,  7.98it/s]01/08/2022 13:31:07 - INFO - __main__ -   Batch Number = 62
Evaluating:  39%|███▉      | 62/159 [00:07<00:12,  8.04it/s]01/08/2022 13:31:07 - INFO - __main__ -   Batch Number = 63
Evaluating:  40%|███▉      | 63/159 [00:07<00:11,  8.09it/s]01/08/2022 13:31:07 - INFO - __main__ -   Batch Number = 64
Evaluating:  40%|████      | 64/159 [00:07<00:11,  8.12it/s]01/08/2022 13:31:07 - INFO - __main__ -   Batch Number = 65
Evaluating:  41%|████      | 65/159 [00:08<00:11,  8.14it/s]01/08/2022 13:31:07 - INFO - __main__ -   Batch Number = 66
Evaluating:  42%|████▏     | 66/159 [00:08<00:11,  8.14it/s]01/08/2022 13:31:07 - INFO - __main__ -   Batch Number = 67
Evaluating:  42%|████▏     | 67/159 [00:08<00:11,  8.14it/s]01/08/2022 13:31:07 - INFO - __main__ -   Batch Number = 68
Evaluating:  43%|████▎     | 68/159 [00:08<00:11,  8.16it/s]01/08/2022 13:31:07 - INFO - __main__ -   Batch Number = 69
Evaluating:  43%|████▎     | 69/159 [00:08<00:11,  8.17it/s]01/08/2022 13:31:08 - INFO - __main__ -   Batch Number = 70
Evaluating:  44%|████▍     | 70/159 [00:08<00:10,  8.18it/s]01/08/2022 13:31:08 - INFO - __main__ -   Batch Number = 71
Evaluating:  45%|████▍     | 71/159 [00:08<00:10,  8.18it/s]01/08/2022 13:31:08 - INFO - __main__ -   Batch Number = 72
Evaluating:  45%|████▌     | 72/159 [00:08<00:10,  8.18it/s]01/08/2022 13:31:08 - INFO - __main__ -   Batch Number = 73
Evaluating:  46%|████▌     | 73/159 [00:08<00:10,  8.19it/s]01/08/2022 13:31:08 - INFO - __main__ -   Batch Number = 74
Evaluating:  47%|████▋     | 74/159 [00:09<00:10,  8.17it/s]01/08/2022 13:31:08 - INFO - __main__ -   Batch Number = 75
Evaluating:  47%|████▋     | 75/159 [00:09<00:10,  8.16it/s]01/08/2022 13:31:08 - INFO - __main__ -   Batch Number = 76
Evaluating:  48%|████▊     | 76/159 [00:09<00:10,  8.15it/s]01/08/2022 13:31:08 - INFO - __main__ -   Batch Number = 77
Evaluating:  48%|████▊     | 77/159 [00:09<00:10,  8.17it/s]01/08/2022 13:31:08 - INFO - __main__ -   Batch Number = 78
Evaluating:  49%|████▉     | 78/159 [00:09<00:09,  8.18it/s]01/08/2022 13:31:09 - INFO - __main__ -   Batch Number = 79
Evaluating:  50%|████▉     | 79/159 [00:09<00:09,  8.18it/s]01/08/2022 13:31:09 - INFO - __main__ -   Batch Number = 80
Evaluating:  50%|█████     | 80/159 [00:09<00:09,  8.18it/s]01/08/2022 13:31:09 - INFO - __main__ -   Batch Number = 81
Evaluating:  51%|█████     | 81/159 [00:09<00:09,  8.17it/s]01/08/2022 13:31:09 - INFO - __main__ -   Batch Number = 82
Evaluating:  52%|█████▏    | 82/159 [00:10<00:09,  8.17it/s]01/08/2022 13:31:09 - INFO - __main__ -   Batch Number = 83
Evaluating:  52%|█████▏    | 83/159 [00:10<00:09,  8.17it/s]01/08/2022 13:31:09 - INFO - __main__ -   Batch Number = 84
Evaluating:  53%|█████▎    | 84/159 [00:10<00:09,  8.14it/s]01/08/2022 13:31:09 - INFO - __main__ -   Batch Number = 85
Evaluating:  53%|█████▎    | 85/159 [00:10<00:09,  8.15it/s]01/08/2022 13:31:09 - INFO - __main__ -   Batch Number = 86
Evaluating:  54%|█████▍    | 86/159 [00:10<00:08,  8.16it/s]01/08/2022 13:31:10 - INFO - __main__ -   Batch Number = 87
Evaluating:  55%|█████▍    | 87/159 [00:10<00:08,  8.16it/s]01/08/2022 13:31:10 - INFO - __main__ -   Batch Number = 88
Evaluating:  55%|█████▌    | 88/159 [00:10<00:08,  8.17it/s]01/08/2022 13:31:10 - INFO - __main__ -   Batch Number = 89
Evaluating:  56%|█████▌    | 89/159 [00:10<00:08,  8.16it/s]01/08/2022 13:31:10 - INFO - __main__ -   Batch Number = 90
Evaluating:  57%|█████▋    | 90/159 [00:11<00:08,  8.16it/s]01/08/2022 13:31:10 - INFO - __main__ -   Batch Number = 91
Evaluating:  57%|█████▋    | 91/159 [00:11<00:08,  8.16it/s]01/08/2022 13:31:10 - INFO - __main__ -   Batch Number = 92
Evaluating:  58%|█████▊    | 92/159 [00:11<00:08,  8.16it/s]01/08/2022 13:31:10 - INFO - __main__ -   Batch Number = 93
Evaluating:  58%|█████▊    | 93/159 [00:11<00:08,  8.17it/s]01/08/2022 13:31:10 - INFO - __main__ -   Batch Number = 94
Evaluating:  59%|█████▉    | 94/159 [00:11<00:07,  8.17it/s]01/08/2022 13:31:11 - INFO - __main__ -   Batch Number = 95
Evaluating:  60%|█████▉    | 95/159 [00:11<00:07,  8.17it/s]01/08/2022 13:31:11 - INFO - __main__ -   Batch Number = 96
Evaluating:  60%|██████    | 96/159 [00:12<00:12,  5.23it/s]01/08/2022 13:31:11 - INFO - __main__ -   Batch Number = 97
Evaluating:  61%|██████    | 97/159 [00:12<00:10,  5.87it/s]01/08/2022 13:31:11 - INFO - __main__ -   Batch Number = 98
Evaluating:  62%|██████▏   | 98/159 [00:12<00:09,  6.42it/s]01/08/2022 13:31:11 - INFO - __main__ -   Batch Number = 99
Evaluating:  62%|██████▏   | 99/159 [00:12<00:08,  6.86it/s]01/08/2022 13:31:11 - INFO - __main__ -   Batch Number = 100
Evaluating:  63%|██████▎   | 100/159 [00:12<00:08,  7.22it/s]01/08/2022 13:31:12 - INFO - __main__ -   Batch Number = 101
Evaluating:  64%|██████▎   | 101/159 [00:12<00:07,  7.48it/s]01/08/2022 13:31:12 - INFO - __main__ -   Batch Number = 102
Evaluating:  64%|██████▍   | 102/159 [00:12<00:07,  7.65it/s]01/08/2022 13:31:12 - INFO - __main__ -   Batch Number = 103
Evaluating:  65%|██████▍   | 103/159 [00:12<00:07,  7.80it/s]01/08/2022 13:31:12 - INFO - __main__ -   Batch Number = 104
Evaluating:  65%|██████▌   | 104/159 [00:13<00:06,  7.88it/s]01/08/2022 13:31:12 - INFO - __main__ -   Batch Number = 105
Evaluating:  66%|██████▌   | 105/159 [00:13<00:06,  7.97it/s]01/08/2022 13:31:12 - INFO - __main__ -   Batch Number = 106
Evaluating:  67%|██████▋   | 106/159 [00:13<00:06,  8.01it/s]01/08/2022 13:31:12 - INFO - __main__ -   Batch Number = 107
Evaluating:  67%|██████▋   | 107/159 [00:13<00:06,  8.07it/s]01/08/2022 13:31:12 - INFO - __main__ -   Batch Number = 108
Evaluating:  68%|██████▊   | 108/159 [00:13<00:06,  8.10it/s]01/08/2022 13:31:13 - INFO - __main__ -   Batch Number = 109
Evaluating:  69%|██████▊   | 109/159 [00:13<00:06,  8.13it/s]01/08/2022 13:31:13 - INFO - __main__ -   Batch Number = 110
Evaluating:  69%|██████▉   | 110/159 [00:13<00:06,  8.15it/s]01/08/2022 13:31:13 - INFO - __main__ -   Batch Number = 111
Evaluating:  70%|██████▉   | 111/159 [00:13<00:05,  8.16it/s]01/08/2022 13:31:13 - INFO - __main__ -   Batch Number = 112
Evaluating:  70%|███████   | 112/159 [00:14<00:05,  8.16it/s]01/08/2022 13:31:13 - INFO - __main__ -   Batch Number = 113
Evaluating:  71%|███████   | 113/159 [00:14<00:05,  8.16it/s]01/08/2022 13:31:13 - INFO - __main__ -   Batch Number = 114
Evaluating:  72%|███████▏  | 114/159 [00:14<00:05,  8.16it/s]01/08/2022 13:31:13 - INFO - __main__ -   Batch Number = 115
Evaluating:  72%|███████▏  | 115/159 [00:14<00:05,  8.15it/s]01/08/2022 13:31:13 - INFO - __main__ -   Batch Number = 116
Evaluating:  73%|███████▎  | 116/159 [00:14<00:05,  8.14it/s]01/08/2022 13:31:13 - INFO - __main__ -   Batch Number = 117
Evaluating:  74%|███████▎  | 117/159 [00:14<00:05,  8.14it/s]01/08/2022 13:31:14 - INFO - __main__ -   Batch Number = 118
Evaluating:  74%|███████▍  | 118/159 [00:14<00:05,  8.14it/s]01/08/2022 13:31:14 - INFO - __main__ -   Batch Number = 119
Evaluating:  75%|███████▍  | 119/159 [00:14<00:04,  8.11it/s]01/08/2022 13:31:14 - INFO - __main__ -   Batch Number = 120
Evaluating:  75%|███████▌  | 120/159 [00:14<00:04,  8.08it/s]01/08/2022 13:31:14 - INFO - __main__ -   Batch Number = 121
Evaluating:  76%|███████▌  | 121/159 [00:15<00:04,  8.06it/s]01/08/2022 13:31:14 - INFO - __main__ -   Batch Number = 122
Evaluating:  77%|███████▋  | 122/159 [00:15<00:04,  8.05it/s]01/08/2022 13:31:14 - INFO - __main__ -   Batch Number = 123
Evaluating:  77%|███████▋  | 123/159 [00:15<00:04,  8.05it/s]01/08/2022 13:31:14 - INFO - __main__ -   Batch Number = 124
Evaluating:  78%|███████▊  | 124/159 [00:15<00:04,  8.08it/s]01/08/2022 13:31:14 - INFO - __main__ -   Batch Number = 125
Evaluating:  79%|███████▊  | 125/159 [00:15<00:04,  8.09it/s]01/08/2022 13:31:15 - INFO - __main__ -   Batch Number = 126
Evaluating:  79%|███████▉  | 126/159 [00:15<00:04,  8.10it/s]01/08/2022 13:31:15 - INFO - __main__ -   Batch Number = 127
Evaluating:  80%|███████▉  | 127/159 [00:15<00:03,  8.11it/s]01/08/2022 13:31:15 - INFO - __main__ -   Batch Number = 128
Evaluating:  81%|████████  | 128/159 [00:15<00:03,  8.10it/s]01/08/2022 13:31:15 - INFO - __main__ -   Batch Number = 129
Evaluating:  81%|████████  | 129/159 [00:16<00:03,  8.08it/s]01/08/2022 13:31:15 - INFO - __main__ -   Batch Number = 130
Evaluating:  82%|████████▏ | 130/159 [00:16<00:03,  8.08it/s]01/08/2022 13:31:15 - INFO - __main__ -   Batch Number = 131
Evaluating:  82%|████████▏ | 131/159 [00:16<00:03,  8.08it/s]01/08/2022 13:31:15 - INFO - __main__ -   Batch Number = 132
Evaluating:  83%|████████▎ | 132/159 [00:16<00:03,  8.06it/s]01/08/2022 13:31:15 - INFO - __main__ -   Batch Number = 133
Evaluating:  84%|████████▎ | 133/159 [00:16<00:03,  8.04it/s]01/08/2022 13:31:16 - INFO - __main__ -   Batch Number = 134
Evaluating:  84%|████████▍ | 134/159 [00:16<00:03,  8.02it/s]01/08/2022 13:31:16 - INFO - __main__ -   Batch Number = 135
Evaluating:  85%|████████▍ | 135/159 [00:17<00:04,  5.64it/s]01/08/2022 13:31:16 - INFO - __main__ -   Batch Number = 136
Evaluating:  86%|████████▌ | 136/159 [00:17<00:03,  6.21it/s]01/08/2022 13:31:16 - INFO - __main__ -   Batch Number = 137
Evaluating:  86%|████████▌ | 137/159 [00:17<00:03,  6.70it/s]01/08/2022 13:31:16 - INFO - __main__ -   Batch Number = 138
Evaluating:  87%|████████▋ | 138/159 [00:17<00:02,  7.08it/s]01/08/2022 13:31:16 - INFO - __main__ -   Batch Number = 139
Evaluating:  87%|████████▋ | 139/159 [00:17<00:02,  7.36it/s]01/08/2022 13:31:17 - INFO - __main__ -   Batch Number = 140
Evaluating:  88%|████████▊ | 140/159 [00:17<00:02,  7.55it/s]01/08/2022 13:31:17 - INFO - __main__ -   Batch Number = 141
Evaluating:  89%|████████▊ | 141/159 [00:17<00:02,  7.70it/s]01/08/2022 13:31:17 - INFO - __main__ -   Batch Number = 142
Evaluating:  89%|████████▉ | 142/159 [00:17<00:02,  7.80it/s]01/08/2022 13:31:17 - INFO - __main__ -   Batch Number = 143
Evaluating:  90%|████████▉ | 143/159 [00:18<00:02,  7.86it/s]01/08/2022 13:31:17 - INFO - __main__ -   Batch Number = 144
Evaluating:  91%|█████████ | 144/159 [00:18<00:01,  7.90it/s]01/08/2022 13:31:17 - INFO - __main__ -   Batch Number = 145
Evaluating:  91%|█████████ | 145/159 [00:18<00:01,  7.92it/s]01/08/2022 13:31:17 - INFO - __main__ -   Batch Number = 146
Evaluating:  92%|█████████▏| 146/159 [00:18<00:01,  7.93it/s]01/08/2022 13:31:17 - INFO - __main__ -   Batch Number = 147
Evaluating:  92%|█████████▏| 147/159 [00:18<00:01,  7.94it/s]01/08/2022 13:31:18 - INFO - __main__ -   Batch Number = 148
Evaluating:  93%|█████████▎| 148/159 [00:18<00:01,  7.95it/s]01/08/2022 13:31:18 - INFO - __main__ -   Batch Number = 149
Evaluating:  94%|█████████▎| 149/159 [00:18<00:01,  7.95it/s]01/08/2022 13:31:18 - INFO - __main__ -   Batch Number = 150
Evaluating:  94%|█████████▍| 150/159 [00:18<00:01,  7.96it/s]01/08/2022 13:31:18 - INFO - __main__ -   Batch Number = 151
Evaluating:  95%|█████████▍| 151/159 [00:19<00:01,  7.96it/s]01/08/2022 13:31:18 - INFO - __main__ -   Batch Number = 152
Evaluating:  96%|█████████▌| 152/159 [00:19<00:00,  7.96it/s]01/08/2022 13:31:18 - INFO - __main__ -   Batch Number = 153
Evaluating:  96%|█████████▌| 153/159 [00:19<00:00,  7.94it/s]01/08/2022 13:31:18 - INFO - __main__ -   Batch Number = 154
Evaluating:  97%|█████████▋| 154/159 [00:19<00:00,  7.95it/s]01/08/2022 13:31:18 - INFO - __main__ -   Batch Number = 155
Evaluating:  97%|█████████▋| 155/159 [00:19<00:00,  7.94it/s]01/08/2022 13:31:19 - INFO - __main__ -   Batch Number = 156
Evaluating:  98%|█████████▊| 156/159 [00:19<00:00,  7.94it/s]01/08/2022 13:31:19 - INFO - __main__ -   Batch Number = 157
Evaluating:  99%|█████████▊| 157/159 [00:19<00:00,  7.91it/s]01/08/2022 13:31:19 - INFO - __main__ -   Batch Number = 158
Evaluating:  99%|█████████▉| 158/159 [00:19<00:00,  7.88it/s]01/08/2022 13:31:19 - INFO - __main__ -   Batch Number = 159
Evaluating: 100%|██████████| 159/159 [00:19<00:00,  7.95it/s]
01/08/2022 13:31:19 - INFO - __main__ -     Evaluation done in total 19.999760 secs (0.015748 sec per example)
Writing predictions to: /home/abhijeet/rohan/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_ar/predictions_en_.json
Writing nbest to: /home/abhijeet/rohan/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_ar/nbest_predictions_en_.json
01/08/2022 13:31:23 - INFO - __main__ -   Results = OrderedDict([('exact', 70.67226890756302), ('f1', 81.89402910986222), ('total', 1190), ('HasAns_exact', 70.67226890756302), ('HasAns_f1', 81.89402910986222), ('HasAns_total', 1190), ('best_exact', 70.67226890756302), ('best_exact_thresh', 0.0), ('best_f1', 81.89402910986222), ('best_f1_thresh', 0.0)])
PyTorch version 1.10.0+cu102 available.
01/08/2022 16:58:06 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/08/2022 16:58:06 - INFO - root -   save model
loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6
Model config XLMRobertaConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /home/abhijeet/.cache/torch/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8
loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/08/2022 16:58:17 - INFO - __main__ -   lang2id = None
Traceback (most recent call last):
  File "third_party/my_run_squad.py", line 1112, in <module>
    main()
  File "third_party/my_run_squad.py", line 994, in main
    model.to(args.device)
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
KeyboardInterrupt
PyTorch version 1.10.0+cu102 available.
01/08/2022 16:58:22 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/08/2022 16:58:22 - INFO - root -   save model
loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6
Model config XLMRobertaConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /home/abhijeet/.cache/torch/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8
Traceback (most recent call last):
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/requests/api.py", line 61, in request
    return session.request(method=method, url=url, **kwargs)
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/requests/sessions.py", line 542, in request
    resp = self.send(prep, **send_kwargs)
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/requests/sessions.py", line 655, in send
    r = adapter.send(request, **kwargs)
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/requests/adapters.py", line 449, in send
    timeout=timeout
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/urllib3/connectionpool.py", line 696, in urlopen
    self._prepare_proxy(conn)
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/urllib3/connectionpool.py", line 964, in _prepare_proxy
    conn.connect()
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/urllib3/connection.py", line 426, in connect
    tls_in_tls=tls_in_tls,
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/urllib3/util/ssl_.py", line 450, in ssl_wrap_socket
    sock, context, tls_in_tls, server_hostname=server_hostname
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/urllib3/util/ssl_.py", line 493, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
  File "/usr/lib/python3.7/ssl.py", line 423, in wrap_socket
    session=session
  File "/usr/lib/python3.7/ssl.py", line 870, in _create
    self.do_handshake()
  File "/usr/lib/python3.7/ssl.py", line 1139, in do_handshake
    self._sslobj.do_handshake()
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "third_party/my_run_squad.py", line 1112, in <module>
    main()
  File "third_party/my_run_squad.py", line 987, in main
    model, tokenizer, lang2id, config_class, model_class, tokenizer_class = load_model(args)
  File "third_party/my_run_squad.py", line 768, in load_model
    cache_dir=args.cache_dir if args.cache_dir else None,
  File "/home/abhijeet/rohan/cloud-emea/src/transformers/modeling_utils.py", line 928, in from_pretrained
    local_files_only=local_files_only,
  File "/home/abhijeet/rohan/cloud-emea/src/transformers/file_utils.py", line 955, in cached_path
    local_files_only=local_files_only,
  File "/home/abhijeet/rohan/cloud-emea/src/transformers/file_utils.py", line 1075, in get_from_cache
    r = requests.head(url, headers=headers, allow_redirects=False, proxies=proxies, timeout=etag_timeout)
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/requests/api.py", line 102, in head
    return request('head', url, **kwargs)
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/requests/api.py", line 61, in request
    return session.request(method=method, url=url, **kwargs)
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/requests/sessions.py", line 428, in __exit__
    self.close()
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/requests/sessions.py", line 747, in close
    v.close()
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/requests/adapters.py", line 325, in close
    self.poolmanager.clear()
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/urllib3/poolmanager.py", line 222, in clear
    self.pools.clear()
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/urllib3/_collections.py", line 93, in clear
    with self.lock:
KeyboardInterrupt
PyTorch version 1.10.0+cu102 available.
01/08/2022 16:58:43 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/08/2022 16:58:43 - INFO - root -   save model
loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6
Model config XLMRobertaConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /home/abhijeet/.cache/torch/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8
loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/08/2022 16:58:53 - INFO - __main__ -   lang2id = None
01/08/2022 16:58:57 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='xlm-roberta-base', model_type='xlm-roberta', config_name=None, tokenizer_name=None, cache_dir=None, data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//xquad', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_ar/', max_seq_length=384, train_file='/home/abhijeet/rohan/cloud-emea-copy/data//xquad/train-v1.1.json', predict_file='/home/abhijeet/rohan/cloud-emea-copy/data//xquad/dev-v1.1.json', eval_test_set=False, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=4, per_gpu_eval_batch_size=8, gradient_accumulation_steps=4, learning_rate=0.0001, weight_decay=0.0001, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=15.0, max_steps=-1, save_steps=1000, warmup_steps=500, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', eval_lang='en', predict_langs='en', train_lang='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_ar//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='qna', threads=8, version_2_with_negative=False, verbose_logging=False, n_best_size=20, max_query_length=64, max_answer_length=30, doc_stride=128, null_score_diff_threshold=0.0, predict_task_adapter='output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s1/checkpoint-best/qna', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix=None, en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/08/2022 16:58:57 - INFO - __main__ -   Results: {}
loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6
Model config XLMRobertaConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /home/abhijeet/.cache/torch/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8
loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/08/2022 16:59:07 - INFO - __main__ -   lang2id = None
01/08/2022 16:59:07 - INFO - __main__ -   Evaluating the model on the test set of all languages specified
01/08/2022 16:59:07 - INFO - __main__ -   Adapter will be loaded from this path: output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s1/checkpoint-best/qna
01/08/2022 16:59:07 - INFO - root -   Trying to decide if add adapter
01/08/2022 16:59:07 - INFO - root -   loading task adapter
Loading module configuration from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s1/checkpoint-best/qna/adapter_config.json
Adding adapter 'qna' of type 'text_task'.
Loading module weights from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s1/checkpoint-best/qna/pytorch_adapter.bin
Loading module configuration from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s1/checkpoint-best/qna/head_config.json
Loading module weights from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s1/checkpoint-best/qna/pytorch_model_head.bin
01/08/2022 16:59:07 - INFO - root -   loading lang adpater ar/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/ar/xlm-roberta-base/pfeiffer/ar_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1f4018c6b187912fb6b47e664a34880506aef8c7320e0135939626a97e0bb137-4b1153cd3ebfdce09e1023c01e3ee723c32a0a57bea16e5ff13609f97eb2af0b-extracted/adapter_config.json
Adding adapter 'ar' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/1f4018c6b187912fb6b47e664a34880506aef8c7320e0135939626a97e0bb137-4b1153cd3ebfdce09e1023c01e3ee723c32a0a57bea16e5ff13609f97eb2af0b-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/1f4018c6b187912fb6b47e664a34880506aef8c7320e0135939626a97e0bb137-4b1153cd3ebfdce09e1023c01e3ee723c32a0a57bea16e5ff13609f97eb2af0b-extracted'
01/08/2022 16:59:09 - INFO - __main__ -   Language adapter for en not found, using ar instead
01/08/2022 16:59:09 - INFO - __main__ -   Set active language adapter to ar
01/08/2022 16:59:09 - INFO - __main__ -   Args Adapter Weight = None
01/08/2022 16:59:09 - INFO - __main__ -   Adapter Languages = ['ar']
01/08/2022 16:59:09 - INFO - __main__ -   Predict File = xquad.en.json
01/08/2022 16:59:09 - INFO - __main__ -   Creating features from dataset file at /home/abhijeet/rohan/cloud-emea-copy/data//xquad
ar ar/wiki@ukp
  0%|          | 0/48 [00:00<?, ?it/s] 29%|██▉       | 14/48 [00:00<00:00, 136.94it/s] 58%|█████▊    | 28/48 [00:00<00:00, 102.38it/s] 81%|████████▏ | 39/48 [00:00<00:00, 104.59it/s]100%|██████████| 48/48 [00:00<00:00, 110.56it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<04:21,  4.55it/s]convert squad examples to features:  27%|██▋       | 321/1190 [00:00<00:00, 1184.28it/s]convert squad examples to features:  41%|████      | 483/1190 [00:00<00:01, 570.07it/s] convert squad examples to features:  94%|█████████▍| 1121/1190 [00:00<00:00, 1561.53it/s]convert squad examples to features: 100%|██████████| 1190/1190 [00:00<00:00, 1229.42it/s]/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|██████████| 1190/1190 [00:00<00:00, 341513.63it/s]
01/08/2022 16:59:11 - INFO - __main__ -   Local Rank = -1
01/08/2022 16:59:11 - INFO - __main__ -   Saving features into cached file /home/abhijeet/rohan/cloud-emea-copy/data//xquad/cached_xquad.en.json_xlm-roberta-base_384_en
01/08/2022 16:59:13 - INFO - __main__ -   ***** Running evaluation  *****
01/08/2022 16:59:13 - INFO - __main__ -     Num examples = 1270
01/08/2022 16:59:13 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/159 [00:00<?, ?it/s]01/08/2022 16:59:13 - INFO - __main__ -   Batch Number = 1
Evaluating:   0%|          | 0/159 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "third_party/my_run_squad.py", line 1112, in <module>
    main()
  File "third_party/my_run_squad.py", line 1105, in main
    predict_and_save(args, adapter_args, model, tokenizer, lang2id, lang_adapter_names, task_name, 'test')
  File "third_party/my_run_squad.py", line 903, in predict_and_save
    results = evaluate(args, model, tokenizer, language=lang, lang2id=lang2id, adapter_weight=adapter_weight, mode=split)
  File "third_party/my_run_squad.py", line 485, in evaluate
    outputs = model(**inputs)
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
TypeError: forward() got an unexpected keyword argument 'adapter_weights'
PyTorch version 1.10.0+cu102 available.
01/08/2022 16:59:15 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/08/2022 16:59:15 - INFO - root -   save model
loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6
Model config XLMRobertaConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /home/abhijeet/.cache/torch/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8
loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/08/2022 16:59:25 - INFO - __main__ -   lang2id = None
01/08/2022 16:59:29 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='xlm-roberta-base', model_type='xlm-roberta', config_name=None, tokenizer_name=None, cache_dir=None, data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//xquad', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_ar/', max_seq_length=384, train_file='/home/abhijeet/rohan/cloud-emea-copy/data//xquad/train-v1.1.json', predict_file='/home/abhijeet/rohan/cloud-emea-copy/data//xquad/dev-v1.1.json', eval_test_set=False, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=4, per_gpu_eval_batch_size=8, gradient_accumulation_steps=4, learning_rate=0.0001, weight_decay=0.0001, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=15.0, max_steps=-1, save_steps=1000, warmup_steps=500, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', eval_lang='en', predict_langs='en', train_lang='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_ar//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='qna', threads=8, version_2_with_negative=False, verbose_logging=False, n_best_size=20, max_query_length=64, max_answer_length=30, doc_stride=128, null_score_diff_threshold=0.0, predict_task_adapter='output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s2/checkpoint-best/qna', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix=None, en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/08/2022 16:59:29 - INFO - __main__ -   Results: {}
loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6
Model config XLMRobertaConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /home/abhijeet/.cache/torch/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8
loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/08/2022 16:59:39 - INFO - __main__ -   lang2id = None
01/08/2022 16:59:40 - INFO - __main__ -   Evaluating the model on the test set of all languages specified
01/08/2022 16:59:40 - INFO - __main__ -   Adapter will be loaded from this path: output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s2/checkpoint-best/qna
01/08/2022 16:59:40 - INFO - root -   Trying to decide if add adapter
01/08/2022 16:59:40 - INFO - root -   loading task adapter
Loading module configuration from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s2/checkpoint-best/qna/adapter_config.json
Adding adapter 'qna' of type 'text_task'.
Loading module weights from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s2/checkpoint-best/qna/pytorch_adapter.bin
Loading module configuration from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s2/checkpoint-best/qna/head_config.json
Loading module weights from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s2/checkpoint-best/qna/pytorch_model_head.bin
01/08/2022 16:59:40 - INFO - root -   loading lang adpater ar/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/ar/xlm-roberta-base/pfeiffer/ar_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1f4018c6b187912fb6b47e664a34880506aef8c7320e0135939626a97e0bb137-4b1153cd3ebfdce09e1023c01e3ee723c32a0a57bea16e5ff13609f97eb2af0b-extracted/adapter_config.json
Adding adapter 'ar' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/1f4018c6b187912fb6b47e664a34880506aef8c7320e0135939626a97e0bb137-4b1153cd3ebfdce09e1023c01e3ee723c32a0a57bea16e5ff13609f97eb2af0b-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/1f4018c6b187912fb6b47e664a34880506aef8c7320e0135939626a97e0bb137-4b1153cd3ebfdce09e1023c01e3ee723c32a0a57bea16e5ff13609f97eb2af0b-extracted'
01/08/2022 16:59:41 - INFO - __main__ -   Language adapter for en not found, using ar instead
01/08/2022 16:59:41 - INFO - __main__ -   Set active language adapter to ar
01/08/2022 16:59:41 - INFO - __main__ -   Args Adapter Weight = None
01/08/2022 16:59:41 - INFO - __main__ -   Adapter Languages = ['ar']
01/08/2022 16:59:41 - INFO - __main__ -   Predict File = xquad.en.json
01/08/2022 16:59:41 - INFO - __main__ -   Creating features from dataset file at /home/abhijeet/rohan/cloud-emea-copy/data//xquad
ar ar/wiki@ukp
  0%|          | 0/48 [00:00<?, ?it/s] 31%|███▏      | 15/48 [00:00<00:00, 141.95it/s] 62%|██████▎   | 30/48 [00:00<00:00, 106.29it/s] 92%|█████████▏| 44/48 [00:00<00:00, 116.46it/s]100%|██████████| 48/48 [00:00<00:00, 117.96it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<03:36,  5.48it/s]convert squad examples to features:  27%|██▋       | 321/1190 [00:00<00:00, 1255.08it/s]convert squad examples to features:  39%|███▉      | 469/1190 [00:00<00:01, 603.45it/s] convert squad examples to features:  94%|█████████▍| 1121/1190 [00:00<00:00, 1698.75it/s]convert squad examples to features: 100%|██████████| 1190/1190 [00:00<00:00, 1345.84it/s]/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|██████████| 1190/1190 [00:00<00:00, 449093.19it/s]
01/08/2022 16:59:43 - INFO - __main__ -   Local Rank = -1
01/08/2022 16:59:43 - INFO - __main__ -   Saving features into cached file /home/abhijeet/rohan/cloud-emea-copy/data//xquad/cached_xquad.en.json_xlm-roberta-base_384_en
01/08/2022 16:59:44 - INFO - __main__ -   ***** Running evaluation  *****
01/08/2022 16:59:44 - INFO - __main__ -     Num examples = 1270
01/08/2022 16:59:44 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/159 [00:00<?, ?it/s]01/08/2022 16:59:44 - INFO - __main__ -   Batch Number = 1
Evaluating:   0%|          | 0/159 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "third_party/my_run_squad.py", line 1112, in <module>
    main()
  File "third_party/my_run_squad.py", line 1105, in main
    predict_and_save(args, adapter_args, model, tokenizer, lang2id, lang_adapter_names, task_name, 'test')
  File "third_party/my_run_squad.py", line 903, in predict_and_save
    results = evaluate(args, model, tokenizer, language=lang, lang2id=lang2id, adapter_weight=adapter_weight, mode=split)
  File "third_party/my_run_squad.py", line 485, in evaluate
    outputs = model(**inputs)
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
TypeError: forward() got an unexpected keyword argument 'adapter_weights'
PyTorch version 1.10.0+cu102 available.
01/08/2022 16:59:47 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/08/2022 16:59:47 - INFO - root -   save model
loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6
Model config XLMRobertaConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /home/abhijeet/.cache/torch/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8
loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/08/2022 16:59:57 - INFO - __main__ -   lang2id = None
01/08/2022 17:00:01 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='xlm-roberta-base', model_type='xlm-roberta', config_name=None, tokenizer_name=None, cache_dir=None, data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//xquad', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_ar/', max_seq_length=384, train_file='/home/abhijeet/rohan/cloud-emea-copy/data//xquad/train-v1.1.json', predict_file='/home/abhijeet/rohan/cloud-emea-copy/data//xquad/dev-v1.1.json', eval_test_set=False, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=4, per_gpu_eval_batch_size=8, gradient_accumulation_steps=4, learning_rate=0.0001, weight_decay=0.0001, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=15.0, max_steps=-1, save_steps=1000, warmup_steps=500, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', eval_lang='en', predict_langs='en', train_lang='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_ar//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='qna', threads=8, version_2_with_negative=False, verbose_logging=False, n_best_size=20, max_query_length=64, max_answer_length=30, doc_stride=128, null_score_diff_threshold=0.0, predict_task_adapter='output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s3/checkpoint-best/qna', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix=None, en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/08/2022 17:00:01 - INFO - __main__ -   Results: {}
loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6
Model config XLMRobertaConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /home/abhijeet/.cache/torch/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8
loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/08/2022 17:00:12 - INFO - __main__ -   lang2id = None
01/08/2022 17:00:12 - INFO - __main__ -   Evaluating the model on the test set of all languages specified
01/08/2022 17:00:12 - INFO - __main__ -   Adapter will be loaded from this path: output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s3/checkpoint-best/qna
01/08/2022 17:00:12 - INFO - root -   Trying to decide if add adapter
01/08/2022 17:00:12 - INFO - root -   loading task adapter
Loading module configuration from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s3/checkpoint-best/qna/adapter_config.json
Adding adapter 'qna' of type 'text_task'.
Loading module weights from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s3/checkpoint-best/qna/pytorch_adapter.bin
Loading module configuration from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s3/checkpoint-best/qna/head_config.json
Loading module weights from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s3/checkpoint-best/qna/pytorch_model_head.bin
01/08/2022 17:00:12 - INFO - root -   loading lang adpater ar/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/ar/xlm-roberta-base/pfeiffer/ar_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1f4018c6b187912fb6b47e664a34880506aef8c7320e0135939626a97e0bb137-4b1153cd3ebfdce09e1023c01e3ee723c32a0a57bea16e5ff13609f97eb2af0b-extracted/adapter_config.json
Adding adapter 'ar' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/1f4018c6b187912fb6b47e664a34880506aef8c7320e0135939626a97e0bb137-4b1153cd3ebfdce09e1023c01e3ee723c32a0a57bea16e5ff13609f97eb2af0b-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/1f4018c6b187912fb6b47e664a34880506aef8c7320e0135939626a97e0bb137-4b1153cd3ebfdce09e1023c01e3ee723c32a0a57bea16e5ff13609f97eb2af0b-extracted'
01/08/2022 17:00:13 - INFO - __main__ -   Language adapter for en not found, using ar instead
01/08/2022 17:00:13 - INFO - __main__ -   Set active language adapter to ar
01/08/2022 17:00:13 - INFO - __main__ -   Args Adapter Weight = None
01/08/2022 17:00:13 - INFO - __main__ -   Adapter Languages = ['ar']
01/08/2022 17:00:13 - INFO - __main__ -   Predict File = xquad.en.json
01/08/2022 17:00:13 - INFO - __main__ -   Creating features from dataset file at /home/abhijeet/rohan/cloud-emea-copy/data//xquad
ar ar/wiki@ukp
  0%|          | 0/48 [00:00<?, ?it/s] 29%|██▉       | 14/48 [00:00<00:00, 136.08it/s] 58%|█████▊    | 28/48 [00:00<00:00, 103.09it/s] 81%|████████▏ | 39/48 [00:00<00:00, 104.95it/s]100%|██████████| 48/48 [00:00<00:00, 111.82it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<03:27,  5.72it/s]convert squad examples to features:  24%|██▍       | 289/1190 [00:00<00:00, 1258.32it/s]convert squad examples to features:  38%|███▊      | 448/1190 [00:00<00:01, 566.31it/s] convert squad examples to features:  94%|█████████▍| 1121/1190 [00:00<00:00, 1662.05it/s]convert squad examples to features: 100%|██████████| 1190/1190 [00:00<00:00, 1314.69it/s]/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|██████████| 1190/1190 [00:00<00:00, 398787.29it/s]
01/08/2022 17:00:15 - INFO - __main__ -   Local Rank = -1
01/08/2022 17:00:15 - INFO - __main__ -   Saving features into cached file /home/abhijeet/rohan/cloud-emea-copy/data//xquad/cached_xquad.en.json_xlm-roberta-base_384_en
01/08/2022 17:00:17 - INFO - __main__ -   ***** Running evaluation  *****
01/08/2022 17:00:17 - INFO - __main__ -     Num examples = 1270
01/08/2022 17:00:17 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/159 [00:00<?, ?it/s]01/08/2022 17:00:17 - INFO - __main__ -   Batch Number = 1
Evaluating:   0%|          | 0/159 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "third_party/my_run_squad.py", line 1112, in <module>
    main()
  File "third_party/my_run_squad.py", line 1105, in main
    predict_and_save(args, adapter_args, model, tokenizer, lang2id, lang_adapter_names, task_name, 'test')
  File "third_party/my_run_squad.py", line 903, in predict_and_save
    results = evaluate(args, model, tokenizer, language=lang, lang2id=lang2id, adapter_weight=adapter_weight, mode=split)
  File "third_party/my_run_squad.py", line 485, in evaluate
    outputs = model(**inputs)
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
TypeError: forward() got an unexpected keyword argument 'adapter_weights'
PyTorch version 1.10.1+cu102 available.
01/09/2022 23:37:12 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/09/2022 23:37:12 - INFO - root -   save model
loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6
Model config XLMRobertaConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /home/abhijeet/.cache/torch/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8
loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/09/2022 23:37:22 - INFO - __main__ -   lang2id = None
01/09/2022 23:37:25 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='xlm-roberta-base', model_type='xlm-roberta', config_name=None, tokenizer_name=None, cache_dir=None, data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//xquad', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//squad/my-xlm-roberta-base-MaxLen384_qna_ar/', max_seq_length=384, train_file='/home/abhijeet/rohan/cloud-emea-copy/data//xquad/train-v1.1.json', predict_file='/home/abhijeet/rohan/cloud-emea-copy/data//xquad/dev-v1.1.json', eval_test_set=False, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=4, per_gpu_eval_batch_size=8, gradient_accumulation_steps=4, learning_rate=0.0001, weight_decay=0.0001, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=15.0, max_steps=-1, save_steps=1000, warmup_steps=500, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', eval_lang='en', predict_langs='ar', train_lang='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//squad/my-xlm-roberta-base-MaxLen384_qna_ar//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='qna', threads=8, version_2_with_negative=False, verbose_logging=False, n_best_size=20, max_query_length=64, max_answer_length=30, doc_stride=128, null_score_diff_threshold=0.0, predict_task_adapter='output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s1/checkpoint-best/qna', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix=None, en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/09/2022 23:37:25 - INFO - __main__ -   Results: {}
loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6
Model config XLMRobertaConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /home/abhijeet/.cache/torch/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8
loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/09/2022 23:37:36 - INFO - __main__ -   lang2id = None
01/09/2022 23:37:36 - INFO - __main__ -   Evaluating the model on the test set of all languages specified
01/09/2022 23:37:36 - INFO - __main__ -   Adapter will be loaded from this path: output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s1/checkpoint-best/qna
01/09/2022 23:37:36 - INFO - root -   Trying to decide if add adapter
01/09/2022 23:37:36 - INFO - root -   loading task adapter
Loading module configuration from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s1/checkpoint-best/qna/adapter_config.json
Adding adapter 'qna' of type 'text_task'.
Loading module weights from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s1/checkpoint-best/qna/pytorch_adapter.bin
Loading module configuration from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s1/checkpoint-best/qna/head_config.json
Loading module weights from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s1/checkpoint-best/qna/pytorch_model_head.bin
01/09/2022 23:37:36 - INFO - root -   loading lang adpater ar/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/ar/xlm-roberta-base/pfeiffer/ar_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1f4018c6b187912fb6b47e664a34880506aef8c7320e0135939626a97e0bb137-4b1153cd3ebfdce09e1023c01e3ee723c32a0a57bea16e5ff13609f97eb2af0b-extracted/adapter_config.json
Adding adapter 'ar' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/1f4018c6b187912fb6b47e664a34880506aef8c7320e0135939626a97e0bb137-4b1153cd3ebfdce09e1023c01e3ee723c32a0a57bea16e5ff13609f97eb2af0b-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/1f4018c6b187912fb6b47e664a34880506aef8c7320e0135939626a97e0bb137-4b1153cd3ebfdce09e1023c01e3ee723c32a0a57bea16e5ff13609f97eb2af0b-extracted'
01/09/2022 23:37:38 - INFO - __main__ -   Language adapter for ar found
01/09/2022 23:37:38 - INFO - __main__ -   Set active language adapter to ar
01/09/2022 23:37:38 - INFO - __main__ -   Args Adapter Weight = None
01/09/2022 23:37:38 - INFO - __main__ -   Adapter Languages = ['ar']
01/09/2022 23:37:38 - INFO - __main__ -   Predict File = xquad.ar.json
01/09/2022 23:37:38 - INFO - __main__ -   Creating features from dataset file at /home/abhijeet/rohan/cloud-emea-copy/data//xquad
ar ar/wiki@ukp
  0%|          | 0/48 [00:00<?, ?it/s] 33%|███▎      | 16/48 [00:00<00:00, 131.07it/s] 62%|██████▎   | 30/48 [00:00<00:00, 127.43it/s] 96%|█████████▌| 46/48 [00:00<00:00, 140.00it/s]100%|██████████| 48/48 [00:00<00:00, 138.19it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<05:08,  3.85it/s]convert squad examples to features:  32%|███▏      | 385/1190 [00:00<00:01, 588.94it/s]convert squad examples to features:  94%|█████████▍| 1121/1190 [00:00<00:00, 1729.08it/s]convert squad examples to features: 100%|██████████| 1190/1190 [00:00<00:00, 1369.61it/s]/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|██████████| 1190/1190 [00:00<00:00, 349868.34it/s]
01/09/2022 23:37:41 - INFO - __main__ -   Local Rank = -1
01/09/2022 23:37:41 - INFO - __main__ -   Saving features into cached file /home/abhijeet/rohan/cloud-emea-copy/data//xquad/cached_xquad.ar.json_xlm-roberta-base_384_ar
01/09/2022 23:37:42 - INFO - __main__ -   ***** Running evaluation  *****
01/09/2022 23:37:42 - INFO - __main__ -     Num examples = 1318
01/09/2022 23:37:42 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/165 [00:00<?, ?it/s]01/09/2022 23:37:42 - INFO - __main__ -   Batch Number = 1
Evaluating:   1%|          | 1/165 [00:00<00:24,  6.65it/s]01/09/2022 23:37:42 - INFO - __main__ -   Batch Number = 2
Evaluating:   1%|          | 2/165 [00:00<00:22,  7.24it/s]01/09/2022 23:37:42 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|▏         | 3/165 [00:00<00:21,  7.58it/s]01/09/2022 23:37:42 - INFO - __main__ -   Batch Number = 4
Evaluating:   2%|▏         | 4/165 [00:00<00:20,  7.85it/s]01/09/2022 23:37:43 - INFO - __main__ -   Batch Number = 5
Evaluating:   3%|▎         | 5/165 [00:00<00:19,  8.06it/s]01/09/2022 23:37:43 - INFO - __main__ -   Batch Number = 6
Evaluating:   4%|▎         | 6/165 [00:00<00:19,  8.18it/s]01/09/2022 23:37:43 - INFO - __main__ -   Batch Number = 7
Evaluating:   4%|▍         | 7/165 [00:00<00:19,  8.27it/s]01/09/2022 23:37:43 - INFO - __main__ -   Batch Number = 8
Evaluating:   5%|▍         | 8/165 [00:00<00:18,  8.33it/s]01/09/2022 23:37:43 - INFO - __main__ -   Batch Number = 9
Evaluating:   5%|▌         | 9/165 [00:01<00:18,  8.36it/s]01/09/2022 23:37:43 - INFO - __main__ -   Batch Number = 10
Evaluating:   6%|▌         | 10/165 [00:01<00:18,  8.37it/s]01/09/2022 23:37:43 - INFO - __main__ -   Batch Number = 11
Evaluating:   7%|▋         | 11/165 [00:01<00:18,  8.39it/s]01/09/2022 23:37:43 - INFO - __main__ -   Batch Number = 12
Evaluating:   7%|▋         | 12/165 [00:01<00:18,  8.40it/s]01/09/2022 23:37:44 - INFO - __main__ -   Batch Number = 13
Evaluating:   8%|▊         | 13/165 [00:01<00:25,  5.92it/s]01/09/2022 23:37:44 - INFO - __main__ -   Batch Number = 14
Evaluating:   8%|▊         | 14/165 [00:01<00:23,  6.49it/s]01/09/2022 23:37:44 - INFO - __main__ -   Batch Number = 15
Evaluating:   9%|▉         | 15/165 [00:02<00:22,  6.79it/s]01/09/2022 23:37:44 - INFO - __main__ -   Batch Number = 16
Evaluating:  10%|▉         | 16/165 [00:02<00:20,  7.21it/s]01/09/2022 23:37:44 - INFO - __main__ -   Batch Number = 17
Evaluating:  10%|█         | 17/165 [00:02<00:19,  7.53it/s]01/09/2022 23:37:44 - INFO - __main__ -   Batch Number = 18
Evaluating:  11%|█         | 18/165 [00:02<00:18,  7.74it/s]01/09/2022 23:37:44 - INFO - __main__ -   Batch Number = 19
Evaluating:  12%|█▏        | 19/165 [00:02<00:18,  7.93it/s]01/09/2022 23:37:45 - INFO - __main__ -   Batch Number = 20
Evaluating:  12%|█▏        | 20/165 [00:02<00:17,  8.07it/s]01/09/2022 23:37:45 - INFO - __main__ -   Batch Number = 21
Evaluating:  13%|█▎        | 21/165 [00:02<00:19,  7.50it/s]01/09/2022 23:37:45 - INFO - __main__ -   Batch Number = 22
Evaluating:  13%|█▎        | 22/165 [00:02<00:18,  7.74it/s]01/09/2022 23:37:45 - INFO - __main__ -   Batch Number = 23
Evaluating:  14%|█▍        | 23/165 [00:02<00:17,  7.93it/s]01/09/2022 23:37:45 - INFO - __main__ -   Batch Number = 24
Evaluating:  15%|█▍        | 24/165 [00:03<00:17,  8.05it/s]01/09/2022 23:37:45 - INFO - __main__ -   Batch Number = 25
Evaluating:  15%|█▌        | 25/165 [00:03<00:17,  8.16it/s]01/09/2022 23:37:45 - INFO - __main__ -   Batch Number = 26
Evaluating:  16%|█▌        | 26/165 [00:03<00:16,  8.20it/s]01/09/2022 23:37:45 - INFO - __main__ -   Batch Number = 27
Evaluating:  16%|█▋        | 27/165 [00:03<00:16,  8.27it/s]01/09/2022 23:37:46 - INFO - __main__ -   Batch Number = 28
Evaluating:  17%|█▋        | 28/165 [00:03<00:16,  8.32it/s]01/09/2022 23:37:46 - INFO - __main__ -   Batch Number = 29
Evaluating:  18%|█▊        | 29/165 [00:03<00:16,  8.35it/s]01/09/2022 23:37:46 - INFO - __main__ -   Batch Number = 30
Evaluating:  18%|█▊        | 30/165 [00:03<00:16,  8.38it/s]01/09/2022 23:37:46 - INFO - __main__ -   Batch Number = 31
Evaluating:  19%|█▉        | 31/165 [00:03<00:15,  8.39it/s]01/09/2022 23:37:46 - INFO - __main__ -   Batch Number = 32
Evaluating:  19%|█▉        | 32/165 [00:04<00:15,  8.39it/s]01/09/2022 23:37:46 - INFO - __main__ -   Batch Number = 33
Evaluating:  20%|██        | 33/165 [00:04<00:15,  8.40it/s]01/09/2022 23:37:46 - INFO - __main__ -   Batch Number = 34
Evaluating:  21%|██        | 34/165 [00:04<00:15,  8.41it/s]01/09/2022 23:37:46 - INFO - __main__ -   Batch Number = 35
Evaluating:  21%|██        | 35/165 [00:04<00:15,  8.41it/s]01/09/2022 23:37:46 - INFO - __main__ -   Batch Number = 36
Evaluating:  22%|██▏       | 36/165 [00:04<00:15,  8.41it/s]01/09/2022 23:37:47 - INFO - __main__ -   Batch Number = 37
Evaluating:  22%|██▏       | 37/165 [00:04<00:15,  8.41it/s]01/09/2022 23:37:47 - INFO - __main__ -   Batch Number = 38
Evaluating:  23%|██▎       | 38/165 [00:04<00:15,  8.40it/s]01/09/2022 23:37:47 - INFO - __main__ -   Batch Number = 39
Evaluating:  24%|██▎       | 39/165 [00:04<00:14,  8.41it/s]01/09/2022 23:37:47 - INFO - __main__ -   Batch Number = 40
Evaluating:  24%|██▍       | 40/165 [00:05<00:14,  8.41it/s]01/09/2022 23:37:47 - INFO - __main__ -   Batch Number = 41
Evaluating:  25%|██▍       | 41/165 [00:05<00:14,  8.39it/s]01/09/2022 23:37:47 - INFO - __main__ -   Batch Number = 42
Evaluating:  25%|██▌       | 42/165 [00:05<00:14,  8.40it/s]01/09/2022 23:37:47 - INFO - __main__ -   Batch Number = 43
Evaluating:  26%|██▌       | 43/165 [00:05<00:14,  8.40it/s]01/09/2022 23:37:47 - INFO - __main__ -   Batch Number = 44
Evaluating:  27%|██▋       | 44/165 [00:05<00:14,  8.40it/s]01/09/2022 23:37:48 - INFO - __main__ -   Batch Number = 45
Evaluating:  27%|██▋       | 45/165 [00:05<00:14,  8.40it/s]01/09/2022 23:37:48 - INFO - __main__ -   Batch Number = 46
Evaluating:  28%|██▊       | 46/165 [00:05<00:14,  8.41it/s]01/09/2022 23:37:48 - INFO - __main__ -   Batch Number = 47
Evaluating:  28%|██▊       | 47/165 [00:05<00:14,  8.42it/s]01/09/2022 23:37:48 - INFO - __main__ -   Batch Number = 48
Evaluating:  29%|██▉       | 48/165 [00:05<00:13,  8.42it/s]01/09/2022 23:37:48 - INFO - __main__ -   Batch Number = 49
Evaluating:  30%|██▉       | 49/165 [00:06<00:13,  8.40it/s]01/09/2022 23:37:48 - INFO - __main__ -   Batch Number = 50
Evaluating:  30%|███       | 50/165 [00:06<00:13,  8.40it/s]01/09/2022 23:37:48 - INFO - __main__ -   Batch Number = 51
Evaluating:  31%|███       | 51/165 [00:06<00:13,  8.40it/s]01/09/2022 23:37:48 - INFO - __main__ -   Batch Number = 52
Evaluating:  32%|███▏      | 52/165 [00:06<00:13,  8.40it/s]01/09/2022 23:37:49 - INFO - __main__ -   Batch Number = 53
Evaluating:  32%|███▏      | 53/165 [00:06<00:13,  8.39it/s]01/09/2022 23:37:49 - INFO - __main__ -   Batch Number = 54
Evaluating:  33%|███▎      | 54/165 [00:06<00:13,  8.38it/s]01/09/2022 23:37:49 - INFO - __main__ -   Batch Number = 55
Evaluating:  33%|███▎      | 55/165 [00:06<00:13,  8.37it/s]01/09/2022 23:37:49 - INFO - __main__ -   Batch Number = 56
Evaluating:  34%|███▍      | 56/165 [00:06<00:13,  8.35it/s]01/09/2022 23:37:49 - INFO - __main__ -   Batch Number = 57
Evaluating:  35%|███▍      | 57/165 [00:07<00:12,  8.36it/s]01/09/2022 23:37:49 - INFO - __main__ -   Batch Number = 58
Evaluating:  35%|███▌      | 58/165 [00:07<00:12,  8.36it/s]01/09/2022 23:37:49 - INFO - __main__ -   Batch Number = 59
Evaluating:  36%|███▌      | 59/165 [00:07<00:12,  8.35it/s]01/09/2022 23:37:49 - INFO - __main__ -   Batch Number = 60
Evaluating:  36%|███▋      | 60/165 [00:07<00:12,  8.36it/s]01/09/2022 23:37:49 - INFO - __main__ -   Batch Number = 61
Evaluating:  37%|███▋      | 61/165 [00:07<00:12,  8.36it/s]01/09/2022 23:37:50 - INFO - __main__ -   Batch Number = 62
Evaluating:  38%|███▊      | 62/165 [00:07<00:12,  8.35it/s]01/09/2022 23:37:50 - INFO - __main__ -   Batch Number = 63
Evaluating:  38%|███▊      | 63/165 [00:07<00:14,  6.90it/s]01/09/2022 23:37:50 - INFO - __main__ -   Batch Number = 64
Evaluating:  39%|███▉      | 64/165 [00:07<00:13,  7.27it/s]01/09/2022 23:37:50 - INFO - __main__ -   Batch Number = 65
Evaluating:  39%|███▉      | 65/165 [00:08<00:13,  7.57it/s]01/09/2022 23:37:50 - INFO - __main__ -   Batch Number = 66
Evaluating:  40%|████      | 66/165 [00:08<00:12,  7.80it/s]01/09/2022 23:37:50 - INFO - __main__ -   Batch Number = 67
Evaluating:  41%|████      | 67/165 [00:08<00:12,  7.97it/s]01/09/2022 23:37:50 - INFO - __main__ -   Batch Number = 68
Evaluating:  41%|████      | 68/165 [00:08<00:11,  8.09it/s]01/09/2022 23:37:51 - INFO - __main__ -   Batch Number = 69
Evaluating:  42%|████▏     | 69/165 [00:08<00:11,  8.17it/s]01/09/2022 23:37:51 - INFO - __main__ -   Batch Number = 70
Evaluating:  42%|████▏     | 70/165 [00:08<00:11,  8.21it/s]01/09/2022 23:37:51 - INFO - __main__ -   Batch Number = 71
Evaluating:  43%|████▎     | 71/165 [00:08<00:11,  8.19it/s]01/09/2022 23:37:51 - INFO - __main__ -   Batch Number = 72
Evaluating:  44%|████▎     | 72/165 [00:08<00:11,  8.24it/s]01/09/2022 23:37:51 - INFO - __main__ -   Batch Number = 73
Evaluating:  44%|████▍     | 73/165 [00:09<00:11,  8.28it/s]01/09/2022 23:37:51 - INFO - __main__ -   Batch Number = 74
Evaluating:  45%|████▍     | 74/165 [00:09<00:10,  8.30it/s]01/09/2022 23:37:51 - INFO - __main__ -   Batch Number = 75
Evaluating:  45%|████▌     | 75/165 [00:09<00:10,  8.32it/s]01/09/2022 23:37:51 - INFO - __main__ -   Batch Number = 76
Evaluating:  46%|████▌     | 76/165 [00:09<00:10,  8.34it/s]01/09/2022 23:37:51 - INFO - __main__ -   Batch Number = 77
Evaluating:  47%|████▋     | 77/165 [00:09<00:10,  8.34it/s]01/09/2022 23:37:52 - INFO - __main__ -   Batch Number = 78
Evaluating:  47%|████▋     | 78/165 [00:09<00:10,  8.35it/s]01/09/2022 23:37:52 - INFO - __main__ -   Batch Number = 79
Evaluating:  48%|████▊     | 79/165 [00:09<00:10,  8.35it/s]01/09/2022 23:37:52 - INFO - __main__ -   Batch Number = 80
Evaluating:  48%|████▊     | 80/165 [00:09<00:10,  8.36it/s]01/09/2022 23:37:52 - INFO - __main__ -   Batch Number = 81
Evaluating:  49%|████▉     | 81/165 [00:10<00:10,  8.36it/s]01/09/2022 23:37:52 - INFO - __main__ -   Batch Number = 82
Evaluating:  50%|████▉     | 82/165 [00:10<00:09,  8.36it/s]01/09/2022 23:37:52 - INFO - __main__ -   Batch Number = 83
Evaluating:  50%|█████     | 83/165 [00:10<00:09,  8.34it/s]01/09/2022 23:37:52 - INFO - __main__ -   Batch Number = 84
Evaluating:  51%|█████     | 84/165 [00:10<00:09,  8.36it/s]01/09/2022 23:37:52 - INFO - __main__ -   Batch Number = 85
Evaluating:  52%|█████▏    | 85/165 [00:10<00:09,  8.37it/s]01/09/2022 23:37:53 - INFO - __main__ -   Batch Number = 86
Evaluating:  52%|█████▏    | 86/165 [00:10<00:09,  8.35it/s]01/09/2022 23:37:53 - INFO - __main__ -   Batch Number = 87
Evaluating:  53%|█████▎    | 87/165 [00:10<00:09,  8.36it/s]01/09/2022 23:37:53 - INFO - __main__ -   Batch Number = 88
Evaluating:  53%|█████▎    | 88/165 [00:10<00:09,  8.37it/s]01/09/2022 23:37:53 - INFO - __main__ -   Batch Number = 89
Evaluating:  54%|█████▍    | 89/165 [00:10<00:09,  8.38it/s]01/09/2022 23:37:53 - INFO - __main__ -   Batch Number = 90
Evaluating:  55%|█████▍    | 90/165 [00:11<00:08,  8.39it/s]01/09/2022 23:37:53 - INFO - __main__ -   Batch Number = 91
Evaluating:  55%|█████▌    | 91/165 [00:11<00:08,  8.39it/s]01/09/2022 23:37:53 - INFO - __main__ -   Batch Number = 92
Evaluating:  56%|█████▌    | 92/165 [00:11<00:08,  8.38it/s]01/09/2022 23:37:53 - INFO - __main__ -   Batch Number = 93
Evaluating:  56%|█████▋    | 93/165 [00:11<00:08,  8.37it/s]01/09/2022 23:37:53 - INFO - __main__ -   Batch Number = 94
Evaluating:  57%|█████▋    | 94/165 [00:11<00:08,  8.37it/s]01/09/2022 23:37:54 - INFO - __main__ -   Batch Number = 95
Evaluating:  58%|█████▊    | 95/165 [00:11<00:08,  8.37it/s]01/09/2022 23:37:54 - INFO - __main__ -   Batch Number = 96
Evaluating:  58%|█████▊    | 96/165 [00:11<00:08,  8.38it/s]01/09/2022 23:37:54 - INFO - __main__ -   Batch Number = 97
Evaluating:  59%|█████▉    | 97/165 [00:11<00:08,  8.39it/s]01/09/2022 23:37:54 - INFO - __main__ -   Batch Number = 98
Evaluating:  59%|█████▉    | 98/165 [00:12<00:07,  8.38it/s]01/09/2022 23:37:54 - INFO - __main__ -   Batch Number = 99
Evaluating:  60%|██████    | 99/165 [00:12<00:07,  8.38it/s]01/09/2022 23:37:54 - INFO - __main__ -   Batch Number = 100
Evaluating:  61%|██████    | 100/165 [00:12<00:07,  8.39it/s]01/09/2022 23:37:54 - INFO - __main__ -   Batch Number = 101
Evaluating:  61%|██████    | 101/165 [00:12<00:07,  8.38it/s]01/09/2022 23:37:54 - INFO - __main__ -   Batch Number = 102
Evaluating:  62%|██████▏   | 102/165 [00:12<00:07,  8.38it/s]01/09/2022 23:37:55 - INFO - __main__ -   Batch Number = 103
Evaluating:  62%|██████▏   | 103/165 [00:12<00:07,  8.36it/s]01/09/2022 23:37:55 - INFO - __main__ -   Batch Number = 104
Evaluating:  63%|██████▎   | 104/165 [00:12<00:07,  8.37it/s]01/09/2022 23:37:55 - INFO - __main__ -   Batch Number = 105
Evaluating:  64%|██████▎   | 105/165 [00:12<00:07,  8.36it/s]01/09/2022 23:37:55 - INFO - __main__ -   Batch Number = 106
Evaluating:  64%|██████▍   | 106/165 [00:12<00:07,  8.37it/s]01/09/2022 23:37:55 - INFO - __main__ -   Batch Number = 107
Evaluating:  65%|██████▍   | 107/165 [00:13<00:06,  8.38it/s]01/09/2022 23:37:55 - INFO - __main__ -   Batch Number = 108
Evaluating:  65%|██████▌   | 108/165 [00:13<00:06,  8.37it/s]01/09/2022 23:37:55 - INFO - __main__ -   Batch Number = 109
Evaluating:  66%|██████▌   | 109/165 [00:13<00:06,  8.37it/s]01/09/2022 23:37:55 - INFO - __main__ -   Batch Number = 110
Evaluating:  67%|██████▋   | 110/165 [00:13<00:06,  8.38it/s]01/09/2022 23:37:56 - INFO - __main__ -   Batch Number = 111
Evaluating:  67%|██████▋   | 111/165 [00:13<00:06,  8.38it/s]01/09/2022 23:37:56 - INFO - __main__ -   Batch Number = 112
Evaluating:  68%|██████▊   | 112/165 [00:13<00:06,  8.35it/s]01/09/2022 23:37:56 - INFO - __main__ -   Batch Number = 113
Evaluating:  68%|██████▊   | 113/165 [00:13<00:06,  8.36it/s]01/09/2022 23:37:56 - INFO - __main__ -   Batch Number = 114
Evaluating:  69%|██████▉   | 114/165 [00:13<00:06,  8.36it/s]01/09/2022 23:37:56 - INFO - __main__ -   Batch Number = 115
Evaluating:  70%|██████▉   | 115/165 [00:14<00:05,  8.36it/s]01/09/2022 23:37:56 - INFO - __main__ -   Batch Number = 116
Evaluating:  70%|███████   | 116/165 [00:14<00:05,  8.34it/s]01/09/2022 23:37:56 - INFO - __main__ -   Batch Number = 117
Evaluating:  71%|███████   | 117/165 [00:14<00:05,  8.34it/s]01/09/2022 23:37:56 - INFO - __main__ -   Batch Number = 118
Evaluating:  72%|███████▏  | 118/165 [00:14<00:05,  8.33it/s]01/09/2022 23:37:56 - INFO - __main__ -   Batch Number = 119
Evaluating:  72%|███████▏  | 119/165 [00:14<00:05,  8.33it/s]01/09/2022 23:37:57 - INFO - __main__ -   Batch Number = 120
Evaluating:  73%|███████▎  | 120/165 [00:14<00:05,  8.32it/s]01/09/2022 23:37:57 - INFO - __main__ -   Batch Number = 121
Evaluating:  73%|███████▎  | 121/165 [00:14<00:05,  8.32it/s]01/09/2022 23:37:57 - INFO - __main__ -   Batch Number = 122
Evaluating:  74%|███████▍  | 122/165 [00:14<00:05,  8.29it/s]01/09/2022 23:37:57 - INFO - __main__ -   Batch Number = 123
Evaluating:  75%|███████▍  | 123/165 [00:15<00:05,  8.29it/s]01/09/2022 23:37:57 - INFO - __main__ -   Batch Number = 124
Evaluating:  75%|███████▌  | 124/165 [00:15<00:04,  8.32it/s]01/09/2022 23:37:57 - INFO - __main__ -   Batch Number = 125
Evaluating:  76%|███████▌  | 125/165 [00:15<00:04,  8.34it/s]01/09/2022 23:37:57 - INFO - __main__ -   Batch Number = 126
Evaluating:  76%|███████▋  | 126/165 [00:15<00:04,  8.33it/s]01/09/2022 23:37:57 - INFO - __main__ -   Batch Number = 127
Evaluating:  77%|███████▋  | 127/165 [00:15<00:04,  8.34it/s]01/09/2022 23:37:58 - INFO - __main__ -   Batch Number = 128
Evaluating:  78%|███████▊  | 128/165 [00:15<00:04,  8.35it/s]01/09/2022 23:37:58 - INFO - __main__ -   Batch Number = 129
Evaluating:  78%|███████▊  | 129/165 [00:15<00:04,  8.35it/s]01/09/2022 23:37:58 - INFO - __main__ -   Batch Number = 130
Evaluating:  79%|███████▉  | 130/165 [00:15<00:04,  8.35it/s]01/09/2022 23:37:58 - INFO - __main__ -   Batch Number = 131
Evaluating:  79%|███████▉  | 131/165 [00:15<00:04,  8.33it/s]01/09/2022 23:37:58 - INFO - __main__ -   Batch Number = 132
Evaluating:  80%|████████  | 132/165 [00:16<00:03,  8.34it/s]01/09/2022 23:37:58 - INFO - __main__ -   Batch Number = 133
Evaluating:  81%|████████  | 133/165 [00:16<00:03,  8.34it/s]01/09/2022 23:37:58 - INFO - __main__ -   Batch Number = 134
Evaluating:  81%|████████  | 134/165 [00:16<00:03,  8.34it/s]01/09/2022 23:37:58 - INFO - __main__ -   Batch Number = 135
Evaluating:  82%|████████▏ | 135/165 [00:16<00:03,  8.34it/s]01/09/2022 23:37:59 - INFO - __main__ -   Batch Number = 136
Evaluating:  82%|████████▏ | 136/165 [00:16<00:03,  8.35it/s]01/09/2022 23:37:59 - INFO - __main__ -   Batch Number = 137
Evaluating:  83%|████████▎ | 137/165 [00:16<00:03,  8.36it/s]01/09/2022 23:37:59 - INFO - __main__ -   Batch Number = 138
Evaluating:  84%|████████▎ | 138/165 [00:16<00:03,  8.36it/s]01/09/2022 23:37:59 - INFO - __main__ -   Batch Number = 139
Evaluating:  84%|████████▍ | 139/165 [00:16<00:03,  8.36it/s]01/09/2022 23:37:59 - INFO - __main__ -   Batch Number = 140
Evaluating:  85%|████████▍ | 140/165 [00:17<00:02,  8.37it/s]01/09/2022 23:37:59 - INFO - __main__ -   Batch Number = 141
Evaluating:  85%|████████▌ | 141/165 [00:17<00:02,  8.36it/s]01/09/2022 23:37:59 - INFO - __main__ -   Batch Number = 142
Evaluating:  86%|████████▌ | 142/165 [00:17<00:02,  8.37it/s]01/09/2022 23:37:59 - INFO - __main__ -   Batch Number = 143
Evaluating:  87%|████████▋ | 143/165 [00:17<00:02,  8.36it/s]01/09/2022 23:37:59 - INFO - __main__ -   Batch Number = 144
Evaluating:  87%|████████▋ | 144/165 [00:17<00:02,  8.36it/s]01/09/2022 23:38:00 - INFO - __main__ -   Batch Number = 145
Evaluating:  88%|████████▊ | 145/165 [00:17<00:02,  8.36it/s]01/09/2022 23:38:00 - INFO - __main__ -   Batch Number = 146
Evaluating:  88%|████████▊ | 146/165 [00:17<00:02,  8.32it/s]01/09/2022 23:38:00 - INFO - __main__ -   Batch Number = 147
Evaluating:  89%|████████▉ | 147/165 [00:17<00:02,  8.33it/s]01/09/2022 23:38:00 - INFO - __main__ -   Batch Number = 148
Evaluating:  90%|████████▉ | 148/165 [00:18<00:02,  8.34it/s]01/09/2022 23:38:00 - INFO - __main__ -   Batch Number = 149
Evaluating:  90%|█████████ | 149/165 [00:18<00:01,  8.35it/s]01/09/2022 23:38:00 - INFO - __main__ -   Batch Number = 150
Evaluating:  91%|█████████ | 150/165 [00:18<00:01,  8.36it/s]01/09/2022 23:38:00 - INFO - __main__ -   Batch Number = 151
Evaluating:  92%|█████████▏| 151/165 [00:18<00:01,  8.36it/s]01/09/2022 23:38:00 - INFO - __main__ -   Batch Number = 152
Evaluating:  92%|█████████▏| 152/165 [00:18<00:01,  8.36it/s]01/09/2022 23:38:01 - INFO - __main__ -   Batch Number = 153
Evaluating:  93%|█████████▎| 153/165 [00:18<00:01,  8.36it/s]01/09/2022 23:38:01 - INFO - __main__ -   Batch Number = 154
Evaluating:  93%|█████████▎| 154/165 [00:18<00:01,  8.36it/s]01/09/2022 23:38:01 - INFO - __main__ -   Batch Number = 155
Evaluating:  94%|█████████▍| 155/165 [00:18<00:01,  8.36it/s]01/09/2022 23:38:01 - INFO - __main__ -   Batch Number = 156
Evaluating:  95%|█████████▍| 156/165 [00:18<00:01,  8.35it/s]01/09/2022 23:38:01 - INFO - __main__ -   Batch Number = 157
Evaluating:  95%|█████████▌| 157/165 [00:19<00:00,  8.33it/s]01/09/2022 23:38:01 - INFO - __main__ -   Batch Number = 158
Evaluating:  96%|█████████▌| 158/165 [00:19<00:00,  8.35it/s]01/09/2022 23:38:01 - INFO - __main__ -   Batch Number = 159
Evaluating:  96%|█████████▋| 159/165 [00:19<00:00,  8.34it/s]01/09/2022 23:38:01 - INFO - __main__ -   Batch Number = 160
Evaluating:  97%|█████████▋| 160/165 [00:19<00:00,  8.31it/s]01/09/2022 23:38:02 - INFO - __main__ -   Batch Number = 161
Evaluating:  98%|█████████▊| 161/165 [00:19<00:00,  8.29it/s]01/09/2022 23:38:02 - INFO - __main__ -   Batch Number = 162
Evaluating:  98%|█████████▊| 162/165 [00:19<00:00,  8.30it/s]01/09/2022 23:38:02 - INFO - __main__ -   Batch Number = 163
Evaluating:  99%|█████████▉| 163/165 [00:19<00:00,  8.29it/s]01/09/2022 23:38:02 - INFO - __main__ -   Batch Number = 164
Evaluating:  99%|█████████▉| 164/165 [00:19<00:00,  8.28it/s]01/09/2022 23:38:02 - INFO - __main__ -   Batch Number = 165
Evaluating: 100%|██████████| 165/165 [00:20<00:00,  8.24it/s]
01/09/2022 23:38:02 - INFO - __main__ -     Evaluation done in total 20.034980 secs (0.015201 sec per example)
Writing predictions to: /home/abhijeet/rohan/cloud-emea-copy/outputs//squad/my-xlm-roberta-base-MaxLen384_qna_ar/predictions_ar_.json
Writing nbest to: /home/abhijeet/rohan/cloud-emea-copy/outputs//squad/my-xlm-roberta-base-MaxLen384_qna_ar/nbest_predictions_ar_.json
01/09/2022 23:38:06 - INFO - __main__ -   Results = OrderedDict([('exact', 48.15126050420168), ('f1', 64.14469544816643), ('total', 1190), ('HasAns_exact', 48.15126050420168), ('HasAns_f1', 64.14469544816643), ('HasAns_total', 1190), ('best_exact', 48.15126050420168), ('best_exact_thresh', 0.0), ('best_f1', 64.14469544816643), ('best_f1_thresh', 0.0)])
PyTorch version 1.10.1+cu102 available.
01/09/2022 23:38:08 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/09/2022 23:38:08 - INFO - root -   save model
loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6
Model config XLMRobertaConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /home/abhijeet/.cache/torch/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8
loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/09/2022 23:38:18 - INFO - __main__ -   lang2id = None
01/09/2022 23:38:21 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='xlm-roberta-base', model_type='xlm-roberta', config_name=None, tokenizer_name=None, cache_dir=None, data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//xquad', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//squad/my-xlm-roberta-base-MaxLen384_qna_ar/', max_seq_length=384, train_file='/home/abhijeet/rohan/cloud-emea-copy/data//xquad/train-v1.1.json', predict_file='/home/abhijeet/rohan/cloud-emea-copy/data//xquad/dev-v1.1.json', eval_test_set=False, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=4, per_gpu_eval_batch_size=8, gradient_accumulation_steps=4, learning_rate=0.0001, weight_decay=0.0001, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=15.0, max_steps=-1, save_steps=1000, warmup_steps=500, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', eval_lang='en', predict_langs='ar', train_lang='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//squad/my-xlm-roberta-base-MaxLen384_qna_ar//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='qna', threads=8, version_2_with_negative=False, verbose_logging=False, n_best_size=20, max_query_length=64, max_answer_length=30, doc_stride=128, null_score_diff_threshold=0.0, predict_task_adapter='output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s2/checkpoint-best/qna', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix=None, en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/09/2022 23:38:21 - INFO - __main__ -   Results: {}
loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6
Model config XLMRobertaConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /home/abhijeet/.cache/torch/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8
loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/09/2022 23:38:31 - INFO - __main__ -   lang2id = None
01/09/2022 23:38:31 - INFO - __main__ -   Evaluating the model on the test set of all languages specified
01/09/2022 23:38:31 - INFO - __main__ -   Adapter will be loaded from this path: output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s2/checkpoint-best/qna
01/09/2022 23:38:31 - INFO - root -   Trying to decide if add adapter
01/09/2022 23:38:31 - INFO - root -   loading task adapter
Loading module configuration from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s2/checkpoint-best/qna/adapter_config.json
Adding adapter 'qna' of type 'text_task'.
Loading module weights from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s2/checkpoint-best/qna/pytorch_adapter.bin
Loading module configuration from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s2/checkpoint-best/qna/head_config.json
Loading module weights from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s2/checkpoint-best/qna/pytorch_model_head.bin
01/09/2022 23:38:31 - INFO - root -   loading lang adpater ar/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/ar/xlm-roberta-base/pfeiffer/ar_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1f4018c6b187912fb6b47e664a34880506aef8c7320e0135939626a97e0bb137-4b1153cd3ebfdce09e1023c01e3ee723c32a0a57bea16e5ff13609f97eb2af0b-extracted/adapter_config.json
Adding adapter 'ar' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/1f4018c6b187912fb6b47e664a34880506aef8c7320e0135939626a97e0bb137-4b1153cd3ebfdce09e1023c01e3ee723c32a0a57bea16e5ff13609f97eb2af0b-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/1f4018c6b187912fb6b47e664a34880506aef8c7320e0135939626a97e0bb137-4b1153cd3ebfdce09e1023c01e3ee723c32a0a57bea16e5ff13609f97eb2af0b-extracted'
01/09/2022 23:38:33 - INFO - __main__ -   Language adapter for ar found
01/09/2022 23:38:33 - INFO - __main__ -   Set active language adapter to ar
01/09/2022 23:38:33 - INFO - __main__ -   Args Adapter Weight = None
01/09/2022 23:38:33 - INFO - __main__ -   Adapter Languages = ['ar']
01/09/2022 23:38:33 - INFO - __main__ -   Predict File = xquad.ar.json
01/09/2022 23:38:33 - INFO - __main__ -   Creating features from dataset file at /home/abhijeet/rohan/cloud-emea-copy/data//xquad
ar ar/wiki@ukp
  0%|          | 0/48 [00:00<?, ?it/s] 33%|███▎      | 16/48 [00:00<00:00, 131.45it/s] 62%|██████▎   | 30/48 [00:00<00:00, 127.13it/s] 96%|█████████▌| 46/48 [00:00<00:00, 138.76it/s]100%|██████████| 48/48 [00:00<00:00, 137.31it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<04:48,  4.13it/s]convert squad examples to features:  32%|███▏      | 385/1190 [00:00<00:01, 551.49it/s]convert squad examples to features:  97%|█████████▋| 1153/1190 [00:00<00:00, 1714.65it/s]convert squad examples to features: 100%|██████████| 1190/1190 [00:00<00:00, 1340.85it/s]/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|██████████| 1190/1190 [00:00<00:00, 347820.33it/s]
01/09/2022 23:38:35 - INFO - __main__ -   Local Rank = -1
01/09/2022 23:38:35 - INFO - __main__ -   Saving features into cached file /home/abhijeet/rohan/cloud-emea-copy/data//xquad/cached_xquad.ar.json_xlm-roberta-base_384_ar
01/09/2022 23:38:36 - INFO - __main__ -   ***** Running evaluation  *****
01/09/2022 23:38:36 - INFO - __main__ -     Num examples = 1318
01/09/2022 23:38:36 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/165 [00:00<?, ?it/s]01/09/2022 23:38:36 - INFO - __main__ -   Batch Number = 1
Evaluating:   1%|          | 1/165 [00:00<00:23,  6.93it/s]01/09/2022 23:38:36 - INFO - __main__ -   Batch Number = 2
Evaluating:   1%|          | 2/165 [00:00<00:21,  7.61it/s]01/09/2022 23:38:36 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|▏         | 3/165 [00:00<00:20,  7.91it/s]01/09/2022 23:38:36 - INFO - __main__ -   Batch Number = 4
Evaluating:   2%|▏         | 4/165 [00:00<00:19,  8.08it/s]01/09/2022 23:38:37 - INFO - __main__ -   Batch Number = 5
Evaluating:   3%|▎         | 5/165 [00:00<00:19,  8.17it/s]01/09/2022 23:38:37 - INFO - __main__ -   Batch Number = 6
Evaluating:   4%|▎         | 6/165 [00:00<00:25,  6.13it/s]01/09/2022 23:38:37 - INFO - __main__ -   Batch Number = 7
Evaluating:   4%|▍         | 7/165 [00:00<00:23,  6.72it/s]01/09/2022 23:38:37 - INFO - __main__ -   Batch Number = 8
Evaluating:   5%|▍         | 8/165 [00:01<00:21,  7.17it/s]01/09/2022 23:38:37 - INFO - __main__ -   Batch Number = 9
Evaluating:   5%|▌         | 9/165 [00:01<00:20,  7.52it/s]01/09/2022 23:38:37 - INFO - __main__ -   Batch Number = 10
Evaluating:   6%|▌         | 10/165 [00:01<00:19,  7.76it/s]01/09/2022 23:38:37 - INFO - __main__ -   Batch Number = 11
Evaluating:   7%|▋         | 11/165 [00:01<00:19,  7.92it/s]01/09/2022 23:38:37 - INFO - __main__ -   Batch Number = 12
Evaluating:   7%|▋         | 12/165 [00:01<00:19,  8.04it/s]01/09/2022 23:38:38 - INFO - __main__ -   Batch Number = 13
Evaluating:   8%|▊         | 13/165 [00:01<00:27,  5.62it/s]01/09/2022 23:38:38 - INFO - __main__ -   Batch Number = 14
Evaluating:   8%|▊         | 14/165 [00:02<00:24,  6.23it/s]01/09/2022 23:38:38 - INFO - __main__ -   Batch Number = 15
Evaluating:   9%|▉         | 15/165 [00:02<00:23,  6.50it/s]01/09/2022 23:38:38 - INFO - __main__ -   Batch Number = 16
Evaluating:  10%|▉         | 16/165 [00:02<00:21,  6.97it/s]01/09/2022 23:38:38 - INFO - __main__ -   Batch Number = 17
Evaluating:  10%|█         | 17/165 [00:02<00:20,  7.33it/s]01/09/2022 23:38:38 - INFO - __main__ -   Batch Number = 18
Evaluating:  11%|█         | 18/165 [00:02<00:19,  7.61it/s]01/09/2022 23:38:39 - INFO - __main__ -   Batch Number = 19
Evaluating:  12%|█▏        | 19/165 [00:02<00:18,  7.81it/s]01/09/2022 23:38:39 - INFO - __main__ -   Batch Number = 20
Evaluating:  12%|█▏        | 20/165 [00:02<00:18,  7.97it/s]01/09/2022 23:38:39 - INFO - __main__ -   Batch Number = 21
Evaluating:  13%|█▎        | 21/165 [00:02<00:17,  8.08it/s]01/09/2022 23:38:39 - INFO - __main__ -   Batch Number = 22
Evaluating:  13%|█▎        | 22/165 [00:02<00:17,  8.16it/s]01/09/2022 23:38:39 - INFO - __main__ -   Batch Number = 23
Evaluating:  14%|█▍        | 23/165 [00:03<00:17,  8.22it/s]01/09/2022 23:38:39 - INFO - __main__ -   Batch Number = 24
Evaluating:  15%|█▍        | 24/165 [00:03<00:17,  8.27it/s]01/09/2022 23:38:39 - INFO - __main__ -   Batch Number = 25
Evaluating:  15%|█▌        | 25/165 [00:03<00:16,  8.30it/s]01/09/2022 23:38:39 - INFO - __main__ -   Batch Number = 26
Evaluating:  16%|█▌        | 26/165 [00:03<00:16,  8.31it/s]01/09/2022 23:38:39 - INFO - __main__ -   Batch Number = 27
Evaluating:  16%|█▋        | 27/165 [00:03<00:16,  8.32it/s]01/09/2022 23:38:40 - INFO - __main__ -   Batch Number = 28
Evaluating:  17%|█▋        | 28/165 [00:03<00:16,  8.34it/s]01/09/2022 23:38:40 - INFO - __main__ -   Batch Number = 29
Evaluating:  18%|█▊        | 29/165 [00:03<00:16,  8.34it/s]01/09/2022 23:38:40 - INFO - __main__ -   Batch Number = 30
Evaluating:  18%|█▊        | 30/165 [00:03<00:16,  8.34it/s]01/09/2022 23:38:40 - INFO - __main__ -   Batch Number = 31
Evaluating:  19%|█▉        | 31/165 [00:04<00:16,  8.34it/s]01/09/2022 23:38:40 - INFO - __main__ -   Batch Number = 32
Evaluating:  19%|█▉        | 32/165 [00:04<00:15,  8.33it/s]01/09/2022 23:38:40 - INFO - __main__ -   Batch Number = 33
Evaluating:  20%|██        | 33/165 [00:04<00:15,  8.33it/s]01/09/2022 23:38:40 - INFO - __main__ -   Batch Number = 34
Evaluating:  21%|██        | 34/165 [00:04<00:15,  8.34it/s]01/09/2022 23:38:40 - INFO - __main__ -   Batch Number = 35
Evaluating:  21%|██        | 35/165 [00:04<00:15,  8.34it/s]01/09/2022 23:38:41 - INFO - __main__ -   Batch Number = 36
Evaluating:  22%|██▏       | 36/165 [00:04<00:15,  8.34it/s]01/09/2022 23:38:41 - INFO - __main__ -   Batch Number = 37
Evaluating:  22%|██▏       | 37/165 [00:04<00:15,  8.33it/s]01/09/2022 23:38:41 - INFO - __main__ -   Batch Number = 38
Evaluating:  23%|██▎       | 38/165 [00:04<00:15,  8.33it/s]01/09/2022 23:38:41 - INFO - __main__ -   Batch Number = 39
Evaluating:  24%|██▎       | 39/165 [00:05<00:15,  8.32it/s]01/09/2022 23:38:41 - INFO - __main__ -   Batch Number = 40
Evaluating:  24%|██▍       | 40/165 [00:05<00:15,  8.33it/s]01/09/2022 23:38:41 - INFO - __main__ -   Batch Number = 41
Evaluating:  25%|██▍       | 41/165 [00:05<00:14,  8.31it/s]01/09/2022 23:38:41 - INFO - __main__ -   Batch Number = 42
Evaluating:  25%|██▌       | 42/165 [00:05<00:14,  8.32it/s]01/09/2022 23:38:41 - INFO - __main__ -   Batch Number = 43
Evaluating:  26%|██▌       | 43/165 [00:05<00:14,  8.30it/s]01/09/2022 23:38:42 - INFO - __main__ -   Batch Number = 44
Evaluating:  27%|██▋       | 44/165 [00:05<00:14,  8.30it/s]01/09/2022 23:38:42 - INFO - __main__ -   Batch Number = 45
Evaluating:  27%|██▋       | 45/165 [00:05<00:14,  8.30it/s]01/09/2022 23:38:42 - INFO - __main__ -   Batch Number = 46
Evaluating:  28%|██▊       | 46/165 [00:05<00:14,  8.20it/s]01/09/2022 23:38:42 - INFO - __main__ -   Batch Number = 47
Evaluating:  28%|██▊       | 47/165 [00:05<00:14,  8.22it/s]01/09/2022 23:38:42 - INFO - __main__ -   Batch Number = 48
Evaluating:  29%|██▉       | 48/165 [00:06<00:14,  8.26it/s]01/09/2022 23:38:42 - INFO - __main__ -   Batch Number = 49
Evaluating:  30%|██▉       | 49/165 [00:06<00:14,  8.28it/s]01/09/2022 23:38:42 - INFO - __main__ -   Batch Number = 50
Evaluating:  30%|███       | 50/165 [00:06<00:13,  8.30it/s]01/09/2022 23:38:42 - INFO - __main__ -   Batch Number = 51
Evaluating:  31%|███       | 51/165 [00:06<00:13,  8.31it/s]01/09/2022 23:38:43 - INFO - __main__ -   Batch Number = 52
Evaluating:  32%|███▏      | 52/165 [00:06<00:13,  8.30it/s]01/09/2022 23:38:43 - INFO - __main__ -   Batch Number = 53
Evaluating:  32%|███▏      | 53/165 [00:06<00:13,  8.28it/s]01/09/2022 23:38:43 - INFO - __main__ -   Batch Number = 54
Evaluating:  33%|███▎      | 54/165 [00:06<00:13,  8.28it/s]01/09/2022 23:38:43 - INFO - __main__ -   Batch Number = 55
Evaluating:  33%|███▎      | 55/165 [00:06<00:13,  8.27it/s]01/09/2022 23:38:43 - INFO - __main__ -   Batch Number = 56
Evaluating:  34%|███▍      | 56/165 [00:07<00:13,  8.25it/s]01/09/2022 23:38:43 - INFO - __main__ -   Batch Number = 57
Evaluating:  35%|███▍      | 57/165 [00:07<00:13,  8.25it/s]01/09/2022 23:38:43 - INFO - __main__ -   Batch Number = 58
Evaluating:  35%|███▌      | 58/165 [00:07<00:12,  8.25it/s]01/09/2022 23:38:43 - INFO - __main__ -   Batch Number = 59
Evaluating:  36%|███▌      | 59/165 [00:07<00:12,  8.25it/s]01/09/2022 23:38:43 - INFO - __main__ -   Batch Number = 60
Evaluating:  36%|███▋      | 60/165 [00:07<00:12,  8.26it/s]01/09/2022 23:38:44 - INFO - __main__ -   Batch Number = 61
Evaluating:  37%|███▋      | 61/165 [00:07<00:12,  8.26it/s]01/09/2022 23:38:44 - INFO - __main__ -   Batch Number = 62
Evaluating:  38%|███▊      | 62/165 [00:07<00:12,  8.28it/s]01/09/2022 23:38:44 - INFO - __main__ -   Batch Number = 63
Evaluating:  38%|███▊      | 63/165 [00:07<00:12,  8.29it/s]01/09/2022 23:38:44 - INFO - __main__ -   Batch Number = 64
Evaluating:  39%|███▉      | 64/165 [00:08<00:12,  8.30it/s]01/09/2022 23:38:44 - INFO - __main__ -   Batch Number = 65
Evaluating:  39%|███▉      | 65/165 [00:08<00:12,  8.30it/s]01/09/2022 23:38:44 - INFO - __main__ -   Batch Number = 66
Evaluating:  40%|████      | 66/165 [00:08<00:11,  8.30it/s]01/09/2022 23:38:44 - INFO - __main__ -   Batch Number = 67
Evaluating:  41%|████      | 67/165 [00:08<00:11,  8.30it/s]01/09/2022 23:38:44 - INFO - __main__ -   Batch Number = 68
Evaluating:  41%|████      | 68/165 [00:08<00:11,  8.31it/s]01/09/2022 23:38:45 - INFO - __main__ -   Batch Number = 69
Evaluating:  42%|████▏     | 69/165 [00:08<00:11,  8.30it/s]01/09/2022 23:38:45 - INFO - __main__ -   Batch Number = 70
Evaluating:  42%|████▏     | 70/165 [00:08<00:11,  8.30it/s]01/09/2022 23:38:45 - INFO - __main__ -   Batch Number = 71
Evaluating:  43%|████▎     | 71/165 [00:08<00:11,  8.27it/s]01/09/2022 23:38:45 - INFO - __main__ -   Batch Number = 72
Evaluating:  44%|████▎     | 72/165 [00:09<00:11,  8.28it/s]01/09/2022 23:38:45 - INFO - __main__ -   Batch Number = 73
Evaluating:  44%|████▍     | 73/165 [00:09<00:11,  8.29it/s]01/09/2022 23:38:45 - INFO - __main__ -   Batch Number = 74
Evaluating:  45%|████▍     | 74/165 [00:09<00:10,  8.31it/s]01/09/2022 23:38:45 - INFO - __main__ -   Batch Number = 75
Evaluating:  45%|████▌     | 75/165 [00:09<00:10,  8.31it/s]01/09/2022 23:38:45 - INFO - __main__ -   Batch Number = 76
Evaluating:  46%|████▌     | 76/165 [00:09<00:10,  8.31it/s]01/09/2022 23:38:46 - INFO - __main__ -   Batch Number = 77
Evaluating:  47%|████▋     | 77/165 [00:09<00:10,  8.31it/s]01/09/2022 23:38:46 - INFO - __main__ -   Batch Number = 78
Evaluating:  47%|████▋     | 78/165 [00:09<00:10,  8.30it/s]01/09/2022 23:38:46 - INFO - __main__ -   Batch Number = 79
Evaluating:  48%|████▊     | 79/165 [00:09<00:10,  8.30it/s]01/09/2022 23:38:46 - INFO - __main__ -   Batch Number = 80
Evaluating:  48%|████▊     | 80/165 [00:09<00:10,  8.30it/s]01/09/2022 23:38:46 - INFO - __main__ -   Batch Number = 81
Evaluating:  49%|████▉     | 81/165 [00:10<00:10,  8.29it/s]01/09/2022 23:38:46 - INFO - __main__ -   Batch Number = 82
Evaluating:  50%|████▉     | 82/165 [00:10<00:10,  8.29it/s]01/09/2022 23:38:46 - INFO - __main__ -   Batch Number = 83
Evaluating:  50%|█████     | 83/165 [00:10<00:09,  8.30it/s]01/09/2022 23:38:46 - INFO - __main__ -   Batch Number = 84
Evaluating:  51%|█████     | 84/165 [00:10<00:09,  8.30it/s]01/09/2022 23:38:46 - INFO - __main__ -   Batch Number = 85
Evaluating:  52%|█████▏    | 85/165 [00:10<00:09,  8.31it/s]01/09/2022 23:38:47 - INFO - __main__ -   Batch Number = 86
Evaluating:  52%|█████▏    | 86/165 [00:10<00:09,  8.29it/s]01/09/2022 23:38:47 - INFO - __main__ -   Batch Number = 87
Evaluating:  53%|█████▎    | 87/165 [00:10<00:09,  7.87it/s]01/09/2022 23:38:47 - INFO - __main__ -   Batch Number = 88
Evaluating:  53%|█████▎    | 88/165 [00:10<00:09,  7.99it/s]01/09/2022 23:38:47 - INFO - __main__ -   Batch Number = 89
Evaluating:  54%|█████▍    | 89/165 [00:11<00:09,  8.07it/s]01/09/2022 23:38:47 - INFO - __main__ -   Batch Number = 90
Evaluating:  55%|█████▍    | 90/165 [00:11<00:09,  8.14it/s]01/09/2022 23:38:47 - INFO - __main__ -   Batch Number = 91
Evaluating:  55%|█████▌    | 91/165 [00:11<00:09,  8.19it/s]01/09/2022 23:38:47 - INFO - __main__ -   Batch Number = 92
Evaluating:  56%|█████▌    | 92/165 [00:11<00:08,  8.22it/s]01/09/2022 23:38:47 - INFO - __main__ -   Batch Number = 93
Evaluating:  56%|█████▋    | 93/165 [00:11<00:08,  8.24it/s]01/09/2022 23:38:48 - INFO - __main__ -   Batch Number = 94
Evaluating:  57%|█████▋    | 94/165 [00:11<00:08,  8.25it/s]01/09/2022 23:38:48 - INFO - __main__ -   Batch Number = 95
Evaluating:  58%|█████▊    | 95/165 [00:11<00:08,  8.26it/s]01/09/2022 23:38:48 - INFO - __main__ -   Batch Number = 96
Evaluating:  58%|█████▊    | 96/165 [00:11<00:08,  8.27it/s]01/09/2022 23:38:48 - INFO - __main__ -   Batch Number = 97
Evaluating:  59%|█████▉    | 97/165 [00:12<00:08,  8.28it/s]01/09/2022 23:38:48 - INFO - __main__ -   Batch Number = 98
Evaluating:  59%|█████▉    | 98/165 [00:12<00:08,  8.29it/s]01/09/2022 23:38:48 - INFO - __main__ -   Batch Number = 99
Evaluating:  60%|██████    | 99/165 [00:12<00:07,  8.29it/s]01/09/2022 23:38:48 - INFO - __main__ -   Batch Number = 100
Evaluating:  61%|██████    | 100/165 [00:12<00:07,  8.28it/s]01/09/2022 23:38:48 - INFO - __main__ -   Batch Number = 101
Evaluating:  61%|██████    | 101/165 [00:12<00:07,  8.25it/s]01/09/2022 23:38:49 - INFO - __main__ -   Batch Number = 102
Evaluating:  62%|██████▏   | 102/165 [00:12<00:07,  8.21it/s]01/09/2022 23:38:49 - INFO - __main__ -   Batch Number = 103
Evaluating:  62%|██████▏   | 103/165 [00:12<00:07,  8.23it/s]01/09/2022 23:38:49 - INFO - __main__ -   Batch Number = 104
Evaluating:  63%|██████▎   | 104/165 [00:12<00:07,  8.23it/s]01/09/2022 23:38:49 - INFO - __main__ -   Batch Number = 105
Evaluating:  64%|██████▎   | 105/165 [00:13<00:07,  8.23it/s]01/09/2022 23:38:49 - INFO - __main__ -   Batch Number = 106
Evaluating:  64%|██████▍   | 106/165 [00:13<00:07,  8.24it/s]01/09/2022 23:38:49 - INFO - __main__ -   Batch Number = 107
Evaluating:  65%|██████▍   | 107/165 [00:13<00:07,  8.23it/s]01/09/2022 23:38:49 - INFO - __main__ -   Batch Number = 108
Evaluating:  65%|██████▌   | 108/165 [00:13<00:06,  8.19it/s]01/09/2022 23:38:49 - INFO - __main__ -   Batch Number = 109
Evaluating:  66%|██████▌   | 109/165 [00:13<00:06,  8.16it/s]01/09/2022 23:38:50 - INFO - __main__ -   Batch Number = 110
Evaluating:  67%|██████▋   | 110/165 [00:13<00:06,  8.19it/s]01/09/2022 23:38:50 - INFO - __main__ -   Batch Number = 111
Evaluating:  67%|██████▋   | 111/165 [00:13<00:06,  8.22it/s]01/09/2022 23:38:50 - INFO - __main__ -   Batch Number = 112
Evaluating:  68%|██████▊   | 112/165 [00:13<00:06,  8.24it/s]01/09/2022 23:38:50 - INFO - __main__ -   Batch Number = 113
Evaluating:  68%|██████▊   | 113/165 [00:13<00:06,  8.26it/s]01/09/2022 23:38:50 - INFO - __main__ -   Batch Number = 114
Evaluating:  69%|██████▉   | 114/165 [00:14<00:06,  8.26it/s]01/09/2022 23:38:50 - INFO - __main__ -   Batch Number = 115
Evaluating:  70%|██████▉   | 115/165 [00:14<00:06,  8.27it/s]01/09/2022 23:38:50 - INFO - __main__ -   Batch Number = 116
Evaluating:  70%|███████   | 116/165 [00:14<00:05,  8.25it/s]01/09/2022 23:38:50 - INFO - __main__ -   Batch Number = 117
Evaluating:  71%|███████   | 117/165 [00:14<00:05,  8.23it/s]01/09/2022 23:38:51 - INFO - __main__ -   Batch Number = 118
Evaluating:  72%|███████▏  | 118/165 [00:14<00:05,  8.26it/s]01/09/2022 23:38:51 - INFO - __main__ -   Batch Number = 119
Evaluating:  72%|███████▏  | 119/165 [00:14<00:05,  8.26it/s]01/09/2022 23:38:51 - INFO - __main__ -   Batch Number = 120
Evaluating:  73%|███████▎  | 120/165 [00:14<00:05,  8.27it/s]01/09/2022 23:38:51 - INFO - __main__ -   Batch Number = 121
Evaluating:  73%|███████▎  | 121/165 [00:14<00:05,  8.26it/s]01/09/2022 23:38:51 - INFO - __main__ -   Batch Number = 122
Evaluating:  74%|███████▍  | 122/165 [00:15<00:05,  8.27it/s]01/09/2022 23:38:51 - INFO - __main__ -   Batch Number = 123
Evaluating:  75%|███████▍  | 123/165 [00:15<00:05,  8.27it/s]01/09/2022 23:38:51 - INFO - __main__ -   Batch Number = 124
Evaluating:  75%|███████▌  | 124/165 [00:15<00:04,  8.27it/s]01/09/2022 23:38:51 - INFO - __main__ -   Batch Number = 125
Evaluating:  76%|███████▌  | 125/165 [00:15<00:04,  8.27it/s]01/09/2022 23:38:51 - INFO - __main__ -   Batch Number = 126
Evaluating:  76%|███████▋  | 126/165 [00:15<00:04,  8.27it/s]01/09/2022 23:38:52 - INFO - __main__ -   Batch Number = 127
Evaluating:  77%|███████▋  | 127/165 [00:15<00:04,  8.27it/s]01/09/2022 23:38:52 - INFO - __main__ -   Batch Number = 128
Evaluating:  78%|███████▊  | 128/165 [00:15<00:04,  7.72it/s]01/09/2022 23:38:52 - INFO - __main__ -   Batch Number = 129
Evaluating:  78%|███████▊  | 129/165 [00:15<00:04,  7.88it/s]01/09/2022 23:38:52 - INFO - __main__ -   Batch Number = 130
Evaluating:  79%|███████▉  | 130/165 [00:16<00:04,  7.99it/s]01/09/2022 23:38:52 - INFO - __main__ -   Batch Number = 131
Evaluating:  79%|███████▉  | 131/165 [00:16<00:04,  8.06it/s]01/09/2022 23:38:52 - INFO - __main__ -   Batch Number = 132
Evaluating:  80%|████████  | 132/165 [00:16<00:04,  8.13it/s]01/09/2022 23:38:52 - INFO - __main__ -   Batch Number = 133
Evaluating:  81%|████████  | 133/165 [00:16<00:03,  8.18it/s]01/09/2022 23:38:52 - INFO - __main__ -   Batch Number = 134
Evaluating:  81%|████████  | 134/165 [00:16<00:03,  8.22it/s]01/09/2022 23:38:53 - INFO - __main__ -   Batch Number = 135
Evaluating:  82%|████████▏ | 135/165 [00:16<00:03,  8.25it/s]01/09/2022 23:38:53 - INFO - __main__ -   Batch Number = 136
Evaluating:  82%|████████▏ | 136/165 [00:16<00:03,  8.26it/s]01/09/2022 23:38:53 - INFO - __main__ -   Batch Number = 137
Evaluating:  83%|████████▎ | 137/165 [00:16<00:03,  8.27it/s]01/09/2022 23:38:53 - INFO - __main__ -   Batch Number = 138
Evaluating:  84%|████████▎ | 138/165 [00:17<00:03,  8.27it/s]01/09/2022 23:38:53 - INFO - __main__ -   Batch Number = 139
Evaluating:  84%|████████▍ | 139/165 [00:17<00:03,  8.26it/s]01/09/2022 23:38:53 - INFO - __main__ -   Batch Number = 140
Evaluating:  85%|████████▍ | 140/165 [00:17<00:03,  8.26it/s]01/09/2022 23:38:53 - INFO - __main__ -   Batch Number = 141
Evaluating:  85%|████████▌ | 141/165 [00:17<00:02,  8.26it/s]01/09/2022 23:38:53 - INFO - __main__ -   Batch Number = 142
Evaluating:  86%|████████▌ | 142/165 [00:17<00:02,  8.26it/s]01/09/2022 23:38:54 - INFO - __main__ -   Batch Number = 143
Evaluating:  87%|████████▋ | 143/165 [00:17<00:02,  8.24it/s]01/09/2022 23:38:54 - INFO - __main__ -   Batch Number = 144
Evaluating:  87%|████████▋ | 144/165 [00:17<00:02,  8.24it/s]01/09/2022 23:38:54 - INFO - __main__ -   Batch Number = 145
Evaluating:  88%|████████▊ | 145/165 [00:17<00:02,  8.24it/s]01/09/2022 23:38:54 - INFO - __main__ -   Batch Number = 146
Evaluating:  88%|████████▊ | 146/165 [00:18<00:02,  8.22it/s]01/09/2022 23:38:54 - INFO - __main__ -   Batch Number = 147
Evaluating:  89%|████████▉ | 147/165 [00:18<00:02,  8.23it/s]01/09/2022 23:38:54 - INFO - __main__ -   Batch Number = 148
Evaluating:  90%|████████▉ | 148/165 [00:18<00:02,  8.23it/s]01/09/2022 23:38:54 - INFO - __main__ -   Batch Number = 149
Evaluating:  90%|█████████ | 149/165 [00:18<00:01,  8.24it/s]01/09/2022 23:38:54 - INFO - __main__ -   Batch Number = 150
Evaluating:  91%|█████████ | 150/165 [00:18<00:01,  8.25it/s]01/09/2022 23:38:55 - INFO - __main__ -   Batch Number = 151
Evaluating:  92%|█████████▏| 151/165 [00:18<00:01,  8.25it/s]01/09/2022 23:38:55 - INFO - __main__ -   Batch Number = 152
Evaluating:  92%|█████████▏| 152/165 [00:18<00:01,  8.24it/s]01/09/2022 23:38:55 - INFO - __main__ -   Batch Number = 153
Evaluating:  93%|█████████▎| 153/165 [00:18<00:01,  8.23it/s]01/09/2022 23:38:55 - INFO - __main__ -   Batch Number = 154
Evaluating:  93%|█████████▎| 154/165 [00:18<00:01,  8.24it/s]01/09/2022 23:38:55 - INFO - __main__ -   Batch Number = 155
Evaluating:  94%|█████████▍| 155/165 [00:19<00:01,  8.24it/s]01/09/2022 23:38:55 - INFO - __main__ -   Batch Number = 156
Evaluating:  95%|█████████▍| 156/165 [00:19<00:01,  8.23it/s]01/09/2022 23:38:55 - INFO - __main__ -   Batch Number = 157
Evaluating:  95%|█████████▌| 157/165 [00:19<00:00,  8.20it/s]01/09/2022 23:38:55 - INFO - __main__ -   Batch Number = 158
Evaluating:  96%|█████████▌| 158/165 [00:19<00:00,  8.20it/s]01/09/2022 23:38:56 - INFO - __main__ -   Batch Number = 159
Evaluating:  96%|█████████▋| 159/165 [00:19<00:00,  8.21it/s]01/09/2022 23:38:56 - INFO - __main__ -   Batch Number = 160
Evaluating:  97%|█████████▋| 160/165 [00:19<00:00,  8.23it/s]01/09/2022 23:38:56 - INFO - __main__ -   Batch Number = 161
Evaluating:  98%|█████████▊| 161/165 [00:19<00:00,  8.22it/s]01/09/2022 23:38:56 - INFO - __main__ -   Batch Number = 162
Evaluating:  98%|█████████▊| 162/165 [00:19<00:00,  8.23it/s]01/09/2022 23:38:56 - INFO - __main__ -   Batch Number = 163
Evaluating:  99%|█████████▉| 163/165 [00:20<00:00,  8.25it/s]01/09/2022 23:38:56 - INFO - __main__ -   Batch Number = 164
Evaluating:  99%|█████████▉| 164/165 [00:20<00:00,  8.25it/s]01/09/2022 23:38:56 - INFO - __main__ -   Batch Number = 165
Evaluating: 100%|██████████| 165/165 [00:20<00:00,  8.13it/s]
01/09/2022 23:38:56 - INFO - __main__ -     Evaluation done in total 20.284934 secs (0.015391 sec per example)
Writing predictions to: /home/abhijeet/rohan/cloud-emea-copy/outputs//squad/my-xlm-roberta-base-MaxLen384_qna_ar/predictions_ar_.json
Writing nbest to: /home/abhijeet/rohan/cloud-emea-copy/outputs//squad/my-xlm-roberta-base-MaxLen384_qna_ar/nbest_predictions_ar_.json
01/09/2022 23:39:00 - INFO - __main__ -   Results = OrderedDict([('exact', 50.924369747899156), ('f1', 66.74959600200837), ('total', 1190), ('HasAns_exact', 50.924369747899156), ('HasAns_f1', 66.74959600200837), ('HasAns_total', 1190), ('best_exact', 50.924369747899156), ('best_exact_thresh', 0.0), ('best_f1', 66.74959600200837), ('best_f1_thresh', 0.0)])
PyTorch version 1.10.1+cu102 available.
01/09/2022 23:39:02 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/09/2022 23:39:02 - INFO - root -   save model
loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6
Model config XLMRobertaConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /home/abhijeet/.cache/torch/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8
loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/09/2022 23:39:12 - INFO - __main__ -   lang2id = None
01/09/2022 23:39:15 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='xlm-roberta-base', model_type='xlm-roberta', config_name=None, tokenizer_name=None, cache_dir=None, data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//xquad', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//squad/my-xlm-roberta-base-MaxLen384_qna_ar/', max_seq_length=384, train_file='/home/abhijeet/rohan/cloud-emea-copy/data//xquad/train-v1.1.json', predict_file='/home/abhijeet/rohan/cloud-emea-copy/data//xquad/dev-v1.1.json', eval_test_set=False, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=4, per_gpu_eval_batch_size=8, gradient_accumulation_steps=4, learning_rate=0.0001, weight_decay=0.0001, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=15.0, max_steps=-1, save_steps=1000, warmup_steps=500, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', eval_lang='en', predict_langs='ar', train_lang='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//squad/my-xlm-roberta-base-MaxLen384_qna_ar//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='qna', threads=8, version_2_with_negative=False, verbose_logging=False, n_best_size=20, max_query_length=64, max_answer_length=30, doc_stride=128, null_score_diff_threshold=0.0, predict_task_adapter='output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s3/checkpoint-best/qna', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix=None, en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/09/2022 23:39:15 - INFO - __main__ -   Results: {}
loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6
Model config XLMRobertaConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /home/abhijeet/.cache/torch/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8
loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/09/2022 23:39:25 - INFO - __main__ -   lang2id = None
01/09/2022 23:39:25 - INFO - __main__ -   Evaluating the model on the test set of all languages specified
01/09/2022 23:39:25 - INFO - __main__ -   Adapter will be loaded from this path: output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s3/checkpoint-best/qna
01/09/2022 23:39:25 - INFO - root -   Trying to decide if add adapter
01/09/2022 23:39:25 - INFO - root -   loading task adapter
Loading module configuration from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s3/checkpoint-best/qna/adapter_config.json
Adding adapter 'qna' of type 'text_task'.
Loading module weights from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s3/checkpoint-best/qna/pytorch_adapter.bin
Loading module configuration from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s3/checkpoint-best/qna/head_config.json
Loading module weights from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s3/checkpoint-best/qna/pytorch_model_head.bin
01/09/2022 23:39:25 - INFO - root -   loading lang adpater ar/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/ar/xlm-roberta-base/pfeiffer/ar_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1f4018c6b187912fb6b47e664a34880506aef8c7320e0135939626a97e0bb137-4b1153cd3ebfdce09e1023c01e3ee723c32a0a57bea16e5ff13609f97eb2af0b-extracted/adapter_config.json
Adding adapter 'ar' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/1f4018c6b187912fb6b47e664a34880506aef8c7320e0135939626a97e0bb137-4b1153cd3ebfdce09e1023c01e3ee723c32a0a57bea16e5ff13609f97eb2af0b-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/1f4018c6b187912fb6b47e664a34880506aef8c7320e0135939626a97e0bb137-4b1153cd3ebfdce09e1023c01e3ee723c32a0a57bea16e5ff13609f97eb2af0b-extracted'
01/09/2022 23:39:26 - INFO - __main__ -   Language adapter for ar found
01/09/2022 23:39:26 - INFO - __main__ -   Set active language adapter to ar
01/09/2022 23:39:26 - INFO - __main__ -   Args Adapter Weight = None
01/09/2022 23:39:26 - INFO - __main__ -   Adapter Languages = ['ar']
01/09/2022 23:39:26 - INFO - __main__ -   Predict File = xquad.ar.json
01/09/2022 23:39:26 - INFO - __main__ -   Creating features from dataset file at /home/abhijeet/rohan/cloud-emea-copy/data//xquad
ar ar/wiki@ukp
  0%|          | 0/48 [00:00<?, ?it/s] 33%|███▎      | 16/48 [00:00<00:00, 136.60it/s] 62%|██████▎   | 30/48 [00:00<00:00, 133.89it/s]100%|██████████| 48/48 [00:00<00:00, 150.67it/s]100%|██████████| 48/48 [00:00<00:00, 146.19it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<04:38,  4.27it/s]convert squad examples to features:  32%|███▏      | 385/1190 [00:00<00:01, 563.24it/s]convert squad examples to features:  94%|█████████▍| 1121/1190 [00:00<00:00, 1716.53it/s]convert squad examples to features: 100%|██████████| 1190/1190 [00:00<00:00, 1373.44it/s]/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|██████████| 1190/1190 [00:00<00:00, 548425.64it/s]
01/09/2022 23:39:28 - INFO - __main__ -   Local Rank = -1
01/09/2022 23:39:28 - INFO - __main__ -   Saving features into cached file /home/abhijeet/rohan/cloud-emea-copy/data//xquad/cached_xquad.ar.json_xlm-roberta-base_384_ar
01/09/2022 23:39:30 - INFO - __main__ -   ***** Running evaluation  *****
01/09/2022 23:39:30 - INFO - __main__ -     Num examples = 1318
01/09/2022 23:39:30 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/165 [00:00<?, ?it/s]01/09/2022 23:39:30 - INFO - __main__ -   Batch Number = 1
Evaluating:   1%|          | 1/165 [00:00<00:23,  6.90it/s]01/09/2022 23:39:30 - INFO - __main__ -   Batch Number = 2
Evaluating:   1%|          | 2/165 [00:00<00:21,  7.43it/s]01/09/2022 23:39:30 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|▏         | 3/165 [00:00<00:20,  7.74it/s]01/09/2022 23:39:30 - INFO - __main__ -   Batch Number = 4
Evaluating:   2%|▏         | 4/165 [00:00<00:20,  7.94it/s]01/09/2022 23:39:30 - INFO - __main__ -   Batch Number = 5
Evaluating:   3%|▎         | 5/165 [00:00<00:19,  8.07it/s]01/09/2022 23:39:30 - INFO - __main__ -   Batch Number = 6
Evaluating:   4%|▎         | 6/165 [00:00<00:19,  8.15it/s]01/09/2022 23:39:30 - INFO - __main__ -   Batch Number = 7
Evaluating:   4%|▍         | 7/165 [00:00<00:19,  8.21it/s]01/09/2022 23:39:31 - INFO - __main__ -   Batch Number = 8
Evaluating:   5%|▍         | 8/165 [00:00<00:19,  8.24it/s]01/09/2022 23:39:31 - INFO - __main__ -   Batch Number = 9
Evaluating:   5%|▌         | 9/165 [00:01<00:18,  8.27it/s]01/09/2022 23:39:31 - INFO - __main__ -   Batch Number = 10
Evaluating:   6%|▌         | 10/165 [00:01<00:18,  8.27it/s]01/09/2022 23:39:31 - INFO - __main__ -   Batch Number = 11
Evaluating:   7%|▋         | 11/165 [00:01<00:18,  8.26it/s]01/09/2022 23:39:31 - INFO - __main__ -   Batch Number = 12
Evaluating:   7%|▋         | 12/165 [00:01<00:18,  8.27it/s]01/09/2022 23:39:31 - INFO - __main__ -   Batch Number = 13
Evaluating:   8%|▊         | 13/165 [00:01<00:26,  5.75it/s]01/09/2022 23:39:31 - INFO - __main__ -   Batch Number = 14
Evaluating:   8%|▊         | 14/165 [00:01<00:23,  6.33it/s]01/09/2022 23:39:32 - INFO - __main__ -   Batch Number = 15
Evaluating:   9%|▉         | 15/165 [00:02<00:22,  6.81it/s]01/09/2022 23:39:32 - INFO - __main__ -   Batch Number = 16
Evaluating:  10%|▉         | 16/165 [00:02<00:20,  7.18it/s]01/09/2022 23:39:32 - INFO - __main__ -   Batch Number = 17
Evaluating:  10%|█         | 17/165 [00:02<00:19,  7.48it/s]01/09/2022 23:39:32 - INFO - __main__ -   Batch Number = 18
Evaluating:  11%|█         | 18/165 [00:02<00:19,  7.71it/s]01/09/2022 23:39:32 - INFO - __main__ -   Batch Number = 19
Evaluating:  12%|█▏        | 19/165 [00:02<00:18,  7.86it/s]01/09/2022 23:39:32 - INFO - __main__ -   Batch Number = 20
Evaluating:  12%|█▏        | 20/165 [00:02<00:18,  7.99it/s]01/09/2022 23:39:32 - INFO - __main__ -   Batch Number = 21
Evaluating:  13%|█▎        | 21/165 [00:02<00:17,  8.07it/s]01/09/2022 23:39:32 - INFO - __main__ -   Batch Number = 22
Evaluating:  13%|█▎        | 22/165 [00:02<00:17,  8.14it/s]01/09/2022 23:39:32 - INFO - __main__ -   Batch Number = 23
Evaluating:  14%|█▍        | 23/165 [00:02<00:17,  8.19it/s]01/09/2022 23:39:33 - INFO - __main__ -   Batch Number = 24
Evaluating:  15%|█▍        | 24/165 [00:03<00:17,  8.22it/s]01/09/2022 23:39:33 - INFO - __main__ -   Batch Number = 25
Evaluating:  15%|█▌        | 25/165 [00:03<00:16,  8.24it/s]01/09/2022 23:39:33 - INFO - __main__ -   Batch Number = 26
Evaluating:  16%|█▌        | 26/165 [00:03<00:16,  8.24it/s]01/09/2022 23:39:33 - INFO - __main__ -   Batch Number = 27
Evaluating:  16%|█▋        | 27/165 [00:03<00:16,  8.24it/s]01/09/2022 23:39:33 - INFO - __main__ -   Batch Number = 28
Evaluating:  17%|█▋        | 28/165 [00:03<00:16,  8.26it/s]01/09/2022 23:39:33 - INFO - __main__ -   Batch Number = 29
Evaluating:  18%|█▊        | 29/165 [00:03<00:16,  8.28it/s]01/09/2022 23:39:33 - INFO - __main__ -   Batch Number = 30
Evaluating:  18%|█▊        | 30/165 [00:03<00:16,  8.29it/s]01/09/2022 23:39:33 - INFO - __main__ -   Batch Number = 31
Evaluating:  19%|█▉        | 31/165 [00:03<00:16,  8.29it/s]01/09/2022 23:39:34 - INFO - __main__ -   Batch Number = 32
Evaluating:  19%|█▉        | 32/165 [00:04<00:16,  8.28it/s]01/09/2022 23:39:34 - INFO - __main__ -   Batch Number = 33
Evaluating:  20%|██        | 33/165 [00:04<00:15,  8.29it/s]01/09/2022 23:39:34 - INFO - __main__ -   Batch Number = 34
Evaluating:  21%|██        | 34/165 [00:04<00:15,  8.28it/s]01/09/2022 23:39:34 - INFO - __main__ -   Batch Number = 35
Evaluating:  21%|██        | 35/165 [00:04<00:15,  8.27it/s]01/09/2022 23:39:34 - INFO - __main__ -   Batch Number = 36
Evaluating:  22%|██▏       | 36/165 [00:04<00:17,  7.22it/s]01/09/2022 23:39:34 - INFO - __main__ -   Batch Number = 37
Evaluating:  22%|██▏       | 37/165 [00:04<00:17,  7.50it/s]01/09/2022 23:39:34 - INFO - __main__ -   Batch Number = 38
Evaluating:  23%|██▎       | 38/165 [00:04<00:16,  7.72it/s]01/09/2022 23:39:34 - INFO - __main__ -   Batch Number = 39
Evaluating:  24%|██▎       | 39/165 [00:04<00:16,  7.87it/s]01/09/2022 23:39:35 - INFO - __main__ -   Batch Number = 40
Evaluating:  24%|██▍       | 40/165 [00:05<00:15,  7.99it/s]01/09/2022 23:39:35 - INFO - __main__ -   Batch Number = 41
Evaluating:  25%|██▍       | 41/165 [00:05<00:15,  8.06it/s]01/09/2022 23:39:35 - INFO - __main__ -   Batch Number = 42
Evaluating:  25%|██▌       | 42/165 [00:05<00:15,  8.09it/s]01/09/2022 23:39:35 - INFO - __main__ -   Batch Number = 43
Evaluating:  26%|██▌       | 43/165 [00:05<00:15,  8.05it/s]01/09/2022 23:39:35 - INFO - __main__ -   Batch Number = 44
Evaluating:  27%|██▋       | 44/165 [00:05<00:14,  8.09it/s]01/09/2022 23:39:35 - INFO - __main__ -   Batch Number = 45
Evaluating:  27%|██▋       | 45/165 [00:05<00:14,  8.14it/s]01/09/2022 23:39:35 - INFO - __main__ -   Batch Number = 46
Evaluating:  28%|██▊       | 46/165 [00:05<00:14,  8.17it/s]01/09/2022 23:39:35 - INFO - __main__ -   Batch Number = 47
Evaluating:  28%|██▊       | 47/165 [00:05<00:14,  8.18it/s]01/09/2022 23:39:36 - INFO - __main__ -   Batch Number = 48
Evaluating:  29%|██▉       | 48/165 [00:06<00:14,  8.19it/s]01/09/2022 23:39:36 - INFO - __main__ -   Batch Number = 49
Evaluating:  30%|██▉       | 49/165 [00:06<00:14,  8.21it/s]01/09/2022 23:39:36 - INFO - __main__ -   Batch Number = 50
Evaluating:  30%|███       | 50/165 [00:06<00:14,  8.20it/s]01/09/2022 23:39:36 - INFO - __main__ -   Batch Number = 51
Evaluating:  31%|███       | 51/165 [00:06<00:13,  8.21it/s]01/09/2022 23:39:36 - INFO - __main__ -   Batch Number = 52
Evaluating:  32%|███▏      | 52/165 [00:06<00:13,  8.17it/s]01/09/2022 23:39:36 - INFO - __main__ -   Batch Number = 53
Evaluating:  32%|███▏      | 53/165 [00:06<00:13,  8.17it/s]01/09/2022 23:39:36 - INFO - __main__ -   Batch Number = 54
Evaluating:  33%|███▎      | 54/165 [00:06<00:13,  8.19it/s]01/09/2022 23:39:36 - INFO - __main__ -   Batch Number = 55
Evaluating:  33%|███▎      | 55/165 [00:06<00:13,  8.20it/s]01/09/2022 23:39:37 - INFO - __main__ -   Batch Number = 56
Evaluating:  34%|███▍      | 56/165 [00:07<00:13,  8.18it/s]01/09/2022 23:39:37 - INFO - __main__ -   Batch Number = 57
Evaluating:  35%|███▍      | 57/165 [00:07<00:13,  8.19it/s]01/09/2022 23:39:37 - INFO - __main__ -   Batch Number = 58
Evaluating:  35%|███▌      | 58/165 [00:07<00:13,  8.20it/s]01/09/2022 23:39:37 - INFO - __main__ -   Batch Number = 59
Evaluating:  36%|███▌      | 59/165 [00:07<00:12,  8.20it/s]01/09/2022 23:39:37 - INFO - __main__ -   Batch Number = 60
Evaluating:  36%|███▋      | 60/165 [00:07<00:12,  8.20it/s]01/09/2022 23:39:37 - INFO - __main__ -   Batch Number = 61
Evaluating:  37%|███▋      | 61/165 [00:07<00:12,  8.19it/s]01/09/2022 23:39:37 - INFO - __main__ -   Batch Number = 62
Evaluating:  38%|███▊      | 62/165 [00:07<00:12,  8.18it/s]01/09/2022 23:39:37 - INFO - __main__ -   Batch Number = 63
Evaluating:  38%|███▊      | 63/165 [00:07<00:12,  8.20it/s]01/09/2022 23:39:38 - INFO - __main__ -   Batch Number = 64
Evaluating:  39%|███▉      | 64/165 [00:08<00:12,  8.21it/s]01/09/2022 23:39:38 - INFO - __main__ -   Batch Number = 65
Evaluating:  39%|███▉      | 65/165 [00:08<00:12,  8.22it/s]01/09/2022 23:39:38 - INFO - __main__ -   Batch Number = 66
Evaluating:  40%|████      | 66/165 [00:08<00:12,  8.23it/s]01/09/2022 23:39:38 - INFO - __main__ -   Batch Number = 67
Evaluating:  41%|████      | 67/165 [00:08<00:11,  8.24it/s]01/09/2022 23:39:38 - INFO - __main__ -   Batch Number = 68
Evaluating:  41%|████      | 68/165 [00:08<00:11,  8.26it/s]01/09/2022 23:39:38 - INFO - __main__ -   Batch Number = 69
Evaluating:  42%|████▏     | 69/165 [00:08<00:11,  8.26it/s]01/09/2022 23:39:38 - INFO - __main__ -   Batch Number = 70
Evaluating:  42%|████▏     | 70/165 [00:08<00:11,  8.25it/s]01/09/2022 23:39:38 - INFO - __main__ -   Batch Number = 71
Evaluating:  43%|████▎     | 71/165 [00:08<00:11,  8.23it/s]01/09/2022 23:39:39 - INFO - __main__ -   Batch Number = 72
Evaluating:  44%|████▎     | 72/165 [00:08<00:11,  8.23it/s]01/09/2022 23:39:39 - INFO - __main__ -   Batch Number = 73
Evaluating:  44%|████▍     | 73/165 [00:09<00:11,  8.23it/s]01/09/2022 23:39:39 - INFO - __main__ -   Batch Number = 74
Evaluating:  45%|████▍     | 74/165 [00:09<00:11,  8.23it/s]01/09/2022 23:39:39 - INFO - __main__ -   Batch Number = 75
Evaluating:  45%|████▌     | 75/165 [00:09<00:10,  8.24it/s]01/09/2022 23:39:39 - INFO - __main__ -   Batch Number = 76
Evaluating:  46%|████▌     | 76/165 [00:09<00:10,  8.24it/s]01/09/2022 23:39:39 - INFO - __main__ -   Batch Number = 77
Evaluating:  47%|████▋     | 77/165 [00:09<00:12,  7.18it/s]01/09/2022 23:39:39 - INFO - __main__ -   Batch Number = 78
Evaluating:  47%|████▋     | 78/165 [00:09<00:11,  7.47it/s]01/09/2022 23:39:39 - INFO - __main__ -   Batch Number = 79
Evaluating:  48%|████▊     | 79/165 [00:09<00:11,  7.68it/s]01/09/2022 23:39:40 - INFO - __main__ -   Batch Number = 80
Evaluating:  48%|████▊     | 80/165 [00:10<00:10,  7.83it/s]01/09/2022 23:39:40 - INFO - __main__ -   Batch Number = 81
Evaluating:  49%|████▉     | 81/165 [00:10<00:10,  7.93it/s]01/09/2022 23:39:40 - INFO - __main__ -   Batch Number = 82
Evaluating:  50%|████▉     | 82/165 [00:10<00:10,  7.96it/s]01/09/2022 23:39:40 - INFO - __main__ -   Batch Number = 83
Evaluating:  50%|█████     | 83/165 [00:10<00:10,  8.03it/s]01/09/2022 23:39:40 - INFO - __main__ -   Batch Number = 84
Evaluating:  51%|█████     | 84/165 [00:10<00:10,  8.09it/s]01/09/2022 23:39:40 - INFO - __main__ -   Batch Number = 85
Evaluating:  52%|█████▏    | 85/165 [00:10<00:09,  8.13it/s]01/09/2022 23:39:40 - INFO - __main__ -   Batch Number = 86
Evaluating:  52%|█████▏    | 86/165 [00:10<00:09,  8.14it/s]01/09/2022 23:39:40 - INFO - __main__ -   Batch Number = 87
Evaluating:  53%|█████▎    | 87/165 [00:10<00:09,  8.13it/s]01/09/2022 23:39:41 - INFO - __main__ -   Batch Number = 88
Evaluating:  53%|█████▎    | 88/165 [00:11<00:09,  8.17it/s]01/09/2022 23:39:41 - INFO - __main__ -   Batch Number = 89
Evaluating:  54%|█████▍    | 89/165 [00:11<00:09,  8.19it/s]01/09/2022 23:39:41 - INFO - __main__ -   Batch Number = 90
Evaluating:  55%|█████▍    | 90/165 [00:11<00:09,  8.20it/s]01/09/2022 23:39:41 - INFO - __main__ -   Batch Number = 91
Evaluating:  55%|█████▌    | 91/165 [00:11<00:09,  8.19it/s]01/09/2022 23:39:41 - INFO - __main__ -   Batch Number = 92
Evaluating:  56%|█████▌    | 92/165 [00:11<00:08,  8.19it/s]01/09/2022 23:39:41 - INFO - __main__ -   Batch Number = 93
Evaluating:  56%|█████▋    | 93/165 [00:11<00:08,  8.17it/s]01/09/2022 23:39:41 - INFO - __main__ -   Batch Number = 94
Evaluating:  57%|█████▋    | 94/165 [00:11<00:08,  8.18it/s]01/09/2022 23:39:41 - INFO - __main__ -   Batch Number = 95
Evaluating:  58%|█████▊    | 95/165 [00:11<00:08,  8.19it/s]01/09/2022 23:39:41 - INFO - __main__ -   Batch Number = 96
Evaluating:  58%|█████▊    | 96/165 [00:11<00:08,  8.21it/s]01/09/2022 23:39:42 - INFO - __main__ -   Batch Number = 97
Evaluating:  59%|█████▉    | 97/165 [00:12<00:08,  8.22it/s]01/09/2022 23:39:42 - INFO - __main__ -   Batch Number = 98
Evaluating:  59%|█████▉    | 98/165 [00:12<00:08,  8.22it/s]01/09/2022 23:39:42 - INFO - __main__ -   Batch Number = 99
Evaluating:  60%|██████    | 99/165 [00:12<00:08,  8.22it/s]01/09/2022 23:39:42 - INFO - __main__ -   Batch Number = 100
Evaluating:  61%|██████    | 100/165 [00:12<00:07,  8.23it/s]01/09/2022 23:39:42 - INFO - __main__ -   Batch Number = 101
Evaluating:  61%|██████    | 101/165 [00:12<00:07,  8.21it/s]01/09/2022 23:39:42 - INFO - __main__ -   Batch Number = 102
Evaluating:  62%|██████▏   | 102/165 [00:12<00:07,  8.20it/s]01/09/2022 23:39:42 - INFO - __main__ -   Batch Number = 103
Evaluating:  62%|██████▏   | 103/165 [00:12<00:07,  8.19it/s]01/09/2022 23:39:42 - INFO - __main__ -   Batch Number = 104
Evaluating:  63%|██████▎   | 104/165 [00:12<00:07,  8.19it/s]01/09/2022 23:39:43 - INFO - __main__ -   Batch Number = 105
Evaluating:  64%|██████▎   | 105/165 [00:13<00:07,  8.19it/s]01/09/2022 23:39:43 - INFO - __main__ -   Batch Number = 106
Evaluating:  64%|██████▍   | 106/165 [00:13<00:07,  8.20it/s]01/09/2022 23:39:43 - INFO - __main__ -   Batch Number = 107
Evaluating:  65%|██████▍   | 107/165 [00:13<00:07,  8.21it/s]01/09/2022 23:39:43 - INFO - __main__ -   Batch Number = 108
Evaluating:  65%|██████▌   | 108/165 [00:13<00:06,  8.23it/s]01/09/2022 23:39:43 - INFO - __main__ -   Batch Number = 109
Evaluating:  66%|██████▌   | 109/165 [00:13<00:06,  8.24it/s]01/09/2022 23:39:43 - INFO - __main__ -   Batch Number = 110
Evaluating:  67%|██████▋   | 110/165 [00:13<00:06,  8.24it/s]01/09/2022 23:39:43 - INFO - __main__ -   Batch Number = 111
Evaluating:  67%|██████▋   | 111/165 [00:13<00:06,  8.24it/s]01/09/2022 23:39:43 - INFO - __main__ -   Batch Number = 112
Evaluating:  68%|██████▊   | 112/165 [00:13<00:06,  8.24it/s]01/09/2022 23:39:44 - INFO - __main__ -   Batch Number = 113
Evaluating:  68%|██████▊   | 113/165 [00:14<00:06,  8.22it/s]01/09/2022 23:39:44 - INFO - __main__ -   Batch Number = 114
Evaluating:  69%|██████▉   | 114/165 [00:14<00:06,  8.15it/s]01/09/2022 23:39:44 - INFO - __main__ -   Batch Number = 115
Evaluating:  70%|██████▉   | 115/165 [00:14<00:06,  8.08it/s]01/09/2022 23:39:44 - INFO - __main__ -   Batch Number = 116
Evaluating:  70%|███████   | 116/165 [00:14<00:06,  8.10it/s]01/09/2022 23:39:44 - INFO - __main__ -   Batch Number = 117
Evaluating:  71%|███████   | 117/165 [00:14<00:05,  8.14it/s]01/09/2022 23:39:44 - INFO - __main__ -   Batch Number = 118
Evaluating:  72%|███████▏  | 118/165 [00:14<00:06,  6.97it/s]01/09/2022 23:39:44 - INFO - __main__ -   Batch Number = 119
Evaluating:  72%|███████▏  | 119/165 [00:14<00:06,  7.32it/s]01/09/2022 23:39:44 - INFO - __main__ -   Batch Number = 120
Evaluating:  73%|███████▎  | 120/165 [00:14<00:05,  7.57it/s]01/09/2022 23:39:45 - INFO - __main__ -   Batch Number = 121
Evaluating:  73%|███████▎  | 121/165 [00:15<00:05,  7.76it/s]01/09/2022 23:39:45 - INFO - __main__ -   Batch Number = 122
Evaluating:  74%|███████▍  | 122/165 [00:15<00:05,  7.90it/s]01/09/2022 23:39:45 - INFO - __main__ -   Batch Number = 123
Evaluating:  75%|███████▍  | 123/165 [00:15<00:05,  7.99it/s]01/09/2022 23:39:45 - INFO - __main__ -   Batch Number = 124
Evaluating:  75%|███████▌  | 124/165 [00:15<00:05,  7.99it/s]01/09/2022 23:39:45 - INFO - __main__ -   Batch Number = 125
Evaluating:  76%|███████▌  | 125/165 [00:15<00:05,  7.98it/s]01/09/2022 23:39:45 - INFO - __main__ -   Batch Number = 126
Evaluating:  76%|███████▋  | 126/165 [00:15<00:04,  8.04it/s]01/09/2022 23:39:45 - INFO - __main__ -   Batch Number = 127
Evaluating:  77%|███████▋  | 127/165 [00:15<00:04,  8.10it/s]01/09/2022 23:39:45 - INFO - __main__ -   Batch Number = 128
Evaluating:  78%|███████▊  | 128/165 [00:15<00:04,  8.13it/s]01/09/2022 23:39:46 - INFO - __main__ -   Batch Number = 129
Evaluating:  78%|███████▊  | 129/165 [00:16<00:04,  8.15it/s]01/09/2022 23:39:46 - INFO - __main__ -   Batch Number = 130
Evaluating:  79%|███████▉  | 130/165 [00:16<00:04,  8.17it/s]01/09/2022 23:39:46 - INFO - __main__ -   Batch Number = 131
Evaluating:  79%|███████▉  | 131/165 [00:16<00:04,  8.17it/s]01/09/2022 23:39:46 - INFO - __main__ -   Batch Number = 132
Evaluating:  80%|████████  | 132/165 [00:16<00:04,  8.17it/s]01/09/2022 23:39:46 - INFO - __main__ -   Batch Number = 133
Evaluating:  81%|████████  | 133/165 [00:16<00:03,  8.17it/s]01/09/2022 23:39:46 - INFO - __main__ -   Batch Number = 134
Evaluating:  81%|████████  | 134/165 [00:16<00:03,  8.14it/s]01/09/2022 23:39:46 - INFO - __main__ -   Batch Number = 135
Evaluating:  82%|████████▏ | 135/165 [00:16<00:03,  8.15it/s]01/09/2022 23:39:46 - INFO - __main__ -   Batch Number = 136
Evaluating:  82%|████████▏ | 136/165 [00:16<00:03,  8.17it/s]01/09/2022 23:39:47 - INFO - __main__ -   Batch Number = 137
Evaluating:  83%|████████▎ | 137/165 [00:17<00:03,  8.18it/s]01/09/2022 23:39:47 - INFO - __main__ -   Batch Number = 138
Evaluating:  84%|████████▎ | 138/165 [00:17<00:03,  8.18it/s]01/09/2022 23:39:47 - INFO - __main__ -   Batch Number = 139
Evaluating:  84%|████████▍ | 139/165 [00:17<00:03,  8.18it/s]01/09/2022 23:39:47 - INFO - __main__ -   Batch Number = 140
Evaluating:  85%|████████▍ | 140/165 [00:17<00:03,  8.18it/s]01/09/2022 23:39:47 - INFO - __main__ -   Batch Number = 141
Evaluating:  85%|████████▌ | 141/165 [00:17<00:02,  8.20it/s]01/09/2022 23:39:47 - INFO - __main__ -   Batch Number = 142
Evaluating:  86%|████████▌ | 142/165 [00:17<00:02,  8.20it/s]01/09/2022 23:39:47 - INFO - __main__ -   Batch Number = 143
Evaluating:  87%|████████▋ | 143/165 [00:17<00:02,  8.20it/s]01/09/2022 23:39:47 - INFO - __main__ -   Batch Number = 144
Evaluating:  87%|████████▋ | 144/165 [00:17<00:02,  8.19it/s]01/09/2022 23:39:48 - INFO - __main__ -   Batch Number = 145
Evaluating:  88%|████████▊ | 145/165 [00:18<00:02,  8.16it/s]01/09/2022 23:39:48 - INFO - __main__ -   Batch Number = 146
Evaluating:  88%|████████▊ | 146/165 [00:18<00:02,  8.15it/s]01/09/2022 23:39:48 - INFO - __main__ -   Batch Number = 147
Evaluating:  89%|████████▉ | 147/165 [00:18<00:02,  8.17it/s]01/09/2022 23:39:48 - INFO - __main__ -   Batch Number = 148
Evaluating:  90%|████████▉ | 148/165 [00:18<00:02,  8.19it/s]01/09/2022 23:39:48 - INFO - __main__ -   Batch Number = 149
Evaluating:  90%|█████████ | 149/165 [00:18<00:01,  8.20it/s]01/09/2022 23:39:48 - INFO - __main__ -   Batch Number = 150
Evaluating:  91%|█████████ | 150/165 [00:18<00:01,  8.20it/s]01/09/2022 23:39:48 - INFO - __main__ -   Batch Number = 151
Evaluating:  92%|█████████▏| 151/165 [00:18<00:01,  8.21it/s]01/09/2022 23:39:48 - INFO - __main__ -   Batch Number = 152
Evaluating:  92%|█████████▏| 152/165 [00:18<00:01,  8.22it/s]01/09/2022 23:39:49 - INFO - __main__ -   Batch Number = 153
Evaluating:  93%|█████████▎| 153/165 [00:19<00:01,  8.22it/s]01/09/2022 23:39:49 - INFO - __main__ -   Batch Number = 154
Evaluating:  93%|█████████▎| 154/165 [00:19<00:01,  8.23it/s]01/09/2022 23:39:49 - INFO - __main__ -   Batch Number = 155
Evaluating:  94%|█████████▍| 155/165 [00:19<00:01,  8.20it/s]01/09/2022 23:39:49 - INFO - __main__ -   Batch Number = 156
Evaluating:  95%|█████████▍| 156/165 [00:19<00:01,  8.14it/s]01/09/2022 23:39:49 - INFO - __main__ -   Batch Number = 157
Evaluating:  95%|█████████▌| 157/165 [00:19<00:00,  8.15it/s]01/09/2022 23:39:49 - INFO - __main__ -   Batch Number = 158
Evaluating:  96%|█████████▌| 158/165 [00:19<00:00,  8.18it/s]01/09/2022 23:39:49 - INFO - __main__ -   Batch Number = 159
Evaluating:  96%|█████████▋| 159/165 [00:19<00:00,  6.96it/s]01/09/2022 23:39:49 - INFO - __main__ -   Batch Number = 160
Evaluating:  97%|█████████▋| 160/165 [00:19<00:00,  7.26it/s]01/09/2022 23:39:50 - INFO - __main__ -   Batch Number = 161
Evaluating:  98%|█████████▊| 161/165 [00:20<00:00,  7.43it/s]01/09/2022 23:39:50 - INFO - __main__ -   Batch Number = 162
Evaluating:  98%|█████████▊| 162/165 [00:20<00:00,  7.65it/s]01/09/2022 23:39:50 - INFO - __main__ -   Batch Number = 163
Evaluating:  99%|█████████▉| 163/165 [00:20<00:00,  7.81it/s]01/09/2022 23:39:50 - INFO - __main__ -   Batch Number = 164
Evaluating:  99%|█████████▉| 164/165 [00:20<00:00,  7.92it/s]01/09/2022 23:39:50 - INFO - __main__ -   Batch Number = 165
Evaluating: 100%|██████████| 165/165 [00:20<00:00,  8.04it/s]
01/09/2022 23:39:50 - INFO - __main__ -     Evaluation done in total 20.528240 secs (0.015575 sec per example)
Writing predictions to: /home/abhijeet/rohan/cloud-emea-copy/outputs//squad/my-xlm-roberta-base-MaxLen384_qna_ar/predictions_ar_.json
Writing nbest to: /home/abhijeet/rohan/cloud-emea-copy/outputs//squad/my-xlm-roberta-base-MaxLen384_qna_ar/nbest_predictions_ar_.json
01/09/2022 23:39:54 - INFO - __main__ -   Results = OrderedDict([('exact', 47.39495798319328), ('f1', 63.631457776579964), ('total', 1190), ('HasAns_exact', 47.39495798319328), ('HasAns_f1', 63.631457776579964), ('HasAns_total', 1190), ('best_exact', 47.39495798319328), ('best_exact_thresh', 0.0), ('best_f1', 63.631457776579964), ('best_f1_thresh', 0.0)])
PyTorch version 1.10.1+cu102 available.
01/09/2022 23:53:13 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/09/2022 23:53:13 - INFO - root -   save model
loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6
Model config XLMRobertaConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /home/abhijeet/.cache/torch/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8
loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/09/2022 23:53:23 - INFO - __main__ -   lang2id = None
01/09/2022 23:53:26 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='xlm-roberta-base', model_type='xlm-roberta', config_name=None, tokenizer_name=None, cache_dir=None, data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//xquad', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//squad/my-xlm-roberta-base-MaxLen384_qna_ar/', max_seq_length=384, train_file='/home/abhijeet/rohan/cloud-emea-copy/data//xquad/train-v1.1.json', predict_file='/home/abhijeet/rohan/cloud-emea-copy/data//xquad/dev-v1.1.json', eval_test_set=False, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=4, per_gpu_eval_batch_size=8, gradient_accumulation_steps=4, learning_rate=0.0001, weight_decay=0.0001, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=15.0, max_steps=-1, save_steps=1000, warmup_steps=500, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', eval_lang='en', predict_langs='ar', train_lang='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//squad/my-xlm-roberta-base-MaxLen384_qna_ar//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='qna', threads=8, version_2_with_negative=False, verbose_logging=False, n_best_size=20, max_query_length=64, max_answer_length=30, doc_stride=128, null_score_diff_threshold=0.0, predict_task_adapter='output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s1/checkpoint-best/qna', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix=None, en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/09/2022 23:53:26 - INFO - __main__ -   Results: {}
loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6
Model config XLMRobertaConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /home/abhijeet/.cache/torch/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8
loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/09/2022 23:53:36 - INFO - __main__ -   lang2id = None
01/09/2022 23:53:36 - INFO - __main__ -   Evaluating the model on the test set of all languages specified
01/09/2022 23:53:36 - INFO - __main__ -   Adapter will be loaded from this path: output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s1/checkpoint-best/qna
01/09/2022 23:53:36 - INFO - root -   Trying to decide if add adapter
01/09/2022 23:53:36 - INFO - root -   loading task adapter
Loading module configuration from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s1/checkpoint-best/qna/adapter_config.json
Adding adapter 'qna' of type 'text_task'.
Loading module weights from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s1/checkpoint-best/qna/pytorch_adapter.bin
Loading module configuration from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s1/checkpoint-best/qna/head_config.json
Loading module weights from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s1/checkpoint-best/qna/pytorch_model_head.bin
01/09/2022 23:53:36 - INFO - root -   loading lang adpater ar/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/ar/xlm-roberta-base/pfeiffer/ar_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1f4018c6b187912fb6b47e664a34880506aef8c7320e0135939626a97e0bb137-4b1153cd3ebfdce09e1023c01e3ee723c32a0a57bea16e5ff13609f97eb2af0b-extracted/adapter_config.json
Adding adapter 'ar' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/1f4018c6b187912fb6b47e664a34880506aef8c7320e0135939626a97e0bb137-4b1153cd3ebfdce09e1023c01e3ee723c32a0a57bea16e5ff13609f97eb2af0b-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/1f4018c6b187912fb6b47e664a34880506aef8c7320e0135939626a97e0bb137-4b1153cd3ebfdce09e1023c01e3ee723c32a0a57bea16e5ff13609f97eb2af0b-extracted'
01/09/2022 23:53:38 - INFO - __main__ -   Language adapter for ar found
01/09/2022 23:53:38 - INFO - __main__ -   Set active language adapter to ar
01/09/2022 23:53:38 - INFO - __main__ -   Args Adapter Weight = None
01/09/2022 23:53:38 - INFO - __main__ -   Adapter Languages = ['ar']
01/09/2022 23:53:38 - INFO - __main__ -   Predict File = xquad.ar.json
01/09/2022 23:53:38 - INFO - __main__ -   Creating features from dataset file at /home/abhijeet/rohan/cloud-emea-copy/data//xquad
ar ar/wiki@ukp
  0%|          | 0/48 [00:00<?, ?it/s] 33%|███▎      | 16/48 [00:00<00:00, 136.34it/s] 62%|██████▎   | 30/48 [00:00<00:00, 131.87it/s] 98%|█████████▊| 47/48 [00:00<00:00, 148.02it/s]100%|██████████| 48/48 [00:00<00:00, 143.82it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<04:27,  4.44it/s]convert squad examples to features:  32%|███▏      | 385/1190 [00:00<00:01, 566.09it/s]convert squad examples to features:  97%|█████████▋| 1153/1190 [00:00<00:00, 1707.43it/s]convert squad examples to features: 100%|██████████| 1190/1190 [00:00<00:00, 1352.54it/s]/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|██████████| 1190/1190 [00:00<00:00, 729497.48it/s]
01/09/2022 23:53:40 - INFO - __main__ -   Local Rank = -1
01/09/2022 23:53:40 - INFO - __main__ -   Saving features into cached file /home/abhijeet/rohan/cloud-emea-copy/data//xquad/cached_xquad.ar.json_xlm-roberta-base_384_ar
01/09/2022 23:53:41 - INFO - __main__ -   ***** Running evaluation  *****
01/09/2022 23:53:41 - INFO - __main__ -     Num examples = 1318
01/09/2022 23:53:41 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/165 [00:00<?, ?it/s]01/09/2022 23:53:41 - INFO - __main__ -   Batch Number = 1
Evaluating:   1%|          | 1/165 [00:00<00:24,  6.67it/s]01/09/2022 23:53:42 - INFO - __main__ -   Batch Number = 2
Evaluating:   1%|          | 2/165 [00:00<00:22,  7.17it/s]01/09/2022 23:53:42 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|▏         | 3/165 [00:00<00:21,  7.51it/s]01/09/2022 23:53:42 - INFO - __main__ -   Batch Number = 4
Evaluating:   2%|▏         | 4/165 [00:00<00:20,  7.79it/s]01/09/2022 23:53:42 - INFO - __main__ -   Batch Number = 5
Evaluating:   3%|▎         | 5/165 [00:00<00:20,  8.00it/s]01/09/2022 23:53:42 - INFO - __main__ -   Batch Number = 6
Evaluating:   4%|▎         | 6/165 [00:00<00:19,  8.13it/s]01/09/2022 23:53:42 - INFO - __main__ -   Batch Number = 7
Evaluating:   4%|▍         | 7/165 [00:00<00:19,  8.21it/s]01/09/2022 23:53:42 - INFO - __main__ -   Batch Number = 8
Evaluating:   5%|▍         | 8/165 [00:01<00:18,  8.27it/s]01/09/2022 23:53:42 - INFO - __main__ -   Batch Number = 9
Evaluating:   5%|▌         | 9/165 [00:01<00:18,  8.32it/s]01/09/2022 23:53:43 - INFO - __main__ -   Batch Number = 10
Evaluating:   6%|▌         | 10/165 [00:01<00:18,  8.34it/s]01/09/2022 23:53:43 - INFO - __main__ -   Batch Number = 11
Evaluating:   7%|▋         | 11/165 [00:01<00:18,  8.35it/s]01/09/2022 23:53:43 - INFO - __main__ -   Batch Number = 12
Evaluating:   7%|▋         | 12/165 [00:01<00:18,  8.37it/s]01/09/2022 23:53:43 - INFO - __main__ -   Batch Number = 13
Evaluating:   8%|▊         | 13/165 [00:01<00:25,  6.07it/s]01/09/2022 23:53:43 - INFO - __main__ -   Batch Number = 14
Evaluating:   8%|▊         | 14/165 [00:01<00:22,  6.62it/s]01/09/2022 23:53:43 - INFO - __main__ -   Batch Number = 15
Evaluating:   9%|▉         | 15/165 [00:01<00:21,  7.07it/s]01/09/2022 23:53:43 - INFO - __main__ -   Batch Number = 16
Evaluating:  10%|▉         | 16/165 [00:02<00:20,  7.25it/s]01/09/2022 23:53:44 - INFO - __main__ -   Batch Number = 17
Evaluating:  10%|█         | 17/165 [00:02<00:22,  6.54it/s]01/09/2022 23:53:44 - INFO - __main__ -   Batch Number = 18
Evaluating:  11%|█         | 18/165 [00:02<00:20,  7.01it/s]01/09/2022 23:53:44 - INFO - __main__ -   Batch Number = 19
Evaluating:  12%|█▏        | 19/165 [00:02<00:19,  7.38it/s]01/09/2022 23:53:44 - INFO - __main__ -   Batch Number = 20
Evaluating:  12%|█▏        | 20/165 [00:02<00:18,  7.66it/s]01/09/2022 23:53:44 - INFO - __main__ -   Batch Number = 21
Evaluating:  13%|█▎        | 21/165 [00:02<00:18,  7.85it/s]01/09/2022 23:53:44 - INFO - __main__ -   Batch Number = 22
Evaluating:  13%|█▎        | 22/165 [00:02<00:17,  8.01it/s]01/09/2022 23:53:44 - INFO - __main__ -   Batch Number = 23
Evaluating:  14%|█▍        | 23/165 [00:03<00:17,  8.12it/s]01/09/2022 23:53:44 - INFO - __main__ -   Batch Number = 24
Evaluating:  15%|█▍        | 24/165 [00:03<00:17,  8.20it/s]01/09/2022 23:53:45 - INFO - __main__ -   Batch Number = 25
Evaluating:  15%|█▌        | 25/165 [00:03<00:16,  8.26it/s]01/09/2022 23:53:45 - INFO - __main__ -   Batch Number = 26
Evaluating:  16%|█▌        | 26/165 [00:03<00:16,  8.28it/s]01/09/2022 23:53:45 - INFO - __main__ -   Batch Number = 27
Evaluating:  16%|█▋        | 27/165 [00:03<00:16,  8.32it/s]01/09/2022 23:53:45 - INFO - __main__ -   Batch Number = 28
Evaluating:  17%|█▋        | 28/165 [00:03<00:16,  8.35it/s]01/09/2022 23:53:45 - INFO - __main__ -   Batch Number = 29
Evaluating:  18%|█▊        | 29/165 [00:03<00:16,  8.36it/s]01/09/2022 23:53:45 - INFO - __main__ -   Batch Number = 30
Evaluating:  18%|█▊        | 30/165 [00:03<00:16,  8.38it/s]01/09/2022 23:53:45 - INFO - __main__ -   Batch Number = 31
Evaluating:  19%|█▉        | 31/165 [00:03<00:15,  8.38it/s]01/09/2022 23:53:45 - INFO - __main__ -   Batch Number = 32
Evaluating:  19%|█▉        | 32/165 [00:04<00:15,  8.39it/s]01/09/2022 23:53:45 - INFO - __main__ -   Batch Number = 33
Evaluating:  20%|██        | 33/165 [00:04<00:15,  8.38it/s]01/09/2022 23:53:46 - INFO - __main__ -   Batch Number = 34
Evaluating:  21%|██        | 34/165 [00:04<00:15,  8.35it/s]01/09/2022 23:53:46 - INFO - __main__ -   Batch Number = 35
Evaluating:  21%|██        | 35/165 [00:04<00:15,  8.36it/s]01/09/2022 23:53:46 - INFO - __main__ -   Batch Number = 36
Evaluating:  22%|██▏       | 36/165 [00:04<00:15,  8.37it/s]01/09/2022 23:53:46 - INFO - __main__ -   Batch Number = 37
Evaluating:  22%|██▏       | 37/165 [00:04<00:15,  8.37it/s]01/09/2022 23:53:46 - INFO - __main__ -   Batch Number = 38
Evaluating:  23%|██▎       | 38/165 [00:04<00:15,  8.38it/s]01/09/2022 23:53:46 - INFO - __main__ -   Batch Number = 39
Evaluating:  24%|██▎       | 39/165 [00:04<00:15,  8.38it/s]01/09/2022 23:53:46 - INFO - __main__ -   Batch Number = 40
Evaluating:  24%|██▍       | 40/165 [00:05<00:14,  8.38it/s]01/09/2022 23:53:46 - INFO - __main__ -   Batch Number = 41
Evaluating:  25%|██▍       | 41/165 [00:05<00:14,  8.37it/s]01/09/2022 23:53:47 - INFO - __main__ -   Batch Number = 42
Evaluating:  25%|██▌       | 42/165 [00:05<00:14,  8.36it/s]01/09/2022 23:53:47 - INFO - __main__ -   Batch Number = 43
Evaluating:  26%|██▌       | 43/165 [00:05<00:14,  8.36it/s]01/09/2022 23:53:47 - INFO - __main__ -   Batch Number = 44
Evaluating:  27%|██▋       | 44/165 [00:05<00:14,  8.37it/s]01/09/2022 23:53:47 - INFO - __main__ -   Batch Number = 45
Evaluating:  27%|██▋       | 45/165 [00:05<00:14,  8.37it/s]01/09/2022 23:53:47 - INFO - __main__ -   Batch Number = 46
Evaluating:  28%|██▊       | 46/165 [00:05<00:14,  8.37it/s]01/09/2022 23:53:47 - INFO - __main__ -   Batch Number = 47
Evaluating:  28%|██▊       | 47/165 [00:05<00:14,  8.37it/s]01/09/2022 23:53:47 - INFO - __main__ -   Batch Number = 48
Evaluating:  29%|██▉       | 48/165 [00:06<00:13,  8.37it/s]01/09/2022 23:53:47 - INFO - __main__ -   Batch Number = 49
Evaluating:  30%|██▉       | 49/165 [00:06<00:13,  8.36it/s]01/09/2022 23:53:48 - INFO - __main__ -   Batch Number = 50
Evaluating:  30%|███       | 50/165 [00:06<00:13,  8.35it/s]01/09/2022 23:53:48 - INFO - __main__ -   Batch Number = 51
Evaluating:  31%|███       | 51/165 [00:06<00:13,  8.36it/s]01/09/2022 23:53:48 - INFO - __main__ -   Batch Number = 52
Evaluating:  32%|███▏      | 52/165 [00:06<00:13,  8.33it/s]01/09/2022 23:53:48 - INFO - __main__ -   Batch Number = 53
Evaluating:  32%|███▏      | 53/165 [00:06<00:13,  8.32it/s]01/09/2022 23:53:48 - INFO - __main__ -   Batch Number = 54
Evaluating:  33%|███▎      | 54/165 [00:06<00:13,  8.33it/s]01/09/2022 23:53:48 - INFO - __main__ -   Batch Number = 55
Evaluating:  33%|███▎      | 55/165 [00:06<00:13,  8.33it/s]01/09/2022 23:53:48 - INFO - __main__ -   Batch Number = 56
Evaluating:  34%|███▍      | 56/165 [00:06<00:13,  8.29it/s]01/09/2022 23:53:48 - INFO - __main__ -   Batch Number = 57
Evaluating:  35%|███▍      | 57/165 [00:07<00:13,  8.28it/s]01/09/2022 23:53:48 - INFO - __main__ -   Batch Number = 58
Evaluating:  35%|███▌      | 58/165 [00:07<00:12,  8.28it/s]01/09/2022 23:53:49 - INFO - __main__ -   Batch Number = 59
Evaluating:  36%|███▌      | 59/165 [00:07<00:14,  7.55it/s]01/09/2022 23:53:49 - INFO - __main__ -   Batch Number = 60
Evaluating:  36%|███▋      | 60/165 [00:07<00:13,  7.76it/s]01/09/2022 23:53:49 - INFO - __main__ -   Batch Number = 61
Evaluating:  37%|███▋      | 61/165 [00:07<00:13,  7.92it/s]01/09/2022 23:53:49 - INFO - __main__ -   Batch Number = 62
Evaluating:  38%|███▊      | 62/165 [00:07<00:12,  8.04it/s]01/09/2022 23:53:49 - INFO - __main__ -   Batch Number = 63
Evaluating:  38%|███▊      | 63/165 [00:07<00:12,  8.12it/s]01/09/2022 23:53:49 - INFO - __main__ -   Batch Number = 64
Evaluating:  39%|███▉      | 64/165 [00:07<00:12,  8.19it/s]01/09/2022 23:53:49 - INFO - __main__ -   Batch Number = 65
Evaluating:  39%|███▉      | 65/165 [00:08<00:12,  8.24it/s]01/09/2022 23:53:49 - INFO - __main__ -   Batch Number = 66
Evaluating:  40%|████      | 66/165 [00:08<00:11,  8.28it/s]01/09/2022 23:53:50 - INFO - __main__ -   Batch Number = 67
Evaluating:  41%|████      | 67/165 [00:08<00:11,  8.30it/s]01/09/2022 23:53:50 - INFO - __main__ -   Batch Number = 68
Evaluating:  41%|████      | 68/165 [00:08<00:11,  8.32it/s]01/09/2022 23:53:50 - INFO - __main__ -   Batch Number = 69
Evaluating:  42%|████▏     | 69/165 [00:08<00:11,  8.32it/s]01/09/2022 23:53:50 - INFO - __main__ -   Batch Number = 70
Evaluating:  42%|████▏     | 70/165 [00:08<00:11,  8.31it/s]01/09/2022 23:53:50 - INFO - __main__ -   Batch Number = 71
Evaluating:  43%|████▎     | 71/165 [00:08<00:11,  8.31it/s]01/09/2022 23:53:50 - INFO - __main__ -   Batch Number = 72
Evaluating:  44%|████▎     | 72/165 [00:08<00:11,  8.32it/s]01/09/2022 23:53:50 - INFO - __main__ -   Batch Number = 73
Evaluating:  44%|████▍     | 73/165 [00:09<00:11,  8.33it/s]01/09/2022 23:53:50 - INFO - __main__ -   Batch Number = 74
Evaluating:  45%|████▍     | 74/165 [00:09<00:10,  8.35it/s]01/09/2022 23:53:51 - INFO - __main__ -   Batch Number = 75
Evaluating:  45%|████▌     | 75/165 [00:09<00:10,  8.36it/s]01/09/2022 23:53:51 - INFO - __main__ -   Batch Number = 76
Evaluating:  46%|████▌     | 76/165 [00:09<00:10,  8.36it/s]01/09/2022 23:53:51 - INFO - __main__ -   Batch Number = 77
Evaluating:  47%|████▋     | 77/165 [00:09<00:10,  8.35it/s]01/09/2022 23:53:51 - INFO - __main__ -   Batch Number = 78
Evaluating:  47%|████▋     | 78/165 [00:09<00:10,  8.35it/s]01/09/2022 23:53:51 - INFO - __main__ -   Batch Number = 79
Evaluating:  48%|████▊     | 79/165 [00:09<00:10,  8.35it/s]01/09/2022 23:53:51 - INFO - __main__ -   Batch Number = 80
Evaluating:  48%|████▊     | 80/165 [00:09<00:10,  8.34it/s]01/09/2022 23:53:51 - INFO - __main__ -   Batch Number = 81
Evaluating:  49%|████▉     | 81/165 [00:10<00:10,  8.33it/s]01/09/2022 23:53:51 - INFO - __main__ -   Batch Number = 82
Evaluating:  50%|████▉     | 82/165 [00:10<00:09,  8.34it/s]01/09/2022 23:53:52 - INFO - __main__ -   Batch Number = 83
Evaluating:  50%|█████     | 83/165 [00:10<00:09,  8.35it/s]01/09/2022 23:53:52 - INFO - __main__ -   Batch Number = 84
Evaluating:  51%|█████     | 84/165 [00:10<00:09,  8.34it/s]01/09/2022 23:53:52 - INFO - __main__ -   Batch Number = 85
Evaluating:  52%|█████▏    | 85/165 [00:10<00:09,  8.35it/s]01/09/2022 23:53:52 - INFO - __main__ -   Batch Number = 86
Evaluating:  52%|█████▏    | 86/165 [00:10<00:09,  8.33it/s]01/09/2022 23:53:52 - INFO - __main__ -   Batch Number = 87
Evaluating:  53%|█████▎    | 87/165 [00:10<00:09,  8.34it/s]01/09/2022 23:53:52 - INFO - __main__ -   Batch Number = 88
Evaluating:  53%|█████▎    | 88/165 [00:10<00:09,  8.33it/s]01/09/2022 23:53:52 - INFO - __main__ -   Batch Number = 89
Evaluating:  54%|█████▍    | 89/165 [00:10<00:09,  8.34it/s]01/09/2022 23:53:52 - INFO - __main__ -   Batch Number = 90
Evaluating:  55%|█████▍    | 90/165 [00:11<00:08,  8.35it/s]01/09/2022 23:53:52 - INFO - __main__ -   Batch Number = 91
Evaluating:  55%|█████▌    | 91/165 [00:11<00:08,  8.36it/s]01/09/2022 23:53:53 - INFO - __main__ -   Batch Number = 92
Evaluating:  56%|█████▌    | 92/165 [00:11<00:08,  8.36it/s]01/09/2022 23:53:53 - INFO - __main__ -   Batch Number = 93
Evaluating:  56%|█████▋    | 93/165 [00:11<00:08,  8.35it/s]01/09/2022 23:53:53 - INFO - __main__ -   Batch Number = 94
Evaluating:  57%|█████▋    | 94/165 [00:11<00:08,  8.34it/s]01/09/2022 23:53:53 - INFO - __main__ -   Batch Number = 95
Evaluating:  58%|█████▊    | 95/165 [00:11<00:08,  8.33it/s]01/09/2022 23:53:53 - INFO - __main__ -   Batch Number = 96
Evaluating:  58%|█████▊    | 96/165 [00:11<00:08,  8.32it/s]01/09/2022 23:53:53 - INFO - __main__ -   Batch Number = 97
Evaluating:  59%|█████▉    | 97/165 [00:11<00:08,  8.33it/s]01/09/2022 23:53:53 - INFO - __main__ -   Batch Number = 98
Evaluating:  59%|█████▉    | 98/165 [00:12<00:08,  8.33it/s]01/09/2022 23:53:53 - INFO - __main__ -   Batch Number = 99
Evaluating:  60%|██████    | 99/165 [00:12<00:07,  8.32it/s]01/09/2022 23:53:54 - INFO - __main__ -   Batch Number = 100
Evaluating:  61%|██████    | 100/165 [00:12<00:07,  8.15it/s]01/09/2022 23:53:54 - INFO - __main__ -   Batch Number = 101
Evaluating:  61%|██████    | 101/165 [00:12<00:07,  8.18it/s]01/09/2022 23:53:54 - INFO - __main__ -   Batch Number = 102
Evaluating:  62%|██████▏   | 102/165 [00:12<00:07,  8.22it/s]01/09/2022 23:53:54 - INFO - __main__ -   Batch Number = 103
Evaluating:  62%|██████▏   | 103/165 [00:12<00:07,  8.23it/s]01/09/2022 23:53:54 - INFO - __main__ -   Batch Number = 104
Evaluating:  63%|██████▎   | 104/165 [00:12<00:07,  8.25it/s]01/09/2022 23:53:54 - INFO - __main__ -   Batch Number = 105
Evaluating:  64%|██████▎   | 105/165 [00:12<00:07,  8.27it/s]01/09/2022 23:53:54 - INFO - __main__ -   Batch Number = 106
Evaluating:  64%|██████▍   | 106/165 [00:13<00:07,  8.30it/s]01/09/2022 23:53:54 - INFO - __main__ -   Batch Number = 107
Evaluating:  65%|██████▍   | 107/165 [00:13<00:06,  8.32it/s]01/09/2022 23:53:55 - INFO - __main__ -   Batch Number = 108
Evaluating:  65%|██████▌   | 108/165 [00:13<00:06,  8.32it/s]01/09/2022 23:53:55 - INFO - __main__ -   Batch Number = 109
Evaluating:  66%|██████▌   | 109/165 [00:13<00:06,  8.33it/s]01/09/2022 23:53:55 - INFO - __main__ -   Batch Number = 110
Evaluating:  67%|██████▋   | 110/165 [00:13<00:06,  8.34it/s]01/09/2022 23:53:55 - INFO - __main__ -   Batch Number = 111
Evaluating:  67%|██████▋   | 111/165 [00:13<00:06,  8.34it/s]01/09/2022 23:53:55 - INFO - __main__ -   Batch Number = 112
Evaluating:  68%|██████▊   | 112/165 [00:13<00:06,  8.35it/s]01/09/2022 23:53:55 - INFO - __main__ -   Batch Number = 113
Evaluating:  68%|██████▊   | 113/165 [00:13<00:06,  8.35it/s]01/09/2022 23:53:55 - INFO - __main__ -   Batch Number = 114
Evaluating:  69%|██████▉   | 114/165 [00:13<00:06,  8.36it/s]01/09/2022 23:53:55 - INFO - __main__ -   Batch Number = 115
Evaluating:  70%|██████▉   | 115/165 [00:14<00:05,  8.35it/s]01/09/2022 23:53:56 - INFO - __main__ -   Batch Number = 116
Evaluating:  70%|███████   | 116/165 [00:14<00:05,  8.34it/s]01/09/2022 23:53:56 - INFO - __main__ -   Batch Number = 117
Evaluating:  71%|███████   | 117/165 [00:14<00:05,  8.34it/s]01/09/2022 23:53:56 - INFO - __main__ -   Batch Number = 118
Evaluating:  72%|███████▏  | 118/165 [00:14<00:05,  8.34it/s]01/09/2022 23:53:56 - INFO - __main__ -   Batch Number = 119
Evaluating:  72%|███████▏  | 119/165 [00:14<00:05,  8.30it/s]01/09/2022 23:53:56 - INFO - __main__ -   Batch Number = 120
Evaluating:  73%|███████▎  | 120/165 [00:14<00:05,  8.31it/s]01/09/2022 23:53:56 - INFO - __main__ -   Batch Number = 121
Evaluating:  73%|███████▎  | 121/165 [00:14<00:05,  8.32it/s]01/09/2022 23:53:56 - INFO - __main__ -   Batch Number = 122
Evaluating:  74%|███████▍  | 122/165 [00:14<00:05,  8.33it/s]01/09/2022 23:53:56 - INFO - __main__ -   Batch Number = 123
Evaluating:  75%|███████▍  | 123/165 [00:15<00:05,  8.34it/s]01/09/2022 23:53:56 - INFO - __main__ -   Batch Number = 124
Evaluating:  75%|███████▌  | 124/165 [00:15<00:04,  8.34it/s]01/09/2022 23:53:57 - INFO - __main__ -   Batch Number = 125
Evaluating:  76%|███████▌  | 125/165 [00:15<00:04,  8.34it/s]01/09/2022 23:53:57 - INFO - __main__ -   Batch Number = 126
Evaluating:  76%|███████▋  | 126/165 [00:15<00:04,  8.34it/s]01/09/2022 23:53:57 - INFO - __main__ -   Batch Number = 127
Evaluating:  77%|███████▋  | 127/165 [00:15<00:04,  8.34it/s]01/09/2022 23:53:57 - INFO - __main__ -   Batch Number = 128
Evaluating:  78%|███████▊  | 128/165 [00:15<00:04,  8.35it/s]01/09/2022 23:53:57 - INFO - __main__ -   Batch Number = 129
Evaluating:  78%|███████▊  | 129/165 [00:15<00:04,  8.34it/s]01/09/2022 23:53:57 - INFO - __main__ -   Batch Number = 130
Evaluating:  79%|███████▉  | 130/165 [00:15<00:04,  8.34it/s]01/09/2022 23:53:57 - INFO - __main__ -   Batch Number = 131
Evaluating:  79%|███████▉  | 131/165 [00:16<00:04,  8.32it/s]01/09/2022 23:53:57 - INFO - __main__ -   Batch Number = 132
Evaluating:  80%|████████  | 132/165 [00:16<00:03,  8.32it/s]01/09/2022 23:53:58 - INFO - __main__ -   Batch Number = 133
Evaluating:  81%|████████  | 133/165 [00:16<00:03,  8.32it/s]01/09/2022 23:53:58 - INFO - __main__ -   Batch Number = 134
Evaluating:  81%|████████  | 134/165 [00:16<00:03,  8.32it/s]01/09/2022 23:53:58 - INFO - __main__ -   Batch Number = 135
Evaluating:  82%|████████▏ | 135/165 [00:16<00:03,  8.33it/s]01/09/2022 23:53:58 - INFO - __main__ -   Batch Number = 136
Evaluating:  82%|████████▏ | 136/165 [00:16<00:03,  8.33it/s]01/09/2022 23:53:58 - INFO - __main__ -   Batch Number = 137
Evaluating:  83%|████████▎ | 137/165 [00:16<00:03,  8.33it/s]01/09/2022 23:53:58 - INFO - __main__ -   Batch Number = 138
Evaluating:  84%|████████▎ | 138/165 [00:16<00:03,  8.33it/s]01/09/2022 23:53:58 - INFO - __main__ -   Batch Number = 139
Evaluating:  84%|████████▍ | 139/165 [00:16<00:03,  8.31it/s]01/09/2022 23:53:58 - INFO - __main__ -   Batch Number = 140
Evaluating:  85%|████████▍ | 140/165 [00:17<00:03,  8.31it/s]01/09/2022 23:53:59 - INFO - __main__ -   Batch Number = 141
Evaluating:  85%|████████▌ | 141/165 [00:17<00:02,  8.31it/s]01/09/2022 23:53:59 - INFO - __main__ -   Batch Number = 142
Evaluating:  86%|████████▌ | 142/165 [00:17<00:02,  8.31it/s]01/09/2022 23:53:59 - INFO - __main__ -   Batch Number = 143
Evaluating:  87%|████████▋ | 143/165 [00:17<00:02,  8.31it/s]01/09/2022 23:53:59 - INFO - __main__ -   Batch Number = 144
Evaluating:  87%|████████▋ | 144/165 [00:17<00:02,  8.31it/s]01/09/2022 23:53:59 - INFO - __main__ -   Batch Number = 145
Evaluating:  88%|████████▊ | 145/165 [00:17<00:02,  8.31it/s]01/09/2022 23:53:59 - INFO - __main__ -   Batch Number = 146
Evaluating:  88%|████████▊ | 146/165 [00:17<00:02,  8.29it/s]01/09/2022 23:53:59 - INFO - __main__ -   Batch Number = 147
Evaluating:  89%|████████▉ | 147/165 [00:17<00:02,  8.30it/s]01/09/2022 23:53:59 - INFO - __main__ -   Batch Number = 148
Evaluating:  90%|████████▉ | 148/165 [00:18<00:02,  8.30it/s]01/09/2022 23:53:59 - INFO - __main__ -   Batch Number = 149
Evaluating:  90%|█████████ | 149/165 [00:18<00:01,  8.29it/s]01/09/2022 23:54:00 - INFO - __main__ -   Batch Number = 150
Evaluating:  91%|█████████ | 150/165 [00:18<00:01,  8.29it/s]01/09/2022 23:54:00 - INFO - __main__ -   Batch Number = 151
Evaluating:  92%|█████████▏| 151/165 [00:18<00:01,  8.30it/s]01/09/2022 23:54:00 - INFO - __main__ -   Batch Number = 152
Evaluating:  92%|█████████▏| 152/165 [00:18<00:01,  8.30it/s]01/09/2022 23:54:00 - INFO - __main__ -   Batch Number = 153
Evaluating:  93%|█████████▎| 153/165 [00:18<00:01,  8.31it/s]01/09/2022 23:54:00 - INFO - __main__ -   Batch Number = 154
Evaluating:  93%|█████████▎| 154/165 [00:18<00:01,  8.31it/s]01/09/2022 23:54:00 - INFO - __main__ -   Batch Number = 155
Evaluating:  94%|█████████▍| 155/165 [00:18<00:01,  8.32it/s]01/09/2022 23:54:00 - INFO - __main__ -   Batch Number = 156
Evaluating:  95%|█████████▍| 156/165 [00:19<00:01,  8.31it/s]01/09/2022 23:54:00 - INFO - __main__ -   Batch Number = 157
Evaluating:  95%|█████████▌| 157/165 [00:19<00:00,  8.28it/s]01/09/2022 23:54:01 - INFO - __main__ -   Batch Number = 158
Evaluating:  96%|█████████▌| 158/165 [00:19<00:00,  8.29it/s]01/09/2022 23:54:01 - INFO - __main__ -   Batch Number = 159
Evaluating:  96%|█████████▋| 159/165 [00:19<00:00,  8.30it/s]01/09/2022 23:54:01 - INFO - __main__ -   Batch Number = 160
Evaluating:  97%|█████████▋| 160/165 [00:19<00:00,  8.30it/s]01/09/2022 23:54:01 - INFO - __main__ -   Batch Number = 161
Evaluating:  98%|█████████▊| 161/165 [00:19<00:00,  8.28it/s]01/09/2022 23:54:01 - INFO - __main__ -   Batch Number = 162
Evaluating:  98%|█████████▊| 162/165 [00:19<00:00,  8.29it/s]01/09/2022 23:54:01 - INFO - __main__ -   Batch Number = 163
Evaluating:  99%|█████████▉| 163/165 [00:19<00:00,  8.29it/s]01/09/2022 23:54:01 - INFO - __main__ -   Batch Number = 164
Evaluating:  99%|█████████▉| 164/165 [00:19<00:00,  8.29it/s]01/09/2022 23:54:01 - INFO - __main__ -   Batch Number = 165
Evaluating: 100%|██████████| 165/165 [00:20<00:00,  8.22it/s]
01/09/2022 23:54:01 - INFO - __main__ -     Evaluation done in total 20.079944 secs (0.015235 sec per example)
Writing predictions to: /home/abhijeet/rohan/cloud-emea-copy/outputs//squad/my-xlm-roberta-base-MaxLen384_qna_ar/predictions_ar_.json
Writing nbest to: /home/abhijeet/rohan/cloud-emea-copy/outputs//squad/my-xlm-roberta-base-MaxLen384_qna_ar/nbest_predictions_ar_.json
Traceback (most recent call last):
  File "third_party/my_run_squad.py", line 1112, in <module>
    main()
  File "third_party/my_run_squad.py", line 1105, in main
    predict_and_save(args, adapter_args, model, tokenizer, lang2id, lang_adapter_names, task_name, 'test')
  File "third_party/my_run_squad.py", line 903, in predict_and_save
    results = evaluate(args, model, tokenizer, language=lang, lang2id=lang2id, adapter_weight=adapter_weight, mode=split)
  File "third_party/my_run_squad.py", line 567, in evaluate
    results = squad_evaluate(examples, predictions)
  File "/home/abhijeet/rohan/cloud-emea-copy/src/transformers/data/metrics/squad_metrics.py", line 240, in squad_evaluate
    exact, f1, precision, recall = get_raw_scores(examples, preds)
  File "/home/abhijeet/rohan/cloud-emea-copy/src/transformers/data/metrics/squad_metrics.py", line 96, in get_raw_scores
    f1, precision, recall = compute_f1(a, prediction)
TypeError: 'int' object is not iterable
PyTorch version 1.10.1+cu102 available.
01/09/2022 23:54:07 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/09/2022 23:54:07 - INFO - root -   save model
loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6
Model config XLMRobertaConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /home/abhijeet/.cache/torch/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8
loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/09/2022 23:54:18 - INFO - __main__ -   lang2id = None
01/09/2022 23:54:21 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='xlm-roberta-base', model_type='xlm-roberta', config_name=None, tokenizer_name=None, cache_dir=None, data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//xquad', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//squad/my-xlm-roberta-base-MaxLen384_qna_ar/', max_seq_length=384, train_file='/home/abhijeet/rohan/cloud-emea-copy/data//xquad/train-v1.1.json', predict_file='/home/abhijeet/rohan/cloud-emea-copy/data//xquad/dev-v1.1.json', eval_test_set=False, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=4, per_gpu_eval_batch_size=8, gradient_accumulation_steps=4, learning_rate=0.0001, weight_decay=0.0001, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=15.0, max_steps=-1, save_steps=1000, warmup_steps=500, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', eval_lang='en', predict_langs='ar', train_lang='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//squad/my-xlm-roberta-base-MaxLen384_qna_ar//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='qna', threads=8, version_2_with_negative=False, verbose_logging=False, n_best_size=20, max_query_length=64, max_answer_length=30, doc_stride=128, null_score_diff_threshold=0.0, predict_task_adapter='output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s2/checkpoint-best/qna', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix=None, en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/09/2022 23:54:21 - INFO - __main__ -   Results: {}
loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6
Model config XLMRobertaConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /home/abhijeet/.cache/torch/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8
loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/09/2022 23:54:32 - INFO - __main__ -   lang2id = None
01/09/2022 23:54:32 - INFO - __main__ -   Evaluating the model on the test set of all languages specified
01/09/2022 23:54:32 - INFO - __main__ -   Adapter will be loaded from this path: output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s2/checkpoint-best/qna
01/09/2022 23:54:32 - INFO - root -   Trying to decide if add adapter
01/09/2022 23:54:32 - INFO - root -   loading task adapter
Loading module configuration from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s2/checkpoint-best/qna/adapter_config.json
Adding adapter 'qna' of type 'text_task'.
Loading module weights from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s2/checkpoint-best/qna/pytorch_adapter.bin
Loading module configuration from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s2/checkpoint-best/qna/head_config.json
Loading module weights from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s2/checkpoint-best/qna/pytorch_model_head.bin
01/09/2022 23:54:32 - INFO - root -   loading lang adpater ar/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/ar/xlm-roberta-base/pfeiffer/ar_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1f4018c6b187912fb6b47e664a34880506aef8c7320e0135939626a97e0bb137-4b1153cd3ebfdce09e1023c01e3ee723c32a0a57bea16e5ff13609f97eb2af0b-extracted/adapter_config.json
Adding adapter 'ar' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/1f4018c6b187912fb6b47e664a34880506aef8c7320e0135939626a97e0bb137-4b1153cd3ebfdce09e1023c01e3ee723c32a0a57bea16e5ff13609f97eb2af0b-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/1f4018c6b187912fb6b47e664a34880506aef8c7320e0135939626a97e0bb137-4b1153cd3ebfdce09e1023c01e3ee723c32a0a57bea16e5ff13609f97eb2af0b-extracted'
01/09/2022 23:54:33 - INFO - __main__ -   Language adapter for ar found
01/09/2022 23:54:33 - INFO - __main__ -   Set active language adapter to ar
01/09/2022 23:54:33 - INFO - __main__ -   Args Adapter Weight = None
01/09/2022 23:54:33 - INFO - __main__ -   Adapter Languages = ['ar']
01/09/2022 23:54:33 - INFO - __main__ -   Predict File = xquad.ar.json
01/09/2022 23:54:33 - INFO - __main__ -   Creating features from dataset file at /home/abhijeet/rohan/cloud-emea-copy/data//xquad
ar ar/wiki@ukp
  0%|          | 0/48 [00:00<?, ?it/s] 33%|███▎      | 16/48 [00:00<00:00, 133.46it/s] 62%|██████▎   | 30/48 [00:00<00:00, 120.89it/s] 96%|█████████▌| 46/48 [00:00<00:00, 134.27it/s]100%|██████████| 48/48 [00:00<00:00, 133.23it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<04:37,  4.28it/s]convert squad examples to features:  32%|███▏      | 385/1190 [00:00<00:01, 570.93it/s]convert squad examples to features:  97%|█████████▋| 1153/1190 [00:00<00:00, 1733.59it/s]convert squad examples to features: 100%|██████████| 1190/1190 [00:00<00:00, 1365.91it/s]/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|██████████| 1190/1190 [00:00<00:00, 343393.31it/s]
01/09/2022 23:54:35 - INFO - __main__ -   Local Rank = -1
01/09/2022 23:54:35 - INFO - __main__ -   Saving features into cached file /home/abhijeet/rohan/cloud-emea-copy/data//xquad/cached_xquad.ar.json_xlm-roberta-base_384_ar
Traceback (most recent call last):
  File "third_party/my_run_squad.py", line 1112, in <module>
    main()
  File "third_party/my_run_squad.py", line 1105, in main
    predict_and_save(args, adapter_args, model, tokenizer, lang2id, lang_adapter_names, task_name, 'test')
  File "third_party/my_run_squad.py", line 903, in predict_and_save
    results = evaluate(args, model, tokenizer, language=lang, lang2id=lang2id, adapter_weight=adapter_weight, mode=split)
  File "third_party/my_run_squad.py", line 439, in evaluate
    language=language, lang2id=lang2id)
  File "third_party/my_run_squad.py", line 633, in load_and_cache_examples
    torch.save({"features": features, "dataset": dataset}, cached_features_file)
  File "/home/abhijeet/rohan/venvs/cloud-emea-copy/lib/python3.6/site-packages/torch/serialization.py", line 379, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol)
  File "/home/abhijeet/rohan/venvs/cloud-emea-copy/lib/python3.6/site-packages/torch/serialization.py", line 484, in _save
    pickler.dump(obj)
  File "/home/abhijeet/rohan/venvs/cloud-emea-copy/lib/python3.6/site-packages/torch/serialization.py", line 467, in persistent_id
    if torch.is_storage(obj):
KeyboardInterrupt
PyTorch version 1.10.1+cu102 available.
01/09/2022 23:55:45 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/09/2022 23:55:45 - INFO - root -   save model
loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6
Model config XLMRobertaConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /home/abhijeet/.cache/torch/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8
loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/09/2022 23:55:56 - INFO - __main__ -   lang2id = None
01/09/2022 23:55:59 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='xlm-roberta-base', model_type='xlm-roberta', config_name=None, tokenizer_name=None, cache_dir=None, data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//xquad', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//squad/my-xlm-roberta-base-MaxLen384_qna_ar/', max_seq_length=384, train_file='/home/abhijeet/rohan/cloud-emea-copy/data//xquad/train-v1.1.json', predict_file='/home/abhijeet/rohan/cloud-emea-copy/data//xquad/dev-v1.1.json', eval_test_set=False, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=4, per_gpu_eval_batch_size=8, gradient_accumulation_steps=4, learning_rate=0.0001, weight_decay=0.0001, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=15.0, max_steps=-1, save_steps=1000, warmup_steps=500, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', eval_lang='en', predict_langs='ar', train_lang='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//squad/my-xlm-roberta-base-MaxLen384_qna_ar//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='qna', threads=8, version_2_with_negative=False, verbose_logging=False, n_best_size=20, max_query_length=64, max_answer_length=30, doc_stride=128, null_score_diff_threshold=0.0, predict_task_adapter='output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s1/checkpoint-best/qna', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix=None, en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/09/2022 23:55:59 - INFO - __main__ -   Results: {}
loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6
Model config XLMRobertaConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /home/abhijeet/.cache/torch/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8
loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/09/2022 23:56:09 - INFO - __main__ -   lang2id = None
01/09/2022 23:56:09 - INFO - __main__ -   Evaluating the model on the test set of all languages specified
01/09/2022 23:56:09 - INFO - __main__ -   Adapter will be loaded from this path: output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s1/checkpoint-best/qna
01/09/2022 23:56:09 - INFO - root -   Trying to decide if add adapter
01/09/2022 23:56:09 - INFO - root -   loading task adapter
Loading module configuration from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s1/checkpoint-best/qna/adapter_config.json
Adding adapter 'qna' of type 'text_task'.
Loading module weights from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s1/checkpoint-best/qna/pytorch_adapter.bin
Loading module configuration from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s1/checkpoint-best/qna/head_config.json
Loading module weights from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s1/checkpoint-best/qna/pytorch_model_head.bin
01/09/2022 23:56:10 - INFO - root -   loading lang adpater ar/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/ar/xlm-roberta-base/pfeiffer/ar_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1f4018c6b187912fb6b47e664a34880506aef8c7320e0135939626a97e0bb137-4b1153cd3ebfdce09e1023c01e3ee723c32a0a57bea16e5ff13609f97eb2af0b-extracted/adapter_config.json
Adding adapter 'ar' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/1f4018c6b187912fb6b47e664a34880506aef8c7320e0135939626a97e0bb137-4b1153cd3ebfdce09e1023c01e3ee723c32a0a57bea16e5ff13609f97eb2af0b-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/1f4018c6b187912fb6b47e664a34880506aef8c7320e0135939626a97e0bb137-4b1153cd3ebfdce09e1023c01e3ee723c32a0a57bea16e5ff13609f97eb2af0b-extracted'
01/09/2022 23:56:11 - INFO - __main__ -   Language adapter for ar found
01/09/2022 23:56:11 - INFO - __main__ -   Set active language adapter to ar
01/09/2022 23:56:11 - INFO - __main__ -   Args Adapter Weight = None
01/09/2022 23:56:11 - INFO - __main__ -   Adapter Languages = ['ar']
01/09/2022 23:56:11 - INFO - __main__ -   Predict File = xquad.ar.json
01/09/2022 23:56:11 - INFO - __main__ -   Creating features from dataset file at /home/abhijeet/rohan/cloud-emea-copy/data//xquad
ar ar/wiki@ukp
  0%|          | 0/48 [00:00<?, ?it/s] 33%|███▎      | 16/48 [00:00<00:00, 135.66it/s] 62%|██████▎   | 30/48 [00:00<00:00, 131.33it/s] 92%|█████████▏| 44/48 [00:00<00:00, 130.83it/s]100%|██████████| 48/48 [00:00<00:00, 133.92it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<04:38,  4.26it/s]convert squad examples to features:  32%|███▏      | 385/1190 [00:00<00:01, 571.75it/s]convert squad examples to features:  94%|█████████▍| 1121/1190 [00:00<00:00, 1734.90it/s]convert squad examples to features: 100%|██████████| 1190/1190 [00:00<00:00, 1386.11it/s]/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|██████████| 1190/1190 [00:00<00:00, 344340.93it/s]
01/09/2022 23:56:13 - INFO - __main__ -   Local Rank = -1
01/09/2022 23:56:13 - INFO - __main__ -   Saving features into cached file /home/abhijeet/rohan/cloud-emea-copy/data//xquad/cached_xquad.ar.json_xlm-roberta-base_384_ar
01/09/2022 23:56:14 - INFO - __main__ -   ***** Running evaluation  *****
01/09/2022 23:56:14 - INFO - __main__ -     Num examples = 1318
01/09/2022 23:56:14 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/165 [00:00<?, ?it/s]01/09/2022 23:56:14 - INFO - __main__ -   Batch Number = 1
Evaluating:   1%|          | 1/165 [00:00<00:24,  6.61it/s]01/09/2022 23:56:15 - INFO - __main__ -   Batch Number = 2
Evaluating:   1%|          | 2/165 [00:00<00:22,  7.29it/s]01/09/2022 23:56:15 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|▏         | 3/165 [00:00<00:21,  7.66it/s]01/09/2022 23:56:15 - INFO - __main__ -   Batch Number = 4
Evaluating:   2%|▏         | 4/165 [00:00<00:20,  7.92it/s]01/09/2022 23:56:15 - INFO - __main__ -   Batch Number = 5
Evaluating:   3%|▎         | 5/165 [00:00<00:19,  8.09it/s]01/09/2022 23:56:15 - INFO - __main__ -   Batch Number = 6
Evaluating:   4%|▎         | 6/165 [00:00<00:19,  8.18it/s]01/09/2022 23:56:15 - INFO - __main__ -   Batch Number = 7
Evaluating:   4%|▍         | 7/165 [00:00<00:19,  8.26it/s]01/09/2022 23:56:15 - INFO - __main__ -   Batch Number = 8
Evaluating:   5%|▍         | 8/165 [00:00<00:18,  8.31it/s]01/09/2022 23:56:15 - INFO - __main__ -   Batch Number = 9
Evaluating:   5%|▌         | 9/165 [00:01<00:18,  8.35it/s]01/09/2022 23:56:16 - INFO - __main__ -   Batch Number = 10
Evaluating:   6%|▌         | 10/165 [00:01<00:18,  8.36it/s]01/09/2022 23:56:16 - INFO - __main__ -   Batch Number = 11
Evaluating:   7%|▋         | 11/165 [00:01<00:18,  8.37it/s]01/09/2022 23:56:16 - INFO - __main__ -   Batch Number = 12
Evaluating:   7%|▋         | 12/165 [00:01<00:18,  8.38it/s]01/09/2022 23:56:16 - INFO - __main__ -   Batch Number = 13
Evaluating:   8%|▊         | 13/165 [00:01<00:27,  5.58it/s]01/09/2022 23:56:16 - INFO - __main__ -   Batch Number = 14
Evaluating:   8%|▊         | 14/165 [00:01<00:24,  6.21it/s]01/09/2022 23:56:16 - INFO - __main__ -   Batch Number = 15
Evaluating:   9%|▉         | 15/165 [00:02<00:25,  5.80it/s]01/09/2022 23:56:17 - INFO - __main__ -   Batch Number = 16
Evaluating:  10%|▉         | 16/165 [00:02<00:23,  6.37it/s]01/09/2022 23:56:17 - INFO - __main__ -   Batch Number = 17
Evaluating:  10%|█         | 17/165 [00:02<00:21,  6.86it/s]01/09/2022 23:56:17 - INFO - __main__ -   Batch Number = 18
Evaluating:  11%|█         | 18/165 [00:02<00:20,  7.25it/s]01/09/2022 23:56:17 - INFO - __main__ -   Batch Number = 19
Evaluating:  12%|█▏        | 19/165 [00:02<00:19,  7.55it/s]01/09/2022 23:56:17 - INFO - __main__ -   Batch Number = 20
Evaluating:  12%|█▏        | 20/165 [00:02<00:18,  7.78it/s]01/09/2022 23:56:17 - INFO - __main__ -   Batch Number = 21
Evaluating:  13%|█▎        | 21/165 [00:02<00:18,  7.92it/s]01/09/2022 23:56:17 - INFO - __main__ -   Batch Number = 22
Evaluating:  13%|█▎        | 22/165 [00:02<00:17,  8.04it/s]01/09/2022 23:56:17 - INFO - __main__ -   Batch Number = 23
Evaluating:  14%|█▍        | 23/165 [00:03<00:17,  8.12it/s]01/09/2022 23:56:17 - INFO - __main__ -   Batch Number = 24
Evaluating:  15%|█▍        | 24/165 [00:03<00:17,  8.18it/s]01/09/2022 23:56:18 - INFO - __main__ -   Batch Number = 25
Evaluating:  15%|█▌        | 25/165 [00:03<00:17,  8.22it/s]01/09/2022 23:56:18 - INFO - __main__ -   Batch Number = 26
Evaluating:  16%|█▌        | 26/165 [00:03<00:16,  8.22it/s]01/09/2022 23:56:18 - INFO - __main__ -   Batch Number = 27
Evaluating:  16%|█▋        | 27/165 [00:03<00:16,  8.20it/s]01/09/2022 23:56:18 - INFO - __main__ -   Batch Number = 28
Evaluating:  17%|█▋        | 28/165 [00:03<00:16,  8.17it/s]01/09/2022 23:56:18 - INFO - __main__ -   Batch Number = 29
Evaluating:  18%|█▊        | 29/165 [00:03<00:16,  8.22it/s]01/09/2022 23:56:18 - INFO - __main__ -   Batch Number = 30
Evaluating:  18%|█▊        | 30/165 [00:03<00:16,  8.25it/s]01/09/2022 23:56:18 - INFO - __main__ -   Batch Number = 31
Evaluating:  19%|█▉        | 31/165 [00:04<00:16,  8.27it/s]01/09/2022 23:56:18 - INFO - __main__ -   Batch Number = 32
Evaluating:  19%|█▉        | 32/165 [00:04<00:16,  8.27it/s]01/09/2022 23:56:19 - INFO - __main__ -   Batch Number = 33
Evaluating:  20%|██        | 33/165 [00:04<00:15,  8.26it/s]01/09/2022 23:56:19 - INFO - __main__ -   Batch Number = 34
Evaluating:  21%|██        | 34/165 [00:04<00:15,  8.28it/s]01/09/2022 23:56:19 - INFO - __main__ -   Batch Number = 35
Evaluating:  21%|██        | 35/165 [00:04<00:15,  8.23it/s]01/09/2022 23:56:19 - INFO - __main__ -   Batch Number = 36
Evaluating:  22%|██▏       | 36/165 [00:04<00:15,  8.23it/s]01/09/2022 23:56:19 - INFO - __main__ -   Batch Number = 37
Evaluating:  22%|██▏       | 37/165 [00:04<00:15,  8.23it/s]01/09/2022 23:56:19 - INFO - __main__ -   Batch Number = 38
Evaluating:  23%|██▎       | 38/165 [00:04<00:15,  8.25it/s]01/09/2022 23:56:19 - INFO - __main__ -   Batch Number = 39
Evaluating:  24%|██▎       | 39/165 [00:05<00:15,  8.26it/s]01/09/2022 23:56:19 - INFO - __main__ -   Batch Number = 40
Evaluating:  24%|██▍       | 40/165 [00:05<00:15,  8.29it/s]01/09/2022 23:56:20 - INFO - __main__ -   Batch Number = 41
Evaluating:  25%|██▍       | 41/165 [00:05<00:15,  8.26it/s]01/09/2022 23:56:20 - INFO - __main__ -   Batch Number = 42
Evaluating:  25%|██▌       | 42/165 [00:05<00:14,  8.28it/s]01/09/2022 23:56:20 - INFO - __main__ -   Batch Number = 43
Evaluating:  26%|██▌       | 43/165 [00:05<00:14,  8.28it/s]01/09/2022 23:56:20 - INFO - __main__ -   Batch Number = 44
Evaluating:  27%|██▋       | 44/165 [00:05<00:14,  8.28it/s]01/09/2022 23:56:20 - INFO - __main__ -   Batch Number = 45
Evaluating:  27%|██▋       | 45/165 [00:05<00:14,  8.29it/s]01/09/2022 23:56:20 - INFO - __main__ -   Batch Number = 46
Evaluating:  28%|██▊       | 46/165 [00:05<00:14,  8.28it/s]01/09/2022 23:56:20 - INFO - __main__ -   Batch Number = 47
Evaluating:  28%|██▊       | 47/165 [00:05<00:14,  8.32it/s]01/09/2022 23:56:20 - INFO - __main__ -   Batch Number = 48
Evaluating:  29%|██▉       | 48/165 [00:06<00:14,  8.34it/s]01/09/2022 23:56:20 - INFO - __main__ -   Batch Number = 49
Evaluating:  30%|██▉       | 49/165 [00:06<00:13,  8.35it/s]01/09/2022 23:56:21 - INFO - __main__ -   Batch Number = 50
Evaluating:  30%|███       | 50/165 [00:06<00:13,  8.36it/s]01/09/2022 23:56:21 - INFO - __main__ -   Batch Number = 51
Evaluating:  31%|███       | 51/165 [00:06<00:13,  8.37it/s]01/09/2022 23:56:21 - INFO - __main__ -   Batch Number = 52
Evaluating:  32%|███▏      | 52/165 [00:06<00:13,  8.36it/s]01/09/2022 23:56:21 - INFO - __main__ -   Batch Number = 53
Evaluating:  32%|███▏      | 53/165 [00:06<00:13,  8.27it/s]01/09/2022 23:56:21 - INFO - __main__ -   Batch Number = 54
Evaluating:  33%|███▎      | 54/165 [00:06<00:13,  8.27it/s]01/09/2022 23:56:21 - INFO - __main__ -   Batch Number = 55
Evaluating:  33%|███▎      | 55/165 [00:06<00:13,  8.28it/s]01/09/2022 23:56:21 - INFO - __main__ -   Batch Number = 56
Evaluating:  34%|███▍      | 56/165 [00:07<00:15,  7.10it/s]01/09/2022 23:56:22 - INFO - __main__ -   Batch Number = 57
Evaluating:  35%|███▍      | 57/165 [00:07<00:14,  7.42it/s]01/09/2022 23:56:22 - INFO - __main__ -   Batch Number = 58
Evaluating:  35%|███▌      | 58/165 [00:07<00:13,  7.67it/s]01/09/2022 23:56:22 - INFO - __main__ -   Batch Number = 59
Evaluating:  36%|███▌      | 59/165 [00:07<00:13,  7.85it/s]01/09/2022 23:56:22 - INFO - __main__ -   Batch Number = 60
Evaluating:  36%|███▋      | 60/165 [00:07<00:13,  7.98it/s]01/09/2022 23:56:22 - INFO - __main__ -   Batch Number = 61
Evaluating:  37%|███▋      | 61/165 [00:07<00:12,  8.07it/s]01/09/2022 23:56:22 - INFO - __main__ -   Batch Number = 62
Evaluating:  38%|███▊      | 62/165 [00:07<00:12,  8.14it/s]01/09/2022 23:56:22 - INFO - __main__ -   Batch Number = 63
Evaluating:  38%|███▊      | 63/165 [00:07<00:12,  8.18it/s]01/09/2022 23:56:22 - INFO - __main__ -   Batch Number = 64
Evaluating:  39%|███▉      | 64/165 [00:08<00:12,  8.21it/s]01/09/2022 23:56:22 - INFO - __main__ -   Batch Number = 65
Evaluating:  39%|███▉      | 65/165 [00:08<00:12,  8.23it/s]01/09/2022 23:56:23 - INFO - __main__ -   Batch Number = 66
Evaluating:  40%|████      | 66/165 [00:08<00:11,  8.26it/s]01/09/2022 23:56:23 - INFO - __main__ -   Batch Number = 67
Evaluating:  41%|████      | 67/165 [00:08<00:11,  8.28it/s]01/09/2022 23:56:23 - INFO - __main__ -   Batch Number = 68
Evaluating:  41%|████      | 68/165 [00:08<00:11,  8.29it/s]01/09/2022 23:56:23 - INFO - __main__ -   Batch Number = 69
Evaluating:  42%|████▏     | 69/165 [00:08<00:11,  8.29it/s]01/09/2022 23:56:23 - INFO - __main__ -   Batch Number = 70
Evaluating:  42%|████▏     | 70/165 [00:08<00:11,  8.28it/s]01/09/2022 23:56:23 - INFO - __main__ -   Batch Number = 71
Evaluating:  43%|████▎     | 71/165 [00:08<00:11,  8.26it/s]01/09/2022 23:56:23 - INFO - __main__ -   Batch Number = 72
Evaluating:  44%|████▎     | 72/165 [00:09<00:11,  8.26it/s]01/09/2022 23:56:23 - INFO - __main__ -   Batch Number = 73
Evaluating:  44%|████▍     | 73/165 [00:09<00:11,  8.27it/s]01/09/2022 23:56:24 - INFO - __main__ -   Batch Number = 74
Evaluating:  45%|████▍     | 74/165 [00:09<00:11,  8.27it/s]01/09/2022 23:56:24 - INFO - __main__ -   Batch Number = 75
Evaluating:  45%|████▌     | 75/165 [00:09<00:10,  8.26it/s]01/09/2022 23:56:24 - INFO - __main__ -   Batch Number = 76
Evaluating:  46%|████▌     | 76/165 [00:09<00:10,  8.28it/s]01/09/2022 23:56:24 - INFO - __main__ -   Batch Number = 77
Evaluating:  47%|████▋     | 77/165 [00:09<00:10,  8.28it/s]01/09/2022 23:56:24 - INFO - __main__ -   Batch Number = 78
Evaluating:  47%|████▋     | 78/165 [00:09<00:10,  8.28it/s]01/09/2022 23:56:24 - INFO - __main__ -   Batch Number = 79
Evaluating:  48%|████▊     | 79/165 [00:09<00:10,  8.29it/s]01/09/2022 23:56:24 - INFO - __main__ -   Batch Number = 80
Evaluating:  48%|████▊     | 80/165 [00:10<00:10,  8.29it/s]01/09/2022 23:56:24 - INFO - __main__ -   Batch Number = 81
Evaluating:  49%|████▉     | 81/165 [00:10<00:10,  8.29it/s]01/09/2022 23:56:25 - INFO - __main__ -   Batch Number = 82
Evaluating:  50%|████▉     | 82/165 [00:10<00:10,  8.29it/s]01/09/2022 23:56:25 - INFO - __main__ -   Batch Number = 83
Evaluating:  50%|█████     | 83/165 [00:10<00:09,  8.32it/s]01/09/2022 23:56:25 - INFO - __main__ -   Batch Number = 84
Evaluating:  51%|█████     | 84/165 [00:10<00:09,  8.33it/s]01/09/2022 23:56:25 - INFO - __main__ -   Batch Number = 85
Evaluating:  52%|█████▏    | 85/165 [00:10<00:09,  8.33it/s]01/09/2022 23:56:25 - INFO - __main__ -   Batch Number = 86
Evaluating:  52%|█████▏    | 86/165 [00:10<00:09,  8.30it/s]01/09/2022 23:56:25 - INFO - __main__ -   Batch Number = 87
Evaluating:  53%|█████▎    | 87/165 [00:10<00:09,  8.30it/s]01/09/2022 23:56:25 - INFO - __main__ -   Batch Number = 88
Evaluating:  53%|█████▎    | 88/165 [00:10<00:09,  8.32it/s]01/09/2022 23:56:25 - INFO - __main__ -   Batch Number = 89
Evaluating:  54%|█████▍    | 89/165 [00:11<00:09,  8.31it/s]01/09/2022 23:56:25 - INFO - __main__ -   Batch Number = 90
Evaluating:  55%|█████▍    | 90/165 [00:11<00:09,  8.31it/s]01/09/2022 23:56:26 - INFO - __main__ -   Batch Number = 91
Evaluating:  55%|█████▌    | 91/165 [00:11<00:08,  8.30it/s]01/09/2022 23:56:26 - INFO - __main__ -   Batch Number = 92
Evaluating:  56%|█████▌    | 92/165 [00:11<00:08,  8.31it/s]01/09/2022 23:56:26 - INFO - __main__ -   Batch Number = 93
Evaluating:  56%|█████▋    | 93/165 [00:11<00:08,  8.31it/s]01/09/2022 23:56:26 - INFO - __main__ -   Batch Number = 94
Evaluating:  57%|█████▋    | 94/165 [00:11<00:08,  8.29it/s]01/09/2022 23:56:26 - INFO - __main__ -   Batch Number = 95
Evaluating:  58%|█████▊    | 95/165 [00:11<00:08,  8.27it/s]01/09/2022 23:56:26 - INFO - __main__ -   Batch Number = 96
Evaluating:  58%|█████▊    | 96/165 [00:11<00:08,  8.27it/s]01/09/2022 23:56:26 - INFO - __main__ -   Batch Number = 97
Evaluating:  59%|█████▉    | 97/165 [00:12<00:09,  7.06it/s]01/09/2022 23:56:27 - INFO - __main__ -   Batch Number = 98
Evaluating:  59%|█████▉    | 98/165 [00:12<00:09,  7.38it/s]01/09/2022 23:56:27 - INFO - __main__ -   Batch Number = 99
Evaluating:  60%|██████    | 99/165 [00:12<00:08,  7.63it/s]01/09/2022 23:56:27 - INFO - __main__ -   Batch Number = 100
Evaluating:  61%|██████    | 100/165 [00:12<00:08,  7.82it/s]01/09/2022 23:56:27 - INFO - __main__ -   Batch Number = 101
Evaluating:  61%|██████    | 101/165 [00:12<00:08,  7.94it/s]01/09/2022 23:56:27 - INFO - __main__ -   Batch Number = 102
Evaluating:  62%|██████▏   | 102/165 [00:12<00:07,  8.04it/s]01/09/2022 23:56:27 - INFO - __main__ -   Batch Number = 103
Evaluating:  62%|██████▏   | 103/165 [00:12<00:07,  8.10it/s]01/09/2022 23:56:27 - INFO - __main__ -   Batch Number = 104
Evaluating:  63%|██████▎   | 104/165 [00:12<00:07,  8.15it/s]01/09/2022 23:56:27 - INFO - __main__ -   Batch Number = 105
Evaluating:  64%|██████▎   | 105/165 [00:13<00:07,  8.19it/s]01/09/2022 23:56:27 - INFO - __main__ -   Batch Number = 106
Evaluating:  64%|██████▍   | 106/165 [00:13<00:07,  8.22it/s]01/09/2022 23:56:28 - INFO - __main__ -   Batch Number = 107
Evaluating:  65%|██████▍   | 107/165 [00:13<00:07,  8.23it/s]01/09/2022 23:56:28 - INFO - __main__ -   Batch Number = 108
Evaluating:  65%|██████▌   | 108/165 [00:13<00:06,  8.25it/s]01/09/2022 23:56:28 - INFO - __main__ -   Batch Number = 109
Evaluating:  66%|██████▌   | 109/165 [00:13<00:06,  8.27it/s]01/09/2022 23:56:28 - INFO - __main__ -   Batch Number = 110
Evaluating:  67%|██████▋   | 110/165 [00:13<00:06,  8.28it/s]01/09/2022 23:56:28 - INFO - __main__ -   Batch Number = 111
Evaluating:  67%|██████▋   | 111/165 [00:13<00:06,  8.29it/s]01/09/2022 23:56:28 - INFO - __main__ -   Batch Number = 112
Evaluating:  68%|██████▊   | 112/165 [00:13<00:06,  8.29it/s]01/09/2022 23:56:28 - INFO - __main__ -   Batch Number = 113
Evaluating:  68%|██████▊   | 113/165 [00:14<00:06,  8.29it/s]01/09/2022 23:56:28 - INFO - __main__ -   Batch Number = 114
Evaluating:  69%|██████▉   | 114/165 [00:14<00:06,  8.28it/s]01/09/2022 23:56:29 - INFO - __main__ -   Batch Number = 115
Evaluating:  70%|██████▉   | 115/165 [00:14<00:06,  8.28it/s]01/09/2022 23:56:29 - INFO - __main__ -   Batch Number = 116
Evaluating:  70%|███████   | 116/165 [00:14<00:05,  8.27it/s]01/09/2022 23:56:29 - INFO - __main__ -   Batch Number = 117
Evaluating:  71%|███████   | 117/165 [00:14<00:05,  8.27it/s]01/09/2022 23:56:29 - INFO - __main__ -   Batch Number = 118
Evaluating:  72%|███████▏  | 118/165 [00:14<00:05,  8.27it/s]01/09/2022 23:56:29 - INFO - __main__ -   Batch Number = 119
Evaluating:  72%|███████▏  | 119/165 [00:14<00:05,  8.26it/s]01/09/2022 23:56:29 - INFO - __main__ -   Batch Number = 120
Evaluating:  73%|███████▎  | 120/165 [00:14<00:05,  8.25it/s]01/09/2022 23:56:29 - INFO - __main__ -   Batch Number = 121
Evaluating:  73%|███████▎  | 121/165 [00:15<00:05,  8.23it/s]01/09/2022 23:56:29 - INFO - __main__ -   Batch Number = 122
Evaluating:  74%|███████▍  | 122/165 [00:15<00:05,  8.23it/s]01/09/2022 23:56:30 - INFO - __main__ -   Batch Number = 123
Evaluating:  75%|███████▍  | 123/165 [00:15<00:05,  8.24it/s]01/09/2022 23:56:30 - INFO - __main__ -   Batch Number = 124
Evaluating:  75%|███████▌  | 124/165 [00:15<00:05,  8.19it/s]01/09/2022 23:56:30 - INFO - __main__ -   Batch Number = 125
Evaluating:  76%|███████▌  | 125/165 [00:15<00:04,  8.20it/s]01/09/2022 23:56:30 - INFO - __main__ -   Batch Number = 126
Evaluating:  76%|███████▋  | 126/165 [00:15<00:04,  8.21it/s]01/09/2022 23:56:30 - INFO - __main__ -   Batch Number = 127
Evaluating:  77%|███████▋  | 127/165 [00:15<00:04,  8.23it/s]01/09/2022 23:56:30 - INFO - __main__ -   Batch Number = 128
Evaluating:  78%|███████▊  | 128/165 [00:15<00:04,  8.24it/s]01/09/2022 23:56:30 - INFO - __main__ -   Batch Number = 129
Evaluating:  78%|███████▊  | 129/165 [00:16<00:04,  8.24it/s]01/09/2022 23:56:30 - INFO - __main__ -   Batch Number = 130
Evaluating:  79%|███████▉  | 130/165 [00:16<00:04,  8.24it/s]01/09/2022 23:56:31 - INFO - __main__ -   Batch Number = 131
Evaluating:  79%|███████▉  | 131/165 [00:16<00:04,  8.22it/s]01/09/2022 23:56:31 - INFO - __main__ -   Batch Number = 132
Evaluating:  80%|████████  | 132/165 [00:16<00:04,  8.22it/s]01/09/2022 23:56:31 - INFO - __main__ -   Batch Number = 133
Evaluating:  81%|████████  | 133/165 [00:16<00:03,  8.23it/s]01/09/2022 23:56:31 - INFO - __main__ -   Batch Number = 134
Evaluating:  81%|████████  | 134/165 [00:16<00:03,  8.23it/s]01/09/2022 23:56:31 - INFO - __main__ -   Batch Number = 135
Evaluating:  82%|████████▏ | 135/165 [00:16<00:03,  8.21it/s]01/09/2022 23:56:31 - INFO - __main__ -   Batch Number = 136
Evaluating:  82%|████████▏ | 136/165 [00:16<00:03,  8.22it/s]01/09/2022 23:56:31 - INFO - __main__ -   Batch Number = 137
Evaluating:  83%|████████▎ | 137/165 [00:16<00:03,  8.23it/s]01/09/2022 23:56:31 - INFO - __main__ -   Batch Number = 138
Evaluating:  84%|████████▎ | 138/165 [00:17<00:03,  7.18it/s]01/09/2022 23:56:32 - INFO - __main__ -   Batch Number = 139
Evaluating:  84%|████████▍ | 139/165 [00:17<00:03,  7.47it/s]01/09/2022 23:56:32 - INFO - __main__ -   Batch Number = 140
Evaluating:  85%|████████▍ | 140/165 [00:17<00:03,  7.68it/s]01/09/2022 23:56:32 - INFO - __main__ -   Batch Number = 141
Evaluating:  85%|████████▌ | 141/165 [00:17<00:03,  7.85it/s]01/09/2022 23:56:32 - INFO - __main__ -   Batch Number = 142
Evaluating:  86%|████████▌ | 142/165 [00:17<00:02,  7.97it/s]01/09/2022 23:56:32 - INFO - __main__ -   Batch Number = 143
Evaluating:  87%|████████▋ | 143/165 [00:17<00:02,  8.07it/s]01/09/2022 23:56:32 - INFO - __main__ -   Batch Number = 144
Evaluating:  87%|████████▋ | 144/165 [00:17<00:02,  8.13it/s]01/09/2022 23:56:32 - INFO - __main__ -   Batch Number = 145
Evaluating:  88%|████████▊ | 145/165 [00:18<00:02,  8.17it/s]01/09/2022 23:56:32 - INFO - __main__ -   Batch Number = 146
Evaluating:  88%|████████▊ | 146/165 [00:18<00:02,  8.18it/s]01/09/2022 23:56:33 - INFO - __main__ -   Batch Number = 147
Evaluating:  89%|████████▉ | 147/165 [00:18<00:02,  8.19it/s]01/09/2022 23:56:33 - INFO - __main__ -   Batch Number = 148
Evaluating:  90%|████████▉ | 148/165 [00:18<00:02,  8.21it/s]01/09/2022 23:56:33 - INFO - __main__ -   Batch Number = 149
Evaluating:  90%|█████████ | 149/165 [00:18<00:01,  8.22it/s]01/09/2022 23:56:33 - INFO - __main__ -   Batch Number = 150
Evaluating:  91%|█████████ | 150/165 [00:18<00:01,  8.23it/s]01/09/2022 23:56:33 - INFO - __main__ -   Batch Number = 151
Evaluating:  92%|█████████▏| 151/165 [00:18<00:01,  8.22it/s]01/09/2022 23:56:33 - INFO - __main__ -   Batch Number = 152
Evaluating:  92%|█████████▏| 152/165 [00:18<00:01,  8.23it/s]01/09/2022 23:56:33 - INFO - __main__ -   Batch Number = 153
Evaluating:  93%|█████████▎| 153/165 [00:18<00:01,  8.25it/s]01/09/2022 23:56:33 - INFO - __main__ -   Batch Number = 154
Evaluating:  93%|█████████▎| 154/165 [00:19<00:01,  8.25it/s]01/09/2022 23:56:33 - INFO - __main__ -   Batch Number = 155
Evaluating:  94%|█████████▍| 155/165 [00:19<00:01,  8.25it/s]01/09/2022 23:56:34 - INFO - __main__ -   Batch Number = 156
Evaluating:  95%|█████████▍| 156/165 [00:19<00:01,  8.24it/s]01/09/2022 23:56:34 - INFO - __main__ -   Batch Number = 157
Evaluating:  95%|█████████▌| 157/165 [00:19<00:00,  8.24it/s]01/09/2022 23:56:34 - INFO - __main__ -   Batch Number = 158
Evaluating:  96%|█████████▌| 158/165 [00:19<00:00,  8.23it/s]01/09/2022 23:56:34 - INFO - __main__ -   Batch Number = 159
Evaluating:  96%|█████████▋| 159/165 [00:19<00:00,  8.24it/s]01/09/2022 23:56:34 - INFO - __main__ -   Batch Number = 160
Evaluating:  97%|█████████▋| 160/165 [00:19<00:00,  8.23it/s]01/09/2022 23:56:34 - INFO - __main__ -   Batch Number = 161
Evaluating:  98%|█████████▊| 161/165 [00:19<00:00,  8.18it/s]01/09/2022 23:56:34 - INFO - __main__ -   Batch Number = 162
Evaluating:  98%|█████████▊| 162/165 [00:20<00:00,  8.17it/s]01/09/2022 23:56:34 - INFO - __main__ -   Batch Number = 163
Evaluating:  99%|█████████▉| 163/165 [00:20<00:00,  8.15it/s]01/09/2022 23:56:35 - INFO - __main__ -   Batch Number = 164
Evaluating:  99%|█████████▉| 164/165 [00:20<00:00,  8.18it/s]01/09/2022 23:56:35 - INFO - __main__ -   Batch Number = 165
Evaluating: 100%|██████████| 165/165 [00:20<00:00,  8.08it/s]
01/09/2022 23:56:35 - INFO - __main__ -     Evaluation done in total 20.413396 secs (0.015488 sec per example)
Writing predictions to: /home/abhijeet/rohan/cloud-emea-copy/outputs//squad/my-xlm-roberta-base-MaxLen384_qna_ar/predictions_ar_.json
Writing nbest to: /home/abhijeet/rohan/cloud-emea-copy/outputs//squad/my-xlm-roberta-base-MaxLen384_qna_ar/nbest_predictions_ar_.json
01/09/2022 23:56:39 - INFO - __main__ -   Results = OrderedDict([('exact', 48.15126050420168), ('f1', 64.14469544816643), ('precision', 65.92482147802217), ('recall', 67.33524970532929), ('total', 1190), ('HasAns_exact', 48.15126050420168), ('HasAns_f1', 64.14469544816643), ('HasAns_total', 1190), ('best_exact', 48.15126050420168), ('best_exact_thresh', 0.0), ('best_f1', 64.14469544816643), ('best_f1_thresh', 0.0)])
PyTorch version 1.10.1+cu102 available.
01/09/2022 23:56:41 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/09/2022 23:56:41 - INFO - root -   save model
loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6
Model config XLMRobertaConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /home/abhijeet/.cache/torch/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8
loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/09/2022 23:56:51 - INFO - __main__ -   lang2id = None
01/09/2022 23:56:54 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='xlm-roberta-base', model_type='xlm-roberta', config_name=None, tokenizer_name=None, cache_dir=None, data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//xquad', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//squad/my-xlm-roberta-base-MaxLen384_qna_ar/', max_seq_length=384, train_file='/home/abhijeet/rohan/cloud-emea-copy/data//xquad/train-v1.1.json', predict_file='/home/abhijeet/rohan/cloud-emea-copy/data//xquad/dev-v1.1.json', eval_test_set=False, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=4, per_gpu_eval_batch_size=8, gradient_accumulation_steps=4, learning_rate=0.0001, weight_decay=0.0001, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=15.0, max_steps=-1, save_steps=1000, warmup_steps=500, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', eval_lang='en', predict_langs='ar', train_lang='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//squad/my-xlm-roberta-base-MaxLen384_qna_ar//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='qna', threads=8, version_2_with_negative=False, verbose_logging=False, n_best_size=20, max_query_length=64, max_answer_length=30, doc_stride=128, null_score_diff_threshold=0.0, predict_task_adapter='output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s2/checkpoint-best/qna', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix=None, en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/09/2022 23:56:54 - INFO - __main__ -   Results: {}
loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6
Model config XLMRobertaConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /home/abhijeet/.cache/torch/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8
loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/09/2022 23:57:04 - INFO - __main__ -   lang2id = None
01/09/2022 23:57:04 - INFO - __main__ -   Evaluating the model on the test set of all languages specified
01/09/2022 23:57:04 - INFO - __main__ -   Adapter will be loaded from this path: output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s2/checkpoint-best/qna
01/09/2022 23:57:04 - INFO - root -   Trying to decide if add adapter
01/09/2022 23:57:04 - INFO - root -   loading task adapter
Loading module configuration from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s2/checkpoint-best/qna/adapter_config.json
Adding adapter 'qna' of type 'text_task'.
Loading module weights from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s2/checkpoint-best/qna/pytorch_adapter.bin
Loading module configuration from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s2/checkpoint-best/qna/head_config.json
Loading module weights from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s2/checkpoint-best/qna/pytorch_model_head.bin
01/09/2022 23:57:04 - INFO - root -   loading lang adpater ar/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/ar/xlm-roberta-base/pfeiffer/ar_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1f4018c6b187912fb6b47e664a34880506aef8c7320e0135939626a97e0bb137-4b1153cd3ebfdce09e1023c01e3ee723c32a0a57bea16e5ff13609f97eb2af0b-extracted/adapter_config.json
Adding adapter 'ar' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/1f4018c6b187912fb6b47e664a34880506aef8c7320e0135939626a97e0bb137-4b1153cd3ebfdce09e1023c01e3ee723c32a0a57bea16e5ff13609f97eb2af0b-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/1f4018c6b187912fb6b47e664a34880506aef8c7320e0135939626a97e0bb137-4b1153cd3ebfdce09e1023c01e3ee723c32a0a57bea16e5ff13609f97eb2af0b-extracted'
01/09/2022 23:57:05 - INFO - __main__ -   Language adapter for ar found
01/09/2022 23:57:05 - INFO - __main__ -   Set active language adapter to ar
01/09/2022 23:57:05 - INFO - __main__ -   Args Adapter Weight = None
01/09/2022 23:57:05 - INFO - __main__ -   Adapter Languages = ['ar']
01/09/2022 23:57:05 - INFO - __main__ -   Predict File = xquad.ar.json
01/09/2022 23:57:05 - INFO - __main__ -   Creating features from dataset file at /home/abhijeet/rohan/cloud-emea-copy/data//xquad
ar ar/wiki@ukp
  0%|          | 0/48 [00:00<?, ?it/s] 33%|███▎      | 16/48 [00:00<00:00, 135.31it/s] 62%|██████▎   | 30/48 [00:00<00:00, 133.48it/s] 96%|█████████▌| 46/48 [00:00<00:00, 144.23it/s]100%|██████████| 48/48 [00:00<00:00, 142.98it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<04:43,  4.19it/s]convert squad examples to features:  32%|███▏      | 385/1190 [00:00<00:01, 553.50it/s]convert squad examples to features:  94%|█████████▍| 1121/1190 [00:00<00:00, 1679.43it/s]convert squad examples to features: 100%|██████████| 1190/1190 [00:00<00:00, 1333.45it/s]/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|██████████| 1190/1190 [00:00<00:00, 341443.55it/s]
01/09/2022 23:57:07 - INFO - __main__ -   Local Rank = -1
01/09/2022 23:57:07 - INFO - __main__ -   Saving features into cached file /home/abhijeet/rohan/cloud-emea-copy/data//xquad/cached_xquad.ar.json_xlm-roberta-base_384_ar
01/09/2022 23:57:09 - INFO - __main__ -   ***** Running evaluation  *****
01/09/2022 23:57:09 - INFO - __main__ -     Num examples = 1318
01/09/2022 23:57:09 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/165 [00:00<?, ?it/s]01/09/2022 23:57:09 - INFO - __main__ -   Batch Number = 1
Evaluating:   1%|          | 1/165 [00:00<00:24,  6.76it/s]01/09/2022 23:57:09 - INFO - __main__ -   Batch Number = 2
Evaluating:   1%|          | 2/165 [00:00<00:22,  7.19it/s]01/09/2022 23:57:09 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|▏         | 3/165 [00:00<00:21,  7.49it/s]01/09/2022 23:57:09 - INFO - __main__ -   Batch Number = 4
Evaluating:   2%|▏         | 4/165 [00:00<00:20,  7.79it/s]01/09/2022 23:57:09 - INFO - __main__ -   Batch Number = 5
Evaluating:   3%|▎         | 5/165 [00:00<00:20,  7.98it/s]01/09/2022 23:57:09 - INFO - __main__ -   Batch Number = 6
Evaluating:   4%|▎         | 6/165 [00:00<00:19,  8.09it/s]01/09/2022 23:57:10 - INFO - __main__ -   Batch Number = 7
Evaluating:   4%|▍         | 7/165 [00:00<00:19,  8.16it/s]01/09/2022 23:57:10 - INFO - __main__ -   Batch Number = 8
Evaluating:   5%|▍         | 8/165 [00:01<00:19,  8.21it/s]01/09/2022 23:57:10 - INFO - __main__ -   Batch Number = 9
Evaluating:   5%|▌         | 9/165 [00:01<00:18,  8.24it/s]01/09/2022 23:57:10 - INFO - __main__ -   Batch Number = 10
Evaluating:   6%|▌         | 10/165 [00:01<00:18,  8.26it/s]01/09/2022 23:57:10 - INFO - __main__ -   Batch Number = 11
Evaluating:   7%|▋         | 11/165 [00:01<00:18,  8.25it/s]01/09/2022 23:57:10 - INFO - __main__ -   Batch Number = 12
Evaluating:   7%|▋         | 12/165 [00:01<00:18,  8.26it/s]01/09/2022 23:57:11 - INFO - __main__ -   Batch Number = 13
Evaluating:   8%|▊         | 13/165 [00:01<00:25,  5.86it/s]01/09/2022 23:57:11 - INFO - __main__ -   Batch Number = 14
Evaluating:   8%|▊         | 14/165 [00:01<00:23,  6.41it/s]01/09/2022 23:57:11 - INFO - __main__ -   Batch Number = 15
Evaluating:   9%|▉         | 15/165 [00:02<00:21,  6.88it/s]01/09/2022 23:57:11 - INFO - __main__ -   Batch Number = 16
Evaluating:  10%|▉         | 16/165 [00:02<00:20,  7.24it/s]01/09/2022 23:57:11 - INFO - __main__ -   Batch Number = 17
Evaluating:  10%|█         | 17/165 [00:02<00:19,  7.53it/s]01/09/2022 23:57:11 - INFO - __main__ -   Batch Number = 18
Evaluating:  11%|█         | 18/165 [00:02<00:18,  7.74it/s]01/09/2022 23:57:11 - INFO - __main__ -   Batch Number = 19
Evaluating:  12%|█▏        | 19/165 [00:02<00:18,  7.90it/s]01/09/2022 23:57:11 - INFO - __main__ -   Batch Number = 20
Evaluating:  12%|█▏        | 20/165 [00:02<00:18,  8.02it/s]01/09/2022 23:57:11 - INFO - __main__ -   Batch Number = 21
Evaluating:  13%|█▎        | 21/165 [00:02<00:17,  8.09it/s]01/09/2022 23:57:12 - INFO - __main__ -   Batch Number = 22
Evaluating:  13%|█▎        | 22/165 [00:02<00:17,  8.14it/s]01/09/2022 23:57:12 - INFO - __main__ -   Batch Number = 23
Evaluating:  14%|█▍        | 23/165 [00:02<00:17,  8.18it/s]01/09/2022 23:57:12 - INFO - __main__ -   Batch Number = 24
Evaluating:  15%|█▍        | 24/165 [00:03<00:17,  8.22it/s]01/09/2022 23:57:12 - INFO - __main__ -   Batch Number = 25
Evaluating:  15%|█▌        | 25/165 [00:03<00:18,  7.62it/s]01/09/2022 23:57:12 - INFO - __main__ -   Batch Number = 26
Evaluating:  16%|█▌        | 26/165 [00:03<00:17,  7.81it/s]01/09/2022 23:57:12 - INFO - __main__ -   Batch Number = 27
Evaluating:  16%|█▋        | 27/165 [00:03<00:17,  7.95it/s]01/09/2022 23:57:12 - INFO - __main__ -   Batch Number = 28
Evaluating:  17%|█▋        | 28/165 [00:03<00:17,  8.05it/s]01/09/2022 23:57:12 - INFO - __main__ -   Batch Number = 29
Evaluating:  18%|█▊        | 29/165 [00:03<00:16,  8.10it/s]01/09/2022 23:57:13 - INFO - __main__ -   Batch Number = 30
Evaluating:  18%|█▊        | 30/165 [00:03<00:16,  8.15it/s]01/09/2022 23:57:13 - INFO - __main__ -   Batch Number = 31
Evaluating:  19%|█▉        | 31/165 [00:03<00:16,  8.18it/s]01/09/2022 23:57:13 - INFO - __main__ -   Batch Number = 32
Evaluating:  19%|█▉        | 32/165 [00:04<00:16,  8.21it/s]01/09/2022 23:57:13 - INFO - __main__ -   Batch Number = 33
Evaluating:  20%|██        | 33/165 [00:04<00:16,  8.22it/s]01/09/2022 23:57:13 - INFO - __main__ -   Batch Number = 34
Evaluating:  21%|██        | 34/165 [00:04<00:15,  8.23it/s]01/09/2022 23:57:13 - INFO - __main__ -   Batch Number = 35
Evaluating:  21%|██        | 35/165 [00:04<00:15,  8.24it/s]01/09/2022 23:57:13 - INFO - __main__ -   Batch Number = 36
Evaluating:  22%|██▏       | 36/165 [00:04<00:15,  8.23it/s]01/09/2022 23:57:13 - INFO - __main__ -   Batch Number = 37
Evaluating:  22%|██▏       | 37/165 [00:04<00:15,  8.21it/s]01/09/2022 23:57:14 - INFO - __main__ -   Batch Number = 38
Evaluating:  23%|██▎       | 38/165 [00:04<00:15,  8.22it/s]01/09/2022 23:57:14 - INFO - __main__ -   Batch Number = 39
Evaluating:  24%|██▎       | 39/165 [00:04<00:15,  8.23it/s]01/09/2022 23:57:14 - INFO - __main__ -   Batch Number = 40
Evaluating:  24%|██▍       | 40/165 [00:05<00:15,  8.24it/s]01/09/2022 23:57:14 - INFO - __main__ -   Batch Number = 41
Evaluating:  25%|██▍       | 41/165 [00:05<00:15,  8.22it/s]01/09/2022 23:57:14 - INFO - __main__ -   Batch Number = 42
Evaluating:  25%|██▌       | 42/165 [00:05<00:14,  8.23it/s]01/09/2022 23:57:14 - INFO - __main__ -   Batch Number = 43
Evaluating:  26%|██▌       | 43/165 [00:05<00:14,  8.23it/s]01/09/2022 23:57:14 - INFO - __main__ -   Batch Number = 44
Evaluating:  27%|██▋       | 44/165 [00:05<00:14,  8.23it/s]01/09/2022 23:57:14 - INFO - __main__ -   Batch Number = 45
Evaluating:  27%|██▋       | 45/165 [00:05<00:14,  8.22it/s]01/09/2022 23:57:15 - INFO - __main__ -   Batch Number = 46
Evaluating:  28%|██▊       | 46/165 [00:05<00:14,  8.24it/s]01/09/2022 23:57:15 - INFO - __main__ -   Batch Number = 47
Evaluating:  28%|██▊       | 47/165 [00:05<00:14,  8.24it/s]01/09/2022 23:57:15 - INFO - __main__ -   Batch Number = 48
Evaluating:  29%|██▉       | 48/165 [00:06<00:14,  8.24it/s]01/09/2022 23:57:15 - INFO - __main__ -   Batch Number = 49
Evaluating:  30%|██▉       | 49/165 [00:06<00:14,  8.24it/s]01/09/2022 23:57:15 - INFO - __main__ -   Batch Number = 50
Evaluating:  30%|███       | 50/165 [00:06<00:13,  8.24it/s]01/09/2022 23:57:15 - INFO - __main__ -   Batch Number = 51
Evaluating:  31%|███       | 51/165 [00:06<00:13,  8.24it/s]01/09/2022 23:57:15 - INFO - __main__ -   Batch Number = 52
Evaluating:  32%|███▏      | 52/165 [00:06<00:13,  8.24it/s]01/09/2022 23:57:15 - INFO - __main__ -   Batch Number = 53
Evaluating:  32%|███▏      | 53/165 [00:06<00:13,  8.24it/s]01/09/2022 23:57:16 - INFO - __main__ -   Batch Number = 54
Evaluating:  33%|███▎      | 54/165 [00:06<00:13,  8.24it/s]01/09/2022 23:57:16 - INFO - __main__ -   Batch Number = 55
Evaluating:  33%|███▎      | 55/165 [00:06<00:13,  8.25it/s]01/09/2022 23:57:16 - INFO - __main__ -   Batch Number = 56
Evaluating:  34%|███▍      | 56/165 [00:07<00:13,  8.23it/s]01/09/2022 23:57:16 - INFO - __main__ -   Batch Number = 57
Evaluating:  35%|███▍      | 57/165 [00:07<00:13,  8.22it/s]01/09/2022 23:57:16 - INFO - __main__ -   Batch Number = 58
Evaluating:  35%|███▌      | 58/165 [00:07<00:13,  8.22it/s]01/09/2022 23:57:16 - INFO - __main__ -   Batch Number = 59
Evaluating:  36%|███▌      | 59/165 [00:07<00:12,  8.21it/s]01/09/2022 23:57:16 - INFO - __main__ -   Batch Number = 60
Evaluating:  36%|███▋      | 60/165 [00:07<00:12,  8.21it/s]01/09/2022 23:57:16 - INFO - __main__ -   Batch Number = 61
Evaluating:  37%|███▋      | 61/165 [00:07<00:12,  8.21it/s]01/09/2022 23:57:16 - INFO - __main__ -   Batch Number = 62
Evaluating:  38%|███▊      | 62/165 [00:07<00:12,  8.21it/s]01/09/2022 23:57:17 - INFO - __main__ -   Batch Number = 63
Evaluating:  38%|███▊      | 63/165 [00:07<00:12,  8.21it/s]01/09/2022 23:57:17 - INFO - __main__ -   Batch Number = 64
Evaluating:  39%|███▉      | 64/165 [00:07<00:12,  8.21it/s]01/09/2022 23:57:17 - INFO - __main__ -   Batch Number = 65
Evaluating:  39%|███▉      | 65/165 [00:08<00:12,  8.22it/s]01/09/2022 23:57:17 - INFO - __main__ -   Batch Number = 66
Evaluating:  40%|████      | 66/165 [00:08<00:12,  8.01it/s]01/09/2022 23:57:17 - INFO - __main__ -   Batch Number = 67
Evaluating:  41%|████      | 67/165 [00:08<00:12,  8.07it/s]01/09/2022 23:57:17 - INFO - __main__ -   Batch Number = 68
Evaluating:  41%|████      | 68/165 [00:08<00:11,  8.13it/s]01/09/2022 23:57:17 - INFO - __main__ -   Batch Number = 69
Evaluating:  42%|████▏     | 69/165 [00:08<00:11,  8.16it/s]01/09/2022 23:57:17 - INFO - __main__ -   Batch Number = 70
Evaluating:  42%|████▏     | 70/165 [00:08<00:11,  8.18it/s]01/09/2022 23:57:18 - INFO - __main__ -   Batch Number = 71
Evaluating:  43%|████▎     | 71/165 [00:08<00:11,  8.19it/s]01/09/2022 23:57:18 - INFO - __main__ -   Batch Number = 72
Evaluating:  44%|████▎     | 72/165 [00:08<00:11,  8.20it/s]01/09/2022 23:57:18 - INFO - __main__ -   Batch Number = 73
Evaluating:  44%|████▍     | 73/165 [00:09<00:11,  8.21it/s]01/09/2022 23:57:18 - INFO - __main__ -   Batch Number = 74
Evaluating:  45%|████▍     | 74/165 [00:09<00:11,  8.22it/s]01/09/2022 23:57:18 - INFO - __main__ -   Batch Number = 75
Evaluating:  45%|████▌     | 75/165 [00:09<00:10,  8.23it/s]01/09/2022 23:57:18 - INFO - __main__ -   Batch Number = 76
Evaluating:  46%|████▌     | 76/165 [00:09<00:10,  8.24it/s]01/09/2022 23:57:18 - INFO - __main__ -   Batch Number = 77
Evaluating:  47%|████▋     | 77/165 [00:09<00:10,  8.23it/s]01/09/2022 23:57:18 - INFO - __main__ -   Batch Number = 78
Evaluating:  47%|████▋     | 78/165 [00:09<00:10,  8.23it/s]01/09/2022 23:57:19 - INFO - __main__ -   Batch Number = 79
Evaluating:  48%|████▊     | 79/165 [00:09<00:10,  8.22it/s]01/09/2022 23:57:19 - INFO - __main__ -   Batch Number = 80
Evaluating:  48%|████▊     | 80/165 [00:09<00:10,  8.22it/s]01/09/2022 23:57:19 - INFO - __main__ -   Batch Number = 81
Evaluating:  49%|████▉     | 81/165 [00:10<00:10,  8.21it/s]01/09/2022 23:57:19 - INFO - __main__ -   Batch Number = 82
Evaluating:  50%|████▉     | 82/165 [00:10<00:10,  8.22it/s]01/09/2022 23:57:19 - INFO - __main__ -   Batch Number = 83
Evaluating:  50%|█████     | 83/165 [00:10<00:09,  8.22it/s]01/09/2022 23:57:19 - INFO - __main__ -   Batch Number = 84
Evaluating:  51%|█████     | 84/165 [00:10<00:09,  8.23it/s]01/09/2022 23:57:19 - INFO - __main__ -   Batch Number = 85
Evaluating:  52%|█████▏    | 85/165 [00:10<00:09,  8.23it/s]01/09/2022 23:57:19 - INFO - __main__ -   Batch Number = 86
Evaluating:  52%|█████▏    | 86/165 [00:10<00:09,  8.21it/s]01/09/2022 23:57:20 - INFO - __main__ -   Batch Number = 87
Evaluating:  53%|█████▎    | 87/165 [00:10<00:09,  8.22it/s]01/09/2022 23:57:20 - INFO - __main__ -   Batch Number = 88
Evaluating:  53%|█████▎    | 88/165 [00:10<00:09,  8.21it/s]01/09/2022 23:57:20 - INFO - __main__ -   Batch Number = 89
Evaluating:  54%|█████▍    | 89/165 [00:11<00:09,  8.21it/s]01/09/2022 23:57:20 - INFO - __main__ -   Batch Number = 90
Evaluating:  55%|█████▍    | 90/165 [00:11<00:09,  8.21it/s]01/09/2022 23:57:20 - INFO - __main__ -   Batch Number = 91
Evaluating:  55%|█████▌    | 91/165 [00:11<00:08,  8.23it/s]01/09/2022 23:57:20 - INFO - __main__ -   Batch Number = 92
Evaluating:  56%|█████▌    | 92/165 [00:11<00:08,  8.22it/s]01/09/2022 23:57:20 - INFO - __main__ -   Batch Number = 93
Evaluating:  56%|█████▋    | 93/165 [00:11<00:08,  8.22it/s]01/09/2022 23:57:20 - INFO - __main__ -   Batch Number = 94
Evaluating:  57%|█████▋    | 94/165 [00:11<00:08,  8.22it/s]01/09/2022 23:57:20 - INFO - __main__ -   Batch Number = 95
Evaluating:  58%|█████▊    | 95/165 [00:11<00:08,  8.21it/s]01/09/2022 23:57:21 - INFO - __main__ -   Batch Number = 96
Evaluating:  58%|█████▊    | 96/165 [00:11<00:08,  8.20it/s]01/09/2022 23:57:21 - INFO - __main__ -   Batch Number = 97
Evaluating:  59%|█████▉    | 97/165 [00:12<00:08,  8.21it/s]01/09/2022 23:57:21 - INFO - __main__ -   Batch Number = 98
Evaluating:  59%|█████▉    | 98/165 [00:12<00:08,  8.21it/s]01/09/2022 23:57:21 - INFO - __main__ -   Batch Number = 99
Evaluating:  60%|██████    | 99/165 [00:12<00:08,  8.20it/s]01/09/2022 23:57:21 - INFO - __main__ -   Batch Number = 100
Evaluating:  61%|██████    | 100/165 [00:12<00:07,  8.20it/s]01/09/2022 23:57:21 - INFO - __main__ -   Batch Number = 101
Evaluating:  61%|██████    | 101/165 [00:12<00:07,  8.19it/s]01/09/2022 23:57:21 - INFO - __main__ -   Batch Number = 102
Evaluating:  62%|██████▏   | 102/165 [00:12<00:07,  8.19it/s]01/09/2022 23:57:21 - INFO - __main__ -   Batch Number = 103
Evaluating:  62%|██████▏   | 103/165 [00:12<00:07,  8.17it/s]01/09/2022 23:57:22 - INFO - __main__ -   Batch Number = 104
Evaluating:  63%|██████▎   | 104/165 [00:12<00:07,  8.19it/s]01/09/2022 23:57:22 - INFO - __main__ -   Batch Number = 105
Evaluating:  64%|██████▎   | 105/165 [00:12<00:07,  8.20it/s]01/09/2022 23:57:22 - INFO - __main__ -   Batch Number = 106
Evaluating:  64%|██████▍   | 106/165 [00:13<00:07,  8.20it/s]01/09/2022 23:57:22 - INFO - __main__ -   Batch Number = 107
Evaluating:  65%|██████▍   | 107/165 [00:13<00:07,  7.96it/s]01/09/2022 23:57:22 - INFO - __main__ -   Batch Number = 108
Evaluating:  65%|██████▌   | 108/165 [00:13<00:07,  8.04it/s]01/09/2022 23:57:22 - INFO - __main__ -   Batch Number = 109
Evaluating:  66%|██████▌   | 109/165 [00:13<00:06,  8.10it/s]01/09/2022 23:57:22 - INFO - __main__ -   Batch Number = 110
Evaluating:  67%|██████▋   | 110/165 [00:13<00:06,  8.14it/s]01/09/2022 23:57:22 - INFO - __main__ -   Batch Number = 111
Evaluating:  67%|██████▋   | 111/165 [00:13<00:06,  8.18it/s]01/09/2022 23:57:23 - INFO - __main__ -   Batch Number = 112
Evaluating:  68%|██████▊   | 112/165 [00:13<00:06,  8.20it/s]01/09/2022 23:57:23 - INFO - __main__ -   Batch Number = 113
Evaluating:  68%|██████▊   | 113/165 [00:13<00:06,  8.20it/s]01/09/2022 23:57:23 - INFO - __main__ -   Batch Number = 114
Evaluating:  69%|██████▉   | 114/165 [00:14<00:06,  8.20it/s]01/09/2022 23:57:23 - INFO - __main__ -   Batch Number = 115
Evaluating:  70%|██████▉   | 115/165 [00:14<00:06,  8.21it/s]01/09/2022 23:57:23 - INFO - __main__ -   Batch Number = 116
Evaluating:  70%|███████   | 116/165 [00:14<00:05,  8.21it/s]01/09/2022 23:57:23 - INFO - __main__ -   Batch Number = 117
Evaluating:  71%|███████   | 117/165 [00:14<00:05,  8.22it/s]01/09/2022 23:57:23 - INFO - __main__ -   Batch Number = 118
Evaluating:  72%|███████▏  | 118/165 [00:14<00:05,  8.23it/s]01/09/2022 23:57:23 - INFO - __main__ -   Batch Number = 119
Evaluating:  72%|███████▏  | 119/165 [00:14<00:05,  8.22it/s]01/09/2022 23:57:24 - INFO - __main__ -   Batch Number = 120
Evaluating:  73%|███████▎  | 120/165 [00:14<00:05,  8.21it/s]01/09/2022 23:57:24 - INFO - __main__ -   Batch Number = 121
Evaluating:  73%|███████▎  | 121/165 [00:14<00:05,  8.20it/s]01/09/2022 23:57:24 - INFO - __main__ -   Batch Number = 122
Evaluating:  74%|███████▍  | 122/165 [00:15<00:05,  8.20it/s]01/09/2022 23:57:24 - INFO - __main__ -   Batch Number = 123
Evaluating:  75%|███████▍  | 123/165 [00:15<00:05,  8.22it/s]01/09/2022 23:57:24 - INFO - __main__ -   Batch Number = 124
Evaluating:  75%|███████▌  | 124/165 [00:15<00:04,  8.22it/s]01/09/2022 23:57:24 - INFO - __main__ -   Batch Number = 125
Evaluating:  76%|███████▌  | 125/165 [00:15<00:04,  8.22it/s]01/09/2022 23:57:24 - INFO - __main__ -   Batch Number = 126
Evaluating:  76%|███████▋  | 126/165 [00:15<00:04,  8.21it/s]01/09/2022 23:57:24 - INFO - __main__ -   Batch Number = 127
Evaluating:  77%|███████▋  | 127/165 [00:15<00:04,  8.21it/s]01/09/2022 23:57:25 - INFO - __main__ -   Batch Number = 128
Evaluating:  78%|███████▊  | 128/165 [00:15<00:04,  8.20it/s]01/09/2022 23:57:25 - INFO - __main__ -   Batch Number = 129
Evaluating:  78%|███████▊  | 129/165 [00:15<00:04,  8.20it/s]01/09/2022 23:57:25 - INFO - __main__ -   Batch Number = 130
Evaluating:  79%|███████▉  | 130/165 [00:16<00:04,  8.20it/s]01/09/2022 23:57:25 - INFO - __main__ -   Batch Number = 131
Evaluating:  79%|███████▉  | 131/165 [00:16<00:04,  8.18it/s]01/09/2022 23:57:25 - INFO - __main__ -   Batch Number = 132
Evaluating:  80%|████████  | 132/165 [00:16<00:04,  8.18it/s]01/09/2022 23:57:25 - INFO - __main__ -   Batch Number = 133
Evaluating:  81%|████████  | 133/165 [00:16<00:03,  8.19it/s]01/09/2022 23:57:25 - INFO - __main__ -   Batch Number = 134
Evaluating:  81%|████████  | 134/165 [00:16<00:03,  8.19it/s]01/09/2022 23:57:25 - INFO - __main__ -   Batch Number = 135
Evaluating:  82%|████████▏ | 135/165 [00:16<00:03,  8.20it/s]01/09/2022 23:57:26 - INFO - __main__ -   Batch Number = 136
Evaluating:  82%|████████▏ | 136/165 [00:16<00:03,  8.21it/s]01/09/2022 23:57:26 - INFO - __main__ -   Batch Number = 137
Evaluating:  83%|████████▎ | 137/165 [00:16<00:03,  8.21it/s]01/09/2022 23:57:26 - INFO - __main__ -   Batch Number = 138
Evaluating:  84%|████████▎ | 138/165 [00:17<00:03,  8.20it/s]01/09/2022 23:57:26 - INFO - __main__ -   Batch Number = 139
Evaluating:  84%|████████▍ | 139/165 [00:17<00:03,  8.20it/s]01/09/2022 23:57:26 - INFO - __main__ -   Batch Number = 140
Evaluating:  85%|████████▍ | 140/165 [00:17<00:03,  8.19it/s]01/09/2022 23:57:26 - INFO - __main__ -   Batch Number = 141
Evaluating:  85%|████████▌ | 141/165 [00:17<00:02,  8.14it/s]01/09/2022 23:57:26 - INFO - __main__ -   Batch Number = 142
Evaluating:  86%|████████▌ | 142/165 [00:17<00:02,  8.15it/s]01/09/2022 23:57:26 - INFO - __main__ -   Batch Number = 143
Evaluating:  87%|████████▋ | 143/165 [00:17<00:02,  8.17it/s]01/09/2022 23:57:26 - INFO - __main__ -   Batch Number = 144
Evaluating:  87%|████████▋ | 144/165 [00:17<00:02,  8.19it/s]01/09/2022 23:57:27 - INFO - __main__ -   Batch Number = 145
Evaluating:  88%|████████▊ | 145/165 [00:17<00:02,  8.19it/s]01/09/2022 23:57:27 - INFO - __main__ -   Batch Number = 146
Evaluating:  88%|████████▊ | 146/165 [00:18<00:02,  8.18it/s]01/09/2022 23:57:27 - INFO - __main__ -   Batch Number = 147
Evaluating:  89%|████████▉ | 147/165 [00:18<00:02,  8.20it/s]01/09/2022 23:57:27 - INFO - __main__ -   Batch Number = 148
Evaluating:  90%|████████▉ | 148/165 [00:18<00:02,  8.21it/s]01/09/2022 23:57:27 - INFO - __main__ -   Batch Number = 149
Evaluating:  90%|█████████ | 149/165 [00:18<00:01,  8.20it/s]01/09/2022 23:57:27 - INFO - __main__ -   Batch Number = 150
Evaluating:  91%|█████████ | 150/165 [00:18<00:01,  8.20it/s]01/09/2022 23:57:27 - INFO - __main__ -   Batch Number = 151
Evaluating:  92%|█████████▏| 151/165 [00:18<00:01,  8.20it/s]01/09/2022 23:57:27 - INFO - __main__ -   Batch Number = 152
Evaluating:  92%|█████████▏| 152/165 [00:18<00:01,  8.20it/s]01/09/2022 23:57:28 - INFO - __main__ -   Batch Number = 153
Evaluating:  93%|█████████▎| 153/165 [00:18<00:01,  8.20it/s]01/09/2022 23:57:28 - INFO - __main__ -   Batch Number = 154
Evaluating:  93%|█████████▎| 154/165 [00:18<00:01,  8.20it/s]01/09/2022 23:57:28 - INFO - __main__ -   Batch Number = 155
Evaluating:  94%|█████████▍| 155/165 [00:19<00:01,  8.21it/s]01/09/2022 23:57:28 - INFO - __main__ -   Batch Number = 156
Evaluating:  95%|█████████▍| 156/165 [00:19<00:01,  8.20it/s]01/09/2022 23:57:28 - INFO - __main__ -   Batch Number = 157
Evaluating:  95%|█████████▌| 157/165 [00:19<00:00,  8.20it/s]01/09/2022 23:57:28 - INFO - __main__ -   Batch Number = 158
Evaluating:  96%|█████████▌| 158/165 [00:19<00:00,  8.20it/s]01/09/2022 23:57:28 - INFO - __main__ -   Batch Number = 159
Evaluating:  96%|█████████▋| 159/165 [00:19<00:00,  8.20it/s]01/09/2022 23:57:28 - INFO - __main__ -   Batch Number = 160
Evaluating:  97%|█████████▋| 160/165 [00:19<00:00,  8.19it/s]01/09/2022 23:57:29 - INFO - __main__ -   Batch Number = 161
Evaluating:  98%|█████████▊| 161/165 [00:19<00:00,  8.18it/s]01/09/2022 23:57:29 - INFO - __main__ -   Batch Number = 162
Evaluating:  98%|█████████▊| 162/165 [00:19<00:00,  8.18it/s]01/09/2022 23:57:29 - INFO - __main__ -   Batch Number = 163
Evaluating:  99%|█████████▉| 163/165 [00:20<00:00,  8.19it/s]01/09/2022 23:57:29 - INFO - __main__ -   Batch Number = 164
Evaluating:  99%|█████████▉| 164/165 [00:20<00:00,  8.17it/s]01/09/2022 23:57:29 - INFO - __main__ -   Batch Number = 165
Evaluating: 100%|██████████| 165/165 [00:20<00:00,  8.13it/s]
01/09/2022 23:57:29 - INFO - __main__ -     Evaluation done in total 20.293698 secs (0.015397 sec per example)
Writing predictions to: /home/abhijeet/rohan/cloud-emea-copy/outputs//squad/my-xlm-roberta-base-MaxLen384_qna_ar/predictions_ar_.json
Writing nbest to: /home/abhijeet/rohan/cloud-emea-copy/outputs//squad/my-xlm-roberta-base-MaxLen384_qna_ar/nbest_predictions_ar_.json
01/09/2022 23:57:33 - INFO - __main__ -   Results = OrderedDict([('exact', 50.924369747899156), ('f1', 66.74959600200837), ('precision', 69.14877568620379), ('recall', 69.52492593774105), ('total', 1190), ('HasAns_exact', 50.924369747899156), ('HasAns_f1', 66.74959600200837), ('HasAns_total', 1190), ('best_exact', 50.924369747899156), ('best_exact_thresh', 0.0), ('best_f1', 66.74959600200837), ('best_f1_thresh', 0.0)])
PyTorch version 1.10.1+cu102 available.
01/09/2022 23:57:35 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/09/2022 23:57:35 - INFO - root -   save model
loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6
Model config XLMRobertaConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /home/abhijeet/.cache/torch/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8
loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/09/2022 23:57:45 - INFO - __main__ -   lang2id = None
01/09/2022 23:57:48 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='xlm-roberta-base', model_type='xlm-roberta', config_name=None, tokenizer_name=None, cache_dir=None, data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//xquad', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//squad/my-xlm-roberta-base-MaxLen384_qna_ar/', max_seq_length=384, train_file='/home/abhijeet/rohan/cloud-emea-copy/data//xquad/train-v1.1.json', predict_file='/home/abhijeet/rohan/cloud-emea-copy/data//xquad/dev-v1.1.json', eval_test_set=False, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=4, per_gpu_eval_batch_size=8, gradient_accumulation_steps=4, learning_rate=0.0001, weight_decay=0.0001, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=15.0, max_steps=-1, save_steps=1000, warmup_steps=500, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', eval_lang='en', predict_langs='ar', train_lang='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//squad/my-xlm-roberta-base-MaxLen384_qna_ar//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='qna', threads=8, version_2_with_negative=False, verbose_logging=False, n_best_size=20, max_query_length=64, max_answer_length=30, doc_stride=128, null_score_diff_threshold=0.0, predict_task_adapter='output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s3/checkpoint-best/qna', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix=None, en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/09/2022 23:57:48 - INFO - __main__ -   Results: {}
loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6
Model config XLMRobertaConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /home/abhijeet/.cache/torch/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8
loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/09/2022 23:57:58 - INFO - __main__ -   lang2id = None
01/09/2022 23:57:59 - INFO - __main__ -   Evaluating the model on the test set of all languages specified
01/09/2022 23:57:59 - INFO - __main__ -   Adapter will be loaded from this path: output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s3/checkpoint-best/qna
01/09/2022 23:57:59 - INFO - root -   Trying to decide if add adapter
01/09/2022 23:57:59 - INFO - root -   loading task adapter
Loading module configuration from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s3/checkpoint-best/qna/adapter_config.json
Adding adapter 'qna' of type 'text_task'.
Loading module weights from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s3/checkpoint-best/qna/pytorch_adapter.bin
Loading module configuration from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s3/checkpoint-best/qna/head_config.json
Loading module weights from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s3/checkpoint-best/qna/pytorch_model_head.bin
01/09/2022 23:57:59 - INFO - root -   loading lang adpater ar/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/ar/xlm-roberta-base/pfeiffer/ar_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1f4018c6b187912fb6b47e664a34880506aef8c7320e0135939626a97e0bb137-4b1153cd3ebfdce09e1023c01e3ee723c32a0a57bea16e5ff13609f97eb2af0b-extracted/adapter_config.json
Adding adapter 'ar' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/1f4018c6b187912fb6b47e664a34880506aef8c7320e0135939626a97e0bb137-4b1153cd3ebfdce09e1023c01e3ee723c32a0a57bea16e5ff13609f97eb2af0b-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/1f4018c6b187912fb6b47e664a34880506aef8c7320e0135939626a97e0bb137-4b1153cd3ebfdce09e1023c01e3ee723c32a0a57bea16e5ff13609f97eb2af0b-extracted'
01/09/2022 23:58:00 - INFO - __main__ -   Language adapter for ar found
01/09/2022 23:58:00 - INFO - __main__ -   Set active language adapter to ar
01/09/2022 23:58:00 - INFO - __main__ -   Args Adapter Weight = None
01/09/2022 23:58:00 - INFO - __main__ -   Adapter Languages = ['ar']
01/09/2022 23:58:00 - INFO - __main__ -   Predict File = xquad.ar.json
01/09/2022 23:58:00 - INFO - __main__ -   Creating features from dataset file at /home/abhijeet/rohan/cloud-emea-copy/data//xquad
ar ar/wiki@ukp
  0%|          | 0/48 [00:00<?, ?it/s] 33%|███▎      | 16/48 [00:00<00:00, 131.07it/s] 62%|██████▎   | 30/48 [00:00<00:00, 123.67it/s] 94%|█████████▍| 45/48 [00:00<00:00, 134.29it/s]100%|██████████| 48/48 [00:00<00:00, 134.35it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<04:33,  4.35it/s]convert squad examples to features:  32%|███▏      | 385/1190 [00:00<00:01, 559.83it/s]convert squad examples to features:  94%|█████████▍| 1121/1190 [00:00<00:00, 1669.78it/s]convert squad examples to features: 100%|██████████| 1190/1190 [00:00<00:00, 1311.74it/s]/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  FutureWarning,
/home/abhijeet/rohan/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|██████████| 1190/1190 [00:00<00:00, 348841.33it/s]
01/09/2022 23:58:02 - INFO - __main__ -   Local Rank = -1
01/09/2022 23:58:02 - INFO - __main__ -   Saving features into cached file /home/abhijeet/rohan/cloud-emea-copy/data//xquad/cached_xquad.ar.json_xlm-roberta-base_384_ar
01/09/2022 23:58:04 - INFO - __main__ -   ***** Running evaluation  *****
01/09/2022 23:58:04 - INFO - __main__ -     Num examples = 1318
01/09/2022 23:58:04 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/165 [00:00<?, ?it/s]01/09/2022 23:58:04 - INFO - __main__ -   Batch Number = 1
Evaluating:   1%|          | 1/165 [00:00<00:23,  6.85it/s]01/09/2022 23:58:04 - INFO - __main__ -   Batch Number = 2
Evaluating:   1%|          | 2/165 [00:00<00:21,  7.44it/s]01/09/2022 23:58:04 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|▏         | 3/165 [00:00<00:21,  7.71it/s]01/09/2022 23:58:04 - INFO - __main__ -   Batch Number = 4
Evaluating:   2%|▏         | 4/165 [00:00<00:20,  7.89it/s]01/09/2022 23:58:04 - INFO - __main__ -   Batch Number = 5
Evaluating:   3%|▎         | 5/165 [00:00<00:19,  8.01it/s]01/09/2022 23:58:04 - INFO - __main__ -   Batch Number = 6
Evaluating:   4%|▎         | 6/165 [00:00<00:19,  8.10it/s]01/09/2022 23:58:04 - INFO - __main__ -   Batch Number = 7
Evaluating:   4%|▍         | 7/165 [00:00<00:19,  8.16it/s]01/09/2022 23:58:05 - INFO - __main__ -   Batch Number = 8
Evaluating:   5%|▍         | 8/165 [00:01<00:19,  8.20it/s]01/09/2022 23:58:05 - INFO - __main__ -   Batch Number = 9
Evaluating:   5%|▌         | 9/165 [00:01<00:18,  8.22it/s]01/09/2022 23:58:05 - INFO - __main__ -   Batch Number = 10
Evaluating:   6%|▌         | 10/165 [00:01<00:18,  8.23it/s]01/09/2022 23:58:05 - INFO - __main__ -   Batch Number = 11
Evaluating:   7%|▋         | 11/165 [00:01<00:18,  8.23it/s]01/09/2022 23:58:05 - INFO - __main__ -   Batch Number = 12
Evaluating:   7%|▋         | 12/165 [00:01<00:18,  8.24it/s]01/09/2022 23:58:05 - INFO - __main__ -   Batch Number = 13
Evaluating:   8%|▊         | 13/165 [00:01<00:29,  5.17it/s]01/09/2022 23:58:06 - INFO - __main__ -   Batch Number = 14
Evaluating:   8%|▊         | 14/165 [00:01<00:25,  5.83it/s]01/09/2022 23:58:06 - INFO - __main__ -   Batch Number = 15
Evaluating:   9%|▉         | 15/165 [00:02<00:23,  6.39it/s]01/09/2022 23:58:06 - INFO - __main__ -   Batch Number = 16
Evaluating:  10%|▉         | 16/165 [00:02<00:21,  6.86it/s]01/09/2022 23:58:06 - INFO - __main__ -   Batch Number = 17
Evaluating:  10%|█         | 17/165 [00:02<00:20,  7.23it/s]01/09/2022 23:58:06 - INFO - __main__ -   Batch Number = 18
Evaluating:  11%|█         | 18/165 [00:02<00:19,  7.52it/s]01/09/2022 23:58:06 - INFO - __main__ -   Batch Number = 19
Evaluating:  12%|█▏        | 19/165 [00:02<00:18,  7.71it/s]01/09/2022 23:58:06 - INFO - __main__ -   Batch Number = 20
Evaluating:  12%|█▏        | 20/165 [00:02<00:18,  7.84it/s]01/09/2022 23:58:06 - INFO - __main__ -   Batch Number = 21
Evaluating:  13%|█▎        | 21/165 [00:02<00:18,  7.97it/s]01/09/2022 23:58:06 - INFO - __main__ -   Batch Number = 22
Evaluating:  13%|█▎        | 22/165 [00:02<00:17,  8.05it/s]01/09/2022 23:58:07 - INFO - __main__ -   Batch Number = 23
Evaluating:  14%|█▍        | 23/165 [00:03<00:17,  8.11it/s]01/09/2022 23:58:07 - INFO - __main__ -   Batch Number = 24
Evaluating:  15%|█▍        | 24/165 [00:03<00:17,  8.14it/s]01/09/2022 23:58:07 - INFO - __main__ -   Batch Number = 25
Evaluating:  15%|█▌        | 25/165 [00:03<00:17,  8.17it/s]01/09/2022 23:58:07 - INFO - __main__ -   Batch Number = 26
Evaluating:  16%|█▌        | 26/165 [00:03<00:17,  7.88it/s]01/09/2022 23:58:07 - INFO - __main__ -   Batch Number = 27
Evaluating:  16%|█▋        | 27/165 [00:03<00:17,  7.98it/s]01/09/2022 23:58:07 - INFO - __main__ -   Batch Number = 28
Evaluating:  17%|█▋        | 28/165 [00:03<00:17,  8.05it/s]01/09/2022 23:58:07 - INFO - __main__ -   Batch Number = 29
Evaluating:  18%|█▊        | 29/165 [00:03<00:16,  8.10it/s]01/09/2022 23:58:07 - INFO - __main__ -   Batch Number = 30
Evaluating:  18%|█▊        | 30/165 [00:03<00:16,  8.13it/s]01/09/2022 23:58:08 - INFO - __main__ -   Batch Number = 31
Evaluating:  19%|█▉        | 31/165 [00:04<00:16,  8.17it/s]01/09/2022 23:58:08 - INFO - __main__ -   Batch Number = 32
Evaluating:  19%|█▉        | 32/165 [00:04<00:16,  8.20it/s]01/09/2022 23:58:08 - INFO - __main__ -   Batch Number = 33
Evaluating:  20%|██        | 33/165 [00:04<00:16,  8.22it/s]01/09/2022 23:58:08 - INFO - __main__ -   Batch Number = 34
Evaluating:  21%|██        | 34/165 [00:04<00:15,  8.22it/s]01/09/2022 23:58:08 - INFO - __main__ -   Batch Number = 35
Evaluating:  21%|██        | 35/165 [00:04<00:15,  8.22it/s]01/09/2022 23:58:08 - INFO - __main__ -   Batch Number = 36
Evaluating:  22%|██▏       | 36/165 [00:04<00:15,  8.24it/s]01/09/2022 23:58:08 - INFO - __main__ -   Batch Number = 37
Evaluating:  22%|██▏       | 37/165 [00:04<00:15,  8.25it/s]01/09/2022 23:58:08 - INFO - __main__ -   Batch Number = 38
Evaluating:  23%|██▎       | 38/165 [00:04<00:15,  8.26it/s]01/09/2022 23:58:09 - INFO - __main__ -   Batch Number = 39
Evaluating:  24%|██▎       | 39/165 [00:05<00:15,  8.27it/s]01/09/2022 23:58:09 - INFO - __main__ -   Batch Number = 40
Evaluating:  24%|██▍       | 40/165 [00:05<00:15,  8.27it/s]01/09/2022 23:58:09 - INFO - __main__ -   Batch Number = 41
Evaluating:  25%|██▍       | 41/165 [00:05<00:15,  8.25it/s]01/09/2022 23:58:09 - INFO - __main__ -   Batch Number = 42
Evaluating:  25%|██▌       | 42/165 [00:05<00:14,  8.26it/s]01/09/2022 23:58:09 - INFO - __main__ -   Batch Number = 43
Evaluating:  26%|██▌       | 43/165 [00:05<00:14,  8.26it/s]01/09/2022 23:58:09 - INFO - __main__ -   Batch Number = 44
Evaluating:  27%|██▋       | 44/165 [00:05<00:14,  8.26it/s]01/09/2022 23:58:09 - INFO - __main__ -   Batch Number = 45
Evaluating:  27%|██▋       | 45/165 [00:05<00:14,  8.28it/s]01/09/2022 23:58:09 - INFO - __main__ -   Batch Number = 46
Evaluating:  28%|██▊       | 46/165 [00:05<00:14,  8.29it/s]01/09/2022 23:58:10 - INFO - __main__ -   Batch Number = 47
Evaluating:  28%|██▊       | 47/165 [00:05<00:14,  8.29it/s]01/09/2022 23:58:10 - INFO - __main__ -   Batch Number = 48
Evaluating:  29%|██▉       | 48/165 [00:06<00:14,  8.29it/s]01/09/2022 23:58:10 - INFO - __main__ -   Batch Number = 49
Evaluating:  30%|██▉       | 49/165 [00:06<00:14,  8.28it/s]01/09/2022 23:58:10 - INFO - __main__ -   Batch Number = 50
Evaluating:  30%|███       | 50/165 [00:06<00:13,  8.28it/s]01/09/2022 23:58:10 - INFO - __main__ -   Batch Number = 51
Evaluating:  31%|███       | 51/165 [00:06<00:13,  8.28it/s]01/09/2022 23:58:10 - INFO - __main__ -   Batch Number = 52
Evaluating:  32%|███▏      | 52/165 [00:06<00:13,  8.28it/s]01/09/2022 23:58:10 - INFO - __main__ -   Batch Number = 53
Evaluating:  32%|███▏      | 53/165 [00:06<00:13,  8.27it/s]01/09/2022 23:58:10 - INFO - __main__ -   Batch Number = 54
Evaluating:  33%|███▎      | 54/165 [00:06<00:13,  8.26it/s]01/09/2022 23:58:10 - INFO - __main__ -   Batch Number = 55
Evaluating:  33%|███▎      | 55/165 [00:06<00:13,  8.25it/s]01/09/2022 23:58:11 - INFO - __main__ -   Batch Number = 56
Evaluating:  34%|███▍      | 56/165 [00:07<00:13,  8.24it/s]01/09/2022 23:58:11 - INFO - __main__ -   Batch Number = 57
Evaluating:  35%|███▍      | 57/165 [00:07<00:13,  8.24it/s]01/09/2022 23:58:11 - INFO - __main__ -   Batch Number = 58
Evaluating:  35%|███▌      | 58/165 [00:07<00:12,  8.25it/s]01/09/2022 23:58:11 - INFO - __main__ -   Batch Number = 59
Evaluating:  36%|███▌      | 59/165 [00:07<00:12,  8.25it/s]01/09/2022 23:58:11 - INFO - __main__ -   Batch Number = 60
Evaluating:  36%|███▋      | 60/165 [00:07<00:12,  8.26it/s]01/09/2022 23:58:11 - INFO - __main__ -   Batch Number = 61
Evaluating:  37%|███▋      | 61/165 [00:07<00:12,  8.27it/s]01/09/2022 23:58:11 - INFO - __main__ -   Batch Number = 62
Evaluating:  38%|███▊      | 62/165 [00:07<00:12,  8.26it/s]01/09/2022 23:58:11 - INFO - __main__ -   Batch Number = 63
Evaluating:  38%|███▊      | 63/165 [00:07<00:12,  8.25it/s]01/09/2022 23:58:12 - INFO - __main__ -   Batch Number = 64
Evaluating:  39%|███▉      | 64/165 [00:08<00:12,  8.25it/s]01/09/2022 23:58:12 - INFO - __main__ -   Batch Number = 65
Evaluating:  39%|███▉      | 65/165 [00:08<00:12,  8.25it/s]01/09/2022 23:58:12 - INFO - __main__ -   Batch Number = 66
Evaluating:  40%|████      | 66/165 [00:08<00:12,  8.24it/s]01/09/2022 23:58:12 - INFO - __main__ -   Batch Number = 67
Evaluating:  41%|████      | 67/165 [00:08<00:12,  7.68it/s]01/09/2022 23:58:12 - INFO - __main__ -   Batch Number = 68
Evaluating:  41%|████      | 68/165 [00:08<00:12,  7.86it/s]01/09/2022 23:58:12 - INFO - __main__ -   Batch Number = 69
Evaluating:  42%|████▏     | 69/165 [00:08<00:12,  7.98it/s]01/09/2022 23:58:12 - INFO - __main__ -   Batch Number = 70
Evaluating:  42%|████▏     | 70/165 [00:08<00:11,  8.05it/s]01/09/2022 23:58:12 - INFO - __main__ -   Batch Number = 71
Evaluating:  43%|████▎     | 71/165 [00:08<00:11,  8.04it/s]01/09/2022 23:58:13 - INFO - __main__ -   Batch Number = 72
Evaluating:  44%|████▎     | 72/165 [00:09<00:11,  8.09it/s]01/09/2022 23:58:13 - INFO - __main__ -   Batch Number = 73
Evaluating:  44%|████▍     | 73/165 [00:09<00:11,  8.12it/s]01/09/2022 23:58:13 - INFO - __main__ -   Batch Number = 74
Evaluating:  45%|████▍     | 74/165 [00:09<00:11,  8.16it/s]01/09/2022 23:58:13 - INFO - __main__ -   Batch Number = 75
Evaluating:  45%|████▌     | 75/165 [00:09<00:10,  8.19it/s]01/09/2022 23:58:13 - INFO - __main__ -   Batch Number = 76
Evaluating:  46%|████▌     | 76/165 [00:09<00:10,  8.22it/s]01/09/2022 23:58:13 - INFO - __main__ -   Batch Number = 77
Evaluating:  47%|████▋     | 77/165 [00:09<00:10,  8.22it/s]01/09/2022 23:58:13 - INFO - __main__ -   Batch Number = 78
Evaluating:  47%|████▋     | 78/165 [00:09<00:10,  8.22it/s]01/09/2022 23:58:13 - INFO - __main__ -   Batch Number = 79
Evaluating:  48%|████▊     | 79/165 [00:09<00:10,  8.23it/s]01/09/2022 23:58:14 - INFO - __main__ -   Batch Number = 80
Evaluating:  48%|████▊     | 80/165 [00:10<00:10,  8.23it/s]01/09/2022 23:58:14 - INFO - __main__ -   Batch Number = 81
Evaluating:  49%|████▉     | 81/165 [00:10<00:10,  8.23it/s]01/09/2022 23:58:14 - INFO - __main__ -   Batch Number = 82
Evaluating:  50%|████▉     | 82/165 [00:10<00:10,  8.25it/s]01/09/2022 23:58:14 - INFO - __main__ -   Batch Number = 83
Evaluating:  50%|█████     | 83/165 [00:10<00:09,  8.26it/s]01/09/2022 23:58:14 - INFO - __main__ -   Batch Number = 84
Evaluating:  51%|█████     | 84/165 [00:10<00:09,  8.25it/s]01/09/2022 23:58:14 - INFO - __main__ -   Batch Number = 85
Evaluating:  52%|█████▏    | 85/165 [00:10<00:09,  8.26it/s]01/09/2022 23:58:14 - INFO - __main__ -   Batch Number = 86
Evaluating:  52%|█████▏    | 86/165 [00:10<00:09,  8.21it/s]01/09/2022 23:58:14 - INFO - __main__ -   Batch Number = 87
Evaluating:  53%|█████▎    | 87/165 [00:10<00:09,  8.21it/s]01/09/2022 23:58:15 - INFO - __main__ -   Batch Number = 88
Evaluating:  53%|█████▎    | 88/165 [00:10<00:09,  8.20it/s]01/09/2022 23:58:15 - INFO - __main__ -   Batch Number = 89
Evaluating:  54%|█████▍    | 89/165 [00:11<00:09,  8.18it/s]01/09/2022 23:58:15 - INFO - __main__ -   Batch Number = 90
Evaluating:  55%|█████▍    | 90/165 [00:11<00:09,  8.18it/s]01/09/2022 23:58:15 - INFO - __main__ -   Batch Number = 91
Evaluating:  55%|█████▌    | 91/165 [00:11<00:09,  8.19it/s]01/09/2022 23:58:15 - INFO - __main__ -   Batch Number = 92
Evaluating:  56%|█████▌    | 92/165 [00:11<00:08,  8.19it/s]01/09/2022 23:58:15 - INFO - __main__ -   Batch Number = 93
Evaluating:  56%|█████▋    | 93/165 [00:11<00:08,  8.19it/s]01/09/2022 23:58:15 - INFO - __main__ -   Batch Number = 94
Evaluating:  57%|█████▋    | 94/165 [00:11<00:08,  8.20it/s]01/09/2022 23:58:15 - INFO - __main__ -   Batch Number = 95
Evaluating:  58%|█████▊    | 95/165 [00:11<00:08,  8.21it/s]01/09/2022 23:58:16 - INFO - __main__ -   Batch Number = 96
Evaluating:  58%|█████▊    | 96/165 [00:11<00:08,  8.21it/s]01/09/2022 23:58:16 - INFO - __main__ -   Batch Number = 97
Evaluating:  59%|█████▉    | 97/165 [00:12<00:08,  8.22it/s]01/09/2022 23:58:16 - INFO - __main__ -   Batch Number = 98
Evaluating:  59%|█████▉    | 98/165 [00:12<00:08,  8.22it/s]01/09/2022 23:58:16 - INFO - __main__ -   Batch Number = 99
Evaluating:  60%|██████    | 99/165 [00:12<00:08,  8.22it/s]01/09/2022 23:58:16 - INFO - __main__ -   Batch Number = 100
Evaluating:  61%|██████    | 100/165 [00:12<00:07,  8.21it/s]01/09/2022 23:58:16 - INFO - __main__ -   Batch Number = 101
Evaluating:  61%|██████    | 101/165 [00:12<00:07,  8.20it/s]01/09/2022 23:58:16 - INFO - __main__ -   Batch Number = 102
Evaluating:  62%|██████▏   | 102/165 [00:12<00:07,  8.22it/s]01/09/2022 23:58:16 - INFO - __main__ -   Batch Number = 103
Evaluating:  62%|██████▏   | 103/165 [00:12<00:07,  8.22it/s]01/09/2022 23:58:16 - INFO - __main__ -   Batch Number = 104
Evaluating:  63%|██████▎   | 104/165 [00:12<00:07,  8.22it/s]01/09/2022 23:58:17 - INFO - __main__ -   Batch Number = 105
Evaluating:  64%|██████▎   | 105/165 [00:13<00:07,  8.22it/s]01/09/2022 23:58:17 - INFO - __main__ -   Batch Number = 106
Evaluating:  64%|██████▍   | 106/165 [00:13<00:07,  8.22it/s]01/09/2022 23:58:17 - INFO - __main__ -   Batch Number = 107
Evaluating:  65%|██████▍   | 107/165 [00:13<00:07,  8.21it/s]01/09/2022 23:58:17 - INFO - __main__ -   Batch Number = 108
Evaluating:  65%|██████▌   | 108/165 [00:13<00:07,  7.35it/s]01/09/2022 23:58:17 - INFO - __main__ -   Batch Number = 109
Evaluating:  66%|██████▌   | 109/165 [00:13<00:07,  7.59it/s]01/09/2022 23:58:17 - INFO - __main__ -   Batch Number = 110
Evaluating:  67%|██████▋   | 110/165 [00:13<00:07,  7.77it/s]01/09/2022 23:58:17 - INFO - __main__ -   Batch Number = 111
Evaluating:  67%|██████▋   | 111/165 [00:13<00:06,  7.91it/s]01/09/2022 23:58:18 - INFO - __main__ -   Batch Number = 112
Evaluating:  68%|██████▊   | 112/165 [00:13<00:06,  8.01it/s]01/09/2022 23:58:18 - INFO - __main__ -   Batch Number = 113
Evaluating:  68%|██████▊   | 113/165 [00:14<00:06,  8.08it/s]01/09/2022 23:58:18 - INFO - __main__ -   Batch Number = 114
Evaluating:  69%|██████▉   | 114/165 [00:14<00:06,  8.12it/s]01/09/2022 23:58:18 - INFO - __main__ -   Batch Number = 115
Evaluating:  70%|██████▉   | 115/165 [00:14<00:06,  8.12it/s]01/09/2022 23:58:18 - INFO - __main__ -   Batch Number = 116
Evaluating:  70%|███████   | 116/165 [00:14<00:06,  8.12it/s]01/09/2022 23:58:18 - INFO - __main__ -   Batch Number = 117
Evaluating:  71%|███████   | 117/165 [00:14<00:05,  8.15it/s]01/09/2022 23:58:18 - INFO - __main__ -   Batch Number = 118
Evaluating:  72%|███████▏  | 118/165 [00:14<00:05,  8.17it/s]01/09/2022 23:58:18 - INFO - __main__ -   Batch Number = 119
Evaluating:  72%|███████▏  | 119/165 [00:14<00:05,  8.19it/s]01/09/2022 23:58:18 - INFO - __main__ -   Batch Number = 120
Evaluating:  73%|███████▎  | 120/165 [00:14<00:05,  8.19it/s]01/09/2022 23:58:19 - INFO - __main__ -   Batch Number = 121
Evaluating:  73%|███████▎  | 121/165 [00:15<00:05,  8.20it/s]01/09/2022 23:58:19 - INFO - __main__ -   Batch Number = 122
Evaluating:  74%|███████▍  | 122/165 [00:15<00:05,  8.20it/s]01/09/2022 23:58:19 - INFO - __main__ -   Batch Number = 123
Evaluating:  75%|███████▍  | 123/165 [00:15<00:05,  8.21it/s]01/09/2022 23:58:19 - INFO - __main__ -   Batch Number = 124
Evaluating:  75%|███████▌  | 124/165 [00:15<00:04,  8.21it/s]01/09/2022 23:58:19 - INFO - __main__ -   Batch Number = 125
Evaluating:  76%|███████▌  | 125/165 [00:15<00:04,  8.21it/s]01/09/2022 23:58:19 - INFO - __main__ -   Batch Number = 126
Evaluating:  76%|███████▋  | 126/165 [00:15<00:04,  8.21it/s]01/09/2022 23:58:19 - INFO - __main__ -   Batch Number = 127
Evaluating:  77%|███████▋  | 127/165 [00:15<00:04,  8.22it/s]01/09/2022 23:58:19 - INFO - __main__ -   Batch Number = 128
Evaluating:  78%|███████▊  | 128/165 [00:15<00:04,  8.22it/s]01/09/2022 23:58:20 - INFO - __main__ -   Batch Number = 129
Evaluating:  78%|███████▊  | 129/165 [00:16<00:04,  8.22it/s]01/09/2022 23:58:20 - INFO - __main__ -   Batch Number = 130
Evaluating:  79%|███████▉  | 130/165 [00:16<00:04,  8.22it/s]01/09/2022 23:58:20 - INFO - __main__ -   Batch Number = 131
Evaluating:  79%|███████▉  | 131/165 [00:16<00:04,  8.20it/s]01/09/2022 23:58:20 - INFO - __main__ -   Batch Number = 132
Evaluating:  80%|████████  | 132/165 [00:16<00:04,  8.19it/s]01/09/2022 23:58:20 - INFO - __main__ -   Batch Number = 133
Evaluating:  81%|████████  | 133/165 [00:16<00:03,  8.19it/s]01/09/2022 23:58:20 - INFO - __main__ -   Batch Number = 134
Evaluating:  81%|████████  | 134/165 [00:16<00:03,  8.18it/s]01/09/2022 23:58:20 - INFO - __main__ -   Batch Number = 135
Evaluating:  82%|████████▏ | 135/165 [00:16<00:03,  8.20it/s]01/09/2022 23:58:20 - INFO - __main__ -   Batch Number = 136
Evaluating:  82%|████████▏ | 136/165 [00:16<00:03,  8.21it/s]01/09/2022 23:58:21 - INFO - __main__ -   Batch Number = 137
Evaluating:  83%|████████▎ | 137/165 [00:17<00:03,  8.21it/s]01/09/2022 23:58:21 - INFO - __main__ -   Batch Number = 138
Evaluating:  84%|████████▎ | 138/165 [00:17<00:03,  8.18it/s]01/09/2022 23:58:21 - INFO - __main__ -   Batch Number = 139
Evaluating:  84%|████████▍ | 139/165 [00:17<00:03,  8.20it/s]01/09/2022 23:58:21 - INFO - __main__ -   Batch Number = 140
Evaluating:  85%|████████▍ | 140/165 [00:17<00:03,  8.20it/s]01/09/2022 23:58:21 - INFO - __main__ -   Batch Number = 141
Evaluating:  85%|████████▌ | 141/165 [00:17<00:02,  8.20it/s]01/09/2022 23:58:21 - INFO - __main__ -   Batch Number = 142
Evaluating:  86%|████████▌ | 142/165 [00:17<00:02,  8.21it/s]01/09/2022 23:58:21 - INFO - __main__ -   Batch Number = 143
Evaluating:  87%|████████▋ | 143/165 [00:17<00:02,  8.21it/s]01/09/2022 23:58:21 - INFO - __main__ -   Batch Number = 144
Evaluating:  87%|████████▋ | 144/165 [00:17<00:02,  8.20it/s]01/09/2022 23:58:22 - INFO - __main__ -   Batch Number = 145
Evaluating:  88%|████████▊ | 145/165 [00:17<00:02,  8.19it/s]01/09/2022 23:58:22 - INFO - __main__ -   Batch Number = 146
Evaluating:  88%|████████▊ | 146/165 [00:18<00:02,  8.17it/s]01/09/2022 23:58:22 - INFO - __main__ -   Batch Number = 147
Evaluating:  89%|████████▉ | 147/165 [00:18<00:02,  8.18it/s]01/09/2022 23:58:22 - INFO - __main__ -   Batch Number = 148
Evaluating:  90%|████████▉ | 148/165 [00:18<00:02,  8.18it/s]01/09/2022 23:58:22 - INFO - __main__ -   Batch Number = 149
Evaluating:  90%|█████████ | 149/165 [00:18<00:01,  8.19it/s]01/09/2022 23:58:22 - INFO - __main__ -   Batch Number = 150
Evaluating:  91%|█████████ | 150/165 [00:18<00:01,  8.19it/s]01/09/2022 23:58:22 - INFO - __main__ -   Batch Number = 151
Evaluating:  92%|█████████▏| 151/165 [00:18<00:01,  8.18it/s]01/09/2022 23:58:22 - INFO - __main__ -   Batch Number = 152
Evaluating:  92%|█████████▏| 152/165 [00:18<00:01,  8.19it/s]01/09/2022 23:58:23 - INFO - __main__ -   Batch Number = 153
Evaluating:  93%|█████████▎| 153/165 [00:18<00:01,  8.20it/s]01/09/2022 23:58:23 - INFO - __main__ -   Batch Number = 154
Evaluating:  93%|█████████▎| 154/165 [00:19<00:01,  8.20it/s]01/09/2022 23:58:23 - INFO - __main__ -   Batch Number = 155
Evaluating:  94%|█████████▍| 155/165 [00:19<00:01,  8.21it/s]01/09/2022 23:58:23 - INFO - __main__ -   Batch Number = 156
Evaluating:  95%|█████████▍| 156/165 [00:19<00:01,  8.21it/s]01/09/2022 23:58:23 - INFO - __main__ -   Batch Number = 157
Evaluating:  95%|█████████▌| 157/165 [00:19<00:00,  8.21it/s]01/09/2022 23:58:23 - INFO - __main__ -   Batch Number = 158
Evaluating:  96%|█████████▌| 158/165 [00:19<00:00,  8.20it/s]01/09/2022 23:58:23 - INFO - __main__ -   Batch Number = 159
Evaluating:  96%|█████████▋| 159/165 [00:19<00:00,  8.21it/s]01/09/2022 23:58:23 - INFO - __main__ -   Batch Number = 160
Evaluating:  97%|█████████▋| 160/165 [00:19<00:00,  8.18it/s]01/09/2022 23:58:23 - INFO - __main__ -   Batch Number = 161
Evaluating:  98%|█████████▊| 161/165 [00:19<00:00,  8.09it/s]01/09/2022 23:58:24 - INFO - __main__ -   Batch Number = 162
Evaluating:  98%|█████████▊| 162/165 [00:20<00:00,  8.11it/s]01/09/2022 23:58:24 - INFO - __main__ -   Batch Number = 163
Evaluating:  99%|█████████▉| 163/165 [00:20<00:00,  8.13it/s]01/09/2022 23:58:24 - INFO - __main__ -   Batch Number = 164
Evaluating:  99%|█████████▉| 164/165 [00:20<00:00,  8.13it/s]01/09/2022 23:58:24 - INFO - __main__ -   Batch Number = 165
Evaluating: 100%|██████████| 165/165 [00:20<00:00,  8.09it/s]
01/09/2022 23:58:24 - INFO - __main__ -     Evaluation done in total 20.398316 secs (0.015477 sec per example)
Writing predictions to: /home/abhijeet/rohan/cloud-emea-copy/outputs//squad/my-xlm-roberta-base-MaxLen384_qna_ar/predictions_ar_.json
Writing nbest to: /home/abhijeet/rohan/cloud-emea-copy/outputs//squad/my-xlm-roberta-base-MaxLen384_qna_ar/nbest_predictions_ar_.json
01/09/2022 23:58:28 - INFO - __main__ -   Results = OrderedDict([('exact', 47.39495798319328), ('f1', 63.631457776579964), ('precision', 65.8357745932381), ('recall', 66.12993937606936), ('total', 1190), ('HasAns_exact', 47.39495798319328), ('HasAns_f1', 63.631457776579964), ('HasAns_total', 1190), ('best_exact', 47.39495798319328), ('best_exact_thresh', 0.0), ('best_f1', 63.631457776579964), ('best_f1_thresh', 0.0)])
