PyTorch version 1.10.0+cu111 available.
01/07/2022 12:38:50 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/07/2022 12:38:50 - INFO - root -   save model
loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /root/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6
Model config XLMRobertaConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /root/.cache/torch/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8
loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/07/2022 12:38:58 - INFO - __main__ -   lang2id = None
01/07/2022 12:39:01 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='xlm-roberta-base', model_type='xlm-roberta', config_name=None, tokenizer_name=None, cache_dir=None, data_dir='/root/Desktop/cloud-emea-copy/data//xquad', output_dir='/root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/', max_seq_length=384, train_file='/root/Desktop/cloud-emea-copy/data//xquad/train-v1.1.json', predict_file='/root/Desktop/cloud-emea-copy/data//xquad/dev-v1.1.json', eval_test_set=False, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=4, per_gpu_eval_batch_size=8, gradient_accumulation_steps=8, learning_rate=0.0003, weight_decay=0.0001, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=15.0, max_steps=-1, save_steps=1000, warmup_steps=500, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', eval_lang='en', predict_langs='ar,de,el,es,hi,ru,th,tr,vi,zh', train_lang='en', log_file='/root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='qna', threads=8, version_2_with_negative=False, verbose_logging=False, n_best_size=20, max_query_length=64, max_answer_length=30, doc_stride=128, null_score_diff_threshold=0.0, predict_task_adapter='output/squad/my_xlm-roberta-base_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8_s1/checkpoint-best/qna', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix=None, en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/07/2022 12:39:01 - INFO - __main__ -   Results: {}
loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /root/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6
Model config XLMRobertaConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /root/.cache/torch/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8
loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/07/2022 12:39:10 - INFO - __main__ -   lang2id = None
01/07/2022 12:39:10 - INFO - __main__ -   Evaluating the model on the test set of all languages specified
01/07/2022 12:39:10 - INFO - __main__ -   Adapter will be loaded from this path: output/squad/my_xlm-roberta-base_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8_s1/checkpoint-best/qna
01/07/2022 12:39:10 - INFO - root -   Trying to decide if add adapter
01/07/2022 12:39:10 - INFO - root -   loading task adapter
https://raw.githubusercontent.com/Adapter-Hub/Hub/master/dist/index_text_task/xlm-roberta-base.json not found in cache or force_download set to True, downloading to /root/.cache/torch/adapters/tmp1ze3zp5u
Downloading:   0%|          | 0.00/173 [00:00<?, ?B/s]Downloading: 406B [00:00, 390kB/s]                    
storing https://raw.githubusercontent.com/Adapter-Hub/Hub/master/dist/index_text_task/xlm-roberta-base.json in cache at /root/.cache/torch/adapters/188147ba7cc19b3b2309f8f39551efacc4db9bed91c3e8935fad185acbecb33e.1b478aac8bf6eb1e4619e1f23d195e19690ce6d05069a891bd7050b7d6e6c949
creating metadata file for /root/.cache/torch/adapters/188147ba7cc19b3b2309f8f39551efacc4db9bed91c3e8935fad185acbecb33e.1b478aac8bf6eb1e4619e1f23d195e19690ce6d05069a891bd7050b7d6e6c949
Traceback (most recent call last):
  File "third_party/my_run_squad.py", line 1111, in <module>
    main()
  File "third_party/my_run_squad.py", line 1101, in main
    model, lang_adapter_names, task_name = setup_adapter(args, adapter_args, model, load_adapter=load_adapter, load_lang_adapter=load_lang_adapter)
  File "third_party/my_run_squad.py", line 788, in setup_adapter
    model.load_adapter(
  File "/root/Desktop/cloud-emea-copy/src/transformers/adapter_model_mixin.py", line 1172, in load_adapter
    return super().load_adapter(
  File "/root/Desktop/cloud-emea-copy/src/transformers/adapter_model_mixin.py", line 999, in load_adapter
    load_dir, load_name = loader.load(adapter_name_or_path, config, version, model_name, load_as, **kwargs)
  File "/root/Desktop/cloud-emea-copy/src/transformers/adapter_model_mixin.py", line 385, in load
    resolved_folder = resolve_adapter_path(
  File "/root/Desktop/cloud-emea-copy/src/transformers/adapter_utils.py", line 436, in resolve_adapter_path
    return pull_from_hub(
  File "/root/Desktop/cloud-emea-copy/src/transformers/adapter_utils.py", line 367, in pull_from_hub
    raise EnvironmentError("No adapter with name '{}' was found in the adapter index.".format(specifier))
OSError: No adapter with name 'output/squad/my_xlm-roberta-base_LR3e-4_EPOCH15_maxlen384_batchsize4_gradacc8_s1/checkpoint-best/qna' was found in the adapter index.
PyTorch version 1.10.0+cu111 available.
01/07/2022 12:39:14 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/07/2022 12:39:14 - INFO - root -   save model
loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /root/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6
Model config XLMRobertaConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /root/.cache/torch/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8
loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2
Traceback (most recent call last):
  File "third_party/my_run_squad.py", line 1111, in <module>
    main()
  File "third_party/my_run_squad.py", line 986, in main
    model, tokenizer, lang2id, config_class, model_class, tokenizer_class = load_model(args)
  File "third_party/my_run_squad.py", line 763, in load_model
    model = model_class.from_pretrained(
  File "/root/Desktop/cloud-emea-copy/src/transformers/modeling_utils.py", line 949, in from_pretrained
    model = cls(config, *model_args, **model_kwargs)
  File "/root/Desktop/cloud-emea-copy/src/transformers/modeling_roberta.py", line 1411, in __init__
    self.roberta = RobertaModel(config, add_pooling_layer=False)
  File "/root/Desktop/cloud-emea-copy/src/transformers/modeling_roberta.py", line 630, in __init__
    self.init_weights()
  File "/root/Desktop/cloud-emea-copy/src/transformers/modeling_utils.py", line 673, in init_weights
    self.apply(self._init_weights)
  File "/root/Desktop/venvs/cloud-emea/lib/python3.8/site-packages/torch/nn/modules/module.py", line 659, in apply
    module.apply(fn)
  File "/root/Desktop/venvs/cloud-emea/lib/python3.8/site-packages/torch/nn/modules/module.py", line 659, in apply
    module.apply(fn)
  File "/root/Desktop/venvs/cloud-emea/lib/python3.8/site-packages/torch/nn/modules/module.py", line 659, in apply
    module.apply(fn)
  [Previous line repeated 3 more times]
  File "/root/Desktop/venvs/cloud-emea/lib/python3.8/site-packages/torch/nn/modules/module.py", line 660, in apply
    fn(self)
  File "/root/Desktop/cloud-emea-copy/src/transformers/modeling_roberta.py", line 519, in _init_weights
    module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
KeyboardInterrupt
PyTorch version 1.10.0+cu111 available.
01/07/2022 12:41:13 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/07/2022 12:41:13 - INFO - root -   save model
loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /root/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6
Model config XLMRobertaConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /root/.cache/torch/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8
loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/07/2022 12:41:22 - INFO - __main__ -   lang2id = None
01/07/2022 12:41:25 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='xlm-roberta-base', model_type='xlm-roberta', config_name=None, tokenizer_name=None, cache_dir=None, data_dir='/root/Desktop/cloud-emea-copy/data//xquad', output_dir='/root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/', max_seq_length=384, train_file='/root/Desktop/cloud-emea-copy/data//xquad/train-v1.1.json', predict_file='/root/Desktop/cloud-emea-copy/data//xquad/dev-v1.1.json', eval_test_set=False, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=4, per_gpu_eval_batch_size=8, gradient_accumulation_steps=8, learning_rate=0.0001, weight_decay=0.0001, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=15.0, max_steps=-1, save_steps=1000, warmup_steps=500, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', eval_lang='en', predict_langs='ar,de,el,es,hi,ru,th,tr,vi,zh', train_lang='en', log_file='/root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='qna', threads=8, version_2_with_negative=False, verbose_logging=False, n_best_size=20, max_query_length=64, max_answer_length=30, doc_stride=128, null_score_diff_threshold=0.0, predict_task_adapter='output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc8_s1/checkpoint-best/qna', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix=None, en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/07/2022 12:41:25 - INFO - __main__ -   Results: {}
loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /root/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6
Model config XLMRobertaConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /root/.cache/torch/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8
loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/07/2022 12:41:33 - INFO - __main__ -   lang2id = None
01/07/2022 12:41:33 - INFO - __main__ -   Evaluating the model on the test set of all languages specified
01/07/2022 12:41:33 - INFO - __main__ -   Adapter will be loaded from this path: output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc8_s1/checkpoint-best/qna
01/07/2022 12:41:33 - INFO - root -   Trying to decide if add adapter
01/07/2022 12:41:33 - INFO - root -   loading task adapter
Traceback (most recent call last):
  File "third_party/my_run_squad.py", line 1111, in <module>
    main()
  File "third_party/my_run_squad.py", line 1101, in main
    model, lang_adapter_names, task_name = setup_adapter(args, adapter_args, model, load_adapter=load_adapter, load_lang_adapter=load_lang_adapter)
  File "third_party/my_run_squad.py", line 788, in setup_adapter
    model.load_adapter(
  File "/root/Desktop/cloud-emea-copy/src/transformers/adapter_model_mixin.py", line 1172, in load_adapter
    return super().load_adapter(
  File "/root/Desktop/cloud-emea-copy/src/transformers/adapter_model_mixin.py", line 999, in load_adapter
    load_dir, load_name = loader.load(adapter_name_or_path, config, version, model_name, load_as, **kwargs)
  File "/root/Desktop/cloud-emea-copy/src/transformers/adapter_model_mixin.py", line 385, in load
    resolved_folder = resolve_adapter_path(
  File "/root/Desktop/cloud-emea-copy/src/transformers/adapter_utils.py", line 436, in resolve_adapter_path
    return pull_from_hub(
  File "/root/Desktop/cloud-emea-copy/src/transformers/adapter_utils.py", line 367, in pull_from_hub
    raise EnvironmentError("No adapter with name '{}' was found in the adapter index.".format(specifier))
OSError: No adapter with name 'output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc8_s1/checkpoint-best/qna' was found in the adapter index.
PyTorch version 1.10.0+cu111 available.
01/07/2022 12:41:36 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/07/2022 12:41:36 - INFO - root -   save model
loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /root/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6
Model config XLMRobertaConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /root/.cache/torch/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8
loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/07/2022 12:41:45 - INFO - __main__ -   lang2id = None
01/07/2022 12:41:48 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='xlm-roberta-base', model_type='xlm-roberta', config_name=None, tokenizer_name=None, cache_dir=None, data_dir='/root/Desktop/cloud-emea-copy/data//xquad', output_dir='/root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/', max_seq_length=384, train_file='/root/Desktop/cloud-emea-copy/data//xquad/train-v1.1.json', predict_file='/root/Desktop/cloud-emea-copy/data//xquad/dev-v1.1.json', eval_test_set=False, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=4, per_gpu_eval_batch_size=8, gradient_accumulation_steps=8, learning_rate=0.0001, weight_decay=0.0001, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=15.0, max_steps=-1, save_steps=1000, warmup_steps=500, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', eval_lang='en', predict_langs='ar,de,el,es,hi,ru,th,tr,vi,zh', train_lang='en', log_file='/root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='qna', threads=8, version_2_with_negative=False, verbose_logging=False, n_best_size=20, max_query_length=64, max_answer_length=30, doc_stride=128, null_score_diff_threshold=0.0, predict_task_adapter='output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc8_s2/checkpoint-best/qna', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix=None, en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/07/2022 12:41:48 - INFO - __main__ -   Results: {}
loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /root/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6
Model config XLMRobertaConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /root/.cache/torch/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8
loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/07/2022 12:41:56 - INFO - __main__ -   lang2id = None
01/07/2022 12:41:56 - INFO - __main__ -   Evaluating the model on the test set of all languages specified
01/07/2022 12:41:56 - INFO - __main__ -   Adapter will be loaded from this path: output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc8_s2/checkpoint-best/qna
01/07/2022 12:41:56 - INFO - root -   Trying to decide if add adapter
01/07/2022 12:41:56 - INFO - root -   loading task adapter
Traceback (most recent call last):
  File "third_party/my_run_squad.py", line 1111, in <module>
    main()
  File "third_party/my_run_squad.py", line 1101, in main
    model, lang_adapter_names, task_name = setup_adapter(args, adapter_args, model, load_adapter=load_adapter, load_lang_adapter=load_lang_adapter)
  File "third_party/my_run_squad.py", line 788, in setup_adapter
    model.load_adapter(
  File "/root/Desktop/cloud-emea-copy/src/transformers/adapter_model_mixin.py", line 1172, in load_adapter
    return super().load_adapter(
  File "/root/Desktop/cloud-emea-copy/src/transformers/adapter_model_mixin.py", line 999, in load_adapter
    load_dir, load_name = loader.load(adapter_name_or_path, config, version, model_name, load_as, **kwargs)
  File "/root/Desktop/cloud-emea-copy/src/transformers/adapter_model_mixin.py", line 385, in load
    resolved_folder = resolve_adapter_path(
  File "/root/Desktop/cloud-emea-copy/src/transformers/adapter_utils.py", line 436, in resolve_adapter_path
    return pull_from_hub(
  File "/root/Desktop/cloud-emea-copy/src/transformers/adapter_utils.py", line 367, in pull_from_hub
    raise EnvironmentError("No adapter with name '{}' was found in the adapter index.".format(specifier))
OSError: No adapter with name 'output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc8_s2/checkpoint-best/qna' was found in the adapter index.
PyTorch version 1.10.0+cu111 available.
01/07/2022 12:41:59 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/07/2022 12:41:59 - INFO - root -   save model
loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /root/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6
Model config XLMRobertaConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /root/.cache/torch/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8
loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/07/2022 12:42:08 - INFO - __main__ -   lang2id = None
01/07/2022 12:42:11 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='xlm-roberta-base', model_type='xlm-roberta', config_name=None, tokenizer_name=None, cache_dir=None, data_dir='/root/Desktop/cloud-emea-copy/data//xquad', output_dir='/root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/', max_seq_length=384, train_file='/root/Desktop/cloud-emea-copy/data//xquad/train-v1.1.json', predict_file='/root/Desktop/cloud-emea-copy/data//xquad/dev-v1.1.json', eval_test_set=False, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=4, per_gpu_eval_batch_size=8, gradient_accumulation_steps=8, learning_rate=0.0001, weight_decay=0.0001, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=15.0, max_steps=-1, save_steps=1000, warmup_steps=500, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', eval_lang='en', predict_langs='ar,de,el,es,hi,ru,th,tr,vi,zh', train_lang='en', log_file='/root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='qna', threads=8, version_2_with_negative=False, verbose_logging=False, n_best_size=20, max_query_length=64, max_answer_length=30, doc_stride=128, null_score_diff_threshold=0.0, predict_task_adapter='output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc8_s3/checkpoint-best/qna', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix=None, en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/07/2022 12:42:11 - INFO - __main__ -   Results: {}
loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /root/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6
Model config XLMRobertaConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /root/.cache/torch/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8
loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/07/2022 12:42:20 - INFO - __main__ -   lang2id = None
01/07/2022 12:42:20 - INFO - __main__ -   Evaluating the model on the test set of all languages specified
01/07/2022 12:42:20 - INFO - __main__ -   Adapter will be loaded from this path: output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc8_s3/checkpoint-best/qna
01/07/2022 12:42:20 - INFO - root -   Trying to decide if add adapter
01/07/2022 12:42:20 - INFO - root -   loading task adapter
Traceback (most recent call last):
  File "third_party/my_run_squad.py", line 1111, in <module>
    main()
  File "third_party/my_run_squad.py", line 1101, in main
    model, lang_adapter_names, task_name = setup_adapter(args, adapter_args, model, load_adapter=load_adapter, load_lang_adapter=load_lang_adapter)
  File "third_party/my_run_squad.py", line 788, in setup_adapter
    model.load_adapter(
  File "/root/Desktop/cloud-emea-copy/src/transformers/adapter_model_mixin.py", line 1172, in load_adapter
    return super().load_adapter(
  File "/root/Desktop/cloud-emea-copy/src/transformers/adapter_model_mixin.py", line 999, in load_adapter
    load_dir, load_name = loader.load(adapter_name_or_path, config, version, model_name, load_as, **kwargs)
  File "/root/Desktop/cloud-emea-copy/src/transformers/adapter_model_mixin.py", line 385, in load
    resolved_folder = resolve_adapter_path(
  File "/root/Desktop/cloud-emea-copy/src/transformers/adapter_utils.py", line 436, in resolve_adapter_path
    return pull_from_hub(
  File "/root/Desktop/cloud-emea-copy/src/transformers/adapter_utils.py", line 367, in pull_from_hub
    raise EnvironmentError("No adapter with name '{}' was found in the adapter index.".format(specifier))
OSError: No adapter with name 'output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc8_s3/checkpoint-best/qna' was found in the adapter index.
PyTorch version 1.10.0+cu111 available.
01/07/2022 12:44:17 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/07/2022 12:44:17 - INFO - root -   save model
loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /root/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6
Model config XLMRobertaConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /root/.cache/torch/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8
loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/07/2022 12:44:26 - INFO - __main__ -   lang2id = None
01/07/2022 12:44:29 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='xlm-roberta-base', model_type='xlm-roberta', config_name=None, tokenizer_name=None, cache_dir=None, data_dir='/root/Desktop/cloud-emea-copy/data//xquad', output_dir='/root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/', max_seq_length=384, train_file='/root/Desktop/cloud-emea-copy/data//xquad/train-v1.1.json', predict_file='/root/Desktop/cloud-emea-copy/data//xquad/dev-v1.1.json', eval_test_set=False, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=4, per_gpu_eval_batch_size=8, gradient_accumulation_steps=4, learning_rate=0.0001, weight_decay=0.0001, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=15.0, max_steps=-1, save_steps=1000, warmup_steps=500, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', eval_lang='en', predict_langs='ar,de,el,es,hi,ru,th,tr,vi,zh', train_lang='en', log_file='/root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='qna', threads=8, version_2_with_negative=False, verbose_logging=False, n_best_size=20, max_query_length=64, max_answer_length=30, doc_stride=128, null_score_diff_threshold=0.0, predict_task_adapter='output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s1/checkpoint-best/qna', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix=None, en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/07/2022 12:44:29 - INFO - __main__ -   Results: {}
loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /root/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6
Model config XLMRobertaConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /root/.cache/torch/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8
loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/07/2022 12:44:37 - INFO - __main__ -   lang2id = None
01/07/2022 12:44:38 - INFO - __main__ -   Evaluating the model on the test set of all languages specified
01/07/2022 12:44:38 - INFO - __main__ -   Adapter will be loaded from this path: output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s1/checkpoint-best/qna
01/07/2022 12:44:38 - INFO - root -   Trying to decide if add adapter
01/07/2022 12:44:38 - INFO - root -   loading task adapter
Loading module configuration from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s1/checkpoint-best/qna/adapter_config.json
Adding adapter 'qna' of type 'text_task'.
Loading module weights from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s1/checkpoint-best/qna/pytorch_adapter.bin
Loading module configuration from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s1/checkpoint-best/qna/head_config.json
Loading module weights from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s1/checkpoint-best/qna/pytorch_model_head.bin
01/07/2022 12:44:38 - INFO - root -   loading lang adpater en/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/xlm-roberta-base/pfeiffer/en_relu_2.zip.
Loading module configuration from /root/.cache/torch/adapters/3e9c8c800af886cd1c684a3acf4929990b35f3ac75e050e942de2ff2ecf895fe-caabba7772816577124631d5b84810c4d32f49afcea4fca637162ec7f432174f-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
Loading module weights from /root/.cache/torch/adapters/3e9c8c800af886cd1c684a3acf4929990b35f3ac75e050e942de2ff2ecf895fe-caabba7772816577124631d5b84810c4d32f49afcea4fca637162ec7f432174f-extracted/pytorch_adapter.bin
No matching prediction head found in '/root/.cache/torch/adapters/3e9c8c800af886cd1c684a3acf4929990b35f3ac75e050e942de2ff2ecf895fe-caabba7772816577124631d5b84810c4d32f49afcea4fca637162ec7f432174f-extracted'
01/07/2022 12:44:40 - INFO - __main__ -   Language adapter for ar not found, using en instead
01/07/2022 12:44:40 - INFO - __main__ -   Set active language adapter to en
01/07/2022 12:44:40 - INFO - __main__ -   Args Adapter Weight = None
01/07/2022 12:44:40 - INFO - __main__ -   Adapter Languages = ['en']
01/07/2022 12:44:40 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
en en/wiki@ukp
Traceback (most recent call last):
  File "third_party/my_run_squad.py", line 1111, in <module>
    main()
  File "third_party/my_run_squad.py", line 1104, in main
    predict_and_save(args, adapter_args, model, tokenizer, lang2id, lang_adapter_names, task_name, 'test')
  File "third_party/my_run_squad.py", line 902, in predict_and_save
    results = evaluate(args, model, tokenizer, language=lang, lang2id=lang2id, adapter_weight=adapter_weight, mode=split)
  File "third_party/my_run_squad.py", line 438, in evaluate
    dataset, examples, features = load_and_cache_examples(args, tokenizer, evaluate=True, output_examples=True,
  File "third_party/my_run_squad.py", line 614, in load_and_cache_examples
    examples = processor.get_dev_examples(args.data_dir, filename=args.predict_file, language=language)
  File "/root/Desktop/cloud-emea-copy/third_party/processors/squad.py", line 535, in get_dev_examples
    with open(
FileNotFoundError: [Errno 2] No such file or directory: '/root/Desktop/cloud-emea-copy/data//xquad/dev-v1.1.json'
PyTorch version 1.10.0+cu111 available.
01/07/2022 12:44:43 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/07/2022 12:44:43 - INFO - root -   save model
loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /root/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6
Model config XLMRobertaConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /root/.cache/torch/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8
loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/07/2022 12:44:51 - INFO - __main__ -   lang2id = None
01/07/2022 12:44:55 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='xlm-roberta-base', model_type='xlm-roberta', config_name=None, tokenizer_name=None, cache_dir=None, data_dir='/root/Desktop/cloud-emea-copy/data//xquad', output_dir='/root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/', max_seq_length=384, train_file='/root/Desktop/cloud-emea-copy/data//xquad/train-v1.1.json', predict_file='/root/Desktop/cloud-emea-copy/data//xquad/dev-v1.1.json', eval_test_set=False, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=4, per_gpu_eval_batch_size=8, gradient_accumulation_steps=4, learning_rate=0.0001, weight_decay=0.0001, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=15.0, max_steps=-1, save_steps=1000, warmup_steps=500, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', eval_lang='en', predict_langs='ar,de,el,es,hi,ru,th,tr,vi,zh', train_lang='en', log_file='/root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='qna', threads=8, version_2_with_negative=False, verbose_logging=False, n_best_size=20, max_query_length=64, max_answer_length=30, doc_stride=128, null_score_diff_threshold=0.0, predict_task_adapter='output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s2/checkpoint-best/qna', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix=None, en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/07/2022 12:44:55 - INFO - __main__ -   Results: {}
loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /root/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6
Model config XLMRobertaConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /root/.cache/torch/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8
loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/07/2022 12:45:03 - INFO - __main__ -   lang2id = None
01/07/2022 12:45:03 - INFO - __main__ -   Evaluating the model on the test set of all languages specified
01/07/2022 12:45:03 - INFO - __main__ -   Adapter will be loaded from this path: output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s2/checkpoint-best/qna
01/07/2022 12:45:03 - INFO - root -   Trying to decide if add adapter
01/07/2022 12:45:03 - INFO - root -   loading task adapter
Loading module configuration from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s2/checkpoint-best/qna/adapter_config.json
Adding adapter 'qna' of type 'text_task'.
Loading module weights from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s2/checkpoint-best/qna/pytorch_adapter.bin
Loading module configuration from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s2/checkpoint-best/qna/head_config.json
Loading module weights from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s2/checkpoint-best/qna/pytorch_model_head.bin
01/07/2022 12:45:03 - INFO - root -   loading lang adpater en/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/xlm-roberta-base/pfeiffer/en_relu_2.zip.
Loading module configuration from /root/.cache/torch/adapters/3e9c8c800af886cd1c684a3acf4929990b35f3ac75e050e942de2ff2ecf895fe-caabba7772816577124631d5b84810c4d32f49afcea4fca637162ec7f432174f-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
Loading module weights from /root/.cache/torch/adapters/3e9c8c800af886cd1c684a3acf4929990b35f3ac75e050e942de2ff2ecf895fe-caabba7772816577124631d5b84810c4d32f49afcea4fca637162ec7f432174f-extracted/pytorch_adapter.bin
No matching prediction head found in '/root/.cache/torch/adapters/3e9c8c800af886cd1c684a3acf4929990b35f3ac75e050e942de2ff2ecf895fe-caabba7772816577124631d5b84810c4d32f49afcea4fca637162ec7f432174f-extracted'
01/07/2022 12:45:05 - INFO - __main__ -   Language adapter for ar not found, using en instead
01/07/2022 12:45:05 - INFO - __main__ -   Set active language adapter to en
01/07/2022 12:45:05 - INFO - __main__ -   Args Adapter Weight = None
01/07/2022 12:45:05 - INFO - __main__ -   Adapter Languages = ['en']
01/07/2022 12:45:05 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
en en/wiki@ukp
Traceback (most recent call last):
  File "third_party/my_run_squad.py", line 1111, in <module>
    main()
  File "third_party/my_run_squad.py", line 1104, in main
    predict_and_save(args, adapter_args, model, tokenizer, lang2id, lang_adapter_names, task_name, 'test')
  File "third_party/my_run_squad.py", line 902, in predict_and_save
    results = evaluate(args, model, tokenizer, language=lang, lang2id=lang2id, adapter_weight=adapter_weight, mode=split)
  File "third_party/my_run_squad.py", line 438, in evaluate
    dataset, examples, features = load_and_cache_examples(args, tokenizer, evaluate=True, output_examples=True,
  File "third_party/my_run_squad.py", line 614, in load_and_cache_examples
    examples = processor.get_dev_examples(args.data_dir, filename=args.predict_file, language=language)
  File "/root/Desktop/cloud-emea-copy/third_party/processors/squad.py", line 535, in get_dev_examples
    with open(
FileNotFoundError: [Errno 2] No such file or directory: '/root/Desktop/cloud-emea-copy/data//xquad/dev-v1.1.json'
PyTorch version 1.10.0+cu111 available.
01/07/2022 12:45:08 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/07/2022 12:45:08 - INFO - root -   save model
loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /root/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6
Model config XLMRobertaConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /root/.cache/torch/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8
loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/07/2022 12:45:16 - INFO - __main__ -   lang2id = None
01/07/2022 12:45:20 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='xlm-roberta-base', model_type='xlm-roberta', config_name=None, tokenizer_name=None, cache_dir=None, data_dir='/root/Desktop/cloud-emea-copy/data//xquad', output_dir='/root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/', max_seq_length=384, train_file='/root/Desktop/cloud-emea-copy/data//xquad/train-v1.1.json', predict_file='/root/Desktop/cloud-emea-copy/data//xquad/dev-v1.1.json', eval_test_set=False, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=4, per_gpu_eval_batch_size=8, gradient_accumulation_steps=4, learning_rate=0.0001, weight_decay=0.0001, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=15.0, max_steps=-1, save_steps=1000, warmup_steps=500, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', eval_lang='en', predict_langs='ar,de,el,es,hi,ru,th,tr,vi,zh', train_lang='en', log_file='/root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='qna', threads=8, version_2_with_negative=False, verbose_logging=False, n_best_size=20, max_query_length=64, max_answer_length=30, doc_stride=128, null_score_diff_threshold=0.0, predict_task_adapter='output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s3/checkpoint-best/qna', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix=None, en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/07/2022 12:45:20 - INFO - __main__ -   Results: {}
loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /root/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6
Model config XLMRobertaConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /root/.cache/torch/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8
Traceback (most recent call last):
  File "third_party/my_run_squad.py", line 1111, in <module>
    main()
  File "third_party/my_run_squad.py", line 1090, in main
    model, tokenizer, lang2id, config_class, model_class, tokenizer_class = load_model(args)
  File "third_party/my_run_squad.py", line 763, in load_model
    model = model_class.from_pretrained(
  File "/root/Desktop/cloud-emea-copy/src/transformers/modeling_utils.py", line 922, in from_pretrained
    resolved_archive_file = cached_path(
  File "/root/Desktop/cloud-emea-copy/src/transformers/file_utils.py", line 948, in cached_path
    output_path = get_from_cache(
  File "/root/Desktop/cloud-emea-copy/src/transformers/file_utils.py", line 1075, in get_from_cache
    r = requests.head(url, headers=headers, allow_redirects=False, proxies=proxies, timeout=etag_timeout)
  File "/root/Desktop/venvs/cloud-emea/lib/python3.8/site-packages/requests/api.py", line 102, in head
    return request('head', url, **kwargs)
  File "/root/Desktop/venvs/cloud-emea/lib/python3.8/site-packages/requests/api.py", line 61, in request
    return session.request(method=method, url=url, **kwargs)
  File "/root/Desktop/venvs/cloud-emea/lib/python3.8/site-packages/requests/sessions.py", line 529, in request
    resp = self.send(prep, **send_kwargs)
  File "/root/Desktop/venvs/cloud-emea/lib/python3.8/site-packages/requests/sessions.py", line 645, in send
    r = adapter.send(request, **kwargs)
  File "/root/Desktop/venvs/cloud-emea/lib/python3.8/site-packages/requests/adapters.py", line 440, in send
    resp = conn.urlopen(
  File "/root/Desktop/venvs/cloud-emea/lib/python3.8/site-packages/urllib3/connectionpool.py", line 699, in urlopen
    httplib_response = self._make_request(
  File "/root/Desktop/venvs/cloud-emea/lib/python3.8/site-packages/urllib3/connectionpool.py", line 382, in _make_request
    self._validate_conn(conn)
  File "/root/Desktop/venvs/cloud-emea/lib/python3.8/site-packages/urllib3/connectionpool.py", line 1010, in _validate_conn
    conn.connect()
  File "/root/Desktop/venvs/cloud-emea/lib/python3.8/site-packages/urllib3/connection.py", line 358, in connect
    conn = self._new_conn()
  File "/root/Desktop/venvs/cloud-emea/lib/python3.8/site-packages/urllib3/connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "/root/Desktop/venvs/cloud-emea/lib/python3.8/site-packages/urllib3/util/connection.py", line 86, in create_connection
    sock.connect(sa)
KeyboardInterrupt
PyTorch version 1.10.0+cu111 available.
01/07/2022 12:46:04 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/07/2022 12:46:04 - INFO - root -   save model
loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /root/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6
Model config XLMRobertaConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /root/.cache/torch/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8
loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/07/2022 12:46:13 - INFO - __main__ -   lang2id = None
01/07/2022 12:46:16 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='xlm-roberta-base', model_type='xlm-roberta', config_name=None, tokenizer_name=None, cache_dir=None, data_dir='/root/Desktop/cloud-emea-copy/data//xquad', output_dir='/root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/', max_seq_length=384, train_file='/root/Desktop/cloud-emea-copy/data//xquad/train-v1.1.json', predict_file='/root/Desktop/cloud-emea-copy/data//xquad/dev-v1.1.json', eval_test_set=False, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=4, per_gpu_eval_batch_size=8, gradient_accumulation_steps=4, learning_rate=0.0001, weight_decay=0.0001, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=15.0, max_steps=-1, save_steps=1000, warmup_steps=500, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', eval_lang='en', predict_langs='ar,de,el,es,hi,ru,th,tr,vi,zh', train_lang='en', log_file='/root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='qna', threads=8, version_2_with_negative=False, verbose_logging=False, n_best_size=20, max_query_length=64, max_answer_length=30, doc_stride=128, null_score_diff_threshold=0.0, predict_task_adapter='output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s1/checkpoint-best/qna', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix=None, en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/07/2022 12:46:16 - INFO - __main__ -   Results: {}
loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /root/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6
Model config XLMRobertaConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /root/.cache/torch/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8
loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/07/2022 12:46:25 - INFO - __main__ -   lang2id = None
01/07/2022 12:46:25 - INFO - __main__ -   Evaluating the model on the test set of all languages specified
01/07/2022 12:46:25 - INFO - __main__ -   Adapter will be loaded from this path: output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s1/checkpoint-best/qna
01/07/2022 12:46:25 - INFO - root -   Trying to decide if add adapter
01/07/2022 12:46:25 - INFO - root -   loading task adapter
Loading module configuration from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s1/checkpoint-best/qna/adapter_config.json
Adding adapter 'qna' of type 'text_task'.
Loading module weights from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s1/checkpoint-best/qna/pytorch_adapter.bin
Loading module configuration from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s1/checkpoint-best/qna/head_config.json
Loading module weights from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s1/checkpoint-best/qna/pytorch_model_head.bin
01/07/2022 12:46:25 - INFO - root -   loading lang adpater en/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/xlm-roberta-base/pfeiffer/en_relu_2.zip.
Loading module configuration from /root/.cache/torch/adapters/3e9c8c800af886cd1c684a3acf4929990b35f3ac75e050e942de2ff2ecf895fe-caabba7772816577124631d5b84810c4d32f49afcea4fca637162ec7f432174f-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
Loading module weights from /root/.cache/torch/adapters/3e9c8c800af886cd1c684a3acf4929990b35f3ac75e050e942de2ff2ecf895fe-caabba7772816577124631d5b84810c4d32f49afcea4fca637162ec7f432174f-extracted/pytorch_adapter.bin
No matching prediction head found in '/root/.cache/torch/adapters/3e9c8c800af886cd1c684a3acf4929990b35f3ac75e050e942de2ff2ecf895fe-caabba7772816577124631d5b84810c4d32f49afcea4fca637162ec7f432174f-extracted'
01/07/2022 12:46:27 - INFO - __main__ -   Language adapter for ar not found, using en instead
01/07/2022 12:46:27 - INFO - __main__ -   Set active language adapter to en
01/07/2022 12:46:27 - INFO - __main__ -   Args Adapter Weight = None
01/07/2022 12:46:27 - INFO - __main__ -   Adapter Languages = ['en']
01/07/2022 12:46:27 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
en en/wiki@ukp
  0%|          | 0/48 [00:00<?, ?it/s] 33%|      | 16/48 [00:00<00:00, 138.45it/s] 62%|   | 30/48 [00:00<00:00, 132.51it/s] 96%|| 46/48 [00:00<00:00, 141.59it/s]100%|| 48/48 [00:00<00:00, 141.26it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<03:51,  5.14it/s]convert squad examples to features:  32%|      | 385/1190 [00:00<00:01, 626.76it/s]convert squad examples to features: 100%|| 1190/1190 [00:00<00:00, 1690.11it/s]/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|| 1190/1190 [00:00<00:00, 500222.67it/s]
01/07/2022 12:46:29 - INFO - __main__ -   Local Rank = -1
01/07/2022 12:46:29 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.ar.json_xlm-roberta-base_384_ar
01/07/2022 12:46:30 - INFO - __main__ -   ***** Running evaluation  *****
01/07/2022 12:46:30 - INFO - __main__ -     Num examples = 1318
01/07/2022 12:46:30 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/165 [00:00<?, ?it/s]01/07/2022 12:46:30 - INFO - __main__ -   Batch Number = 1
01/07/2022 12:46:30 - INFO - __main__ -   Batch Number = 2
01/07/2022 12:46:30 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|         | 3/165 [00:00<00:06, 26.70it/s]01/07/2022 12:46:30 - INFO - __main__ -   Batch Number = 4
01/07/2022 12:46:30 - INFO - __main__ -   Batch Number = 5
01/07/2022 12:46:30 - INFO - __main__ -   Batch Number = 6
01/07/2022 12:46:30 - INFO - __main__ -   Batch Number = 7
Evaluating:   4%|         | 7/165 [00:00<00:04, 31.74it/s]01/07/2022 12:46:30 - INFO - __main__ -   Batch Number = 8
01/07/2022 12:46:30 - INFO - __main__ -   Batch Number = 9
01/07/2022 12:46:30 - INFO - __main__ -   Batch Number = 10
01/07/2022 12:46:30 - INFO - __main__ -   Batch Number = 11
Evaluating:   7%|         | 11/165 [00:00<00:04, 33.89it/s]01/07/2022 12:46:30 - INFO - __main__ -   Batch Number = 12
01/07/2022 12:46:30 - INFO - __main__ -   Batch Number = 13
01/07/2022 12:46:30 - INFO - __main__ -   Batch Number = 14
01/07/2022 12:46:30 - INFO - __main__ -   Batch Number = 15
01/07/2022 12:46:30 - INFO - __main__ -   Batch Number = 16
Evaluating:  10%|         | 16/165 [00:00<00:03, 37.87it/s]01/07/2022 12:46:30 - INFO - __main__ -   Batch Number = 17
01/07/2022 12:46:30 - INFO - __main__ -   Batch Number = 18
01/07/2022 12:46:30 - INFO - __main__ -   Batch Number = 19
01/07/2022 12:46:30 - INFO - __main__ -   Batch Number = 20
01/07/2022 12:46:30 - INFO - __main__ -   Batch Number = 21
Evaluating:  13%|        | 21/165 [00:00<00:03, 40.63it/s]01/07/2022 12:46:30 - INFO - __main__ -   Batch Number = 22
01/07/2022 12:46:30 - INFO - __main__ -   Batch Number = 23
01/07/2022 12:46:30 - INFO - __main__ -   Batch Number = 24
01/07/2022 12:46:30 - INFO - __main__ -   Batch Number = 25
01/07/2022 12:46:30 - INFO - __main__ -   Batch Number = 26
Evaluating:  16%|        | 26/165 [00:00<00:03, 42.10it/s]01/07/2022 12:46:30 - INFO - __main__ -   Batch Number = 27
01/07/2022 12:46:30 - INFO - __main__ -   Batch Number = 28
01/07/2022 12:46:30 - INFO - __main__ -   Batch Number = 29
01/07/2022 12:46:30 - INFO - __main__ -   Batch Number = 30
01/07/2022 12:46:30 - INFO - __main__ -   Batch Number = 31
Evaluating:  19%|        | 31/165 [00:00<00:03, 42.95it/s]01/07/2022 12:46:30 - INFO - __main__ -   Batch Number = 32
01/07/2022 12:46:30 - INFO - __main__ -   Batch Number = 33
01/07/2022 12:46:30 - INFO - __main__ -   Batch Number = 34
01/07/2022 12:46:30 - INFO - __main__ -   Batch Number = 35
01/07/2022 12:46:30 - INFO - __main__ -   Batch Number = 36
Evaluating:  22%|       | 36/165 [00:00<00:02, 43.57it/s]01/07/2022 12:46:31 - INFO - __main__ -   Batch Number = 37
01/07/2022 12:46:31 - INFO - __main__ -   Batch Number = 38
01/07/2022 12:46:31 - INFO - __main__ -   Batch Number = 39
01/07/2022 12:46:31 - INFO - __main__ -   Batch Number = 40
01/07/2022 12:46:31 - INFO - __main__ -   Batch Number = 41
Evaluating:  25%|       | 41/165 [00:01<00:02, 43.37it/s]01/07/2022 12:46:31 - INFO - __main__ -   Batch Number = 42
01/07/2022 12:46:31 - INFO - __main__ -   Batch Number = 43
01/07/2022 12:46:31 - INFO - __main__ -   Batch Number = 44
01/07/2022 12:46:31 - INFO - __main__ -   Batch Number = 45
01/07/2022 12:46:31 - INFO - __main__ -   Batch Number = 46
Evaluating:  28%|       | 46/165 [00:01<00:02, 41.95it/s]01/07/2022 12:46:31 - INFO - __main__ -   Batch Number = 47
01/07/2022 12:46:31 - INFO - __main__ -   Batch Number = 48
01/07/2022 12:46:31 - INFO - __main__ -   Batch Number = 49
01/07/2022 12:46:31 - INFO - __main__ -   Batch Number = 50
01/07/2022 12:46:31 - INFO - __main__ -   Batch Number = 51
Evaluating:  31%|       | 51/165 [00:01<00:02, 42.91it/s]01/07/2022 12:46:31 - INFO - __main__ -   Batch Number = 52
01/07/2022 12:46:31 - INFO - __main__ -   Batch Number = 53
01/07/2022 12:46:31 - INFO - __main__ -   Batch Number = 54
01/07/2022 12:46:31 - INFO - __main__ -   Batch Number = 55
01/07/2022 12:46:31 - INFO - __main__ -   Batch Number = 56
Evaluating:  34%|      | 56/165 [00:01<00:02, 42.98it/s]01/07/2022 12:46:31 - INFO - __main__ -   Batch Number = 57
01/07/2022 12:46:31 - INFO - __main__ -   Batch Number = 58
01/07/2022 12:46:31 - INFO - __main__ -   Batch Number = 59
01/07/2022 12:46:31 - INFO - __main__ -   Batch Number = 60
01/07/2022 12:46:31 - INFO - __main__ -   Batch Number = 61
Evaluating:  37%|      | 61/165 [00:01<00:03, 31.84it/s]01/07/2022 12:46:31 - INFO - __main__ -   Batch Number = 62
01/07/2022 12:46:31 - INFO - __main__ -   Batch Number = 63
01/07/2022 12:46:31 - INFO - __main__ -   Batch Number = 64
01/07/2022 12:46:31 - INFO - __main__ -   Batch Number = 65
01/07/2022 12:46:31 - INFO - __main__ -   Batch Number = 66
Evaluating:  40%|      | 66/165 [00:01<00:02, 34.87it/s]01/07/2022 12:46:31 - INFO - __main__ -   Batch Number = 67
01/07/2022 12:46:31 - INFO - __main__ -   Batch Number = 68
01/07/2022 12:46:31 - INFO - __main__ -   Batch Number = 69
01/07/2022 12:46:31 - INFO - __main__ -   Batch Number = 70
01/07/2022 12:46:31 - INFO - __main__ -   Batch Number = 71
Evaluating:  43%|     | 71/165 [00:01<00:02, 37.39it/s]01/07/2022 12:46:31 - INFO - __main__ -   Batch Number = 72
01/07/2022 12:46:31 - INFO - __main__ -   Batch Number = 73
01/07/2022 12:46:32 - INFO - __main__ -   Batch Number = 74
01/07/2022 12:46:32 - INFO - __main__ -   Batch Number = 75
01/07/2022 12:46:32 - INFO - __main__ -   Batch Number = 76
Evaluating:  46%|     | 76/165 [00:01<00:02, 39.18it/s]01/07/2022 12:46:32 - INFO - __main__ -   Batch Number = 77
01/07/2022 12:46:32 - INFO - __main__ -   Batch Number = 78
01/07/2022 12:46:32 - INFO - __main__ -   Batch Number = 79
01/07/2022 12:46:32 - INFO - __main__ -   Batch Number = 80
01/07/2022 12:46:32 - INFO - __main__ -   Batch Number = 81
Evaluating:  49%|     | 81/165 [00:02<00:02, 40.81it/s]01/07/2022 12:46:32 - INFO - __main__ -   Batch Number = 82
01/07/2022 12:46:32 - INFO - __main__ -   Batch Number = 83
01/07/2022 12:46:32 - INFO - __main__ -   Batch Number = 84
01/07/2022 12:46:32 - INFO - __main__ -   Batch Number = 85
01/07/2022 12:46:32 - INFO - __main__ -   Batch Number = 86
Evaluating:  52%|    | 86/165 [00:02<00:01, 41.80it/s]01/07/2022 12:46:32 - INFO - __main__ -   Batch Number = 87
01/07/2022 12:46:32 - INFO - __main__ -   Batch Number = 88
01/07/2022 12:46:32 - INFO - __main__ -   Batch Number = 89
01/07/2022 12:46:32 - INFO - __main__ -   Batch Number = 90
01/07/2022 12:46:32 - INFO - __main__ -   Batch Number = 91
Evaluating:  55%|    | 91/165 [00:02<00:01, 42.46it/s]01/07/2022 12:46:32 - INFO - __main__ -   Batch Number = 92
01/07/2022 12:46:32 - INFO - __main__ -   Batch Number = 93
01/07/2022 12:46:32 - INFO - __main__ -   Batch Number = 94
01/07/2022 12:46:32 - INFO - __main__ -   Batch Number = 95
01/07/2022 12:46:32 - INFO - __main__ -   Batch Number = 96
Evaluating:  58%|    | 96/165 [00:02<00:01, 43.12it/s]01/07/2022 12:46:32 - INFO - __main__ -   Batch Number = 97
01/07/2022 12:46:32 - INFO - __main__ -   Batch Number = 98
01/07/2022 12:46:32 - INFO - __main__ -   Batch Number = 99
01/07/2022 12:46:32 - INFO - __main__ -   Batch Number = 100
01/07/2022 12:46:32 - INFO - __main__ -   Batch Number = 101
Evaluating:  61%|    | 101/165 [00:02<00:01, 43.51it/s]01/07/2022 12:46:32 - INFO - __main__ -   Batch Number = 102
01/07/2022 12:46:32 - INFO - __main__ -   Batch Number = 103
01/07/2022 12:46:32 - INFO - __main__ -   Batch Number = 104
01/07/2022 12:46:32 - INFO - __main__ -   Batch Number = 105
01/07/2022 12:46:32 - INFO - __main__ -   Batch Number = 106
Evaluating:  64%|   | 106/165 [00:02<00:01, 43.76it/s]01/07/2022 12:46:32 - INFO - __main__ -   Batch Number = 107
01/07/2022 12:46:32 - INFO - __main__ -   Batch Number = 108
01/07/2022 12:46:32 - INFO - __main__ -   Batch Number = 109
01/07/2022 12:46:32 - INFO - __main__ -   Batch Number = 110
01/07/2022 12:46:32 - INFO - __main__ -   Batch Number = 111
Evaluating:  67%|   | 111/165 [00:02<00:01, 44.13it/s]01/07/2022 12:46:32 - INFO - __main__ -   Batch Number = 112
01/07/2022 12:46:32 - INFO - __main__ -   Batch Number = 113
01/07/2022 12:46:32 - INFO - __main__ -   Batch Number = 114
01/07/2022 12:46:32 - INFO - __main__ -   Batch Number = 115
01/07/2022 12:46:32 - INFO - __main__ -   Batch Number = 116
Evaluating:  70%|   | 116/165 [00:02<00:01, 44.42it/s]01/07/2022 12:46:32 - INFO - __main__ -   Batch Number = 117
01/07/2022 12:46:32 - INFO - __main__ -   Batch Number = 118
01/07/2022 12:46:33 - INFO - __main__ -   Batch Number = 119
01/07/2022 12:46:33 - INFO - __main__ -   Batch Number = 120
01/07/2022 12:46:33 - INFO - __main__ -   Batch Number = 121
Evaluating:  73%|  | 121/165 [00:02<00:00, 44.52it/s]01/07/2022 12:46:33 - INFO - __main__ -   Batch Number = 122
01/07/2022 12:46:33 - INFO - __main__ -   Batch Number = 123
01/07/2022 12:46:33 - INFO - __main__ -   Batch Number = 124
01/07/2022 12:46:33 - INFO - __main__ -   Batch Number = 125
01/07/2022 12:46:33 - INFO - __main__ -   Batch Number = 126
Evaluating:  76%|  | 126/165 [00:03<00:00, 44.71it/s]01/07/2022 12:46:33 - INFO - __main__ -   Batch Number = 127
01/07/2022 12:46:33 - INFO - __main__ -   Batch Number = 128
01/07/2022 12:46:33 - INFO - __main__ -   Batch Number = 129
01/07/2022 12:46:33 - INFO - __main__ -   Batch Number = 130
01/07/2022 12:46:33 - INFO - __main__ -   Batch Number = 131
Evaluating:  79%|  | 131/165 [00:03<00:00, 44.87it/s]01/07/2022 12:46:33 - INFO - __main__ -   Batch Number = 132
01/07/2022 12:46:33 - INFO - __main__ -   Batch Number = 133
01/07/2022 12:46:33 - INFO - __main__ -   Batch Number = 134
01/07/2022 12:46:33 - INFO - __main__ -   Batch Number = 135
01/07/2022 12:46:33 - INFO - __main__ -   Batch Number = 136
Evaluating:  82%| | 136/165 [00:03<00:00, 44.75it/s]01/07/2022 12:46:33 - INFO - __main__ -   Batch Number = 137
01/07/2022 12:46:33 - INFO - __main__ -   Batch Number = 138
01/07/2022 12:46:33 - INFO - __main__ -   Batch Number = 139
01/07/2022 12:46:33 - INFO - __main__ -   Batch Number = 140
01/07/2022 12:46:33 - INFO - __main__ -   Batch Number = 141
Evaluating:  85%| | 141/165 [00:03<00:00, 44.83it/s]01/07/2022 12:46:33 - INFO - __main__ -   Batch Number = 142
01/07/2022 12:46:33 - INFO - __main__ -   Batch Number = 143
01/07/2022 12:46:33 - INFO - __main__ -   Batch Number = 144
01/07/2022 12:46:33 - INFO - __main__ -   Batch Number = 145
01/07/2022 12:46:33 - INFO - __main__ -   Batch Number = 146
Evaluating:  88%| | 146/165 [00:03<00:00, 44.81it/s]01/07/2022 12:46:33 - INFO - __main__ -   Batch Number = 147
01/07/2022 12:46:33 - INFO - __main__ -   Batch Number = 148
01/07/2022 12:46:33 - INFO - __main__ -   Batch Number = 149
01/07/2022 12:46:33 - INFO - __main__ -   Batch Number = 150
01/07/2022 12:46:33 - INFO - __main__ -   Batch Number = 151
Evaluating:  92%|| 151/165 [00:03<00:00, 44.75it/s]01/07/2022 12:46:33 - INFO - __main__ -   Batch Number = 152
01/07/2022 12:46:33 - INFO - __main__ -   Batch Number = 153
01/07/2022 12:46:33 - INFO - __main__ -   Batch Number = 154
01/07/2022 12:46:33 - INFO - __main__ -   Batch Number = 155
01/07/2022 12:46:33 - INFO - __main__ -   Batch Number = 156
Evaluating:  95%|| 156/165 [00:03<00:00, 44.78it/s]01/07/2022 12:46:33 - INFO - __main__ -   Batch Number = 157
01/07/2022 12:46:33 - INFO - __main__ -   Batch Number = 158
01/07/2022 12:46:33 - INFO - __main__ -   Batch Number = 159
01/07/2022 12:46:33 - INFO - __main__ -   Batch Number = 160
01/07/2022 12:46:33 - INFO - __main__ -   Batch Number = 161
Evaluating:  98%|| 161/165 [00:03<00:00, 44.82it/s]01/07/2022 12:46:33 - INFO - __main__ -   Batch Number = 162
01/07/2022 12:46:33 - INFO - __main__ -   Batch Number = 163
01/07/2022 12:46:34 - INFO - __main__ -   Batch Number = 164
01/07/2022 12:46:34 - INFO - __main__ -   Batch Number = 165
Evaluating: 100%|| 165/165 [00:03<00:00, 41.93it/s]
01/07/2022 12:46:34 - INFO - __main__ -     Evaluation done in total 3.935939 secs (0.002986 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/predictions_ar_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/nbest_predictions_ar_.json
01/07/2022 12:46:37 - INFO - __main__ -   Results = OrderedDict([('exact', 43.865546218487395), ('f1', 60.30570324957831), ('total', 1190), ('HasAns_exact', 43.865546218487395), ('HasAns_f1', 60.30570324957831), ('HasAns_total', 1190), ('best_exact', 43.865546218487395), ('best_exact_thresh', 0.0), ('best_f1', 60.30570324957831), ('best_f1_thresh', 0.0)])
01/07/2022 12:46:37 - INFO - __main__ -   Language adapter for de not found, using en instead
01/07/2022 12:46:37 - INFO - __main__ -   Set active language adapter to en
01/07/2022 12:46:37 - INFO - __main__ -   Args Adapter Weight = None
01/07/2022 12:46:37 - INFO - __main__ -   Adapter Languages = ['en']
01/07/2022 12:46:37 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
  0%|          | 0/48 [00:00<?, ?it/s] 33%|      | 16/48 [00:00<00:00, 121.40it/s] 60%|    | 29/48 [00:00<00:00, 114.47it/s] 92%|| 44/48 [00:00<00:00, 125.49it/s]100%|| 48/48 [00:00<00:00, 124.58it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<04:46,  4.15it/s]convert squad examples to features:  32%|      | 385/1190 [00:00<00:01, 600.84it/s]convert squad examples to features: 100%|| 1190/1190 [00:00<00:00, 1580.07it/s]/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|| 1190/1190 [00:00<00:00, 575091.80it/s]
01/07/2022 12:46:38 - INFO - __main__ -   Local Rank = -1
01/07/2022 12:46:38 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.de.json_xlm-roberta-base_384_de
01/07/2022 12:46:39 - INFO - __main__ -   ***** Running evaluation  *****
01/07/2022 12:46:39 - INFO - __main__ -     Num examples = 1303
01/07/2022 12:46:39 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/163 [00:00<?, ?it/s]01/07/2022 12:46:39 - INFO - __main__ -   Batch Number = 1
01/07/2022 12:46:40 - INFO - __main__ -   Batch Number = 2
01/07/2022 12:46:40 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|         | 3/163 [00:00<00:05, 29.45it/s]01/07/2022 12:46:40 - INFO - __main__ -   Batch Number = 4
01/07/2022 12:46:40 - INFO - __main__ -   Batch Number = 5
01/07/2022 12:46:40 - INFO - __main__ -   Batch Number = 6
01/07/2022 12:46:40 - INFO - __main__ -   Batch Number = 7
Evaluating:   4%|         | 7/163 [00:00<00:04, 33.94it/s]01/07/2022 12:46:40 - INFO - __main__ -   Batch Number = 8
01/07/2022 12:46:40 - INFO - __main__ -   Batch Number = 9
01/07/2022 12:46:40 - INFO - __main__ -   Batch Number = 10
01/07/2022 12:46:40 - INFO - __main__ -   Batch Number = 11
01/07/2022 12:46:40 - INFO - __main__ -   Batch Number = 12
Evaluating:   7%|         | 12/163 [00:00<00:03, 38.98it/s]01/07/2022 12:46:40 - INFO - __main__ -   Batch Number = 13
01/07/2022 12:46:40 - INFO - __main__ -   Batch Number = 14
01/07/2022 12:46:40 - INFO - __main__ -   Batch Number = 15
01/07/2022 12:46:40 - INFO - __main__ -   Batch Number = 16
01/07/2022 12:46:40 - INFO - __main__ -   Batch Number = 17
Evaluating:  10%|         | 17/163 [00:00<00:03, 41.51it/s]01/07/2022 12:46:40 - INFO - __main__ -   Batch Number = 18
01/07/2022 12:46:40 - INFO - __main__ -   Batch Number = 19
01/07/2022 12:46:40 - INFO - __main__ -   Batch Number = 20
01/07/2022 12:46:40 - INFO - __main__ -   Batch Number = 21
01/07/2022 12:46:40 - INFO - __main__ -   Batch Number = 22
Evaluating:  13%|        | 22/163 [00:00<00:04, 32.85it/s]01/07/2022 12:46:40 - INFO - __main__ -   Batch Number = 23
01/07/2022 12:46:40 - INFO - __main__ -   Batch Number = 24
01/07/2022 12:46:40 - INFO - __main__ -   Batch Number = 25
01/07/2022 12:46:40 - INFO - __main__ -   Batch Number = 26
01/07/2022 12:46:40 - INFO - __main__ -   Batch Number = 27
Evaluating:  17%|        | 27/163 [00:00<00:03, 36.22it/s]01/07/2022 12:46:40 - INFO - __main__ -   Batch Number = 28
01/07/2022 12:46:40 - INFO - __main__ -   Batch Number = 29
01/07/2022 12:46:40 - INFO - __main__ -   Batch Number = 30
01/07/2022 12:46:40 - INFO - __main__ -   Batch Number = 31
01/07/2022 12:46:40 - INFO - __main__ -   Batch Number = 32
Evaluating:  20%|        | 32/163 [00:00<00:03, 38.60it/s]01/07/2022 12:46:40 - INFO - __main__ -   Batch Number = 33
01/07/2022 12:46:40 - INFO - __main__ -   Batch Number = 34
01/07/2022 12:46:40 - INFO - __main__ -   Batch Number = 35
01/07/2022 12:46:40 - INFO - __main__ -   Batch Number = 36
01/07/2022 12:46:40 - INFO - __main__ -   Batch Number = 37
Evaluating:  23%|       | 37/163 [00:00<00:03, 40.44it/s]01/07/2022 12:46:40 - INFO - __main__ -   Batch Number = 38
01/07/2022 12:46:40 - INFO - __main__ -   Batch Number = 39
01/07/2022 12:46:40 - INFO - __main__ -   Batch Number = 40
01/07/2022 12:46:41 - INFO - __main__ -   Batch Number = 41
01/07/2022 12:46:41 - INFO - __main__ -   Batch Number = 42
Evaluating:  26%|       | 42/163 [00:01<00:02, 41.87it/s]01/07/2022 12:46:41 - INFO - __main__ -   Batch Number = 43
01/07/2022 12:46:41 - INFO - __main__ -   Batch Number = 44
01/07/2022 12:46:41 - INFO - __main__ -   Batch Number = 45
01/07/2022 12:46:41 - INFO - __main__ -   Batch Number = 46
01/07/2022 12:46:41 - INFO - __main__ -   Batch Number = 47
Evaluating:  29%|       | 47/163 [00:01<00:02, 42.63it/s]01/07/2022 12:46:41 - INFO - __main__ -   Batch Number = 48
01/07/2022 12:46:41 - INFO - __main__ -   Batch Number = 49
01/07/2022 12:46:41 - INFO - __main__ -   Batch Number = 50
01/07/2022 12:46:41 - INFO - __main__ -   Batch Number = 51
01/07/2022 12:46:41 - INFO - __main__ -   Batch Number = 52
Evaluating:  32%|      | 52/163 [00:01<00:02, 43.40it/s]01/07/2022 12:46:41 - INFO - __main__ -   Batch Number = 53
01/07/2022 12:46:41 - INFO - __main__ -   Batch Number = 54
01/07/2022 12:46:41 - INFO - __main__ -   Batch Number = 55
01/07/2022 12:46:41 - INFO - __main__ -   Batch Number = 56
01/07/2022 12:46:41 - INFO - __main__ -   Batch Number = 57
Evaluating:  35%|      | 57/163 [00:01<00:02, 43.74it/s]01/07/2022 12:46:41 - INFO - __main__ -   Batch Number = 58
01/07/2022 12:46:41 - INFO - __main__ -   Batch Number = 59
01/07/2022 12:46:41 - INFO - __main__ -   Batch Number = 60
01/07/2022 12:46:41 - INFO - __main__ -   Batch Number = 61
01/07/2022 12:46:41 - INFO - __main__ -   Batch Number = 62
Evaluating:  38%|      | 62/163 [00:01<00:02, 44.07it/s]01/07/2022 12:46:41 - INFO - __main__ -   Batch Number = 63
01/07/2022 12:46:41 - INFO - __main__ -   Batch Number = 64
01/07/2022 12:46:41 - INFO - __main__ -   Batch Number = 65
01/07/2022 12:46:41 - INFO - __main__ -   Batch Number = 66
01/07/2022 12:46:41 - INFO - __main__ -   Batch Number = 67
Evaluating:  41%|      | 67/163 [00:01<00:02, 44.24it/s]01/07/2022 12:46:41 - INFO - __main__ -   Batch Number = 68
01/07/2022 12:46:41 - INFO - __main__ -   Batch Number = 69
01/07/2022 12:46:41 - INFO - __main__ -   Batch Number = 70
01/07/2022 12:46:41 - INFO - __main__ -   Batch Number = 71
01/07/2022 12:46:41 - INFO - __main__ -   Batch Number = 72
Evaluating:  44%|     | 72/163 [00:01<00:02, 44.52it/s]01/07/2022 12:46:41 - INFO - __main__ -   Batch Number = 73
01/07/2022 12:46:41 - INFO - __main__ -   Batch Number = 74
01/07/2022 12:46:41 - INFO - __main__ -   Batch Number = 75
01/07/2022 12:46:41 - INFO - __main__ -   Batch Number = 76
01/07/2022 12:46:41 - INFO - __main__ -   Batch Number = 77
Evaluating:  47%|     | 77/163 [00:01<00:01, 44.51it/s]01/07/2022 12:46:41 - INFO - __main__ -   Batch Number = 78
01/07/2022 12:46:41 - INFO - __main__ -   Batch Number = 79
01/07/2022 12:46:41 - INFO - __main__ -   Batch Number = 80
01/07/2022 12:46:41 - INFO - __main__ -   Batch Number = 81
01/07/2022 12:46:41 - INFO - __main__ -   Batch Number = 82
Evaluating:  50%|     | 82/163 [00:01<00:01, 44.73it/s]01/07/2022 12:46:41 - INFO - __main__ -   Batch Number = 83
01/07/2022 12:46:41 - INFO - __main__ -   Batch Number = 84
01/07/2022 12:46:41 - INFO - __main__ -   Batch Number = 85
01/07/2022 12:46:42 - INFO - __main__ -   Batch Number = 86
01/07/2022 12:46:42 - INFO - __main__ -   Batch Number = 87
Evaluating:  53%|    | 87/163 [00:02<00:01, 44.76it/s]01/07/2022 12:46:42 - INFO - __main__ -   Batch Number = 88
01/07/2022 12:46:42 - INFO - __main__ -   Batch Number = 89
01/07/2022 12:46:42 - INFO - __main__ -   Batch Number = 90
01/07/2022 12:46:42 - INFO - __main__ -   Batch Number = 91
01/07/2022 12:46:42 - INFO - __main__ -   Batch Number = 92
Evaluating:  56%|    | 92/163 [00:02<00:01, 44.78it/s]01/07/2022 12:46:42 - INFO - __main__ -   Batch Number = 93
01/07/2022 12:46:42 - INFO - __main__ -   Batch Number = 94
01/07/2022 12:46:42 - INFO - __main__ -   Batch Number = 95
01/07/2022 12:46:42 - INFO - __main__ -   Batch Number = 96
01/07/2022 12:46:42 - INFO - __main__ -   Batch Number = 97
Evaluating:  60%|    | 97/163 [00:02<00:01, 44.85it/s]01/07/2022 12:46:42 - INFO - __main__ -   Batch Number = 98
01/07/2022 12:46:42 - INFO - __main__ -   Batch Number = 99
01/07/2022 12:46:42 - INFO - __main__ -   Batch Number = 100
01/07/2022 12:46:42 - INFO - __main__ -   Batch Number = 101
01/07/2022 12:46:42 - INFO - __main__ -   Batch Number = 102
Evaluating:  63%|   | 102/163 [00:02<00:01, 44.92it/s]01/07/2022 12:46:42 - INFO - __main__ -   Batch Number = 103
01/07/2022 12:46:42 - INFO - __main__ -   Batch Number = 104
01/07/2022 12:46:42 - INFO - __main__ -   Batch Number = 105
01/07/2022 12:46:42 - INFO - __main__ -   Batch Number = 106
01/07/2022 12:46:42 - INFO - __main__ -   Batch Number = 107
Evaluating:  66%|   | 107/163 [00:02<00:01, 44.70it/s]01/07/2022 12:46:42 - INFO - __main__ -   Batch Number = 108
01/07/2022 12:46:42 - INFO - __main__ -   Batch Number = 109
01/07/2022 12:46:42 - INFO - __main__ -   Batch Number = 110
01/07/2022 12:46:42 - INFO - __main__ -   Batch Number = 111
01/07/2022 12:46:42 - INFO - __main__ -   Batch Number = 112
Evaluating:  69%|   | 112/163 [00:02<00:01, 44.84it/s]01/07/2022 12:46:42 - INFO - __main__ -   Batch Number = 113
01/07/2022 12:46:42 - INFO - __main__ -   Batch Number = 114
01/07/2022 12:46:42 - INFO - __main__ -   Batch Number = 115
01/07/2022 12:46:42 - INFO - __main__ -   Batch Number = 116
01/07/2022 12:46:42 - INFO - __main__ -   Batch Number = 117
Evaluating:  72%|  | 117/163 [00:02<00:01, 44.83it/s]01/07/2022 12:46:42 - INFO - __main__ -   Batch Number = 118
01/07/2022 12:46:42 - INFO - __main__ -   Batch Number = 119
01/07/2022 12:46:42 - INFO - __main__ -   Batch Number = 120
01/07/2022 12:46:42 - INFO - __main__ -   Batch Number = 121
01/07/2022 12:46:42 - INFO - __main__ -   Batch Number = 122
Evaluating:  75%|  | 122/163 [00:02<00:00, 44.81it/s]01/07/2022 12:46:42 - INFO - __main__ -   Batch Number = 123
01/07/2022 12:46:42 - INFO - __main__ -   Batch Number = 124
01/07/2022 12:46:42 - INFO - __main__ -   Batch Number = 125
01/07/2022 12:46:42 - INFO - __main__ -   Batch Number = 126
01/07/2022 12:46:42 - INFO - __main__ -   Batch Number = 127
Evaluating:  78%|  | 127/163 [00:02<00:00, 44.90it/s]01/07/2022 12:46:42 - INFO - __main__ -   Batch Number = 128
01/07/2022 12:46:42 - INFO - __main__ -   Batch Number = 129
01/07/2022 12:46:42 - INFO - __main__ -   Batch Number = 130
01/07/2022 12:46:43 - INFO - __main__ -   Batch Number = 131
01/07/2022 12:46:43 - INFO - __main__ -   Batch Number = 132
Evaluating:  81%|  | 132/163 [00:03<00:00, 44.86it/s]01/07/2022 12:46:43 - INFO - __main__ -   Batch Number = 133
01/07/2022 12:46:43 - INFO - __main__ -   Batch Number = 134
01/07/2022 12:46:43 - INFO - __main__ -   Batch Number = 135
01/07/2022 12:46:43 - INFO - __main__ -   Batch Number = 136
01/07/2022 12:46:43 - INFO - __main__ -   Batch Number = 137
Evaluating:  84%| | 137/163 [00:03<00:00, 44.70it/s]01/07/2022 12:46:43 - INFO - __main__ -   Batch Number = 138
01/07/2022 12:46:43 - INFO - __main__ -   Batch Number = 139
01/07/2022 12:46:43 - INFO - __main__ -   Batch Number = 140
01/07/2022 12:46:43 - INFO - __main__ -   Batch Number = 141
01/07/2022 12:46:43 - INFO - __main__ -   Batch Number = 142
Evaluating:  87%| | 142/163 [00:03<00:00, 44.83it/s]01/07/2022 12:46:43 - INFO - __main__ -   Batch Number = 143
01/07/2022 12:46:43 - INFO - __main__ -   Batch Number = 144
01/07/2022 12:46:43 - INFO - __main__ -   Batch Number = 145
01/07/2022 12:46:43 - INFO - __main__ -   Batch Number = 146
01/07/2022 12:46:43 - INFO - __main__ -   Batch Number = 147
Evaluating:  90%| | 147/163 [00:03<00:00, 44.83it/s]01/07/2022 12:46:43 - INFO - __main__ -   Batch Number = 148
01/07/2022 12:46:43 - INFO - __main__ -   Batch Number = 149
01/07/2022 12:46:43 - INFO - __main__ -   Batch Number = 150
01/07/2022 12:46:43 - INFO - __main__ -   Batch Number = 151
01/07/2022 12:46:43 - INFO - __main__ -   Batch Number = 152
Evaluating:  93%|| 152/163 [00:03<00:00, 44.76it/s]01/07/2022 12:46:43 - INFO - __main__ -   Batch Number = 153
01/07/2022 12:46:43 - INFO - __main__ -   Batch Number = 154
01/07/2022 12:46:43 - INFO - __main__ -   Batch Number = 155
01/07/2022 12:46:43 - INFO - __main__ -   Batch Number = 156
01/07/2022 12:46:43 - INFO - __main__ -   Batch Number = 157
Evaluating:  96%|| 157/163 [00:03<00:00, 44.73it/s]01/07/2022 12:46:43 - INFO - __main__ -   Batch Number = 158
01/07/2022 12:46:43 - INFO - __main__ -   Batch Number = 159
01/07/2022 12:46:43 - INFO - __main__ -   Batch Number = 160
01/07/2022 12:46:43 - INFO - __main__ -   Batch Number = 161
01/07/2022 12:46:43 - INFO - __main__ -   Batch Number = 162
Evaluating:  99%|| 162/163 [00:03<00:00, 44.80it/s]01/07/2022 12:46:43 - INFO - __main__ -   Batch Number = 163
Evaluating: 100%|| 163/163 [00:03<00:00, 43.11it/s]
01/07/2022 12:46:43 - INFO - __main__ -     Evaluation done in total 3.781165 secs (0.002902 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/predictions_de_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/nbest_predictions_de_.json
01/07/2022 12:46:47 - INFO - __main__ -   Results = OrderedDict([('exact', 56.80672268907563), ('f1', 72.90478654912818), ('total', 1190), ('HasAns_exact', 56.80672268907563), ('HasAns_f1', 72.90478654912818), ('HasAns_total', 1190), ('best_exact', 56.80672268907563), ('best_exact_thresh', 0.0), ('best_f1', 72.90478654912818), ('best_f1_thresh', 0.0)])
01/07/2022 12:46:47 - INFO - __main__ -   Language adapter for el not found, using en instead
01/07/2022 12:46:47 - INFO - __main__ -   Set active language adapter to en
01/07/2022 12:46:47 - INFO - __main__ -   Args Adapter Weight = None
01/07/2022 12:46:47 - INFO - __main__ -   Adapter Languages = ['en']
01/07/2022 12:46:47 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
  0%|          | 0/48 [00:00<?, ?it/s] 29%|       | 14/48 [00:00<00:00, 137.32it/s] 58%|    | 28/48 [00:00<00:00, 103.87it/s] 83%| | 40/48 [00:00<00:00, 109.41it/s]100%|| 48/48 [00:00<00:00, 114.19it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<05:19,  3.72it/s]convert squad examples to features:  27%|       | 321/1190 [00:00<00:00, 911.48it/s]convert squad examples to features:  37%|      | 436/1190 [00:00<00:01, 468.11it/s]convert squad examples to features: 100%|| 1190/1190 [00:00<00:00, 1279.64it/s]/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|| 1190/1190 [00:00<00:00, 632761.38it/s]
01/07/2022 12:46:49 - INFO - __main__ -   Local Rank = -1
01/07/2022 12:46:49 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.el.json_xlm-roberta-base_384_el
01/07/2022 12:46:50 - INFO - __main__ -   ***** Running evaluation  *****
01/07/2022 12:46:50 - INFO - __main__ -     Num examples = 1488
01/07/2022 12:46:50 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/186 [00:00<?, ?it/s]01/07/2022 12:46:50 - INFO - __main__ -   Batch Number = 1
01/07/2022 12:46:50 - INFO - __main__ -   Batch Number = 2
01/07/2022 12:46:50 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|         | 3/186 [00:00<00:06, 29.52it/s]01/07/2022 12:46:50 - INFO - __main__ -   Batch Number = 4
01/07/2022 12:46:50 - INFO - __main__ -   Batch Number = 5
01/07/2022 12:46:50 - INFO - __main__ -   Batch Number = 6
01/07/2022 12:46:50 - INFO - __main__ -   Batch Number = 7
Evaluating:   4%|         | 7/186 [00:00<00:05, 32.33it/s]01/07/2022 12:46:50 - INFO - __main__ -   Batch Number = 8
01/07/2022 12:46:50 - INFO - __main__ -   Batch Number = 9
01/07/2022 12:46:50 - INFO - __main__ -   Batch Number = 10
01/07/2022 12:46:50 - INFO - __main__ -   Batch Number = 11
01/07/2022 12:46:50 - INFO - __main__ -   Batch Number = 12
Evaluating:   6%|         | 12/186 [00:00<00:04, 37.90it/s]01/07/2022 12:46:50 - INFO - __main__ -   Batch Number = 13
01/07/2022 12:46:50 - INFO - __main__ -   Batch Number = 14
01/07/2022 12:46:50 - INFO - __main__ -   Batch Number = 15
01/07/2022 12:46:50 - INFO - __main__ -   Batch Number = 16
01/07/2022 12:46:50 - INFO - __main__ -   Batch Number = 17
Evaluating:   9%|         | 17/186 [00:00<00:04, 40.41it/s]01/07/2022 12:46:50 - INFO - __main__ -   Batch Number = 18
01/07/2022 12:46:50 - INFO - __main__ -   Batch Number = 19
01/07/2022 12:46:50 - INFO - __main__ -   Batch Number = 20
01/07/2022 12:46:50 - INFO - __main__ -   Batch Number = 21
01/07/2022 12:46:50 - INFO - __main__ -   Batch Number = 22
Evaluating:  12%|        | 22/186 [00:00<00:03, 42.02it/s]01/07/2022 12:46:50 - INFO - __main__ -   Batch Number = 23
01/07/2022 12:46:51 - INFO - __main__ -   Batch Number = 24
01/07/2022 12:46:51 - INFO - __main__ -   Batch Number = 25
01/07/2022 12:46:51 - INFO - __main__ -   Batch Number = 26
01/07/2022 12:46:51 - INFO - __main__ -   Batch Number = 27
Evaluating:  15%|        | 27/186 [00:00<00:03, 43.07it/s]01/07/2022 12:46:51 - INFO - __main__ -   Batch Number = 28
01/07/2022 12:46:51 - INFO - __main__ -   Batch Number = 29
01/07/2022 12:46:51 - INFO - __main__ -   Batch Number = 30
01/07/2022 12:46:51 - INFO - __main__ -   Batch Number = 31
01/07/2022 12:46:51 - INFO - __main__ -   Batch Number = 32
Evaluating:  17%|        | 32/186 [00:00<00:03, 43.42it/s]01/07/2022 12:46:51 - INFO - __main__ -   Batch Number = 33
01/07/2022 12:46:51 - INFO - __main__ -   Batch Number = 34
01/07/2022 12:46:51 - INFO - __main__ -   Batch Number = 35
01/07/2022 12:46:51 - INFO - __main__ -   Batch Number = 36
01/07/2022 12:46:51 - INFO - __main__ -   Batch Number = 37
Evaluating:  20%|        | 37/186 [00:00<00:03, 43.94it/s]01/07/2022 12:46:51 - INFO - __main__ -   Batch Number = 38
01/07/2022 12:46:51 - INFO - __main__ -   Batch Number = 39
01/07/2022 12:46:51 - INFO - __main__ -   Batch Number = 40
01/07/2022 12:46:51 - INFO - __main__ -   Batch Number = 41
01/07/2022 12:46:51 - INFO - __main__ -   Batch Number = 42
Evaluating:  23%|       | 42/186 [00:01<00:03, 44.05it/s]01/07/2022 12:46:51 - INFO - __main__ -   Batch Number = 43
01/07/2022 12:46:51 - INFO - __main__ -   Batch Number = 44
01/07/2022 12:46:51 - INFO - __main__ -   Batch Number = 45
01/07/2022 12:46:51 - INFO - __main__ -   Batch Number = 46
01/07/2022 12:46:51 - INFO - __main__ -   Batch Number = 47
Evaluating:  25%|       | 47/186 [00:01<00:03, 44.22it/s]01/07/2022 12:46:51 - INFO - __main__ -   Batch Number = 48
01/07/2022 12:46:51 - INFO - __main__ -   Batch Number = 49
01/07/2022 12:46:51 - INFO - __main__ -   Batch Number = 50
01/07/2022 12:46:51 - INFO - __main__ -   Batch Number = 51
01/07/2022 12:46:51 - INFO - __main__ -   Batch Number = 52
Evaluating:  28%|       | 52/186 [00:01<00:03, 44.30it/s]01/07/2022 12:46:51 - INFO - __main__ -   Batch Number = 53
01/07/2022 12:46:51 - INFO - __main__ -   Batch Number = 54
01/07/2022 12:46:51 - INFO - __main__ -   Batch Number = 55
01/07/2022 12:46:51 - INFO - __main__ -   Batch Number = 56
01/07/2022 12:46:51 - INFO - __main__ -   Batch Number = 57
Evaluating:  31%|       | 57/186 [00:01<00:02, 44.53it/s]01/07/2022 12:46:51 - INFO - __main__ -   Batch Number = 58
01/07/2022 12:46:51 - INFO - __main__ -   Batch Number = 59
01/07/2022 12:46:51 - INFO - __main__ -   Batch Number = 60
01/07/2022 12:46:51 - INFO - __main__ -   Batch Number = 61
01/07/2022 12:46:51 - INFO - __main__ -   Batch Number = 62
Evaluating:  33%|      | 62/186 [00:01<00:02, 44.31it/s]01/07/2022 12:46:51 - INFO - __main__ -   Batch Number = 63
01/07/2022 12:46:51 - INFO - __main__ -   Batch Number = 64
01/07/2022 12:46:51 - INFO - __main__ -   Batch Number = 65
01/07/2022 12:46:51 - INFO - __main__ -   Batch Number = 66
01/07/2022 12:46:51 - INFO - __main__ -   Batch Number = 67
Evaluating:  36%|      | 67/186 [00:01<00:02, 44.46it/s]01/07/2022 12:46:51 - INFO - __main__ -   Batch Number = 68
01/07/2022 12:46:52 - INFO - __main__ -   Batch Number = 69
01/07/2022 12:46:52 - INFO - __main__ -   Batch Number = 70
01/07/2022 12:46:52 - INFO - __main__ -   Batch Number = 71
01/07/2022 12:46:52 - INFO - __main__ -   Batch Number = 72
Evaluating:  39%|      | 72/186 [00:01<00:02, 44.48it/s]01/07/2022 12:46:52 - INFO - __main__ -   Batch Number = 73
01/07/2022 12:46:52 - INFO - __main__ -   Batch Number = 74
01/07/2022 12:46:52 - INFO - __main__ -   Batch Number = 75
01/07/2022 12:46:52 - INFO - __main__ -   Batch Number = 76
01/07/2022 12:46:52 - INFO - __main__ -   Batch Number = 77
Evaluating:  41%|     | 77/186 [00:01<00:02, 44.26it/s]01/07/2022 12:46:52 - INFO - __main__ -   Batch Number = 78
01/07/2022 12:46:52 - INFO - __main__ -   Batch Number = 79
01/07/2022 12:46:52 - INFO - __main__ -   Batch Number = 80
01/07/2022 12:46:52 - INFO - __main__ -   Batch Number = 81
01/07/2022 12:46:52 - INFO - __main__ -   Batch Number = 82
Evaluating:  44%|     | 82/186 [00:01<00:02, 44.36it/s]01/07/2022 12:46:52 - INFO - __main__ -   Batch Number = 83
01/07/2022 12:46:52 - INFO - __main__ -   Batch Number = 84
01/07/2022 12:46:52 - INFO - __main__ -   Batch Number = 85
01/07/2022 12:46:52 - INFO - __main__ -   Batch Number = 86
01/07/2022 12:46:52 - INFO - __main__ -   Batch Number = 87
Evaluating:  47%|     | 87/186 [00:02<00:02, 44.48it/s]01/07/2022 12:46:52 - INFO - __main__ -   Batch Number = 88
01/07/2022 12:46:52 - INFO - __main__ -   Batch Number = 89
01/07/2022 12:46:52 - INFO - __main__ -   Batch Number = 90
01/07/2022 12:46:52 - INFO - __main__ -   Batch Number = 91
01/07/2022 12:46:52 - INFO - __main__ -   Batch Number = 92
Evaluating:  49%|     | 92/186 [00:02<00:02, 44.47it/s]01/07/2022 12:46:52 - INFO - __main__ -   Batch Number = 93
01/07/2022 12:46:52 - INFO - __main__ -   Batch Number = 94
01/07/2022 12:46:52 - INFO - __main__ -   Batch Number = 95
01/07/2022 12:46:52 - INFO - __main__ -   Batch Number = 96
01/07/2022 12:46:52 - INFO - __main__ -   Batch Number = 97
Evaluating:  52%|    | 97/186 [00:02<00:01, 44.54it/s]01/07/2022 12:46:52 - INFO - __main__ -   Batch Number = 98
01/07/2022 12:46:52 - INFO - __main__ -   Batch Number = 99
01/07/2022 12:46:52 - INFO - __main__ -   Batch Number = 100
01/07/2022 12:46:52 - INFO - __main__ -   Batch Number = 101
01/07/2022 12:46:52 - INFO - __main__ -   Batch Number = 102
Evaluating:  55%|    | 102/186 [00:02<00:01, 44.69it/s]01/07/2022 12:46:52 - INFO - __main__ -   Batch Number = 103
01/07/2022 12:46:52 - INFO - __main__ -   Batch Number = 104
01/07/2022 12:46:52 - INFO - __main__ -   Batch Number = 105
01/07/2022 12:46:52 - INFO - __main__ -   Batch Number = 106
01/07/2022 12:46:52 - INFO - __main__ -   Batch Number = 107
Evaluating:  58%|    | 107/186 [00:02<00:01, 44.56it/s]01/07/2022 12:46:52 - INFO - __main__ -   Batch Number = 108
01/07/2022 12:46:52 - INFO - __main__ -   Batch Number = 109
01/07/2022 12:46:52 - INFO - __main__ -   Batch Number = 110
01/07/2022 12:46:52 - INFO - __main__ -   Batch Number = 111
01/07/2022 12:46:52 - INFO - __main__ -   Batch Number = 112
Evaluating:  60%|    | 112/186 [00:02<00:01, 44.64it/s]01/07/2022 12:46:53 - INFO - __main__ -   Batch Number = 113
01/07/2022 12:46:53 - INFO - __main__ -   Batch Number = 114
01/07/2022 12:46:53 - INFO - __main__ -   Batch Number = 115
01/07/2022 12:46:53 - INFO - __main__ -   Batch Number = 116
01/07/2022 12:46:53 - INFO - __main__ -   Batch Number = 117
Evaluating:  63%|   | 117/186 [00:02<00:01, 44.68it/s]01/07/2022 12:46:53 - INFO - __main__ -   Batch Number = 118
01/07/2022 12:46:53 - INFO - __main__ -   Batch Number = 119
01/07/2022 12:46:53 - INFO - __main__ -   Batch Number = 120
01/07/2022 12:46:53 - INFO - __main__ -   Batch Number = 121
01/07/2022 12:46:53 - INFO - __main__ -   Batch Number = 122
Evaluating:  66%|   | 122/186 [00:02<00:01, 42.81it/s]01/07/2022 12:46:53 - INFO - __main__ -   Batch Number = 123
01/07/2022 12:46:53 - INFO - __main__ -   Batch Number = 124
01/07/2022 12:46:53 - INFO - __main__ -   Batch Number = 125
01/07/2022 12:46:53 - INFO - __main__ -   Batch Number = 126
01/07/2022 12:46:53 - INFO - __main__ -   Batch Number = 127
Evaluating:  68%|   | 127/186 [00:02<00:01, 43.37it/s]01/07/2022 12:46:53 - INFO - __main__ -   Batch Number = 128
01/07/2022 12:46:53 - INFO - __main__ -   Batch Number = 129
01/07/2022 12:46:53 - INFO - __main__ -   Batch Number = 130
01/07/2022 12:46:53 - INFO - __main__ -   Batch Number = 131
01/07/2022 12:46:53 - INFO - __main__ -   Batch Number = 132
Evaluating:  71%|   | 132/186 [00:03<00:01, 43.91it/s]01/07/2022 12:46:53 - INFO - __main__ -   Batch Number = 133
01/07/2022 12:46:53 - INFO - __main__ -   Batch Number = 134
01/07/2022 12:46:53 - INFO - __main__ -   Batch Number = 135
01/07/2022 12:46:53 - INFO - __main__ -   Batch Number = 136
01/07/2022 12:46:53 - INFO - __main__ -   Batch Number = 137
Evaluating:  74%|  | 137/186 [00:03<00:01, 43.95it/s]01/07/2022 12:46:53 - INFO - __main__ -   Batch Number = 138
01/07/2022 12:46:53 - INFO - __main__ -   Batch Number = 139
01/07/2022 12:46:53 - INFO - __main__ -   Batch Number = 140
01/07/2022 12:46:53 - INFO - __main__ -   Batch Number = 141
01/07/2022 12:46:53 - INFO - __main__ -   Batch Number = 142
Evaluating:  76%|  | 142/186 [00:03<00:00, 44.27it/s]01/07/2022 12:46:53 - INFO - __main__ -   Batch Number = 143
01/07/2022 12:46:53 - INFO - __main__ -   Batch Number = 144
01/07/2022 12:46:53 - INFO - __main__ -   Batch Number = 145
01/07/2022 12:46:53 - INFO - __main__ -   Batch Number = 146
01/07/2022 12:46:53 - INFO - __main__ -   Batch Number = 147
Evaluating:  79%|  | 147/186 [00:03<00:00, 44.38it/s]01/07/2022 12:46:53 - INFO - __main__ -   Batch Number = 148
01/07/2022 12:46:53 - INFO - __main__ -   Batch Number = 149
01/07/2022 12:46:53 - INFO - __main__ -   Batch Number = 150
01/07/2022 12:46:53 - INFO - __main__ -   Batch Number = 151
01/07/2022 12:46:53 - INFO - __main__ -   Batch Number = 152
Evaluating:  82%| | 152/186 [00:03<00:00, 44.32it/s]01/07/2022 12:46:53 - INFO - __main__ -   Batch Number = 153
01/07/2022 12:46:53 - INFO - __main__ -   Batch Number = 154
01/07/2022 12:46:53 - INFO - __main__ -   Batch Number = 155
01/07/2022 12:46:53 - INFO - __main__ -   Batch Number = 156
01/07/2022 12:46:54 - INFO - __main__ -   Batch Number = 157
Evaluating:  84%| | 157/186 [00:03<00:00, 44.36it/s]01/07/2022 12:46:54 - INFO - __main__ -   Batch Number = 158
01/07/2022 12:46:54 - INFO - __main__ -   Batch Number = 159
01/07/2022 12:46:54 - INFO - __main__ -   Batch Number = 160
01/07/2022 12:46:54 - INFO - __main__ -   Batch Number = 161
01/07/2022 12:46:54 - INFO - __main__ -   Batch Number = 162
Evaluating:  87%| | 162/186 [00:03<00:00, 44.48it/s]01/07/2022 12:46:54 - INFO - __main__ -   Batch Number = 163
01/07/2022 12:46:54 - INFO - __main__ -   Batch Number = 164
01/07/2022 12:46:54 - INFO - __main__ -   Batch Number = 165
01/07/2022 12:46:54 - INFO - __main__ -   Batch Number = 166
01/07/2022 12:46:54 - INFO - __main__ -   Batch Number = 167
Evaluating:  90%| | 167/186 [00:03<00:00, 44.46it/s]01/07/2022 12:46:54 - INFO - __main__ -   Batch Number = 168
01/07/2022 12:46:54 - INFO - __main__ -   Batch Number = 169
01/07/2022 12:46:54 - INFO - __main__ -   Batch Number = 170
01/07/2022 12:46:54 - INFO - __main__ -   Batch Number = 171
01/07/2022 12:46:54 - INFO - __main__ -   Batch Number = 172
Evaluating:  92%|| 172/186 [00:03<00:00, 44.53it/s]01/07/2022 12:46:54 - INFO - __main__ -   Batch Number = 173
01/07/2022 12:46:54 - INFO - __main__ -   Batch Number = 174
01/07/2022 12:46:54 - INFO - __main__ -   Batch Number = 175
01/07/2022 12:46:54 - INFO - __main__ -   Batch Number = 176
01/07/2022 12:46:54 - INFO - __main__ -   Batch Number = 177
Evaluating:  95%|| 177/186 [00:04<00:00, 44.67it/s]01/07/2022 12:46:54 - INFO - __main__ -   Batch Number = 178
01/07/2022 12:46:54 - INFO - __main__ -   Batch Number = 179
01/07/2022 12:46:54 - INFO - __main__ -   Batch Number = 180
01/07/2022 12:46:54 - INFO - __main__ -   Batch Number = 181
01/07/2022 12:46:54 - INFO - __main__ -   Batch Number = 182
Evaluating:  98%|| 182/186 [00:04<00:00, 44.51it/s]01/07/2022 12:46:54 - INFO - __main__ -   Batch Number = 183
01/07/2022 12:46:54 - INFO - __main__ -   Batch Number = 184
01/07/2022 12:46:54 - INFO - __main__ -   Batch Number = 185
01/07/2022 12:46:54 - INFO - __main__ -   Batch Number = 186
Evaluating: 100%|| 186/186 [00:04<00:00, 43.77it/s]
01/07/2022 12:46:54 - INFO - __main__ -     Evaluation done in total 4.250133 secs (0.002856 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/predictions_el_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/nbest_predictions_el_.json
01/07/2022 12:46:57 - INFO - __main__ -   Results = OrderedDict([('exact', 50.7563025210084), ('f1', 67.64918824575591), ('total', 1190), ('HasAns_exact', 50.7563025210084), ('HasAns_f1', 67.64918824575591), ('HasAns_total', 1190), ('best_exact', 50.7563025210084), ('best_exact_thresh', 0.0), ('best_f1', 67.64918824575591), ('best_f1_thresh', 0.0)])
01/07/2022 12:46:57 - INFO - __main__ -   Language adapter for es not found, using en instead
01/07/2022 12:46:57 - INFO - __main__ -   Set active language adapter to en
01/07/2022 12:46:57 - INFO - __main__ -   Args Adapter Weight = None
01/07/2022 12:46:57 - INFO - __main__ -   Adapter Languages = ['en']
01/07/2022 12:46:57 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
  0%|          | 0/48 [00:00<?, ?it/s] 33%|      | 16/48 [00:00<00:00, 123.02it/s] 60%|    | 29/48 [00:00<00:00, 116.69it/s] 90%| | 43/48 [00:00<00:00, 124.32it/s]100%|| 48/48 [00:00<00:00, 125.13it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<05:18,  3.73it/s]convert squad examples to features:  32%|      | 385/1190 [00:00<00:01, 521.20it/s]convert squad examples to features:  97%|| 1153/1190 [00:00<00:00, 1663.12it/s]convert squad examples to features: 100%|| 1190/1190 [00:00<00:00, 1284.40it/s]/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|| 1190/1190 [00:00<00:00, 677694.74it/s]
01/07/2022 12:46:59 - INFO - __main__ -   Local Rank = -1
01/07/2022 12:46:59 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.es.json_xlm-roberta-base_384_es
01/07/2022 12:47:00 - INFO - __main__ -   ***** Running evaluation  *****
01/07/2022 12:47:00 - INFO - __main__ -     Num examples = 1304
01/07/2022 12:47:00 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/163 [00:00<?, ?it/s]01/07/2022 12:47:00 - INFO - __main__ -   Batch Number = 1
01/07/2022 12:47:01 - INFO - __main__ -   Batch Number = 2
01/07/2022 12:47:01 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|         | 3/163 [00:00<00:05, 29.67it/s]01/07/2022 12:47:01 - INFO - __main__ -   Batch Number = 4
01/07/2022 12:47:01 - INFO - __main__ -   Batch Number = 5
01/07/2022 12:47:01 - INFO - __main__ -   Batch Number = 6
01/07/2022 12:47:01 - INFO - __main__ -   Batch Number = 7
Evaluating:   4%|         | 7/163 [00:00<00:04, 32.83it/s]01/07/2022 12:47:01 - INFO - __main__ -   Batch Number = 8
01/07/2022 12:47:01 - INFO - __main__ -   Batch Number = 9
01/07/2022 12:47:01 - INFO - __main__ -   Batch Number = 10
01/07/2022 12:47:01 - INFO - __main__ -   Batch Number = 11
Evaluating:   7%|         | 11/163 [00:00<00:04, 34.46it/s]01/07/2022 12:47:01 - INFO - __main__ -   Batch Number = 12
01/07/2022 12:47:01 - INFO - __main__ -   Batch Number = 13
01/07/2022 12:47:01 - INFO - __main__ -   Batch Number = 14
01/07/2022 12:47:01 - INFO - __main__ -   Batch Number = 15
01/07/2022 12:47:01 - INFO - __main__ -   Batch Number = 16
Evaluating:  10%|         | 16/163 [00:00<00:03, 38.55it/s]01/07/2022 12:47:01 - INFO - __main__ -   Batch Number = 17
01/07/2022 12:47:01 - INFO - __main__ -   Batch Number = 18
01/07/2022 12:47:01 - INFO - __main__ -   Batch Number = 19
01/07/2022 12:47:01 - INFO - __main__ -   Batch Number = 20
01/07/2022 12:47:01 - INFO - __main__ -   Batch Number = 21
Evaluating:  13%|        | 21/163 [00:00<00:03, 40.47it/s]01/07/2022 12:47:01 - INFO - __main__ -   Batch Number = 22
01/07/2022 12:47:01 - INFO - __main__ -   Batch Number = 23
01/07/2022 12:47:01 - INFO - __main__ -   Batch Number = 24
01/07/2022 12:47:01 - INFO - __main__ -   Batch Number = 25
01/07/2022 12:47:01 - INFO - __main__ -   Batch Number = 26
Evaluating:  16%|        | 26/163 [00:00<00:03, 42.00it/s]01/07/2022 12:47:01 - INFO - __main__ -   Batch Number = 27
01/07/2022 12:47:01 - INFO - __main__ -   Batch Number = 28
01/07/2022 12:47:01 - INFO - __main__ -   Batch Number = 29
01/07/2022 12:47:01 - INFO - __main__ -   Batch Number = 30
01/07/2022 12:47:01 - INFO - __main__ -   Batch Number = 31
Evaluating:  19%|        | 31/163 [00:00<00:03, 42.79it/s]01/07/2022 12:47:01 - INFO - __main__ -   Batch Number = 32
01/07/2022 12:47:01 - INFO - __main__ -   Batch Number = 33
01/07/2022 12:47:01 - INFO - __main__ -   Batch Number = 34
01/07/2022 12:47:01 - INFO - __main__ -   Batch Number = 35
01/07/2022 12:47:01 - INFO - __main__ -   Batch Number = 36
Evaluating:  22%|       | 36/163 [00:00<00:02, 43.19it/s]01/07/2022 12:47:01 - INFO - __main__ -   Batch Number = 37
01/07/2022 12:47:01 - INFO - __main__ -   Batch Number = 38
01/07/2022 12:47:01 - INFO - __main__ -   Batch Number = 39
01/07/2022 12:47:01 - INFO - __main__ -   Batch Number = 40
01/07/2022 12:47:01 - INFO - __main__ -   Batch Number = 41
Evaluating:  25%|       | 41/163 [00:01<00:02, 43.68it/s]01/07/2022 12:47:01 - INFO - __main__ -   Batch Number = 42
01/07/2022 12:47:02 - INFO - __main__ -   Batch Number = 43
01/07/2022 12:47:02 - INFO - __main__ -   Batch Number = 44
01/07/2022 12:47:02 - INFO - __main__ -   Batch Number = 45
01/07/2022 12:47:02 - INFO - __main__ -   Batch Number = 46
Evaluating:  28%|       | 46/163 [00:01<00:02, 44.08it/s]01/07/2022 12:47:02 - INFO - __main__ -   Batch Number = 47
01/07/2022 12:47:02 - INFO - __main__ -   Batch Number = 48
01/07/2022 12:47:02 - INFO - __main__ -   Batch Number = 49
01/07/2022 12:47:02 - INFO - __main__ -   Batch Number = 50
01/07/2022 12:47:02 - INFO - __main__ -   Batch Number = 51
Evaluating:  31%|      | 51/163 [00:01<00:02, 44.22it/s]01/07/2022 12:47:02 - INFO - __main__ -   Batch Number = 52
01/07/2022 12:47:02 - INFO - __main__ -   Batch Number = 53
01/07/2022 12:47:02 - INFO - __main__ -   Batch Number = 54
01/07/2022 12:47:02 - INFO - __main__ -   Batch Number = 55
01/07/2022 12:47:02 - INFO - __main__ -   Batch Number = 56
Evaluating:  34%|      | 56/163 [00:01<00:02, 44.38it/s]01/07/2022 12:47:02 - INFO - __main__ -   Batch Number = 57
01/07/2022 12:47:02 - INFO - __main__ -   Batch Number = 58
01/07/2022 12:47:02 - INFO - __main__ -   Batch Number = 59
01/07/2022 12:47:02 - INFO - __main__ -   Batch Number = 60
01/07/2022 12:47:02 - INFO - __main__ -   Batch Number = 61
Evaluating:  37%|      | 61/163 [00:01<00:02, 44.54it/s]01/07/2022 12:47:02 - INFO - __main__ -   Batch Number = 62
01/07/2022 12:47:02 - INFO - __main__ -   Batch Number = 63
01/07/2022 12:47:02 - INFO - __main__ -   Batch Number = 64
01/07/2022 12:47:02 - INFO - __main__ -   Batch Number = 65
01/07/2022 12:47:02 - INFO - __main__ -   Batch Number = 66
Evaluating:  40%|      | 66/163 [00:01<00:02, 42.90it/s]01/07/2022 12:47:02 - INFO - __main__ -   Batch Number = 67
01/07/2022 12:47:02 - INFO - __main__ -   Batch Number = 68
01/07/2022 12:47:02 - INFO - __main__ -   Batch Number = 69
01/07/2022 12:47:02 - INFO - __main__ -   Batch Number = 70
01/07/2022 12:47:02 - INFO - __main__ -   Batch Number = 71
Evaluating:  44%|     | 71/163 [00:01<00:02, 43.45it/s]01/07/2022 12:47:02 - INFO - __main__ -   Batch Number = 72
01/07/2022 12:47:02 - INFO - __main__ -   Batch Number = 73
01/07/2022 12:47:02 - INFO - __main__ -   Batch Number = 74
01/07/2022 12:47:02 - INFO - __main__ -   Batch Number = 75
01/07/2022 12:47:02 - INFO - __main__ -   Batch Number = 76
Evaluating:  47%|     | 76/163 [00:01<00:01, 44.02it/s]01/07/2022 12:47:02 - INFO - __main__ -   Batch Number = 77
01/07/2022 12:47:02 - INFO - __main__ -   Batch Number = 78
01/07/2022 12:47:02 - INFO - __main__ -   Batch Number = 79
01/07/2022 12:47:02 - INFO - __main__ -   Batch Number = 80
01/07/2022 12:47:02 - INFO - __main__ -   Batch Number = 81
Evaluating:  50%|     | 81/163 [00:01<00:01, 44.10it/s]01/07/2022 12:47:02 - INFO - __main__ -   Batch Number = 82
01/07/2022 12:47:02 - INFO - __main__ -   Batch Number = 83
01/07/2022 12:47:02 - INFO - __main__ -   Batch Number = 84
01/07/2022 12:47:02 - INFO - __main__ -   Batch Number = 85
01/07/2022 12:47:02 - INFO - __main__ -   Batch Number = 86
Evaluating:  53%|    | 86/163 [00:02<00:01, 44.32it/s]01/07/2022 12:47:03 - INFO - __main__ -   Batch Number = 87
01/07/2022 12:47:03 - INFO - __main__ -   Batch Number = 88
01/07/2022 12:47:03 - INFO - __main__ -   Batch Number = 89
01/07/2022 12:47:03 - INFO - __main__ -   Batch Number = 90
01/07/2022 12:47:03 - INFO - __main__ -   Batch Number = 91
Evaluating:  56%|    | 91/163 [00:02<00:01, 44.35it/s]01/07/2022 12:47:03 - INFO - __main__ -   Batch Number = 92
01/07/2022 12:47:03 - INFO - __main__ -   Batch Number = 93
01/07/2022 12:47:03 - INFO - __main__ -   Batch Number = 94
01/07/2022 12:47:03 - INFO - __main__ -   Batch Number = 95
01/07/2022 12:47:03 - INFO - __main__ -   Batch Number = 96
Evaluating:  59%|    | 96/163 [00:02<00:01, 44.27it/s]01/07/2022 12:47:03 - INFO - __main__ -   Batch Number = 97
01/07/2022 12:47:03 - INFO - __main__ -   Batch Number = 98
01/07/2022 12:47:03 - INFO - __main__ -   Batch Number = 99
01/07/2022 12:47:03 - INFO - __main__ -   Batch Number = 100
01/07/2022 12:47:03 - INFO - __main__ -   Batch Number = 101
Evaluating:  62%|   | 101/163 [00:02<00:01, 44.34it/s]01/07/2022 12:47:03 - INFO - __main__ -   Batch Number = 102
01/07/2022 12:47:03 - INFO - __main__ -   Batch Number = 103
01/07/2022 12:47:03 - INFO - __main__ -   Batch Number = 104
01/07/2022 12:47:03 - INFO - __main__ -   Batch Number = 105
01/07/2022 12:47:03 - INFO - __main__ -   Batch Number = 106
Evaluating:  65%|   | 106/163 [00:02<00:01, 44.54it/s]01/07/2022 12:47:03 - INFO - __main__ -   Batch Number = 107
01/07/2022 12:47:03 - INFO - __main__ -   Batch Number = 108
01/07/2022 12:47:03 - INFO - __main__ -   Batch Number = 109
01/07/2022 12:47:03 - INFO - __main__ -   Batch Number = 110
01/07/2022 12:47:03 - INFO - __main__ -   Batch Number = 111
Evaluating:  68%|   | 111/163 [00:02<00:01, 44.53it/s]01/07/2022 12:47:03 - INFO - __main__ -   Batch Number = 112
01/07/2022 12:47:03 - INFO - __main__ -   Batch Number = 113
01/07/2022 12:47:03 - INFO - __main__ -   Batch Number = 114
01/07/2022 12:47:03 - INFO - __main__ -   Batch Number = 115
01/07/2022 12:47:03 - INFO - __main__ -   Batch Number = 116
Evaluating:  71%|   | 116/163 [00:02<00:01, 44.66it/s]01/07/2022 12:47:03 - INFO - __main__ -   Batch Number = 117
01/07/2022 12:47:03 - INFO - __main__ -   Batch Number = 118
01/07/2022 12:47:03 - INFO - __main__ -   Batch Number = 119
01/07/2022 12:47:03 - INFO - __main__ -   Batch Number = 120
01/07/2022 12:47:03 - INFO - __main__ -   Batch Number = 121
Evaluating:  74%|  | 121/163 [00:02<00:00, 44.66it/s]01/07/2022 12:47:03 - INFO - __main__ -   Batch Number = 122
01/07/2022 12:47:03 - INFO - __main__ -   Batch Number = 123
01/07/2022 12:47:03 - INFO - __main__ -   Batch Number = 124
01/07/2022 12:47:03 - INFO - __main__ -   Batch Number = 125
01/07/2022 12:47:03 - INFO - __main__ -   Batch Number = 126
Evaluating:  77%|  | 126/163 [00:02<00:00, 42.34it/s]01/07/2022 12:47:03 - INFO - __main__ -   Batch Number = 127
01/07/2022 12:47:03 - INFO - __main__ -   Batch Number = 128
01/07/2022 12:47:04 - INFO - __main__ -   Batch Number = 129
01/07/2022 12:47:04 - INFO - __main__ -   Batch Number = 130
01/07/2022 12:47:04 - INFO - __main__ -   Batch Number = 131
Evaluating:  80%|  | 131/163 [00:03<00:00, 37.27it/s]01/07/2022 12:47:04 - INFO - __main__ -   Batch Number = 132
01/07/2022 12:47:04 - INFO - __main__ -   Batch Number = 133
01/07/2022 12:47:04 - INFO - __main__ -   Batch Number = 134
01/07/2022 12:47:04 - INFO - __main__ -   Batch Number = 135
01/07/2022 12:47:04 - INFO - __main__ -   Batch Number = 136
Evaluating:  83%| | 136/163 [00:03<00:00, 39.13it/s]01/07/2022 12:47:04 - INFO - __main__ -   Batch Number = 137
01/07/2022 12:47:04 - INFO - __main__ -   Batch Number = 138
01/07/2022 12:47:04 - INFO - __main__ -   Batch Number = 139
01/07/2022 12:47:04 - INFO - __main__ -   Batch Number = 140
01/07/2022 12:47:04 - INFO - __main__ -   Batch Number = 141
Evaluating:  87%| | 141/163 [00:03<00:00, 40.45it/s]01/07/2022 12:47:04 - INFO - __main__ -   Batch Number = 142
01/07/2022 12:47:04 - INFO - __main__ -   Batch Number = 143
01/07/2022 12:47:04 - INFO - __main__ -   Batch Number = 144
01/07/2022 12:47:04 - INFO - __main__ -   Batch Number = 145
01/07/2022 12:47:04 - INFO - __main__ -   Batch Number = 146
Evaluating:  90%| | 146/163 [00:03<00:00, 41.60it/s]01/07/2022 12:47:04 - INFO - __main__ -   Batch Number = 147
01/07/2022 12:47:04 - INFO - __main__ -   Batch Number = 148
01/07/2022 12:47:04 - INFO - __main__ -   Batch Number = 149
01/07/2022 12:47:04 - INFO - __main__ -   Batch Number = 150
01/07/2022 12:47:04 - INFO - __main__ -   Batch Number = 151
Evaluating:  93%|| 151/163 [00:03<00:00, 42.53it/s]01/07/2022 12:47:04 - INFO - __main__ -   Batch Number = 152
01/07/2022 12:47:04 - INFO - __main__ -   Batch Number = 153
01/07/2022 12:47:04 - INFO - __main__ -   Batch Number = 154
01/07/2022 12:47:04 - INFO - __main__ -   Batch Number = 155
01/07/2022 12:47:04 - INFO - __main__ -   Batch Number = 156
Evaluating:  96%|| 156/163 [00:03<00:00, 42.96it/s]01/07/2022 12:47:04 - INFO - __main__ -   Batch Number = 157
01/07/2022 12:47:04 - INFO - __main__ -   Batch Number = 158
01/07/2022 12:47:04 - INFO - __main__ -   Batch Number = 159
01/07/2022 12:47:04 - INFO - __main__ -   Batch Number = 160
01/07/2022 12:47:04 - INFO - __main__ -   Batch Number = 161
Evaluating:  99%|| 161/163 [00:03<00:00, 43.35it/s]01/07/2022 12:47:04 - INFO - __main__ -   Batch Number = 162
01/07/2022 12:47:04 - INFO - __main__ -   Batch Number = 163
Evaluating: 100%|| 163/163 [00:03<00:00, 42.54it/s]
01/07/2022 12:47:04 - INFO - __main__ -     Evaluation done in total 3.831980 secs (0.002939 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/predictions_es_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/nbest_predictions_es_.json
01/07/2022 12:47:08 - INFO - __main__ -   Results = OrderedDict([('exact', 55.79831932773109), ('f1', 73.70603293660602), ('total', 1190), ('HasAns_exact', 55.79831932773109), ('HasAns_f1', 73.70603293660602), ('HasAns_total', 1190), ('best_exact', 55.79831932773109), ('best_exact_thresh', 0.0), ('best_f1', 73.70603293660602), ('best_f1_thresh', 0.0)])
01/07/2022 12:47:08 - INFO - __main__ -   Language adapter for hi not found, using en instead
01/07/2022 12:47:08 - INFO - __main__ -   Set active language adapter to en
01/07/2022 12:47:08 - INFO - __main__ -   Args Adapter Weight = None
01/07/2022 12:47:08 - INFO - __main__ -   Adapter Languages = ['en']
01/07/2022 12:47:08 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
  0%|          | 0/48 [00:00<?, ?it/s] 33%|      | 16/48 [00:00<00:00, 130.26it/s] 62%|   | 30/48 [00:00<00:00, 125.31it/s] 94%|| 45/48 [00:00<00:00, 134.36it/s]100%|| 48/48 [00:00<00:00, 134.10it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<03:52,  5.12it/s]convert squad examples to features:  24%|       | 289/1190 [00:00<00:00, 984.58it/s]convert squad examples to features:  33%|      | 398/1190 [00:00<00:01, 582.38it/s]convert squad examples to features:  67%|   | 801/1190 [00:00<00:00, 1265.61it/s]convert squad examples to features:  86%| | 1025/1190 [00:00<00:00, 1475.39it/s]convert squad examples to features: 100%|| 1190/1190 [00:00<00:00, 1260.87it/s]/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|| 1190/1190 [00:00<00:00, 450959.68it/s]
01/07/2022 12:47:10 - INFO - __main__ -   Local Rank = -1
01/07/2022 12:47:10 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.hi.json_xlm-roberta-base_384_hi
01/07/2022 12:47:11 - INFO - __main__ -   ***** Running evaluation  *****
01/07/2022 12:47:11 - INFO - __main__ -     Num examples = 1382
01/07/2022 12:47:11 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/173 [00:00<?, ?it/s]01/07/2022 12:47:11 - INFO - __main__ -   Batch Number = 1
01/07/2022 12:47:11 - INFO - __main__ -   Batch Number = 2
01/07/2022 12:47:11 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|         | 3/173 [00:00<00:05, 29.49it/s]01/07/2022 12:47:11 - INFO - __main__ -   Batch Number = 4
01/07/2022 12:47:11 - INFO - __main__ -   Batch Number = 5
01/07/2022 12:47:11 - INFO - __main__ -   Batch Number = 6
Evaluating:   3%|         | 6/173 [00:00<00:05, 29.54it/s]01/07/2022 12:47:11 - INFO - __main__ -   Batch Number = 7
01/07/2022 12:47:11 - INFO - __main__ -   Batch Number = 8
01/07/2022 12:47:11 - INFO - __main__ -   Batch Number = 9
01/07/2022 12:47:11 - INFO - __main__ -   Batch Number = 10
Evaluating:   6%|         | 10/173 [00:00<00:05, 32.33it/s]01/07/2022 12:47:11 - INFO - __main__ -   Batch Number = 11
01/07/2022 12:47:11 - INFO - __main__ -   Batch Number = 12
01/07/2022 12:47:11 - INFO - __main__ -   Batch Number = 13
01/07/2022 12:47:11 - INFO - __main__ -   Batch Number = 14
01/07/2022 12:47:11 - INFO - __main__ -   Batch Number = 15
Evaluating:   9%|         | 15/173 [00:00<00:04, 37.24it/s]01/07/2022 12:47:11 - INFO - __main__ -   Batch Number = 16
01/07/2022 12:47:11 - INFO - __main__ -   Batch Number = 17
01/07/2022 12:47:11 - INFO - __main__ -   Batch Number = 18
01/07/2022 12:47:12 - INFO - __main__ -   Batch Number = 19
01/07/2022 12:47:12 - INFO - __main__ -   Batch Number = 20
Evaluating:  12%|        | 20/173 [00:00<00:03, 39.70it/s]01/07/2022 12:47:12 - INFO - __main__ -   Batch Number = 21
01/07/2022 12:47:12 - INFO - __main__ -   Batch Number = 22
01/07/2022 12:47:12 - INFO - __main__ -   Batch Number = 23
01/07/2022 12:47:12 - INFO - __main__ -   Batch Number = 24
01/07/2022 12:47:12 - INFO - __main__ -   Batch Number = 25
Evaluating:  14%|        | 25/173 [00:00<00:03, 41.26it/s]01/07/2022 12:47:12 - INFO - __main__ -   Batch Number = 26
01/07/2022 12:47:12 - INFO - __main__ -   Batch Number = 27
01/07/2022 12:47:12 - INFO - __main__ -   Batch Number = 28
01/07/2022 12:47:12 - INFO - __main__ -   Batch Number = 29
01/07/2022 12:47:12 - INFO - __main__ -   Batch Number = 30
Evaluating:  17%|        | 30/173 [00:00<00:03, 42.38it/s]01/07/2022 12:47:12 - INFO - __main__ -   Batch Number = 31
01/07/2022 12:47:12 - INFO - __main__ -   Batch Number = 32
01/07/2022 12:47:12 - INFO - __main__ -   Batch Number = 33
01/07/2022 12:47:12 - INFO - __main__ -   Batch Number = 34
01/07/2022 12:47:12 - INFO - __main__ -   Batch Number = 35
Evaluating:  20%|        | 35/173 [00:00<00:03, 42.84it/s]01/07/2022 12:47:12 - INFO - __main__ -   Batch Number = 36
01/07/2022 12:47:12 - INFO - __main__ -   Batch Number = 37
01/07/2022 12:47:12 - INFO - __main__ -   Batch Number = 38
01/07/2022 12:47:12 - INFO - __main__ -   Batch Number = 39
01/07/2022 12:47:12 - INFO - __main__ -   Batch Number = 40
Evaluating:  23%|       | 40/173 [00:00<00:03, 43.21it/s]01/07/2022 12:47:12 - INFO - __main__ -   Batch Number = 41
01/07/2022 12:47:12 - INFO - __main__ -   Batch Number = 42
01/07/2022 12:47:12 - INFO - __main__ -   Batch Number = 43
01/07/2022 12:47:12 - INFO - __main__ -   Batch Number = 44
01/07/2022 12:47:12 - INFO - __main__ -   Batch Number = 45
Evaluating:  26%|       | 45/173 [00:01<00:02, 43.49it/s]01/07/2022 12:47:12 - INFO - __main__ -   Batch Number = 46
01/07/2022 12:47:12 - INFO - __main__ -   Batch Number = 47
01/07/2022 12:47:12 - INFO - __main__ -   Batch Number = 48
01/07/2022 12:47:12 - INFO - __main__ -   Batch Number = 49
01/07/2022 12:47:12 - INFO - __main__ -   Batch Number = 50
Evaluating:  29%|       | 50/173 [00:01<00:02, 43.59it/s]01/07/2022 12:47:12 - INFO - __main__ -   Batch Number = 51
01/07/2022 12:47:12 - INFO - __main__ -   Batch Number = 52
01/07/2022 12:47:12 - INFO - __main__ -   Batch Number = 53
01/07/2022 12:47:12 - INFO - __main__ -   Batch Number = 54
01/07/2022 12:47:12 - INFO - __main__ -   Batch Number = 55
Evaluating:  32%|      | 55/173 [00:01<00:02, 43.84it/s]01/07/2022 12:47:12 - INFO - __main__ -   Batch Number = 56
01/07/2022 12:47:12 - INFO - __main__ -   Batch Number = 57
01/07/2022 12:47:12 - INFO - __main__ -   Batch Number = 58
01/07/2022 12:47:12 - INFO - __main__ -   Batch Number = 59
01/07/2022 12:47:12 - INFO - __main__ -   Batch Number = 60
Evaluating:  35%|      | 60/173 [00:01<00:02, 43.95it/s]01/07/2022 12:47:12 - INFO - __main__ -   Batch Number = 61
01/07/2022 12:47:12 - INFO - __main__ -   Batch Number = 62
01/07/2022 12:47:13 - INFO - __main__ -   Batch Number = 63
01/07/2022 12:47:13 - INFO - __main__ -   Batch Number = 64
01/07/2022 12:47:13 - INFO - __main__ -   Batch Number = 65
Evaluating:  38%|      | 65/173 [00:01<00:02, 44.07it/s]01/07/2022 12:47:13 - INFO - __main__ -   Batch Number = 66
01/07/2022 12:47:13 - INFO - __main__ -   Batch Number = 67
01/07/2022 12:47:13 - INFO - __main__ -   Batch Number = 68
01/07/2022 12:47:13 - INFO - __main__ -   Batch Number = 69
01/07/2022 12:47:13 - INFO - __main__ -   Batch Number = 70
Evaluating:  40%|      | 70/173 [00:01<00:02, 44.04it/s]01/07/2022 12:47:13 - INFO - __main__ -   Batch Number = 71
01/07/2022 12:47:13 - INFO - __main__ -   Batch Number = 72
01/07/2022 12:47:13 - INFO - __main__ -   Batch Number = 73
01/07/2022 12:47:13 - INFO - __main__ -   Batch Number = 74
01/07/2022 12:47:13 - INFO - __main__ -   Batch Number = 75
Evaluating:  43%|     | 75/173 [00:01<00:02, 44.23it/s]01/07/2022 12:47:13 - INFO - __main__ -   Batch Number = 76
01/07/2022 12:47:13 - INFO - __main__ -   Batch Number = 77
01/07/2022 12:47:13 - INFO - __main__ -   Batch Number = 78
01/07/2022 12:47:13 - INFO - __main__ -   Batch Number = 79
01/07/2022 12:47:13 - INFO - __main__ -   Batch Number = 80
Evaluating:  46%|     | 80/173 [00:01<00:02, 44.21it/s]01/07/2022 12:47:13 - INFO - __main__ -   Batch Number = 81
01/07/2022 12:47:13 - INFO - __main__ -   Batch Number = 82
01/07/2022 12:47:13 - INFO - __main__ -   Batch Number = 83
01/07/2022 12:47:13 - INFO - __main__ -   Batch Number = 84
01/07/2022 12:47:13 - INFO - __main__ -   Batch Number = 85
Evaluating:  49%|     | 85/173 [00:02<00:01, 44.18it/s]01/07/2022 12:47:13 - INFO - __main__ -   Batch Number = 86
01/07/2022 12:47:13 - INFO - __main__ -   Batch Number = 87
01/07/2022 12:47:13 - INFO - __main__ -   Batch Number = 88
01/07/2022 12:47:13 - INFO - __main__ -   Batch Number = 89
01/07/2022 12:47:13 - INFO - __main__ -   Batch Number = 90
Evaluating:  52%|    | 90/173 [00:02<00:01, 44.31it/s]01/07/2022 12:47:13 - INFO - __main__ -   Batch Number = 91
01/07/2022 12:47:13 - INFO - __main__ -   Batch Number = 92
01/07/2022 12:47:13 - INFO - __main__ -   Batch Number = 93
01/07/2022 12:47:13 - INFO - __main__ -   Batch Number = 94
01/07/2022 12:47:13 - INFO - __main__ -   Batch Number = 95
Evaluating:  55%|    | 95/173 [00:02<00:01, 44.14it/s]01/07/2022 12:47:13 - INFO - __main__ -   Batch Number = 96
01/07/2022 12:47:13 - INFO - __main__ -   Batch Number = 97
01/07/2022 12:47:13 - INFO - __main__ -   Batch Number = 98
01/07/2022 12:47:13 - INFO - __main__ -   Batch Number = 99
01/07/2022 12:47:13 - INFO - __main__ -   Batch Number = 100
Evaluating:  58%|    | 100/173 [00:02<00:01, 44.18it/s]01/07/2022 12:47:13 - INFO - __main__ -   Batch Number = 101
01/07/2022 12:47:13 - INFO - __main__ -   Batch Number = 102
01/07/2022 12:47:13 - INFO - __main__ -   Batch Number = 103
01/07/2022 12:47:13 - INFO - __main__ -   Batch Number = 104
01/07/2022 12:47:13 - INFO - __main__ -   Batch Number = 105
Evaluating:  61%|    | 105/173 [00:02<00:01, 44.22it/s]01/07/2022 12:47:13 - INFO - __main__ -   Batch Number = 106
01/07/2022 12:47:13 - INFO - __main__ -   Batch Number = 107
01/07/2022 12:47:14 - INFO - __main__ -   Batch Number = 108
01/07/2022 12:47:14 - INFO - __main__ -   Batch Number = 109
01/07/2022 12:47:14 - INFO - __main__ -   Batch Number = 110
Evaluating:  64%|   | 110/173 [00:02<00:01, 44.01it/s]01/07/2022 12:47:14 - INFO - __main__ -   Batch Number = 111
01/07/2022 12:47:14 - INFO - __main__ -   Batch Number = 112
01/07/2022 12:47:14 - INFO - __main__ -   Batch Number = 113
01/07/2022 12:47:14 - INFO - __main__ -   Batch Number = 114
01/07/2022 12:47:14 - INFO - __main__ -   Batch Number = 115
Evaluating:  66%|   | 115/173 [00:02<00:01, 44.13it/s]01/07/2022 12:47:14 - INFO - __main__ -   Batch Number = 116
01/07/2022 12:47:14 - INFO - __main__ -   Batch Number = 117
01/07/2022 12:47:14 - INFO - __main__ -   Batch Number = 118
01/07/2022 12:47:14 - INFO - __main__ -   Batch Number = 119
01/07/2022 12:47:14 - INFO - __main__ -   Batch Number = 120
Evaluating:  69%|   | 120/173 [00:02<00:01, 44.18it/s]01/07/2022 12:47:14 - INFO - __main__ -   Batch Number = 121
01/07/2022 12:47:14 - INFO - __main__ -   Batch Number = 122
01/07/2022 12:47:14 - INFO - __main__ -   Batch Number = 123
01/07/2022 12:47:14 - INFO - __main__ -   Batch Number = 124
01/07/2022 12:47:14 - INFO - __main__ -   Batch Number = 125
Evaluating:  72%|  | 125/173 [00:02<00:01, 44.16it/s]01/07/2022 12:47:14 - INFO - __main__ -   Batch Number = 126
01/07/2022 12:47:14 - INFO - __main__ -   Batch Number = 127
01/07/2022 12:47:14 - INFO - __main__ -   Batch Number = 128
01/07/2022 12:47:14 - INFO - __main__ -   Batch Number = 129
01/07/2022 12:47:14 - INFO - __main__ -   Batch Number = 130
Evaluating:  75%|  | 130/173 [00:03<00:00, 44.26it/s]01/07/2022 12:47:14 - INFO - __main__ -   Batch Number = 131
01/07/2022 12:47:14 - INFO - __main__ -   Batch Number = 132
01/07/2022 12:47:14 - INFO - __main__ -   Batch Number = 133
01/07/2022 12:47:14 - INFO - __main__ -   Batch Number = 134
01/07/2022 12:47:14 - INFO - __main__ -   Batch Number = 135
Evaluating:  78%|  | 135/173 [00:03<00:00, 44.20it/s]01/07/2022 12:47:14 - INFO - __main__ -   Batch Number = 136
01/07/2022 12:47:14 - INFO - __main__ -   Batch Number = 137
01/07/2022 12:47:14 - INFO - __main__ -   Batch Number = 138
01/07/2022 12:47:14 - INFO - __main__ -   Batch Number = 139
01/07/2022 12:47:14 - INFO - __main__ -   Batch Number = 140
Evaluating:  81%|  | 140/173 [00:03<00:00, 44.22it/s]01/07/2022 12:47:14 - INFO - __main__ -   Batch Number = 141
01/07/2022 12:47:14 - INFO - __main__ -   Batch Number = 142
01/07/2022 12:47:14 - INFO - __main__ -   Batch Number = 143
01/07/2022 12:47:14 - INFO - __main__ -   Batch Number = 144
01/07/2022 12:47:14 - INFO - __main__ -   Batch Number = 145
Evaluating:  84%| | 145/173 [00:03<00:00, 44.28it/s]01/07/2022 12:47:14 - INFO - __main__ -   Batch Number = 146
01/07/2022 12:47:14 - INFO - __main__ -   Batch Number = 147
01/07/2022 12:47:14 - INFO - __main__ -   Batch Number = 148
01/07/2022 12:47:14 - INFO - __main__ -   Batch Number = 149
01/07/2022 12:47:14 - INFO - __main__ -   Batch Number = 150
Evaluating:  87%| | 150/173 [00:03<00:00, 44.46it/s]01/07/2022 12:47:14 - INFO - __main__ -   Batch Number = 151
01/07/2022 12:47:15 - INFO - __main__ -   Batch Number = 152
01/07/2022 12:47:15 - INFO - __main__ -   Batch Number = 153
01/07/2022 12:47:15 - INFO - __main__ -   Batch Number = 154
01/07/2022 12:47:15 - INFO - __main__ -   Batch Number = 155
Evaluating:  90%| | 155/173 [00:03<00:00, 44.31it/s]01/07/2022 12:47:15 - INFO - __main__ -   Batch Number = 156
01/07/2022 12:47:15 - INFO - __main__ -   Batch Number = 157
01/07/2022 12:47:15 - INFO - __main__ -   Batch Number = 158
01/07/2022 12:47:15 - INFO - __main__ -   Batch Number = 159
01/07/2022 12:47:15 - INFO - __main__ -   Batch Number = 160
Evaluating:  92%|| 160/173 [00:03<00:00, 44.36it/s]01/07/2022 12:47:15 - INFO - __main__ -   Batch Number = 161
01/07/2022 12:47:15 - INFO - __main__ -   Batch Number = 162
01/07/2022 12:47:15 - INFO - __main__ -   Batch Number = 163
01/07/2022 12:47:15 - INFO - __main__ -   Batch Number = 164
01/07/2022 12:47:15 - INFO - __main__ -   Batch Number = 165
Evaluating:  95%|| 165/173 [00:03<00:00, 44.47it/s]01/07/2022 12:47:15 - INFO - __main__ -   Batch Number = 166
01/07/2022 12:47:15 - INFO - __main__ -   Batch Number = 167
01/07/2022 12:47:15 - INFO - __main__ -   Batch Number = 168
01/07/2022 12:47:15 - INFO - __main__ -   Batch Number = 169
01/07/2022 12:47:15 - INFO - __main__ -   Batch Number = 170
Evaluating:  98%|| 170/173 [00:03<00:00, 42.74it/s]01/07/2022 12:47:15 - INFO - __main__ -   Batch Number = 171
01/07/2022 12:47:15 - INFO - __main__ -   Batch Number = 172
01/07/2022 12:47:15 - INFO - __main__ -   Batch Number = 173
Evaluating: 100%|| 173/173 [00:04<00:00, 43.13it/s]
01/07/2022 12:47:15 - INFO - __main__ -     Evaluation done in total 4.011741 secs (0.002903 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/predictions_hi_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/nbest_predictions_hi_.json
01/07/2022 12:47:18 - INFO - __main__ -   Results = OrderedDict([('exact', 47.73109243697479), ('f1', 65.11131556503332), ('total', 1190), ('HasAns_exact', 47.73109243697479), ('HasAns_f1', 65.11131556503332), ('HasAns_total', 1190), ('best_exact', 47.73109243697479), ('best_exact_thresh', 0.0), ('best_f1', 65.11131556503332), ('best_f1_thresh', 0.0)])
01/07/2022 12:47:18 - INFO - __main__ -   Language adapter for ru not found, using en instead
01/07/2022 12:47:18 - INFO - __main__ -   Set active language adapter to en
01/07/2022 12:47:18 - INFO - __main__ -   Args Adapter Weight = None
01/07/2022 12:47:18 - INFO - __main__ -   Adapter Languages = ['en']
01/07/2022 12:47:18 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
  0%|          | 0/48 [00:00<?, ?it/s] 29%|       | 14/48 [00:00<00:00, 139.69it/s] 58%|    | 28/48 [00:00<00:00, 107.19it/s] 83%| | 40/48 [00:00<00:00, 111.26it/s]100%|| 48/48 [00:00<00:00, 116.48it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<04:13,  4.70it/s]convert squad examples to features:   5%|         | 65/1190 [00:00<00:05, 201.57it/s]convert squad examples to features:  32%|      | 385/1190 [00:00<00:01, 642.06it/s]convert squad examples to features:  94%|| 1121/1190 [00:00<00:00, 1949.68it/s]convert squad examples to features: 100%|| 1190/1190 [00:00<00:00, 1345.23it/s]/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|| 1190/1190 [00:00<00:00, 621339.69it/s]
01/07/2022 12:47:20 - INFO - __main__ -   Local Rank = -1
01/07/2022 12:47:20 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.ru.json_xlm-roberta-base_384_ru
01/07/2022 12:47:21 - INFO - __main__ -   ***** Running evaluation  *****
01/07/2022 12:47:21 - INFO - __main__ -     Num examples = 1332
01/07/2022 12:47:21 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/167 [00:00<?, ?it/s]01/07/2022 12:47:21 - INFO - __main__ -   Batch Number = 1
01/07/2022 12:47:21 - INFO - __main__ -   Batch Number = 2
01/07/2022 12:47:21 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|         | 3/167 [00:00<00:05, 29.36it/s]01/07/2022 12:47:21 - INFO - __main__ -   Batch Number = 4
01/07/2022 12:47:21 - INFO - __main__ -   Batch Number = 5
01/07/2022 12:47:21 - INFO - __main__ -   Batch Number = 6
01/07/2022 12:47:21 - INFO - __main__ -   Batch Number = 7
Evaluating:   4%|         | 7/167 [00:00<00:04, 34.22it/s]01/07/2022 12:47:21 - INFO - __main__ -   Batch Number = 8
01/07/2022 12:47:21 - INFO - __main__ -   Batch Number = 9
01/07/2022 12:47:21 - INFO - __main__ -   Batch Number = 10
01/07/2022 12:47:21 - INFO - __main__ -   Batch Number = 11
01/07/2022 12:47:22 - INFO - __main__ -   Batch Number = 12
Evaluating:   7%|         | 12/167 [00:00<00:04, 38.74it/s]01/07/2022 12:47:22 - INFO - __main__ -   Batch Number = 13
01/07/2022 12:47:22 - INFO - __main__ -   Batch Number = 14
01/07/2022 12:47:22 - INFO - __main__ -   Batch Number = 15
01/07/2022 12:47:22 - INFO - __main__ -   Batch Number = 16
01/07/2022 12:47:22 - INFO - __main__ -   Batch Number = 17
Evaluating:  10%|         | 17/167 [00:00<00:03, 40.47it/s]01/07/2022 12:47:22 - INFO - __main__ -   Batch Number = 18
01/07/2022 12:47:22 - INFO - __main__ -   Batch Number = 19
01/07/2022 12:47:22 - INFO - __main__ -   Batch Number = 20
01/07/2022 12:47:22 - INFO - __main__ -   Batch Number = 21
01/07/2022 12:47:22 - INFO - __main__ -   Batch Number = 22
Evaluating:  13%|        | 22/167 [00:00<00:03, 42.05it/s]01/07/2022 12:47:22 - INFO - __main__ -   Batch Number = 23
01/07/2022 12:47:22 - INFO - __main__ -   Batch Number = 24
01/07/2022 12:47:22 - INFO - __main__ -   Batch Number = 25
01/07/2022 12:47:22 - INFO - __main__ -   Batch Number = 26
01/07/2022 12:47:22 - INFO - __main__ -   Batch Number = 27
Evaluating:  16%|        | 27/167 [00:00<00:03, 42.93it/s]01/07/2022 12:47:22 - INFO - __main__ -   Batch Number = 28
01/07/2022 12:47:22 - INFO - __main__ -   Batch Number = 29
01/07/2022 12:47:22 - INFO - __main__ -   Batch Number = 30
01/07/2022 12:47:22 - INFO - __main__ -   Batch Number = 31
01/07/2022 12:47:22 - INFO - __main__ -   Batch Number = 32
Evaluating:  19%|        | 32/167 [00:00<00:03, 43.50it/s]01/07/2022 12:47:22 - INFO - __main__ -   Batch Number = 33
01/07/2022 12:47:22 - INFO - __main__ -   Batch Number = 34
01/07/2022 12:47:22 - INFO - __main__ -   Batch Number = 35
01/07/2022 12:47:22 - INFO - __main__ -   Batch Number = 36
01/07/2022 12:47:22 - INFO - __main__ -   Batch Number = 37
Evaluating:  22%|       | 37/167 [00:00<00:02, 43.87it/s]01/07/2022 12:47:22 - INFO - __main__ -   Batch Number = 38
01/07/2022 12:47:22 - INFO - __main__ -   Batch Number = 39
01/07/2022 12:47:22 - INFO - __main__ -   Batch Number = 40
01/07/2022 12:47:22 - INFO - __main__ -   Batch Number = 41
01/07/2022 12:47:22 - INFO - __main__ -   Batch Number = 42
Evaluating:  25%|       | 42/167 [00:00<00:02, 44.03it/s]01/07/2022 12:47:22 - INFO - __main__ -   Batch Number = 43
01/07/2022 12:47:22 - INFO - __main__ -   Batch Number = 44
01/07/2022 12:47:22 - INFO - __main__ -   Batch Number = 45
01/07/2022 12:47:22 - INFO - __main__ -   Batch Number = 46
01/07/2022 12:47:22 - INFO - __main__ -   Batch Number = 47
Evaluating:  28%|       | 47/167 [00:01<00:02, 44.12it/s]01/07/2022 12:47:22 - INFO - __main__ -   Batch Number = 48
01/07/2022 12:47:22 - INFO - __main__ -   Batch Number = 49
01/07/2022 12:47:22 - INFO - __main__ -   Batch Number = 50
01/07/2022 12:47:22 - INFO - __main__ -   Batch Number = 51
01/07/2022 12:47:22 - INFO - __main__ -   Batch Number = 52
Evaluating:  31%|       | 52/167 [00:01<00:02, 44.31it/s]01/07/2022 12:47:22 - INFO - __main__ -   Batch Number = 53
01/07/2022 12:47:22 - INFO - __main__ -   Batch Number = 54
01/07/2022 12:47:22 - INFO - __main__ -   Batch Number = 55
01/07/2022 12:47:23 - INFO - __main__ -   Batch Number = 56
01/07/2022 12:47:23 - INFO - __main__ -   Batch Number = 57
Evaluating:  34%|      | 57/167 [00:01<00:02, 44.42it/s]01/07/2022 12:47:23 - INFO - __main__ -   Batch Number = 58
01/07/2022 12:47:23 - INFO - __main__ -   Batch Number = 59
01/07/2022 12:47:23 - INFO - __main__ -   Batch Number = 60
01/07/2022 12:47:23 - INFO - __main__ -   Batch Number = 61
01/07/2022 12:47:23 - INFO - __main__ -   Batch Number = 62
Evaluating:  37%|      | 62/167 [00:01<00:02, 44.31it/s]01/07/2022 12:47:23 - INFO - __main__ -   Batch Number = 63
01/07/2022 12:47:23 - INFO - __main__ -   Batch Number = 64
01/07/2022 12:47:23 - INFO - __main__ -   Batch Number = 65
01/07/2022 12:47:23 - INFO - __main__ -   Batch Number = 66
01/07/2022 12:47:23 - INFO - __main__ -   Batch Number = 67
Evaluating:  40%|      | 67/167 [00:01<00:02, 44.41it/s]01/07/2022 12:47:23 - INFO - __main__ -   Batch Number = 68
01/07/2022 12:47:23 - INFO - __main__ -   Batch Number = 69
01/07/2022 12:47:23 - INFO - __main__ -   Batch Number = 70
01/07/2022 12:47:23 - INFO - __main__ -   Batch Number = 71
01/07/2022 12:47:23 - INFO - __main__ -   Batch Number = 72
Evaluating:  43%|     | 72/167 [00:01<00:02, 44.39it/s]01/07/2022 12:47:23 - INFO - __main__ -   Batch Number = 73
01/07/2022 12:47:23 - INFO - __main__ -   Batch Number = 74
01/07/2022 12:47:23 - INFO - __main__ -   Batch Number = 75
01/07/2022 12:47:23 - INFO - __main__ -   Batch Number = 76
01/07/2022 12:47:23 - INFO - __main__ -   Batch Number = 77
Evaluating:  46%|     | 77/167 [00:01<00:02, 44.26it/s]01/07/2022 12:47:23 - INFO - __main__ -   Batch Number = 78
01/07/2022 12:47:23 - INFO - __main__ -   Batch Number = 79
01/07/2022 12:47:23 - INFO - __main__ -   Batch Number = 80
01/07/2022 12:47:23 - INFO - __main__ -   Batch Number = 81
01/07/2022 12:47:23 - INFO - __main__ -   Batch Number = 82
Evaluating:  49%|     | 82/167 [00:01<00:01, 44.28it/s]01/07/2022 12:47:23 - INFO - __main__ -   Batch Number = 83
01/07/2022 12:47:23 - INFO - __main__ -   Batch Number = 84
01/07/2022 12:47:23 - INFO - __main__ -   Batch Number = 85
01/07/2022 12:47:23 - INFO - __main__ -   Batch Number = 86
01/07/2022 12:47:23 - INFO - __main__ -   Batch Number = 87
Evaluating:  52%|    | 87/167 [00:02<00:01, 44.35it/s]01/07/2022 12:47:23 - INFO - __main__ -   Batch Number = 88
01/07/2022 12:47:23 - INFO - __main__ -   Batch Number = 89
01/07/2022 12:47:23 - INFO - __main__ -   Batch Number = 90
01/07/2022 12:47:23 - INFO - __main__ -   Batch Number = 91
01/07/2022 12:47:23 - INFO - __main__ -   Batch Number = 92
Evaluating:  55%|    | 92/167 [00:02<00:01, 44.35it/s]01/07/2022 12:47:23 - INFO - __main__ -   Batch Number = 93
01/07/2022 12:47:23 - INFO - __main__ -   Batch Number = 94
01/07/2022 12:47:23 - INFO - __main__ -   Batch Number = 95
01/07/2022 12:47:23 - INFO - __main__ -   Batch Number = 96
01/07/2022 12:47:23 - INFO - __main__ -   Batch Number = 97
Evaluating:  58%|    | 97/167 [00:02<00:01, 44.37it/s]01/07/2022 12:47:23 - INFO - __main__ -   Batch Number = 98
01/07/2022 12:47:23 - INFO - __main__ -   Batch Number = 99
01/07/2022 12:47:23 - INFO - __main__ -   Batch Number = 100
01/07/2022 12:47:24 - INFO - __main__ -   Batch Number = 101
01/07/2022 12:47:24 - INFO - __main__ -   Batch Number = 102
Evaluating:  61%|    | 102/167 [00:02<00:01, 44.43it/s]01/07/2022 12:47:24 - INFO - __main__ -   Batch Number = 103
01/07/2022 12:47:24 - INFO - __main__ -   Batch Number = 104
01/07/2022 12:47:24 - INFO - __main__ -   Batch Number = 105
01/07/2022 12:47:24 - INFO - __main__ -   Batch Number = 106
01/07/2022 12:47:24 - INFO - __main__ -   Batch Number = 107
Evaluating:  64%|   | 107/167 [00:02<00:01, 44.20it/s]01/07/2022 12:47:24 - INFO - __main__ -   Batch Number = 108
01/07/2022 12:47:24 - INFO - __main__ -   Batch Number = 109
01/07/2022 12:47:24 - INFO - __main__ -   Batch Number = 110
01/07/2022 12:47:24 - INFO - __main__ -   Batch Number = 111
01/07/2022 12:47:24 - INFO - __main__ -   Batch Number = 112
Evaluating:  67%|   | 112/167 [00:02<00:01, 44.28it/s]01/07/2022 12:47:24 - INFO - __main__ -   Batch Number = 113
01/07/2022 12:47:24 - INFO - __main__ -   Batch Number = 114
01/07/2022 12:47:24 - INFO - __main__ -   Batch Number = 115
01/07/2022 12:47:24 - INFO - __main__ -   Batch Number = 116
01/07/2022 12:47:24 - INFO - __main__ -   Batch Number = 117
Evaluating:  70%|   | 117/167 [00:02<00:01, 44.28it/s]01/07/2022 12:47:24 - INFO - __main__ -   Batch Number = 118
01/07/2022 12:47:24 - INFO - __main__ -   Batch Number = 119
01/07/2022 12:47:24 - INFO - __main__ -   Batch Number = 120
01/07/2022 12:47:24 - INFO - __main__ -   Batch Number = 121
01/07/2022 12:47:24 - INFO - __main__ -   Batch Number = 122
Evaluating:  73%|  | 122/167 [00:02<00:01, 42.49it/s]01/07/2022 12:47:24 - INFO - __main__ -   Batch Number = 123
01/07/2022 12:47:24 - INFO - __main__ -   Batch Number = 124
01/07/2022 12:47:24 - INFO - __main__ -   Batch Number = 125
01/07/2022 12:47:24 - INFO - __main__ -   Batch Number = 126
01/07/2022 12:47:24 - INFO - __main__ -   Batch Number = 127
Evaluating:  76%|  | 127/167 [00:02<00:00, 42.98it/s]01/07/2022 12:47:24 - INFO - __main__ -   Batch Number = 128
01/07/2022 12:47:24 - INFO - __main__ -   Batch Number = 129
01/07/2022 12:47:24 - INFO - __main__ -   Batch Number = 130
01/07/2022 12:47:24 - INFO - __main__ -   Batch Number = 131
01/07/2022 12:47:24 - INFO - __main__ -   Batch Number = 132
Evaluating:  79%|  | 132/167 [00:03<00:00, 43.48it/s]01/07/2022 12:47:24 - INFO - __main__ -   Batch Number = 133
01/07/2022 12:47:24 - INFO - __main__ -   Batch Number = 134
01/07/2022 12:47:24 - INFO - __main__ -   Batch Number = 135
01/07/2022 12:47:24 - INFO - __main__ -   Batch Number = 136
01/07/2022 12:47:24 - INFO - __main__ -   Batch Number = 137
Evaluating:  82%| | 137/167 [00:03<00:00, 43.69it/s]01/07/2022 12:47:24 - INFO - __main__ -   Batch Number = 138
01/07/2022 12:47:24 - INFO - __main__ -   Batch Number = 139
01/07/2022 12:47:24 - INFO - __main__ -   Batch Number = 140
01/07/2022 12:47:24 - INFO - __main__ -   Batch Number = 141
01/07/2022 12:47:24 - INFO - __main__ -   Batch Number = 142
Evaluating:  85%| | 142/167 [00:03<00:00, 44.03it/s]01/07/2022 12:47:24 - INFO - __main__ -   Batch Number = 143
01/07/2022 12:47:25 - INFO - __main__ -   Batch Number = 144
01/07/2022 12:47:25 - INFO - __main__ -   Batch Number = 145
01/07/2022 12:47:25 - INFO - __main__ -   Batch Number = 146
01/07/2022 12:47:25 - INFO - __main__ -   Batch Number = 147
Evaluating:  88%| | 147/167 [00:03<00:00, 44.24it/s]01/07/2022 12:47:25 - INFO - __main__ -   Batch Number = 148
01/07/2022 12:47:25 - INFO - __main__ -   Batch Number = 149
01/07/2022 12:47:25 - INFO - __main__ -   Batch Number = 150
01/07/2022 12:47:25 - INFO - __main__ -   Batch Number = 151
01/07/2022 12:47:25 - INFO - __main__ -   Batch Number = 152
Evaluating:  91%| | 152/167 [00:03<00:00, 44.07it/s]01/07/2022 12:47:25 - INFO - __main__ -   Batch Number = 153
01/07/2022 12:47:25 - INFO - __main__ -   Batch Number = 154
01/07/2022 12:47:25 - INFO - __main__ -   Batch Number = 155
01/07/2022 12:47:25 - INFO - __main__ -   Batch Number = 156
01/07/2022 12:47:25 - INFO - __main__ -   Batch Number = 157
Evaluating:  94%|| 157/167 [00:03<00:00, 44.32it/s]01/07/2022 12:47:25 - INFO - __main__ -   Batch Number = 158
01/07/2022 12:47:25 - INFO - __main__ -   Batch Number = 159
01/07/2022 12:47:25 - INFO - __main__ -   Batch Number = 160
01/07/2022 12:47:25 - INFO - __main__ -   Batch Number = 161
01/07/2022 12:47:25 - INFO - __main__ -   Batch Number = 162
Evaluating:  97%|| 162/167 [00:03<00:00, 44.34it/s]01/07/2022 12:47:25 - INFO - __main__ -   Batch Number = 163
01/07/2022 12:47:25 - INFO - __main__ -   Batch Number = 164
01/07/2022 12:47:25 - INFO - __main__ -   Batch Number = 165
01/07/2022 12:47:25 - INFO - __main__ -   Batch Number = 166
01/07/2022 12:47:25 - INFO - __main__ -   Batch Number = 167
Evaluating: 100%|| 167/167 [00:03<00:00, 45.12it/s]Evaluating: 100%|| 167/167 [00:03<00:00, 43.65it/s]
01/07/2022 12:47:25 - INFO - __main__ -     Evaluation done in total 3.826388 secs (0.002873 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/predictions_ru_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/nbest_predictions_ru_.json
01/07/2022 12:47:29 - INFO - __main__ -   Results = OrderedDict([('exact', 54.87394957983193), ('f1', 71.4950540971754), ('total', 1190), ('HasAns_exact', 54.87394957983193), ('HasAns_f1', 71.4950540971754), ('HasAns_total', 1190), ('best_exact', 54.87394957983193), ('best_exact_thresh', 0.0), ('best_f1', 71.4950540971754), ('best_f1_thresh', 0.0)])
01/07/2022 12:47:29 - INFO - __main__ -   Language adapter for th not found, using en instead
01/07/2022 12:47:29 - INFO - __main__ -   Set active language adapter to en
01/07/2022 12:47:29 - INFO - __main__ -   Args Adapter Weight = None
01/07/2022 12:47:29 - INFO - __main__ -   Adapter Languages = ['en']
01/07/2022 12:47:29 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
  0%|          | 0/48 [00:00<?, ?it/s] 31%|      | 15/48 [00:00<00:00, 148.90it/s] 62%|   | 30/48 [00:00<00:00, 112.38it/s] 92%|| 44/48 [00:00<00:00, 122.76it/s]100%|| 48/48 [00:00<00:00, 124.34it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<06:48,  2.91it/s]convert squad examples to features:  32%|      | 385/1190 [00:00<00:01, 561.91it/s]convert squad examples to features: 100%|| 1190/1190 [00:00<00:00, 1509.00it/s]/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|| 1190/1190 [00:00<00:00, 590886.91it/s]
01/07/2022 12:47:31 - INFO - __main__ -   Local Rank = -1
01/07/2022 12:47:31 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.th.json_xlm-roberta-base_384_th
01/07/2022 12:47:32 - INFO - __main__ -   ***** Running evaluation  *****
01/07/2022 12:47:32 - INFO - __main__ -     Num examples = 1314
01/07/2022 12:47:32 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/165 [00:00<?, ?it/s]01/07/2022 12:47:32 - INFO - __main__ -   Batch Number = 1
01/07/2022 12:47:32 - INFO - __main__ -   Batch Number = 2
01/07/2022 12:47:32 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|         | 3/165 [00:00<00:05, 29.15it/s]01/07/2022 12:47:32 - INFO - __main__ -   Batch Number = 4
01/07/2022 12:47:32 - INFO - __main__ -   Batch Number = 5
01/07/2022 12:47:32 - INFO - __main__ -   Batch Number = 6
Evaluating:   4%|         | 6/165 [00:00<00:05, 29.17it/s]01/07/2022 12:47:32 - INFO - __main__ -   Batch Number = 7
01/07/2022 12:47:32 - INFO - __main__ -   Batch Number = 8
01/07/2022 12:47:32 - INFO - __main__ -   Batch Number = 9
01/07/2022 12:47:32 - INFO - __main__ -   Batch Number = 10
Evaluating:   6%|         | 10/165 [00:00<00:04, 31.39it/s]01/07/2022 12:47:32 - INFO - __main__ -   Batch Number = 11
01/07/2022 12:47:32 - INFO - __main__ -   Batch Number = 12
01/07/2022 12:47:32 - INFO - __main__ -   Batch Number = 13
01/07/2022 12:47:32 - INFO - __main__ -   Batch Number = 14
01/07/2022 12:47:32 - INFO - __main__ -   Batch Number = 15
Evaluating:   9%|         | 15/165 [00:00<00:04, 36.55it/s]01/07/2022 12:47:32 - INFO - __main__ -   Batch Number = 16
01/07/2022 12:47:32 - INFO - __main__ -   Batch Number = 17
01/07/2022 12:47:32 - INFO - __main__ -   Batch Number = 18
01/07/2022 12:47:32 - INFO - __main__ -   Batch Number = 19
01/07/2022 12:47:32 - INFO - __main__ -   Batch Number = 20
Evaluating:  12%|        | 20/165 [00:00<00:03, 39.30it/s]01/07/2022 12:47:32 - INFO - __main__ -   Batch Number = 21
01/07/2022 12:47:32 - INFO - __main__ -   Batch Number = 22
01/07/2022 12:47:32 - INFO - __main__ -   Batch Number = 23
01/07/2022 12:47:32 - INFO - __main__ -   Batch Number = 24
01/07/2022 12:47:32 - INFO - __main__ -   Batch Number = 25
Evaluating:  15%|        | 25/165 [00:00<00:03, 41.30it/s]01/07/2022 12:47:32 - INFO - __main__ -   Batch Number = 26
01/07/2022 12:47:32 - INFO - __main__ -   Batch Number = 27
01/07/2022 12:47:32 - INFO - __main__ -   Batch Number = 28
01/07/2022 12:47:32 - INFO - __main__ -   Batch Number = 29
01/07/2022 12:47:33 - INFO - __main__ -   Batch Number = 30
Evaluating:  18%|        | 30/165 [00:00<00:03, 42.64it/s]01/07/2022 12:47:33 - INFO - __main__ -   Batch Number = 31
01/07/2022 12:47:33 - INFO - __main__ -   Batch Number = 32
01/07/2022 12:47:33 - INFO - __main__ -   Batch Number = 33
01/07/2022 12:47:33 - INFO - __main__ -   Batch Number = 34
01/07/2022 12:47:33 - INFO - __main__ -   Batch Number = 35
Evaluating:  21%|        | 35/165 [00:00<00:03, 43.17it/s]01/07/2022 12:47:33 - INFO - __main__ -   Batch Number = 36
01/07/2022 12:47:33 - INFO - __main__ -   Batch Number = 37
01/07/2022 12:47:33 - INFO - __main__ -   Batch Number = 38
01/07/2022 12:47:33 - INFO - __main__ -   Batch Number = 39
01/07/2022 12:47:33 - INFO - __main__ -   Batch Number = 40
Evaluating:  24%|       | 40/165 [00:00<00:02, 43.74it/s]01/07/2022 12:47:33 - INFO - __main__ -   Batch Number = 41
01/07/2022 12:47:33 - INFO - __main__ -   Batch Number = 42
01/07/2022 12:47:33 - INFO - __main__ -   Batch Number = 43
01/07/2022 12:47:33 - INFO - __main__ -   Batch Number = 44
01/07/2022 12:47:33 - INFO - __main__ -   Batch Number = 45
Evaluating:  27%|       | 45/165 [00:01<00:02, 43.97it/s]01/07/2022 12:47:33 - INFO - __main__ -   Batch Number = 46
01/07/2022 12:47:33 - INFO - __main__ -   Batch Number = 47
01/07/2022 12:47:33 - INFO - __main__ -   Batch Number = 48
01/07/2022 12:47:33 - INFO - __main__ -   Batch Number = 49
01/07/2022 12:47:33 - INFO - __main__ -   Batch Number = 50
Evaluating:  30%|       | 50/165 [00:01<00:02, 44.14it/s]01/07/2022 12:47:33 - INFO - __main__ -   Batch Number = 51
01/07/2022 12:47:33 - INFO - __main__ -   Batch Number = 52
01/07/2022 12:47:33 - INFO - __main__ -   Batch Number = 53
01/07/2022 12:47:33 - INFO - __main__ -   Batch Number = 54
01/07/2022 12:47:33 - INFO - __main__ -   Batch Number = 55
Evaluating:  33%|      | 55/165 [00:01<00:02, 44.27it/s]01/07/2022 12:47:33 - INFO - __main__ -   Batch Number = 56
01/07/2022 12:47:33 - INFO - __main__ -   Batch Number = 57
01/07/2022 12:47:33 - INFO - __main__ -   Batch Number = 58
01/07/2022 12:47:33 - INFO - __main__ -   Batch Number = 59
01/07/2022 12:47:33 - INFO - __main__ -   Batch Number = 60
Evaluating:  36%|      | 60/165 [00:01<00:02, 44.44it/s]01/07/2022 12:47:33 - INFO - __main__ -   Batch Number = 61
01/07/2022 12:47:33 - INFO - __main__ -   Batch Number = 62
01/07/2022 12:47:33 - INFO - __main__ -   Batch Number = 63
01/07/2022 12:47:33 - INFO - __main__ -   Batch Number = 64
01/07/2022 12:47:33 - INFO - __main__ -   Batch Number = 65
Evaluating:  39%|      | 65/165 [00:01<00:02, 44.38it/s]01/07/2022 12:47:33 - INFO - __main__ -   Batch Number = 66
01/07/2022 12:47:33 - INFO - __main__ -   Batch Number = 67
01/07/2022 12:47:33 - INFO - __main__ -   Batch Number = 68
01/07/2022 12:47:33 - INFO - __main__ -   Batch Number = 69
01/07/2022 12:47:33 - INFO - __main__ -   Batch Number = 70
Evaluating:  42%|     | 70/165 [00:01<00:02, 44.52it/s]01/07/2022 12:47:33 - INFO - __main__ -   Batch Number = 71
01/07/2022 12:47:33 - INFO - __main__ -   Batch Number = 72
01/07/2022 12:47:33 - INFO - __main__ -   Batch Number = 73
01/07/2022 12:47:33 - INFO - __main__ -   Batch Number = 74
01/07/2022 12:47:34 - INFO - __main__ -   Batch Number = 75
Evaluating:  45%|     | 75/165 [00:01<00:02, 44.56it/s]01/07/2022 12:47:34 - INFO - __main__ -   Batch Number = 76
01/07/2022 12:47:34 - INFO - __main__ -   Batch Number = 77
01/07/2022 12:47:34 - INFO - __main__ -   Batch Number = 78
01/07/2022 12:47:34 - INFO - __main__ -   Batch Number = 79
01/07/2022 12:47:34 - INFO - __main__ -   Batch Number = 80
Evaluating:  48%|     | 80/165 [00:01<00:01, 44.53it/s]01/07/2022 12:47:34 - INFO - __main__ -   Batch Number = 81
01/07/2022 12:47:34 - INFO - __main__ -   Batch Number = 82
01/07/2022 12:47:34 - INFO - __main__ -   Batch Number = 83
01/07/2022 12:47:34 - INFO - __main__ -   Batch Number = 84
01/07/2022 12:47:34 - INFO - __main__ -   Batch Number = 85
Evaluating:  52%|    | 85/165 [00:02<00:01, 44.44it/s]01/07/2022 12:47:34 - INFO - __main__ -   Batch Number = 86
01/07/2022 12:47:34 - INFO - __main__ -   Batch Number = 87
01/07/2022 12:47:34 - INFO - __main__ -   Batch Number = 88
01/07/2022 12:47:34 - INFO - __main__ -   Batch Number = 89
01/07/2022 12:47:34 - INFO - __main__ -   Batch Number = 90
Evaluating:  55%|    | 90/165 [00:02<00:01, 44.43it/s]01/07/2022 12:47:34 - INFO - __main__ -   Batch Number = 91
01/07/2022 12:47:34 - INFO - __main__ -   Batch Number = 92
01/07/2022 12:47:34 - INFO - __main__ -   Batch Number = 93
01/07/2022 12:47:34 - INFO - __main__ -   Batch Number = 94
01/07/2022 12:47:34 - INFO - __main__ -   Batch Number = 95
Evaluating:  58%|    | 95/165 [00:02<00:01, 42.55it/s]01/07/2022 12:47:34 - INFO - __main__ -   Batch Number = 96
01/07/2022 12:47:34 - INFO - __main__ -   Batch Number = 97
01/07/2022 12:47:34 - INFO - __main__ -   Batch Number = 98
01/07/2022 12:47:34 - INFO - __main__ -   Batch Number = 99
01/07/2022 12:47:34 - INFO - __main__ -   Batch Number = 100
Evaluating:  61%|    | 100/165 [00:02<00:01, 42.86it/s]01/07/2022 12:47:34 - INFO - __main__ -   Batch Number = 101
01/07/2022 12:47:34 - INFO - __main__ -   Batch Number = 102
01/07/2022 12:47:34 - INFO - __main__ -   Batch Number = 103
01/07/2022 12:47:34 - INFO - __main__ -   Batch Number = 104
01/07/2022 12:47:34 - INFO - __main__ -   Batch Number = 105
Evaluating:  64%|   | 105/165 [00:02<00:01, 43.28it/s]01/07/2022 12:47:34 - INFO - __main__ -   Batch Number = 106
01/07/2022 12:47:34 - INFO - __main__ -   Batch Number = 107
01/07/2022 12:47:34 - INFO - __main__ -   Batch Number = 108
01/07/2022 12:47:34 - INFO - __main__ -   Batch Number = 109
01/07/2022 12:47:34 - INFO - __main__ -   Batch Number = 110
Evaluating:  67%|   | 110/165 [00:02<00:01, 43.22it/s]01/07/2022 12:47:34 - INFO - __main__ -   Batch Number = 111
01/07/2022 12:47:34 - INFO - __main__ -   Batch Number = 112
01/07/2022 12:47:34 - INFO - __main__ -   Batch Number = 113
01/07/2022 12:47:34 - INFO - __main__ -   Batch Number = 114
01/07/2022 12:47:34 - INFO - __main__ -   Batch Number = 115
Evaluating:  70%|   | 115/165 [00:02<00:01, 43.22it/s]01/07/2022 12:47:34 - INFO - __main__ -   Batch Number = 116
01/07/2022 12:47:34 - INFO - __main__ -   Batch Number = 117
01/07/2022 12:47:35 - INFO - __main__ -   Batch Number = 118
01/07/2022 12:47:35 - INFO - __main__ -   Batch Number = 119
01/07/2022 12:47:35 - INFO - __main__ -   Batch Number = 120
Evaluating:  73%|  | 120/165 [00:02<00:01, 43.22it/s]01/07/2022 12:47:35 - INFO - __main__ -   Batch Number = 121
01/07/2022 12:47:35 - INFO - __main__ -   Batch Number = 122
01/07/2022 12:47:35 - INFO - __main__ -   Batch Number = 123
01/07/2022 12:47:35 - INFO - __main__ -   Batch Number = 124
01/07/2022 12:47:35 - INFO - __main__ -   Batch Number = 125
Evaluating:  76%|  | 125/165 [00:02<00:00, 43.16it/s]01/07/2022 12:47:35 - INFO - __main__ -   Batch Number = 126
01/07/2022 12:47:35 - INFO - __main__ -   Batch Number = 127
01/07/2022 12:47:35 - INFO - __main__ -   Batch Number = 128
01/07/2022 12:47:35 - INFO - __main__ -   Batch Number = 129
01/07/2022 12:47:35 - INFO - __main__ -   Batch Number = 130
Evaluating:  79%|  | 130/165 [00:03<00:00, 43.19it/s]01/07/2022 12:47:35 - INFO - __main__ -   Batch Number = 131
01/07/2022 12:47:35 - INFO - __main__ -   Batch Number = 132
01/07/2022 12:47:35 - INFO - __main__ -   Batch Number = 133
01/07/2022 12:47:35 - INFO - __main__ -   Batch Number = 134
01/07/2022 12:47:35 - INFO - __main__ -   Batch Number = 135
Evaluating:  82%| | 135/165 [00:03<00:00, 43.38it/s]01/07/2022 12:47:35 - INFO - __main__ -   Batch Number = 136
01/07/2022 12:47:35 - INFO - __main__ -   Batch Number = 137
01/07/2022 12:47:35 - INFO - __main__ -   Batch Number = 138
01/07/2022 12:47:35 - INFO - __main__ -   Batch Number = 139
01/07/2022 12:47:35 - INFO - __main__ -   Batch Number = 140
Evaluating:  85%| | 140/165 [00:03<00:00, 43.43it/s]01/07/2022 12:47:35 - INFO - __main__ -   Batch Number = 141
01/07/2022 12:47:35 - INFO - __main__ -   Batch Number = 142
01/07/2022 12:47:35 - INFO - __main__ -   Batch Number = 143
01/07/2022 12:47:35 - INFO - __main__ -   Batch Number = 144
01/07/2022 12:47:35 - INFO - __main__ -   Batch Number = 145
Evaluating:  88%| | 145/165 [00:03<00:00, 43.74it/s]01/07/2022 12:47:35 - INFO - __main__ -   Batch Number = 146
01/07/2022 12:47:35 - INFO - __main__ -   Batch Number = 147
01/07/2022 12:47:35 - INFO - __main__ -   Batch Number = 148
01/07/2022 12:47:35 - INFO - __main__ -   Batch Number = 149
01/07/2022 12:47:35 - INFO - __main__ -   Batch Number = 150
Evaluating:  91%| | 150/165 [00:03<00:00, 44.01it/s]01/07/2022 12:47:35 - INFO - __main__ -   Batch Number = 151
01/07/2022 12:47:35 - INFO - __main__ -   Batch Number = 152
01/07/2022 12:47:35 - INFO - __main__ -   Batch Number = 153
01/07/2022 12:47:35 - INFO - __main__ -   Batch Number = 154
01/07/2022 12:47:35 - INFO - __main__ -   Batch Number = 155
Evaluating:  94%|| 155/165 [00:03<00:00, 44.02it/s]01/07/2022 12:47:35 - INFO - __main__ -   Batch Number = 156
01/07/2022 12:47:35 - INFO - __main__ -   Batch Number = 157
01/07/2022 12:47:35 - INFO - __main__ -   Batch Number = 158
01/07/2022 12:47:35 - INFO - __main__ -   Batch Number = 159
01/07/2022 12:47:35 - INFO - __main__ -   Batch Number = 160
Evaluating:  97%|| 160/165 [00:03<00:00, 44.22it/s]01/07/2022 12:47:35 - INFO - __main__ -   Batch Number = 161
01/07/2022 12:47:36 - INFO - __main__ -   Batch Number = 162
01/07/2022 12:47:36 - INFO - __main__ -   Batch Number = 163
01/07/2022 12:47:36 - INFO - __main__ -   Batch Number = 164
01/07/2022 12:47:36 - INFO - __main__ -   Batch Number = 165
Evaluating: 100%|| 165/165 [00:03<00:00, 44.73it/s]Evaluating: 100%|| 165/165 [00:03<00:00, 42.95it/s]
01/07/2022 12:47:36 - INFO - __main__ -     Evaluation done in total 3.842494 secs (0.002924 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/predictions_th_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/nbest_predictions_th_.json
01/07/2022 12:47:41 - INFO - __main__ -   Results = OrderedDict([('exact', 53.69747899159664), ('f1', 65.19880073241406), ('total', 1190), ('HasAns_exact', 53.69747899159664), ('HasAns_f1', 65.19880073241406), ('HasAns_total', 1190), ('best_exact', 53.69747899159664), ('best_exact_thresh', 0.0), ('best_f1', 65.19880073241406), ('best_f1_thresh', 0.0)])
01/07/2022 12:47:41 - INFO - __main__ -   Language adapter for tr not found, using en instead
01/07/2022 12:47:41 - INFO - __main__ -   Set active language adapter to en
01/07/2022 12:47:41 - INFO - __main__ -   Args Adapter Weight = None
01/07/2022 12:47:41 - INFO - __main__ -   Adapter Languages = ['en']
01/07/2022 12:47:41 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
  0%|          | 0/48 [00:00<?, ?it/s] 33%|      | 16/48 [00:00<00:00, 132.10it/s] 62%|   | 30/48 [00:00<00:00, 125.65it/s] 94%|| 45/48 [00:00<00:00, 135.74it/s]100%|| 48/48 [00:00<00:00, 135.61it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<05:30,  3.60it/s]convert squad examples to features:  32%|      | 385/1190 [00:00<00:00, 864.51it/s]convert squad examples to features:  73%|  | 865/1190 [00:00<00:00, 1672.21it/s]convert squad examples to features: 100%|| 1190/1190 [00:00<00:00, 1667.30it/s]/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|| 1190/1190 [00:00<00:00, 561126.67it/s]
01/07/2022 12:47:43 - INFO - __main__ -   Local Rank = -1
01/07/2022 12:47:43 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.tr.json_xlm-roberta-base_384_tr
01/07/2022 12:47:44 - INFO - __main__ -   ***** Running evaluation  *****
01/07/2022 12:47:44 - INFO - __main__ -     Num examples = 1274
01/07/2022 12:47:44 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/160 [00:00<?, ?it/s]01/07/2022 12:47:44 - INFO - __main__ -   Batch Number = 1
01/07/2022 12:47:44 - INFO - __main__ -   Batch Number = 2
01/07/2022 12:47:44 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|         | 3/160 [00:00<00:05, 29.54it/s]01/07/2022 12:47:44 - INFO - __main__ -   Batch Number = 4
01/07/2022 12:47:44 - INFO - __main__ -   Batch Number = 5
01/07/2022 12:47:44 - INFO - __main__ -   Batch Number = 6
Evaluating:   4%|         | 6/160 [00:00<00:05, 29.60it/s]01/07/2022 12:47:44 - INFO - __main__ -   Batch Number = 7
01/07/2022 12:47:44 - INFO - __main__ -   Batch Number = 8
01/07/2022 12:47:44 - INFO - __main__ -   Batch Number = 9
01/07/2022 12:47:44 - INFO - __main__ -   Batch Number = 10
Evaluating:   6%|         | 10/160 [00:00<00:04, 32.21it/s]01/07/2022 12:47:44 - INFO - __main__ -   Batch Number = 11
01/07/2022 12:47:44 - INFO - __main__ -   Batch Number = 12
01/07/2022 12:47:44 - INFO - __main__ -   Batch Number = 13
01/07/2022 12:47:44 - INFO - __main__ -   Batch Number = 14
01/07/2022 12:47:44 - INFO - __main__ -   Batch Number = 15
Evaluating:   9%|         | 15/160 [00:00<00:03, 37.27it/s]01/07/2022 12:47:44 - INFO - __main__ -   Batch Number = 16
01/07/2022 12:47:44 - INFO - __main__ -   Batch Number = 17
01/07/2022 12:47:44 - INFO - __main__ -   Batch Number = 18
01/07/2022 12:47:44 - INFO - __main__ -   Batch Number = 19
01/07/2022 12:47:44 - INFO - __main__ -   Batch Number = 20
Evaluating:  12%|        | 20/160 [00:00<00:03, 39.85it/s]01/07/2022 12:47:44 - INFO - __main__ -   Batch Number = 21
01/07/2022 12:47:44 - INFO - __main__ -   Batch Number = 22
01/07/2022 12:47:44 - INFO - __main__ -   Batch Number = 23
01/07/2022 12:47:44 - INFO - __main__ -   Batch Number = 24
01/07/2022 12:47:44 - INFO - __main__ -   Batch Number = 25
Evaluating:  16%|        | 25/160 [00:00<00:03, 41.46it/s]01/07/2022 12:47:44 - INFO - __main__ -   Batch Number = 26
01/07/2022 12:47:44 - INFO - __main__ -   Batch Number = 27
01/07/2022 12:47:44 - INFO - __main__ -   Batch Number = 28
01/07/2022 12:47:44 - INFO - __main__ -   Batch Number = 29
01/07/2022 12:47:44 - INFO - __main__ -   Batch Number = 30
Evaluating:  19%|        | 30/160 [00:00<00:03, 42.60it/s]01/07/2022 12:47:44 - INFO - __main__ -   Batch Number = 31
01/07/2022 12:47:44 - INFO - __main__ -   Batch Number = 32
01/07/2022 12:47:44 - INFO - __main__ -   Batch Number = 33
01/07/2022 12:47:45 - INFO - __main__ -   Batch Number = 34
01/07/2022 12:47:45 - INFO - __main__ -   Batch Number = 35
Evaluating:  22%|       | 35/160 [00:00<00:02, 43.08it/s]01/07/2022 12:47:45 - INFO - __main__ -   Batch Number = 36
01/07/2022 12:47:45 - INFO - __main__ -   Batch Number = 37
01/07/2022 12:47:45 - INFO - __main__ -   Batch Number = 38
01/07/2022 12:47:45 - INFO - __main__ -   Batch Number = 39
01/07/2022 12:47:45 - INFO - __main__ -   Batch Number = 40
Evaluating:  25%|       | 40/160 [00:00<00:02, 43.66it/s]01/07/2022 12:47:45 - INFO - __main__ -   Batch Number = 41
01/07/2022 12:47:45 - INFO - __main__ -   Batch Number = 42
01/07/2022 12:47:45 - INFO - __main__ -   Batch Number = 43
01/07/2022 12:47:45 - INFO - __main__ -   Batch Number = 44
01/07/2022 12:47:45 - INFO - __main__ -   Batch Number = 45
Evaluating:  28%|       | 45/160 [00:01<00:02, 43.94it/s]01/07/2022 12:47:45 - INFO - __main__ -   Batch Number = 46
01/07/2022 12:47:45 - INFO - __main__ -   Batch Number = 47
01/07/2022 12:47:45 - INFO - __main__ -   Batch Number = 48
01/07/2022 12:47:45 - INFO - __main__ -   Batch Number = 49
01/07/2022 12:47:45 - INFO - __main__ -   Batch Number = 50
Evaluating:  31%|      | 50/160 [00:01<00:02, 44.08it/s]01/07/2022 12:47:45 - INFO - __main__ -   Batch Number = 51
01/07/2022 12:47:45 - INFO - __main__ -   Batch Number = 52
01/07/2022 12:47:45 - INFO - __main__ -   Batch Number = 53
01/07/2022 12:47:45 - INFO - __main__ -   Batch Number = 54
01/07/2022 12:47:45 - INFO - __main__ -   Batch Number = 55
Evaluating:  34%|      | 55/160 [00:01<00:02, 44.20it/s]01/07/2022 12:47:45 - INFO - __main__ -   Batch Number = 56
01/07/2022 12:47:45 - INFO - __main__ -   Batch Number = 57
01/07/2022 12:47:45 - INFO - __main__ -   Batch Number = 58
01/07/2022 12:47:45 - INFO - __main__ -   Batch Number = 59
01/07/2022 12:47:45 - INFO - __main__ -   Batch Number = 60
Evaluating:  38%|      | 60/160 [00:01<00:02, 44.34it/s]01/07/2022 12:47:45 - INFO - __main__ -   Batch Number = 61
01/07/2022 12:47:45 - INFO - __main__ -   Batch Number = 62
01/07/2022 12:47:45 - INFO - __main__ -   Batch Number = 63
01/07/2022 12:47:45 - INFO - __main__ -   Batch Number = 64
01/07/2022 12:47:45 - INFO - __main__ -   Batch Number = 65
Evaluating:  41%|      | 65/160 [00:01<00:02, 44.26it/s]01/07/2022 12:47:45 - INFO - __main__ -   Batch Number = 66
01/07/2022 12:47:45 - INFO - __main__ -   Batch Number = 67
01/07/2022 12:47:45 - INFO - __main__ -   Batch Number = 68
01/07/2022 12:47:45 - INFO - __main__ -   Batch Number = 69
01/07/2022 12:47:45 - INFO - __main__ -   Batch Number = 70
Evaluating:  44%|     | 70/160 [00:01<00:02, 44.29it/s]01/07/2022 12:47:45 - INFO - __main__ -   Batch Number = 71
01/07/2022 12:47:45 - INFO - __main__ -   Batch Number = 72
01/07/2022 12:47:45 - INFO - __main__ -   Batch Number = 73
01/07/2022 12:47:45 - INFO - __main__ -   Batch Number = 74
01/07/2022 12:47:45 - INFO - __main__ -   Batch Number = 75
Evaluating:  47%|     | 75/160 [00:01<00:01, 44.45it/s]01/07/2022 12:47:45 - INFO - __main__ -   Batch Number = 76
01/07/2022 12:47:45 - INFO - __main__ -   Batch Number = 77
01/07/2022 12:47:45 - INFO - __main__ -   Batch Number = 78
01/07/2022 12:47:46 - INFO - __main__ -   Batch Number = 79
01/07/2022 12:47:46 - INFO - __main__ -   Batch Number = 80
Evaluating:  50%|     | 80/160 [00:01<00:01, 44.35it/s]01/07/2022 12:47:46 - INFO - __main__ -   Batch Number = 81
01/07/2022 12:47:46 - INFO - __main__ -   Batch Number = 82
01/07/2022 12:47:46 - INFO - __main__ -   Batch Number = 83
01/07/2022 12:47:46 - INFO - __main__ -   Batch Number = 84
01/07/2022 12:47:46 - INFO - __main__ -   Batch Number = 85
Evaluating:  53%|    | 85/160 [00:02<00:01, 44.57it/s]01/07/2022 12:47:46 - INFO - __main__ -   Batch Number = 86
01/07/2022 12:47:46 - INFO - __main__ -   Batch Number = 87
01/07/2022 12:47:46 - INFO - __main__ -   Batch Number = 88
01/07/2022 12:47:46 - INFO - __main__ -   Batch Number = 89
01/07/2022 12:47:46 - INFO - __main__ -   Batch Number = 90
Evaluating:  56%|    | 90/160 [00:02<00:01, 44.54it/s]01/07/2022 12:47:46 - INFO - __main__ -   Batch Number = 91
01/07/2022 12:47:46 - INFO - __main__ -   Batch Number = 92
01/07/2022 12:47:46 - INFO - __main__ -   Batch Number = 93
01/07/2022 12:47:46 - INFO - __main__ -   Batch Number = 94
01/07/2022 12:47:46 - INFO - __main__ -   Batch Number = 95
Evaluating:  59%|    | 95/160 [00:02<00:01, 44.53it/s]01/07/2022 12:47:46 - INFO - __main__ -   Batch Number = 96
01/07/2022 12:47:46 - INFO - __main__ -   Batch Number = 97
01/07/2022 12:47:46 - INFO - __main__ -   Batch Number = 98
01/07/2022 12:47:46 - INFO - __main__ -   Batch Number = 99
01/07/2022 12:47:46 - INFO - __main__ -   Batch Number = 100
Evaluating:  62%|   | 100/160 [00:02<00:01, 44.50it/s]01/07/2022 12:47:46 - INFO - __main__ -   Batch Number = 101
01/07/2022 12:47:46 - INFO - __main__ -   Batch Number = 102
01/07/2022 12:47:46 - INFO - __main__ -   Batch Number = 103
01/07/2022 12:47:46 - INFO - __main__ -   Batch Number = 104
01/07/2022 12:47:46 - INFO - __main__ -   Batch Number = 105
Evaluating:  66%|   | 105/160 [00:02<00:01, 44.28it/s]01/07/2022 12:47:46 - INFO - __main__ -   Batch Number = 106
01/07/2022 12:47:46 - INFO - __main__ -   Batch Number = 107
01/07/2022 12:47:46 - INFO - __main__ -   Batch Number = 108
01/07/2022 12:47:46 - INFO - __main__ -   Batch Number = 109
01/07/2022 12:47:46 - INFO - __main__ -   Batch Number = 110
Evaluating:  69%|   | 110/160 [00:02<00:01, 42.72it/s]01/07/2022 12:47:46 - INFO - __main__ -   Batch Number = 111
01/07/2022 12:47:46 - INFO - __main__ -   Batch Number = 112
01/07/2022 12:47:46 - INFO - __main__ -   Batch Number = 113
01/07/2022 12:47:46 - INFO - __main__ -   Batch Number = 114
01/07/2022 12:47:46 - INFO - __main__ -   Batch Number = 115
Evaluating:  72%|  | 115/160 [00:02<00:01, 43.33it/s]01/07/2022 12:47:46 - INFO - __main__ -   Batch Number = 116
01/07/2022 12:47:46 - INFO - __main__ -   Batch Number = 117
01/07/2022 12:47:46 - INFO - __main__ -   Batch Number = 118
01/07/2022 12:47:46 - INFO - __main__ -   Batch Number = 119
01/07/2022 12:47:46 - INFO - __main__ -   Batch Number = 120
Evaluating:  75%|  | 120/160 [00:02<00:00, 43.64it/s]01/07/2022 12:47:46 - INFO - __main__ -   Batch Number = 121
01/07/2022 12:47:46 - INFO - __main__ -   Batch Number = 122
01/07/2022 12:47:47 - INFO - __main__ -   Batch Number = 123
01/07/2022 12:47:47 - INFO - __main__ -   Batch Number = 124
01/07/2022 12:47:47 - INFO - __main__ -   Batch Number = 125
Evaluating:  78%|  | 125/160 [00:02<00:00, 43.77it/s]01/07/2022 12:47:47 - INFO - __main__ -   Batch Number = 126
01/07/2022 12:47:47 - INFO - __main__ -   Batch Number = 127
01/07/2022 12:47:47 - INFO - __main__ -   Batch Number = 128
01/07/2022 12:47:47 - INFO - __main__ -   Batch Number = 129
01/07/2022 12:47:47 - INFO - __main__ -   Batch Number = 130
Evaluating:  81%| | 130/160 [00:03<00:00, 44.00it/s]01/07/2022 12:47:47 - INFO - __main__ -   Batch Number = 131
01/07/2022 12:47:47 - INFO - __main__ -   Batch Number = 132
01/07/2022 12:47:47 - INFO - __main__ -   Batch Number = 133
01/07/2022 12:47:47 - INFO - __main__ -   Batch Number = 134
01/07/2022 12:47:47 - INFO - __main__ -   Batch Number = 135
Evaluating:  84%| | 135/160 [00:03<00:00, 44.10it/s]01/07/2022 12:47:47 - INFO - __main__ -   Batch Number = 136
01/07/2022 12:47:47 - INFO - __main__ -   Batch Number = 137
01/07/2022 12:47:47 - INFO - __main__ -   Batch Number = 138
01/07/2022 12:47:47 - INFO - __main__ -   Batch Number = 139
01/07/2022 12:47:47 - INFO - __main__ -   Batch Number = 140
Evaluating:  88%| | 140/160 [00:03<00:00, 44.18it/s]01/07/2022 12:47:47 - INFO - __main__ -   Batch Number = 141
01/07/2022 12:47:47 - INFO - __main__ -   Batch Number = 142
01/07/2022 12:47:47 - INFO - __main__ -   Batch Number = 143
01/07/2022 12:47:47 - INFO - __main__ -   Batch Number = 144
01/07/2022 12:47:47 - INFO - __main__ -   Batch Number = 145
Evaluating:  91%| | 145/160 [00:03<00:00, 44.16it/s]01/07/2022 12:47:47 - INFO - __main__ -   Batch Number = 146
01/07/2022 12:47:47 - INFO - __main__ -   Batch Number = 147
01/07/2022 12:47:47 - INFO - __main__ -   Batch Number = 148
01/07/2022 12:47:47 - INFO - __main__ -   Batch Number = 149
01/07/2022 12:47:47 - INFO - __main__ -   Batch Number = 150
Evaluating:  94%|| 150/160 [00:03<00:00, 44.34it/s]01/07/2022 12:47:47 - INFO - __main__ -   Batch Number = 151
01/07/2022 12:47:47 - INFO - __main__ -   Batch Number = 152
01/07/2022 12:47:47 - INFO - __main__ -   Batch Number = 153
01/07/2022 12:47:47 - INFO - __main__ -   Batch Number = 154
01/07/2022 12:47:47 - INFO - __main__ -   Batch Number = 155
Evaluating:  97%|| 155/160 [00:03<00:00, 42.75it/s]01/07/2022 12:47:47 - INFO - __main__ -   Batch Number = 156
01/07/2022 12:47:47 - INFO - __main__ -   Batch Number = 157
01/07/2022 12:47:47 - INFO - __main__ -   Batch Number = 158
01/07/2022 12:47:47 - INFO - __main__ -   Batch Number = 159
01/07/2022 12:47:47 - INFO - __main__ -   Batch Number = 160
Evaluating: 100%|| 160/160 [00:03<00:00, 39.52it/s]Evaluating: 100%|| 160/160 [00:03<00:00, 42.60it/s]
01/07/2022 12:47:47 - INFO - __main__ -     Evaluation done in total 3.756037 secs (0.002948 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/predictions_tr_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/nbest_predictions_tr_.json
01/07/2022 12:47:51 - INFO - __main__ -   Results = OrderedDict([('exact', 49.57983193277311), ('f1', 66.22538990178751), ('total', 1190), ('HasAns_exact', 49.57983193277311), ('HasAns_f1', 66.22538990178751), ('HasAns_total', 1190), ('best_exact', 49.57983193277311), ('best_exact_thresh', 0.0), ('best_f1', 66.22538990178751), ('best_f1_thresh', 0.0)])
01/07/2022 12:47:51 - INFO - __main__ -   Language adapter for vi not found, using en instead
01/07/2022 12:47:51 - INFO - __main__ -   Set active language adapter to en
01/07/2022 12:47:51 - INFO - __main__ -   Args Adapter Weight = None
01/07/2022 12:47:51 - INFO - __main__ -   Adapter Languages = ['en']
01/07/2022 12:47:51 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
  0%|          | 0/48 [00:00<?, ?it/s] 33%|      | 16/48 [00:00<00:00, 135.29it/s] 62%|   | 30/48 [00:00<00:00, 129.17it/s] 96%|| 46/48 [00:00<00:00, 140.21it/s]100%|| 48/48 [00:00<00:00, 138.98it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<07:21,  2.69it/s]convert squad examples to features:  32%|      | 385/1190 [00:00<00:01, 446.37it/s]convert squad examples to features: 100%|| 1190/1190 [00:01<00:00, 1167.22it/s]/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|| 1190/1190 [00:00<00:00, 693610.58it/s]
01/07/2022 12:47:53 - INFO - __main__ -   Local Rank = -1
01/07/2022 12:47:53 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.vi.json_xlm-roberta-base_384_vi
01/07/2022 12:47:54 - INFO - __main__ -   ***** Running evaluation  *****
01/07/2022 12:47:54 - INFO - __main__ -     Num examples = 1314
01/07/2022 12:47:54 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/165 [00:00<?, ?it/s]01/07/2022 12:47:54 - INFO - __main__ -   Batch Number = 1
01/07/2022 12:47:54 - INFO - __main__ -   Batch Number = 2
01/07/2022 12:47:54 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|         | 3/165 [00:00<00:05, 29.20it/s]01/07/2022 12:47:54 - INFO - __main__ -   Batch Number = 4
01/07/2022 12:47:54 - INFO - __main__ -   Batch Number = 5
01/07/2022 12:47:54 - INFO - __main__ -   Batch Number = 6
01/07/2022 12:47:54 - INFO - __main__ -   Batch Number = 7
Evaluating:   4%|         | 7/165 [00:00<00:04, 33.84it/s]01/07/2022 12:47:54 - INFO - __main__ -   Batch Number = 8
01/07/2022 12:47:54 - INFO - __main__ -   Batch Number = 9
01/07/2022 12:47:54 - INFO - __main__ -   Batch Number = 10
01/07/2022 12:47:54 - INFO - __main__ -   Batch Number = 11
01/07/2022 12:47:54 - INFO - __main__ -   Batch Number = 12
Evaluating:   7%|         | 12/165 [00:00<00:03, 38.53it/s]01/07/2022 12:47:54 - INFO - __main__ -   Batch Number = 13
01/07/2022 12:47:54 - INFO - __main__ -   Batch Number = 14
01/07/2022 12:47:54 - INFO - __main__ -   Batch Number = 15
01/07/2022 12:47:54 - INFO - __main__ -   Batch Number = 16
01/07/2022 12:47:54 - INFO - __main__ -   Batch Number = 17
Evaluating:  10%|         | 17/165 [00:00<00:03, 40.47it/s]01/07/2022 12:47:54 - INFO - __main__ -   Batch Number = 18
01/07/2022 12:47:54 - INFO - __main__ -   Batch Number = 19
01/07/2022 12:47:54 - INFO - __main__ -   Batch Number = 20
01/07/2022 12:47:54 - INFO - __main__ -   Batch Number = 21
01/07/2022 12:47:54 - INFO - __main__ -   Batch Number = 22
Evaluating:  13%|        | 22/165 [00:00<00:03, 41.92it/s]01/07/2022 12:47:54 - INFO - __main__ -   Batch Number = 23
01/07/2022 12:47:54 - INFO - __main__ -   Batch Number = 24
01/07/2022 12:47:54 - INFO - __main__ -   Batch Number = 25
01/07/2022 12:47:54 - INFO - __main__ -   Batch Number = 26
01/07/2022 12:47:55 - INFO - __main__ -   Batch Number = 27
Evaluating:  16%|        | 27/165 [00:00<00:03, 42.99it/s]01/07/2022 12:47:55 - INFO - __main__ -   Batch Number = 28
01/07/2022 12:47:55 - INFO - __main__ -   Batch Number = 29
01/07/2022 12:47:55 - INFO - __main__ -   Batch Number = 30
01/07/2022 12:47:55 - INFO - __main__ -   Batch Number = 31
01/07/2022 12:47:55 - INFO - __main__ -   Batch Number = 32
Evaluating:  19%|        | 32/165 [00:00<00:03, 43.29it/s]01/07/2022 12:47:55 - INFO - __main__ -   Batch Number = 33
01/07/2022 12:47:55 - INFO - __main__ -   Batch Number = 34
01/07/2022 12:47:55 - INFO - __main__ -   Batch Number = 35
01/07/2022 12:47:55 - INFO - __main__ -   Batch Number = 36
01/07/2022 12:47:55 - INFO - __main__ -   Batch Number = 37
Evaluating:  22%|       | 37/165 [00:00<00:02, 42.97it/s]01/07/2022 12:47:55 - INFO - __main__ -   Batch Number = 38
01/07/2022 12:47:55 - INFO - __main__ -   Batch Number = 39
01/07/2022 12:47:55 - INFO - __main__ -   Batch Number = 40
01/07/2022 12:47:55 - INFO - __main__ -   Batch Number = 41
01/07/2022 12:47:55 - INFO - __main__ -   Batch Number = 42
Evaluating:  25%|       | 42/165 [00:01<00:02, 43.38it/s]01/07/2022 12:47:55 - INFO - __main__ -   Batch Number = 43
01/07/2022 12:47:55 - INFO - __main__ -   Batch Number = 44
01/07/2022 12:47:55 - INFO - __main__ -   Batch Number = 45
01/07/2022 12:47:55 - INFO - __main__ -   Batch Number = 46
01/07/2022 12:47:55 - INFO - __main__ -   Batch Number = 47
Evaluating:  28%|       | 47/165 [00:01<00:02, 43.13it/s]01/07/2022 12:47:55 - INFO - __main__ -   Batch Number = 48
01/07/2022 12:47:55 - INFO - __main__ -   Batch Number = 49
01/07/2022 12:47:55 - INFO - __main__ -   Batch Number = 50
01/07/2022 12:47:55 - INFO - __main__ -   Batch Number = 51
01/07/2022 12:47:55 - INFO - __main__ -   Batch Number = 52
Evaluating:  32%|      | 52/165 [00:01<00:02, 43.23it/s]01/07/2022 12:47:55 - INFO - __main__ -   Batch Number = 53
01/07/2022 12:47:55 - INFO - __main__ -   Batch Number = 54
01/07/2022 12:47:55 - INFO - __main__ -   Batch Number = 55
01/07/2022 12:47:55 - INFO - __main__ -   Batch Number = 56
01/07/2022 12:47:55 - INFO - __main__ -   Batch Number = 57
Evaluating:  35%|      | 57/165 [00:01<00:02, 43.47it/s]01/07/2022 12:47:55 - INFO - __main__ -   Batch Number = 58
01/07/2022 12:47:55 - INFO - __main__ -   Batch Number = 59
01/07/2022 12:47:55 - INFO - __main__ -   Batch Number = 60
01/07/2022 12:47:55 - INFO - __main__ -   Batch Number = 61
01/07/2022 12:47:55 - INFO - __main__ -   Batch Number = 62
Evaluating:  38%|      | 62/165 [00:01<00:02, 43.39it/s]01/07/2022 12:47:55 - INFO - __main__ -   Batch Number = 63
01/07/2022 12:47:55 - INFO - __main__ -   Batch Number = 64
01/07/2022 12:47:55 - INFO - __main__ -   Batch Number = 65
01/07/2022 12:47:55 - INFO - __main__ -   Batch Number = 66
01/07/2022 12:47:55 - INFO - __main__ -   Batch Number = 67
Evaluating:  41%|      | 67/165 [00:01<00:02, 43.58it/s]01/07/2022 12:47:55 - INFO - __main__ -   Batch Number = 68
01/07/2022 12:47:55 - INFO - __main__ -   Batch Number = 69
01/07/2022 12:47:55 - INFO - __main__ -   Batch Number = 70
01/07/2022 12:47:56 - INFO - __main__ -   Batch Number = 71
01/07/2022 12:47:56 - INFO - __main__ -   Batch Number = 72
Evaluating:  44%|     | 72/165 [00:01<00:02, 43.91it/s]01/07/2022 12:47:56 - INFO - __main__ -   Batch Number = 73
01/07/2022 12:47:56 - INFO - __main__ -   Batch Number = 74
01/07/2022 12:47:56 - INFO - __main__ -   Batch Number = 75
01/07/2022 12:47:56 - INFO - __main__ -   Batch Number = 76
01/07/2022 12:47:56 - INFO - __main__ -   Batch Number = 77
Evaluating:  47%|     | 77/165 [00:01<00:01, 44.03it/s]01/07/2022 12:47:56 - INFO - __main__ -   Batch Number = 78
01/07/2022 12:47:56 - INFO - __main__ -   Batch Number = 79
01/07/2022 12:47:56 - INFO - __main__ -   Batch Number = 80
01/07/2022 12:47:56 - INFO - __main__ -   Batch Number = 81
01/07/2022 12:47:56 - INFO - __main__ -   Batch Number = 82
Evaluating:  50%|     | 82/165 [00:01<00:01, 44.21it/s]01/07/2022 12:47:56 - INFO - __main__ -   Batch Number = 83
01/07/2022 12:47:56 - INFO - __main__ -   Batch Number = 84
01/07/2022 12:47:56 - INFO - __main__ -   Batch Number = 85
01/07/2022 12:47:56 - INFO - __main__ -   Batch Number = 86
01/07/2022 12:47:56 - INFO - __main__ -   Batch Number = 87
Evaluating:  53%|    | 87/165 [00:02<00:01, 44.27it/s]01/07/2022 12:47:56 - INFO - __main__ -   Batch Number = 88
01/07/2022 12:47:56 - INFO - __main__ -   Batch Number = 89
01/07/2022 12:47:56 - INFO - __main__ -   Batch Number = 90
01/07/2022 12:47:56 - INFO - __main__ -   Batch Number = 91
01/07/2022 12:47:56 - INFO - __main__ -   Batch Number = 92
Evaluating:  56%|    | 92/165 [00:02<00:01, 42.52it/s]01/07/2022 12:47:56 - INFO - __main__ -   Batch Number = 93
01/07/2022 12:47:56 - INFO - __main__ -   Batch Number = 94
01/07/2022 12:47:56 - INFO - __main__ -   Batch Number = 95
01/07/2022 12:47:56 - INFO - __main__ -   Batch Number = 96
01/07/2022 12:47:56 - INFO - __main__ -   Batch Number = 97
Evaluating:  59%|    | 97/165 [00:02<00:01, 43.15it/s]01/07/2022 12:47:56 - INFO - __main__ -   Batch Number = 98
01/07/2022 12:47:56 - INFO - __main__ -   Batch Number = 99
01/07/2022 12:47:56 - INFO - __main__ -   Batch Number = 100
01/07/2022 12:47:56 - INFO - __main__ -   Batch Number = 101
01/07/2022 12:47:56 - INFO - __main__ -   Batch Number = 102
Evaluating:  62%|   | 102/165 [00:02<00:01, 43.37it/s]01/07/2022 12:47:56 - INFO - __main__ -   Batch Number = 103
01/07/2022 12:47:56 - INFO - __main__ -   Batch Number = 104
01/07/2022 12:47:56 - INFO - __main__ -   Batch Number = 105
01/07/2022 12:47:56 - INFO - __main__ -   Batch Number = 106
01/07/2022 12:47:56 - INFO - __main__ -   Batch Number = 107
Evaluating:  65%|   | 107/165 [00:02<00:01, 43.59it/s]01/07/2022 12:47:56 - INFO - __main__ -   Batch Number = 108
01/07/2022 12:47:56 - INFO - __main__ -   Batch Number = 109
01/07/2022 12:47:56 - INFO - __main__ -   Batch Number = 110
01/07/2022 12:47:56 - INFO - __main__ -   Batch Number = 111
01/07/2022 12:47:56 - INFO - __main__ -   Batch Number = 112
Evaluating:  68%|   | 112/165 [00:02<00:01, 43.92it/s]01/07/2022 12:47:56 - INFO - __main__ -   Batch Number = 113
01/07/2022 12:47:56 - INFO - __main__ -   Batch Number = 114
01/07/2022 12:47:57 - INFO - __main__ -   Batch Number = 115
01/07/2022 12:47:57 - INFO - __main__ -   Batch Number = 116
01/07/2022 12:47:57 - INFO - __main__ -   Batch Number = 117
Evaluating:  71%|   | 117/165 [00:02<00:01, 43.86it/s]01/07/2022 12:47:57 - INFO - __main__ -   Batch Number = 118
01/07/2022 12:47:57 - INFO - __main__ -   Batch Number = 119
01/07/2022 12:47:57 - INFO - __main__ -   Batch Number = 120
01/07/2022 12:47:57 - INFO - __main__ -   Batch Number = 121
01/07/2022 12:47:57 - INFO - __main__ -   Batch Number = 122
Evaluating:  74%|  | 122/165 [00:02<00:00, 43.55it/s]01/07/2022 12:47:57 - INFO - __main__ -   Batch Number = 123
01/07/2022 12:47:57 - INFO - __main__ -   Batch Number = 124
01/07/2022 12:47:57 - INFO - __main__ -   Batch Number = 125
01/07/2022 12:47:57 - INFO - __main__ -   Batch Number = 126
01/07/2022 12:47:57 - INFO - __main__ -   Batch Number = 127
Evaluating:  77%|  | 127/165 [00:02<00:00, 43.60it/s]01/07/2022 12:47:57 - INFO - __main__ -   Batch Number = 128
01/07/2022 12:47:57 - INFO - __main__ -   Batch Number = 129
01/07/2022 12:47:57 - INFO - __main__ -   Batch Number = 130
01/07/2022 12:47:57 - INFO - __main__ -   Batch Number = 131
01/07/2022 12:47:57 - INFO - __main__ -   Batch Number = 132
Evaluating:  80%|  | 132/165 [00:03<00:00, 43.47it/s]01/07/2022 12:47:57 - INFO - __main__ -   Batch Number = 133
01/07/2022 12:47:57 - INFO - __main__ -   Batch Number = 134
01/07/2022 12:47:57 - INFO - __main__ -   Batch Number = 135
01/07/2022 12:47:57 - INFO - __main__ -   Batch Number = 136
01/07/2022 12:47:57 - INFO - __main__ -   Batch Number = 137
Evaluating:  83%| | 137/165 [00:03<00:00, 43.27it/s]01/07/2022 12:47:57 - INFO - __main__ -   Batch Number = 138
01/07/2022 12:47:57 - INFO - __main__ -   Batch Number = 139
01/07/2022 12:47:57 - INFO - __main__ -   Batch Number = 140
01/07/2022 12:47:57 - INFO - __main__ -   Batch Number = 141
01/07/2022 12:47:57 - INFO - __main__ -   Batch Number = 142
Evaluating:  86%| | 142/165 [00:03<00:00, 43.17it/s]01/07/2022 12:47:57 - INFO - __main__ -   Batch Number = 143
01/07/2022 12:47:57 - INFO - __main__ -   Batch Number = 144
01/07/2022 12:47:57 - INFO - __main__ -   Batch Number = 145
01/07/2022 12:47:57 - INFO - __main__ -   Batch Number = 146
01/07/2022 12:47:57 - INFO - __main__ -   Batch Number = 147
Evaluating:  89%| | 147/165 [00:03<00:00, 43.26it/s]01/07/2022 12:47:57 - INFO - __main__ -   Batch Number = 148
01/07/2022 12:47:57 - INFO - __main__ -   Batch Number = 149
01/07/2022 12:47:57 - INFO - __main__ -   Batch Number = 150
01/07/2022 12:47:57 - INFO - __main__ -   Batch Number = 151
01/07/2022 12:47:57 - INFO - __main__ -   Batch Number = 152
Evaluating:  92%|| 152/165 [00:03<00:00, 43.18it/s]01/07/2022 12:47:57 - INFO - __main__ -   Batch Number = 153
01/07/2022 12:47:57 - INFO - __main__ -   Batch Number = 154
01/07/2022 12:47:57 - INFO - __main__ -   Batch Number = 155
01/07/2022 12:47:57 - INFO - __main__ -   Batch Number = 156
01/07/2022 12:47:57 - INFO - __main__ -   Batch Number = 157
Evaluating:  95%|| 157/165 [00:03<00:00, 43.34it/s]01/07/2022 12:47:58 - INFO - __main__ -   Batch Number = 158
01/07/2022 12:47:58 - INFO - __main__ -   Batch Number = 159
01/07/2022 12:47:58 - INFO - __main__ -   Batch Number = 160
01/07/2022 12:47:58 - INFO - __main__ -   Batch Number = 161
01/07/2022 12:47:58 - INFO - __main__ -   Batch Number = 162
Evaluating:  98%|| 162/165 [00:03<00:00, 43.46it/s]01/07/2022 12:47:58 - INFO - __main__ -   Batch Number = 163
01/07/2022 12:47:58 - INFO - __main__ -   Batch Number = 164
01/07/2022 12:47:58 - INFO - __main__ -   Batch Number = 165
Evaluating: 100%|| 165/165 [00:03<00:00, 43.07it/s]
01/07/2022 12:47:58 - INFO - __main__ -     Evaluation done in total 3.831259 secs (0.002916 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/predictions_vi_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/nbest_predictions_vi_.json
01/07/2022 12:48:01 - INFO - __main__ -   Results = OrderedDict([('exact', 51.84873949579832), ('f1', 71.7717756131231), ('total', 1190), ('HasAns_exact', 51.84873949579832), ('HasAns_f1', 71.7717756131231), ('HasAns_total', 1190), ('best_exact', 51.84873949579832), ('best_exact_thresh', 0.0), ('best_f1', 71.7717756131231), ('best_f1_thresh', 0.0)])
01/07/2022 12:48:01 - INFO - __main__ -   Language adapter for zh not found, using en instead
01/07/2022 12:48:01 - INFO - __main__ -   Set active language adapter to en
01/07/2022 12:48:01 - INFO - __main__ -   Args Adapter Weight = None
01/07/2022 12:48:01 - INFO - __main__ -   Adapter Languages = ['en']
01/07/2022 12:48:01 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
  0%|          | 0/48 [00:00<?, ?it/s] 73%|  | 35/48 [00:00<00:00, 347.71it/s]100%|| 48/48 [00:00<00:00, 258.66it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:  19%|        | 225/1190 [00:00<00:00, 1917.80it/s]convert squad examples to features:  35%|      | 417/1190 [00:00<00:00, 1382.68it/s]convert squad examples to features: 100%|| 1190/1190 [00:00<00:00, 3136.66it/s]/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|| 1190/1190 [00:00<00:00, 707272.46it/s]
01/07/2022 12:48:02 - INFO - __main__ -   Local Rank = -1
01/07/2022 12:48:02 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.zh.json_xlm-roberta-base_384_zh
Traceback (most recent call last):
  File "third_party/my_run_squad.py", line 1112, in <module>
    main()
  File "third_party/my_run_squad.py", line 1105, in main
    predict_and_save(args, adapter_args, model, tokenizer, lang2id, lang_adapter_names, task_name, 'test')
  File "third_party/my_run_squad.py", line 903, in predict_and_save
    results = evaluate(args, model, tokenizer, language=lang, lang2id=lang2id, adapter_weight=adapter_weight, mode=split)
  File "third_party/my_run_squad.py", line 438, in evaluate
    dataset, examples, features = load_and_cache_examples(args, tokenizer, evaluate=True, output_examples=True,
  File "third_party/my_run_squad.py", line 633, in load_and_cache_examples
    torch.save({"features": features, "dataset": dataset}, cached_features_file)
  File "/root/Desktop/venvs/cloud-emea/lib/python3.8/site-packages/torch/serialization.py", line 379, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol)
  File "/root/Desktop/venvs/cloud-emea/lib/python3.8/site-packages/torch/serialization.py", line 484, in _save
    pickler.dump(obj)
  File "/root/Desktop/venvs/cloud-emea/lib/python3.8/site-packages/torch/serialization.py", line 467, in persistent_id
    if torch.is_storage(obj):
KeyboardInterrupt
PyTorch version 1.10.0+cu111 available.
01/07/2022 12:49:16 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/07/2022 12:49:16 - INFO - root -   save model
loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /root/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6
Model config XLMRobertaConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /root/.cache/torch/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8
loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/07/2022 12:49:24 - INFO - __main__ -   lang2id = None
01/07/2022 12:49:27 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='xlm-roberta-base', model_type='xlm-roberta', config_name=None, tokenizer_name=None, cache_dir=None, data_dir='/root/Desktop/cloud-emea-copy/data//xquad', output_dir='/root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/', max_seq_length=384, train_file='/root/Desktop/cloud-emea-copy/data//xquad/train-v1.1.json', predict_file='/root/Desktop/cloud-emea-copy/data//xquad/dev-v1.1.json', eval_test_set=False, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=4, per_gpu_eval_batch_size=8, gradient_accumulation_steps=4, learning_rate=0.0001, weight_decay=0.0001, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=15.0, max_steps=-1, save_steps=1000, warmup_steps=500, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', eval_lang='en', predict_langs='ar,de,el,es,hi,ru,th,tr,vi,zh', train_lang='en', log_file='/root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='qna', threads=8, version_2_with_negative=False, verbose_logging=False, n_best_size=20, max_query_length=64, max_answer_length=30, doc_stride=128, null_score_diff_threshold=0.0, predict_task_adapter='output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s1/checkpoint-best/qna', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix=None, en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/07/2022 12:49:27 - INFO - __main__ -   Results: {}
loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /root/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6
Model config XLMRobertaConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /root/.cache/torch/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8
loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/07/2022 12:49:36 - INFO - __main__ -   lang2id = None
01/07/2022 12:49:36 - INFO - __main__ -   Evaluating the model on the test set of all languages specified
01/07/2022 12:49:36 - INFO - __main__ -   Adapter will be loaded from this path: output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s1/checkpoint-best/qna
01/07/2022 12:49:36 - INFO - root -   Trying to decide if add adapter
01/07/2022 12:49:36 - INFO - root -   loading task adapter
Loading module configuration from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s1/checkpoint-best/qna/adapter_config.json
Adding adapter 'qna' of type 'text_task'.
Loading module weights from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s1/checkpoint-best/qna/pytorch_adapter.bin
Loading module configuration from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s1/checkpoint-best/qna/head_config.json
Loading module weights from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s1/checkpoint-best/qna/pytorch_model_head.bin
01/07/2022 12:49:36 - INFO - root -   loading lang adpater en/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/xlm-roberta-base/pfeiffer/en_relu_2.zip.
Loading module configuration from /root/.cache/torch/adapters/3e9c8c800af886cd1c684a3acf4929990b35f3ac75e050e942de2ff2ecf895fe-caabba7772816577124631d5b84810c4d32f49afcea4fca637162ec7f432174f-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
Loading module weights from /root/.cache/torch/adapters/3e9c8c800af886cd1c684a3acf4929990b35f3ac75e050e942de2ff2ecf895fe-caabba7772816577124631d5b84810c4d32f49afcea4fca637162ec7f432174f-extracted/pytorch_adapter.bin
No matching prediction head found in '/root/.cache/torch/adapters/3e9c8c800af886cd1c684a3acf4929990b35f3ac75e050e942de2ff2ecf895fe-caabba7772816577124631d5b84810c4d32f49afcea4fca637162ec7f432174f-extracted'
01/07/2022 12:49:37 - INFO - __main__ -   Language adapter for ar not found, using en instead
01/07/2022 12:49:37 - INFO - __main__ -   Set active language adapter to en
01/07/2022 12:49:37 - INFO - __main__ -   Args Adapter Weight = None
01/07/2022 12:49:37 - INFO - __main__ -   Adapter Languages = ['en']
01/07/2022 12:49:37 - INFO - __main__ -   Predict File = xquad.ar.json
01/07/2022 12:49:37 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
en en/wiki@ukp
  0%|          | 0/48 [00:00<?, ?it/s] 33%|      | 16/48 [00:00<00:00, 137.87it/s] 62%|   | 30/48 [00:00<00:00, 131.62it/s] 94%|| 45/48 [00:00<00:00, 138.11it/s]100%|| 48/48 [00:00<00:00, 139.83it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<04:46,  4.14it/s]convert squad examples to features:  24%|       | 289/1190 [00:00<00:00, 1051.80it/s]convert squad examples to features:  38%|      | 450/1190 [00:00<00:00, 810.14it/s] convert squad examples to features:  94%|| 1121/1190 [00:00<00:00, 2075.09it/s]convert squad examples to features: 100%|| 1190/1190 [00:00<00:00, 1570.59it/s]/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|| 1190/1190 [00:00<00:00, 720649.98it/s]
01/07/2022 12:49:39 - INFO - __main__ -   Local Rank = -1
01/07/2022 12:49:39 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.ar.json_xlm-roberta-base_384_ar
01/07/2022 12:49:40 - INFO - __main__ -   ***** Running evaluation  *****
01/07/2022 12:49:40 - INFO - __main__ -     Num examples = 1318
01/07/2022 12:49:40 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/165 [00:00<?, ?it/s]01/07/2022 12:49:40 - INFO - __main__ -   Batch Number = 1
01/07/2022 12:49:40 - INFO - __main__ -   Batch Number = 2
01/07/2022 12:49:40 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|         | 3/165 [00:00<00:06, 26.56it/s]01/07/2022 12:49:40 - INFO - __main__ -   Batch Number = 4
01/07/2022 12:49:40 - INFO - __main__ -   Batch Number = 5
01/07/2022 12:49:40 - INFO - __main__ -   Batch Number = 6
Evaluating:   4%|         | 6/165 [00:00<00:05, 28.39it/s]01/07/2022 12:49:40 - INFO - __main__ -   Batch Number = 7
01/07/2022 12:49:40 - INFO - __main__ -   Batch Number = 8
01/07/2022 12:49:40 - INFO - __main__ -   Batch Number = 9
01/07/2022 12:49:40 - INFO - __main__ -   Batch Number = 10
Evaluating:   6%|         | 10/165 [00:00<00:04, 32.26it/s]01/07/2022 12:49:40 - INFO - __main__ -   Batch Number = 11
01/07/2022 12:49:40 - INFO - __main__ -   Batch Number = 12
01/07/2022 12:49:40 - INFO - __main__ -   Batch Number = 13
01/07/2022 12:49:40 - INFO - __main__ -   Batch Number = 14
01/07/2022 12:49:40 - INFO - __main__ -   Batch Number = 15
Evaluating:   9%|         | 15/165 [00:00<00:04, 36.97it/s]01/07/2022 12:49:40 - INFO - __main__ -   Batch Number = 16
01/07/2022 12:49:40 - INFO - __main__ -   Batch Number = 17
01/07/2022 12:49:40 - INFO - __main__ -   Batch Number = 18
01/07/2022 12:49:40 - INFO - __main__ -   Batch Number = 19
01/07/2022 12:49:40 - INFO - __main__ -   Batch Number = 20
Evaluating:  12%|        | 20/165 [00:00<00:03, 39.60it/s]01/07/2022 12:49:41 - INFO - __main__ -   Batch Number = 21
01/07/2022 12:49:41 - INFO - __main__ -   Batch Number = 22
01/07/2022 12:49:41 - INFO - __main__ -   Batch Number = 23
01/07/2022 12:49:41 - INFO - __main__ -   Batch Number = 24
01/07/2022 12:49:41 - INFO - __main__ -   Batch Number = 25
Evaluating:  15%|        | 25/165 [00:00<00:03, 41.04it/s]01/07/2022 12:49:41 - INFO - __main__ -   Batch Number = 26
01/07/2022 12:49:41 - INFO - __main__ -   Batch Number = 27
01/07/2022 12:49:41 - INFO - __main__ -   Batch Number = 28
01/07/2022 12:49:41 - INFO - __main__ -   Batch Number = 29
01/07/2022 12:49:41 - INFO - __main__ -   Batch Number = 30
Evaluating:  18%|        | 30/165 [00:00<00:03, 41.97it/s]01/07/2022 12:49:41 - INFO - __main__ -   Batch Number = 31
01/07/2022 12:49:41 - INFO - __main__ -   Batch Number = 32
01/07/2022 12:49:41 - INFO - __main__ -   Batch Number = 33
01/07/2022 12:49:41 - INFO - __main__ -   Batch Number = 34
01/07/2022 12:49:41 - INFO - __main__ -   Batch Number = 35
Evaluating:  21%|        | 35/165 [00:00<00:03, 42.45it/s]01/07/2022 12:49:41 - INFO - __main__ -   Batch Number = 36
01/07/2022 12:49:41 - INFO - __main__ -   Batch Number = 37
01/07/2022 12:49:41 - INFO - __main__ -   Batch Number = 38
01/07/2022 12:49:41 - INFO - __main__ -   Batch Number = 39
01/07/2022 12:49:41 - INFO - __main__ -   Batch Number = 40
Evaluating:  24%|       | 40/165 [00:01<00:02, 43.12it/s]01/07/2022 12:49:41 - INFO - __main__ -   Batch Number = 41
01/07/2022 12:49:41 - INFO - __main__ -   Batch Number = 42
01/07/2022 12:49:41 - INFO - __main__ -   Batch Number = 43
01/07/2022 12:49:41 - INFO - __main__ -   Batch Number = 44
01/07/2022 12:49:41 - INFO - __main__ -   Batch Number = 45
Evaluating:  27%|       | 45/165 [00:01<00:02, 41.57it/s]01/07/2022 12:49:41 - INFO - __main__ -   Batch Number = 46
01/07/2022 12:49:41 - INFO - __main__ -   Batch Number = 47
01/07/2022 12:49:41 - INFO - __main__ -   Batch Number = 48
01/07/2022 12:49:41 - INFO - __main__ -   Batch Number = 49
01/07/2022 12:49:41 - INFO - __main__ -   Batch Number = 50
Evaluating:  30%|       | 50/165 [00:01<00:02, 42.65it/s]01/07/2022 12:49:41 - INFO - __main__ -   Batch Number = 51
01/07/2022 12:49:41 - INFO - __main__ -   Batch Number = 52
01/07/2022 12:49:41 - INFO - __main__ -   Batch Number = 53
01/07/2022 12:49:41 - INFO - __main__ -   Batch Number = 54
01/07/2022 12:49:41 - INFO - __main__ -   Batch Number = 55
Evaluating:  33%|      | 55/165 [00:01<00:02, 43.12it/s]01/07/2022 12:49:41 - INFO - __main__ -   Batch Number = 56
01/07/2022 12:49:41 - INFO - __main__ -   Batch Number = 57
01/07/2022 12:49:41 - INFO - __main__ -   Batch Number = 58
01/07/2022 12:49:41 - INFO - __main__ -   Batch Number = 59
01/07/2022 12:49:42 - INFO - __main__ -   Batch Number = 60
Evaluating:  36%|      | 60/165 [00:01<00:03, 31.54it/s]01/07/2022 12:49:42 - INFO - __main__ -   Batch Number = 61
01/07/2022 12:49:42 - INFO - __main__ -   Batch Number = 62
01/07/2022 12:49:42 - INFO - __main__ -   Batch Number = 63
01/07/2022 12:49:42 - INFO - __main__ -   Batch Number = 64
01/07/2022 12:49:42 - INFO - __main__ -   Batch Number = 65
Evaluating:  39%|      | 65/165 [00:01<00:02, 34.69it/s]01/07/2022 12:49:42 - INFO - __main__ -   Batch Number = 66
01/07/2022 12:49:42 - INFO - __main__ -   Batch Number = 67
01/07/2022 12:49:42 - INFO - __main__ -   Batch Number = 68
01/07/2022 12:49:42 - INFO - __main__ -   Batch Number = 69
01/07/2022 12:49:42 - INFO - __main__ -   Batch Number = 70
Evaluating:  42%|     | 70/165 [00:01<00:02, 37.12it/s]01/07/2022 12:49:42 - INFO - __main__ -   Batch Number = 71
01/07/2022 12:49:42 - INFO - __main__ -   Batch Number = 72
01/07/2022 12:49:42 - INFO - __main__ -   Batch Number = 73
01/07/2022 12:49:42 - INFO - __main__ -   Batch Number = 74
01/07/2022 12:49:42 - INFO - __main__ -   Batch Number = 75
Evaluating:  45%|     | 75/165 [00:01<00:02, 39.08it/s]01/07/2022 12:49:42 - INFO - __main__ -   Batch Number = 76
01/07/2022 12:49:42 - INFO - __main__ -   Batch Number = 77
01/07/2022 12:49:42 - INFO - __main__ -   Batch Number = 78
01/07/2022 12:49:42 - INFO - __main__ -   Batch Number = 79
01/07/2022 12:49:42 - INFO - __main__ -   Batch Number = 80
Evaluating:  48%|     | 80/165 [00:02<00:02, 40.60it/s]01/07/2022 12:49:42 - INFO - __main__ -   Batch Number = 81
01/07/2022 12:49:42 - INFO - __main__ -   Batch Number = 82
01/07/2022 12:49:42 - INFO - __main__ -   Batch Number = 83
01/07/2022 12:49:42 - INFO - __main__ -   Batch Number = 84
01/07/2022 12:49:42 - INFO - __main__ -   Batch Number = 85
Evaluating:  52%|    | 85/165 [00:02<00:01, 41.90it/s]01/07/2022 12:49:42 - INFO - __main__ -   Batch Number = 86
01/07/2022 12:49:42 - INFO - __main__ -   Batch Number = 87
01/07/2022 12:49:42 - INFO - __main__ -   Batch Number = 88
01/07/2022 12:49:42 - INFO - __main__ -   Batch Number = 89
01/07/2022 12:49:42 - INFO - __main__ -   Batch Number = 90
Evaluating:  55%|    | 90/165 [00:02<00:01, 42.55it/s]01/07/2022 12:49:42 - INFO - __main__ -   Batch Number = 91
01/07/2022 12:49:42 - INFO - __main__ -   Batch Number = 92
01/07/2022 12:49:42 - INFO - __main__ -   Batch Number = 93
01/07/2022 12:49:42 - INFO - __main__ -   Batch Number = 94
01/07/2022 12:49:42 - INFO - __main__ -   Batch Number = 95
Evaluating:  58%|    | 95/165 [00:02<00:01, 43.28it/s]01/07/2022 12:49:42 - INFO - __main__ -   Batch Number = 96
01/07/2022 12:49:42 - INFO - __main__ -   Batch Number = 97
01/07/2022 12:49:42 - INFO - __main__ -   Batch Number = 98
01/07/2022 12:49:42 - INFO - __main__ -   Batch Number = 99
01/07/2022 12:49:42 - INFO - __main__ -   Batch Number = 100
Evaluating:  61%|    | 100/165 [00:02<00:01, 43.54it/s]01/07/2022 12:49:42 - INFO - __main__ -   Batch Number = 101
01/07/2022 12:49:42 - INFO - __main__ -   Batch Number = 102
01/07/2022 12:49:43 - INFO - __main__ -   Batch Number = 103
01/07/2022 12:49:43 - INFO - __main__ -   Batch Number = 104
01/07/2022 12:49:43 - INFO - __main__ -   Batch Number = 105
Evaluating:  64%|   | 105/165 [00:02<00:01, 43.58it/s]01/07/2022 12:49:43 - INFO - __main__ -   Batch Number = 106
01/07/2022 12:49:43 - INFO - __main__ -   Batch Number = 107
01/07/2022 12:49:43 - INFO - __main__ -   Batch Number = 108
01/07/2022 12:49:43 - INFO - __main__ -   Batch Number = 109
01/07/2022 12:49:43 - INFO - __main__ -   Batch Number = 110
Evaluating:  67%|   | 110/165 [00:02<00:01, 43.75it/s]01/07/2022 12:49:43 - INFO - __main__ -   Batch Number = 111
01/07/2022 12:49:43 - INFO - __main__ -   Batch Number = 112
01/07/2022 12:49:43 - INFO - __main__ -   Batch Number = 113
01/07/2022 12:49:43 - INFO - __main__ -   Batch Number = 114
01/07/2022 12:49:43 - INFO - __main__ -   Batch Number = 115
Evaluating:  70%|   | 115/165 [00:02<00:01, 43.86it/s]01/07/2022 12:49:43 - INFO - __main__ -   Batch Number = 116
01/07/2022 12:49:43 - INFO - __main__ -   Batch Number = 117
01/07/2022 12:49:43 - INFO - __main__ -   Batch Number = 118
01/07/2022 12:49:43 - INFO - __main__ -   Batch Number = 119
01/07/2022 12:49:43 - INFO - __main__ -   Batch Number = 120
Evaluating:  73%|  | 120/165 [00:02<00:01, 43.95it/s]01/07/2022 12:49:43 - INFO - __main__ -   Batch Number = 121
01/07/2022 12:49:43 - INFO - __main__ -   Batch Number = 122
01/07/2022 12:49:43 - INFO - __main__ -   Batch Number = 123
01/07/2022 12:49:43 - INFO - __main__ -   Batch Number = 124
01/07/2022 12:49:43 - INFO - __main__ -   Batch Number = 125
Evaluating:  76%|  | 125/165 [00:03<00:00, 44.08it/s]01/07/2022 12:49:43 - INFO - __main__ -   Batch Number = 126
01/07/2022 12:49:43 - INFO - __main__ -   Batch Number = 127
01/07/2022 12:49:43 - INFO - __main__ -   Batch Number = 128
01/07/2022 12:49:43 - INFO - __main__ -   Batch Number = 129
01/07/2022 12:49:43 - INFO - __main__ -   Batch Number = 130
Evaluating:  79%|  | 130/165 [00:03<00:00, 44.25it/s]01/07/2022 12:49:43 - INFO - __main__ -   Batch Number = 131
01/07/2022 12:49:43 - INFO - __main__ -   Batch Number = 132
01/07/2022 12:49:43 - INFO - __main__ -   Batch Number = 133
01/07/2022 12:49:43 - INFO - __main__ -   Batch Number = 134
01/07/2022 12:49:43 - INFO - __main__ -   Batch Number = 135
Evaluating:  82%| | 135/165 [00:03<00:00, 44.23it/s]01/07/2022 12:49:43 - INFO - __main__ -   Batch Number = 136
01/07/2022 12:49:43 - INFO - __main__ -   Batch Number = 137
01/07/2022 12:49:43 - INFO - __main__ -   Batch Number = 138
01/07/2022 12:49:43 - INFO - __main__ -   Batch Number = 139
01/07/2022 12:49:43 - INFO - __main__ -   Batch Number = 140
Evaluating:  85%| | 140/165 [00:03<00:00, 44.45it/s]01/07/2022 12:49:43 - INFO - __main__ -   Batch Number = 141
01/07/2022 12:49:43 - INFO - __main__ -   Batch Number = 142
01/07/2022 12:49:43 - INFO - __main__ -   Batch Number = 143
01/07/2022 12:49:43 - INFO - __main__ -   Batch Number = 144
01/07/2022 12:49:43 - INFO - __main__ -   Batch Number = 145
Evaluating:  88%| | 145/165 [00:03<00:00, 44.60it/s]01/07/2022 12:49:43 - INFO - __main__ -   Batch Number = 146
01/07/2022 12:49:44 - INFO - __main__ -   Batch Number = 147
01/07/2022 12:49:44 - INFO - __main__ -   Batch Number = 148
01/07/2022 12:49:44 - INFO - __main__ -   Batch Number = 149
01/07/2022 12:49:44 - INFO - __main__ -   Batch Number = 150
Evaluating:  91%| | 150/165 [00:03<00:00, 44.48it/s]01/07/2022 12:49:44 - INFO - __main__ -   Batch Number = 151
01/07/2022 12:49:44 - INFO - __main__ -   Batch Number = 152
01/07/2022 12:49:44 - INFO - __main__ -   Batch Number = 153
01/07/2022 12:49:44 - INFO - __main__ -   Batch Number = 154
01/07/2022 12:49:44 - INFO - __main__ -   Batch Number = 155
Evaluating:  94%|| 155/165 [00:03<00:00, 44.48it/s]01/07/2022 12:49:44 - INFO - __main__ -   Batch Number = 156
01/07/2022 12:49:44 - INFO - __main__ -   Batch Number = 157
01/07/2022 12:49:44 - INFO - __main__ -   Batch Number = 158
01/07/2022 12:49:44 - INFO - __main__ -   Batch Number = 159
01/07/2022 12:49:44 - INFO - __main__ -   Batch Number = 160
Evaluating:  97%|| 160/165 [00:03<00:00, 44.30it/s]01/07/2022 12:49:44 - INFO - __main__ -   Batch Number = 161
01/07/2022 12:49:44 - INFO - __main__ -   Batch Number = 162
01/07/2022 12:49:44 - INFO - __main__ -   Batch Number = 163
01/07/2022 12:49:44 - INFO - __main__ -   Batch Number = 164
01/07/2022 12:49:44 - INFO - __main__ -   Batch Number = 165
Evaluating: 100%|| 165/165 [00:03<00:00, 44.89it/s]Evaluating: 100%|| 165/165 [00:03<00:00, 41.53it/s]
01/07/2022 12:49:44 - INFO - __main__ -     Evaluation done in total 3.973121 secs (0.003015 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/predictions_ar_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/nbest_predictions_ar_.json
01/07/2022 12:49:47 - INFO - __main__ -   Results = OrderedDict([('exact', 43.865546218487395), ('f1', 60.30570324957831), ('total', 1190), ('HasAns_exact', 43.865546218487395), ('HasAns_f1', 60.30570324957831), ('HasAns_total', 1190), ('best_exact', 43.865546218487395), ('best_exact_thresh', 0.0), ('best_f1', 60.30570324957831), ('best_f1_thresh', 0.0)])
01/07/2022 12:49:47 - INFO - __main__ -   Language adapter for de not found, using en instead
01/07/2022 12:49:47 - INFO - __main__ -   Set active language adapter to en
01/07/2022 12:49:47 - INFO - __main__ -   Args Adapter Weight = None
01/07/2022 12:49:47 - INFO - __main__ -   Adapter Languages = ['en']
01/07/2022 12:49:47 - INFO - __main__ -   Predict File = xquad.de.json
01/07/2022 12:49:47 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
  0%|          | 0/48 [00:00<?, ?it/s] 33%|      | 16/48 [00:00<00:00, 120.51it/s] 60%|    | 29/48 [00:00<00:00, 113.68it/s] 92%|| 44/48 [00:00<00:00, 124.65it/s]100%|| 48/48 [00:00<00:00, 123.76it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<05:06,  3.88it/s]convert squad examples to features:  32%|      | 385/1190 [00:00<00:01, 758.40it/s]convert squad examples to features:  94%|| 1121/1190 [00:00<00:00, 2094.36it/s]convert squad examples to features: 100%|| 1190/1190 [00:00<00:00, 1575.81it/s]/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|| 1190/1190 [00:00<00:00, 567248.75it/s]
01/07/2022 12:49:49 - INFO - __main__ -   Local Rank = -1
01/07/2022 12:49:49 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.de.json_xlm-roberta-base_384_de
01/07/2022 12:49:50 - INFO - __main__ -   ***** Running evaluation  *****
01/07/2022 12:49:50 - INFO - __main__ -     Num examples = 1303
01/07/2022 12:49:50 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/163 [00:00<?, ?it/s]01/07/2022 12:49:50 - INFO - __main__ -   Batch Number = 1
01/07/2022 12:49:50 - INFO - __main__ -   Batch Number = 2
01/07/2022 12:49:50 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|         | 3/163 [00:00<00:05, 29.32it/s]01/07/2022 12:49:50 - INFO - __main__ -   Batch Number = 4
01/07/2022 12:49:50 - INFO - __main__ -   Batch Number = 5
01/07/2022 12:49:50 - INFO - __main__ -   Batch Number = 6
01/07/2022 12:49:50 - INFO - __main__ -   Batch Number = 7
Evaluating:   4%|         | 7/163 [00:00<00:04, 34.13it/s]01/07/2022 12:49:50 - INFO - __main__ -   Batch Number = 8
01/07/2022 12:49:50 - INFO - __main__ -   Batch Number = 9
01/07/2022 12:49:50 - INFO - __main__ -   Batch Number = 10
01/07/2022 12:49:50 - INFO - __main__ -   Batch Number = 11
01/07/2022 12:49:50 - INFO - __main__ -   Batch Number = 12
Evaluating:   7%|         | 12/163 [00:00<00:03, 37.94it/s]01/07/2022 12:49:50 - INFO - __main__ -   Batch Number = 13
01/07/2022 12:49:50 - INFO - __main__ -   Batch Number = 14
01/07/2022 12:49:50 - INFO - __main__ -   Batch Number = 15
01/07/2022 12:49:50 - INFO - __main__ -   Batch Number = 16
01/07/2022 12:49:50 - INFO - __main__ -   Batch Number = 17
Evaluating:  10%|         | 17/163 [00:00<00:03, 40.04it/s]01/07/2022 12:49:50 - INFO - __main__ -   Batch Number = 18
01/07/2022 12:49:51 - INFO - __main__ -   Batch Number = 19
01/07/2022 12:49:51 - INFO - __main__ -   Batch Number = 20
01/07/2022 12:49:51 - INFO - __main__ -   Batch Number = 21
Evaluating:  13%|        | 21/163 [00:00<00:04, 31.12it/s]01/07/2022 12:49:51 - INFO - __main__ -   Batch Number = 22
01/07/2022 12:49:51 - INFO - __main__ -   Batch Number = 23
01/07/2022 12:49:51 - INFO - __main__ -   Batch Number = 24
01/07/2022 12:49:51 - INFO - __main__ -   Batch Number = 25
01/07/2022 12:49:51 - INFO - __main__ -   Batch Number = 26
Evaluating:  16%|        | 26/163 [00:00<00:03, 35.00it/s]01/07/2022 12:49:51 - INFO - __main__ -   Batch Number = 27
01/07/2022 12:49:51 - INFO - __main__ -   Batch Number = 28
01/07/2022 12:49:51 - INFO - __main__ -   Batch Number = 29
01/07/2022 12:49:51 - INFO - __main__ -   Batch Number = 30
01/07/2022 12:49:51 - INFO - __main__ -   Batch Number = 31
Evaluating:  19%|        | 31/163 [00:00<00:03, 37.69it/s]01/07/2022 12:49:51 - INFO - __main__ -   Batch Number = 32
01/07/2022 12:49:51 - INFO - __main__ -   Batch Number = 33
01/07/2022 12:49:51 - INFO - __main__ -   Batch Number = 34
01/07/2022 12:49:51 - INFO - __main__ -   Batch Number = 35
01/07/2022 12:49:51 - INFO - __main__ -   Batch Number = 36
Evaluating:  22%|       | 36/163 [00:00<00:03, 39.49it/s]01/07/2022 12:49:51 - INFO - __main__ -   Batch Number = 37
01/07/2022 12:49:51 - INFO - __main__ -   Batch Number = 38
01/07/2022 12:49:51 - INFO - __main__ -   Batch Number = 39
01/07/2022 12:49:51 - INFO - __main__ -   Batch Number = 40
01/07/2022 12:49:51 - INFO - __main__ -   Batch Number = 41
Evaluating:  25%|       | 41/163 [00:01<00:02, 40.80it/s]01/07/2022 12:49:51 - INFO - __main__ -   Batch Number = 42
01/07/2022 12:49:51 - INFO - __main__ -   Batch Number = 43
01/07/2022 12:49:51 - INFO - __main__ -   Batch Number = 44
01/07/2022 12:49:51 - INFO - __main__ -   Batch Number = 45
01/07/2022 12:49:51 - INFO - __main__ -   Batch Number = 46
Evaluating:  28%|       | 46/163 [00:01<00:02, 41.58it/s]01/07/2022 12:49:51 - INFO - __main__ -   Batch Number = 47
01/07/2022 12:49:51 - INFO - __main__ -   Batch Number = 48
01/07/2022 12:49:51 - INFO - __main__ -   Batch Number = 49
01/07/2022 12:49:51 - INFO - __main__ -   Batch Number = 50
01/07/2022 12:49:51 - INFO - __main__ -   Batch Number = 51
Evaluating:  31%|      | 51/163 [00:01<00:02, 42.28it/s]01/07/2022 12:49:51 - INFO - __main__ -   Batch Number = 52
01/07/2022 12:49:51 - INFO - __main__ -   Batch Number = 53
01/07/2022 12:49:51 - INFO - __main__ -   Batch Number = 54
01/07/2022 12:49:51 - INFO - __main__ -   Batch Number = 55
01/07/2022 12:49:51 - INFO - __main__ -   Batch Number = 56
Evaluating:  34%|      | 56/163 [00:01<00:02, 42.52it/s]01/07/2022 12:49:51 - INFO - __main__ -   Batch Number = 57
01/07/2022 12:49:51 - INFO - __main__ -   Batch Number = 58
01/07/2022 12:49:51 - INFO - __main__ -   Batch Number = 59
01/07/2022 12:49:51 - INFO - __main__ -   Batch Number = 60
01/07/2022 12:49:51 - INFO - __main__ -   Batch Number = 61
Evaluating:  37%|      | 61/163 [00:01<00:02, 42.83it/s]01/07/2022 12:49:51 - INFO - __main__ -   Batch Number = 62
01/07/2022 12:49:52 - INFO - __main__ -   Batch Number = 63
01/07/2022 12:49:52 - INFO - __main__ -   Batch Number = 64
01/07/2022 12:49:52 - INFO - __main__ -   Batch Number = 65
01/07/2022 12:49:52 - INFO - __main__ -   Batch Number = 66
Evaluating:  40%|      | 66/163 [00:01<00:02, 43.26it/s]01/07/2022 12:49:52 - INFO - __main__ -   Batch Number = 67
01/07/2022 12:49:52 - INFO - __main__ -   Batch Number = 68
01/07/2022 12:49:52 - INFO - __main__ -   Batch Number = 69
01/07/2022 12:49:52 - INFO - __main__ -   Batch Number = 70
01/07/2022 12:49:52 - INFO - __main__ -   Batch Number = 71
Evaluating:  44%|     | 71/163 [00:01<00:02, 43.56it/s]01/07/2022 12:49:52 - INFO - __main__ -   Batch Number = 72
01/07/2022 12:49:52 - INFO - __main__ -   Batch Number = 73
01/07/2022 12:49:52 - INFO - __main__ -   Batch Number = 74
01/07/2022 12:49:52 - INFO - __main__ -   Batch Number = 75
01/07/2022 12:49:52 - INFO - __main__ -   Batch Number = 76
Evaluating:  47%|     | 76/163 [00:01<00:01, 43.75it/s]01/07/2022 12:49:52 - INFO - __main__ -   Batch Number = 77
01/07/2022 12:49:52 - INFO - __main__ -   Batch Number = 78
01/07/2022 12:49:52 - INFO - __main__ -   Batch Number = 79
01/07/2022 12:49:52 - INFO - __main__ -   Batch Number = 80
01/07/2022 12:49:52 - INFO - __main__ -   Batch Number = 81
Evaluating:  50%|     | 81/163 [00:01<00:01, 43.89it/s]01/07/2022 12:49:52 - INFO - __main__ -   Batch Number = 82
01/07/2022 12:49:52 - INFO - __main__ -   Batch Number = 83
01/07/2022 12:49:52 - INFO - __main__ -   Batch Number = 84
01/07/2022 12:49:52 - INFO - __main__ -   Batch Number = 85
01/07/2022 12:49:52 - INFO - __main__ -   Batch Number = 86
Evaluating:  53%|    | 86/163 [00:02<00:01, 44.21it/s]01/07/2022 12:49:52 - INFO - __main__ -   Batch Number = 87
01/07/2022 12:49:52 - INFO - __main__ -   Batch Number = 88
01/07/2022 12:49:52 - INFO - __main__ -   Batch Number = 89
01/07/2022 12:49:52 - INFO - __main__ -   Batch Number = 90
01/07/2022 12:49:52 - INFO - __main__ -   Batch Number = 91
Evaluating:  56%|    | 91/163 [00:02<00:01, 44.27it/s]01/07/2022 12:49:52 - INFO - __main__ -   Batch Number = 92
01/07/2022 12:49:52 - INFO - __main__ -   Batch Number = 93
01/07/2022 12:49:52 - INFO - __main__ -   Batch Number = 94
01/07/2022 12:49:52 - INFO - __main__ -   Batch Number = 95
01/07/2022 12:49:52 - INFO - __main__ -   Batch Number = 96
Evaluating:  59%|    | 96/163 [00:02<00:01, 44.41it/s]01/07/2022 12:49:52 - INFO - __main__ -   Batch Number = 97
01/07/2022 12:49:52 - INFO - __main__ -   Batch Number = 98
01/07/2022 12:49:52 - INFO - __main__ -   Batch Number = 99
01/07/2022 12:49:52 - INFO - __main__ -   Batch Number = 100
01/07/2022 12:49:52 - INFO - __main__ -   Batch Number = 101
Evaluating:  62%|   | 101/163 [00:02<00:01, 44.56it/s]01/07/2022 12:49:52 - INFO - __main__ -   Batch Number = 102
01/07/2022 12:49:52 - INFO - __main__ -   Batch Number = 103
01/07/2022 12:49:52 - INFO - __main__ -   Batch Number = 104
01/07/2022 12:49:52 - INFO - __main__ -   Batch Number = 105
01/07/2022 12:49:52 - INFO - __main__ -   Batch Number = 106
Evaluating:  65%|   | 106/163 [00:02<00:01, 44.23it/s]01/07/2022 12:49:53 - INFO - __main__ -   Batch Number = 107
01/07/2022 12:49:53 - INFO - __main__ -   Batch Number = 108
01/07/2022 12:49:53 - INFO - __main__ -   Batch Number = 109
01/07/2022 12:49:53 - INFO - __main__ -   Batch Number = 110
01/07/2022 12:49:53 - INFO - __main__ -   Batch Number = 111
Evaluating:  68%|   | 111/163 [00:02<00:01, 44.25it/s]01/07/2022 12:49:53 - INFO - __main__ -   Batch Number = 112
01/07/2022 12:49:53 - INFO - __main__ -   Batch Number = 113
01/07/2022 12:49:53 - INFO - __main__ -   Batch Number = 114
01/07/2022 12:49:53 - INFO - __main__ -   Batch Number = 115
01/07/2022 12:49:53 - INFO - __main__ -   Batch Number = 116
Evaluating:  71%|   | 116/163 [00:02<00:01, 44.27it/s]01/07/2022 12:49:53 - INFO - __main__ -   Batch Number = 117
01/07/2022 12:49:53 - INFO - __main__ -   Batch Number = 118
01/07/2022 12:49:53 - INFO - __main__ -   Batch Number = 119
01/07/2022 12:49:53 - INFO - __main__ -   Batch Number = 120
01/07/2022 12:49:53 - INFO - __main__ -   Batch Number = 121
Evaluating:  74%|  | 121/163 [00:02<00:00, 44.19it/s]01/07/2022 12:49:53 - INFO - __main__ -   Batch Number = 122
01/07/2022 12:49:53 - INFO - __main__ -   Batch Number = 123
01/07/2022 12:49:53 - INFO - __main__ -   Batch Number = 124
01/07/2022 12:49:53 - INFO - __main__ -   Batch Number = 125
01/07/2022 12:49:53 - INFO - __main__ -   Batch Number = 126
Evaluating:  77%|  | 126/163 [00:03<00:00, 44.38it/s]01/07/2022 12:49:53 - INFO - __main__ -   Batch Number = 127
01/07/2022 12:49:53 - INFO - __main__ -   Batch Number = 128
01/07/2022 12:49:53 - INFO - __main__ -   Batch Number = 129
01/07/2022 12:49:53 - INFO - __main__ -   Batch Number = 130
01/07/2022 12:49:53 - INFO - __main__ -   Batch Number = 131
Evaluating:  80%|  | 131/163 [00:03<00:00, 44.53it/s]01/07/2022 12:49:53 - INFO - __main__ -   Batch Number = 132
01/07/2022 12:49:53 - INFO - __main__ -   Batch Number = 133
01/07/2022 12:49:53 - INFO - __main__ -   Batch Number = 134
01/07/2022 12:49:53 - INFO - __main__ -   Batch Number = 135
01/07/2022 12:49:53 - INFO - __main__ -   Batch Number = 136
Evaluating:  83%| | 136/163 [00:03<00:00, 44.53it/s]01/07/2022 12:49:53 - INFO - __main__ -   Batch Number = 137
01/07/2022 12:49:53 - INFO - __main__ -   Batch Number = 138
01/07/2022 12:49:53 - INFO - __main__ -   Batch Number = 139
01/07/2022 12:49:53 - INFO - __main__ -   Batch Number = 140
01/07/2022 12:49:53 - INFO - __main__ -   Batch Number = 141
Evaluating:  87%| | 141/163 [00:03<00:00, 44.59it/s]01/07/2022 12:49:53 - INFO - __main__ -   Batch Number = 142
01/07/2022 12:49:53 - INFO - __main__ -   Batch Number = 143
01/07/2022 12:49:53 - INFO - __main__ -   Batch Number = 144
01/07/2022 12:49:53 - INFO - __main__ -   Batch Number = 145
01/07/2022 12:49:53 - INFO - __main__ -   Batch Number = 146
Evaluating:  90%| | 146/163 [00:03<00:00, 44.70it/s]01/07/2022 12:49:53 - INFO - __main__ -   Batch Number = 147
01/07/2022 12:49:53 - INFO - __main__ -   Batch Number = 148
01/07/2022 12:49:53 - INFO - __main__ -   Batch Number = 149
01/07/2022 12:49:53 - INFO - __main__ -   Batch Number = 150
01/07/2022 12:49:54 - INFO - __main__ -   Batch Number = 151
Evaluating:  93%|| 151/163 [00:03<00:00, 44.35it/s]01/07/2022 12:49:54 - INFO - __main__ -   Batch Number = 152
01/07/2022 12:49:54 - INFO - __main__ -   Batch Number = 153
01/07/2022 12:49:54 - INFO - __main__ -   Batch Number = 154
01/07/2022 12:49:54 - INFO - __main__ -   Batch Number = 155
01/07/2022 12:49:54 - INFO - __main__ -   Batch Number = 156
Evaluating:  96%|| 156/163 [00:03<00:00, 44.44it/s]01/07/2022 12:49:54 - INFO - __main__ -   Batch Number = 157
01/07/2022 12:49:54 - INFO - __main__ -   Batch Number = 158
01/07/2022 12:49:54 - INFO - __main__ -   Batch Number = 159
01/07/2022 12:49:54 - INFO - __main__ -   Batch Number = 160
01/07/2022 12:49:54 - INFO - __main__ -   Batch Number = 161
Evaluating:  99%|| 161/163 [00:03<00:00, 44.32it/s]01/07/2022 12:49:54 - INFO - __main__ -   Batch Number = 162
01/07/2022 12:49:54 - INFO - __main__ -   Batch Number = 163
Evaluating: 100%|| 163/163 [00:03<00:00, 42.45it/s]
01/07/2022 12:49:54 - INFO - __main__ -     Evaluation done in total 3.840533 secs (0.002947 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/predictions_de_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/nbest_predictions_de_.json
01/07/2022 12:49:57 - INFO - __main__ -   Results = OrderedDict([('exact', 56.80672268907563), ('f1', 72.90478654912818), ('total', 1190), ('HasAns_exact', 56.80672268907563), ('HasAns_f1', 72.90478654912818), ('HasAns_total', 1190), ('best_exact', 56.80672268907563), ('best_exact_thresh', 0.0), ('best_f1', 72.90478654912818), ('best_f1_thresh', 0.0)])
01/07/2022 12:49:57 - INFO - __main__ -   Language adapter for el not found, using en instead
01/07/2022 12:49:57 - INFO - __main__ -   Set active language adapter to en
01/07/2022 12:49:57 - INFO - __main__ -   Args Adapter Weight = None
01/07/2022 12:49:57 - INFO - __main__ -   Adapter Languages = ['en']
01/07/2022 12:49:57 - INFO - __main__ -   Predict File = xquad.el.json
01/07/2022 12:49:57 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
  0%|          | 0/48 [00:00<?, ?it/s] 29%|       | 14/48 [00:00<00:00, 138.54it/s] 58%|    | 28/48 [00:00<00:00, 104.38it/s] 83%| | 40/48 [00:00<00:00, 109.39it/s]100%|| 48/48 [00:00<00:00, 114.15it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<05:04,  3.90it/s]convert squad examples to features:  27%|       | 321/1190 [00:00<00:00, 1049.71it/s]convert squad examples to features:  40%|      | 471/1190 [00:00<00:01, 463.24it/s] convert squad examples to features: 100%|| 1190/1190 [00:00<00:00, 1209.05it/s]/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|| 1190/1190 [00:00<00:00, 638835.50it/s]
01/07/2022 12:49:59 - INFO - __main__ -   Local Rank = -1
01/07/2022 12:49:59 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.el.json_xlm-roberta-base_384_el
01/07/2022 12:50:01 - INFO - __main__ -   ***** Running evaluation  *****
01/07/2022 12:50:01 - INFO - __main__ -     Num examples = 1488
01/07/2022 12:50:01 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/186 [00:00<?, ?it/s]01/07/2022 12:50:01 - INFO - __main__ -   Batch Number = 1
01/07/2022 12:50:01 - INFO - __main__ -   Batch Number = 2
01/07/2022 12:50:01 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|         | 3/186 [00:00<00:07, 26.10it/s]01/07/2022 12:50:01 - INFO - __main__ -   Batch Number = 4
01/07/2022 12:50:01 - INFO - __main__ -   Batch Number = 5
01/07/2022 12:50:01 - INFO - __main__ -   Batch Number = 6
01/07/2022 12:50:01 - INFO - __main__ -   Batch Number = 7
Evaluating:   4%|         | 7/186 [00:00<00:05, 30.46it/s]01/07/2022 12:50:01 - INFO - __main__ -   Batch Number = 8
01/07/2022 12:50:01 - INFO - __main__ -   Batch Number = 9
01/07/2022 12:50:01 - INFO - __main__ -   Batch Number = 10
01/07/2022 12:50:01 - INFO - __main__ -   Batch Number = 11
01/07/2022 12:50:01 - INFO - __main__ -   Batch Number = 12
Evaluating:   6%|         | 12/186 [00:00<00:04, 36.86it/s]01/07/2022 12:50:01 - INFO - __main__ -   Batch Number = 13
01/07/2022 12:50:01 - INFO - __main__ -   Batch Number = 14
01/07/2022 12:50:01 - INFO - __main__ -   Batch Number = 15
01/07/2022 12:50:01 - INFO - __main__ -   Batch Number = 16
01/07/2022 12:50:01 - INFO - __main__ -   Batch Number = 17
Evaluating:   9%|         | 17/186 [00:00<00:04, 39.82it/s]01/07/2022 12:50:01 - INFO - __main__ -   Batch Number = 18
01/07/2022 12:50:01 - INFO - __main__ -   Batch Number = 19
01/07/2022 12:50:01 - INFO - __main__ -   Batch Number = 20
01/07/2022 12:50:01 - INFO - __main__ -   Batch Number = 21
01/07/2022 12:50:01 - INFO - __main__ -   Batch Number = 22
Evaluating:  12%|        | 22/186 [00:00<00:03, 41.68it/s]01/07/2022 12:50:01 - INFO - __main__ -   Batch Number = 23
01/07/2022 12:50:01 - INFO - __main__ -   Batch Number = 24
01/07/2022 12:50:01 - INFO - __main__ -   Batch Number = 25
01/07/2022 12:50:01 - INFO - __main__ -   Batch Number = 26
01/07/2022 12:50:01 - INFO - __main__ -   Batch Number = 27
Evaluating:  15%|        | 27/186 [00:00<00:03, 42.73it/s]01/07/2022 12:50:01 - INFO - __main__ -   Batch Number = 28
01/07/2022 12:50:01 - INFO - __main__ -   Batch Number = 29
01/07/2022 12:50:01 - INFO - __main__ -   Batch Number = 30
01/07/2022 12:50:01 - INFO - __main__ -   Batch Number = 31
01/07/2022 12:50:01 - INFO - __main__ -   Batch Number = 32
Evaluating:  17%|        | 32/186 [00:00<00:03, 43.22it/s]01/07/2022 12:50:01 - INFO - __main__ -   Batch Number = 33
01/07/2022 12:50:01 - INFO - __main__ -   Batch Number = 34
01/07/2022 12:50:01 - INFO - __main__ -   Batch Number = 35
01/07/2022 12:50:01 - INFO - __main__ -   Batch Number = 36
01/07/2022 12:50:01 - INFO - __main__ -   Batch Number = 37
Evaluating:  20%|        | 37/186 [00:00<00:03, 43.41it/s]01/07/2022 12:50:01 - INFO - __main__ -   Batch Number = 38
01/07/2022 12:50:02 - INFO - __main__ -   Batch Number = 39
01/07/2022 12:50:02 - INFO - __main__ -   Batch Number = 40
01/07/2022 12:50:02 - INFO - __main__ -   Batch Number = 41
01/07/2022 12:50:02 - INFO - __main__ -   Batch Number = 42
Evaluating:  23%|       | 42/186 [00:01<00:03, 43.80it/s]01/07/2022 12:50:02 - INFO - __main__ -   Batch Number = 43
01/07/2022 12:50:02 - INFO - __main__ -   Batch Number = 44
01/07/2022 12:50:02 - INFO - __main__ -   Batch Number = 45
01/07/2022 12:50:02 - INFO - __main__ -   Batch Number = 46
01/07/2022 12:50:02 - INFO - __main__ -   Batch Number = 47
Evaluating:  25%|       | 47/186 [00:01<00:03, 43.70it/s]01/07/2022 12:50:02 - INFO - __main__ -   Batch Number = 48
01/07/2022 12:50:02 - INFO - __main__ -   Batch Number = 49
01/07/2022 12:50:02 - INFO - __main__ -   Batch Number = 50
01/07/2022 12:50:02 - INFO - __main__ -   Batch Number = 51
01/07/2022 12:50:02 - INFO - __main__ -   Batch Number = 52
Evaluating:  28%|       | 52/186 [00:01<00:03, 44.05it/s]01/07/2022 12:50:02 - INFO - __main__ -   Batch Number = 53
01/07/2022 12:50:02 - INFO - __main__ -   Batch Number = 54
01/07/2022 12:50:02 - INFO - __main__ -   Batch Number = 55
01/07/2022 12:50:02 - INFO - __main__ -   Batch Number = 56
01/07/2022 12:50:02 - INFO - __main__ -   Batch Number = 57
Evaluating:  31%|       | 57/186 [00:01<00:02, 44.22it/s]01/07/2022 12:50:02 - INFO - __main__ -   Batch Number = 58
01/07/2022 12:50:02 - INFO - __main__ -   Batch Number = 59
01/07/2022 12:50:02 - INFO - __main__ -   Batch Number = 60
01/07/2022 12:50:02 - INFO - __main__ -   Batch Number = 61
01/07/2022 12:50:02 - INFO - __main__ -   Batch Number = 62
Evaluating:  33%|      | 62/186 [00:01<00:02, 44.18it/s]01/07/2022 12:50:02 - INFO - __main__ -   Batch Number = 63
01/07/2022 12:50:02 - INFO - __main__ -   Batch Number = 64
01/07/2022 12:50:02 - INFO - __main__ -   Batch Number = 65
01/07/2022 12:50:02 - INFO - __main__ -   Batch Number = 66
01/07/2022 12:50:02 - INFO - __main__ -   Batch Number = 67
Evaluating:  36%|      | 67/186 [00:01<00:02, 43.35it/s]01/07/2022 12:50:02 - INFO - __main__ -   Batch Number = 68
01/07/2022 12:50:02 - INFO - __main__ -   Batch Number = 69
01/07/2022 12:50:02 - INFO - __main__ -   Batch Number = 70
01/07/2022 12:50:02 - INFO - __main__ -   Batch Number = 71
01/07/2022 12:50:02 - INFO - __main__ -   Batch Number = 72
Evaluating:  39%|      | 72/186 [00:01<00:02, 43.79it/s]01/07/2022 12:50:02 - INFO - __main__ -   Batch Number = 73
01/07/2022 12:50:02 - INFO - __main__ -   Batch Number = 74
01/07/2022 12:50:02 - INFO - __main__ -   Batch Number = 75
01/07/2022 12:50:02 - INFO - __main__ -   Batch Number = 76
01/07/2022 12:50:02 - INFO - __main__ -   Batch Number = 77
Evaluating:  41%|     | 77/186 [00:01<00:02, 43.99it/s]01/07/2022 12:50:02 - INFO - __main__ -   Batch Number = 78
01/07/2022 12:50:02 - INFO - __main__ -   Batch Number = 79
01/07/2022 12:50:02 - INFO - __main__ -   Batch Number = 80
01/07/2022 12:50:02 - INFO - __main__ -   Batch Number = 81
01/07/2022 12:50:02 - INFO - __main__ -   Batch Number = 82
Evaluating:  44%|     | 82/186 [00:01<00:02, 44.10it/s]01/07/2022 12:50:03 - INFO - __main__ -   Batch Number = 83
01/07/2022 12:50:03 - INFO - __main__ -   Batch Number = 84
01/07/2022 12:50:03 - INFO - __main__ -   Batch Number = 85
01/07/2022 12:50:03 - INFO - __main__ -   Batch Number = 86
01/07/2022 12:50:03 - INFO - __main__ -   Batch Number = 87
Evaluating:  47%|     | 87/186 [00:02<00:02, 44.11it/s]01/07/2022 12:50:03 - INFO - __main__ -   Batch Number = 88
01/07/2022 12:50:03 - INFO - __main__ -   Batch Number = 89
01/07/2022 12:50:03 - INFO - __main__ -   Batch Number = 90
01/07/2022 12:50:03 - INFO - __main__ -   Batch Number = 91
01/07/2022 12:50:03 - INFO - __main__ -   Batch Number = 92
Evaluating:  49%|     | 92/186 [00:02<00:02, 44.13it/s]01/07/2022 12:50:03 - INFO - __main__ -   Batch Number = 93
01/07/2022 12:50:03 - INFO - __main__ -   Batch Number = 94
01/07/2022 12:50:03 - INFO - __main__ -   Batch Number = 95
01/07/2022 12:50:03 - INFO - __main__ -   Batch Number = 96
01/07/2022 12:50:03 - INFO - __main__ -   Batch Number = 97
Evaluating:  52%|    | 97/186 [00:02<00:02, 44.20it/s]01/07/2022 12:50:03 - INFO - __main__ -   Batch Number = 98
01/07/2022 12:50:03 - INFO - __main__ -   Batch Number = 99
01/07/2022 12:50:03 - INFO - __main__ -   Batch Number = 100
01/07/2022 12:50:03 - INFO - __main__ -   Batch Number = 101
01/07/2022 12:50:03 - INFO - __main__ -   Batch Number = 102
Evaluating:  55%|    | 102/186 [00:02<00:01, 44.39it/s]01/07/2022 12:50:03 - INFO - __main__ -   Batch Number = 103
01/07/2022 12:50:03 - INFO - __main__ -   Batch Number = 104
01/07/2022 12:50:03 - INFO - __main__ -   Batch Number = 105
01/07/2022 12:50:03 - INFO - __main__ -   Batch Number = 106
01/07/2022 12:50:03 - INFO - __main__ -   Batch Number = 107
Evaluating:  58%|    | 107/186 [00:02<00:01, 44.25it/s]01/07/2022 12:50:03 - INFO - __main__ -   Batch Number = 108
01/07/2022 12:50:03 - INFO - __main__ -   Batch Number = 109
01/07/2022 12:50:03 - INFO - __main__ -   Batch Number = 110
01/07/2022 12:50:03 - INFO - __main__ -   Batch Number = 111
01/07/2022 12:50:03 - INFO - __main__ -   Batch Number = 112
Evaluating:  60%|    | 112/186 [00:02<00:01, 44.28it/s]01/07/2022 12:50:03 - INFO - __main__ -   Batch Number = 113
01/07/2022 12:50:03 - INFO - __main__ -   Batch Number = 114
01/07/2022 12:50:03 - INFO - __main__ -   Batch Number = 115
01/07/2022 12:50:03 - INFO - __main__ -   Batch Number = 116
01/07/2022 12:50:03 - INFO - __main__ -   Batch Number = 117
Evaluating:  63%|   | 117/186 [00:02<00:01, 44.41it/s]01/07/2022 12:50:03 - INFO - __main__ -   Batch Number = 118
01/07/2022 12:50:03 - INFO - __main__ -   Batch Number = 119
01/07/2022 12:50:03 - INFO - __main__ -   Batch Number = 120
01/07/2022 12:50:03 - INFO - __main__ -   Batch Number = 121
01/07/2022 12:50:03 - INFO - __main__ -   Batch Number = 122
Evaluating:  66%|   | 122/186 [00:02<00:01, 42.51it/s]01/07/2022 12:50:03 - INFO - __main__ -   Batch Number = 123
01/07/2022 12:50:03 - INFO - __main__ -   Batch Number = 124
01/07/2022 12:50:03 - INFO - __main__ -   Batch Number = 125
01/07/2022 12:50:04 - INFO - __main__ -   Batch Number = 126
01/07/2022 12:50:04 - INFO - __main__ -   Batch Number = 127
Evaluating:  68%|   | 127/186 [00:02<00:01, 43.11it/s]01/07/2022 12:50:04 - INFO - __main__ -   Batch Number = 128
01/07/2022 12:50:04 - INFO - __main__ -   Batch Number = 129
01/07/2022 12:50:04 - INFO - __main__ -   Batch Number = 130
01/07/2022 12:50:04 - INFO - __main__ -   Batch Number = 131
01/07/2022 12:50:04 - INFO - __main__ -   Batch Number = 132
Evaluating:  71%|   | 132/186 [00:03<00:01, 43.55it/s]01/07/2022 12:50:04 - INFO - __main__ -   Batch Number = 133
01/07/2022 12:50:04 - INFO - __main__ -   Batch Number = 134
01/07/2022 12:50:04 - INFO - __main__ -   Batch Number = 135
01/07/2022 12:50:04 - INFO - __main__ -   Batch Number = 136
01/07/2022 12:50:04 - INFO - __main__ -   Batch Number = 137
Evaluating:  74%|  | 137/186 [00:03<00:01, 43.41it/s]01/07/2022 12:50:04 - INFO - __main__ -   Batch Number = 138
01/07/2022 12:50:04 - INFO - __main__ -   Batch Number = 139
01/07/2022 12:50:04 - INFO - __main__ -   Batch Number = 140
01/07/2022 12:50:04 - INFO - __main__ -   Batch Number = 141
01/07/2022 12:50:04 - INFO - __main__ -   Batch Number = 142
Evaluating:  76%|  | 142/186 [00:03<00:01, 43.48it/s]01/07/2022 12:50:04 - INFO - __main__ -   Batch Number = 143
01/07/2022 12:50:04 - INFO - __main__ -   Batch Number = 144
01/07/2022 12:50:04 - INFO - __main__ -   Batch Number = 145
01/07/2022 12:50:04 - INFO - __main__ -   Batch Number = 146
01/07/2022 12:50:04 - INFO - __main__ -   Batch Number = 147
Evaluating:  79%|  | 147/186 [00:03<00:00, 43.74it/s]01/07/2022 12:50:04 - INFO - __main__ -   Batch Number = 148
01/07/2022 12:50:04 - INFO - __main__ -   Batch Number = 149
01/07/2022 12:50:04 - INFO - __main__ -   Batch Number = 150
01/07/2022 12:50:04 - INFO - __main__ -   Batch Number = 151
01/07/2022 12:50:04 - INFO - __main__ -   Batch Number = 152
Evaluating:  82%| | 152/186 [00:03<00:00, 43.74it/s]01/07/2022 12:50:04 - INFO - __main__ -   Batch Number = 153
01/07/2022 12:50:04 - INFO - __main__ -   Batch Number = 154
01/07/2022 12:50:04 - INFO - __main__ -   Batch Number = 155
01/07/2022 12:50:04 - INFO - __main__ -   Batch Number = 156
01/07/2022 12:50:04 - INFO - __main__ -   Batch Number = 157
Evaluating:  84%| | 157/186 [00:03<00:00, 43.83it/s]01/07/2022 12:50:04 - INFO - __main__ -   Batch Number = 158
01/07/2022 12:50:04 - INFO - __main__ -   Batch Number = 159
01/07/2022 12:50:04 - INFO - __main__ -   Batch Number = 160
01/07/2022 12:50:04 - INFO - __main__ -   Batch Number = 161
01/07/2022 12:50:04 - INFO - __main__ -   Batch Number = 162
Evaluating:  87%| | 162/186 [00:03<00:00, 44.17it/s]01/07/2022 12:50:04 - INFO - __main__ -   Batch Number = 163
01/07/2022 12:50:04 - INFO - __main__ -   Batch Number = 164
01/07/2022 12:50:04 - INFO - __main__ -   Batch Number = 165
01/07/2022 12:50:04 - INFO - __main__ -   Batch Number = 166
01/07/2022 12:50:04 - INFO - __main__ -   Batch Number = 167
Evaluating:  90%| | 167/186 [00:03<00:00, 44.14it/s]01/07/2022 12:50:04 - INFO - __main__ -   Batch Number = 168
01/07/2022 12:50:04 - INFO - __main__ -   Batch Number = 169
01/07/2022 12:50:04 - INFO - __main__ -   Batch Number = 170
01/07/2022 12:50:05 - INFO - __main__ -   Batch Number = 171
01/07/2022 12:50:05 - INFO - __main__ -   Batch Number = 172
Evaluating:  92%|| 172/186 [00:03<00:00, 44.37it/s]01/07/2022 12:50:05 - INFO - __main__ -   Batch Number = 173
01/07/2022 12:50:05 - INFO - __main__ -   Batch Number = 174
01/07/2022 12:50:05 - INFO - __main__ -   Batch Number = 175
01/07/2022 12:50:05 - INFO - __main__ -   Batch Number = 176
01/07/2022 12:50:05 - INFO - __main__ -   Batch Number = 177
Evaluating:  95%|| 177/186 [00:04<00:00, 44.28it/s]01/07/2022 12:50:05 - INFO - __main__ -   Batch Number = 178
01/07/2022 12:50:05 - INFO - __main__ -   Batch Number = 179
01/07/2022 12:50:05 - INFO - __main__ -   Batch Number = 180
01/07/2022 12:50:05 - INFO - __main__ -   Batch Number = 181
01/07/2022 12:50:05 - INFO - __main__ -   Batch Number = 182
Evaluating:  98%|| 182/186 [00:04<00:00, 43.91it/s]01/07/2022 12:50:05 - INFO - __main__ -   Batch Number = 183
01/07/2022 12:50:05 - INFO - __main__ -   Batch Number = 184
01/07/2022 12:50:05 - INFO - __main__ -   Batch Number = 185
01/07/2022 12:50:05 - INFO - __main__ -   Batch Number = 186
Evaluating: 100%|| 186/186 [00:04<00:00, 43.25it/s]
01/07/2022 12:50:05 - INFO - __main__ -     Evaluation done in total 4.301085 secs (0.002891 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/predictions_el_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/nbest_predictions_el_.json
01/07/2022 12:50:08 - INFO - __main__ -   Results = OrderedDict([('exact', 50.7563025210084), ('f1', 67.64918824575591), ('total', 1190), ('HasAns_exact', 50.7563025210084), ('HasAns_f1', 67.64918824575591), ('HasAns_total', 1190), ('best_exact', 50.7563025210084), ('best_exact_thresh', 0.0), ('best_f1', 67.64918824575591), ('best_f1_thresh', 0.0)])
01/07/2022 12:50:08 - INFO - __main__ -   Language adapter for es not found, using en instead
01/07/2022 12:50:08 - INFO - __main__ -   Set active language adapter to en
01/07/2022 12:50:08 - INFO - __main__ -   Args Adapter Weight = None
01/07/2022 12:50:08 - INFO - __main__ -   Adapter Languages = ['en']
01/07/2022 12:50:08 - INFO - __main__ -   Predict File = xquad.es.json
01/07/2022 12:50:08 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
  0%|          | 0/48 [00:00<?, ?it/s] 33%|      | 16/48 [00:00<00:00, 122.86it/s] 60%|    | 29/48 [00:00<00:00, 116.26it/s] 90%| | 43/48 [00:00<00:00, 124.00it/s]100%|| 48/48 [00:00<00:00, 124.96it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<05:33,  3.56it/s]convert squad examples to features:  32%|      | 385/1190 [00:00<00:01, 587.56it/s]convert squad examples to features:  86%| | 1025/1190 [00:00<00:00, 1627.58it/s]convert squad examples to features: 100%|| 1190/1190 [00:00<00:00, 1317.33it/s]/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|| 1190/1190 [00:00<00:00, 452965.04it/s]
01/07/2022 12:50:10 - INFO - __main__ -   Local Rank = -1
01/07/2022 12:50:10 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.es.json_xlm-roberta-base_384_es
01/07/2022 12:50:11 - INFO - __main__ -   ***** Running evaluation  *****
01/07/2022 12:50:11 - INFO - __main__ -     Num examples = 1304
01/07/2022 12:50:11 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/163 [00:00<?, ?it/s]01/07/2022 12:50:11 - INFO - __main__ -   Batch Number = 1
01/07/2022 12:50:11 - INFO - __main__ -   Batch Number = 2
01/07/2022 12:50:11 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|         | 3/163 [00:00<00:05, 29.24it/s]01/07/2022 12:50:11 - INFO - __main__ -   Batch Number = 4
01/07/2022 12:50:11 - INFO - __main__ -   Batch Number = 5
01/07/2022 12:50:11 - INFO - __main__ -   Batch Number = 6
01/07/2022 12:50:11 - INFO - __main__ -   Batch Number = 7
Evaluating:   4%|         | 7/163 [00:00<00:04, 32.57it/s]01/07/2022 12:50:12 - INFO - __main__ -   Batch Number = 8
01/07/2022 12:50:12 - INFO - __main__ -   Batch Number = 9
01/07/2022 12:50:12 - INFO - __main__ -   Batch Number = 10
01/07/2022 12:50:12 - INFO - __main__ -   Batch Number = 11
01/07/2022 12:50:12 - INFO - __main__ -   Batch Number = 12
Evaluating:   7%|         | 12/163 [00:00<00:03, 37.86it/s]01/07/2022 12:50:12 - INFO - __main__ -   Batch Number = 13
01/07/2022 12:50:12 - INFO - __main__ -   Batch Number = 14
01/07/2022 12:50:12 - INFO - __main__ -   Batch Number = 15
01/07/2022 12:50:12 - INFO - __main__ -   Batch Number = 16
01/07/2022 12:50:12 - INFO - __main__ -   Batch Number = 17
Evaluating:  10%|         | 17/163 [00:00<00:03, 40.36it/s]01/07/2022 12:50:12 - INFO - __main__ -   Batch Number = 18
01/07/2022 12:50:12 - INFO - __main__ -   Batch Number = 19
01/07/2022 12:50:12 - INFO - __main__ -   Batch Number = 20
01/07/2022 12:50:12 - INFO - __main__ -   Batch Number = 21
01/07/2022 12:50:12 - INFO - __main__ -   Batch Number = 22
Evaluating:  13%|        | 22/163 [00:00<00:03, 41.95it/s]01/07/2022 12:50:12 - INFO - __main__ -   Batch Number = 23
01/07/2022 12:50:12 - INFO - __main__ -   Batch Number = 24
01/07/2022 12:50:12 - INFO - __main__ -   Batch Number = 25
01/07/2022 12:50:12 - INFO - __main__ -   Batch Number = 26
01/07/2022 12:50:12 - INFO - __main__ -   Batch Number = 27
Evaluating:  17%|        | 27/163 [00:00<00:03, 42.74it/s]01/07/2022 12:50:12 - INFO - __main__ -   Batch Number = 28
01/07/2022 12:50:12 - INFO - __main__ -   Batch Number = 29
01/07/2022 12:50:12 - INFO - __main__ -   Batch Number = 30
01/07/2022 12:50:12 - INFO - __main__ -   Batch Number = 31
01/07/2022 12:50:12 - INFO - __main__ -   Batch Number = 32
Evaluating:  20%|        | 32/163 [00:00<00:03, 42.90it/s]01/07/2022 12:50:12 - INFO - __main__ -   Batch Number = 33
01/07/2022 12:50:12 - INFO - __main__ -   Batch Number = 34
01/07/2022 12:50:12 - INFO - __main__ -   Batch Number = 35
01/07/2022 12:50:12 - INFO - __main__ -   Batch Number = 36
01/07/2022 12:50:12 - INFO - __main__ -   Batch Number = 37
Evaluating:  23%|       | 37/163 [00:00<00:02, 43.47it/s]01/07/2022 12:50:12 - INFO - __main__ -   Batch Number = 38
01/07/2022 12:50:12 - INFO - __main__ -   Batch Number = 39
01/07/2022 12:50:12 - INFO - __main__ -   Batch Number = 40
01/07/2022 12:50:12 - INFO - __main__ -   Batch Number = 41
01/07/2022 12:50:12 - INFO - __main__ -   Batch Number = 42
Evaluating:  26%|       | 42/163 [00:01<00:02, 43.74it/s]01/07/2022 12:50:12 - INFO - __main__ -   Batch Number = 43
01/07/2022 12:50:12 - INFO - __main__ -   Batch Number = 44
01/07/2022 12:50:12 - INFO - __main__ -   Batch Number = 45
01/07/2022 12:50:12 - INFO - __main__ -   Batch Number = 46
01/07/2022 12:50:12 - INFO - __main__ -   Batch Number = 47
Evaluating:  29%|       | 47/163 [00:01<00:02, 43.96it/s]01/07/2022 12:50:12 - INFO - __main__ -   Batch Number = 48
01/07/2022 12:50:12 - INFO - __main__ -   Batch Number = 49
01/07/2022 12:50:12 - INFO - __main__ -   Batch Number = 50
01/07/2022 12:50:12 - INFO - __main__ -   Batch Number = 51
01/07/2022 12:50:13 - INFO - __main__ -   Batch Number = 52
Evaluating:  32%|      | 52/163 [00:01<00:02, 43.99it/s]01/07/2022 12:50:13 - INFO - __main__ -   Batch Number = 53
01/07/2022 12:50:13 - INFO - __main__ -   Batch Number = 54
01/07/2022 12:50:13 - INFO - __main__ -   Batch Number = 55
01/07/2022 12:50:13 - INFO - __main__ -   Batch Number = 56
01/07/2022 12:50:13 - INFO - __main__ -   Batch Number = 57
Evaluating:  35%|      | 57/163 [00:01<00:02, 43.72it/s]01/07/2022 12:50:13 - INFO - __main__ -   Batch Number = 58
01/07/2022 12:50:13 - INFO - __main__ -   Batch Number = 59
01/07/2022 12:50:13 - INFO - __main__ -   Batch Number = 60
01/07/2022 12:50:13 - INFO - __main__ -   Batch Number = 61
01/07/2022 12:50:13 - INFO - __main__ -   Batch Number = 62
Evaluating:  38%|      | 62/163 [00:01<00:02, 42.08it/s]01/07/2022 12:50:13 - INFO - __main__ -   Batch Number = 63
01/07/2022 12:50:13 - INFO - __main__ -   Batch Number = 64
01/07/2022 12:50:13 - INFO - __main__ -   Batch Number = 65
01/07/2022 12:50:13 - INFO - __main__ -   Batch Number = 66
01/07/2022 12:50:13 - INFO - __main__ -   Batch Number = 67
Evaluating:  41%|      | 67/163 [00:01<00:02, 42.88it/s]01/07/2022 12:50:13 - INFO - __main__ -   Batch Number = 68
01/07/2022 12:50:13 - INFO - __main__ -   Batch Number = 69
01/07/2022 12:50:13 - INFO - __main__ -   Batch Number = 70
01/07/2022 12:50:13 - INFO - __main__ -   Batch Number = 71
01/07/2022 12:50:13 - INFO - __main__ -   Batch Number = 72
Evaluating:  44%|     | 72/163 [00:01<00:02, 43.30it/s]01/07/2022 12:50:13 - INFO - __main__ -   Batch Number = 73
01/07/2022 12:50:13 - INFO - __main__ -   Batch Number = 74
01/07/2022 12:50:13 - INFO - __main__ -   Batch Number = 75
01/07/2022 12:50:13 - INFO - __main__ -   Batch Number = 76
01/07/2022 12:50:13 - INFO - __main__ -   Batch Number = 77
Evaluating:  47%|     | 77/163 [00:01<00:01, 43.47it/s]01/07/2022 12:50:13 - INFO - __main__ -   Batch Number = 78
01/07/2022 12:50:13 - INFO - __main__ -   Batch Number = 79
01/07/2022 12:50:13 - INFO - __main__ -   Batch Number = 80
01/07/2022 12:50:13 - INFO - __main__ -   Batch Number = 81
01/07/2022 12:50:13 - INFO - __main__ -   Batch Number = 82
Evaluating:  50%|     | 82/163 [00:01<00:01, 43.84it/s]01/07/2022 12:50:13 - INFO - __main__ -   Batch Number = 83
01/07/2022 12:50:13 - INFO - __main__ -   Batch Number = 84
01/07/2022 12:50:13 - INFO - __main__ -   Batch Number = 85
01/07/2022 12:50:13 - INFO - __main__ -   Batch Number = 86
01/07/2022 12:50:13 - INFO - __main__ -   Batch Number = 87
Evaluating:  53%|    | 87/163 [00:02<00:01, 43.98it/s]01/07/2022 12:50:13 - INFO - __main__ -   Batch Number = 88
01/07/2022 12:50:13 - INFO - __main__ -   Batch Number = 89
01/07/2022 12:50:13 - INFO - __main__ -   Batch Number = 90
01/07/2022 12:50:13 - INFO - __main__ -   Batch Number = 91
01/07/2022 12:50:13 - INFO - __main__ -   Batch Number = 92
Evaluating:  56%|    | 92/163 [00:02<00:01, 44.02it/s]01/07/2022 12:50:13 - INFO - __main__ -   Batch Number = 93
01/07/2022 12:50:13 - INFO - __main__ -   Batch Number = 94
01/07/2022 12:50:13 - INFO - __main__ -   Batch Number = 95
01/07/2022 12:50:14 - INFO - __main__ -   Batch Number = 96
01/07/2022 12:50:14 - INFO - __main__ -   Batch Number = 97
Evaluating:  60%|    | 97/163 [00:02<00:01, 44.15it/s]01/07/2022 12:50:14 - INFO - __main__ -   Batch Number = 98
01/07/2022 12:50:14 - INFO - __main__ -   Batch Number = 99
01/07/2022 12:50:14 - INFO - __main__ -   Batch Number = 100
01/07/2022 12:50:14 - INFO - __main__ -   Batch Number = 101
01/07/2022 12:50:14 - INFO - __main__ -   Batch Number = 102
Evaluating:  63%|   | 102/163 [00:02<00:01, 44.27it/s]01/07/2022 12:50:14 - INFO - __main__ -   Batch Number = 103
01/07/2022 12:50:14 - INFO - __main__ -   Batch Number = 104
01/07/2022 12:50:14 - INFO - __main__ -   Batch Number = 105
01/07/2022 12:50:14 - INFO - __main__ -   Batch Number = 106
01/07/2022 12:50:14 - INFO - __main__ -   Batch Number = 107
Evaluating:  66%|   | 107/163 [00:02<00:01, 44.19it/s]01/07/2022 12:50:14 - INFO - __main__ -   Batch Number = 108
01/07/2022 12:50:14 - INFO - __main__ -   Batch Number = 109
01/07/2022 12:50:14 - INFO - __main__ -   Batch Number = 110
01/07/2022 12:50:14 - INFO - __main__ -   Batch Number = 111
01/07/2022 12:50:14 - INFO - __main__ -   Batch Number = 112
Evaluating:  69%|   | 112/163 [00:02<00:01, 44.37it/s]01/07/2022 12:50:14 - INFO - __main__ -   Batch Number = 113
01/07/2022 12:50:14 - INFO - __main__ -   Batch Number = 114
01/07/2022 12:50:14 - INFO - __main__ -   Batch Number = 115
01/07/2022 12:50:14 - INFO - __main__ -   Batch Number = 116
01/07/2022 12:50:14 - INFO - __main__ -   Batch Number = 117
Evaluating:  72%|  | 117/163 [00:02<00:01, 44.42it/s]01/07/2022 12:50:14 - INFO - __main__ -   Batch Number = 118
01/07/2022 12:50:14 - INFO - __main__ -   Batch Number = 119
01/07/2022 12:50:14 - INFO - __main__ -   Batch Number = 120
01/07/2022 12:50:14 - INFO - __main__ -   Batch Number = 121
01/07/2022 12:50:14 - INFO - __main__ -   Batch Number = 122
Evaluating:  75%|  | 122/163 [00:02<00:00, 44.37it/s]01/07/2022 12:50:14 - INFO - __main__ -   Batch Number = 123
01/07/2022 12:50:14 - INFO - __main__ -   Batch Number = 124
01/07/2022 12:50:14 - INFO - __main__ -   Batch Number = 125
01/07/2022 12:50:14 - INFO - __main__ -   Batch Number = 126
01/07/2022 12:50:14 - INFO - __main__ -   Batch Number = 127
Evaluating:  78%|  | 127/163 [00:02<00:00, 44.43it/s]01/07/2022 12:50:14 - INFO - __main__ -   Batch Number = 128
01/07/2022 12:50:14 - INFO - __main__ -   Batch Number = 129
01/07/2022 12:50:14 - INFO - __main__ -   Batch Number = 130
01/07/2022 12:50:14 - INFO - __main__ -   Batch Number = 131
01/07/2022 12:50:14 - INFO - __main__ -   Batch Number = 132
Evaluating:  81%|  | 132/163 [00:03<00:00, 44.51it/s]01/07/2022 12:50:14 - INFO - __main__ -   Batch Number = 133
01/07/2022 12:50:14 - INFO - __main__ -   Batch Number = 134
01/07/2022 12:50:14 - INFO - __main__ -   Batch Number = 135
01/07/2022 12:50:14 - INFO - __main__ -   Batch Number = 136
01/07/2022 12:50:14 - INFO - __main__ -   Batch Number = 137
Evaluating:  84%| | 137/163 [00:03<00:00, 44.40it/s]01/07/2022 12:50:14 - INFO - __main__ -   Batch Number = 138
01/07/2022 12:50:14 - INFO - __main__ -   Batch Number = 139
01/07/2022 12:50:15 - INFO - __main__ -   Batch Number = 140
01/07/2022 12:50:15 - INFO - __main__ -   Batch Number = 141
01/07/2022 12:50:15 - INFO - __main__ -   Batch Number = 142
Evaluating:  87%| | 142/163 [00:03<00:00, 44.48it/s]01/07/2022 12:50:15 - INFO - __main__ -   Batch Number = 143
01/07/2022 12:50:15 - INFO - __main__ -   Batch Number = 144
01/07/2022 12:50:15 - INFO - __main__ -   Batch Number = 145
01/07/2022 12:50:15 - INFO - __main__ -   Batch Number = 146
01/07/2022 12:50:15 - INFO - __main__ -   Batch Number = 147
Evaluating:  90%| | 147/163 [00:03<00:00, 44.46it/s]01/07/2022 12:50:15 - INFO - __main__ -   Batch Number = 148
01/07/2022 12:50:15 - INFO - __main__ -   Batch Number = 149
01/07/2022 12:50:15 - INFO - __main__ -   Batch Number = 150
01/07/2022 12:50:15 - INFO - __main__ -   Batch Number = 151
01/07/2022 12:50:15 - INFO - __main__ -   Batch Number = 152
Evaluating:  93%|| 152/163 [00:03<00:00, 44.43it/s]01/07/2022 12:50:15 - INFO - __main__ -   Batch Number = 153
01/07/2022 12:50:15 - INFO - __main__ -   Batch Number = 154
01/07/2022 12:50:15 - INFO - __main__ -   Batch Number = 155
01/07/2022 12:50:15 - INFO - __main__ -   Batch Number = 156
01/07/2022 12:50:15 - INFO - __main__ -   Batch Number = 157
Evaluating:  96%|| 157/163 [00:03<00:00, 44.53it/s]01/07/2022 12:50:15 - INFO - __main__ -   Batch Number = 158
01/07/2022 12:50:15 - INFO - __main__ -   Batch Number = 159
01/07/2022 12:50:15 - INFO - __main__ -   Batch Number = 160
01/07/2022 12:50:15 - INFO - __main__ -   Batch Number = 161
01/07/2022 12:50:15 - INFO - __main__ -   Batch Number = 162
Evaluating:  99%|| 162/163 [00:03<00:00, 44.70it/s]01/07/2022 12:50:15 - INFO - __main__ -   Batch Number = 163
Evaluating: 100%|| 163/163 [00:03<00:00, 43.44it/s]
01/07/2022 12:50:15 - INFO - __main__ -     Evaluation done in total 3.752800 secs (0.002878 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/predictions_es_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/nbest_predictions_es_.json
01/07/2022 12:50:18 - INFO - __main__ -   Results = OrderedDict([('exact', 55.79831932773109), ('f1', 73.70603293660602), ('total', 1190), ('HasAns_exact', 55.79831932773109), ('HasAns_f1', 73.70603293660602), ('HasAns_total', 1190), ('best_exact', 55.79831932773109), ('best_exact_thresh', 0.0), ('best_f1', 73.70603293660602), ('best_f1_thresh', 0.0)])
01/07/2022 12:50:18 - INFO - __main__ -   Language adapter for hi not found, using en instead
01/07/2022 12:50:18 - INFO - __main__ -   Set active language adapter to en
01/07/2022 12:50:18 - INFO - __main__ -   Args Adapter Weight = None
01/07/2022 12:50:18 - INFO - __main__ -   Adapter Languages = ['en']
01/07/2022 12:50:18 - INFO - __main__ -   Predict File = xquad.hi.json
01/07/2022 12:50:18 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
  0%|          | 0/48 [00:00<?, ?it/s] 33%|      | 16/48 [00:00<00:00, 133.92it/s] 62%|   | 30/48 [00:00<00:00, 127.63it/s] 94%|| 45/48 [00:00<00:00, 136.53it/s]100%|| 48/48 [00:00<00:00, 136.39it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<04:15,  4.65it/s]convert squad examples to features:  27%|       | 321/1190 [00:00<00:00, 1245.39it/s]convert squad examples to features:  42%|     | 498/1190 [00:00<00:00, 758.99it/s] convert squad examples to features:  78%|  | 929/1190 [00:00<00:00, 1441.33it/s]convert squad examples to features:  97%|| 1153/1190 [00:00<00:00, 1543.11it/s]convert squad examples to features: 100%|| 1190/1190 [00:00<00:00, 1297.19it/s]/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|| 1190/1190 [00:00<00:00, 449902.81it/s]
01/07/2022 12:50:20 - INFO - __main__ -   Local Rank = -1
01/07/2022 12:50:20 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.hi.json_xlm-roberta-base_384_hi
01/07/2022 12:50:22 - INFO - __main__ -   ***** Running evaluation  *****
01/07/2022 12:50:22 - INFO - __main__ -     Num examples = 1382
01/07/2022 12:50:22 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/173 [00:00<?, ?it/s]01/07/2022 12:50:22 - INFO - __main__ -   Batch Number = 1
01/07/2022 12:50:22 - INFO - __main__ -   Batch Number = 2
01/07/2022 12:50:22 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|         | 3/173 [00:00<00:05, 29.38it/s]01/07/2022 12:50:22 - INFO - __main__ -   Batch Number = 4
01/07/2022 12:50:22 - INFO - __main__ -   Batch Number = 5
01/07/2022 12:50:22 - INFO - __main__ -   Batch Number = 6
01/07/2022 12:50:22 - INFO - __main__ -   Batch Number = 7
Evaluating:   4%|         | 7/173 [00:00<00:05, 29.80it/s]01/07/2022 12:50:22 - INFO - __main__ -   Batch Number = 8
01/07/2022 12:50:22 - INFO - __main__ -   Batch Number = 9
01/07/2022 12:50:22 - INFO - __main__ -   Batch Number = 10
01/07/2022 12:50:22 - INFO - __main__ -   Batch Number = 11
01/07/2022 12:50:22 - INFO - __main__ -   Batch Number = 12
Evaluating:   7%|         | 12/173 [00:00<00:04, 36.27it/s]01/07/2022 12:50:22 - INFO - __main__ -   Batch Number = 13
01/07/2022 12:50:22 - INFO - __main__ -   Batch Number = 14
01/07/2022 12:50:22 - INFO - __main__ -   Batch Number = 15
01/07/2022 12:50:22 - INFO - __main__ -   Batch Number = 16
01/07/2022 12:50:22 - INFO - __main__ -   Batch Number = 17
Evaluating:  10%|         | 17/173 [00:00<00:03, 39.21it/s]01/07/2022 12:50:22 - INFO - __main__ -   Batch Number = 18
01/07/2022 12:50:22 - INFO - __main__ -   Batch Number = 19
01/07/2022 12:50:22 - INFO - __main__ -   Batch Number = 20
01/07/2022 12:50:22 - INFO - __main__ -   Batch Number = 21
01/07/2022 12:50:22 - INFO - __main__ -   Batch Number = 22
Evaluating:  13%|        | 22/173 [00:00<00:03, 41.27it/s]01/07/2022 12:50:22 - INFO - __main__ -   Batch Number = 23
01/07/2022 12:50:22 - INFO - __main__ -   Batch Number = 24
01/07/2022 12:50:22 - INFO - __main__ -   Batch Number = 25
01/07/2022 12:50:22 - INFO - __main__ -   Batch Number = 26
01/07/2022 12:50:22 - INFO - __main__ -   Batch Number = 27
Evaluating:  16%|        | 27/173 [00:00<00:03, 42.36it/s]01/07/2022 12:50:22 - INFO - __main__ -   Batch Number = 28
01/07/2022 12:50:22 - INFO - __main__ -   Batch Number = 29
01/07/2022 12:50:22 - INFO - __main__ -   Batch Number = 30
01/07/2022 12:50:22 - INFO - __main__ -   Batch Number = 31
01/07/2022 12:50:22 - INFO - __main__ -   Batch Number = 32
Evaluating:  18%|        | 32/173 [00:00<00:03, 43.12it/s]01/07/2022 12:50:22 - INFO - __main__ -   Batch Number = 33
01/07/2022 12:50:22 - INFO - __main__ -   Batch Number = 34
01/07/2022 12:50:22 - INFO - __main__ -   Batch Number = 35
01/07/2022 12:50:22 - INFO - __main__ -   Batch Number = 36
01/07/2022 12:50:22 - INFO - __main__ -   Batch Number = 37
Evaluating:  21%|       | 37/173 [00:00<00:03, 43.63it/s]01/07/2022 12:50:22 - INFO - __main__ -   Batch Number = 38
01/07/2022 12:50:22 - INFO - __main__ -   Batch Number = 39
01/07/2022 12:50:22 - INFO - __main__ -   Batch Number = 40
01/07/2022 12:50:23 - INFO - __main__ -   Batch Number = 41
01/07/2022 12:50:23 - INFO - __main__ -   Batch Number = 42
Evaluating:  24%|       | 42/173 [00:01<00:02, 44.05it/s]01/07/2022 12:50:23 - INFO - __main__ -   Batch Number = 43
01/07/2022 12:50:23 - INFO - __main__ -   Batch Number = 44
01/07/2022 12:50:23 - INFO - __main__ -   Batch Number = 45
01/07/2022 12:50:23 - INFO - __main__ -   Batch Number = 46
01/07/2022 12:50:23 - INFO - __main__ -   Batch Number = 47
Evaluating:  27%|       | 47/173 [00:01<00:02, 44.04it/s]01/07/2022 12:50:23 - INFO - __main__ -   Batch Number = 48
01/07/2022 12:50:23 - INFO - __main__ -   Batch Number = 49
01/07/2022 12:50:23 - INFO - __main__ -   Batch Number = 50
01/07/2022 12:50:23 - INFO - __main__ -   Batch Number = 51
01/07/2022 12:50:23 - INFO - __main__ -   Batch Number = 52
Evaluating:  30%|       | 52/173 [00:01<00:02, 44.11it/s]01/07/2022 12:50:23 - INFO - __main__ -   Batch Number = 53
01/07/2022 12:50:23 - INFO - __main__ -   Batch Number = 54
01/07/2022 12:50:23 - INFO - __main__ -   Batch Number = 55
01/07/2022 12:50:23 - INFO - __main__ -   Batch Number = 56
01/07/2022 12:50:23 - INFO - __main__ -   Batch Number = 57
Evaluating:  33%|      | 57/173 [00:01<00:02, 44.33it/s]01/07/2022 12:50:23 - INFO - __main__ -   Batch Number = 58
01/07/2022 12:50:23 - INFO - __main__ -   Batch Number = 59
01/07/2022 12:50:23 - INFO - __main__ -   Batch Number = 60
01/07/2022 12:50:23 - INFO - __main__ -   Batch Number = 61
01/07/2022 12:50:23 - INFO - __main__ -   Batch Number = 62
Evaluating:  36%|      | 62/173 [00:01<00:02, 44.31it/s]01/07/2022 12:50:23 - INFO - __main__ -   Batch Number = 63
01/07/2022 12:50:23 - INFO - __main__ -   Batch Number = 64
01/07/2022 12:50:23 - INFO - __main__ -   Batch Number = 65
01/07/2022 12:50:23 - INFO - __main__ -   Batch Number = 66
01/07/2022 12:50:23 - INFO - __main__ -   Batch Number = 67
Evaluating:  39%|      | 67/173 [00:01<00:02, 44.38it/s]01/07/2022 12:50:23 - INFO - __main__ -   Batch Number = 68
01/07/2022 12:50:23 - INFO - __main__ -   Batch Number = 69
01/07/2022 12:50:23 - INFO - __main__ -   Batch Number = 70
01/07/2022 12:50:23 - INFO - __main__ -   Batch Number = 71
01/07/2022 12:50:23 - INFO - __main__ -   Batch Number = 72
Evaluating:  42%|     | 72/173 [00:01<00:02, 44.31it/s]01/07/2022 12:50:23 - INFO - __main__ -   Batch Number = 73
01/07/2022 12:50:23 - INFO - __main__ -   Batch Number = 74
01/07/2022 12:50:23 - INFO - __main__ -   Batch Number = 75
01/07/2022 12:50:23 - INFO - __main__ -   Batch Number = 76
01/07/2022 12:50:23 - INFO - __main__ -   Batch Number = 77
Evaluating:  45%|     | 77/173 [00:01<00:02, 44.26it/s]01/07/2022 12:50:23 - INFO - __main__ -   Batch Number = 78
01/07/2022 12:50:23 - INFO - __main__ -   Batch Number = 79
01/07/2022 12:50:23 - INFO - __main__ -   Batch Number = 80
01/07/2022 12:50:23 - INFO - __main__ -   Batch Number = 81
01/07/2022 12:50:23 - INFO - __main__ -   Batch Number = 82
Evaluating:  47%|     | 82/173 [00:01<00:02, 44.31it/s]01/07/2022 12:50:23 - INFO - __main__ -   Batch Number = 83
01/07/2022 12:50:23 - INFO - __main__ -   Batch Number = 84
01/07/2022 12:50:23 - INFO - __main__ -   Batch Number = 85
01/07/2022 12:50:24 - INFO - __main__ -   Batch Number = 86
01/07/2022 12:50:24 - INFO - __main__ -   Batch Number = 87
Evaluating:  50%|     | 87/173 [00:02<00:01, 44.39it/s]01/07/2022 12:50:24 - INFO - __main__ -   Batch Number = 88
01/07/2022 12:50:24 - INFO - __main__ -   Batch Number = 89
01/07/2022 12:50:24 - INFO - __main__ -   Batch Number = 90
01/07/2022 12:50:24 - INFO - __main__ -   Batch Number = 91
01/07/2022 12:50:24 - INFO - __main__ -   Batch Number = 92
Evaluating:  53%|    | 92/173 [00:02<00:01, 44.28it/s]01/07/2022 12:50:24 - INFO - __main__ -   Batch Number = 93
01/07/2022 12:50:24 - INFO - __main__ -   Batch Number = 94
01/07/2022 12:50:24 - INFO - __main__ -   Batch Number = 95
01/07/2022 12:50:24 - INFO - __main__ -   Batch Number = 96
01/07/2022 12:50:24 - INFO - __main__ -   Batch Number = 97
Evaluating:  56%|    | 97/173 [00:02<00:01, 44.37it/s]01/07/2022 12:50:24 - INFO - __main__ -   Batch Number = 98
01/07/2022 12:50:24 - INFO - __main__ -   Batch Number = 99
01/07/2022 12:50:24 - INFO - __main__ -   Batch Number = 100
01/07/2022 12:50:24 - INFO - __main__ -   Batch Number = 101
01/07/2022 12:50:24 - INFO - __main__ -   Batch Number = 102
Evaluating:  59%|    | 102/173 [00:02<00:01, 44.46it/s]01/07/2022 12:50:24 - INFO - __main__ -   Batch Number = 103
01/07/2022 12:50:24 - INFO - __main__ -   Batch Number = 104
01/07/2022 12:50:24 - INFO - __main__ -   Batch Number = 105
01/07/2022 12:50:24 - INFO - __main__ -   Batch Number = 106
01/07/2022 12:50:24 - INFO - __main__ -   Batch Number = 107
Evaluating:  62%|   | 107/173 [00:02<00:01, 44.34it/s]01/07/2022 12:50:24 - INFO - __main__ -   Batch Number = 108
01/07/2022 12:50:24 - INFO - __main__ -   Batch Number = 109
01/07/2022 12:50:24 - INFO - __main__ -   Batch Number = 110
01/07/2022 12:50:24 - INFO - __main__ -   Batch Number = 111
01/07/2022 12:50:24 - INFO - __main__ -   Batch Number = 112
Evaluating:  65%|   | 112/173 [00:02<00:01, 44.53it/s]01/07/2022 12:50:24 - INFO - __main__ -   Batch Number = 113
01/07/2022 12:50:24 - INFO - __main__ -   Batch Number = 114
01/07/2022 12:50:24 - INFO - __main__ -   Batch Number = 115
01/07/2022 12:50:24 - INFO - __main__ -   Batch Number = 116
01/07/2022 12:50:24 - INFO - __main__ -   Batch Number = 117
Evaluating:  68%|   | 117/173 [00:02<00:01, 44.58it/s]01/07/2022 12:50:24 - INFO - __main__ -   Batch Number = 118
01/07/2022 12:50:24 - INFO - __main__ -   Batch Number = 119
01/07/2022 12:50:24 - INFO - __main__ -   Batch Number = 120
01/07/2022 12:50:24 - INFO - __main__ -   Batch Number = 121
01/07/2022 12:50:24 - INFO - __main__ -   Batch Number = 122
Evaluating:  71%|   | 122/173 [00:02<00:01, 44.55it/s]01/07/2022 12:50:24 - INFO - __main__ -   Batch Number = 123
01/07/2022 12:50:24 - INFO - __main__ -   Batch Number = 124
01/07/2022 12:50:24 - INFO - __main__ -   Batch Number = 125
01/07/2022 12:50:24 - INFO - __main__ -   Batch Number = 126
01/07/2022 12:50:24 - INFO - __main__ -   Batch Number = 127
Evaluating:  73%|  | 127/173 [00:02<00:01, 44.46it/s]01/07/2022 12:50:24 - INFO - __main__ -   Batch Number = 128
01/07/2022 12:50:24 - INFO - __main__ -   Batch Number = 129
01/07/2022 12:50:25 - INFO - __main__ -   Batch Number = 130
01/07/2022 12:50:25 - INFO - __main__ -   Batch Number = 131
01/07/2022 12:50:25 - INFO - __main__ -   Batch Number = 132
Evaluating:  76%|  | 132/173 [00:03<00:00, 44.55it/s]01/07/2022 12:50:25 - INFO - __main__ -   Batch Number = 133
01/07/2022 12:50:25 - INFO - __main__ -   Batch Number = 134
01/07/2022 12:50:25 - INFO - __main__ -   Batch Number = 135
01/07/2022 12:50:25 - INFO - __main__ -   Batch Number = 136
01/07/2022 12:50:25 - INFO - __main__ -   Batch Number = 137
Evaluating:  79%|  | 137/173 [00:03<00:00, 44.31it/s]01/07/2022 12:50:25 - INFO - __main__ -   Batch Number = 138
01/07/2022 12:50:25 - INFO - __main__ -   Batch Number = 139
01/07/2022 12:50:25 - INFO - __main__ -   Batch Number = 140
01/07/2022 12:50:25 - INFO - __main__ -   Batch Number = 141
01/07/2022 12:50:25 - INFO - __main__ -   Batch Number = 142
Evaluating:  82%| | 142/173 [00:03<00:00, 44.33it/s]01/07/2022 12:50:25 - INFO - __main__ -   Batch Number = 143
01/07/2022 12:50:25 - INFO - __main__ -   Batch Number = 144
01/07/2022 12:50:25 - INFO - __main__ -   Batch Number = 145
01/07/2022 12:50:25 - INFO - __main__ -   Batch Number = 146
01/07/2022 12:50:25 - INFO - __main__ -   Batch Number = 147
Evaluating:  85%| | 147/173 [00:03<00:00, 44.32it/s]01/07/2022 12:50:25 - INFO - __main__ -   Batch Number = 148
01/07/2022 12:50:25 - INFO - __main__ -   Batch Number = 149
01/07/2022 12:50:25 - INFO - __main__ -   Batch Number = 150
01/07/2022 12:50:25 - INFO - __main__ -   Batch Number = 151
01/07/2022 12:50:25 - INFO - __main__ -   Batch Number = 152
Evaluating:  88%| | 152/173 [00:03<00:00, 44.21it/s]01/07/2022 12:50:25 - INFO - __main__ -   Batch Number = 153
01/07/2022 12:50:25 - INFO - __main__ -   Batch Number = 154
01/07/2022 12:50:25 - INFO - __main__ -   Batch Number = 155
01/07/2022 12:50:25 - INFO - __main__ -   Batch Number = 156
01/07/2022 12:50:25 - INFO - __main__ -   Batch Number = 157
Evaluating:  91%| | 157/173 [00:03<00:00, 44.12it/s]01/07/2022 12:50:25 - INFO - __main__ -   Batch Number = 158
01/07/2022 12:50:25 - INFO - __main__ -   Batch Number = 159
01/07/2022 12:50:25 - INFO - __main__ -   Batch Number = 160
01/07/2022 12:50:25 - INFO - __main__ -   Batch Number = 161
01/07/2022 12:50:25 - INFO - __main__ -   Batch Number = 162
Evaluating:  94%|| 162/173 [00:03<00:00, 44.23it/s]01/07/2022 12:50:25 - INFO - __main__ -   Batch Number = 163
01/07/2022 12:50:25 - INFO - __main__ -   Batch Number = 164
01/07/2022 12:50:25 - INFO - __main__ -   Batch Number = 165
01/07/2022 12:50:25 - INFO - __main__ -   Batch Number = 166
01/07/2022 12:50:25 - INFO - __main__ -   Batch Number = 167
Evaluating:  97%|| 167/173 [00:03<00:00, 42.51it/s]01/07/2022 12:50:25 - INFO - __main__ -   Batch Number = 168
01/07/2022 12:50:25 - INFO - __main__ -   Batch Number = 169
01/07/2022 12:50:25 - INFO - __main__ -   Batch Number = 170
01/07/2022 12:50:25 - INFO - __main__ -   Batch Number = 171
01/07/2022 12:50:25 - INFO - __main__ -   Batch Number = 172
Evaluating:  99%|| 172/173 [00:03<00:00, 43.06it/s]01/07/2022 12:50:25 - INFO - __main__ -   Batch Number = 173
Evaluating: 100%|| 173/173 [00:03<00:00, 43.43it/s]
01/07/2022 12:50:26 - INFO - __main__ -     Evaluation done in total 3.983993 secs (0.002883 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/predictions_hi_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/nbest_predictions_hi_.json
01/07/2022 12:50:28 - INFO - __main__ -   Results = OrderedDict([('exact', 47.73109243697479), ('f1', 65.11131556503332), ('total', 1190), ('HasAns_exact', 47.73109243697479), ('HasAns_f1', 65.11131556503332), ('HasAns_total', 1190), ('best_exact', 47.73109243697479), ('best_exact_thresh', 0.0), ('best_f1', 65.11131556503332), ('best_f1_thresh', 0.0)])
01/07/2022 12:50:29 - INFO - __main__ -   Language adapter for ru not found, using en instead
01/07/2022 12:50:29 - INFO - __main__ -   Set active language adapter to en
01/07/2022 12:50:29 - INFO - __main__ -   Args Adapter Weight = None
01/07/2022 12:50:29 - INFO - __main__ -   Adapter Languages = ['en']
01/07/2022 12:50:29 - INFO - __main__ -   Predict File = xquad.ru.json
01/07/2022 12:50:29 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
  0%|          | 0/48 [00:00<?, ?it/s] 31%|      | 15/48 [00:00<00:00, 144.81it/s] 62%|   | 30/48 [00:00<00:00, 107.38it/s] 92%|| 44/48 [00:00<00:00, 116.40it/s]100%|| 48/48 [00:00<00:00, 118.69it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<08:10,  2.43it/s]convert squad examples to features:  32%|      | 385/1190 [00:00<00:01, 734.45it/s]convert squad examples to features:  75%|  | 897/1190 [00:00<00:00, 1612.52it/s]convert squad examples to features: 100%|| 1190/1190 [00:00<00:00, 1397.86it/s]/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|| 1190/1190 [00:00<00:00, 459385.34it/s]
01/07/2022 12:50:31 - INFO - __main__ -   Local Rank = -1
01/07/2022 12:50:31 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.ru.json_xlm-roberta-base_384_ru
01/07/2022 12:50:32 - INFO - __main__ -   ***** Running evaluation  *****
01/07/2022 12:50:32 - INFO - __main__ -     Num examples = 1332
01/07/2022 12:50:32 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/167 [00:00<?, ?it/s]01/07/2022 12:50:32 - INFO - __main__ -   Batch Number = 1
01/07/2022 12:50:32 - INFO - __main__ -   Batch Number = 2
01/07/2022 12:50:32 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|         | 3/167 [00:00<00:05, 29.35it/s]01/07/2022 12:50:32 - INFO - __main__ -   Batch Number = 4
01/07/2022 12:50:32 - INFO - __main__ -   Batch Number = 5
01/07/2022 12:50:32 - INFO - __main__ -   Batch Number = 6
01/07/2022 12:50:32 - INFO - __main__ -   Batch Number = 7
Evaluating:   4%|         | 7/167 [00:00<00:04, 32.65it/s]01/07/2022 12:50:32 - INFO - __main__ -   Batch Number = 8
01/07/2022 12:50:32 - INFO - __main__ -   Batch Number = 9
01/07/2022 12:50:32 - INFO - __main__ -   Batch Number = 10
01/07/2022 12:50:32 - INFO - __main__ -   Batch Number = 11
01/07/2022 12:50:32 - INFO - __main__ -   Batch Number = 12
Evaluating:   7%|         | 12/167 [00:00<00:04, 37.95it/s]01/07/2022 12:50:32 - INFO - __main__ -   Batch Number = 13
01/07/2022 12:50:32 - INFO - __main__ -   Batch Number = 14
01/07/2022 12:50:32 - INFO - __main__ -   Batch Number = 15
01/07/2022 12:50:32 - INFO - __main__ -   Batch Number = 16
01/07/2022 12:50:32 - INFO - __main__ -   Batch Number = 17
Evaluating:  10%|         | 17/167 [00:00<00:03, 39.80it/s]01/07/2022 12:50:32 - INFO - __main__ -   Batch Number = 18
01/07/2022 12:50:32 - INFO - __main__ -   Batch Number = 19
01/07/2022 12:50:32 - INFO - __main__ -   Batch Number = 20
01/07/2022 12:50:32 - INFO - __main__ -   Batch Number = 21
01/07/2022 12:50:32 - INFO - __main__ -   Batch Number = 22
Evaluating:  13%|        | 22/167 [00:00<00:03, 41.48it/s]01/07/2022 12:50:32 - INFO - __main__ -   Batch Number = 23
01/07/2022 12:50:32 - INFO - __main__ -   Batch Number = 24
01/07/2022 12:50:32 - INFO - __main__ -   Batch Number = 25
01/07/2022 12:50:32 - INFO - __main__ -   Batch Number = 26
01/07/2022 12:50:32 - INFO - __main__ -   Batch Number = 27
Evaluating:  16%|        | 27/167 [00:00<00:03, 42.50it/s]01/07/2022 12:50:32 - INFO - __main__ -   Batch Number = 28
01/07/2022 12:50:32 - INFO - __main__ -   Batch Number = 29
01/07/2022 12:50:32 - INFO - __main__ -   Batch Number = 30
01/07/2022 12:50:32 - INFO - __main__ -   Batch Number = 31
01/07/2022 12:50:32 - INFO - __main__ -   Batch Number = 32
Evaluating:  19%|        | 32/167 [00:00<00:03, 42.97it/s]01/07/2022 12:50:32 - INFO - __main__ -   Batch Number = 33
01/07/2022 12:50:32 - INFO - __main__ -   Batch Number = 34
01/07/2022 12:50:33 - INFO - __main__ -   Batch Number = 35
01/07/2022 12:50:33 - INFO - __main__ -   Batch Number = 36
01/07/2022 12:50:33 - INFO - __main__ -   Batch Number = 37
Evaluating:  22%|       | 37/167 [00:00<00:02, 43.54it/s]01/07/2022 12:50:33 - INFO - __main__ -   Batch Number = 38
01/07/2022 12:50:33 - INFO - __main__ -   Batch Number = 39
01/07/2022 12:50:33 - INFO - __main__ -   Batch Number = 40
01/07/2022 12:50:33 - INFO - __main__ -   Batch Number = 41
01/07/2022 12:50:33 - INFO - __main__ -   Batch Number = 42
Evaluating:  25%|       | 42/167 [00:01<00:02, 43.76it/s]01/07/2022 12:50:33 - INFO - __main__ -   Batch Number = 43
01/07/2022 12:50:33 - INFO - __main__ -   Batch Number = 44
01/07/2022 12:50:33 - INFO - __main__ -   Batch Number = 45
01/07/2022 12:50:33 - INFO - __main__ -   Batch Number = 46
01/07/2022 12:50:33 - INFO - __main__ -   Batch Number = 47
Evaluating:  28%|       | 47/167 [00:01<00:02, 43.97it/s]01/07/2022 12:50:33 - INFO - __main__ -   Batch Number = 48
01/07/2022 12:50:33 - INFO - __main__ -   Batch Number = 49
01/07/2022 12:50:33 - INFO - __main__ -   Batch Number = 50
01/07/2022 12:50:33 - INFO - __main__ -   Batch Number = 51
01/07/2022 12:50:33 - INFO - __main__ -   Batch Number = 52
Evaluating:  31%|       | 52/167 [00:01<00:02, 44.05it/s]01/07/2022 12:50:33 - INFO - __main__ -   Batch Number = 53
01/07/2022 12:50:33 - INFO - __main__ -   Batch Number = 54
01/07/2022 12:50:33 - INFO - __main__ -   Batch Number = 55
01/07/2022 12:50:33 - INFO - __main__ -   Batch Number = 56
01/07/2022 12:50:33 - INFO - __main__ -   Batch Number = 57
Evaluating:  34%|      | 57/167 [00:01<00:02, 44.08it/s]01/07/2022 12:50:33 - INFO - __main__ -   Batch Number = 58
01/07/2022 12:50:33 - INFO - __main__ -   Batch Number = 59
01/07/2022 12:50:33 - INFO - __main__ -   Batch Number = 60
01/07/2022 12:50:33 - INFO - __main__ -   Batch Number = 61
01/07/2022 12:50:33 - INFO - __main__ -   Batch Number = 62
Evaluating:  37%|      | 62/167 [00:01<00:02, 44.01it/s]01/07/2022 12:50:33 - INFO - __main__ -   Batch Number = 63
01/07/2022 12:50:33 - INFO - __main__ -   Batch Number = 64
01/07/2022 12:50:33 - INFO - __main__ -   Batch Number = 65
01/07/2022 12:50:33 - INFO - __main__ -   Batch Number = 66
01/07/2022 12:50:33 - INFO - __main__ -   Batch Number = 67
Evaluating:  40%|      | 67/167 [00:01<00:02, 43.90it/s]01/07/2022 12:50:33 - INFO - __main__ -   Batch Number = 68
01/07/2022 12:50:33 - INFO - __main__ -   Batch Number = 69
01/07/2022 12:50:33 - INFO - __main__ -   Batch Number = 70
01/07/2022 12:50:33 - INFO - __main__ -   Batch Number = 71
01/07/2022 12:50:33 - INFO - __main__ -   Batch Number = 72
Evaluating:  43%|     | 72/167 [00:01<00:02, 44.00it/s]01/07/2022 12:50:33 - INFO - __main__ -   Batch Number = 73
01/07/2022 12:50:33 - INFO - __main__ -   Batch Number = 74
01/07/2022 12:50:33 - INFO - __main__ -   Batch Number = 75
01/07/2022 12:50:33 - INFO - __main__ -   Batch Number = 76
01/07/2022 12:50:33 - INFO - __main__ -   Batch Number = 77
Evaluating:  46%|     | 77/167 [00:01<00:02, 43.98it/s]01/07/2022 12:50:33 - INFO - __main__ -   Batch Number = 78
01/07/2022 12:50:34 - INFO - __main__ -   Batch Number = 79
01/07/2022 12:50:34 - INFO - __main__ -   Batch Number = 80
01/07/2022 12:50:34 - INFO - __main__ -   Batch Number = 81
01/07/2022 12:50:34 - INFO - __main__ -   Batch Number = 82
Evaluating:  49%|     | 82/167 [00:01<00:01, 44.10it/s]01/07/2022 12:50:34 - INFO - __main__ -   Batch Number = 83
01/07/2022 12:50:34 - INFO - __main__ -   Batch Number = 84
01/07/2022 12:50:34 - INFO - __main__ -   Batch Number = 85
01/07/2022 12:50:34 - INFO - __main__ -   Batch Number = 86
01/07/2022 12:50:34 - INFO - __main__ -   Batch Number = 87
Evaluating:  52%|    | 87/167 [00:02<00:01, 44.22it/s]01/07/2022 12:50:34 - INFO - __main__ -   Batch Number = 88
01/07/2022 12:50:34 - INFO - __main__ -   Batch Number = 89
01/07/2022 12:50:34 - INFO - __main__ -   Batch Number = 90
01/07/2022 12:50:34 - INFO - __main__ -   Batch Number = 91
01/07/2022 12:50:34 - INFO - __main__ -   Batch Number = 92
Evaluating:  55%|    | 92/167 [00:02<00:01, 44.11it/s]01/07/2022 12:50:34 - INFO - __main__ -   Batch Number = 93
01/07/2022 12:50:34 - INFO - __main__ -   Batch Number = 94
01/07/2022 12:50:34 - INFO - __main__ -   Batch Number = 95
01/07/2022 12:50:34 - INFO - __main__ -   Batch Number = 96
01/07/2022 12:50:34 - INFO - __main__ -   Batch Number = 97
Evaluating:  58%|    | 97/167 [00:02<00:01, 44.26it/s]01/07/2022 12:50:34 - INFO - __main__ -   Batch Number = 98
01/07/2022 12:50:34 - INFO - __main__ -   Batch Number = 99
01/07/2022 12:50:34 - INFO - __main__ -   Batch Number = 100
01/07/2022 12:50:34 - INFO - __main__ -   Batch Number = 101
01/07/2022 12:50:34 - INFO - __main__ -   Batch Number = 102
Evaluating:  61%|    | 102/167 [00:02<00:01, 44.21it/s]01/07/2022 12:50:34 - INFO - __main__ -   Batch Number = 103
01/07/2022 12:50:34 - INFO - __main__ -   Batch Number = 104
01/07/2022 12:50:34 - INFO - __main__ -   Batch Number = 105
01/07/2022 12:50:34 - INFO - __main__ -   Batch Number = 106
01/07/2022 12:50:34 - INFO - __main__ -   Batch Number = 107
Evaluating:  64%|   | 107/167 [00:02<00:01, 44.18it/s]01/07/2022 12:50:34 - INFO - __main__ -   Batch Number = 108
01/07/2022 12:50:34 - INFO - __main__ -   Batch Number = 109
01/07/2022 12:50:34 - INFO - __main__ -   Batch Number = 110
01/07/2022 12:50:34 - INFO - __main__ -   Batch Number = 111
01/07/2022 12:50:34 - INFO - __main__ -   Batch Number = 112
Evaluating:  67%|   | 112/167 [00:02<00:01, 44.21it/s]01/07/2022 12:50:34 - INFO - __main__ -   Batch Number = 113
01/07/2022 12:50:34 - INFO - __main__ -   Batch Number = 114
01/07/2022 12:50:34 - INFO - __main__ -   Batch Number = 115
01/07/2022 12:50:34 - INFO - __main__ -   Batch Number = 116
01/07/2022 12:50:34 - INFO - __main__ -   Batch Number = 117
Evaluating:  70%|   | 117/167 [00:02<00:01, 44.12it/s]01/07/2022 12:50:34 - INFO - __main__ -   Batch Number = 118
01/07/2022 12:50:34 - INFO - __main__ -   Batch Number = 119
01/07/2022 12:50:34 - INFO - __main__ -   Batch Number = 120
01/07/2022 12:50:34 - INFO - __main__ -   Batch Number = 121
01/07/2022 12:50:34 - INFO - __main__ -   Batch Number = 122
Evaluating:  73%|  | 122/167 [00:02<00:01, 42.43it/s]01/07/2022 12:50:35 - INFO - __main__ -   Batch Number = 123
01/07/2022 12:50:35 - INFO - __main__ -   Batch Number = 124
01/07/2022 12:50:35 - INFO - __main__ -   Batch Number = 125
01/07/2022 12:50:35 - INFO - __main__ -   Batch Number = 126
01/07/2022 12:50:35 - INFO - __main__ -   Batch Number = 127
Evaluating:  76%|  | 127/167 [00:02<00:00, 42.94it/s]01/07/2022 12:50:35 - INFO - __main__ -   Batch Number = 128
01/07/2022 12:50:35 - INFO - __main__ -   Batch Number = 129
01/07/2022 12:50:35 - INFO - __main__ -   Batch Number = 130
01/07/2022 12:50:35 - INFO - __main__ -   Batch Number = 131
01/07/2022 12:50:35 - INFO - __main__ -   Batch Number = 132
Evaluating:  79%|  | 132/167 [00:03<00:00, 43.39it/s]01/07/2022 12:50:35 - INFO - __main__ -   Batch Number = 133
01/07/2022 12:50:35 - INFO - __main__ -   Batch Number = 134
01/07/2022 12:50:35 - INFO - __main__ -   Batch Number = 135
01/07/2022 12:50:35 - INFO - __main__ -   Batch Number = 136
01/07/2022 12:50:35 - INFO - __main__ -   Batch Number = 137
Evaluating:  82%| | 137/167 [00:03<00:00, 43.21it/s]01/07/2022 12:50:35 - INFO - __main__ -   Batch Number = 138
01/07/2022 12:50:35 - INFO - __main__ -   Batch Number = 139
01/07/2022 12:50:35 - INFO - __main__ -   Batch Number = 140
01/07/2022 12:50:35 - INFO - __main__ -   Batch Number = 141
01/07/2022 12:50:35 - INFO - __main__ -   Batch Number = 142
Evaluating:  85%| | 142/167 [00:03<00:00, 43.32it/s]01/07/2022 12:50:35 - INFO - __main__ -   Batch Number = 143
01/07/2022 12:50:35 - INFO - __main__ -   Batch Number = 144
01/07/2022 12:50:35 - INFO - __main__ -   Batch Number = 145
01/07/2022 12:50:35 - INFO - __main__ -   Batch Number = 146
01/07/2022 12:50:35 - INFO - __main__ -   Batch Number = 147
Evaluating:  88%| | 147/167 [00:03<00:00, 43.63it/s]01/07/2022 12:50:35 - INFO - __main__ -   Batch Number = 148
01/07/2022 12:50:35 - INFO - __main__ -   Batch Number = 149
01/07/2022 12:50:35 - INFO - __main__ -   Batch Number = 150
01/07/2022 12:50:35 - INFO - __main__ -   Batch Number = 151
01/07/2022 12:50:35 - INFO - __main__ -   Batch Number = 152
Evaluating:  91%| | 152/167 [00:03<00:00, 43.52it/s]01/07/2022 12:50:35 - INFO - __main__ -   Batch Number = 153
01/07/2022 12:50:35 - INFO - __main__ -   Batch Number = 154
01/07/2022 12:50:35 - INFO - __main__ -   Batch Number = 155
01/07/2022 12:50:35 - INFO - __main__ -   Batch Number = 156
01/07/2022 12:50:35 - INFO - __main__ -   Batch Number = 157
Evaluating:  94%|| 157/167 [00:03<00:00, 43.74it/s]01/07/2022 12:50:35 - INFO - __main__ -   Batch Number = 158
01/07/2022 12:50:35 - INFO - __main__ -   Batch Number = 159
01/07/2022 12:50:35 - INFO - __main__ -   Batch Number = 160
01/07/2022 12:50:35 - INFO - __main__ -   Batch Number = 161
01/07/2022 12:50:35 - INFO - __main__ -   Batch Number = 162
Evaluating:  97%|| 162/167 [00:03<00:00, 43.96it/s]01/07/2022 12:50:35 - INFO - __main__ -   Batch Number = 163
01/07/2022 12:50:35 - INFO - __main__ -   Batch Number = 164
01/07/2022 12:50:35 - INFO - __main__ -   Batch Number = 165
01/07/2022 12:50:35 - INFO - __main__ -   Batch Number = 166
01/07/2022 12:50:36 - INFO - __main__ -   Batch Number = 167
Evaluating: 100%|| 167/167 [00:03<00:00, 44.68it/s]Evaluating: 100%|| 167/167 [00:03<00:00, 43.29it/s]
01/07/2022 12:50:36 - INFO - __main__ -     Evaluation done in total 3.858236 secs (0.002897 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/predictions_ru_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/nbest_predictions_ru_.json
01/07/2022 12:50:39 - INFO - __main__ -   Results = OrderedDict([('exact', 54.87394957983193), ('f1', 71.4950540971754), ('total', 1190), ('HasAns_exact', 54.87394957983193), ('HasAns_f1', 71.4950540971754), ('HasAns_total', 1190), ('best_exact', 54.87394957983193), ('best_exact_thresh', 0.0), ('best_f1', 71.4950540971754), ('best_f1_thresh', 0.0)])
01/07/2022 12:50:39 - INFO - __main__ -   Language adapter for th not found, using en instead
01/07/2022 12:50:39 - INFO - __main__ -   Set active language adapter to en
01/07/2022 12:50:39 - INFO - __main__ -   Args Adapter Weight = None
01/07/2022 12:50:39 - INFO - __main__ -   Adapter Languages = ['en']
01/07/2022 12:50:39 - INFO - __main__ -   Predict File = xquad.th.json
01/07/2022 12:50:39 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
  0%|          | 0/48 [00:00<?, ?it/s] 33%|      | 16/48 [00:00<00:00, 126.43it/s] 60%|    | 29/48 [00:00<00:00, 117.46it/s] 92%|| 44/48 [00:00<00:00, 128.36it/s]100%|| 48/48 [00:00<00:00, 127.41it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<07:06,  2.79it/s]convert squad examples to features:  32%|      | 385/1190 [00:00<00:01, 643.83it/s]convert squad examples to features: 100%|| 1190/1190 [00:00<00:00, 1673.82it/s]/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|| 1190/1190 [00:00<00:00, 448407.31it/s]
01/07/2022 12:50:41 - INFO - __main__ -   Local Rank = -1
01/07/2022 12:50:41 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.th.json_xlm-roberta-base_384_th
01/07/2022 12:50:42 - INFO - __main__ -   ***** Running evaluation  *****
01/07/2022 12:50:42 - INFO - __main__ -     Num examples = 1314
01/07/2022 12:50:42 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/165 [00:00<?, ?it/s]01/07/2022 12:50:42 - INFO - __main__ -   Batch Number = 1
01/07/2022 12:50:42 - INFO - __main__ -   Batch Number = 2
01/07/2022 12:50:42 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|         | 3/165 [00:00<00:05, 29.58it/s]01/07/2022 12:50:42 - INFO - __main__ -   Batch Number = 4
01/07/2022 12:50:42 - INFO - __main__ -   Batch Number = 5
01/07/2022 12:50:42 - INFO - __main__ -   Batch Number = 6
Evaluating:   4%|         | 6/165 [00:00<00:05, 29.79it/s]01/07/2022 12:50:42 - INFO - __main__ -   Batch Number = 7
01/07/2022 12:50:42 - INFO - __main__ -   Batch Number = 8
01/07/2022 12:50:42 - INFO - __main__ -   Batch Number = 9
01/07/2022 12:50:42 - INFO - __main__ -   Batch Number = 10
Evaluating:   6%|         | 10/165 [00:00<00:04, 33.47it/s]01/07/2022 12:50:42 - INFO - __main__ -   Batch Number = 11
01/07/2022 12:50:42 - INFO - __main__ -   Batch Number = 12
01/07/2022 12:50:42 - INFO - __main__ -   Batch Number = 13
01/07/2022 12:50:42 - INFO - __main__ -   Batch Number = 14
01/07/2022 12:50:42 - INFO - __main__ -   Batch Number = 15
Evaluating:   9%|         | 15/165 [00:00<00:03, 38.19it/s]01/07/2022 12:50:42 - INFO - __main__ -   Batch Number = 16
01/07/2022 12:50:42 - INFO - __main__ -   Batch Number = 17
01/07/2022 12:50:42 - INFO - __main__ -   Batch Number = 18
01/07/2022 12:50:42 - INFO - __main__ -   Batch Number = 19
01/07/2022 12:50:42 - INFO - __main__ -   Batch Number = 20
Evaluating:  12%|        | 20/165 [00:00<00:03, 40.28it/s]01/07/2022 12:50:42 - INFO - __main__ -   Batch Number = 21
01/07/2022 12:50:42 - INFO - __main__ -   Batch Number = 22
01/07/2022 12:50:42 - INFO - __main__ -   Batch Number = 23
01/07/2022 12:50:42 - INFO - __main__ -   Batch Number = 24
01/07/2022 12:50:43 - INFO - __main__ -   Batch Number = 25
Evaluating:  15%|        | 25/165 [00:00<00:03, 41.74it/s]01/07/2022 12:50:43 - INFO - __main__ -   Batch Number = 26
01/07/2022 12:50:43 - INFO - __main__ -   Batch Number = 27
01/07/2022 12:50:43 - INFO - __main__ -   Batch Number = 28
01/07/2022 12:50:43 - INFO - __main__ -   Batch Number = 29
01/07/2022 12:50:43 - INFO - __main__ -   Batch Number = 30
Evaluating:  18%|        | 30/165 [00:00<00:03, 42.69it/s]01/07/2022 12:50:43 - INFO - __main__ -   Batch Number = 31
01/07/2022 12:50:43 - INFO - __main__ -   Batch Number = 32
01/07/2022 12:50:43 - INFO - __main__ -   Batch Number = 33
01/07/2022 12:50:43 - INFO - __main__ -   Batch Number = 34
01/07/2022 12:50:43 - INFO - __main__ -   Batch Number = 35
Evaluating:  21%|        | 35/165 [00:00<00:03, 43.22it/s]01/07/2022 12:50:43 - INFO - __main__ -   Batch Number = 36
01/07/2022 12:50:43 - INFO - __main__ -   Batch Number = 37
01/07/2022 12:50:43 - INFO - __main__ -   Batch Number = 38
01/07/2022 12:50:43 - INFO - __main__ -   Batch Number = 39
01/07/2022 12:50:43 - INFO - __main__ -   Batch Number = 40
Evaluating:  24%|       | 40/165 [00:00<00:02, 43.73it/s]01/07/2022 12:50:43 - INFO - __main__ -   Batch Number = 41
01/07/2022 12:50:43 - INFO - __main__ -   Batch Number = 42
01/07/2022 12:50:43 - INFO - __main__ -   Batch Number = 43
01/07/2022 12:50:43 - INFO - __main__ -   Batch Number = 44
01/07/2022 12:50:43 - INFO - __main__ -   Batch Number = 45
Evaluating:  27%|       | 45/165 [00:01<00:02, 44.01it/s]01/07/2022 12:50:43 - INFO - __main__ -   Batch Number = 46
01/07/2022 12:50:43 - INFO - __main__ -   Batch Number = 47
01/07/2022 12:50:43 - INFO - __main__ -   Batch Number = 48
01/07/2022 12:50:43 - INFO - __main__ -   Batch Number = 49
01/07/2022 12:50:43 - INFO - __main__ -   Batch Number = 50
Evaluating:  30%|       | 50/165 [00:01<00:02, 44.02it/s]01/07/2022 12:50:43 - INFO - __main__ -   Batch Number = 51
01/07/2022 12:50:43 - INFO - __main__ -   Batch Number = 52
01/07/2022 12:50:43 - INFO - __main__ -   Batch Number = 53
01/07/2022 12:50:43 - INFO - __main__ -   Batch Number = 54
01/07/2022 12:50:43 - INFO - __main__ -   Batch Number = 55
Evaluating:  33%|      | 55/165 [00:01<00:02, 44.23it/s]01/07/2022 12:50:43 - INFO - __main__ -   Batch Number = 56
01/07/2022 12:50:43 - INFO - __main__ -   Batch Number = 57
01/07/2022 12:50:43 - INFO - __main__ -   Batch Number = 58
01/07/2022 12:50:43 - INFO - __main__ -   Batch Number = 59
01/07/2022 12:50:43 - INFO - __main__ -   Batch Number = 60
Evaluating:  36%|      | 60/165 [00:01<00:02, 44.21it/s]01/07/2022 12:50:43 - INFO - __main__ -   Batch Number = 61
01/07/2022 12:50:43 - INFO - __main__ -   Batch Number = 62
01/07/2022 12:50:43 - INFO - __main__ -   Batch Number = 63
01/07/2022 12:50:43 - INFO - __main__ -   Batch Number = 64
01/07/2022 12:50:43 - INFO - __main__ -   Batch Number = 65
Evaluating:  39%|      | 65/165 [00:01<00:02, 44.23it/s]01/07/2022 12:50:43 - INFO - __main__ -   Batch Number = 66
01/07/2022 12:50:43 - INFO - __main__ -   Batch Number = 67
01/07/2022 12:50:43 - INFO - __main__ -   Batch Number = 68
01/07/2022 12:50:43 - INFO - __main__ -   Batch Number = 69
01/07/2022 12:50:44 - INFO - __main__ -   Batch Number = 70
Evaluating:  42%|     | 70/165 [00:01<00:02, 44.25it/s]01/07/2022 12:50:44 - INFO - __main__ -   Batch Number = 71
01/07/2022 12:50:44 - INFO - __main__ -   Batch Number = 72
01/07/2022 12:50:44 - INFO - __main__ -   Batch Number = 73
01/07/2022 12:50:44 - INFO - __main__ -   Batch Number = 74
01/07/2022 12:50:44 - INFO - __main__ -   Batch Number = 75
Evaluating:  45%|     | 75/165 [00:01<00:02, 44.25it/s]01/07/2022 12:50:44 - INFO - __main__ -   Batch Number = 76
01/07/2022 12:50:44 - INFO - __main__ -   Batch Number = 77
01/07/2022 12:50:44 - INFO - __main__ -   Batch Number = 78
01/07/2022 12:50:44 - INFO - __main__ -   Batch Number = 79
01/07/2022 12:50:44 - INFO - __main__ -   Batch Number = 80
Evaluating:  48%|     | 80/165 [00:01<00:01, 43.99it/s]01/07/2022 12:50:44 - INFO - __main__ -   Batch Number = 81
01/07/2022 12:50:44 - INFO - __main__ -   Batch Number = 82
01/07/2022 12:50:44 - INFO - __main__ -   Batch Number = 83
01/07/2022 12:50:44 - INFO - __main__ -   Batch Number = 84
01/07/2022 12:50:44 - INFO - __main__ -   Batch Number = 85
Evaluating:  52%|    | 85/165 [00:02<00:01, 43.83it/s]01/07/2022 12:50:44 - INFO - __main__ -   Batch Number = 86
01/07/2022 12:50:44 - INFO - __main__ -   Batch Number = 87
01/07/2022 12:50:44 - INFO - __main__ -   Batch Number = 88
01/07/2022 12:50:44 - INFO - __main__ -   Batch Number = 89
01/07/2022 12:50:44 - INFO - __main__ -   Batch Number = 90
Evaluating:  55%|    | 90/165 [00:02<00:01, 43.97it/s]01/07/2022 12:50:44 - INFO - __main__ -   Batch Number = 91
01/07/2022 12:50:44 - INFO - __main__ -   Batch Number = 92
01/07/2022 12:50:44 - INFO - __main__ -   Batch Number = 93
01/07/2022 12:50:44 - INFO - __main__ -   Batch Number = 94
01/07/2022 12:50:44 - INFO - __main__ -   Batch Number = 95
Evaluating:  58%|    | 95/165 [00:02<00:01, 42.30it/s]01/07/2022 12:50:44 - INFO - __main__ -   Batch Number = 96
01/07/2022 12:50:44 - INFO - __main__ -   Batch Number = 97
01/07/2022 12:50:44 - INFO - __main__ -   Batch Number = 98
01/07/2022 12:50:44 - INFO - __main__ -   Batch Number = 99
01/07/2022 12:50:44 - INFO - __main__ -   Batch Number = 100
Evaluating:  61%|    | 100/165 [00:02<00:01, 43.07it/s]01/07/2022 12:50:44 - INFO - __main__ -   Batch Number = 101
01/07/2022 12:50:44 - INFO - __main__ -   Batch Number = 102
01/07/2022 12:50:44 - INFO - __main__ -   Batch Number = 103
01/07/2022 12:50:44 - INFO - __main__ -   Batch Number = 104
01/07/2022 12:50:44 - INFO - __main__ -   Batch Number = 105
Evaluating:  64%|   | 105/165 [00:02<00:01, 43.51it/s]01/07/2022 12:50:44 - INFO - __main__ -   Batch Number = 106
01/07/2022 12:50:44 - INFO - __main__ -   Batch Number = 107
01/07/2022 12:50:44 - INFO - __main__ -   Batch Number = 108
01/07/2022 12:50:44 - INFO - __main__ -   Batch Number = 109
01/07/2022 12:50:44 - INFO - __main__ -   Batch Number = 110
Evaluating:  67%|   | 110/165 [00:02<00:01, 43.76it/s]01/07/2022 12:50:44 - INFO - __main__ -   Batch Number = 111
01/07/2022 12:50:44 - INFO - __main__ -   Batch Number = 112
01/07/2022 12:50:45 - INFO - __main__ -   Batch Number = 113
01/07/2022 12:50:45 - INFO - __main__ -   Batch Number = 114
01/07/2022 12:50:45 - INFO - __main__ -   Batch Number = 115
Evaluating:  70%|   | 115/165 [00:02<00:01, 44.07it/s]01/07/2022 12:50:45 - INFO - __main__ -   Batch Number = 116
01/07/2022 12:50:45 - INFO - __main__ -   Batch Number = 117
01/07/2022 12:50:45 - INFO - __main__ -   Batch Number = 118
01/07/2022 12:50:45 - INFO - __main__ -   Batch Number = 119
01/07/2022 12:50:45 - INFO - __main__ -   Batch Number = 120
Evaluating:  73%|  | 120/165 [00:02<00:01, 44.18it/s]01/07/2022 12:50:45 - INFO - __main__ -   Batch Number = 121
01/07/2022 12:50:45 - INFO - __main__ -   Batch Number = 122
01/07/2022 12:50:45 - INFO - __main__ -   Batch Number = 123
01/07/2022 12:50:45 - INFO - __main__ -   Batch Number = 124
01/07/2022 12:50:45 - INFO - __main__ -   Batch Number = 125
Evaluating:  76%|  | 125/165 [00:02<00:00, 44.00it/s]01/07/2022 12:50:45 - INFO - __main__ -   Batch Number = 126
01/07/2022 12:50:45 - INFO - __main__ -   Batch Number = 127
01/07/2022 12:50:45 - INFO - __main__ -   Batch Number = 128
01/07/2022 12:50:45 - INFO - __main__ -   Batch Number = 129
01/07/2022 12:50:45 - INFO - __main__ -   Batch Number = 130
Evaluating:  79%|  | 130/165 [00:03<00:00, 44.03it/s]01/07/2022 12:50:45 - INFO - __main__ -   Batch Number = 131
01/07/2022 12:50:45 - INFO - __main__ -   Batch Number = 132
01/07/2022 12:50:45 - INFO - __main__ -   Batch Number = 133
01/07/2022 12:50:45 - INFO - __main__ -   Batch Number = 134
01/07/2022 12:50:45 - INFO - __main__ -   Batch Number = 135
Evaluating:  82%| | 135/165 [00:03<00:00, 44.22it/s]01/07/2022 12:50:45 - INFO - __main__ -   Batch Number = 136
01/07/2022 12:50:45 - INFO - __main__ -   Batch Number = 137
01/07/2022 12:50:45 - INFO - __main__ -   Batch Number = 138
01/07/2022 12:50:45 - INFO - __main__ -   Batch Number = 139
01/07/2022 12:50:45 - INFO - __main__ -   Batch Number = 140
Evaluating:  85%| | 140/165 [00:03<00:00, 44.13it/s]01/07/2022 12:50:45 - INFO - __main__ -   Batch Number = 141
01/07/2022 12:50:45 - INFO - __main__ -   Batch Number = 142
01/07/2022 12:50:45 - INFO - __main__ -   Batch Number = 143
01/07/2022 12:50:45 - INFO - __main__ -   Batch Number = 144
01/07/2022 12:50:45 - INFO - __main__ -   Batch Number = 145
Evaluating:  88%| | 145/165 [00:03<00:00, 44.20it/s]01/07/2022 12:50:45 - INFO - __main__ -   Batch Number = 146
01/07/2022 12:50:45 - INFO - __main__ -   Batch Number = 147
01/07/2022 12:50:45 - INFO - __main__ -   Batch Number = 148
01/07/2022 12:50:45 - INFO - __main__ -   Batch Number = 149
01/07/2022 12:50:45 - INFO - __main__ -   Batch Number = 150
Evaluating:  91%| | 150/165 [00:03<00:00, 44.12it/s]01/07/2022 12:50:45 - INFO - __main__ -   Batch Number = 151
01/07/2022 12:50:45 - INFO - __main__ -   Batch Number = 152
01/07/2022 12:50:45 - INFO - __main__ -   Batch Number = 153
01/07/2022 12:50:45 - INFO - __main__ -   Batch Number = 154
01/07/2022 12:50:45 - INFO - __main__ -   Batch Number = 155
Evaluating:  94%|| 155/165 [00:03<00:00, 44.04it/s]01/07/2022 12:50:45 - INFO - __main__ -   Batch Number = 156
01/07/2022 12:50:46 - INFO - __main__ -   Batch Number = 157
01/07/2022 12:50:46 - INFO - __main__ -   Batch Number = 158
01/07/2022 12:50:46 - INFO - __main__ -   Batch Number = 159
01/07/2022 12:50:46 - INFO - __main__ -   Batch Number = 160
Evaluating:  97%|| 160/165 [00:03<00:00, 44.28it/s]01/07/2022 12:50:46 - INFO - __main__ -   Batch Number = 161
01/07/2022 12:50:46 - INFO - __main__ -   Batch Number = 162
01/07/2022 12:50:46 - INFO - __main__ -   Batch Number = 163
01/07/2022 12:50:46 - INFO - __main__ -   Batch Number = 164
01/07/2022 12:50:46 - INFO - __main__ -   Batch Number = 165
Evaluating: 100%|| 165/165 [00:03<00:00, 45.00it/s]Evaluating: 100%|| 165/165 [00:03<00:00, 43.23it/s]
01/07/2022 12:50:46 - INFO - __main__ -     Evaluation done in total 3.817018 secs (0.002905 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/predictions_th_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/nbest_predictions_th_.json
01/07/2022 12:50:51 - INFO - __main__ -   Results = OrderedDict([('exact', 53.69747899159664), ('f1', 65.19880073241406), ('total', 1190), ('HasAns_exact', 53.69747899159664), ('HasAns_f1', 65.19880073241406), ('HasAns_total', 1190), ('best_exact', 53.69747899159664), ('best_exact_thresh', 0.0), ('best_f1', 65.19880073241406), ('best_f1_thresh', 0.0)])
01/07/2022 12:50:51 - INFO - __main__ -   Language adapter for tr not found, using en instead
01/07/2022 12:50:51 - INFO - __main__ -   Set active language adapter to en
01/07/2022 12:50:51 - INFO - __main__ -   Args Adapter Weight = None
01/07/2022 12:50:51 - INFO - __main__ -   Adapter Languages = ['en']
01/07/2022 12:50:51 - INFO - __main__ -   Predict File = xquad.tr.json
01/07/2022 12:50:51 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
  0%|          | 0/48 [00:00<?, ?it/s] 33%|      | 16/48 [00:00<00:00, 134.68it/s] 62%|   | 30/48 [00:00<00:00, 127.46it/s] 94%|| 45/48 [00:00<00:00, 134.98it/s]100%|| 48/48 [00:00<00:00, 135.81it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<06:43,  2.95it/s]convert squad examples to features:  32%|      | 385/1190 [00:00<00:01, 736.32it/s]convert squad examples to features:  94%|| 1121/1190 [00:00<00:00, 2105.09it/s]convert squad examples to features: 100%|| 1190/1190 [00:00<00:00, 1530.36it/s]/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|| 1190/1190 [00:00<00:00, 432065.60it/s]
01/07/2022 12:50:53 - INFO - __main__ -   Local Rank = -1
01/07/2022 12:50:53 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.tr.json_xlm-roberta-base_384_tr
01/07/2022 12:50:54 - INFO - __main__ -   ***** Running evaluation  *****
01/07/2022 12:50:54 - INFO - __main__ -     Num examples = 1274
01/07/2022 12:50:54 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/160 [00:00<?, ?it/s]01/07/2022 12:50:54 - INFO - __main__ -   Batch Number = 1
01/07/2022 12:50:54 - INFO - __main__ -   Batch Number = 2
01/07/2022 12:50:54 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|         | 3/160 [00:00<00:05, 29.56it/s]01/07/2022 12:50:54 - INFO - __main__ -   Batch Number = 4
01/07/2022 12:50:54 - INFO - __main__ -   Batch Number = 5
01/07/2022 12:50:54 - INFO - __main__ -   Batch Number = 6
01/07/2022 12:50:54 - INFO - __main__ -   Batch Number = 7
Evaluating:   4%|         | 7/160 [00:00<00:04, 33.05it/s]01/07/2022 12:50:54 - INFO - __main__ -   Batch Number = 8
01/07/2022 12:50:54 - INFO - __main__ -   Batch Number = 9
01/07/2022 12:50:54 - INFO - __main__ -   Batch Number = 10
01/07/2022 12:50:54 - INFO - __main__ -   Batch Number = 11
Evaluating:   7%|         | 11/160 [00:00<00:04, 34.66it/s]01/07/2022 12:50:54 - INFO - __main__ -   Batch Number = 12
01/07/2022 12:50:54 - INFO - __main__ -   Batch Number = 13
01/07/2022 12:50:54 - INFO - __main__ -   Batch Number = 14
01/07/2022 12:50:54 - INFO - __main__ -   Batch Number = 15
01/07/2022 12:50:54 - INFO - __main__ -   Batch Number = 16
Evaluating:  10%|         | 16/160 [00:00<00:03, 38.77it/s]01/07/2022 12:50:54 - INFO - __main__ -   Batch Number = 17
01/07/2022 12:50:54 - INFO - __main__ -   Batch Number = 18
01/07/2022 12:50:54 - INFO - __main__ -   Batch Number = 19
01/07/2022 12:50:54 - INFO - __main__ -   Batch Number = 20
01/07/2022 12:50:54 - INFO - __main__ -   Batch Number = 21
Evaluating:  13%|        | 21/160 [00:00<00:03, 40.67it/s]01/07/2022 12:50:54 - INFO - __main__ -   Batch Number = 22
01/07/2022 12:50:54 - INFO - __main__ -   Batch Number = 23
01/07/2022 12:50:54 - INFO - __main__ -   Batch Number = 24
01/07/2022 12:50:54 - INFO - __main__ -   Batch Number = 25
01/07/2022 12:50:54 - INFO - __main__ -   Batch Number = 26
Evaluating:  16%|        | 26/160 [00:00<00:03, 42.10it/s]01/07/2022 12:50:54 - INFO - __main__ -   Batch Number = 27
01/07/2022 12:50:54 - INFO - __main__ -   Batch Number = 28
01/07/2022 12:50:54 - INFO - __main__ -   Batch Number = 29
01/07/2022 12:50:54 - INFO - __main__ -   Batch Number = 30
01/07/2022 12:50:54 - INFO - __main__ -   Batch Number = 31
Evaluating:  19%|        | 31/160 [00:00<00:03, 42.86it/s]01/07/2022 12:50:54 - INFO - __main__ -   Batch Number = 32
01/07/2022 12:50:54 - INFO - __main__ -   Batch Number = 33
01/07/2022 12:50:55 - INFO - __main__ -   Batch Number = 34
01/07/2022 12:50:55 - INFO - __main__ -   Batch Number = 35
01/07/2022 12:50:55 - INFO - __main__ -   Batch Number = 36
Evaluating:  22%|       | 36/160 [00:00<00:02, 43.33it/s]01/07/2022 12:50:55 - INFO - __main__ -   Batch Number = 37
01/07/2022 12:50:55 - INFO - __main__ -   Batch Number = 38
01/07/2022 12:50:55 - INFO - __main__ -   Batch Number = 39
01/07/2022 12:50:55 - INFO - __main__ -   Batch Number = 40
01/07/2022 12:50:55 - INFO - __main__ -   Batch Number = 41
Evaluating:  26%|       | 41/160 [00:00<00:02, 43.65it/s]01/07/2022 12:50:55 - INFO - __main__ -   Batch Number = 42
01/07/2022 12:50:55 - INFO - __main__ -   Batch Number = 43
01/07/2022 12:50:55 - INFO - __main__ -   Batch Number = 44
01/07/2022 12:50:55 - INFO - __main__ -   Batch Number = 45
01/07/2022 12:50:55 - INFO - __main__ -   Batch Number = 46
Evaluating:  29%|       | 46/160 [00:01<00:02, 44.08it/s]01/07/2022 12:50:55 - INFO - __main__ -   Batch Number = 47
01/07/2022 12:50:55 - INFO - __main__ -   Batch Number = 48
01/07/2022 12:50:55 - INFO - __main__ -   Batch Number = 49
01/07/2022 12:50:55 - INFO - __main__ -   Batch Number = 50
01/07/2022 12:50:55 - INFO - __main__ -   Batch Number = 51
Evaluating:  32%|      | 51/160 [00:01<00:02, 44.08it/s]01/07/2022 12:50:55 - INFO - __main__ -   Batch Number = 52
01/07/2022 12:50:55 - INFO - __main__ -   Batch Number = 53
01/07/2022 12:50:55 - INFO - __main__ -   Batch Number = 54
01/07/2022 12:50:55 - INFO - __main__ -   Batch Number = 55
01/07/2022 12:50:55 - INFO - __main__ -   Batch Number = 56
Evaluating:  35%|      | 56/160 [00:01<00:02, 44.30it/s]01/07/2022 12:50:55 - INFO - __main__ -   Batch Number = 57
01/07/2022 12:50:55 - INFO - __main__ -   Batch Number = 58
01/07/2022 12:50:55 - INFO - __main__ -   Batch Number = 59
01/07/2022 12:50:55 - INFO - __main__ -   Batch Number = 60
01/07/2022 12:50:55 - INFO - __main__ -   Batch Number = 61
Evaluating:  38%|      | 61/160 [00:01<00:02, 44.32it/s]01/07/2022 12:50:55 - INFO - __main__ -   Batch Number = 62
01/07/2022 12:50:55 - INFO - __main__ -   Batch Number = 63
01/07/2022 12:50:55 - INFO - __main__ -   Batch Number = 64
01/07/2022 12:50:55 - INFO - __main__ -   Batch Number = 65
01/07/2022 12:50:55 - INFO - __main__ -   Batch Number = 66
Evaluating:  41%|     | 66/160 [00:01<00:02, 44.33it/s]01/07/2022 12:50:55 - INFO - __main__ -   Batch Number = 67
01/07/2022 12:50:55 - INFO - __main__ -   Batch Number = 68
01/07/2022 12:50:55 - INFO - __main__ -   Batch Number = 69
01/07/2022 12:50:55 - INFO - __main__ -   Batch Number = 70
01/07/2022 12:50:55 - INFO - __main__ -   Batch Number = 71
Evaluating:  44%|     | 71/160 [00:01<00:02, 44.44it/s]01/07/2022 12:50:55 - INFO - __main__ -   Batch Number = 72
01/07/2022 12:50:55 - INFO - __main__ -   Batch Number = 73
01/07/2022 12:50:55 - INFO - __main__ -   Batch Number = 74
01/07/2022 12:50:55 - INFO - __main__ -   Batch Number = 75
01/07/2022 12:50:55 - INFO - __main__ -   Batch Number = 76
Evaluating:  48%|     | 76/160 [00:01<00:01, 44.50it/s]01/07/2022 12:50:55 - INFO - __main__ -   Batch Number = 77
01/07/2022 12:50:56 - INFO - __main__ -   Batch Number = 78
01/07/2022 12:50:56 - INFO - __main__ -   Batch Number = 79
01/07/2022 12:50:56 - INFO - __main__ -   Batch Number = 80
01/07/2022 12:50:56 - INFO - __main__ -   Batch Number = 81
Evaluating:  51%|     | 81/160 [00:01<00:01, 44.46it/s]01/07/2022 12:50:56 - INFO - __main__ -   Batch Number = 82
01/07/2022 12:50:56 - INFO - __main__ -   Batch Number = 83
01/07/2022 12:50:56 - INFO - __main__ -   Batch Number = 84
01/07/2022 12:50:56 - INFO - __main__ -   Batch Number = 85
01/07/2022 12:50:56 - INFO - __main__ -   Batch Number = 86
Evaluating:  54%|    | 86/160 [00:02<00:01, 44.40it/s]01/07/2022 12:50:56 - INFO - __main__ -   Batch Number = 87
01/07/2022 12:50:56 - INFO - __main__ -   Batch Number = 88
01/07/2022 12:50:56 - INFO - __main__ -   Batch Number = 89
01/07/2022 12:50:56 - INFO - __main__ -   Batch Number = 90
01/07/2022 12:50:56 - INFO - __main__ -   Batch Number = 91
Evaluating:  57%|    | 91/160 [00:02<00:01, 44.36it/s]01/07/2022 12:50:56 - INFO - __main__ -   Batch Number = 92
01/07/2022 12:50:56 - INFO - __main__ -   Batch Number = 93
01/07/2022 12:50:56 - INFO - __main__ -   Batch Number = 94
01/07/2022 12:50:56 - INFO - __main__ -   Batch Number = 95
01/07/2022 12:50:56 - INFO - __main__ -   Batch Number = 96
Evaluating:  60%|    | 96/160 [00:02<00:01, 44.28it/s]01/07/2022 12:50:56 - INFO - __main__ -   Batch Number = 97
01/07/2022 12:50:56 - INFO - __main__ -   Batch Number = 98
01/07/2022 12:50:56 - INFO - __main__ -   Batch Number = 99
01/07/2022 12:50:56 - INFO - __main__ -   Batch Number = 100
01/07/2022 12:50:56 - INFO - __main__ -   Batch Number = 101
Evaluating:  63%|   | 101/160 [00:02<00:01, 44.45it/s]01/07/2022 12:50:56 - INFO - __main__ -   Batch Number = 102
01/07/2022 12:50:56 - INFO - __main__ -   Batch Number = 103
01/07/2022 12:50:56 - INFO - __main__ -   Batch Number = 104
01/07/2022 12:50:56 - INFO - __main__ -   Batch Number = 105
01/07/2022 12:50:56 - INFO - __main__ -   Batch Number = 106
Evaluating:  66%|   | 106/160 [00:02<00:01, 44.53it/s]01/07/2022 12:50:56 - INFO - __main__ -   Batch Number = 107
01/07/2022 12:50:56 - INFO - __main__ -   Batch Number = 108
01/07/2022 12:50:56 - INFO - __main__ -   Batch Number = 109
01/07/2022 12:50:56 - INFO - __main__ -   Batch Number = 110
01/07/2022 12:50:56 - INFO - __main__ -   Batch Number = 111
Evaluating:  69%|   | 111/160 [00:02<00:01, 42.94it/s]01/07/2022 12:50:56 - INFO - __main__ -   Batch Number = 112
01/07/2022 12:50:56 - INFO - __main__ -   Batch Number = 113
01/07/2022 12:50:56 - INFO - __main__ -   Batch Number = 114
01/07/2022 12:50:56 - INFO - __main__ -   Batch Number = 115
01/07/2022 12:50:56 - INFO - __main__ -   Batch Number = 116
Evaluating:  72%|  | 116/160 [00:02<00:01, 43.30it/s]01/07/2022 12:50:56 - INFO - __main__ -   Batch Number = 117
01/07/2022 12:50:56 - INFO - __main__ -   Batch Number = 118
01/07/2022 12:50:56 - INFO - __main__ -   Batch Number = 119
01/07/2022 12:50:56 - INFO - __main__ -   Batch Number = 120
01/07/2022 12:50:56 - INFO - __main__ -   Batch Number = 121
Evaluating:  76%|  | 121/160 [00:02<00:00, 43.69it/s]01/07/2022 12:50:57 - INFO - __main__ -   Batch Number = 122
01/07/2022 12:50:57 - INFO - __main__ -   Batch Number = 123
01/07/2022 12:50:57 - INFO - __main__ -   Batch Number = 124
01/07/2022 12:50:57 - INFO - __main__ -   Batch Number = 125
01/07/2022 12:50:57 - INFO - __main__ -   Batch Number = 126
Evaluating:  79%|  | 126/160 [00:02<00:00, 43.81it/s]01/07/2022 12:50:57 - INFO - __main__ -   Batch Number = 127
01/07/2022 12:50:57 - INFO - __main__ -   Batch Number = 128
01/07/2022 12:50:57 - INFO - __main__ -   Batch Number = 129
01/07/2022 12:50:57 - INFO - __main__ -   Batch Number = 130
01/07/2022 12:50:57 - INFO - __main__ -   Batch Number = 131
Evaluating:  82%| | 131/160 [00:03<00:00, 43.98it/s]01/07/2022 12:50:57 - INFO - __main__ -   Batch Number = 132
01/07/2022 12:50:57 - INFO - __main__ -   Batch Number = 133
01/07/2022 12:50:57 - INFO - __main__ -   Batch Number = 134
01/07/2022 12:50:57 - INFO - __main__ -   Batch Number = 135
01/07/2022 12:50:57 - INFO - __main__ -   Batch Number = 136
Evaluating:  85%| | 136/160 [00:03<00:00, 44.21it/s]01/07/2022 12:50:57 - INFO - __main__ -   Batch Number = 137
01/07/2022 12:50:57 - INFO - __main__ -   Batch Number = 138
01/07/2022 12:50:57 - INFO - __main__ -   Batch Number = 139
01/07/2022 12:50:57 - INFO - __main__ -   Batch Number = 140
01/07/2022 12:50:57 - INFO - __main__ -   Batch Number = 141
Evaluating:  88%| | 141/160 [00:03<00:00, 44.13it/s]01/07/2022 12:50:57 - INFO - __main__ -   Batch Number = 142
01/07/2022 12:50:57 - INFO - __main__ -   Batch Number = 143
01/07/2022 12:50:57 - INFO - __main__ -   Batch Number = 144
01/07/2022 12:50:57 - INFO - __main__ -   Batch Number = 145
01/07/2022 12:50:57 - INFO - __main__ -   Batch Number = 146
Evaluating:  91%|| 146/160 [00:03<00:00, 44.32it/s]01/07/2022 12:50:57 - INFO - __main__ -   Batch Number = 147
01/07/2022 12:50:57 - INFO - __main__ -   Batch Number = 148
01/07/2022 12:50:57 - INFO - __main__ -   Batch Number = 149
01/07/2022 12:50:57 - INFO - __main__ -   Batch Number = 150
01/07/2022 12:50:57 - INFO - __main__ -   Batch Number = 151
Evaluating:  94%|| 151/160 [00:03<00:00, 44.33it/s]01/07/2022 12:50:57 - INFO - __main__ -   Batch Number = 152
01/07/2022 12:50:57 - INFO - __main__ -   Batch Number = 153
01/07/2022 12:50:57 - INFO - __main__ -   Batch Number = 154
01/07/2022 12:50:57 - INFO - __main__ -   Batch Number = 155
01/07/2022 12:50:57 - INFO - __main__ -   Batch Number = 156
Evaluating:  98%|| 156/160 [00:03<00:00, 44.35it/s]01/07/2022 12:50:57 - INFO - __main__ -   Batch Number = 157
01/07/2022 12:50:57 - INFO - __main__ -   Batch Number = 158
01/07/2022 12:50:57 - INFO - __main__ -   Batch Number = 159
01/07/2022 12:50:57 - INFO - __main__ -   Batch Number = 160
Evaluating: 100%|| 160/160 [00:03<00:00, 43.43it/s]
01/07/2022 12:50:57 - INFO - __main__ -     Evaluation done in total 3.684695 secs (0.002892 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/predictions_tr_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/nbest_predictions_tr_.json
01/07/2022 12:51:01 - INFO - __main__ -   Results = OrderedDict([('exact', 49.57983193277311), ('f1', 66.22538990178751), ('total', 1190), ('HasAns_exact', 49.57983193277311), ('HasAns_f1', 66.22538990178751), ('HasAns_total', 1190), ('best_exact', 49.57983193277311), ('best_exact_thresh', 0.0), ('best_f1', 66.22538990178751), ('best_f1_thresh', 0.0)])
01/07/2022 12:51:01 - INFO - __main__ -   Language adapter for vi not found, using en instead
01/07/2022 12:51:01 - INFO - __main__ -   Set active language adapter to en
01/07/2022 12:51:01 - INFO - __main__ -   Args Adapter Weight = None
01/07/2022 12:51:01 - INFO - __main__ -   Adapter Languages = ['en']
01/07/2022 12:51:01 - INFO - __main__ -   Predict File = xquad.vi.json
01/07/2022 12:51:01 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
  0%|          | 0/48 [00:00<?, ?it/s] 33%|      | 16/48 [00:00<00:00, 135.94it/s] 62%|   | 30/48 [00:00<00:00, 128.08it/s] 96%|| 46/48 [00:00<00:00, 139.83it/s]100%|| 48/48 [00:00<00:00, 138.62it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<06:13,  3.18it/s]convert squad examples to features:  27%|       | 321/1190 [00:00<00:00, 964.19it/s]convert squad examples to features:  41%|      | 489/1190 [00:00<00:01, 503.81it/s]convert squad examples to features: 100%|| 1190/1190 [00:00<00:00, 1216.47it/s]/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|| 1190/1190 [00:00<00:00, 453747.43it/s]
01/07/2022 12:51:03 - INFO - __main__ -   Local Rank = -1
01/07/2022 12:51:03 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.vi.json_xlm-roberta-base_384_vi
01/07/2022 12:51:04 - INFO - __main__ -   ***** Running evaluation  *****
01/07/2022 12:51:04 - INFO - __main__ -     Num examples = 1314
01/07/2022 12:51:04 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/165 [00:00<?, ?it/s]01/07/2022 12:51:04 - INFO - __main__ -   Batch Number = 1
01/07/2022 12:51:04 - INFO - __main__ -   Batch Number = 2
01/07/2022 12:51:04 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|         | 3/165 [00:00<00:05, 29.44it/s]01/07/2022 12:51:04 - INFO - __main__ -   Batch Number = 4
01/07/2022 12:51:04 - INFO - __main__ -   Batch Number = 5
01/07/2022 12:51:04 - INFO - __main__ -   Batch Number = 6
Evaluating:   4%|         | 6/165 [00:00<00:05, 29.62it/s]01/07/2022 12:51:04 - INFO - __main__ -   Batch Number = 7
01/07/2022 12:51:04 - INFO - __main__ -   Batch Number = 8
01/07/2022 12:51:04 - INFO - __main__ -   Batch Number = 9
01/07/2022 12:51:04 - INFO - __main__ -   Batch Number = 10
Evaluating:   6%|         | 10/165 [00:00<00:04, 32.80it/s]01/07/2022 12:51:04 - INFO - __main__ -   Batch Number = 11
01/07/2022 12:51:04 - INFO - __main__ -   Batch Number = 12
01/07/2022 12:51:04 - INFO - __main__ -   Batch Number = 13
01/07/2022 12:51:04 - INFO - __main__ -   Batch Number = 14
01/07/2022 12:51:04 - INFO - __main__ -   Batch Number = 15
Evaluating:   9%|         | 15/165 [00:00<00:03, 37.53it/s]01/07/2022 12:51:04 - INFO - __main__ -   Batch Number = 16
01/07/2022 12:51:04 - INFO - __main__ -   Batch Number = 17
01/07/2022 12:51:04 - INFO - __main__ -   Batch Number = 18
01/07/2022 12:51:04 - INFO - __main__ -   Batch Number = 19
01/07/2022 12:51:04 - INFO - __main__ -   Batch Number = 20
Evaluating:  12%|        | 20/165 [00:00<00:03, 39.83it/s]01/07/2022 12:51:04 - INFO - __main__ -   Batch Number = 21
01/07/2022 12:51:04 - INFO - __main__ -   Batch Number = 22
01/07/2022 12:51:04 - INFO - __main__ -   Batch Number = 23
01/07/2022 12:51:04 - INFO - __main__ -   Batch Number = 24
01/07/2022 12:51:04 - INFO - __main__ -   Batch Number = 25
Evaluating:  15%|        | 25/165 [00:00<00:03, 41.52it/s]01/07/2022 12:51:04 - INFO - __main__ -   Batch Number = 26
01/07/2022 12:51:04 - INFO - __main__ -   Batch Number = 27
01/07/2022 12:51:05 - INFO - __main__ -   Batch Number = 28
01/07/2022 12:51:05 - INFO - __main__ -   Batch Number = 29
01/07/2022 12:51:05 - INFO - __main__ -   Batch Number = 30
Evaluating:  18%|        | 30/165 [00:00<00:03, 42.72it/s]01/07/2022 12:51:05 - INFO - __main__ -   Batch Number = 31
01/07/2022 12:51:05 - INFO - __main__ -   Batch Number = 32
01/07/2022 12:51:05 - INFO - __main__ -   Batch Number = 33
01/07/2022 12:51:05 - INFO - __main__ -   Batch Number = 34
01/07/2022 12:51:05 - INFO - __main__ -   Batch Number = 35
Evaluating:  21%|        | 35/165 [00:00<00:03, 43.19it/s]01/07/2022 12:51:05 - INFO - __main__ -   Batch Number = 36
01/07/2022 12:51:05 - INFO - __main__ -   Batch Number = 37
01/07/2022 12:51:05 - INFO - __main__ -   Batch Number = 38
01/07/2022 12:51:05 - INFO - __main__ -   Batch Number = 39
01/07/2022 12:51:05 - INFO - __main__ -   Batch Number = 40
Evaluating:  24%|       | 40/165 [00:00<00:02, 43.64it/s]01/07/2022 12:51:05 - INFO - __main__ -   Batch Number = 41
01/07/2022 12:51:05 - INFO - __main__ -   Batch Number = 42
01/07/2022 12:51:05 - INFO - __main__ -   Batch Number = 43
01/07/2022 12:51:05 - INFO - __main__ -   Batch Number = 44
01/07/2022 12:51:05 - INFO - __main__ -   Batch Number = 45
Evaluating:  27%|       | 45/165 [00:01<00:02, 43.99it/s]01/07/2022 12:51:05 - INFO - __main__ -   Batch Number = 46
01/07/2022 12:51:05 - INFO - __main__ -   Batch Number = 47
01/07/2022 12:51:05 - INFO - __main__ -   Batch Number = 48
01/07/2022 12:51:05 - INFO - __main__ -   Batch Number = 49
01/07/2022 12:51:05 - INFO - __main__ -   Batch Number = 50
Evaluating:  30%|       | 50/165 [00:01<00:02, 43.97it/s]01/07/2022 12:51:05 - INFO - __main__ -   Batch Number = 51
01/07/2022 12:51:05 - INFO - __main__ -   Batch Number = 52
01/07/2022 12:51:05 - INFO - __main__ -   Batch Number = 53
01/07/2022 12:51:05 - INFO - __main__ -   Batch Number = 54
01/07/2022 12:51:05 - INFO - __main__ -   Batch Number = 55
Evaluating:  33%|      | 55/165 [00:01<00:02, 44.06it/s]01/07/2022 12:51:05 - INFO - __main__ -   Batch Number = 56
01/07/2022 12:51:05 - INFO - __main__ -   Batch Number = 57
01/07/2022 12:51:05 - INFO - __main__ -   Batch Number = 58
01/07/2022 12:51:05 - INFO - __main__ -   Batch Number = 59
01/07/2022 12:51:05 - INFO - __main__ -   Batch Number = 60
Evaluating:  36%|      | 60/165 [00:01<00:02, 44.12it/s]01/07/2022 12:51:05 - INFO - __main__ -   Batch Number = 61
01/07/2022 12:51:05 - INFO - __main__ -   Batch Number = 62
01/07/2022 12:51:05 - INFO - __main__ -   Batch Number = 63
01/07/2022 12:51:05 - INFO - __main__ -   Batch Number = 64
01/07/2022 12:51:05 - INFO - __main__ -   Batch Number = 65
Evaluating:  39%|      | 65/165 [00:01<00:02, 44.07it/s]01/07/2022 12:51:05 - INFO - __main__ -   Batch Number = 66
01/07/2022 12:51:05 - INFO - __main__ -   Batch Number = 67
01/07/2022 12:51:05 - INFO - __main__ -   Batch Number = 68
01/07/2022 12:51:05 - INFO - __main__ -   Batch Number = 69
01/07/2022 12:51:05 - INFO - __main__ -   Batch Number = 70
Evaluating:  42%|     | 70/165 [00:01<00:02, 44.26it/s]01/07/2022 12:51:05 - INFO - __main__ -   Batch Number = 71
01/07/2022 12:51:06 - INFO - __main__ -   Batch Number = 72
01/07/2022 12:51:06 - INFO - __main__ -   Batch Number = 73
01/07/2022 12:51:06 - INFO - __main__ -   Batch Number = 74
01/07/2022 12:51:06 - INFO - __main__ -   Batch Number = 75
Evaluating:  45%|     | 75/165 [00:01<00:02, 44.37it/s]01/07/2022 12:51:06 - INFO - __main__ -   Batch Number = 76
01/07/2022 12:51:06 - INFO - __main__ -   Batch Number = 77
01/07/2022 12:51:06 - INFO - __main__ -   Batch Number = 78
01/07/2022 12:51:06 - INFO - __main__ -   Batch Number = 79
01/07/2022 12:51:06 - INFO - __main__ -   Batch Number = 80
Evaluating:  48%|     | 80/165 [00:01<00:01, 44.32it/s]01/07/2022 12:51:06 - INFO - __main__ -   Batch Number = 81
01/07/2022 12:51:06 - INFO - __main__ -   Batch Number = 82
01/07/2022 12:51:06 - INFO - __main__ -   Batch Number = 83
01/07/2022 12:51:06 - INFO - __main__ -   Batch Number = 84
01/07/2022 12:51:06 - INFO - __main__ -   Batch Number = 85
Evaluating:  52%|    | 85/165 [00:02<00:01, 44.32it/s]01/07/2022 12:51:06 - INFO - __main__ -   Batch Number = 86
01/07/2022 12:51:06 - INFO - __main__ -   Batch Number = 87
01/07/2022 12:51:06 - INFO - __main__ -   Batch Number = 88
01/07/2022 12:51:06 - INFO - __main__ -   Batch Number = 89
01/07/2022 12:51:06 - INFO - __main__ -   Batch Number = 90
Evaluating:  55%|    | 90/165 [00:02<00:01, 44.47it/s]01/07/2022 12:51:06 - INFO - __main__ -   Batch Number = 91
01/07/2022 12:51:06 - INFO - __main__ -   Batch Number = 92
01/07/2022 12:51:06 - INFO - __main__ -   Batch Number = 93
01/07/2022 12:51:06 - INFO - __main__ -   Batch Number = 94
01/07/2022 12:51:06 - INFO - __main__ -   Batch Number = 95
Evaluating:  58%|    | 95/165 [00:02<00:01, 42.52it/s]01/07/2022 12:51:06 - INFO - __main__ -   Batch Number = 96
01/07/2022 12:51:06 - INFO - __main__ -   Batch Number = 97
01/07/2022 12:51:06 - INFO - __main__ -   Batch Number = 98
01/07/2022 12:51:06 - INFO - __main__ -   Batch Number = 99
01/07/2022 12:51:06 - INFO - __main__ -   Batch Number = 100
Evaluating:  61%|    | 100/165 [00:02<00:01, 43.18it/s]01/07/2022 12:51:06 - INFO - __main__ -   Batch Number = 101
01/07/2022 12:51:06 - INFO - __main__ -   Batch Number = 102
01/07/2022 12:51:06 - INFO - __main__ -   Batch Number = 103
01/07/2022 12:51:06 - INFO - __main__ -   Batch Number = 104
01/07/2022 12:51:06 - INFO - __main__ -   Batch Number = 105
Evaluating:  64%|   | 105/165 [00:02<00:01, 43.57it/s]01/07/2022 12:51:06 - INFO - __main__ -   Batch Number = 106
01/07/2022 12:51:06 - INFO - __main__ -   Batch Number = 107
01/07/2022 12:51:06 - INFO - __main__ -   Batch Number = 108
01/07/2022 12:51:06 - INFO - __main__ -   Batch Number = 109
01/07/2022 12:51:06 - INFO - __main__ -   Batch Number = 110
Evaluating:  67%|   | 110/165 [00:02<00:01, 43.81it/s]01/07/2022 12:51:06 - INFO - __main__ -   Batch Number = 111
01/07/2022 12:51:06 - INFO - __main__ -   Batch Number = 112
01/07/2022 12:51:06 - INFO - __main__ -   Batch Number = 113
01/07/2022 12:51:06 - INFO - __main__ -   Batch Number = 114
01/07/2022 12:51:06 - INFO - __main__ -   Batch Number = 115
Evaluating:  70%|   | 115/165 [00:02<00:01, 44.02it/s]01/07/2022 12:51:07 - INFO - __main__ -   Batch Number = 116
01/07/2022 12:51:07 - INFO - __main__ -   Batch Number = 117
01/07/2022 12:51:07 - INFO - __main__ -   Batch Number = 118
01/07/2022 12:51:07 - INFO - __main__ -   Batch Number = 119
01/07/2022 12:51:07 - INFO - __main__ -   Batch Number = 120
Evaluating:  73%|  | 120/165 [00:02<00:01, 44.30it/s]01/07/2022 12:51:07 - INFO - __main__ -   Batch Number = 121
01/07/2022 12:51:07 - INFO - __main__ -   Batch Number = 122
01/07/2022 12:51:07 - INFO - __main__ -   Batch Number = 123
01/07/2022 12:51:07 - INFO - __main__ -   Batch Number = 124
01/07/2022 12:51:07 - INFO - __main__ -   Batch Number = 125
Evaluating:  76%|  | 125/165 [00:02<00:00, 44.22it/s]01/07/2022 12:51:07 - INFO - __main__ -   Batch Number = 126
01/07/2022 12:51:07 - INFO - __main__ -   Batch Number = 127
01/07/2022 12:51:07 - INFO - __main__ -   Batch Number = 128
01/07/2022 12:51:07 - INFO - __main__ -   Batch Number = 129
01/07/2022 12:51:07 - INFO - __main__ -   Batch Number = 130
Evaluating:  79%|  | 130/165 [00:03<00:00, 44.33it/s]01/07/2022 12:51:07 - INFO - __main__ -   Batch Number = 131
01/07/2022 12:51:07 - INFO - __main__ -   Batch Number = 132
01/07/2022 12:51:07 - INFO - __main__ -   Batch Number = 133
01/07/2022 12:51:07 - INFO - __main__ -   Batch Number = 134
01/07/2022 12:51:07 - INFO - __main__ -   Batch Number = 135
Evaluating:  82%| | 135/165 [00:03<00:00, 44.38it/s]01/07/2022 12:51:07 - INFO - __main__ -   Batch Number = 136
01/07/2022 12:51:07 - INFO - __main__ -   Batch Number = 137
01/07/2022 12:51:07 - INFO - __main__ -   Batch Number = 138
01/07/2022 12:51:07 - INFO - __main__ -   Batch Number = 139
01/07/2022 12:51:07 - INFO - __main__ -   Batch Number = 140
Evaluating:  85%| | 140/165 [00:03<00:00, 44.32it/s]01/07/2022 12:51:07 - INFO - __main__ -   Batch Number = 141
01/07/2022 12:51:07 - INFO - __main__ -   Batch Number = 142
01/07/2022 12:51:07 - INFO - __main__ -   Batch Number = 143
01/07/2022 12:51:07 - INFO - __main__ -   Batch Number = 144
01/07/2022 12:51:07 - INFO - __main__ -   Batch Number = 145
Evaluating:  88%| | 145/165 [00:03<00:00, 44.33it/s]01/07/2022 12:51:07 - INFO - __main__ -   Batch Number = 146
01/07/2022 12:51:07 - INFO - __main__ -   Batch Number = 147
01/07/2022 12:51:07 - INFO - __main__ -   Batch Number = 148
01/07/2022 12:51:07 - INFO - __main__ -   Batch Number = 149
01/07/2022 12:51:07 - INFO - __main__ -   Batch Number = 150
Evaluating:  91%| | 150/165 [00:03<00:00, 44.28it/s]01/07/2022 12:51:07 - INFO - __main__ -   Batch Number = 151
01/07/2022 12:51:07 - INFO - __main__ -   Batch Number = 152
01/07/2022 12:51:07 - INFO - __main__ -   Batch Number = 153
01/07/2022 12:51:07 - INFO - __main__ -   Batch Number = 154
01/07/2022 12:51:07 - INFO - __main__ -   Batch Number = 155
Evaluating:  94%|| 155/165 [00:03<00:00, 44.25it/s]01/07/2022 12:51:07 - INFO - __main__ -   Batch Number = 156
01/07/2022 12:51:07 - INFO - __main__ -   Batch Number = 157
01/07/2022 12:51:07 - INFO - __main__ -   Batch Number = 158
01/07/2022 12:51:07 - INFO - __main__ -   Batch Number = 159
01/07/2022 12:51:08 - INFO - __main__ -   Batch Number = 160
Evaluating:  97%|| 160/165 [00:03<00:00, 43.16it/s]01/07/2022 12:51:08 - INFO - __main__ -   Batch Number = 161
01/07/2022 12:51:08 - INFO - __main__ -   Batch Number = 162
01/07/2022 12:51:08 - INFO - __main__ -   Batch Number = 163
01/07/2022 12:51:08 - INFO - __main__ -   Batch Number = 164
01/07/2022 12:51:08 - INFO - __main__ -   Batch Number = 165
Evaluating: 100%|| 165/165 [00:03<00:00, 43.90it/s]Evaluating: 100%|| 165/165 [00:03<00:00, 43.12it/s]
01/07/2022 12:51:08 - INFO - __main__ -     Evaluation done in total 3.827012 secs (0.002912 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/predictions_vi_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/nbest_predictions_vi_.json
01/07/2022 12:51:11 - INFO - __main__ -   Results = OrderedDict([('exact', 51.84873949579832), ('f1', 71.7717756131231), ('total', 1190), ('HasAns_exact', 51.84873949579832), ('HasAns_f1', 71.7717756131231), ('HasAns_total', 1190), ('best_exact', 51.84873949579832), ('best_exact_thresh', 0.0), ('best_f1', 71.7717756131231), ('best_f1_thresh', 0.0)])
01/07/2022 12:51:11 - INFO - __main__ -   Language adapter for zh not found, using en instead
01/07/2022 12:51:11 - INFO - __main__ -   Set active language adapter to en
01/07/2022 12:51:11 - INFO - __main__ -   Args Adapter Weight = None
01/07/2022 12:51:11 - INFO - __main__ -   Adapter Languages = ['en']
01/07/2022 12:51:11 - INFO - __main__ -   Predict File = xquad.zh.json
01/07/2022 12:51:11 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
  0%|          | 0/48 [00:00<?, ?it/s] 75%|  | 36/48 [00:00<00:00, 346.37it/s]100%|| 48/48 [00:00<00:00, 258.54it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<03:18,  6.00it/s]convert squad examples to features:  32%|      | 385/1190 [00:00<00:00, 1081.83it/s]convert squad examples to features: 100%|| 1190/1190 [00:00<00:00, 2872.35it/s]/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|| 1190/1190 [00:00<00:00, 728114.04it/s]
01/07/2022 12:51:12 - INFO - __main__ -   Local Rank = -1
01/07/2022 12:51:12 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.zh.json_xlm-roberta-base_384_zh
01/07/2022 12:51:13 - INFO - __main__ -   ***** Running evaluation  *****
01/07/2022 12:51:13 - INFO - __main__ -     Num examples = 1246
01/07/2022 12:51:13 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/156 [00:00<?, ?it/s]01/07/2022 12:51:13 - INFO - __main__ -   Batch Number = 1
01/07/2022 12:51:13 - INFO - __main__ -   Batch Number = 2
01/07/2022 12:51:13 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|         | 3/156 [00:00<00:05, 26.36it/s]01/07/2022 12:51:13 - INFO - __main__ -   Batch Number = 4
01/07/2022 12:51:13 - INFO - __main__ -   Batch Number = 5
01/07/2022 12:51:13 - INFO - __main__ -   Batch Number = 6
Evaluating:   4%|         | 6/156 [00:00<00:05, 28.26it/s]01/07/2022 12:51:13 - INFO - __main__ -   Batch Number = 7
01/07/2022 12:51:13 - INFO - __main__ -   Batch Number = 8
01/07/2022 12:51:13 - INFO - __main__ -   Batch Number = 9
01/07/2022 12:51:13 - INFO - __main__ -   Batch Number = 10
01/07/2022 12:51:13 - INFO - __main__ -   Batch Number = 11
Evaluating:   7%|         | 11/156 [00:00<00:04, 35.06it/s]01/07/2022 12:51:13 - INFO - __main__ -   Batch Number = 12
01/07/2022 12:51:13 - INFO - __main__ -   Batch Number = 13
01/07/2022 12:51:14 - INFO - __main__ -   Batch Number = 14
01/07/2022 12:51:14 - INFO - __main__ -   Batch Number = 15
01/07/2022 12:51:14 - INFO - __main__ -   Batch Number = 16
Evaluating:  10%|         | 16/156 [00:00<00:03, 38.95it/s]01/07/2022 12:51:14 - INFO - __main__ -   Batch Number = 17
01/07/2022 12:51:14 - INFO - __main__ -   Batch Number = 18
01/07/2022 12:51:14 - INFO - __main__ -   Batch Number = 19
01/07/2022 12:51:14 - INFO - __main__ -   Batch Number = 20
01/07/2022 12:51:14 - INFO - __main__ -   Batch Number = 21
Evaluating:  13%|        | 21/156 [00:00<00:03, 40.69it/s]01/07/2022 12:51:14 - INFO - __main__ -   Batch Number = 22
01/07/2022 12:51:14 - INFO - __main__ -   Batch Number = 23
01/07/2022 12:51:14 - INFO - __main__ -   Batch Number = 24
01/07/2022 12:51:14 - INFO - __main__ -   Batch Number = 25
01/07/2022 12:51:14 - INFO - __main__ -   Batch Number = 26
Evaluating:  17%|        | 26/156 [00:00<00:03, 42.26it/s]01/07/2022 12:51:14 - INFO - __main__ -   Batch Number = 27
01/07/2022 12:51:14 - INFO - __main__ -   Batch Number = 28
01/07/2022 12:51:14 - INFO - __main__ -   Batch Number = 29
01/07/2022 12:51:14 - INFO - __main__ -   Batch Number = 30
01/07/2022 12:51:14 - INFO - __main__ -   Batch Number = 31
Evaluating:  20%|        | 31/156 [00:00<00:02, 43.02it/s]01/07/2022 12:51:14 - INFO - __main__ -   Batch Number = 32
01/07/2022 12:51:14 - INFO - __main__ -   Batch Number = 33
01/07/2022 12:51:14 - INFO - __main__ -   Batch Number = 34
01/07/2022 12:51:14 - INFO - __main__ -   Batch Number = 35
01/07/2022 12:51:14 - INFO - __main__ -   Batch Number = 36
Evaluating:  23%|       | 36/156 [00:00<00:02, 43.44it/s]01/07/2022 12:51:14 - INFO - __main__ -   Batch Number = 37
01/07/2022 12:51:14 - INFO - __main__ -   Batch Number = 38
01/07/2022 12:51:14 - INFO - __main__ -   Batch Number = 39
01/07/2022 12:51:14 - INFO - __main__ -   Batch Number = 40
01/07/2022 12:51:14 - INFO - __main__ -   Batch Number = 41
Evaluating:  26%|       | 41/156 [00:01<00:02, 43.76it/s]01/07/2022 12:51:14 - INFO - __main__ -   Batch Number = 42
01/07/2022 12:51:14 - INFO - __main__ -   Batch Number = 43
01/07/2022 12:51:14 - INFO - __main__ -   Batch Number = 44
01/07/2022 12:51:14 - INFO - __main__ -   Batch Number = 45
01/07/2022 12:51:14 - INFO - __main__ -   Batch Number = 46
Evaluating:  29%|       | 46/156 [00:01<00:02, 44.19it/s]01/07/2022 12:51:14 - INFO - __main__ -   Batch Number = 47
01/07/2022 12:51:14 - INFO - __main__ -   Batch Number = 48
01/07/2022 12:51:14 - INFO - __main__ -   Batch Number = 49
01/07/2022 12:51:14 - INFO - __main__ -   Batch Number = 50
01/07/2022 12:51:14 - INFO - __main__ -   Batch Number = 51
Evaluating:  33%|      | 51/156 [00:01<00:02, 44.28it/s]01/07/2022 12:51:14 - INFO - __main__ -   Batch Number = 52
01/07/2022 12:51:14 - INFO - __main__ -   Batch Number = 53
01/07/2022 12:51:14 - INFO - __main__ -   Batch Number = 54
01/07/2022 12:51:14 - INFO - __main__ -   Batch Number = 55
01/07/2022 12:51:14 - INFO - __main__ -   Batch Number = 56
Evaluating:  36%|      | 56/156 [00:01<00:02, 44.50it/s]01/07/2022 12:51:14 - INFO - __main__ -   Batch Number = 57
01/07/2022 12:51:14 - INFO - __main__ -   Batch Number = 58
01/07/2022 12:51:15 - INFO - __main__ -   Batch Number = 59
01/07/2022 12:51:15 - INFO - __main__ -   Batch Number = 60
01/07/2022 12:51:15 - INFO - __main__ -   Batch Number = 61
Evaluating:  39%|      | 61/156 [00:01<00:02, 44.49it/s]01/07/2022 12:51:15 - INFO - __main__ -   Batch Number = 62
01/07/2022 12:51:15 - INFO - __main__ -   Batch Number = 63
01/07/2022 12:51:15 - INFO - __main__ -   Batch Number = 64
01/07/2022 12:51:15 - INFO - __main__ -   Batch Number = 65
01/07/2022 12:51:15 - INFO - __main__ -   Batch Number = 66
Evaluating:  42%|     | 66/156 [00:01<00:02, 44.50it/s]01/07/2022 12:51:15 - INFO - __main__ -   Batch Number = 67
01/07/2022 12:51:15 - INFO - __main__ -   Batch Number = 68
01/07/2022 12:51:15 - INFO - __main__ -   Batch Number = 69
01/07/2022 12:51:15 - INFO - __main__ -   Batch Number = 70
01/07/2022 12:51:15 - INFO - __main__ -   Batch Number = 71
Evaluating:  46%|     | 71/156 [00:01<00:01, 44.54it/s]01/07/2022 12:51:15 - INFO - __main__ -   Batch Number = 72
01/07/2022 12:51:15 - INFO - __main__ -   Batch Number = 73
01/07/2022 12:51:15 - INFO - __main__ -   Batch Number = 74
01/07/2022 12:51:15 - INFO - __main__ -   Batch Number = 75
01/07/2022 12:51:15 - INFO - __main__ -   Batch Number = 76
Evaluating:  49%|     | 76/156 [00:01<00:01, 44.72it/s]01/07/2022 12:51:15 - INFO - __main__ -   Batch Number = 77
01/07/2022 12:51:15 - INFO - __main__ -   Batch Number = 78
01/07/2022 12:51:15 - INFO - __main__ -   Batch Number = 79
01/07/2022 12:51:15 - INFO - __main__ -   Batch Number = 80
01/07/2022 12:51:15 - INFO - __main__ -   Batch Number = 81
Evaluating:  52%|    | 81/156 [00:01<00:01, 43.04it/s]01/07/2022 12:51:15 - INFO - __main__ -   Batch Number = 82
01/07/2022 12:51:15 - INFO - __main__ -   Batch Number = 83
01/07/2022 12:51:15 - INFO - __main__ -   Batch Number = 84
01/07/2022 12:51:15 - INFO - __main__ -   Batch Number = 85
01/07/2022 12:51:15 - INFO - __main__ -   Batch Number = 86
Evaluating:  55%|    | 86/156 [00:02<00:01, 43.50it/s]01/07/2022 12:51:15 - INFO - __main__ -   Batch Number = 87
01/07/2022 12:51:15 - INFO - __main__ -   Batch Number = 88
01/07/2022 12:51:15 - INFO - __main__ -   Batch Number = 89
01/07/2022 12:51:15 - INFO - __main__ -   Batch Number = 90
01/07/2022 12:51:15 - INFO - __main__ -   Batch Number = 91
Evaluating:  58%|    | 91/156 [00:02<00:01, 43.89it/s]01/07/2022 12:51:15 - INFO - __main__ -   Batch Number = 92
01/07/2022 12:51:15 - INFO - __main__ -   Batch Number = 93
01/07/2022 12:51:15 - INFO - __main__ -   Batch Number = 94
01/07/2022 12:51:15 - INFO - __main__ -   Batch Number = 95
01/07/2022 12:51:15 - INFO - __main__ -   Batch Number = 96
Evaluating:  62%|   | 96/156 [00:02<00:01, 43.93it/s]01/07/2022 12:51:15 - INFO - __main__ -   Batch Number = 97
01/07/2022 12:51:15 - INFO - __main__ -   Batch Number = 98
01/07/2022 12:51:15 - INFO - __main__ -   Batch Number = 99
01/07/2022 12:51:15 - INFO - __main__ -   Batch Number = 100
01/07/2022 12:51:15 - INFO - __main__ -   Batch Number = 101
Evaluating:  65%|   | 101/156 [00:02<00:01, 44.10it/s]01/07/2022 12:51:15 - INFO - __main__ -   Batch Number = 102
01/07/2022 12:51:16 - INFO - __main__ -   Batch Number = 103
01/07/2022 12:51:16 - INFO - __main__ -   Batch Number = 104
01/07/2022 12:51:16 - INFO - __main__ -   Batch Number = 105
01/07/2022 12:51:16 - INFO - __main__ -   Batch Number = 106
Evaluating:  68%|   | 106/156 [00:02<00:01, 44.32it/s]01/07/2022 12:51:16 - INFO - __main__ -   Batch Number = 107
01/07/2022 12:51:16 - INFO - __main__ -   Batch Number = 108
01/07/2022 12:51:16 - INFO - __main__ -   Batch Number = 109
01/07/2022 12:51:16 - INFO - __main__ -   Batch Number = 110
01/07/2022 12:51:16 - INFO - __main__ -   Batch Number = 111
Evaluating:  71%|   | 111/156 [00:02<00:01, 44.25it/s]01/07/2022 12:51:16 - INFO - __main__ -   Batch Number = 112
01/07/2022 12:51:16 - INFO - __main__ -   Batch Number = 113
01/07/2022 12:51:16 - INFO - __main__ -   Batch Number = 114
01/07/2022 12:51:16 - INFO - __main__ -   Batch Number = 115
01/07/2022 12:51:16 - INFO - __main__ -   Batch Number = 116
Evaluating:  74%|  | 116/156 [00:02<00:00, 44.35it/s]01/07/2022 12:51:16 - INFO - __main__ -   Batch Number = 117
01/07/2022 12:51:16 - INFO - __main__ -   Batch Number = 118
01/07/2022 12:51:16 - INFO - __main__ -   Batch Number = 119
01/07/2022 12:51:16 - INFO - __main__ -   Batch Number = 120
01/07/2022 12:51:16 - INFO - __main__ -   Batch Number = 121
Evaluating:  78%|  | 121/156 [00:02<00:00, 43.34it/s]01/07/2022 12:51:16 - INFO - __main__ -   Batch Number = 122
01/07/2022 12:51:16 - INFO - __main__ -   Batch Number = 123
01/07/2022 12:51:16 - INFO - __main__ -   Batch Number = 124
01/07/2022 12:51:16 - INFO - __main__ -   Batch Number = 125
01/07/2022 12:51:16 - INFO - __main__ -   Batch Number = 126
Evaluating:  81%|  | 126/156 [00:02<00:00, 43.23it/s]01/07/2022 12:51:16 - INFO - __main__ -   Batch Number = 127
01/07/2022 12:51:16 - INFO - __main__ -   Batch Number = 128
01/07/2022 12:51:16 - INFO - __main__ -   Batch Number = 129
01/07/2022 12:51:16 - INFO - __main__ -   Batch Number = 130
01/07/2022 12:51:16 - INFO - __main__ -   Batch Number = 131
Evaluating:  84%| | 131/156 [00:03<00:00, 43.33it/s]01/07/2022 12:51:16 - INFO - __main__ -   Batch Number = 132
01/07/2022 12:51:16 - INFO - __main__ -   Batch Number = 133
01/07/2022 12:51:16 - INFO - __main__ -   Batch Number = 134
01/07/2022 12:51:16 - INFO - __main__ -   Batch Number = 135
01/07/2022 12:51:16 - INFO - __main__ -   Batch Number = 136
Evaluating:  87%| | 136/156 [00:03<00:00, 43.50it/s]01/07/2022 12:51:16 - INFO - __main__ -   Batch Number = 137
01/07/2022 12:51:16 - INFO - __main__ -   Batch Number = 138
01/07/2022 12:51:16 - INFO - __main__ -   Batch Number = 139
01/07/2022 12:51:16 - INFO - __main__ -   Batch Number = 140
01/07/2022 12:51:16 - INFO - __main__ -   Batch Number = 141
Evaluating:  90%| | 141/156 [00:03<00:00, 43.66it/s]01/07/2022 12:51:16 - INFO - __main__ -   Batch Number = 142
01/07/2022 12:51:16 - INFO - __main__ -   Batch Number = 143
01/07/2022 12:51:16 - INFO - __main__ -   Batch Number = 144
01/07/2022 12:51:16 - INFO - __main__ -   Batch Number = 145
01/07/2022 12:51:16 - INFO - __main__ -   Batch Number = 146
Evaluating:  94%|| 146/156 [00:03<00:00, 43.89it/s]01/07/2022 12:51:17 - INFO - __main__ -   Batch Number = 147
01/07/2022 12:51:17 - INFO - __main__ -   Batch Number = 148
01/07/2022 12:51:17 - INFO - __main__ -   Batch Number = 149
01/07/2022 12:51:17 - INFO - __main__ -   Batch Number = 150
01/07/2022 12:51:17 - INFO - __main__ -   Batch Number = 151
Evaluating:  97%|| 151/156 [00:03<00:00, 43.75it/s]01/07/2022 12:51:17 - INFO - __main__ -   Batch Number = 152
01/07/2022 12:51:17 - INFO - __main__ -   Batch Number = 153
01/07/2022 12:51:17 - INFO - __main__ -   Batch Number = 154
01/07/2022 12:51:17 - INFO - __main__ -   Batch Number = 155
01/07/2022 12:51:17 - INFO - __main__ -   Batch Number = 156
Evaluating: 100%|| 156/156 [00:03<00:00, 44.64it/s]Evaluating: 100%|| 156/156 [00:03<00:00, 43.15it/s]
01/07/2022 12:51:17 - INFO - __main__ -     Evaluation done in total 3.615829 secs (0.002902 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/predictions_zh_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/nbest_predictions_zh_.json
01/07/2022 12:51:39 - INFO - __main__ -   Results = OrderedDict([('exact', 39.747899159663866), ('f1', 50.796318527410946), ('total', 1190), ('HasAns_exact', 39.747899159663866), ('HasAns_f1', 50.796318527410946), ('HasAns_total', 1190), ('best_exact', 39.747899159663866), ('best_exact_thresh', 0.0), ('best_f1', 50.796318527410946), ('best_f1_thresh', 0.0)])
PyTorch version 1.10.0+cu111 available.
01/07/2022 12:51:42 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/07/2022 12:51:42 - INFO - root -   save model
loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /root/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6
Model config XLMRobertaConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /root/.cache/torch/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8
loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/07/2022 12:51:50 - INFO - __main__ -   lang2id = None
01/07/2022 12:51:53 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='xlm-roberta-base', model_type='xlm-roberta', config_name=None, tokenizer_name=None, cache_dir=None, data_dir='/root/Desktop/cloud-emea-copy/data//xquad', output_dir='/root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/', max_seq_length=384, train_file='/root/Desktop/cloud-emea-copy/data//xquad/train-v1.1.json', predict_file='/root/Desktop/cloud-emea-copy/data//xquad/dev-v1.1.json', eval_test_set=False, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=4, per_gpu_eval_batch_size=8, gradient_accumulation_steps=4, learning_rate=0.0001, weight_decay=0.0001, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=15.0, max_steps=-1, save_steps=1000, warmup_steps=500, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', eval_lang='en', predict_langs='ar,de,el,es,hi,ru,th,tr,vi,zh', train_lang='en', log_file='/root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='qna', threads=8, version_2_with_negative=False, verbose_logging=False, n_best_size=20, max_query_length=64, max_answer_length=30, doc_stride=128, null_score_diff_threshold=0.0, predict_task_adapter='output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s2/checkpoint-best/qna', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix=None, en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/07/2022 12:51:53 - INFO - __main__ -   Results: {}
loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /root/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6
Model config XLMRobertaConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /root/.cache/torch/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8
loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/07/2022 12:52:02 - INFO - __main__ -   lang2id = None
01/07/2022 12:52:02 - INFO - __main__ -   Evaluating the model on the test set of all languages specified
01/07/2022 12:52:02 - INFO - __main__ -   Adapter will be loaded from this path: output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s2/checkpoint-best/qna
01/07/2022 12:52:02 - INFO - root -   Trying to decide if add adapter
01/07/2022 12:52:02 - INFO - root -   loading task adapter
Loading module configuration from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s2/checkpoint-best/qna/adapter_config.json
Adding adapter 'qna' of type 'text_task'.
Loading module weights from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s2/checkpoint-best/qna/pytorch_adapter.bin
Loading module configuration from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s2/checkpoint-best/qna/head_config.json
Loading module weights from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s2/checkpoint-best/qna/pytorch_model_head.bin
01/07/2022 12:52:02 - INFO - root -   loading lang adpater en/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/xlm-roberta-base/pfeiffer/en_relu_2.zip.
Loading module configuration from /root/.cache/torch/adapters/3e9c8c800af886cd1c684a3acf4929990b35f3ac75e050e942de2ff2ecf895fe-caabba7772816577124631d5b84810c4d32f49afcea4fca637162ec7f432174f-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
Loading module weights from /root/.cache/torch/adapters/3e9c8c800af886cd1c684a3acf4929990b35f3ac75e050e942de2ff2ecf895fe-caabba7772816577124631d5b84810c4d32f49afcea4fca637162ec7f432174f-extracted/pytorch_adapter.bin
No matching prediction head found in '/root/.cache/torch/adapters/3e9c8c800af886cd1c684a3acf4929990b35f3ac75e050e942de2ff2ecf895fe-caabba7772816577124631d5b84810c4d32f49afcea4fca637162ec7f432174f-extracted'
01/07/2022 12:52:05 - INFO - __main__ -   Language adapter for ar not found, using en instead
01/07/2022 12:52:05 - INFO - __main__ -   Set active language adapter to en
01/07/2022 12:52:05 - INFO - __main__ -   Args Adapter Weight = None
01/07/2022 12:52:05 - INFO - __main__ -   Adapter Languages = ['en']
01/07/2022 12:52:05 - INFO - __main__ -   Predict File = xquad.ar.json
01/07/2022 12:52:05 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
en en/wiki@ukp
  0%|          | 0/48 [00:00<?, ?it/s] 33%|      | 16/48 [00:00<00:00, 139.26it/s] 62%|   | 30/48 [00:00<00:00, 133.08it/s] 94%|| 45/48 [00:00<00:00, 140.18it/s]100%|| 48/48 [00:00<00:00, 141.58it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<03:51,  5.13it/s]convert squad examples to features:  32%|      | 385/1190 [00:00<00:01, 710.69it/s]convert squad examples to features:  97%|| 1153/1190 [00:00<00:00, 2074.90it/s]convert squad examples to features: 100%|| 1190/1190 [00:00<00:00, 1649.13it/s]/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|| 1190/1190 [00:00<00:00, 489479.43it/s]
01/07/2022 12:52:07 - INFO - __main__ -   Local Rank = -1
01/07/2022 12:52:07 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.ar.json_xlm-roberta-base_384_ar
01/07/2022 12:52:08 - INFO - __main__ -   ***** Running evaluation  *****
01/07/2022 12:52:08 - INFO - __main__ -     Num examples = 1318
01/07/2022 12:52:08 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/165 [00:00<?, ?it/s]01/07/2022 12:52:08 - INFO - __main__ -   Batch Number = 1
01/07/2022 12:52:08 - INFO - __main__ -   Batch Number = 2
01/07/2022 12:52:08 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|         | 3/165 [00:00<00:06, 26.88it/s]01/07/2022 12:52:08 - INFO - __main__ -   Batch Number = 4
01/07/2022 12:52:08 - INFO - __main__ -   Batch Number = 5
01/07/2022 12:52:08 - INFO - __main__ -   Batch Number = 6
01/07/2022 12:52:08 - INFO - __main__ -   Batch Number = 7
Evaluating:   4%|         | 7/165 [00:00<00:04, 31.74it/s]01/07/2022 12:52:08 - INFO - __main__ -   Batch Number = 8
01/07/2022 12:52:08 - INFO - __main__ -   Batch Number = 9
01/07/2022 12:52:08 - INFO - __main__ -   Batch Number = 10
01/07/2022 12:52:08 - INFO - __main__ -   Batch Number = 11
01/07/2022 12:52:08 - INFO - __main__ -   Batch Number = 12
Evaluating:   7%|         | 12/165 [00:00<00:04, 37.79it/s]01/07/2022 12:52:08 - INFO - __main__ -   Batch Number = 13
01/07/2022 12:52:08 - INFO - __main__ -   Batch Number = 14
01/07/2022 12:52:08 - INFO - __main__ -   Batch Number = 15
01/07/2022 12:52:08 - INFO - __main__ -   Batch Number = 16
01/07/2022 12:52:08 - INFO - __main__ -   Batch Number = 17
Evaluating:  10%|         | 17/165 [00:00<00:03, 40.22it/s]01/07/2022 12:52:08 - INFO - __main__ -   Batch Number = 18
01/07/2022 12:52:09 - INFO - __main__ -   Batch Number = 19
01/07/2022 12:52:09 - INFO - __main__ -   Batch Number = 20
01/07/2022 12:52:09 - INFO - __main__ -   Batch Number = 21
01/07/2022 12:52:09 - INFO - __main__ -   Batch Number = 22
Evaluating:  13%|        | 22/165 [00:00<00:03, 41.95it/s]01/07/2022 12:52:09 - INFO - __main__ -   Batch Number = 23
01/07/2022 12:52:09 - INFO - __main__ -   Batch Number = 24
01/07/2022 12:52:09 - INFO - __main__ -   Batch Number = 25
01/07/2022 12:52:09 - INFO - __main__ -   Batch Number = 26
01/07/2022 12:52:09 - INFO - __main__ -   Batch Number = 27
Evaluating:  16%|        | 27/165 [00:00<00:03, 42.97it/s]01/07/2022 12:52:09 - INFO - __main__ -   Batch Number = 28
01/07/2022 12:52:09 - INFO - __main__ -   Batch Number = 29
01/07/2022 12:52:09 - INFO - __main__ -   Batch Number = 30
01/07/2022 12:52:09 - INFO - __main__ -   Batch Number = 31
01/07/2022 12:52:09 - INFO - __main__ -   Batch Number = 32
Evaluating:  19%|        | 32/165 [00:00<00:03, 43.52it/s]01/07/2022 12:52:09 - INFO - __main__ -   Batch Number = 33
01/07/2022 12:52:09 - INFO - __main__ -   Batch Number = 34
01/07/2022 12:52:09 - INFO - __main__ -   Batch Number = 35
01/07/2022 12:52:09 - INFO - __main__ -   Batch Number = 36
01/07/2022 12:52:09 - INFO - __main__ -   Batch Number = 37
Evaluating:  22%|       | 37/165 [00:00<00:02, 43.69it/s]01/07/2022 12:52:09 - INFO - __main__ -   Batch Number = 38
01/07/2022 12:52:09 - INFO - __main__ -   Batch Number = 39
01/07/2022 12:52:09 - INFO - __main__ -   Batch Number = 40
01/07/2022 12:52:09 - INFO - __main__ -   Batch Number = 41
01/07/2022 12:52:09 - INFO - __main__ -   Batch Number = 42
Evaluating:  25%|       | 42/165 [00:01<00:02, 43.69it/s]01/07/2022 12:52:09 - INFO - __main__ -   Batch Number = 43
01/07/2022 12:52:09 - INFO - __main__ -   Batch Number = 44
01/07/2022 12:52:09 - INFO - __main__ -   Batch Number = 45
01/07/2022 12:52:09 - INFO - __main__ -   Batch Number = 46
01/07/2022 12:52:09 - INFO - __main__ -   Batch Number = 47
Evaluating:  28%|       | 47/165 [00:01<00:02, 42.10it/s]01/07/2022 12:52:09 - INFO - __main__ -   Batch Number = 48
01/07/2022 12:52:09 - INFO - __main__ -   Batch Number = 49
01/07/2022 12:52:09 - INFO - __main__ -   Batch Number = 50
01/07/2022 12:52:09 - INFO - __main__ -   Batch Number = 51
01/07/2022 12:52:09 - INFO - __main__ -   Batch Number = 52
Evaluating:  32%|      | 52/165 [00:01<00:02, 43.01it/s]01/07/2022 12:52:09 - INFO - __main__ -   Batch Number = 53
01/07/2022 12:52:09 - INFO - __main__ -   Batch Number = 54
01/07/2022 12:52:09 - INFO - __main__ -   Batch Number = 55
01/07/2022 12:52:09 - INFO - __main__ -   Batch Number = 56
01/07/2022 12:52:09 - INFO - __main__ -   Batch Number = 57
Evaluating:  35%|      | 57/165 [00:01<00:02, 43.33it/s]01/07/2022 12:52:09 - INFO - __main__ -   Batch Number = 58
01/07/2022 12:52:09 - INFO - __main__ -   Batch Number = 59
01/07/2022 12:52:10 - INFO - __main__ -   Batch Number = 60
01/07/2022 12:52:10 - INFO - __main__ -   Batch Number = 61
01/07/2022 12:52:10 - INFO - __main__ -   Batch Number = 62
Evaluating:  38%|      | 62/165 [00:01<00:03, 31.44it/s]01/07/2022 12:52:10 - INFO - __main__ -   Batch Number = 63
01/07/2022 12:52:10 - INFO - __main__ -   Batch Number = 64
01/07/2022 12:52:10 - INFO - __main__ -   Batch Number = 65
01/07/2022 12:52:10 - INFO - __main__ -   Batch Number = 66
01/07/2022 12:52:10 - INFO - __main__ -   Batch Number = 67
Evaluating:  41%|      | 67/165 [00:01<00:02, 34.40it/s]01/07/2022 12:52:10 - INFO - __main__ -   Batch Number = 68
01/07/2022 12:52:10 - INFO - __main__ -   Batch Number = 69
01/07/2022 12:52:10 - INFO - __main__ -   Batch Number = 70
01/07/2022 12:52:10 - INFO - __main__ -   Batch Number = 71
01/07/2022 12:52:10 - INFO - __main__ -   Batch Number = 72
Evaluating:  44%|     | 72/165 [00:01<00:02, 36.97it/s]01/07/2022 12:52:10 - INFO - __main__ -   Batch Number = 73
01/07/2022 12:52:10 - INFO - __main__ -   Batch Number = 74
01/07/2022 12:52:10 - INFO - __main__ -   Batch Number = 75
01/07/2022 12:52:10 - INFO - __main__ -   Batch Number = 76
01/07/2022 12:52:10 - INFO - __main__ -   Batch Number = 77
Evaluating:  47%|     | 77/165 [00:01<00:02, 38.73it/s]01/07/2022 12:52:10 - INFO - __main__ -   Batch Number = 78
01/07/2022 12:52:10 - INFO - __main__ -   Batch Number = 79
01/07/2022 12:52:10 - INFO - __main__ -   Batch Number = 80
01/07/2022 12:52:10 - INFO - __main__ -   Batch Number = 81
01/07/2022 12:52:10 - INFO - __main__ -   Batch Number = 82
Evaluating:  50%|     | 82/165 [00:02<00:02, 40.29it/s]01/07/2022 12:52:10 - INFO - __main__ -   Batch Number = 83
01/07/2022 12:52:10 - INFO - __main__ -   Batch Number = 84
01/07/2022 12:52:10 - INFO - __main__ -   Batch Number = 85
01/07/2022 12:52:10 - INFO - __main__ -   Batch Number = 86
01/07/2022 12:52:10 - INFO - __main__ -   Batch Number = 87
Evaluating:  53%|    | 87/165 [00:02<00:01, 41.44it/s]01/07/2022 12:52:10 - INFO - __main__ -   Batch Number = 88
01/07/2022 12:52:10 - INFO - __main__ -   Batch Number = 89
01/07/2022 12:52:10 - INFO - __main__ -   Batch Number = 90
01/07/2022 12:52:10 - INFO - __main__ -   Batch Number = 91
01/07/2022 12:52:10 - INFO - __main__ -   Batch Number = 92
Evaluating:  56%|    | 92/165 [00:02<00:01, 42.27it/s]01/07/2022 12:52:10 - INFO - __main__ -   Batch Number = 93
01/07/2022 12:52:10 - INFO - __main__ -   Batch Number = 94
01/07/2022 12:52:10 - INFO - __main__ -   Batch Number = 95
01/07/2022 12:52:10 - INFO - __main__ -   Batch Number = 96
01/07/2022 12:52:10 - INFO - __main__ -   Batch Number = 97
Evaluating:  59%|    | 97/165 [00:02<00:01, 42.87it/s]01/07/2022 12:52:10 - INFO - __main__ -   Batch Number = 98
01/07/2022 12:52:10 - INFO - __main__ -   Batch Number = 99
01/07/2022 12:52:10 - INFO - __main__ -   Batch Number = 100
01/07/2022 12:52:11 - INFO - __main__ -   Batch Number = 101
01/07/2022 12:52:11 - INFO - __main__ -   Batch Number = 102
Evaluating:  62%|   | 102/165 [00:02<00:01, 43.39it/s]01/07/2022 12:52:11 - INFO - __main__ -   Batch Number = 103
01/07/2022 12:52:11 - INFO - __main__ -   Batch Number = 104
01/07/2022 12:52:11 - INFO - __main__ -   Batch Number = 105
01/07/2022 12:52:11 - INFO - __main__ -   Batch Number = 106
01/07/2022 12:52:11 - INFO - __main__ -   Batch Number = 107
Evaluating:  65%|   | 107/165 [00:02<00:01, 43.59it/s]01/07/2022 12:52:11 - INFO - __main__ -   Batch Number = 108
01/07/2022 12:52:11 - INFO - __main__ -   Batch Number = 109
01/07/2022 12:52:11 - INFO - __main__ -   Batch Number = 110
01/07/2022 12:52:11 - INFO - __main__ -   Batch Number = 111
01/07/2022 12:52:11 - INFO - __main__ -   Batch Number = 112
Evaluating:  68%|   | 112/165 [00:02<00:01, 43.87it/s]01/07/2022 12:52:11 - INFO - __main__ -   Batch Number = 113
01/07/2022 12:52:11 - INFO - __main__ -   Batch Number = 114
01/07/2022 12:52:11 - INFO - __main__ -   Batch Number = 115
01/07/2022 12:52:11 - INFO - __main__ -   Batch Number = 116
01/07/2022 12:52:11 - INFO - __main__ -   Batch Number = 117
Evaluating:  71%|   | 117/165 [00:02<00:01, 44.17it/s]01/07/2022 12:52:11 - INFO - __main__ -   Batch Number = 118
01/07/2022 12:52:11 - INFO - __main__ -   Batch Number = 119
01/07/2022 12:52:11 - INFO - __main__ -   Batch Number = 120
01/07/2022 12:52:11 - INFO - __main__ -   Batch Number = 121
01/07/2022 12:52:11 - INFO - __main__ -   Batch Number = 122
Evaluating:  74%|  | 122/165 [00:02<00:00, 44.16it/s]01/07/2022 12:52:11 - INFO - __main__ -   Batch Number = 123
01/07/2022 12:52:11 - INFO - __main__ -   Batch Number = 124
01/07/2022 12:52:11 - INFO - __main__ -   Batch Number = 125
01/07/2022 12:52:11 - INFO - __main__ -   Batch Number = 126
01/07/2022 12:52:11 - INFO - __main__ -   Batch Number = 127
Evaluating:  77%|  | 127/165 [00:03<00:00, 44.22it/s]01/07/2022 12:52:11 - INFO - __main__ -   Batch Number = 128
01/07/2022 12:52:11 - INFO - __main__ -   Batch Number = 129
01/07/2022 12:52:11 - INFO - __main__ -   Batch Number = 130
01/07/2022 12:52:11 - INFO - __main__ -   Batch Number = 131
01/07/2022 12:52:11 - INFO - __main__ -   Batch Number = 132
Evaluating:  80%|  | 132/165 [00:03<00:00, 44.26it/s]01/07/2022 12:52:11 - INFO - __main__ -   Batch Number = 133
01/07/2022 12:52:11 - INFO - __main__ -   Batch Number = 134
01/07/2022 12:52:11 - INFO - __main__ -   Batch Number = 135
01/07/2022 12:52:11 - INFO - __main__ -   Batch Number = 136
01/07/2022 12:52:11 - INFO - __main__ -   Batch Number = 137
Evaluating:  83%| | 137/165 [00:03<00:00, 44.29it/s]01/07/2022 12:52:11 - INFO - __main__ -   Batch Number = 138
01/07/2022 12:52:11 - INFO - __main__ -   Batch Number = 139
01/07/2022 12:52:11 - INFO - __main__ -   Batch Number = 140
01/07/2022 12:52:11 - INFO - __main__ -   Batch Number = 141
01/07/2022 12:52:11 - INFO - __main__ -   Batch Number = 142
Evaluating:  86%| | 142/165 [00:03<00:00, 44.41it/s]01/07/2022 12:52:11 - INFO - __main__ -   Batch Number = 143
01/07/2022 12:52:11 - INFO - __main__ -   Batch Number = 144
01/07/2022 12:52:12 - INFO - __main__ -   Batch Number = 145
01/07/2022 12:52:12 - INFO - __main__ -   Batch Number = 146
01/07/2022 12:52:12 - INFO - __main__ -   Batch Number = 147
Evaluating:  89%| | 147/165 [00:03<00:00, 44.55it/s]01/07/2022 12:52:12 - INFO - __main__ -   Batch Number = 148
01/07/2022 12:52:12 - INFO - __main__ -   Batch Number = 149
01/07/2022 12:52:12 - INFO - __main__ -   Batch Number = 150
01/07/2022 12:52:12 - INFO - __main__ -   Batch Number = 151
01/07/2022 12:52:12 - INFO - __main__ -   Batch Number = 152
Evaluating:  92%|| 152/165 [00:03<00:00, 44.50it/s]01/07/2022 12:52:12 - INFO - __main__ -   Batch Number = 153
01/07/2022 12:52:12 - INFO - __main__ -   Batch Number = 154
01/07/2022 12:52:12 - INFO - __main__ -   Batch Number = 155
01/07/2022 12:52:12 - INFO - __main__ -   Batch Number = 156
01/07/2022 12:52:12 - INFO - __main__ -   Batch Number = 157
Evaluating:  95%|| 157/165 [00:03<00:00, 44.53it/s]01/07/2022 12:52:12 - INFO - __main__ -   Batch Number = 158
01/07/2022 12:52:12 - INFO - __main__ -   Batch Number = 159
01/07/2022 12:52:12 - INFO - __main__ -   Batch Number = 160
01/07/2022 12:52:12 - INFO - __main__ -   Batch Number = 161
01/07/2022 12:52:12 - INFO - __main__ -   Batch Number = 162
Evaluating:  98%|| 162/165 [00:03<00:00, 44.59it/s]01/07/2022 12:52:12 - INFO - __main__ -   Batch Number = 163
01/07/2022 12:52:12 - INFO - __main__ -   Batch Number = 164
01/07/2022 12:52:12 - INFO - __main__ -   Batch Number = 165
Evaluating: 100%|| 165/165 [00:03<00:00, 41.89it/s]
01/07/2022 12:52:12 - INFO - __main__ -     Evaluation done in total 3.939262 secs (0.002989 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/predictions_ar_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/nbest_predictions_ar_.json
01/07/2022 12:52:15 - INFO - __main__ -   Results = OrderedDict([('exact', 47.64705882352941), ('f1', 64.68031521014225), ('total', 1190), ('HasAns_exact', 47.64705882352941), ('HasAns_f1', 64.68031521014225), ('HasAns_total', 1190), ('best_exact', 47.64705882352941), ('best_exact_thresh', 0.0), ('best_f1', 64.68031521014225), ('best_f1_thresh', 0.0)])
01/07/2022 12:52:15 - INFO - __main__ -   Language adapter for de not found, using en instead
01/07/2022 12:52:15 - INFO - __main__ -   Set active language adapter to en
01/07/2022 12:52:15 - INFO - __main__ -   Args Adapter Weight = None
01/07/2022 12:52:15 - INFO - __main__ -   Adapter Languages = ['en']
01/07/2022 12:52:15 - INFO - __main__ -   Predict File = xquad.de.json
01/07/2022 12:52:15 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
  0%|          | 0/48 [00:00<?, ?it/s] 33%|      | 16/48 [00:00<00:00, 120.95it/s] 60%|    | 29/48 [00:00<00:00, 113.56it/s] 92%|| 44/48 [00:00<00:00, 124.69it/s]100%|| 48/48 [00:00<00:00, 123.86it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<04:38,  4.26it/s]convert squad examples to features:  32%|      | 385/1190 [00:00<00:01, 650.14it/s]convert squad examples to features:  97%|| 1153/1190 [00:00<00:00, 1931.24it/s]convert squad examples to features: 100%|| 1190/1190 [00:00<00:00, 1515.02it/s]/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|| 1190/1190 [00:00<00:00, 442562.67it/s]
01/07/2022 12:52:17 - INFO - __main__ -   Local Rank = -1
01/07/2022 12:52:17 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.de.json_xlm-roberta-base_384_de
01/07/2022 12:52:18 - INFO - __main__ -   ***** Running evaluation  *****
01/07/2022 12:52:18 - INFO - __main__ -     Num examples = 1303
01/07/2022 12:52:18 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/163 [00:00<?, ?it/s]01/07/2022 12:52:18 - INFO - __main__ -   Batch Number = 1
01/07/2022 12:52:18 - INFO - __main__ -   Batch Number = 2
01/07/2022 12:52:18 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|         | 3/163 [00:00<00:05, 29.22it/s]01/07/2022 12:52:18 - INFO - __main__ -   Batch Number = 4
01/07/2022 12:52:18 - INFO - __main__ -   Batch Number = 5
01/07/2022 12:52:18 - INFO - __main__ -   Batch Number = 6
Evaluating:   4%|         | 6/163 [00:00<00:05, 29.39it/s]01/07/2022 12:52:18 - INFO - __main__ -   Batch Number = 7
01/07/2022 12:52:18 - INFO - __main__ -   Batch Number = 8
01/07/2022 12:52:18 - INFO - __main__ -   Batch Number = 9
01/07/2022 12:52:18 - INFO - __main__ -   Batch Number = 10
01/07/2022 12:52:18 - INFO - __main__ -   Batch Number = 11
Evaluating:   7%|         | 11/163 [00:00<00:04, 36.11it/s]01/07/2022 12:52:18 - INFO - __main__ -   Batch Number = 12
01/07/2022 12:52:18 - INFO - __main__ -   Batch Number = 13
01/07/2022 12:52:18 - INFO - __main__ -   Batch Number = 14
01/07/2022 12:52:18 - INFO - __main__ -   Batch Number = 15
01/07/2022 12:52:18 - INFO - __main__ -   Batch Number = 16
Evaluating:  10%|         | 16/163 [00:00<00:03, 39.51it/s]01/07/2022 12:52:18 - INFO - __main__ -   Batch Number = 17
01/07/2022 12:52:18 - INFO - __main__ -   Batch Number = 18
01/07/2022 12:52:18 - INFO - __main__ -   Batch Number = 19
01/07/2022 12:52:18 - INFO - __main__ -   Batch Number = 20
Evaluating:  12%|        | 20/163 [00:00<00:04, 30.70it/s]01/07/2022 12:52:19 - INFO - __main__ -   Batch Number = 21
01/07/2022 12:52:19 - INFO - __main__ -   Batch Number = 22
01/07/2022 12:52:19 - INFO - __main__ -   Batch Number = 23
01/07/2022 12:52:19 - INFO - __main__ -   Batch Number = 24
01/07/2022 12:52:19 - INFO - __main__ -   Batch Number = 25
Evaluating:  15%|        | 25/163 [00:00<00:03, 34.79it/s]01/07/2022 12:52:19 - INFO - __main__ -   Batch Number = 26
01/07/2022 12:52:19 - INFO - __main__ -   Batch Number = 27
01/07/2022 12:52:19 - INFO - __main__ -   Batch Number = 28
01/07/2022 12:52:19 - INFO - __main__ -   Batch Number = 29
01/07/2022 12:52:19 - INFO - __main__ -   Batch Number = 30
Evaluating:  18%|        | 30/163 [00:00<00:03, 37.62it/s]01/07/2022 12:52:19 - INFO - __main__ -   Batch Number = 31
01/07/2022 12:52:19 - INFO - __main__ -   Batch Number = 32
01/07/2022 12:52:19 - INFO - __main__ -   Batch Number = 33
01/07/2022 12:52:19 - INFO - __main__ -   Batch Number = 34
01/07/2022 12:52:19 - INFO - __main__ -   Batch Number = 35
Evaluating:  21%|       | 35/163 [00:00<00:03, 39.64it/s]01/07/2022 12:52:19 - INFO - __main__ -   Batch Number = 36
01/07/2022 12:52:19 - INFO - __main__ -   Batch Number = 37
01/07/2022 12:52:19 - INFO - __main__ -   Batch Number = 38
01/07/2022 12:52:19 - INFO - __main__ -   Batch Number = 39
01/07/2022 12:52:19 - INFO - __main__ -   Batch Number = 40
Evaluating:  25%|       | 40/163 [00:01<00:02, 41.01it/s]01/07/2022 12:52:19 - INFO - __main__ -   Batch Number = 41
01/07/2022 12:52:19 - INFO - __main__ -   Batch Number = 42
01/07/2022 12:52:19 - INFO - __main__ -   Batch Number = 43
01/07/2022 12:52:19 - INFO - __main__ -   Batch Number = 44
01/07/2022 12:52:19 - INFO - __main__ -   Batch Number = 45
Evaluating:  28%|       | 45/163 [00:01<00:02, 42.06it/s]01/07/2022 12:52:19 - INFO - __main__ -   Batch Number = 46
01/07/2022 12:52:19 - INFO - __main__ -   Batch Number = 47
01/07/2022 12:52:19 - INFO - __main__ -   Batch Number = 48
01/07/2022 12:52:19 - INFO - __main__ -   Batch Number = 49
01/07/2022 12:52:19 - INFO - __main__ -   Batch Number = 50
Evaluating:  31%|       | 50/163 [00:01<00:02, 42.60it/s]01/07/2022 12:52:19 - INFO - __main__ -   Batch Number = 51
01/07/2022 12:52:19 - INFO - __main__ -   Batch Number = 52
01/07/2022 12:52:19 - INFO - __main__ -   Batch Number = 53
01/07/2022 12:52:19 - INFO - __main__ -   Batch Number = 54
01/07/2022 12:52:19 - INFO - __main__ -   Batch Number = 55
Evaluating:  34%|      | 55/163 [00:01<00:02, 43.15it/s]01/07/2022 12:52:19 - INFO - __main__ -   Batch Number = 56
01/07/2022 12:52:19 - INFO - __main__ -   Batch Number = 57
01/07/2022 12:52:19 - INFO - __main__ -   Batch Number = 58
01/07/2022 12:52:19 - INFO - __main__ -   Batch Number = 59
01/07/2022 12:52:19 - INFO - __main__ -   Batch Number = 60
Evaluating:  37%|      | 60/163 [00:01<00:02, 43.52it/s]01/07/2022 12:52:19 - INFO - __main__ -   Batch Number = 61
01/07/2022 12:52:19 - INFO - __main__ -   Batch Number = 62
01/07/2022 12:52:19 - INFO - __main__ -   Batch Number = 63
01/07/2022 12:52:19 - INFO - __main__ -   Batch Number = 64
01/07/2022 12:52:20 - INFO - __main__ -   Batch Number = 65
Evaluating:  40%|      | 65/163 [00:01<00:02, 43.66it/s]01/07/2022 12:52:20 - INFO - __main__ -   Batch Number = 66
01/07/2022 12:52:20 - INFO - __main__ -   Batch Number = 67
01/07/2022 12:52:20 - INFO - __main__ -   Batch Number = 68
01/07/2022 12:52:20 - INFO - __main__ -   Batch Number = 69
01/07/2022 12:52:20 - INFO - __main__ -   Batch Number = 70
Evaluating:  43%|     | 70/163 [00:01<00:02, 43.76it/s]01/07/2022 12:52:20 - INFO - __main__ -   Batch Number = 71
01/07/2022 12:52:20 - INFO - __main__ -   Batch Number = 72
01/07/2022 12:52:20 - INFO - __main__ -   Batch Number = 73
01/07/2022 12:52:20 - INFO - __main__ -   Batch Number = 74
01/07/2022 12:52:20 - INFO - __main__ -   Batch Number = 75
Evaluating:  46%|     | 75/163 [00:01<00:02, 41.33it/s]01/07/2022 12:52:20 - INFO - __main__ -   Batch Number = 76
01/07/2022 12:52:20 - INFO - __main__ -   Batch Number = 77
01/07/2022 12:52:20 - INFO - __main__ -   Batch Number = 78
01/07/2022 12:52:20 - INFO - __main__ -   Batch Number = 79
01/07/2022 12:52:20 - INFO - __main__ -   Batch Number = 80
Evaluating:  49%|     | 80/163 [00:02<00:02, 38.53it/s]01/07/2022 12:52:20 - INFO - __main__ -   Batch Number = 81
01/07/2022 12:52:20 - INFO - __main__ -   Batch Number = 82
01/07/2022 12:52:20 - INFO - __main__ -   Batch Number = 83
01/07/2022 12:52:20 - INFO - __main__ -   Batch Number = 84
01/07/2022 12:52:20 - INFO - __main__ -   Batch Number = 85
Evaluating:  52%|    | 85/163 [00:02<00:01, 40.25it/s]01/07/2022 12:52:20 - INFO - __main__ -   Batch Number = 86
01/07/2022 12:52:20 - INFO - __main__ -   Batch Number = 87
01/07/2022 12:52:20 - INFO - __main__ -   Batch Number = 88
01/07/2022 12:52:20 - INFO - __main__ -   Batch Number = 89
01/07/2022 12:52:20 - INFO - __main__ -   Batch Number = 90
Evaluating:  55%|    | 90/163 [00:02<00:01, 41.34it/s]01/07/2022 12:52:20 - INFO - __main__ -   Batch Number = 91
01/07/2022 12:52:20 - INFO - __main__ -   Batch Number = 92
01/07/2022 12:52:20 - INFO - __main__ -   Batch Number = 93
01/07/2022 12:52:20 - INFO - __main__ -   Batch Number = 94
01/07/2022 12:52:20 - INFO - __main__ -   Batch Number = 95
Evaluating:  58%|    | 95/163 [00:02<00:01, 42.15it/s]01/07/2022 12:52:20 - INFO - __main__ -   Batch Number = 96
01/07/2022 12:52:20 - INFO - __main__ -   Batch Number = 97
01/07/2022 12:52:20 - INFO - __main__ -   Batch Number = 98
01/07/2022 12:52:20 - INFO - __main__ -   Batch Number = 99
01/07/2022 12:52:20 - INFO - __main__ -   Batch Number = 100
Evaluating:  61%|   | 100/163 [00:02<00:01, 42.74it/s]01/07/2022 12:52:20 - INFO - __main__ -   Batch Number = 101
01/07/2022 12:52:20 - INFO - __main__ -   Batch Number = 102
01/07/2022 12:52:20 - INFO - __main__ -   Batch Number = 103
01/07/2022 12:52:20 - INFO - __main__ -   Batch Number = 104
01/07/2022 12:52:20 - INFO - __main__ -   Batch Number = 105
Evaluating:  64%|   | 105/163 [00:02<00:01, 43.28it/s]01/07/2022 12:52:20 - INFO - __main__ -   Batch Number = 106
01/07/2022 12:52:21 - INFO - __main__ -   Batch Number = 107
01/07/2022 12:52:21 - INFO - __main__ -   Batch Number = 108
01/07/2022 12:52:21 - INFO - __main__ -   Batch Number = 109
01/07/2022 12:52:21 - INFO - __main__ -   Batch Number = 110
Evaluating:  67%|   | 110/163 [00:02<00:01, 43.36it/s]01/07/2022 12:52:21 - INFO - __main__ -   Batch Number = 111
01/07/2022 12:52:21 - INFO - __main__ -   Batch Number = 112
01/07/2022 12:52:21 - INFO - __main__ -   Batch Number = 113
01/07/2022 12:52:21 - INFO - __main__ -   Batch Number = 114
01/07/2022 12:52:21 - INFO - __main__ -   Batch Number = 115
Evaluating:  71%|   | 115/163 [00:02<00:01, 43.54it/s]01/07/2022 12:52:21 - INFO - __main__ -   Batch Number = 116
01/07/2022 12:52:21 - INFO - __main__ -   Batch Number = 117
01/07/2022 12:52:21 - INFO - __main__ -   Batch Number = 118
01/07/2022 12:52:21 - INFO - __main__ -   Batch Number = 119
01/07/2022 12:52:21 - INFO - __main__ -   Batch Number = 120
Evaluating:  74%|  | 120/163 [00:02<00:00, 43.89it/s]01/07/2022 12:52:21 - INFO - __main__ -   Batch Number = 121
01/07/2022 12:52:21 - INFO - __main__ -   Batch Number = 122
01/07/2022 12:52:21 - INFO - __main__ -   Batch Number = 123
01/07/2022 12:52:21 - INFO - __main__ -   Batch Number = 124
01/07/2022 12:52:21 - INFO - __main__ -   Batch Number = 125
Evaluating:  77%|  | 125/163 [00:03<00:00, 43.90it/s]01/07/2022 12:52:21 - INFO - __main__ -   Batch Number = 126
01/07/2022 12:52:21 - INFO - __main__ -   Batch Number = 127
01/07/2022 12:52:21 - INFO - __main__ -   Batch Number = 128
01/07/2022 12:52:21 - INFO - __main__ -   Batch Number = 129
01/07/2022 12:52:21 - INFO - __main__ -   Batch Number = 130
Evaluating:  80%|  | 130/163 [00:03<00:00, 39.84it/s]01/07/2022 12:52:21 - INFO - __main__ -   Batch Number = 131
01/07/2022 12:52:21 - INFO - __main__ -   Batch Number = 132
01/07/2022 12:52:21 - INFO - __main__ -   Batch Number = 133
01/07/2022 12:52:21 - INFO - __main__ -   Batch Number = 134
01/07/2022 12:52:21 - INFO - __main__ -   Batch Number = 135
Evaluating:  83%| | 135/163 [00:03<00:00, 40.27it/s]01/07/2022 12:52:21 - INFO - __main__ -   Batch Number = 136
01/07/2022 12:52:21 - INFO - __main__ -   Batch Number = 137
01/07/2022 12:52:21 - INFO - __main__ -   Batch Number = 138
01/07/2022 12:52:21 - INFO - __main__ -   Batch Number = 139
01/07/2022 12:52:21 - INFO - __main__ -   Batch Number = 140
Evaluating:  86%| | 140/163 [00:03<00:00, 41.29it/s]01/07/2022 12:52:21 - INFO - __main__ -   Batch Number = 141
01/07/2022 12:52:21 - INFO - __main__ -   Batch Number = 142
01/07/2022 12:52:21 - INFO - __main__ -   Batch Number = 143
01/07/2022 12:52:21 - INFO - __main__ -   Batch Number = 144
01/07/2022 12:52:21 - INFO - __main__ -   Batch Number = 145
Evaluating:  89%| | 145/163 [00:03<00:00, 42.21it/s]01/07/2022 12:52:21 - INFO - __main__ -   Batch Number = 146
01/07/2022 12:52:21 - INFO - __main__ -   Batch Number = 147
01/07/2022 12:52:21 - INFO - __main__ -   Batch Number = 148
01/07/2022 12:52:22 - INFO - __main__ -   Batch Number = 149
01/07/2022 12:52:22 - INFO - __main__ -   Batch Number = 150
Evaluating:  92%|| 150/163 [00:03<00:00, 42.87it/s]01/07/2022 12:52:22 - INFO - __main__ -   Batch Number = 151
01/07/2022 12:52:22 - INFO - __main__ -   Batch Number = 152
01/07/2022 12:52:22 - INFO - __main__ -   Batch Number = 153
01/07/2022 12:52:22 - INFO - __main__ -   Batch Number = 154
01/07/2022 12:52:22 - INFO - __main__ -   Batch Number = 155
Evaluating:  95%|| 155/163 [00:03<00:00, 43.14it/s]01/07/2022 12:52:22 - INFO - __main__ -   Batch Number = 156
01/07/2022 12:52:22 - INFO - __main__ -   Batch Number = 157
01/07/2022 12:52:22 - INFO - __main__ -   Batch Number = 158
01/07/2022 12:52:22 - INFO - __main__ -   Batch Number = 159
01/07/2022 12:52:22 - INFO - __main__ -   Batch Number = 160
Evaluating:  98%|| 160/163 [00:03<00:00, 43.62it/s]01/07/2022 12:52:22 - INFO - __main__ -   Batch Number = 161
01/07/2022 12:52:22 - INFO - __main__ -   Batch Number = 162
01/07/2022 12:52:22 - INFO - __main__ -   Batch Number = 163
Evaluating: 100%|| 163/163 [00:03<00:00, 41.19it/s]
01/07/2022 12:52:22 - INFO - __main__ -     Evaluation done in total 3.957638 secs (0.003037 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/predictions_de_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/nbest_predictions_de_.json
01/07/2022 12:52:25 - INFO - __main__ -   Results = OrderedDict([('exact', 57.47899159663866), ('f1', 74.28634225416451), ('total', 1190), ('HasAns_exact', 57.47899159663866), ('HasAns_f1', 74.28634225416451), ('HasAns_total', 1190), ('best_exact', 57.47899159663866), ('best_exact_thresh', 0.0), ('best_f1', 74.28634225416451), ('best_f1_thresh', 0.0)])
01/07/2022 12:52:25 - INFO - __main__ -   Language adapter for el not found, using en instead
01/07/2022 12:52:25 - INFO - __main__ -   Set active language adapter to en
01/07/2022 12:52:25 - INFO - __main__ -   Args Adapter Weight = None
01/07/2022 12:52:25 - INFO - __main__ -   Adapter Languages = ['en']
01/07/2022 12:52:25 - INFO - __main__ -   Predict File = xquad.el.json
01/07/2022 12:52:25 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
  0%|          | 0/48 [00:00<?, ?it/s] 29%|       | 14/48 [00:00<00:00, 137.41it/s] 58%|    | 28/48 [00:00<00:00, 103.71it/s] 83%| | 40/48 [00:00<00:00, 108.83it/s]100%|| 48/48 [00:00<00:00, 113.58it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<04:45,  4.17it/s]convert squad examples to features:  30%|       | 353/1190 [00:00<00:00, 1116.77it/s]convert squad examples to features:  42%|     | 494/1190 [00:00<00:01, 529.72it/s] convert squad examples to features: 100%|| 1190/1190 [00:00<00:00, 1254.70it/s]/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|| 1190/1190 [00:00<00:00, 541876.21it/s]
01/07/2022 12:52:27 - INFO - __main__ -   Local Rank = -1
01/07/2022 12:52:27 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.el.json_xlm-roberta-base_384_el
01/07/2022 12:52:29 - INFO - __main__ -   ***** Running evaluation  *****
01/07/2022 12:52:29 - INFO - __main__ -     Num examples = 1488
01/07/2022 12:52:29 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/186 [00:00<?, ?it/s]01/07/2022 12:52:29 - INFO - __main__ -   Batch Number = 1
01/07/2022 12:52:29 - INFO - __main__ -   Batch Number = 2
01/07/2022 12:52:29 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|         | 3/186 [00:00<00:06, 29.21it/s]01/07/2022 12:52:29 - INFO - __main__ -   Batch Number = 4
01/07/2022 12:52:29 - INFO - __main__ -   Batch Number = 5
01/07/2022 12:52:29 - INFO - __main__ -   Batch Number = 6
01/07/2022 12:52:29 - INFO - __main__ -   Batch Number = 7
Evaluating:   4%|         | 7/186 [00:00<00:05, 31.99it/s]01/07/2022 12:52:29 - INFO - __main__ -   Batch Number = 8
01/07/2022 12:52:29 - INFO - __main__ -   Batch Number = 9
01/07/2022 12:52:29 - INFO - __main__ -   Batch Number = 10
01/07/2022 12:52:29 - INFO - __main__ -   Batch Number = 11
01/07/2022 12:52:29 - INFO - __main__ -   Batch Number = 12
Evaluating:   6%|         | 12/186 [00:00<00:04, 37.61it/s]01/07/2022 12:52:29 - INFO - __main__ -   Batch Number = 13
01/07/2022 12:52:29 - INFO - __main__ -   Batch Number = 14
01/07/2022 12:52:29 - INFO - __main__ -   Batch Number = 15
01/07/2022 12:52:29 - INFO - __main__ -   Batch Number = 16
01/07/2022 12:52:29 - INFO - __main__ -   Batch Number = 17
Evaluating:   9%|         | 17/186 [00:00<00:04, 39.91it/s]01/07/2022 12:52:29 - INFO - __main__ -   Batch Number = 18
01/07/2022 12:52:29 - INFO - __main__ -   Batch Number = 19
01/07/2022 12:52:29 - INFO - __main__ -   Batch Number = 20
01/07/2022 12:52:29 - INFO - __main__ -   Batch Number = 21
01/07/2022 12:52:29 - INFO - __main__ -   Batch Number = 22
Evaluating:  12%|        | 22/186 [00:00<00:03, 41.42it/s]01/07/2022 12:52:29 - INFO - __main__ -   Batch Number = 23
01/07/2022 12:52:29 - INFO - __main__ -   Batch Number = 24
01/07/2022 12:52:29 - INFO - __main__ -   Batch Number = 25
01/07/2022 12:52:29 - INFO - __main__ -   Batch Number = 26
01/07/2022 12:52:29 - INFO - __main__ -   Batch Number = 27
Evaluating:  15%|        | 27/186 [00:00<00:03, 42.49it/s]01/07/2022 12:52:29 - INFO - __main__ -   Batch Number = 28
01/07/2022 12:52:29 - INFO - __main__ -   Batch Number = 29
01/07/2022 12:52:29 - INFO - __main__ -   Batch Number = 30
01/07/2022 12:52:29 - INFO - __main__ -   Batch Number = 31
01/07/2022 12:52:29 - INFO - __main__ -   Batch Number = 32
Evaluating:  17%|        | 32/186 [00:00<00:03, 42.95it/s]01/07/2022 12:52:29 - INFO - __main__ -   Batch Number = 33
01/07/2022 12:52:29 - INFO - __main__ -   Batch Number = 34
01/07/2022 12:52:29 - INFO - __main__ -   Batch Number = 35
01/07/2022 12:52:29 - INFO - __main__ -   Batch Number = 36
01/07/2022 12:52:29 - INFO - __main__ -   Batch Number = 37
Evaluating:  20%|        | 37/186 [00:00<00:03, 43.41it/s]01/07/2022 12:52:30 - INFO - __main__ -   Batch Number = 38
01/07/2022 12:52:30 - INFO - __main__ -   Batch Number = 39
01/07/2022 12:52:30 - INFO - __main__ -   Batch Number = 40
01/07/2022 12:52:30 - INFO - __main__ -   Batch Number = 41
01/07/2022 12:52:30 - INFO - __main__ -   Batch Number = 42
Evaluating:  23%|       | 42/186 [00:01<00:03, 43.68it/s]01/07/2022 12:52:30 - INFO - __main__ -   Batch Number = 43
01/07/2022 12:52:30 - INFO - __main__ -   Batch Number = 44
01/07/2022 12:52:30 - INFO - __main__ -   Batch Number = 45
01/07/2022 12:52:30 - INFO - __main__ -   Batch Number = 46
01/07/2022 12:52:30 - INFO - __main__ -   Batch Number = 47
Evaluating:  25%|       | 47/186 [00:01<00:03, 43.78it/s]01/07/2022 12:52:30 - INFO - __main__ -   Batch Number = 48
01/07/2022 12:52:30 - INFO - __main__ -   Batch Number = 49
01/07/2022 12:52:30 - INFO - __main__ -   Batch Number = 50
01/07/2022 12:52:30 - INFO - __main__ -   Batch Number = 51
01/07/2022 12:52:30 - INFO - __main__ -   Batch Number = 52
Evaluating:  28%|       | 52/186 [00:01<00:03, 43.93it/s]01/07/2022 12:52:30 - INFO - __main__ -   Batch Number = 53
01/07/2022 12:52:30 - INFO - __main__ -   Batch Number = 54
01/07/2022 12:52:30 - INFO - __main__ -   Batch Number = 55
01/07/2022 12:52:30 - INFO - __main__ -   Batch Number = 56
01/07/2022 12:52:30 - INFO - __main__ -   Batch Number = 57
Evaluating:  31%|       | 57/186 [00:01<00:02, 44.02it/s]01/07/2022 12:52:30 - INFO - __main__ -   Batch Number = 58
01/07/2022 12:52:30 - INFO - __main__ -   Batch Number = 59
01/07/2022 12:52:30 - INFO - __main__ -   Batch Number = 60
01/07/2022 12:52:30 - INFO - __main__ -   Batch Number = 61
01/07/2022 12:52:30 - INFO - __main__ -   Batch Number = 62
Evaluating:  33%|      | 62/186 [00:01<00:02, 43.85it/s]01/07/2022 12:52:30 - INFO - __main__ -   Batch Number = 63
01/07/2022 12:52:30 - INFO - __main__ -   Batch Number = 64
01/07/2022 12:52:30 - INFO - __main__ -   Batch Number = 65
01/07/2022 12:52:30 - INFO - __main__ -   Batch Number = 66
01/07/2022 12:52:30 - INFO - __main__ -   Batch Number = 67
Evaluating:  36%|      | 67/186 [00:01<00:02, 43.71it/s]01/07/2022 12:52:30 - INFO - __main__ -   Batch Number = 68
01/07/2022 12:52:30 - INFO - __main__ -   Batch Number = 69
01/07/2022 12:52:30 - INFO - __main__ -   Batch Number = 70
01/07/2022 12:52:30 - INFO - __main__ -   Batch Number = 71
01/07/2022 12:52:30 - INFO - __main__ -   Batch Number = 72
Evaluating:  39%|      | 72/186 [00:01<00:02, 43.84it/s]01/07/2022 12:52:30 - INFO - __main__ -   Batch Number = 73
01/07/2022 12:52:30 - INFO - __main__ -   Batch Number = 74
01/07/2022 12:52:30 - INFO - __main__ -   Batch Number = 75
01/07/2022 12:52:30 - INFO - __main__ -   Batch Number = 76
01/07/2022 12:52:30 - INFO - __main__ -   Batch Number = 77
Evaluating:  41%|     | 77/186 [00:01<00:02, 43.73it/s]01/07/2022 12:52:30 - INFO - __main__ -   Batch Number = 78
01/07/2022 12:52:30 - INFO - __main__ -   Batch Number = 79
01/07/2022 12:52:30 - INFO - __main__ -   Batch Number = 80
01/07/2022 12:52:30 - INFO - __main__ -   Batch Number = 81
01/07/2022 12:52:31 - INFO - __main__ -   Batch Number = 82
Evaluating:  44%|     | 82/186 [00:01<00:02, 43.90it/s]01/07/2022 12:52:31 - INFO - __main__ -   Batch Number = 83
01/07/2022 12:52:31 - INFO - __main__ -   Batch Number = 84
01/07/2022 12:52:31 - INFO - __main__ -   Batch Number = 85
01/07/2022 12:52:31 - INFO - __main__ -   Batch Number = 86
01/07/2022 12:52:31 - INFO - __main__ -   Batch Number = 87
Evaluating:  47%|     | 87/186 [00:02<00:02, 44.06it/s]01/07/2022 12:52:31 - INFO - __main__ -   Batch Number = 88
01/07/2022 12:52:31 - INFO - __main__ -   Batch Number = 89
01/07/2022 12:52:31 - INFO - __main__ -   Batch Number = 90
01/07/2022 12:52:31 - INFO - __main__ -   Batch Number = 91
01/07/2022 12:52:31 - INFO - __main__ -   Batch Number = 92
Evaluating:  49%|     | 92/186 [00:02<00:02, 43.87it/s]01/07/2022 12:52:31 - INFO - __main__ -   Batch Number = 93
01/07/2022 12:52:31 - INFO - __main__ -   Batch Number = 94
01/07/2022 12:52:31 - INFO - __main__ -   Batch Number = 95
01/07/2022 12:52:31 - INFO - __main__ -   Batch Number = 96
01/07/2022 12:52:31 - INFO - __main__ -   Batch Number = 97
Evaluating:  52%|    | 97/186 [00:02<00:02, 44.08it/s]01/07/2022 12:52:31 - INFO - __main__ -   Batch Number = 98
01/07/2022 12:52:31 - INFO - __main__ -   Batch Number = 99
01/07/2022 12:52:31 - INFO - __main__ -   Batch Number = 100
01/07/2022 12:52:31 - INFO - __main__ -   Batch Number = 101
01/07/2022 12:52:31 - INFO - __main__ -   Batch Number = 102
Evaluating:  55%|    | 102/186 [00:02<00:01, 43.98it/s]01/07/2022 12:52:31 - INFO - __main__ -   Batch Number = 103
01/07/2022 12:52:31 - INFO - __main__ -   Batch Number = 104
01/07/2022 12:52:31 - INFO - __main__ -   Batch Number = 105
01/07/2022 12:52:31 - INFO - __main__ -   Batch Number = 106
01/07/2022 12:52:31 - INFO - __main__ -   Batch Number = 107
Evaluating:  58%|    | 107/186 [00:02<00:01, 43.76it/s]01/07/2022 12:52:31 - INFO - __main__ -   Batch Number = 108
01/07/2022 12:52:31 - INFO - __main__ -   Batch Number = 109
01/07/2022 12:52:31 - INFO - __main__ -   Batch Number = 110
01/07/2022 12:52:31 - INFO - __main__ -   Batch Number = 111
01/07/2022 12:52:31 - INFO - __main__ -   Batch Number = 112
Evaluating:  60%|    | 112/186 [00:02<00:01, 43.88it/s]01/07/2022 12:52:31 - INFO - __main__ -   Batch Number = 113
01/07/2022 12:52:31 - INFO - __main__ -   Batch Number = 114
01/07/2022 12:52:31 - INFO - __main__ -   Batch Number = 115
01/07/2022 12:52:31 - INFO - __main__ -   Batch Number = 116
01/07/2022 12:52:31 - INFO - __main__ -   Batch Number = 117
Evaluating:  63%|   | 117/186 [00:02<00:01, 43.90it/s]01/07/2022 12:52:31 - INFO - __main__ -   Batch Number = 118
01/07/2022 12:52:31 - INFO - __main__ -   Batch Number = 119
01/07/2022 12:52:31 - INFO - __main__ -   Batch Number = 120
01/07/2022 12:52:31 - INFO - __main__ -   Batch Number = 121
01/07/2022 12:52:31 - INFO - __main__ -   Batch Number = 122
Evaluating:  66%|   | 122/186 [00:02<00:01, 42.11it/s]01/07/2022 12:52:31 - INFO - __main__ -   Batch Number = 123
01/07/2022 12:52:31 - INFO - __main__ -   Batch Number = 124
01/07/2022 12:52:31 - INFO - __main__ -   Batch Number = 125
01/07/2022 12:52:32 - INFO - __main__ -   Batch Number = 126
01/07/2022 12:52:32 - INFO - __main__ -   Batch Number = 127
Evaluating:  68%|   | 127/186 [00:02<00:01, 42.71it/s]01/07/2022 12:52:32 - INFO - __main__ -   Batch Number = 128
01/07/2022 12:52:32 - INFO - __main__ -   Batch Number = 129
01/07/2022 12:52:32 - INFO - __main__ -   Batch Number = 130
01/07/2022 12:52:32 - INFO - __main__ -   Batch Number = 131
01/07/2022 12:52:32 - INFO - __main__ -   Batch Number = 132
Evaluating:  71%|   | 132/186 [00:03<00:01, 43.11it/s]01/07/2022 12:52:32 - INFO - __main__ -   Batch Number = 133
01/07/2022 12:52:32 - INFO - __main__ -   Batch Number = 134
01/07/2022 12:52:32 - INFO - __main__ -   Batch Number = 135
01/07/2022 12:52:32 - INFO - __main__ -   Batch Number = 136
01/07/2022 12:52:32 - INFO - __main__ -   Batch Number = 137
Evaluating:  74%|  | 137/186 [00:03<00:01, 43.31it/s]01/07/2022 12:52:32 - INFO - __main__ -   Batch Number = 138
01/07/2022 12:52:32 - INFO - __main__ -   Batch Number = 139
01/07/2022 12:52:32 - INFO - __main__ -   Batch Number = 140
01/07/2022 12:52:32 - INFO - __main__ -   Batch Number = 141
01/07/2022 12:52:32 - INFO - __main__ -   Batch Number = 142
Evaluating:  76%|  | 142/186 [00:03<00:01, 43.45it/s]01/07/2022 12:52:32 - INFO - __main__ -   Batch Number = 143
01/07/2022 12:52:32 - INFO - __main__ -   Batch Number = 144
01/07/2022 12:52:32 - INFO - __main__ -   Batch Number = 145
01/07/2022 12:52:32 - INFO - __main__ -   Batch Number = 146
01/07/2022 12:52:32 - INFO - __main__ -   Batch Number = 147
Evaluating:  79%|  | 147/186 [00:03<00:00, 43.76it/s]01/07/2022 12:52:32 - INFO - __main__ -   Batch Number = 148
01/07/2022 12:52:32 - INFO - __main__ -   Batch Number = 149
01/07/2022 12:52:32 - INFO - __main__ -   Batch Number = 150
01/07/2022 12:52:32 - INFO - __main__ -   Batch Number = 151
01/07/2022 12:52:32 - INFO - __main__ -   Batch Number = 152
Evaluating:  82%| | 152/186 [00:03<00:00, 43.43it/s]01/07/2022 12:52:32 - INFO - __main__ -   Batch Number = 153
01/07/2022 12:52:32 - INFO - __main__ -   Batch Number = 154
01/07/2022 12:52:32 - INFO - __main__ -   Batch Number = 155
01/07/2022 12:52:32 - INFO - __main__ -   Batch Number = 156
01/07/2022 12:52:32 - INFO - __main__ -   Batch Number = 157
Evaluating:  84%| | 157/186 [00:03<00:00, 43.51it/s]01/07/2022 12:52:32 - INFO - __main__ -   Batch Number = 158
01/07/2022 12:52:32 - INFO - __main__ -   Batch Number = 159
01/07/2022 12:52:32 - INFO - __main__ -   Batch Number = 160
01/07/2022 12:52:32 - INFO - __main__ -   Batch Number = 161
01/07/2022 12:52:32 - INFO - __main__ -   Batch Number = 162
Evaluating:  87%| | 162/186 [00:03<00:00, 43.72it/s]01/07/2022 12:52:32 - INFO - __main__ -   Batch Number = 163
01/07/2022 12:52:32 - INFO - __main__ -   Batch Number = 164
01/07/2022 12:52:32 - INFO - __main__ -   Batch Number = 165
01/07/2022 12:52:32 - INFO - __main__ -   Batch Number = 166
01/07/2022 12:52:32 - INFO - __main__ -   Batch Number = 167
Evaluating:  90%| | 167/186 [00:03<00:00, 43.76it/s]01/07/2022 12:52:32 - INFO - __main__ -   Batch Number = 168
01/07/2022 12:52:33 - INFO - __main__ -   Batch Number = 169
01/07/2022 12:52:33 - INFO - __main__ -   Batch Number = 170
01/07/2022 12:52:33 - INFO - __main__ -   Batch Number = 171
01/07/2022 12:52:33 - INFO - __main__ -   Batch Number = 172
Evaluating:  92%|| 172/186 [00:03<00:00, 43.96it/s]01/07/2022 12:52:33 - INFO - __main__ -   Batch Number = 173
01/07/2022 12:52:33 - INFO - __main__ -   Batch Number = 174
01/07/2022 12:52:33 - INFO - __main__ -   Batch Number = 175
01/07/2022 12:52:33 - INFO - __main__ -   Batch Number = 176
01/07/2022 12:52:33 - INFO - __main__ -   Batch Number = 177
Evaluating:  95%|| 177/186 [00:04<00:00, 44.00it/s]01/07/2022 12:52:33 - INFO - __main__ -   Batch Number = 178
01/07/2022 12:52:33 - INFO - __main__ -   Batch Number = 179
01/07/2022 12:52:33 - INFO - __main__ -   Batch Number = 180
01/07/2022 12:52:33 - INFO - __main__ -   Batch Number = 181
01/07/2022 12:52:33 - INFO - __main__ -   Batch Number = 182
Evaluating:  98%|| 182/186 [00:04<00:00, 43.98it/s]01/07/2022 12:52:33 - INFO - __main__ -   Batch Number = 183
01/07/2022 12:52:33 - INFO - __main__ -   Batch Number = 184
01/07/2022 12:52:33 - INFO - __main__ -   Batch Number = 185
01/07/2022 12:52:33 - INFO - __main__ -   Batch Number = 186
Evaluating: 100%|| 186/186 [00:04<00:00, 43.16it/s]
01/07/2022 12:52:33 - INFO - __main__ -     Evaluation done in total 4.309712 secs (0.002896 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/predictions_el_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/nbest_predictions_el_.json
01/07/2022 12:52:36 - INFO - __main__ -   Results = OrderedDict([('exact', 51.680672268907564), ('f1', 69.15111931083828), ('total', 1190), ('HasAns_exact', 51.680672268907564), ('HasAns_f1', 69.15111931083828), ('HasAns_total', 1190), ('best_exact', 51.680672268907564), ('best_exact_thresh', 0.0), ('best_f1', 69.15111931083828), ('best_f1_thresh', 0.0)])
01/07/2022 12:52:36 - INFO - __main__ -   Language adapter for es not found, using en instead
01/07/2022 12:52:36 - INFO - __main__ -   Set active language adapter to en
01/07/2022 12:52:36 - INFO - __main__ -   Args Adapter Weight = None
01/07/2022 12:52:36 - INFO - __main__ -   Adapter Languages = ['en']
01/07/2022 12:52:36 - INFO - __main__ -   Predict File = xquad.es.json
01/07/2022 12:52:36 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
  0%|          | 0/48 [00:00<?, ?it/s] 33%|      | 16/48 [00:00<00:00, 123.21it/s] 60%|    | 29/48 [00:00<00:00, 116.73it/s] 92%|| 44/48 [00:00<00:00, 129.01it/s]100%|| 48/48 [00:00<00:00, 128.01it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<06:29,  3.06it/s]convert squad examples to features:  32%|      | 385/1190 [00:00<00:01, 639.97it/s]convert squad examples to features:  57%|    | 673/1190 [00:00<00:00, 1027.92it/s]convert squad examples to features:  89%| | 1057/1190 [00:00<00:00, 1612.99it/s]convert squad examples to features: 100%|| 1190/1190 [00:01<00:00, 1120.95it/s]/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='lon
add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|| 1190/1190 [00:00<00:00, 675219.39it/s]
01/07/2022 12:52:38 - INFO - __main__ -   Local Rank = -1
01/07/2022 12:52:38 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.es.json_xlm-roberta-base_384_es
01/07/2022 12:52:39 - INFO - __main__ -   ***** Running evaluation  *****
01/07/2022 12:52:39 - INFO - __main__ -     Num examples = 1304
01/07/2022 12:52:39 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/163 [00:00<?, ?it/s]01/07/2022 12:52:39 - INFO - __main__ -   Batch Number = 1
01/07/2022 12:52:39 - INFO - __main__ -   Batch Number = 2
01/07/2022 12:52:39 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|         | 3/163 [00:00<00:05, 29.40it/s]01/07/2022 12:52:40 - INFO - __main__ -   Batch Number = 4
01/07/2022 12:52:40 - INFO - __main__ -   Batch Number = 5
01/07/2022 12:52:40 - INFO - __main__ -   Batch Number = 6
Evaluating:   4%|         | 6/163 [00:00<00:05, 29.62it/s]01/07/2022 12:52:40 - INFO - __main__ -   Batch Number = 7
01/07/2022 12:52:40 - INFO - __main__ -   Batch Number = 8
01/07/2022 12:52:40 - INFO - __main__ -   Batch Number = 9
01/07/2022 12:52:40 - INFO - __main__ -   Batch Number = 10
Evaluating:   6%|         | 10/163 [00:00<00:04, 32.22it/s]01/07/2022 12:52:40 - INFO - __main__ -   Batch Number = 11
01/07/2022 12:52:40 - INFO - __main__ -   Batch Number = 12
01/07/2022 12:52:40 - INFO - __main__ -   Batch Number = 13
01/07/2022 12:52:40 - INFO - __main__ -   Batch Number = 14
01/07/2022 12:52:40 - INFO - __main__ -   Batch Number = 15
Evaluating:   9%|         | 15/163 [00:00<00:03, 37.19it/s]01/07/2022 12:52:40 - INFO - __main__ -   Batch Number = 16
01/07/2022 12:52:40 - INFO - __main__ -   Batch Number = 17
01/07/2022 12:52:40 - INFO - __main__ -   Batch Number = 18
01/07/2022 12:52:40 - INFO - __main__ -   Batch Number = 19
01/07/2022 12:52:40 - INFO - __main__ -   Batch Number = 20
Evaluating:  12%|        | 20/163 [00:00<00:03, 39.69it/s]01/07/2022 12:52:40 - INFO - __main__ -   Batch Number = 21
01/07/2022 12:52:40 - INFO - __main__ -   Batch Number = 22
01/07/2022 12:52:40 - INFO - __main__ -   Batch Number = 23
01/07/2022 12:52:40 - INFO - __main__ -   Batch Number = 24
01/07/2022 12:52:40 - INFO - __main__ -   Batch Number = 25
Evaluating:  15%|        | 25/163 [00:00<00:03, 41.30it/s]01/07/2022 12:52:40 - INFO - __main__ -   Batch Number = 26
01/07/2022 12:52:40 - INFO - __main__ -   Batch Number = 27
01/07/2022 12:52:40 - INFO - __main__ -   Batch Number = 28
01/07/2022 12:52:40 - INFO - __main__ -   Batch Number = 29
01/07/2022 12:52:40 - INFO - __main__ -   Batch Number = 30
Evaluating:  18%|        | 30/163 [00:00<00:03, 42.35it/s]01/07/2022 12:52:40 - INFO - __main__ -   Batch Number = 31
01/07/2022 12:52:40 - INFO - __main__ -   Batch Number = 32
01/07/2022 12:52:40 - INFO - __main__ -   Batch Number = 33
01/07/2022 12:52:40 - INFO - __main__ -   Batch Number = 34
01/07/2022 12:52:40 - INFO - __main__ -   Batch Number = 35
Evaluating:  21%|       | 35/163 [00:00<00:02, 42.76it/s]01/07/2022 12:52:40 - INFO - __main__ -   Batch Number = 36
01/07/2022 12:52:40 - INFO - __main__ -   Batch Number = 37
01/07/2022 12:52:40 - INFO - __main__ -   Batch Number = 38
01/07/2022 12:52:40 - INFO - __main__ -   Batch Number = 39
01/07/2022 12:52:40 - INFO - __main__ -   Batch Number = 40
Evaluating:  25%|       | 40/163 [00:00<00:02, 43.15it/s]01/07/2022 12:52:40 - INFO - __main__ -   Batch Number = 41
01/07/2022 12:52:40 - INFO - __main__ -   Batch Number = 42
01/07/2022 12:52:40 - INFO - __main__ -   Batch Number = 43
01/07/2022 12:52:40 - INFO - __main__ -   Batch Number = 44
01/07/2022 12:52:41 - INFO - __main__ -   Batch Number = 45
Evaluating:  28%|       | 45/163 [00:01<00:02, 43.55it/s]01/07/2022 12:52:41 - INFO - __main__ -   Batch Number = 46
01/07/2022 12:52:41 - INFO - __main__ -   Batch Number = 47
01/07/2022 12:52:41 - INFO - __main__ -   Batch Number = 48
01/07/2022 12:52:41 - INFO - __main__ -   Batch Number = 49
01/07/2022 12:52:41 - INFO - __main__ -   Batch Number = 50
Evaluating:  31%|       | 50/163 [00:01<00:02, 43.59it/s]01/07/2022 12:52:41 - INFO - __main__ -   Batch Number = 51
01/07/2022 12:52:41 - INFO - __main__ -   Batch Number = 52
01/07/2022 12:52:41 - INFO - __main__ -   Batch Number = 53
01/07/2022 12:52:41 - INFO - __main__ -   Batch Number = 54
01/07/2022 12:52:41 - INFO - __main__ -   Batch Number = 55
Evaluating:  34%|      | 55/163 [00:01<00:02, 43.81it/s]01/07/2022 12:52:41 - INFO - __main__ -   Batch Number = 56
01/07/2022 12:52:41 - INFO - __main__ -   Batch Number = 57
01/07/2022 12:52:41 - INFO - __main__ -   Batch Number = 58
01/07/2022 12:52:41 - INFO - __main__ -   Batch Number = 59
01/07/2022 12:52:41 - INFO - __main__ -   Batch Number = 60
Evaluating:  37%|      | 60/163 [00:01<00:02, 43.83it/s]01/07/2022 12:52:41 - INFO - __main__ -   Batch Number = 61
01/07/2022 12:52:41 - INFO - __main__ -   Batch Number = 62
01/07/2022 12:52:41 - INFO - __main__ -   Batch Number = 63
01/07/2022 12:52:41 - INFO - __main__ -   Batch Number = 64
01/07/2022 12:52:41 - INFO - __main__ -   Batch Number = 65
Evaluating:  40%|      | 65/163 [00:01<00:02, 43.74it/s]01/07/2022 12:52:41 - INFO - __main__ -   Batch Number = 66
01/07/2022 12:52:41 - INFO - __main__ -   Batch Number = 67
01/07/2022 12:52:41 - INFO - __main__ -   Batch Number = 68
01/07/2022 12:52:41 - INFO - __main__ -   Batch Number = 69
01/07/2022 12:52:41 - INFO - __main__ -   Batch Number = 70
Evaluating:  43%|     | 70/163 [00:01<00:02, 43.91it/s]01/07/2022 12:52:41 - INFO - __main__ -   Batch Number = 71
01/07/2022 12:52:41 - INFO - __main__ -   Batch Number = 72
01/07/2022 12:52:41 - INFO - __main__ -   Batch Number = 73
01/07/2022 12:52:41 - INFO - __main__ -   Batch Number = 74
01/07/2022 12:52:41 - INFO - __main__ -   Batch Number = 75
Evaluating:  46%|     | 75/163 [00:01<00:02, 43.98it/s]01/07/2022 12:52:41 - INFO - __main__ -   Batch Number = 76
01/07/2022 12:52:41 - INFO - __main__ -   Batch Number = 77
01/07/2022 12:52:41 - INFO - __main__ -   Batch Number = 78
01/07/2022 12:52:41 - INFO - __main__ -   Batch Number = 79
01/07/2022 12:52:41 - INFO - __main__ -   Batch Number = 80
Evaluating:  49%|     | 80/163 [00:01<00:01, 43.77it/s]01/07/2022 12:52:41 - INFO - __main__ -   Batch Number = 81
01/07/2022 12:52:41 - INFO - __main__ -   Batch Number = 82
01/07/2022 12:52:41 - INFO - __main__ -   Batch Number = 83
01/07/2022 12:52:41 - INFO - __main__ -   Batch Number = 84
01/07/2022 12:52:41 - INFO - __main__ -   Batch Number = 85
Evaluating:  52%|    | 85/163 [00:02<00:01, 43.89it/s]01/07/2022 12:52:41 - INFO - __main__ -   Batch Number = 86
01/07/2022 12:52:41 - INFO - __main__ -   Batch Number = 87
01/07/2022 12:52:41 - INFO - __main__ -   Batch Number = 88
01/07/2022 12:52:42 - INFO - __main__ -   Batch Number = 89
01/07/2022 12:52:42 - INFO - __main__ -   Batch Number = 90
Evaluating:  55%|    | 90/163 [00:02<00:01, 43.93it/s]01/07/2022 12:52:42 - INFO - __main__ -   Batch Number = 91
01/07/2022 12:52:42 - INFO - __main__ -   Batch Number = 92
01/07/2022 12:52:42 - INFO - __main__ -   Batch Number = 93
01/07/2022 12:52:42 - INFO - __main__ -   Batch Number = 94
01/07/2022 12:52:42 - INFO - __main__ -   Batch Number = 95
Evaluating:  58%|    | 95/163 [00:02<00:01, 42.15it/s]01/07/2022 12:52:42 - INFO - __main__ -   Batch Number = 96
01/07/2022 12:52:42 - INFO - __main__ -   Batch Number = 97
01/07/2022 12:52:42 - INFO - __main__ -   Batch Number = 98
01/07/2022 12:52:42 - INFO - __main__ -   Batch Number = 99
01/07/2022 12:52:42 - INFO - __main__ -   Batch Number = 100
Evaluating:  61%|   | 100/163 [00:02<00:01, 42.63it/s]01/07/2022 12:52:42 - INFO - __main__ -   Batch Number = 101
01/07/2022 12:52:42 - INFO - __main__ -   Batch Number = 102
01/07/2022 12:52:42 - INFO - __main__ -   Batch Number = 103
01/07/2022 12:52:42 - INFO - __main__ -   Batch Number = 104
01/07/2022 12:52:42 - INFO - __main__ -   Batch Number = 105
Evaluating:  64%|   | 105/163 [00:02<00:01, 43.10it/s]01/07/2022 12:52:42 - INFO - __main__ -   Batch Number = 106
01/07/2022 12:52:42 - INFO - __main__ -   Batch Number = 107
01/07/2022 12:52:42 - INFO - __main__ -   Batch Number = 108
01/07/2022 12:52:42 - INFO - __main__ -   Batch Number = 109
01/07/2022 12:52:42 - INFO - __main__ -   Batch Number = 110
Evaluating:  67%|   | 110/163 [00:02<00:01, 43.25it/s]01/07/2022 12:52:42 - INFO - __main__ -   Batch Number = 111
01/07/2022 12:52:42 - INFO - __main__ -   Batch Number = 112
01/07/2022 12:52:42 - INFO - __main__ -   Batch Number = 113
01/07/2022 12:52:42 - INFO - __main__ -   Batch Number = 114
01/07/2022 12:52:42 - INFO - __main__ -   Batch Number = 115
Evaluating:  71%|   | 115/163 [00:02<00:01, 43.62it/s]01/07/2022 12:52:42 - INFO - __main__ -   Batch Number = 116
01/07/2022 12:52:42 - INFO - __main__ -   Batch Number = 117
01/07/2022 12:52:42 - INFO - __main__ -   Batch Number = 118
01/07/2022 12:52:42 - INFO - __main__ -   Batch Number = 119
01/07/2022 12:52:42 - INFO - __main__ -   Batch Number = 120
Evaluating:  74%|  | 120/163 [00:02<00:00, 43.83it/s]01/07/2022 12:52:42 - INFO - __main__ -   Batch Number = 121
01/07/2022 12:52:42 - INFO - __main__ -   Batch Number = 122
01/07/2022 12:52:42 - INFO - __main__ -   Batch Number = 123
01/07/2022 12:52:42 - INFO - __main__ -   Batch Number = 124
01/07/2022 12:52:42 - INFO - __main__ -   Batch Number = 125
Evaluating:  77%|  | 125/163 [00:02<00:00, 43.63it/s]01/07/2022 12:52:42 - INFO - __main__ -   Batch Number = 126
01/07/2022 12:52:42 - INFO - __main__ -   Batch Number = 127
01/07/2022 12:52:42 - INFO - __main__ -   Batch Number = 128
01/07/2022 12:52:42 - INFO - __main__ -   Batch Number = 129
01/07/2022 12:52:42 - INFO - __main__ -   Batch Number = 130
Evaluating:  80%|  | 130/163 [00:03<00:00, 43.83it/s]01/07/2022 12:52:42 - INFO - __main__ -   Batch Number = 131
01/07/2022 12:52:42 - INFO - __main__ -   Batch Number = 132
01/07/2022 12:52:43 - INFO - __main__ -   Batch Number = 133
01/07/2022 12:52:43 - INFO - __main__ -   Batch Number = 134
01/07/2022 12:52:43 - INFO - __main__ -   Batch Number = 135
Evaluating:  83%| | 135/163 [00:03<00:00, 44.00it/s]01/07/2022 12:52:43 - INFO - __main__ -   Batch Number = 136
01/07/2022 12:52:43 - INFO - __main__ -   Batch Number = 137
01/07/2022 12:52:43 - INFO - __main__ -   Batch Number = 138
01/07/2022 12:52:43 - INFO - __main__ -   Batch Number = 139
01/07/2022 12:52:43 - INFO - __main__ -   Batch Number = 140
Evaluating:  86%| | 140/163 [00:03<00:00, 43.91it/s]01/07/2022 12:52:43 - INFO - __main__ -   Batch Number = 141
01/07/2022 12:52:43 - INFO - __main__ -   Batch Number = 142
01/07/2022 12:52:43 - INFO - __main__ -   Batch Number = 143
01/07/2022 12:52:43 - INFO - __main__ -   Batch Number = 144
01/07/2022 12:52:43 - INFO - __main__ -   Batch Number = 145
Evaluating:  89%| | 145/163 [00:03<00:00, 44.01it/s]01/07/2022 12:52:43 - INFO - __main__ -   Batch Number = 146
01/07/2022 12:52:43 - INFO - __main__ -   Batch Number = 147
01/07/2022 12:52:43 - INFO - __main__ -   Batch Number = 148
01/07/2022 12:52:43 - INFO - __main__ -   Batch Number = 149
01/07/2022 12:52:43 - INFO - __main__ -   Batch Number = 150
Evaluating:  92%|| 150/163 [00:03<00:00, 44.01it/s]01/07/2022 12:52:43 - INFO - __main__ -   Batch Number = 151
01/07/2022 12:52:43 - INFO - __main__ -   Batch Number = 152
01/07/2022 12:52:43 - INFO - __main__ -   Batch Number = 153
01/07/2022 12:52:43 - INFO - __main__ -   Batch Number = 154
01/07/2022 12:52:43 - INFO - __main__ -   Batch Number = 155
Evaluating:  95%|| 155/163 [00:03<00:00, 43.90it/s]01/07/2022 12:52:43 - INFO - __main__ -   Batch Number = 156
01/07/2022 12:52:43 - INFO - __main__ -   Batch Number = 157
01/07/2022 12:52:43 - INFO - __main__ -   Batch Number = 158
01/07/2022 12:52:43 - INFO - __main__ -   Batch Number = 159
01/07/2022 12:52:43 - INFO - __main__ -   Batch Number = 160
Evaluating:  98%|| 160/163 [00:03<00:00, 43.89it/s]01/07/2022 12:52:43 - INFO - __main__ -   Batch Number = 161
01/07/2022 12:52:43 - INFO - __main__ -   Batch Number = 162
01/07/2022 12:52:43 - INFO - __main__ -   Batch Number = 163
Evaluating: 100%|| 163/163 [00:03<00:00, 42.77it/s]
01/07/2022 12:52:43 - INFO - __main__ -     Evaluation done in total 3.811196 secs (0.002923 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/predictions_es_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/nbest_predictions_es_.json
01/07/2022 12:52:46 - INFO - __main__ -   Results = OrderedDict([('exact', 58.0672268907563), ('f1', 76.21990242313481), ('total', 1190), ('HasAns_exact', 58.0672268907563), ('HasAns_f1', 76.21990242313481), ('HasAns_total', 1190), ('best_exact', 58.0672268907563), ('best_exact_thresh', 0.0), ('best_f1', 76.21990242313481), ('best_f1_thresh', 0.0)])
01/07/2022 12:52:46 - INFO - __main__ -   Language adapter for hi not found, using en instead
01/07/2022 12:52:46 - INFO - __main__ -   Set active language adapter to en
01/07/2022 12:52:46 - INFO - __main__ -   Args Adapter Weight = None
01/07/2022 12:52:47 - INFO - __main__ -   Adapter Languages = ['en']
01/07/2022 12:52:47 - INFO - __main__ -   Predict File = xquad.hi.json
01/07/2022 12:52:47 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
  0%|          | 0/48 [00:00<?, ?it/s] 33%|      | 16/48 [00:00<00:00, 133.33it/s] 62%|   | 30/48 [00:00<00:00, 127.15it/s] 92%|| 44/48 [00:00<00:00, 132.45it/s]100%|| 48/48 [00:00<00:00, 133.25it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<04:39,  4.25it/s]convert squad examples to features:  27%|       | 321/1190 [00:00<00:00, 1194.02it/s]convert squad examples to features:  42%|     | 503/1190 [00:00<00:00, 698.38it/s] convert squad examples to features:  75%|  | 897/1190 [00:00<00:00, 1295.36it/s]convert squad examples to features:  97%|| 1153/1190 [00:00<00:00, 1525.84it/s]convert squad examples to features: 100%|| 1190/1190 [00:00<00:00, 1236.61it/s]/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|| 1190/1190 [00:00<00:00, 517117.88it/s]
01/07/2022 12:52:49 - INFO - __main__ -   Local Rank = -1
01/07/2022 12:52:49 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.hi.json_xlm-roberta-base_384_hi
01/07/2022 12:52:50 - INFO - __main__ -   ***** Running evaluation  *****
01/07/2022 12:52:50 - INFO - __main__ -     Num examples = 1382
01/07/2022 12:52:50 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/173 [00:00<?, ?it/s]01/07/2022 12:52:50 - INFO - __main__ -   Batch Number = 1
01/07/2022 12:52:50 - INFO - __main__ -   Batch Number = 2
01/07/2022 12:52:50 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|         | 3/173 [00:00<00:05, 29.05it/s]01/07/2022 12:52:50 - INFO - __main__ -   Batch Number = 4
01/07/2022 12:52:50 - INFO - __main__ -   Batch Number = 5
01/07/2022 12:52:50 - INFO - __main__ -   Batch Number = 6
01/07/2022 12:52:50 - INFO - __main__ -   Batch Number = 7
Evaluating:   4%|         | 7/173 [00:00<00:05, 29.68it/s]01/07/2022 12:52:50 - INFO - __main__ -   Batch Number = 8
01/07/2022 12:52:50 - INFO - __main__ -   Batch Number = 9
01/07/2022 12:52:50 - INFO - __main__ -   Batch Number = 10
Evaluating:   6%|         | 10/173 [00:00<00:05, 28.09it/s]01/07/2022 12:52:50 - INFO - __main__ -   Batch Number = 11
01/07/2022 12:52:50 - INFO - __main__ -   Batch Number = 12
01/07/2022 12:52:50 - INFO - __main__ -   Batch Number = 13
01/07/2022 12:52:50 - INFO - __main__ -   Batch Number = 14
01/07/2022 12:52:50 - INFO - __main__ -   Batch Number = 15
Evaluating:   9%|         | 15/173 [00:00<00:04, 33.98it/s]01/07/2022 12:52:50 - INFO - __main__ -   Batch Number = 16
01/07/2022 12:52:50 - INFO - __main__ -   Batch Number = 17
01/07/2022 12:52:50 - INFO - __main__ -   Batch Number = 18
01/07/2022 12:52:50 - INFO - __main__ -   Batch Number = 19
01/07/2022 12:52:50 - INFO - __main__ -   Batch Number = 20
Evaluating:  12%|        | 20/173 [00:00<00:04, 37.45it/s]01/07/2022 12:52:50 - INFO - __main__ -   Batch Number = 21
01/07/2022 12:52:50 - INFO - __main__ -   Batch Number = 22
01/07/2022 12:52:50 - INFO - __main__ -   Batch Number = 23
01/07/2022 12:52:50 - INFO - __main__ -   Batch Number = 24
01/07/2022 12:52:50 - INFO - __main__ -   Batch Number = 25
Evaluating:  14%|        | 25/173 [00:00<00:03, 39.86it/s]01/07/2022 12:52:51 - INFO - __main__ -   Batch Number = 26
01/07/2022 12:52:51 - INFO - __main__ -   Batch Number = 27
01/07/2022 12:52:51 - INFO - __main__ -   Batch Number = 28
01/07/2022 12:52:51 - INFO - __main__ -   Batch Number = 29
01/07/2022 12:52:51 - INFO - __main__ -   Batch Number = 30
Evaluating:  17%|        | 30/173 [00:00<00:03, 41.50it/s]01/07/2022 12:52:51 - INFO - __main__ -   Batch Number = 31
01/07/2022 12:52:51 - INFO - __main__ -   Batch Number = 32
01/07/2022 12:52:51 - INFO - __main__ -   Batch Number = 33
01/07/2022 12:52:51 - INFO - __main__ -   Batch Number = 34
01/07/2022 12:52:51 - INFO - __main__ -   Batch Number = 35
Evaluating:  20%|        | 35/173 [00:00<00:03, 42.34it/s]01/07/2022 12:52:51 - INFO - __main__ -   Batch Number = 36
01/07/2022 12:52:51 - INFO - __main__ -   Batch Number = 37
01/07/2022 12:52:51 - INFO - __main__ -   Batch Number = 38
01/07/2022 12:52:51 - INFO - __main__ -   Batch Number = 39
01/07/2022 12:52:51 - INFO - __main__ -   Batch Number = 40
Evaluating:  23%|       | 40/173 [00:01<00:03, 43.20it/s]01/07/2022 12:52:51 - INFO - __main__ -   Batch Number = 41
01/07/2022 12:52:51 - INFO - __main__ -   Batch Number = 42
01/07/2022 12:52:51 - INFO - __main__ -   Batch Number = 43
01/07/2022 12:52:51 - INFO - __main__ -   Batch Number = 44
01/07/2022 12:52:51 - INFO - __main__ -   Batch Number = 45
Evaluating:  26%|       | 45/173 [00:01<00:03, 41.78it/s]01/07/2022 12:52:51 - INFO - __main__ -   Batch Number = 46
01/07/2022 12:52:51 - INFO - __main__ -   Batch Number = 47
01/07/2022 12:52:51 - INFO - __main__ -   Batch Number = 48
01/07/2022 12:52:51 - INFO - __main__ -   Batch Number = 49
01/07/2022 12:52:51 - INFO - __main__ -   Batch Number = 50
Evaluating:  29%|       | 50/173 [00:01<00:03, 39.79it/s]01/07/2022 12:52:51 - INFO - __main__ -   Batch Number = 51
01/07/2022 12:52:51 - INFO - __main__ -   Batch Number = 52
01/07/2022 12:52:51 - INFO - __main__ -   Batch Number = 53
01/07/2022 12:52:51 - INFO - __main__ -   Batch Number = 54
01/07/2022 12:52:51 - INFO - __main__ -   Batch Number = 55
Evaluating:  32%|      | 55/173 [00:01<00:02, 39.87it/s]01/07/2022 12:52:51 - INFO - __main__ -   Batch Number = 56
01/07/2022 12:52:51 - INFO - __main__ -   Batch Number = 57
01/07/2022 12:52:51 - INFO - __main__ -   Batch Number = 58
01/07/2022 12:52:51 - INFO - __main__ -   Batch Number = 59
01/07/2022 12:52:51 - INFO - __main__ -   Batch Number = 60
Evaluating:  35%|      | 60/173 [00:01<00:02, 40.32it/s]01/07/2022 12:52:51 - INFO - __main__ -   Batch Number = 61
01/07/2022 12:52:51 - INFO - __main__ -   Batch Number = 62
01/07/2022 12:52:51 - INFO - __main__ -   Batch Number = 63
01/07/2022 12:52:51 - INFO - __main__ -   Batch Number = 64
01/07/2022 12:52:51 - INFO - __main__ -   Batch Number = 65
Evaluating:  38%|      | 65/173 [00:01<00:02, 40.71it/s]01/07/2022 12:52:51 - INFO - __main__ -   Batch Number = 66
01/07/2022 12:52:52 - INFO - __main__ -   Batch Number = 67
01/07/2022 12:52:52 - INFO - __main__ -   Batch Number = 68
01/07/2022 12:52:52 - INFO - __main__ -   Batch Number = 69
01/07/2022 12:52:52 - INFO - __main__ -   Batch Number = 70
Evaluating:  40%|      | 70/173 [00:01<00:02, 40.42it/s]01/07/2022 12:52:52 - INFO - __main__ -   Batch Number = 71
01/07/2022 12:52:52 - INFO - __main__ -   Batch Number = 72
01/07/2022 12:52:52 - INFO - __main__ -   Batch Number = 73
01/07/2022 12:52:52 - INFO - __main__ -   Batch Number = 74
01/07/2022 12:52:52 - INFO - __main__ -   Batch Number = 75
Evaluating:  43%|     | 75/173 [00:01<00:02, 39.84it/s]01/07/2022 12:52:52 - INFO - __main__ -   Batch Number = 76
01/07/2022 12:52:52 - INFO - __main__ -   Batch Number = 77
01/07/2022 12:52:52 - INFO - __main__ -   Batch Number = 78
01/07/2022 12:52:52 - INFO - __main__ -   Batch Number = 79
Evaluating:  46%|     | 79/173 [00:02<00:02, 39.63it/s]01/07/2022 12:52:52 - INFO - __main__ -   Batch Number = 80
01/07/2022 12:52:52 - INFO - __main__ -   Batch Number = 81
01/07/2022 12:52:52 - INFO - __main__ -   Batch Number = 82
01/07/2022 12:52:52 - INFO - __main__ -   Batch Number = 83
Evaluating:  48%|     | 83/173 [00:02<00:02, 39.49it/s]01/07/2022 12:52:52 - INFO - __main__ -   Batch Number = 84
01/07/2022 12:52:52 - INFO - __main__ -   Batch Number = 85
01/07/2022 12:52:52 - INFO - __main__ -   Batch Number = 86
01/07/2022 12:52:52 - INFO - __main__ -   Batch Number = 87
01/07/2022 12:52:52 - INFO - __main__ -   Batch Number = 88
Evaluating:  51%|     | 88/173 [00:02<00:02, 39.74it/s]01/07/2022 12:52:52 - INFO - __main__ -   Batch Number = 89
01/07/2022 12:52:52 - INFO - __main__ -   Batch Number = 90
01/07/2022 12:52:52 - INFO - __main__ -   Batch Number = 91
01/07/2022 12:52:52 - INFO - __main__ -   Batch Number = 92
Evaluating:  53%|    | 92/173 [00:02<00:02, 39.72it/s]01/07/2022 12:52:52 - INFO - __main__ -   Batch Number = 93
01/07/2022 12:52:52 - INFO - __main__ -   Batch Number = 94
01/07/2022 12:52:52 - INFO - __main__ -   Batch Number = 95
01/07/2022 12:52:52 - INFO - __main__ -   Batch Number = 96
01/07/2022 12:52:52 - INFO - __main__ -   Batch Number = 97
Evaluating:  56%|    | 97/173 [00:02<00:01, 40.02it/s]01/07/2022 12:52:52 - INFO - __main__ -   Batch Number = 98
01/07/2022 12:52:52 - INFO - __main__ -   Batch Number = 99
01/07/2022 12:52:52 - INFO - __main__ -   Batch Number = 100
01/07/2022 12:52:52 - INFO - __main__ -   Batch Number = 101
01/07/2022 12:52:52 - INFO - __main__ -   Batch Number = 102
Evaluating:  59%|    | 102/173 [00:02<00:01, 39.97it/s]01/07/2022 12:52:52 - INFO - __main__ -   Batch Number = 103
01/07/2022 12:52:52 - INFO - __main__ -   Batch Number = 104
01/07/2022 12:52:52 - INFO - __main__ -   Batch Number = 105
01/07/2022 12:52:52 - INFO - __main__ -   Batch Number = 106
Evaluating:  61%|   | 106/173 [00:02<00:01, 39.30it/s]01/07/2022 12:52:53 - INFO - __main__ -   Batch Number = 107
01/07/2022 12:52:53 - INFO - __main__ -   Batch Number = 108
01/07/2022 12:52:53 - INFO - __main__ -   Batch Number = 109
01/07/2022 12:52:53 - INFO - __main__ -   Batch Number = 110
Evaluating:  64%|   | 110/173 [00:02<00:01, 38.93it/s]01/07/2022 12:52:53 - INFO - __main__ -   Batch Number = 111
01/07/2022 12:52:53 - INFO - __main__ -   Batch Number = 112
01/07/2022 12:52:53 - INFO - __main__ -   Batch Number = 113
01/07/2022 12:52:53 - INFO - __main__ -   Batch Number = 114
Evaluating:  66%|   | 114/173 [00:02<00:01, 38.72it/s]01/07/2022 12:52:53 - INFO - __main__ -   Batch Number = 115
01/07/2022 12:52:53 - INFO - __main__ -   Batch Number = 116
01/07/2022 12:52:53 - INFO - __main__ -   Batch Number = 117
01/07/2022 12:52:53 - INFO - __main__ -   Batch Number = 118
Evaluating:  68%|   | 118/173 [00:03<00:01, 38.45it/s]01/07/2022 12:52:53 - INFO - __main__ -   Batch Number = 119
01/07/2022 12:52:53 - INFO - __main__ -   Batch Number = 120
01/07/2022 12:52:53 - INFO - __main__ -   Batch Number = 121
01/07/2022 12:52:53 - INFO - __main__ -   Batch Number = 122
Evaluating:  71%|   | 122/173 [00:03<00:01, 38.64it/s]01/07/2022 12:52:53 - INFO - __main__ -   Batch Number = 123
01/07/2022 12:52:53 - INFO - __main__ -   Batch Number = 124
01/07/2022 12:52:53 - INFO - __main__ -   Batch Number = 125
01/07/2022 12:52:53 - INFO - __main__ -   Batch Number = 126
Evaluating:  73%|  | 126/173 [00:03<00:01, 38.79it/s]01/07/2022 12:52:53 - INFO - __main__ -   Batch Number = 127
01/07/2022 12:52:53 - INFO - __main__ -   Batch Number = 128
01/07/2022 12:52:53 - INFO - __main__ -   Batch Number = 129
01/07/2022 12:52:53 - INFO - __main__ -   Batch Number = 130
Evaluating:  75%|  | 130/173 [00:03<00:01, 38.59it/s]01/07/2022 12:52:53 - INFO - __main__ -   Batch Number = 131
01/07/2022 12:52:53 - INFO - __main__ -   Batch Number = 132
01/07/2022 12:52:53 - INFO - __main__ -   Batch Number = 133
01/07/2022 12:52:53 - INFO - __main__ -   Batch Number = 134
01/07/2022 12:52:53 - INFO - __main__ -   Batch Number = 135
Evaluating:  78%|  | 135/173 [00:03<00:00, 39.34it/s]01/07/2022 12:52:53 - INFO - __main__ -   Batch Number = 136
01/07/2022 12:52:53 - INFO - __main__ -   Batch Number = 137
01/07/2022 12:52:53 - INFO - __main__ -   Batch Number = 138
01/07/2022 12:52:53 - INFO - __main__ -   Batch Number = 139
01/07/2022 12:52:53 - INFO - __main__ -   Batch Number = 140
Evaluating:  81%|  | 140/173 [00:03<00:00, 39.92it/s]01/07/2022 12:52:53 - INFO - __main__ -   Batch Number = 141
01/07/2022 12:52:53 - INFO - __main__ -   Batch Number = 142
01/07/2022 12:52:53 - INFO - __main__ -   Batch Number = 143
01/07/2022 12:52:53 - INFO - __main__ -   Batch Number = 144
01/07/2022 12:52:53 - INFO - __main__ -   Batch Number = 145
Evaluating:  84%| | 145/173 [00:03<00:00, 40.83it/s]01/07/2022 12:52:54 - INFO - __main__ -   Batch Number = 146
01/07/2022 12:52:54 - INFO - __main__ -   Batch Number = 147
01/07/2022 12:52:54 - INFO - __main__ -   Batch Number = 148
01/07/2022 12:52:54 - INFO - __main__ -   Batch Number = 149
01/07/2022 12:52:54 - INFO - __main__ -   Batch Number = 150
Evaluating:  87%| | 150/173 [00:03<00:00, 39.89it/s]01/07/2022 12:52:54 - INFO - __main__ -   Batch Number = 151
01/07/2022 12:52:54 - INFO - __main__ -   Batch Number = 152
01/07/2022 12:52:54 - INFO - __main__ -   Batch Number = 153
01/07/2022 12:52:54 - INFO - __main__ -   Batch Number = 154
01/07/2022 12:52:54 - INFO - __main__ -   Batch Number = 155
Evaluating:  90%| | 155/173 [00:03<00:00, 40.21it/s]01/07/2022 12:52:54 - INFO - __main__ -   Batch Number = 156
01/07/2022 12:52:54 - INFO - __main__ -   Batch Number = 157
01/07/2022 12:52:54 - INFO - __main__ -   Batch Number = 158
01/07/2022 12:52:54 - INFO - __main__ -   Batch Number = 159
01/07/2022 12:52:54 - INFO - __main__ -   Batch Number = 160
Evaluating:  92%|| 160/173 [00:04<00:00, 40.27it/s]01/07/2022 12:52:54 - INFO - __main__ -   Batch Number = 161
01/07/2022 12:52:54 - INFO - __main__ -   Batch Number = 162
01/07/2022 12:52:54 - INFO - __main__ -   Batch Number = 163
01/07/2022 12:52:54 - INFO - __main__ -   Batch Number = 164
01/07/2022 12:52:54 - INFO - __main__ -   Batch Number = 165
Evaluating:  95%|| 165/173 [00:04<00:00, 40.39it/s]01/07/2022 12:52:54 - INFO - __main__ -   Batch Number = 166
01/07/2022 12:52:54 - INFO - __main__ -   Batch Number = 167
01/07/2022 12:52:54 - INFO - __main__ -   Batch Number = 168
01/07/2022 12:52:54 - INFO - __main__ -   Batch Number = 169
01/07/2022 12:52:54 - INFO - __main__ -   Batch Number = 170
Evaluating:  98%|| 170/173 [00:04<00:00, 40.12it/s]01/07/2022 12:52:54 - INFO - __main__ -   Batch Number = 171
01/07/2022 12:52:54 - INFO - __main__ -   Batch Number = 172
01/07/2022 12:52:54 - INFO - __main__ -   Batch Number = 173
Evaluating: 100%|| 173/173 [00:04<00:00, 39.44it/s]
01/07/2022 12:52:54 - INFO - __main__ -     Evaluation done in total 4.386475 secs (0.003174 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/predictions_hi_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/nbest_predictions_hi_.json
01/07/2022 12:52:57 - INFO - __main__ -   Results = OrderedDict([('exact', 49.747899159663866), ('f1', 66.65017841234011), ('total', 1190), ('HasAns_exact', 49.747899159663866), ('HasAns_f1', 66.65017841234011), ('HasAns_total', 1190), ('best_exact', 49.747899159663866), ('best_exact_thresh', 0.0), ('best_f1', 66.65017841234011), ('best_f1_thresh', 0.0)])
01/07/2022 12:52:57 - INFO - __main__ -   Language adapter for ru not found, using en instead
01/07/2022 12:52:57 - INFO - __main__ -   Set active language adapter to en
01/07/2022 12:52:57 - INFO - __main__ -   Args Adapter Weight = None
01/07/2022 12:52:57 - INFO - __main__ -   Adapter Languages = ['en']
01/07/2022 12:52:57 - INFO - __main__ -   Predict File = xquad.ru.json
01/07/2022 12:52:57 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
  0%|          | 0/48 [00:00<?, ?it/s] 31%|      | 15/48 [00:00<00:00, 142.08it/s] 62%|   | 30/48 [00:00<00:00, 106.26it/s] 92%|| 44/48 [00:00<00:00, 115.55it/s]100%|| 48/48 [00:00<00:00, 117.61it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<05:17,  3.75it/s]convert squad examples to features:  32%|      | 385/1190 [00:00<00:01, 621.11it/s]convert squad examples to features:  89%| | 1057/1190 [00:00<00:00, 1486.17it/s]convert squad examples to features: 100%|| 1190/1190 [00:00<00:00, 1322.45it/s]/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|| 1190/1190 [00:00<00:00, 731636.14it/s]
01/07/2022 12:52:59 - INFO - __main__ -   Local Rank = -1
01/07/2022 12:52:59 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.ru.json_xlm-roberta-base_384_ru
01/07/2022 12:53:00 - INFO - __main__ -   ***** Running evaluation  *****
01/07/2022 12:53:00 - INFO - __main__ -     Num examples = 1332
01/07/2022 12:53:00 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/167 [00:00<?, ?it/s]01/07/2022 12:53:00 - INFO - __main__ -   Batch Number = 1
01/07/2022 12:53:00 - INFO - __main__ -   Batch Number = 2
01/07/2022 12:53:00 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|         | 3/167 [00:00<00:06, 26.02it/s]01/07/2022 12:53:00 - INFO - __main__ -   Batch Number = 4
01/07/2022 12:53:01 - INFO - __main__ -   Batch Number = 5
01/07/2022 12:53:01 - INFO - __main__ -   Batch Number = 6
Evaluating:   4%|         | 6/167 [00:00<00:05, 27.37it/s]01/07/2022 12:53:01 - INFO - __main__ -   Batch Number = 7
01/07/2022 12:53:01 - INFO - __main__ -   Batch Number = 8
01/07/2022 12:53:01 - INFO - __main__ -   Batch Number = 9
01/07/2022 12:53:01 - INFO - __main__ -   Batch Number = 10
Evaluating:   6%|         | 10/167 [00:00<00:05, 31.24it/s]01/07/2022 12:53:01 - INFO - __main__ -   Batch Number = 11
01/07/2022 12:53:01 - INFO - __main__ -   Batch Number = 12
01/07/2022 12:53:01 - INFO - __main__ -   Batch Number = 13
01/07/2022 12:53:01 - INFO - __main__ -   Batch Number = 14
01/07/2022 12:53:01 - INFO - __main__ -   Batch Number = 15
Evaluating:   9%|         | 15/167 [00:00<00:04, 36.58it/s]01/07/2022 12:53:01 - INFO - __main__ -   Batch Number = 16
01/07/2022 12:53:01 - INFO - __main__ -   Batch Number = 17
01/07/2022 12:53:01 - INFO - __main__ -   Batch Number = 18
01/07/2022 12:53:01 - INFO - __main__ -   Batch Number = 19
01/07/2022 12:53:01 - INFO - __main__ -   Batch Number = 20
Evaluating:  12%|        | 20/167 [00:00<00:03, 39.05it/s]01/07/2022 12:53:01 - INFO - __main__ -   Batch Number = 21
01/07/2022 12:53:01 - INFO - __main__ -   Batch Number = 22
01/07/2022 12:53:01 - INFO - __main__ -   Batch Number = 23
01/07/2022 12:53:01 - INFO - __main__ -   Batch Number = 24
01/07/2022 12:53:01 - INFO - __main__ -   Batch Number = 25
Evaluating:  15%|        | 25/167 [00:00<00:03, 40.93it/s]01/07/2022 12:53:01 - INFO - __main__ -   Batch Number = 26
01/07/2022 12:53:01 - INFO - __main__ -   Batch Number = 27
01/07/2022 12:53:01 - INFO - __main__ -   Batch Number = 28
01/07/2022 12:53:01 - INFO - __main__ -   Batch Number = 29
01/07/2022 12:53:01 - INFO - __main__ -   Batch Number = 30
Evaluating:  18%|        | 30/167 [00:00<00:03, 42.09it/s]01/07/2022 12:53:01 - INFO - __main__ -   Batch Number = 31
01/07/2022 12:53:01 - INFO - __main__ -   Batch Number = 32
01/07/2022 12:53:01 - INFO - __main__ -   Batch Number = 33
01/07/2022 12:53:01 - INFO - __main__ -   Batch Number = 34
01/07/2022 12:53:01 - INFO - __main__ -   Batch Number = 35
Evaluating:  21%|        | 35/167 [00:00<00:03, 42.72it/s]01/07/2022 12:53:01 - INFO - __main__ -   Batch Number = 36
01/07/2022 12:53:01 - INFO - __main__ -   Batch Number = 37
01/07/2022 12:53:01 - INFO - __main__ -   Batch Number = 38
01/07/2022 12:53:01 - INFO - __main__ -   Batch Number = 39
01/07/2022 12:53:01 - INFO - __main__ -   Batch Number = 40
Evaluating:  24%|       | 40/167 [00:01<00:02, 43.43it/s]01/07/2022 12:53:01 - INFO - __main__ -   Batch Number = 41
01/07/2022 12:53:01 - INFO - __main__ -   Batch Number = 42
01/07/2022 12:53:01 - INFO - __main__ -   Batch Number = 43
01/07/2022 12:53:01 - INFO - __main__ -   Batch Number = 44
01/07/2022 12:53:01 - INFO - __main__ -   Batch Number = 45
Evaluating:  27%|       | 45/167 [00:01<00:02, 43.62it/s]01/07/2022 12:53:01 - INFO - __main__ -   Batch Number = 46
01/07/2022 12:53:02 - INFO - __main__ -   Batch Number = 47
01/07/2022 12:53:02 - INFO - __main__ -   Batch Number = 48
01/07/2022 12:53:02 - INFO - __main__ -   Batch Number = 49
01/07/2022 12:53:02 - INFO - __main__ -   Batch Number = 50
Evaluating:  30%|       | 50/167 [00:01<00:02, 43.72it/s]01/07/2022 12:53:02 - INFO - __main__ -   Batch Number = 51
01/07/2022 12:53:02 - INFO - __main__ -   Batch Number = 52
01/07/2022 12:53:02 - INFO - __main__ -   Batch Number = 53
01/07/2022 12:53:02 - INFO - __main__ -   Batch Number = 54
01/07/2022 12:53:02 - INFO - __main__ -   Batch Number = 55
Evaluating:  33%|      | 55/167 [00:01<00:02, 43.82it/s]01/07/2022 12:53:02 - INFO - __main__ -   Batch Number = 56
01/07/2022 12:53:02 - INFO - __main__ -   Batch Number = 57
01/07/2022 12:53:02 - INFO - __main__ -   Batch Number = 58
01/07/2022 12:53:02 - INFO - __main__ -   Batch Number = 59
01/07/2022 12:53:02 - INFO - __main__ -   Batch Number = 60
Evaluating:  36%|      | 60/167 [00:01<00:02, 39.44it/s]01/07/2022 12:53:02 - INFO - __main__ -   Batch Number = 61
01/07/2022 12:53:02 - INFO - __main__ -   Batch Number = 62
01/07/2022 12:53:02 - INFO - __main__ -   Batch Number = 63
01/07/2022 12:53:02 - INFO - __main__ -   Batch Number = 64
01/07/2022 12:53:02 - INFO - __main__ -   Batch Number = 65
Evaluating:  39%|      | 65/167 [00:01<00:02, 38.98it/s]01/07/2022 12:53:02 - INFO - __main__ -   Batch Number = 66
01/07/2022 12:53:02 - INFO - __main__ -   Batch Number = 67
01/07/2022 12:53:02 - INFO - __main__ -   Batch Number = 68
01/07/2022 12:53:02 - INFO - __main__ -   Batch Number = 69
01/07/2022 12:53:02 - INFO - __main__ -   Batch Number = 70
Evaluating:  42%|     | 70/167 [00:01<00:02, 40.33it/s]01/07/2022 12:53:02 - INFO - __main__ -   Batch Number = 71
01/07/2022 12:53:02 - INFO - __main__ -   Batch Number = 72
01/07/2022 12:53:02 - INFO - __main__ -   Batch Number = 73
01/07/2022 12:53:02 - INFO - __main__ -   Batch Number = 74
01/07/2022 12:53:02 - INFO - __main__ -   Batch Number = 75
Evaluating:  45%|     | 75/167 [00:01<00:02, 41.48it/s]01/07/2022 12:53:02 - INFO - __main__ -   Batch Number = 76
01/07/2022 12:53:02 - INFO - __main__ -   Batch Number = 77
01/07/2022 12:53:02 - INFO - __main__ -   Batch Number = 78
01/07/2022 12:53:02 - INFO - __main__ -   Batch Number = 79
01/07/2022 12:53:02 - INFO - __main__ -   Batch Number = 80
Evaluating:  48%|     | 80/167 [00:01<00:02, 42.17it/s]01/07/2022 12:53:02 - INFO - __main__ -   Batch Number = 81
01/07/2022 12:53:02 - INFO - __main__ -   Batch Number = 82
01/07/2022 12:53:02 - INFO - __main__ -   Batch Number = 83
01/07/2022 12:53:02 - INFO - __main__ -   Batch Number = 84
01/07/2022 12:53:02 - INFO - __main__ -   Batch Number = 85
Evaluating:  51%|     | 85/167 [00:02<00:01, 42.76it/s]01/07/2022 12:53:02 - INFO - __main__ -   Batch Number = 86
01/07/2022 12:53:02 - INFO - __main__ -   Batch Number = 87
01/07/2022 12:53:03 - INFO - __main__ -   Batch Number = 88
01/07/2022 12:53:03 - INFO - __main__ -   Batch Number = 89
01/07/2022 12:53:03 - INFO - __main__ -   Batch Number = 90
Evaluating:  54%|    | 90/167 [00:02<00:01, 43.27it/s]01/07/2022 12:53:03 - INFO - __main__ -   Batch Number = 91
01/07/2022 12:53:03 - INFO - __main__ -   Batch Number = 92
01/07/2022 12:53:03 - INFO - __main__ -   Batch Number = 93
01/07/2022 12:53:03 - INFO - __main__ -   Batch Number = 94
01/07/2022 12:53:03 - INFO - __main__ -   Batch Number = 95
Evaluating:  57%|    | 95/167 [00:02<00:01, 43.48it/s]01/07/2022 12:53:03 - INFO - __main__ -   Batch Number = 96
01/07/2022 12:53:03 - INFO - __main__ -   Batch Number = 97
01/07/2022 12:53:03 - INFO - __main__ -   Batch Number = 98
01/07/2022 12:53:03 - INFO - __main__ -   Batch Number = 99
01/07/2022 12:53:03 - INFO - __main__ -   Batch Number = 100
Evaluating:  60%|    | 100/167 [00:02<00:01, 43.89it/s]01/07/2022 12:53:03 - INFO - __main__ -   Batch Number = 101
01/07/2022 12:53:03 - INFO - __main__ -   Batch Number = 102
01/07/2022 12:53:03 - INFO - __main__ -   Batch Number = 103
01/07/2022 12:53:03 - INFO - __main__ -   Batch Number = 104
01/07/2022 12:53:03 - INFO - __main__ -   Batch Number = 105
Evaluating:  63%|   | 105/167 [00:02<00:01, 44.06it/s]01/07/2022 12:53:03 - INFO - __main__ -   Batch Number = 106
01/07/2022 12:53:03 - INFO - __main__ -   Batch Number = 107
01/07/2022 12:53:03 - INFO - __main__ -   Batch Number = 108
01/07/2022 12:53:03 - INFO - __main__ -   Batch Number = 109
01/07/2022 12:53:03 - INFO - __main__ -   Batch Number = 110
Evaluating:  66%|   | 110/167 [00:02<00:01, 44.09it/s]01/07/2022 12:53:03 - INFO - __main__ -   Batch Number = 111
01/07/2022 12:53:03 - INFO - __main__ -   Batch Number = 112
01/07/2022 12:53:03 - INFO - __main__ -   Batch Number = 113
01/07/2022 12:53:03 - INFO - __main__ -   Batch Number = 114
01/07/2022 12:53:03 - INFO - __main__ -   Batch Number = 115
Evaluating:  69%|   | 115/167 [00:02<00:01, 44.22it/s]01/07/2022 12:53:03 - INFO - __main__ -   Batch Number = 116
01/07/2022 12:53:03 - INFO - __main__ -   Batch Number = 117
01/07/2022 12:53:03 - INFO - __main__ -   Batch Number = 118
01/07/2022 12:53:03 - INFO - __main__ -   Batch Number = 119
01/07/2022 12:53:03 - INFO - __main__ -   Batch Number = 120
Evaluating:  72%|  | 120/167 [00:02<00:01, 44.26it/s]01/07/2022 12:53:03 - INFO - __main__ -   Batch Number = 121
01/07/2022 12:53:03 - INFO - __main__ -   Batch Number = 122
01/07/2022 12:53:03 - INFO - __main__ -   Batch Number = 123
01/07/2022 12:53:03 - INFO - __main__ -   Batch Number = 124
01/07/2022 12:53:03 - INFO - __main__ -   Batch Number = 125
Evaluating:  75%|  | 125/167 [00:02<00:00, 44.28it/s]01/07/2022 12:53:03 - INFO - __main__ -   Batch Number = 126
01/07/2022 12:53:03 - INFO - __main__ -   Batch Number = 127
01/07/2022 12:53:03 - INFO - __main__ -   Batch Number = 128
01/07/2022 12:53:03 - INFO - __main__ -   Batch Number = 129
01/07/2022 12:53:03 - INFO - __main__ -   Batch Number = 130
Evaluating:  78%|  | 130/167 [00:03<00:00, 43.93it/s]01/07/2022 12:53:03 - INFO - __main__ -   Batch Number = 131
01/07/2022 12:53:04 - INFO - __main__ -   Batch Number = 132
01/07/2022 12:53:04 - INFO - __main__ -   Batch Number = 133
01/07/2022 12:53:04 - INFO - __main__ -   Batch Number = 134
01/07/2022 12:53:04 - INFO - __main__ -   Batch Number = 135
Evaluating:  81%|  | 135/167 [00:03<00:00, 43.91it/s]01/07/2022 12:53:04 - INFO - __main__ -   Batch Number = 136
01/07/2022 12:53:04 - INFO - __main__ -   Batch Number = 137
01/07/2022 12:53:04 - INFO - __main__ -   Batch Number = 138
01/07/2022 12:53:04 - INFO - __main__ -   Batch Number = 139
01/07/2022 12:53:04 - INFO - __main__ -   Batch Number = 140
Evaluating:  84%| | 140/167 [00:03<00:00, 43.80it/s]01/07/2022 12:53:04 - INFO - __main__ -   Batch Number = 141
01/07/2022 12:53:04 - INFO - __main__ -   Batch Number = 142
01/07/2022 12:53:04 - INFO - __main__ -   Batch Number = 143
01/07/2022 12:53:04 - INFO - __main__ -   Batch Number = 144
01/07/2022 12:53:04 - INFO - __main__ -   Batch Number = 145
Evaluating:  87%| | 145/167 [00:03<00:00, 43.82it/s]01/07/2022 12:53:04 - INFO - __main__ -   Batch Number = 146
01/07/2022 12:53:04 - INFO - __main__ -   Batch Number = 147
01/07/2022 12:53:04 - INFO - __main__ -   Batch Number = 148
01/07/2022 12:53:04 - INFO - __main__ -   Batch Number = 149
01/07/2022 12:53:04 - INFO - __main__ -   Batch Number = 150
Evaluating:  90%| | 150/167 [00:03<00:00, 44.06it/s]01/07/2022 12:53:04 - INFO - __main__ -   Batch Number = 151
01/07/2022 12:53:04 - INFO - __main__ -   Batch Number = 152
01/07/2022 12:53:04 - INFO - __main__ -   Batch Number = 153
01/07/2022 12:53:04 - INFO - __main__ -   Batch Number = 154
01/07/2022 12:53:04 - INFO - __main__ -   Batch Number = 155
Evaluating:  93%|| 155/167 [00:03<00:00, 44.08it/s]01/07/2022 12:53:04 - INFO - __main__ -   Batch Number = 156
01/07/2022 12:53:04 - INFO - __main__ -   Batch Number = 157
01/07/2022 12:53:04 - INFO - __main__ -   Batch Number = 158
01/07/2022 12:53:04 - INFO - __main__ -   Batch Number = 159
01/07/2022 12:53:04 - INFO - __main__ -   Batch Number = 160
Evaluating:  96%|| 160/167 [00:03<00:00, 37.53it/s]01/07/2022 12:53:04 - INFO - __main__ -   Batch Number = 161
01/07/2022 12:53:04 - INFO - __main__ -   Batch Number = 162
01/07/2022 12:53:04 - INFO - __main__ -   Batch Number = 163
01/07/2022 12:53:04 - INFO - __main__ -   Batch Number = 164
Evaluating:  98%|| 164/167 [00:03<00:00, 35.43it/s]01/07/2022 12:53:04 - INFO - __main__ -   Batch Number = 165
01/07/2022 12:53:04 - INFO - __main__ -   Batch Number = 166
01/07/2022 12:53:04 - INFO - __main__ -   Batch Number = 167
Evaluating: 100%|| 167/167 [00:04<00:00, 40.97it/s]
01/07/2022 12:53:04 - INFO - __main__ -     Evaluation done in total 4.076695 secs (0.003061 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/predictions_ru_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/nbest_predictions_ru_.json
01/07/2022 12:53:08 - INFO - __main__ -   Results = OrderedDict([('exact', 57.05882352941177), ('f1', 73.02766853528385), ('total', 1190), ('HasAns_exact', 57.05882352941177), ('HasAns_f1', 73.02766853528385), ('HasAns_total', 1190), ('best_exact', 57.05882352941177), ('best_exact_thresh', 0.0), ('best_f1', 73.02766853528385), ('best_f1_thresh', 0.0)])
01/07/2022 12:53:08 - INFO - __main__ -   Language adapter for th not found, using en instead
01/07/2022 12:53:08 - INFO - __main__ -   Set active language adapter to en
01/07/2022 12:53:08 - INFO - __main__ -   Args Adapter Weight = None
01/07/2022 12:53:08 - INFO - __main__ -   Adapter Languages = ['en']
01/07/2022 12:53:08 - INFO - __main__ -   Predict File = xquad.th.json
01/07/2022 12:53:08 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
  0%|          | 0/48 [00:00<?, ?it/s] 33%|      | 16/48 [00:00<00:00, 128.57it/s] 60%|    | 29/48 [00:00<00:00, 119.16it/s] 92%|| 44/48 [00:00<00:00, 130.30it/s]100%|| 48/48 [00:00<00:00, 129.38it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<02:46,  7.15it/s]convert squad examples to features:  32%|      | 385/1190 [00:00<00:00, 1040.63it/s]convert squad examples to features:  75%|  | 897/1190 [00:00<00:00, 1850.35it/s]convert squad examples to features: 100%|| 1190/1190 [00:00<00:00, 2066.21it/s]/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|| 1190/1190 [00:00<00:00, 700129.30it/s]
01/07/2022 12:53:09 - INFO - __main__ -   Local Rank = -1
01/07/2022 12:53:09 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.th.json_xlm-roberta-base_384_th
01/07/2022 12:53:10 - INFO - __main__ -   ***** Running evaluation  *****
01/07/2022 12:53:10 - INFO - __main__ -     Num examples = 1314
01/07/2022 12:53:10 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/165 [00:00<?, ?it/s]01/07/2022 12:53:10 - INFO - __main__ -   Batch Number = 1
01/07/2022 12:53:11 - INFO - __main__ -   Batch Number = 2
01/07/2022 12:53:11 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|         | 3/165 [00:00<00:05, 29.61it/s]01/07/2022 12:53:11 - INFO - __main__ -   Batch Number = 4
01/07/2022 12:53:11 - INFO - __main__ -   Batch Number = 5
01/07/2022 12:53:11 - INFO - __main__ -   Batch Number = 6
Evaluating:   4%|         | 6/165 [00:00<00:05, 29.81it/s]01/07/2022 12:53:11 - INFO - __main__ -   Batch Number = 7
01/07/2022 12:53:11 - INFO - __main__ -   Batch Number = 8
01/07/2022 12:53:11 - INFO - __main__ -   Batch Number = 9
01/07/2022 12:53:11 - INFO - __main__ -   Batch Number = 10
Evaluating:   6%|         | 10/165 [00:00<00:04, 33.58it/s]01/07/2022 12:53:11 - INFO - __main__ -   Batch Number = 11
01/07/2022 12:53:11 - INFO - __main__ -   Batch Number = 12
01/07/2022 12:53:11 - INFO - __main__ -   Batch Number = 13
01/07/2022 12:53:11 - INFO - __main__ -   Batch Number = 14
01/07/2022 12:53:11 - INFO - __main__ -   Batch Number = 15
Evaluating:   9%|         | 15/165 [00:00<00:03, 38.18it/s]01/07/2022 12:53:11 - INFO - __main__ -   Batch Number = 16
01/07/2022 12:53:11 - INFO - __main__ -   Batch Number = 17
01/07/2022 12:53:11 - INFO - __main__ -   Batch Number = 18
01/07/2022 12:53:11 - INFO - __main__ -   Batch Number = 19
01/07/2022 12:53:11 - INFO - __main__ -   Batch Number = 20
Evaluating:  12%|        | 20/165 [00:00<00:03, 40.47it/s]01/07/2022 12:53:11 - INFO - __main__ -   Batch Number = 21
01/07/2022 12:53:11 - INFO - __main__ -   Batch Number = 22
01/07/2022 12:53:11 - INFO - __main__ -   Batch Number = 23
01/07/2022 12:53:11 - INFO - __main__ -   Batch Number = 24
01/07/2022 12:53:11 - INFO - __main__ -   Batch Number = 25
Evaluating:  15%|        | 25/165 [00:00<00:03, 42.00it/s]01/07/2022 12:53:11 - INFO - __main__ -   Batch Number = 26
01/07/2022 12:53:11 - INFO - __main__ -   Batch Number = 27
01/07/2022 12:53:11 - INFO - __main__ -   Batch Number = 28
01/07/2022 12:53:11 - INFO - __main__ -   Batch Number = 29
01/07/2022 12:53:11 - INFO - __main__ -   Batch Number = 30
Evaluating:  18%|        | 30/165 [00:00<00:03, 42.98it/s]01/07/2022 12:53:11 - INFO - __main__ -   Batch Number = 31
01/07/2022 12:53:11 - INFO - __main__ -   Batch Number = 32
01/07/2022 12:53:11 - INFO - __main__ -   Batch Number = 33
01/07/2022 12:53:11 - INFO - __main__ -   Batch Number = 34
01/07/2022 12:53:11 - INFO - __main__ -   Batch Number = 35
Evaluating:  21%|        | 35/165 [00:00<00:02, 43.41it/s]01/07/2022 12:53:11 - INFO - __main__ -   Batch Number = 36
01/07/2022 12:53:11 - INFO - __main__ -   Batch Number = 37
01/07/2022 12:53:11 - INFO - __main__ -   Batch Number = 38
01/07/2022 12:53:11 - INFO - __main__ -   Batch Number = 39
01/07/2022 12:53:11 - INFO - __main__ -   Batch Number = 40
Evaluating:  24%|       | 40/165 [00:00<00:02, 43.81it/s]01/07/2022 12:53:11 - INFO - __main__ -   Batch Number = 41
01/07/2022 12:53:11 - INFO - __main__ -   Batch Number = 42
01/07/2022 12:53:12 - INFO - __main__ -   Batch Number = 43
01/07/2022 12:53:12 - INFO - __main__ -   Batch Number = 44
01/07/2022 12:53:12 - INFO - __main__ -   Batch Number = 45
Evaluating:  27%|       | 45/165 [00:01<00:02, 44.12it/s]01/07/2022 12:53:12 - INFO - __main__ -   Batch Number = 46
01/07/2022 12:53:12 - INFO - __main__ -   Batch Number = 47
01/07/2022 12:53:12 - INFO - __main__ -   Batch Number = 48
01/07/2022 12:53:12 - INFO - __main__ -   Batch Number = 49
01/07/2022 12:53:12 - INFO - __main__ -   Batch Number = 50
Evaluating:  30%|       | 50/165 [00:01<00:02, 44.11it/s]01/07/2022 12:53:12 - INFO - __main__ -   Batch Number = 51
01/07/2022 12:53:12 - INFO - __main__ -   Batch Number = 52
01/07/2022 12:53:12 - INFO - __main__ -   Batch Number = 53
01/07/2022 12:53:12 - INFO - __main__ -   Batch Number = 54
01/07/2022 12:53:12 - INFO - __main__ -   Batch Number = 55
Evaluating:  33%|      | 55/165 [00:01<00:02, 44.36it/s]01/07/2022 12:53:12 - INFO - __main__ -   Batch Number = 56
01/07/2022 12:53:12 - INFO - __main__ -   Batch Number = 57
01/07/2022 12:53:12 - INFO - __main__ -   Batch Number = 58
01/07/2022 12:53:12 - INFO - __main__ -   Batch Number = 59
01/07/2022 12:53:12 - INFO - __main__ -   Batch Number = 60
Evaluating:  36%|      | 60/165 [00:01<00:02, 44.38it/s]01/07/2022 12:53:12 - INFO - __main__ -   Batch Number = 61
01/07/2022 12:53:12 - INFO - __main__ -   Batch Number = 62
01/07/2022 12:53:12 - INFO - __main__ -   Batch Number = 63
01/07/2022 12:53:12 - INFO - __main__ -   Batch Number = 64
01/07/2022 12:53:12 - INFO - __main__ -   Batch Number = 65
Evaluating:  39%|      | 65/165 [00:01<00:02, 44.31it/s]01/07/2022 12:53:12 - INFO - __main__ -   Batch Number = 66
01/07/2022 12:53:12 - INFO - __main__ -   Batch Number = 67
01/07/2022 12:53:12 - INFO - __main__ -   Batch Number = 68
01/07/2022 12:53:12 - INFO - __main__ -   Batch Number = 69
01/07/2022 12:53:12 - INFO - __main__ -   Batch Number = 70
Evaluating:  42%|     | 70/165 [00:01<00:02, 44.35it/s]01/07/2022 12:53:12 - INFO - __main__ -   Batch Number = 71
01/07/2022 12:53:12 - INFO - __main__ -   Batch Number = 72
01/07/2022 12:53:12 - INFO - __main__ -   Batch Number = 73
01/07/2022 12:53:12 - INFO - __main__ -   Batch Number = 74
01/07/2022 12:53:12 - INFO - __main__ -   Batch Number = 75
Evaluating:  45%|     | 75/165 [00:01<00:02, 44.38it/s]01/07/2022 12:53:12 - INFO - __main__ -   Batch Number = 76
01/07/2022 12:53:12 - INFO - __main__ -   Batch Number = 77
01/07/2022 12:53:12 - INFO - __main__ -   Batch Number = 78
01/07/2022 12:53:12 - INFO - __main__ -   Batch Number = 79
01/07/2022 12:53:12 - INFO - __main__ -   Batch Number = 80
Evaluating:  48%|     | 80/165 [00:01<00:01, 44.25it/s]01/07/2022 12:53:12 - INFO - __main__ -   Batch Number = 81
01/07/2022 12:53:12 - INFO - __main__ -   Batch Number = 82
01/07/2022 12:53:12 - INFO - __main__ -   Batch Number = 83
01/07/2022 12:53:12 - INFO - __main__ -   Batch Number = 84
01/07/2022 12:53:12 - INFO - __main__ -   Batch Number = 85
Evaluating:  52%|    | 85/165 [00:02<00:01, 40.78it/s]01/07/2022 12:53:13 - INFO - __main__ -   Batch Number = 86
01/07/2022 12:53:13 - INFO - __main__ -   Batch Number = 87
01/07/2022 12:53:13 - INFO - __main__ -   Batch Number = 88
01/07/2022 12:53:13 - INFO - __main__ -   Batch Number = 89
01/07/2022 12:53:13 - INFO - __main__ -   Batch Number = 90
Evaluating:  55%|    | 90/165 [00:02<00:01, 41.55it/s]01/07/2022 12:53:13 - INFO - __main__ -   Batch Number = 91
01/07/2022 12:53:13 - INFO - __main__ -   Batch Number = 92
01/07/2022 12:53:13 - INFO - __main__ -   Batch Number = 93
01/07/2022 12:53:13 - INFO - __main__ -   Batch Number = 94
01/07/2022 12:53:13 - INFO - __main__ -   Batch Number = 95
Evaluating:  58%|    | 95/165 [00:02<00:01, 42.08it/s]01/07/2022 12:53:13 - INFO - __main__ -   Batch Number = 96
01/07/2022 12:53:13 - INFO - __main__ -   Batch Number = 97
01/07/2022 12:53:13 - INFO - __main__ -   Batch Number = 98
01/07/2022 12:53:13 - INFO - __main__ -   Batch Number = 99
01/07/2022 12:53:13 - INFO - __main__ -   Batch Number = 100
Evaluating:  61%|    | 100/165 [00:02<00:01, 42.64it/s]01/07/2022 12:53:13 - INFO - __main__ -   Batch Number = 101
01/07/2022 12:53:13 - INFO - __main__ -   Batch Number = 102
01/07/2022 12:53:13 - INFO - __main__ -   Batch Number = 103
01/07/2022 12:53:13 - INFO - __main__ -   Batch Number = 104
01/07/2022 12:53:13 - INFO - __main__ -   Batch Number = 105
Evaluating:  64%|   | 105/165 [00:02<00:01, 43.12it/s]01/07/2022 12:53:13 - INFO - __main__ -   Batch Number = 106
01/07/2022 12:53:13 - INFO - __main__ -   Batch Number = 107
01/07/2022 12:53:13 - INFO - __main__ -   Batch Number = 108
01/07/2022 12:53:13 - INFO - __main__ -   Batch Number = 109
01/07/2022 12:53:13 - INFO - __main__ -   Batch Number = 110
Evaluating:  67%|   | 110/165 [00:02<00:01, 43.48it/s]01/07/2022 12:53:13 - INFO - __main__ -   Batch Number = 111
01/07/2022 12:53:13 - INFO - __main__ -   Batch Number = 112
01/07/2022 12:53:13 - INFO - __main__ -   Batch Number = 113
01/07/2022 12:53:13 - INFO - __main__ -   Batch Number = 114
01/07/2022 12:53:13 - INFO - __main__ -   Batch Number = 115
Evaluating:  70%|   | 115/165 [00:02<00:01, 43.82it/s]01/07/2022 12:53:13 - INFO - __main__ -   Batch Number = 116
01/07/2022 12:53:13 - INFO - __main__ -   Batch Number = 117
01/07/2022 12:53:13 - INFO - __main__ -   Batch Number = 118
01/07/2022 12:53:13 - INFO - __main__ -   Batch Number = 119
01/07/2022 12:53:13 - INFO - __main__ -   Batch Number = 120
Evaluating:  73%|  | 120/165 [00:02<00:01, 44.08it/s]01/07/2022 12:53:13 - INFO - __main__ -   Batch Number = 121
01/07/2022 12:53:13 - INFO - __main__ -   Batch Number = 122
01/07/2022 12:53:13 - INFO - __main__ -   Batch Number = 123
01/07/2022 12:53:13 - INFO - __main__ -   Batch Number = 124
01/07/2022 12:53:13 - INFO - __main__ -   Batch Number = 125
Evaluating:  76%|  | 125/165 [00:02<00:00, 44.21it/s]01/07/2022 12:53:13 - INFO - __main__ -   Batch Number = 126
01/07/2022 12:53:13 - INFO - __main__ -   Batch Number = 127
01/07/2022 12:53:13 - INFO - __main__ -   Batch Number = 128
01/07/2022 12:53:13 - INFO - __main__ -   Batch Number = 129
01/07/2022 12:53:14 - INFO - __main__ -   Batch Number = 130
Evaluating:  79%|  | 130/165 [00:03<00:00, 44.24it/s]01/07/2022 12:53:14 - INFO - __main__ -   Batch Number = 131
01/07/2022 12:53:14 - INFO - __main__ -   Batch Number = 132
01/07/2022 12:53:14 - INFO - __main__ -   Batch Number = 133
01/07/2022 12:53:14 - INFO - __main__ -   Batch Number = 134
01/07/2022 12:53:14 - INFO - __main__ -   Batch Number = 135
Evaluating:  82%| | 135/165 [00:03<00:00, 44.40it/s]01/07/2022 12:53:14 - INFO - __main__ -   Batch Number = 136
01/07/2022 12:53:14 - INFO - __main__ -   Batch Number = 137
01/07/2022 12:53:14 - INFO - __main__ -   Batch Number = 138
01/07/2022 12:53:14 - INFO - __main__ -   Batch Number = 139
01/07/2022 12:53:14 - INFO - __main__ -   Batch Number = 140
Evaluating:  85%| | 140/165 [00:03<00:00, 44.22it/s]01/07/2022 12:53:14 - INFO - __main__ -   Batch Number = 141
01/07/2022 12:53:14 - INFO - __main__ -   Batch Number = 142
01/07/2022 12:53:14 - INFO - __main__ -   Batch Number = 143
01/07/2022 12:53:14 - INFO - __main__ -   Batch Number = 144
01/07/2022 12:53:14 - INFO - __main__ -   Batch Number = 145
Evaluating:  88%| | 145/165 [00:03<00:00, 44.31it/s]01/07/2022 12:53:14 - INFO - __main__ -   Batch Number = 146
01/07/2022 12:53:14 - INFO - __main__ -   Batch Number = 147
01/07/2022 12:53:14 - INFO - __main__ -   Batch Number = 148
01/07/2022 12:53:14 - INFO - __main__ -   Batch Number = 149
01/07/2022 12:53:14 - INFO - __main__ -   Batch Number = 150
Evaluating:  91%| | 150/165 [00:03<00:00, 44.31it/s]01/07/2022 12:53:14 - INFO - __main__ -   Batch Number = 151
01/07/2022 12:53:14 - INFO - __main__ -   Batch Number = 152
01/07/2022 12:53:14 - INFO - __main__ -   Batch Number = 153
01/07/2022 12:53:14 - INFO - __main__ -   Batch Number = 154
01/07/2022 12:53:14 - INFO - __main__ -   Batch Number = 155
Evaluating:  94%|| 155/165 [00:03<00:00, 44.25it/s]01/07/2022 12:53:14 - INFO - __main__ -   Batch Number = 156
01/07/2022 12:53:14 - INFO - __main__ -   Batch Number = 157
01/07/2022 12:53:14 - INFO - __main__ -   Batch Number = 158
01/07/2022 12:53:14 - INFO - __main__ -   Batch Number = 159
01/07/2022 12:53:14 - INFO - __main__ -   Batch Number = 160
Evaluating:  97%|| 160/165 [00:03<00:00, 44.35it/s]01/07/2022 12:53:14 - INFO - __main__ -   Batch Number = 161
01/07/2022 12:53:14 - INFO - __main__ -   Batch Number = 162
01/07/2022 12:53:14 - INFO - __main__ -   Batch Number = 163
01/07/2022 12:53:14 - INFO - __main__ -   Batch Number = 164
01/07/2022 12:53:14 - INFO - __main__ -   Batch Number = 165
Evaluating: 100%|| 165/165 [00:03<00:00, 44.99it/s]Evaluating: 100%|| 165/165 [00:03<00:00, 43.11it/s]
01/07/2022 12:53:14 - INFO - __main__ -     Evaluation done in total 3.828192 secs (0.002913 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/predictions_th_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/nbest_predictions_th_.json
01/07/2022 12:53:19 - INFO - __main__ -   Results = OrderedDict([('exact', 57.05882352941177), ('f1', 66.57543227081035), ('total', 1190), ('HasAns_exact', 57.05882352941177), ('HasAns_f1', 66.57543227081035), ('HasAns_total', 1190), ('best_exact', 57.05882352941177), ('best_exact_thresh', 0.0), ('best_f1', 66.57543227081035), ('best_f1_thresh', 0.0)])
01/07/2022 12:53:19 - INFO - __main__ -   Language adapter for tr not found, using en instead
01/07/2022 12:53:19 - INFO - __main__ -   Set active language adapter to en
01/07/2022 12:53:19 - INFO - __main__ -   Args Adapter Weight = None
01/07/2022 12:53:19 - INFO - __main__ -   Adapter Languages = ['en']
01/07/2022 12:53:19 - INFO - __main__ -   Predict File = xquad.tr.json
01/07/2022 12:53:19 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
  0%|          | 0/48 [00:00<?, ?it/s] 33%|      | 16/48 [00:00<00:00, 135.59it/s] 62%|   | 30/48 [00:00<00:00, 128.51it/s] 96%|| 46/48 [00:00<00:00, 139.92it/s]100%|| 48/48 [00:00<00:00, 138.64it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<02:58,  6.65it/s]convert squad examples to features:   5%|         | 65/1190 [00:00<00:03, 310.05it/s]convert squad examples to features:  30%|       | 353/1190 [00:00<00:00, 1081.44it/s]convert squad examples to features:  38%|      | 455/1190 [00:00<00:00, 978.36it/s] convert squad examples to features:  81%|  | 961/1190 [00:00<00:00, 2167.79it/s]convert squad examples to features: 100%|| 1190/1190 [00:00<00:00, 1648.09it/s]/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|| 1190/1190 [00:00<00:00, 642040.36it/s]
01/07/2022 12:53:21 - INFO - __main__ -   Local Rank = -1
01/07/2022 12:53:21 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.tr.json_xlm-roberta-base_384_tr
01/07/2022 12:53:22 - INFO - __main__ -   ***** Running evaluation  *****
01/07/2022 12:53:22 - INFO - __main__ -     Num examples = 1274
01/07/2022 12:53:22 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/160 [00:00<?, ?it/s]01/07/2022 12:53:22 - INFO - __main__ -   Batch Number = 1
01/07/2022 12:53:22 - INFO - __main__ -   Batch Number = 2
01/07/2022 12:53:22 - INFO - __main__ -   Batch Number = 3
01/07/2022 12:53:22 - INFO - __main__ -   Batch Number = 4
01/07/2022 12:53:22 - INFO - __main__ -   Batch Number = 5
Evaluating:   3%|         | 5/160 [00:00<00:03, 44.71it/s]01/07/2022 12:53:22 - INFO - __main__ -   Batch Number = 6
01/07/2022 12:53:22 - INFO - __main__ -   Batch Number = 7
01/07/2022 12:53:22 - INFO - __main__ -   Batch Number = 8
01/07/2022 12:53:22 - INFO - __main__ -   Batch Number = 9
01/07/2022 12:53:22 - INFO - __main__ -   Batch Number = 10
Evaluating:   6%|         | 10/160 [00:00<00:03, 44.53it/s]01/07/2022 12:53:22 - INFO - __main__ -   Batch Number = 11
01/07/2022 12:53:22 - INFO - __main__ -   Batch Number = 12
01/07/2022 12:53:22 - INFO - __main__ -   Batch Number = 13
01/07/2022 12:53:22 - INFO - __main__ -   Batch Number = 14
01/07/2022 12:53:22 - INFO - __main__ -   Batch Number = 15
Evaluating:   9%|         | 15/160 [00:00<00:03, 44.73it/s]01/07/2022 12:53:22 - INFO - __main__ -   Batch Number = 16
01/07/2022 12:53:22 - INFO - __main__ -   Batch Number = 17
01/07/2022 12:53:22 - INFO - __main__ -   Batch Number = 18
01/07/2022 12:53:22 - INFO - __main__ -   Batch Number = 19
01/07/2022 12:53:22 - INFO - __main__ -   Batch Number = 20
Evaluating:  12%|        | 20/160 [00:00<00:03, 44.36it/s]01/07/2022 12:53:22 - INFO - __main__ -   Batch Number = 21
01/07/2022 12:53:22 - INFO - __main__ -   Batch Number = 22
01/07/2022 12:53:22 - INFO - __main__ -   Batch Number = 23
01/07/2022 12:53:22 - INFO - __main__ -   Batch Number = 24
01/07/2022 12:53:22 - INFO - __main__ -   Batch Number = 25
Evaluating:  16%|        | 25/160 [00:00<00:03, 44.57it/s]01/07/2022 12:53:22 - INFO - __main__ -   Batch Number = 26
01/07/2022 12:53:22 - INFO - __main__ -   Batch Number = 27
01/07/2022 12:53:22 - INFO - __main__ -   Batch Number = 28
01/07/2022 12:53:23 - INFO - __main__ -   Batch Number = 29
01/07/2022 12:53:23 - INFO - __main__ -   Batch Number = 30
Evaluating:  19%|        | 30/160 [00:00<00:02, 44.57it/s]01/07/2022 12:53:23 - INFO - __main__ -   Batch Number = 31
01/07/2022 12:53:23 - INFO - __main__ -   Batch Number = 32
01/07/2022 12:53:23 - INFO - __main__ -   Batch Number = 33
01/07/2022 12:53:23 - INFO - __main__ -   Batch Number = 34
01/07/2022 12:53:23 - INFO - __main__ -   Batch Number = 35
Evaluating:  22%|       | 35/160 [00:00<00:02, 44.46it/s]01/07/2022 12:53:23 - INFO - __main__ -   Batch Number = 36
01/07/2022 12:53:23 - INFO - __main__ -   Batch Number = 37
01/07/2022 12:53:23 - INFO - __main__ -   Batch Number = 38
01/07/2022 12:53:23 - INFO - __main__ -   Batch Number = 39
01/07/2022 12:53:23 - INFO - __main__ -   Batch Number = 40
Evaluating:  25%|       | 40/160 [00:00<00:02, 44.36it/s]01/07/2022 12:53:23 - INFO - __main__ -   Batch Number = 41
01/07/2022 12:53:23 - INFO - __main__ -   Batch Number = 42
01/07/2022 12:53:23 - INFO - __main__ -   Batch Number = 43
01/07/2022 12:53:23 - INFO - __main__ -   Batch Number = 44
01/07/2022 12:53:23 - INFO - __main__ -   Batch Number = 45
Evaluating:  28%|       | 45/160 [00:01<00:02, 44.43it/s]01/07/2022 12:53:23 - INFO - __main__ -   Batch Number = 46
01/07/2022 12:53:23 - INFO - __main__ -   Batch Number = 47
01/07/2022 12:53:23 - INFO - __main__ -   Batch Number = 48
01/07/2022 12:53:23 - INFO - __main__ -   Batch Number = 49
01/07/2022 12:53:23 - INFO - __main__ -   Batch Number = 50
Evaluating:  31%|      | 50/160 [00:01<00:02, 44.30it/s]01/07/2022 12:53:23 - INFO - __main__ -   Batch Number = 51
01/07/2022 12:53:23 - INFO - __main__ -   Batch Number = 52
01/07/2022 12:53:23 - INFO - __main__ -   Batch Number = 53
01/07/2022 12:53:23 - INFO - __main__ -   Batch Number = 54
01/07/2022 12:53:23 - INFO - __main__ -   Batch Number = 55
Evaluating:  34%|      | 55/160 [00:01<00:02, 44.29it/s]01/07/2022 12:53:23 - INFO - __main__ -   Batch Number = 56
01/07/2022 12:53:23 - INFO - __main__ -   Batch Number = 57
01/07/2022 12:53:23 - INFO - __main__ -   Batch Number = 58
01/07/2022 12:53:23 - INFO - __main__ -   Batch Number = 59
01/07/2022 12:53:23 - INFO - __main__ -   Batch Number = 60
Evaluating:  38%|      | 60/160 [00:01<00:02, 44.16it/s]01/07/2022 12:53:23 - INFO - __main__ -   Batch Number = 61
01/07/2022 12:53:23 - INFO - __main__ -   Batch Number = 62
01/07/2022 12:53:23 - INFO - __main__ -   Batch Number = 63
01/07/2022 12:53:23 - INFO - __main__ -   Batch Number = 64
01/07/2022 12:53:23 - INFO - __main__ -   Batch Number = 65
Evaluating:  41%|      | 65/160 [00:01<00:02, 43.84it/s]01/07/2022 12:53:23 - INFO - __main__ -   Batch Number = 66
01/07/2022 12:53:23 - INFO - __main__ -   Batch Number = 67
01/07/2022 12:53:23 - INFO - __main__ -   Batch Number = 68
01/07/2022 12:53:23 - INFO - __main__ -   Batch Number = 69
01/07/2022 12:53:23 - INFO - __main__ -   Batch Number = 70
Evaluating:  44%|     | 70/160 [00:01<00:02, 43.66it/s]01/07/2022 12:53:23 - INFO - __main__ -   Batch Number = 71
01/07/2022 12:53:23 - INFO - __main__ -   Batch Number = 72
01/07/2022 12:53:24 - INFO - __main__ -   Batch Number = 73
01/07/2022 12:53:24 - INFO - __main__ -   Batch Number = 74
01/07/2022 12:53:24 - INFO - __main__ -   Batch Number = 75
Evaluating:  47%|     | 75/160 [00:01<00:01, 43.97it/s]01/07/2022 12:53:24 - INFO - __main__ -   Batch Number = 76
01/07/2022 12:53:24 - INFO - __main__ -   Batch Number = 77
01/07/2022 12:53:24 - INFO - __main__ -   Batch Number = 78
01/07/2022 12:53:24 - INFO - __main__ -   Batch Number = 79
01/07/2022 12:53:24 - INFO - __main__ -   Batch Number = 80
Evaluating:  50%|     | 80/160 [00:01<00:01, 43.92it/s]01/07/2022 12:53:24 - INFO - __main__ -   Batch Number = 81
01/07/2022 12:53:24 - INFO - __main__ -   Batch Number = 82
01/07/2022 12:53:24 - INFO - __main__ -   Batch Number = 83
01/07/2022 12:53:24 - INFO - __main__ -   Batch Number = 84
01/07/2022 12:53:24 - INFO - __main__ -   Batch Number = 85
Evaluating:  53%|    | 85/160 [00:01<00:01, 43.77it/s]01/07/2022 12:53:24 - INFO - __main__ -   Batch Number = 86
01/07/2022 12:53:24 - INFO - __main__ -   Batch Number = 87
01/07/2022 12:53:24 - INFO - __main__ -   Batch Number = 88
01/07/2022 12:53:24 - INFO - __main__ -   Batch Number = 89
01/07/2022 12:53:24 - INFO - __main__ -   Batch Number = 90
Evaluating:  56%|    | 90/160 [00:02<00:01, 43.67it/s]01/07/2022 12:53:24 - INFO - __main__ -   Batch Number = 91
01/07/2022 12:53:24 - INFO - __main__ -   Batch Number = 92
01/07/2022 12:53:24 - INFO - __main__ -   Batch Number = 93
01/07/2022 12:53:24 - INFO - __main__ -   Batch Number = 94
01/07/2022 12:53:24 - INFO - __main__ -   Batch Number = 95
Evaluating:  59%|    | 95/160 [00:02<00:01, 43.47it/s]01/07/2022 12:53:24 - INFO - __main__ -   Batch Number = 96
01/07/2022 12:53:24 - INFO - __main__ -   Batch Number = 97
01/07/2022 12:53:24 - INFO - __main__ -   Batch Number = 98
01/07/2022 12:53:24 - INFO - __main__ -   Batch Number = 99
01/07/2022 12:53:24 - INFO - __main__ -   Batch Number = 100
Evaluating:  62%|   | 100/160 [00:02<00:01, 43.71it/s]01/07/2022 12:53:24 - INFO - __main__ -   Batch Number = 101
01/07/2022 12:53:24 - INFO - __main__ -   Batch Number = 102
01/07/2022 12:53:24 - INFO - __main__ -   Batch Number = 103
01/07/2022 12:53:24 - INFO - __main__ -   Batch Number = 104
01/07/2022 12:53:24 - INFO - __main__ -   Batch Number = 105
Evaluating:  66%|   | 105/160 [00:02<00:01, 43.90it/s]01/07/2022 12:53:24 - INFO - __main__ -   Batch Number = 106
01/07/2022 12:53:24 - INFO - __main__ -   Batch Number = 107
01/07/2022 12:53:24 - INFO - __main__ -   Batch Number = 108
01/07/2022 12:53:24 - INFO - __main__ -   Batch Number = 109
01/07/2022 12:53:24 - INFO - __main__ -   Batch Number = 110
Evaluating:  69%|   | 110/160 [00:02<00:01, 43.82it/s]01/07/2022 12:53:24 - INFO - __main__ -   Batch Number = 111
01/07/2022 12:53:24 - INFO - __main__ -   Batch Number = 112
01/07/2022 12:53:24 - INFO - __main__ -   Batch Number = 113
01/07/2022 12:53:24 - INFO - __main__ -   Batch Number = 114
01/07/2022 12:53:24 - INFO - __main__ -   Batch Number = 115
Evaluating:  72%|  | 115/160 [00:02<00:01, 44.00it/s]01/07/2022 12:53:24 - INFO - __main__ -   Batch Number = 116
01/07/2022 12:53:25 - INFO - __main__ -   Batch Number = 117
01/07/2022 12:53:25 - INFO - __main__ -   Batch Number = 118
01/07/2022 12:53:25 - INFO - __main__ -   Batch Number = 119
01/07/2022 12:53:25 - INFO - __main__ -   Batch Number = 120
Evaluating:  75%|  | 120/160 [00:02<00:00, 44.07it/s]01/07/2022 12:53:25 - INFO - __main__ -   Batch Number = 121
01/07/2022 12:53:25 - INFO - __main__ -   Batch Number = 122
01/07/2022 12:53:25 - INFO - __main__ -   Batch Number = 123
01/07/2022 12:53:25 - INFO - __main__ -   Batch Number = 124
01/07/2022 12:53:25 - INFO - __main__ -   Batch Number = 125
Evaluating:  78%|  | 125/160 [00:02<00:00, 44.00it/s]01/07/2022 12:53:25 - INFO - __main__ -   Batch Number = 126
01/07/2022 12:53:25 - INFO - __main__ -   Batch Number = 127
01/07/2022 12:53:25 - INFO - __main__ -   Batch Number = 128
01/07/2022 12:53:25 - INFO - __main__ -   Batch Number = 129
01/07/2022 12:53:25 - INFO - __main__ -   Batch Number = 130
Evaluating:  81%| | 130/160 [00:02<00:00, 44.12it/s]01/07/2022 12:53:25 - INFO - __main__ -   Batch Number = 131
01/07/2022 12:53:25 - INFO - __main__ -   Batch Number = 132
01/07/2022 12:53:25 - INFO - __main__ -   Batch Number = 133
01/07/2022 12:53:25 - INFO - __main__ -   Batch Number = 134
01/07/2022 12:53:25 - INFO - __main__ -   Batch Number = 135
Evaluating:  84%| | 135/160 [00:03<00:00, 44.13it/s]01/07/2022 12:53:25 - INFO - __main__ -   Batch Number = 136
01/07/2022 12:53:25 - INFO - __main__ -   Batch Number = 137
01/07/2022 12:53:25 - INFO - __main__ -   Batch Number = 138
01/07/2022 12:53:25 - INFO - __main__ -   Batch Number = 139
01/07/2022 12:53:25 - INFO - __main__ -   Batch Number = 140
Evaluating:  88%| | 140/160 [00:03<00:00, 43.93it/s]01/07/2022 12:53:25 - INFO - __main__ -   Batch Number = 141
01/07/2022 12:53:25 - INFO - __main__ -   Batch Number = 142
01/07/2022 12:53:25 - INFO - __main__ -   Batch Number = 143
01/07/2022 12:53:25 - INFO - __main__ -   Batch Number = 144
01/07/2022 12:53:25 - INFO - __main__ -   Batch Number = 145
Evaluating:  91%| | 145/160 [00:03<00:00, 44.03it/s]01/07/2022 12:53:25 - INFO - __main__ -   Batch Number = 146
01/07/2022 12:53:25 - INFO - __main__ -   Batch Number = 147
01/07/2022 12:53:25 - INFO - __main__ -   Batch Number = 148
01/07/2022 12:53:25 - INFO - __main__ -   Batch Number = 149
01/07/2022 12:53:25 - INFO - __main__ -   Batch Number = 150
Evaluating:  94%|| 150/160 [00:03<00:00, 44.07it/s]01/07/2022 12:53:25 - INFO - __main__ -   Batch Number = 151
01/07/2022 12:53:25 - INFO - __main__ -   Batch Number = 152
01/07/2022 12:53:25 - INFO - __main__ -   Batch Number = 153
01/07/2022 12:53:25 - INFO - __main__ -   Batch Number = 154
01/07/2022 12:53:25 - INFO - __main__ -   Batch Number = 155
Evaluating:  97%|| 155/160 [00:03<00:00, 44.00it/s]01/07/2022 12:53:25 - INFO - __main__ -   Batch Number = 156
01/07/2022 12:53:25 - INFO - __main__ -   Batch Number = 157
01/07/2022 12:53:25 - INFO - __main__ -   Batch Number = 158
01/07/2022 12:53:25 - INFO - __main__ -   Batch Number = 159
01/07/2022 12:53:25 - INFO - __main__ -   Batch Number = 160
Evaluating: 100%|| 160/160 [00:03<00:00, 44.73it/s]Evaluating: 100%|| 160/160 [00:03<00:00, 44.14it/s]
01/07/2022 12:53:26 - INFO - __main__ -     Evaluation done in total 3.625404 secs (0.002846 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/predictions_tr_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/nbest_predictions_tr_.json
01/07/2022 12:53:29 - INFO - __main__ -   Results = OrderedDict([('exact', 52.94117647058823), ('f1', 69.28494459320727), ('total', 1190), ('HasAns_exact', 52.94117647058823), ('HasAns_f1', 69.28494459320727), ('HasAns_total', 1190), ('best_exact', 52.94117647058823), ('best_exact_thresh', 0.0), ('best_f1', 69.28494459320727), ('best_f1_thresh', 0.0)])
01/07/2022 12:53:29 - INFO - __main__ -   Language adapter for vi not found, using en instead
01/07/2022 12:53:29 - INFO - __main__ -   Set active language adapter to en
01/07/2022 12:53:29 - INFO - __main__ -   Args Adapter Weight = None
01/07/2022 12:53:29 - INFO - __main__ -   Adapter Languages = ['en']
01/07/2022 12:53:29 - INFO - __main__ -   Predict File = xquad.vi.json
01/07/2022 12:53:29 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
  0%|          | 0/48 [00:00<?, ?it/s] 33%|      | 16/48 [00:00<00:00, 135.82it/s] 62%|   | 30/48 [00:00<00:00, 129.70it/s] 96%|| 46/48 [00:00<00:00, 140.77it/s]100%|| 48/48 [00:00<00:00, 139.49it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<06:26,  3.07it/s]convert squad examples to features:  30%|       | 353/1190 [00:00<00:00, 947.32it/s]convert squad examples to features:  42%|     | 497/1190 [00:00<00:00, 754.30it/s]convert squad examples to features:  73%|  | 865/1190 [00:00<00:00, 1357.92it/s]convert squad examples to features:  97%|| 1153/1190 [00:00<00:00, 1702.96it/s]convert squad examples to features: 100%|| 1190/1190 [00:00<00:00, 1266.10it/s]/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|| 1190/1190 [00:00<00:00, 685419.08it/s]
01/07/2022 12:53:31 - INFO - __main__ -   Local Rank = -1
01/07/2022 12:53:31 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.vi.json_xlm-roberta-base_384_vi
01/07/2022 12:53:32 - INFO - __main__ -   ***** Running evaluation  *****
01/07/2022 12:53:32 - INFO - __main__ -     Num examples = 1314
01/07/2022 12:53:32 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/165 [00:00<?, ?it/s]01/07/2022 12:53:32 - INFO - __main__ -   Batch Number = 1
01/07/2022 12:53:32 - INFO - __main__ -   Batch Number = 2
01/07/2022 12:53:32 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|         | 3/165 [00:00<00:05, 28.86it/s]01/07/2022 12:53:32 - INFO - __main__ -   Batch Number = 4
01/07/2022 12:53:32 - INFO - __main__ -   Batch Number = 5
01/07/2022 12:53:32 - INFO - __main__ -   Batch Number = 6
Evaluating:   4%|         | 6/165 [00:00<00:05, 29.09it/s]01/07/2022 12:53:32 - INFO - __main__ -   Batch Number = 7
01/07/2022 12:53:32 - INFO - __main__ -   Batch Number = 8
01/07/2022 12:53:32 - INFO - __main__ -   Batch Number = 9
01/07/2022 12:53:32 - INFO - __main__ -   Batch Number = 10
01/07/2022 12:53:32 - INFO - __main__ -   Batch Number = 11
Evaluating:   7%|         | 11/165 [00:00<00:04, 35.94it/s]01/07/2022 12:53:32 - INFO - __main__ -   Batch Number = 12
01/07/2022 12:53:32 - INFO - __main__ -   Batch Number = 13
01/07/2022 12:53:32 - INFO - __main__ -   Batch Number = 14
01/07/2022 12:53:32 - INFO - __main__ -   Batch Number = 15
01/07/2022 12:53:32 - INFO - __main__ -   Batch Number = 16
Evaluating:  10%|         | 16/165 [00:00<00:03, 39.11it/s]01/07/2022 12:53:32 - INFO - __main__ -   Batch Number = 17
01/07/2022 12:53:32 - INFO - __main__ -   Batch Number = 18
01/07/2022 12:53:32 - INFO - __main__ -   Batch Number = 19
01/07/2022 12:53:32 - INFO - __main__ -   Batch Number = 20
01/07/2022 12:53:32 - INFO - __main__ -   Batch Number = 21
Evaluating:  13%|        | 21/165 [00:00<00:03, 40.46it/s]01/07/2022 12:53:32 - INFO - __main__ -   Batch Number = 22
01/07/2022 12:53:32 - INFO - __main__ -   Batch Number = 23
01/07/2022 12:53:33 - INFO - __main__ -   Batch Number = 24
01/07/2022 12:53:33 - INFO - __main__ -   Batch Number = 25
01/07/2022 12:53:33 - INFO - __main__ -   Batch Number = 26
Evaluating:  16%|        | 26/165 [00:00<00:03, 41.48it/s]01/07/2022 12:53:33 - INFO - __main__ -   Batch Number = 27
01/07/2022 12:53:33 - INFO - __main__ -   Batch Number = 28
01/07/2022 12:53:33 - INFO - __main__ -   Batch Number = 29
01/07/2022 12:53:33 - INFO - __main__ -   Batch Number = 30
01/07/2022 12:53:33 - INFO - __main__ -   Batch Number = 31
Evaluating:  19%|        | 31/165 [00:00<00:03, 42.17it/s]01/07/2022 12:53:33 - INFO - __main__ -   Batch Number = 32
01/07/2022 12:53:33 - INFO - __main__ -   Batch Number = 33
01/07/2022 12:53:33 - INFO - __main__ -   Batch Number = 34
01/07/2022 12:53:33 - INFO - __main__ -   Batch Number = 35
01/07/2022 12:53:33 - INFO - __main__ -   Batch Number = 36
Evaluating:  22%|       | 36/165 [00:00<00:03, 42.46it/s]01/07/2022 12:53:33 - INFO - __main__ -   Batch Number = 37
01/07/2022 12:53:33 - INFO - __main__ -   Batch Number = 38
01/07/2022 12:53:33 - INFO - __main__ -   Batch Number = 39
01/07/2022 12:53:33 - INFO - __main__ -   Batch Number = 40
01/07/2022 12:53:33 - INFO - __main__ -   Batch Number = 41
Evaluating:  25%|       | 41/165 [00:01<00:02, 42.70it/s]01/07/2022 12:53:33 - INFO - __main__ -   Batch Number = 42
01/07/2022 12:53:33 - INFO - __main__ -   Batch Number = 43
01/07/2022 12:53:33 - INFO - __main__ -   Batch Number = 44
01/07/2022 12:53:33 - INFO - __main__ -   Batch Number = 45
01/07/2022 12:53:33 - INFO - __main__ -   Batch Number = 46
Evaluating:  28%|       | 46/165 [00:01<00:02, 42.73it/s]01/07/2022 12:53:33 - INFO - __main__ -   Batch Number = 47
01/07/2022 12:53:33 - INFO - __main__ -   Batch Number = 48
01/07/2022 12:53:33 - INFO - __main__ -   Batch Number = 49
01/07/2022 12:53:33 - INFO - __main__ -   Batch Number = 50
01/07/2022 12:53:33 - INFO - __main__ -   Batch Number = 51
Evaluating:  31%|       | 51/165 [00:01<00:02, 42.51it/s]01/07/2022 12:53:33 - INFO - __main__ -   Batch Number = 52
01/07/2022 12:53:33 - INFO - __main__ -   Batch Number = 53
01/07/2022 12:53:33 - INFO - __main__ -   Batch Number = 54
01/07/2022 12:53:33 - INFO - __main__ -   Batch Number = 55
01/07/2022 12:53:33 - INFO - __main__ -   Batch Number = 56
Evaluating:  34%|      | 56/165 [00:01<00:02, 42.66it/s]01/07/2022 12:53:33 - INFO - __main__ -   Batch Number = 57
01/07/2022 12:53:33 - INFO - __main__ -   Batch Number = 58
01/07/2022 12:53:33 - INFO - __main__ -   Batch Number = 59
01/07/2022 12:53:33 - INFO - __main__ -   Batch Number = 60
01/07/2022 12:53:33 - INFO - __main__ -   Batch Number = 61
Evaluating:  37%|      | 61/165 [00:01<00:02, 42.64it/s]01/07/2022 12:53:33 - INFO - __main__ -   Batch Number = 62
01/07/2022 12:53:33 - INFO - __main__ -   Batch Number = 63
01/07/2022 12:53:33 - INFO - __main__ -   Batch Number = 64
01/07/2022 12:53:33 - INFO - __main__ -   Batch Number = 65
01/07/2022 12:53:33 - INFO - __main__ -   Batch Number = 66
Evaluating:  40%|      | 66/165 [00:01<00:02, 42.60it/s]01/07/2022 12:53:34 - INFO - __main__ -   Batch Number = 67
01/07/2022 12:53:34 - INFO - __main__ -   Batch Number = 68
01/07/2022 12:53:34 - INFO - __main__ -   Batch Number = 69
01/07/2022 12:53:34 - INFO - __main__ -   Batch Number = 70
01/07/2022 12:53:34 - INFO - __main__ -   Batch Number = 71
Evaluating:  43%|     | 71/165 [00:01<00:02, 42.77it/s]01/07/2022 12:53:34 - INFO - __main__ -   Batch Number = 72
01/07/2022 12:53:34 - INFO - __main__ -   Batch Number = 73
01/07/2022 12:53:34 - INFO - __main__ -   Batch Number = 74
01/07/2022 12:53:34 - INFO - __main__ -   Batch Number = 75
01/07/2022 12:53:34 - INFO - __main__ -   Batch Number = 76
Evaluating:  46%|     | 76/165 [00:01<00:02, 42.91it/s]01/07/2022 12:53:34 - INFO - __main__ -   Batch Number = 77
01/07/2022 12:53:34 - INFO - __main__ -   Batch Number = 78
01/07/2022 12:53:34 - INFO - __main__ -   Batch Number = 79
01/07/2022 12:53:34 - INFO - __main__ -   Batch Number = 80
01/07/2022 12:53:34 - INFO - __main__ -   Batch Number = 81
Evaluating:  49%|     | 81/165 [00:01<00:02, 40.03it/s]01/07/2022 12:53:34 - INFO - __main__ -   Batch Number = 82
01/07/2022 12:53:34 - INFO - __main__ -   Batch Number = 83
01/07/2022 12:53:34 - INFO - __main__ -   Batch Number = 84
01/07/2022 12:53:34 - INFO - __main__ -   Batch Number = 85
01/07/2022 12:53:34 - INFO - __main__ -   Batch Number = 86
Evaluating:  52%|    | 86/165 [00:02<00:01, 40.94it/s]01/07/2022 12:53:34 - INFO - __main__ -   Batch Number = 87
01/07/2022 12:53:34 - INFO - __main__ -   Batch Number = 88
01/07/2022 12:53:34 - INFO - __main__ -   Batch Number = 89
01/07/2022 12:53:34 - INFO - __main__ -   Batch Number = 90
01/07/2022 12:53:34 - INFO - __main__ -   Batch Number = 91
Evaluating:  55%|    | 91/165 [00:02<00:01, 41.57it/s]01/07/2022 12:53:34 - INFO - __main__ -   Batch Number = 92
01/07/2022 12:53:34 - INFO - __main__ -   Batch Number = 93
01/07/2022 12:53:34 - INFO - __main__ -   Batch Number = 94
01/07/2022 12:53:34 - INFO - __main__ -   Batch Number = 95
01/07/2022 12:53:34 - INFO - __main__ -   Batch Number = 96
Evaluating:  58%|    | 96/165 [00:02<00:01, 41.97it/s]01/07/2022 12:53:34 - INFO - __main__ -   Batch Number = 97
01/07/2022 12:53:34 - INFO - __main__ -   Batch Number = 98
01/07/2022 12:53:34 - INFO - __main__ -   Batch Number = 99
01/07/2022 12:53:34 - INFO - __main__ -   Batch Number = 100
01/07/2022 12:53:34 - INFO - __main__ -   Batch Number = 101
Evaluating:  61%|    | 101/165 [00:02<00:01, 42.26it/s]01/07/2022 12:53:34 - INFO - __main__ -   Batch Number = 102
01/07/2022 12:53:34 - INFO - __main__ -   Batch Number = 103
01/07/2022 12:53:34 - INFO - __main__ -   Batch Number = 104
01/07/2022 12:53:34 - INFO - __main__ -   Batch Number = 105
01/07/2022 12:53:34 - INFO - __main__ -   Batch Number = 106
Evaluating:  64%|   | 106/165 [00:02<00:01, 42.24it/s]01/07/2022 12:53:34 - INFO - __main__ -   Batch Number = 107
01/07/2022 12:53:34 - INFO - __main__ -   Batch Number = 108
01/07/2022 12:53:35 - INFO - __main__ -   Batch Number = 109
01/07/2022 12:53:35 - INFO - __main__ -   Batch Number = 110
01/07/2022 12:53:35 - INFO - __main__ -   Batch Number = 111
Evaluating:  67%|   | 111/165 [00:02<00:01, 42.19it/s]01/07/2022 12:53:35 - INFO - __main__ -   Batch Number = 112
01/07/2022 12:53:35 - INFO - __main__ -   Batch Number = 113
01/07/2022 12:53:35 - INFO - __main__ -   Batch Number = 114
01/07/2022 12:53:35 - INFO - __main__ -   Batch Number = 115
01/07/2022 12:53:35 - INFO - __main__ -   Batch Number = 116
Evaluating:  70%|   | 116/165 [00:02<00:01, 42.39it/s]01/07/2022 12:53:35 - INFO - __main__ -   Batch Number = 117
01/07/2022 12:53:35 - INFO - __main__ -   Batch Number = 118
01/07/2022 12:53:35 - INFO - __main__ -   Batch Number = 119
01/07/2022 12:53:35 - INFO - __main__ -   Batch Number = 120
01/07/2022 12:53:35 - INFO - __main__ -   Batch Number = 121
Evaluating:  73%|  | 121/165 [00:02<00:01, 42.49it/s]01/07/2022 12:53:35 - INFO - __main__ -   Batch Number = 122
01/07/2022 12:53:35 - INFO - __main__ -   Batch Number = 123
01/07/2022 12:53:35 - INFO - __main__ -   Batch Number = 124
01/07/2022 12:53:35 - INFO - __main__ -   Batch Number = 125
01/07/2022 12:53:35 - INFO - __main__ -   Batch Number = 126
Evaluating:  76%|  | 126/165 [00:03<00:00, 42.54it/s]01/07/2022 12:53:35 - INFO - __main__ -   Batch Number = 127
01/07/2022 12:53:35 - INFO - __main__ -   Batch Number = 128
01/07/2022 12:53:35 - INFO - __main__ -   Batch Number = 129
01/07/2022 12:53:35 - INFO - __main__ -   Batch Number = 130
01/07/2022 12:53:35 - INFO - __main__ -   Batch Number = 131
Evaluating:  79%|  | 131/165 [00:03<00:00, 42.56it/s]01/07/2022 12:53:35 - INFO - __main__ -   Batch Number = 132
01/07/2022 12:53:35 - INFO - __main__ -   Batch Number = 133
01/07/2022 12:53:35 - INFO - __main__ -   Batch Number = 134
01/07/2022 12:53:35 - INFO - __main__ -   Batch Number = 135
01/07/2022 12:53:35 - INFO - __main__ -   Batch Number = 136
Evaluating:  82%| | 136/165 [00:03<00:00, 42.47it/s]01/07/2022 12:53:35 - INFO - __main__ -   Batch Number = 137
01/07/2022 12:53:35 - INFO - __main__ -   Batch Number = 138
01/07/2022 12:53:35 - INFO - __main__ -   Batch Number = 139
01/07/2022 12:53:35 - INFO - __main__ -   Batch Number = 140
01/07/2022 12:53:35 - INFO - __main__ -   Batch Number = 141
Evaluating:  85%| | 141/165 [00:03<00:00, 42.32it/s]01/07/2022 12:53:35 - INFO - __main__ -   Batch Number = 142
01/07/2022 12:53:35 - INFO - __main__ -   Batch Number = 143
01/07/2022 12:53:35 - INFO - __main__ -   Batch Number = 144
01/07/2022 12:53:35 - INFO - __main__ -   Batch Number = 145
01/07/2022 12:53:35 - INFO - __main__ -   Batch Number = 146
Evaluating:  88%| | 146/165 [00:03<00:00, 42.26it/s]01/07/2022 12:53:35 - INFO - __main__ -   Batch Number = 147
01/07/2022 12:53:35 - INFO - __main__ -   Batch Number = 148
01/07/2022 12:53:35 - INFO - __main__ -   Batch Number = 149
01/07/2022 12:53:35 - INFO - __main__ -   Batch Number = 150
01/07/2022 12:53:36 - INFO - __main__ -   Batch Number = 151
Evaluating:  92%|| 151/165 [00:03<00:00, 41.73it/s]01/07/2022 12:53:36 - INFO - __main__ -   Batch Number = 152
01/07/2022 12:53:36 - INFO - __main__ -   Batch Number = 153
01/07/2022 12:53:36 - INFO - __main__ -   Batch Number = 154
01/07/2022 12:53:36 - INFO - __main__ -   Batch Number = 155
01/07/2022 12:53:36 - INFO - __main__ -   Batch Number = 156
Evaluating:  95%|| 156/165 [00:03<00:00, 40.96it/s]01/07/2022 12:53:36 - INFO - __main__ -   Batch Number = 157
01/07/2022 12:53:36 - INFO - __main__ -   Batch Number = 158
01/07/2022 12:53:36 - INFO - __main__ -   Batch Number = 159
01/07/2022 12:53:36 - INFO - __main__ -   Batch Number = 160
01/07/2022 12:53:36 - INFO - __main__ -   Batch Number = 161
Evaluating:  98%|| 161/165 [00:03<00:00, 41.84it/s]01/07/2022 12:53:36 - INFO - __main__ -   Batch Number = 162
01/07/2022 12:53:36 - INFO - __main__ -   Batch Number = 163
01/07/2022 12:53:36 - INFO - __main__ -   Batch Number = 164
01/07/2022 12:53:36 - INFO - __main__ -   Batch Number = 165
Evaluating: 100%|| 165/165 [00:03<00:00, 41.71it/s]
01/07/2022 12:53:36 - INFO - __main__ -     Evaluation done in total 3.956111 secs (0.003011 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/predictions_vi_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/nbest_predictions_vi_.json
01/07/2022 12:53:39 - INFO - __main__ -   Results = OrderedDict([('exact', 53.02521008403362), ('f1', 72.51952195511345), ('total', 1190), ('HasAns_exact', 53.02521008403362), ('HasAns_f1', 72.51952195511345), ('HasAns_total', 1190), ('best_exact', 53.02521008403362), ('best_exact_thresh', 0.0), ('best_f1', 72.51952195511345), ('best_f1_thresh', 0.0)])
01/07/2022 12:53:39 - INFO - __main__ -   Language adapter for zh not found, using en instead
01/07/2022 12:53:39 - INFO - __main__ -   Set active language adapter to en
01/07/2022 12:53:39 - INFO - __main__ -   Args Adapter Weight = None
01/07/2022 12:53:39 - INFO - __main__ -   Adapter Languages = ['en']
01/07/2022 12:53:39 - INFO - __main__ -   Predict File = xquad.zh.json
01/07/2022 12:53:39 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
  0%|          | 0/48 [00:00<?, ?it/s] 75%|  | 36/48 [00:00<00:00, 349.83it/s]100%|| 48/48 [00:00<00:00, 369.00it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<03:52,  5.11it/s]convert squad examples to features:  43%|     | 513/1190 [00:00<00:00, 2130.14it/s]convert squad examples to features:  97%|| 1153/1190 [00:00<00:00, 3648.13it/s]convert squad examples to features: 100%|| 1190/1190 [00:00<00:00, 2937.64it/s]/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|| 1190/1190 [00:00<00:00, 633323.41it/s]
01/07/2022 12:53:40 - INFO - __main__ -   Local Rank = -1
01/07/2022 12:53:40 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.zh.json_xlm-roberta-base_384_zh
01/07/2022 12:53:41 - INFO - __main__ -   ***** Running evaluation  *****
01/07/2022 12:53:41 - INFO - __main__ -     Num examples = 1246
01/07/2022 12:53:41 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/156 [00:00<?, ?it/s]01/07/2022 12:53:41 - INFO - __main__ -   Batch Number = 1
01/07/2022 12:53:41 - INFO - __main__ -   Batch Number = 2
01/07/2022 12:53:41 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|         | 3/156 [00:00<00:05, 29.21it/s]01/07/2022 12:53:41 - INFO - __main__ -   Batch Number = 4
01/07/2022 12:53:41 - INFO - __main__ -   Batch Number = 5
01/07/2022 12:53:41 - INFO - __main__ -   Batch Number = 6
Evaluating:   4%|         | 6/156 [00:00<00:05, 29.61it/s]01/07/2022 12:53:41 - INFO - __main__ -   Batch Number = 7
01/07/2022 12:53:41 - INFO - __main__ -   Batch Number = 8
01/07/2022 12:53:41 - INFO - __main__ -   Batch Number = 9
01/07/2022 12:53:41 - INFO - __main__ -   Batch Number = 10
Evaluating:   6%|         | 10/156 [00:00<00:04, 33.13it/s]01/07/2022 12:53:41 - INFO - __main__ -   Batch Number = 11
01/07/2022 12:53:41 - INFO - __main__ -   Batch Number = 12
01/07/2022 12:53:42 - INFO - __main__ -   Batch Number = 13
01/07/2022 12:53:42 - INFO - __main__ -   Batch Number = 14
01/07/2022 12:53:42 - INFO - __main__ -   Batch Number = 15
Evaluating:  10%|         | 15/156 [00:00<00:03, 37.83it/s]01/07/2022 12:53:42 - INFO - __main__ -   Batch Number = 16
01/07/2022 12:53:42 - INFO - __main__ -   Batch Number = 17
01/07/2022 12:53:42 - INFO - __main__ -   Batch Number = 18
01/07/2022 12:53:42 - INFO - __main__ -   Batch Number = 19
01/07/2022 12:53:42 - INFO - __main__ -   Batch Number = 20
Evaluating:  13%|        | 20/156 [00:00<00:03, 40.16it/s]01/07/2022 12:53:42 - INFO - __main__ -   Batch Number = 21
01/07/2022 12:53:42 - INFO - __main__ -   Batch Number = 22
01/07/2022 12:53:42 - INFO - __main__ -   Batch Number = 23
01/07/2022 12:53:42 - INFO - __main__ -   Batch Number = 24
01/07/2022 12:53:42 - INFO - __main__ -   Batch Number = 25
Evaluating:  16%|        | 25/156 [00:00<00:03, 41.66it/s]01/07/2022 12:53:42 - INFO - __main__ -   Batch Number = 26
01/07/2022 12:53:42 - INFO - __main__ -   Batch Number = 27
01/07/2022 12:53:42 - INFO - __main__ -   Batch Number = 28
01/07/2022 12:53:42 - INFO - __main__ -   Batch Number = 29
01/07/2022 12:53:42 - INFO - __main__ -   Batch Number = 30
Evaluating:  19%|        | 30/156 [00:00<00:02, 42.72it/s]01/07/2022 12:53:42 - INFO - __main__ -   Batch Number = 31
01/07/2022 12:53:42 - INFO - __main__ -   Batch Number = 32
01/07/2022 12:53:42 - INFO - __main__ -   Batch Number = 33
01/07/2022 12:53:42 - INFO - __main__ -   Batch Number = 34
01/07/2022 12:53:42 - INFO - __main__ -   Batch Number = 35
Evaluating:  22%|       | 35/156 [00:00<00:02, 43.20it/s]01/07/2022 12:53:42 - INFO - __main__ -   Batch Number = 36
01/07/2022 12:53:42 - INFO - __main__ -   Batch Number = 37
01/07/2022 12:53:42 - INFO - __main__ -   Batch Number = 38
01/07/2022 12:53:42 - INFO - __main__ -   Batch Number = 39
01/07/2022 12:53:42 - INFO - __main__ -   Batch Number = 40
Evaluating:  26%|       | 40/156 [00:00<00:02, 43.76it/s]01/07/2022 12:53:42 - INFO - __main__ -   Batch Number = 41
01/07/2022 12:53:42 - INFO - __main__ -   Batch Number = 42
01/07/2022 12:53:42 - INFO - __main__ -   Batch Number = 43
01/07/2022 12:53:42 - INFO - __main__ -   Batch Number = 44
01/07/2022 12:53:42 - INFO - __main__ -   Batch Number = 45
Evaluating:  29%|       | 45/156 [00:01<00:02, 43.99it/s]01/07/2022 12:53:42 - INFO - __main__ -   Batch Number = 46
01/07/2022 12:53:42 - INFO - __main__ -   Batch Number = 47
01/07/2022 12:53:42 - INFO - __main__ -   Batch Number = 48
01/07/2022 12:53:42 - INFO - __main__ -   Batch Number = 49
01/07/2022 12:53:42 - INFO - __main__ -   Batch Number = 50
Evaluating:  32%|      | 50/156 [00:01<00:02, 44.15it/s]01/07/2022 12:53:42 - INFO - __main__ -   Batch Number = 51
01/07/2022 12:53:42 - INFO - __main__ -   Batch Number = 52
01/07/2022 12:53:42 - INFO - __main__ -   Batch Number = 53
01/07/2022 12:53:42 - INFO - __main__ -   Batch Number = 54
01/07/2022 12:53:42 - INFO - __main__ -   Batch Number = 55
Evaluating:  35%|      | 55/156 [00:01<00:02, 44.28it/s]01/07/2022 12:53:42 - INFO - __main__ -   Batch Number = 56
01/07/2022 12:53:42 - INFO - __main__ -   Batch Number = 57
01/07/2022 12:53:43 - INFO - __main__ -   Batch Number = 58
01/07/2022 12:53:43 - INFO - __main__ -   Batch Number = 59
01/07/2022 12:53:43 - INFO - __main__ -   Batch Number = 60
Evaluating:  38%|      | 60/156 [00:01<00:02, 44.45it/s]01/07/2022 12:53:43 - INFO - __main__ -   Batch Number = 61
01/07/2022 12:53:43 - INFO - __main__ -   Batch Number = 62
01/07/2022 12:53:43 - INFO - __main__ -   Batch Number = 63
01/07/2022 12:53:43 - INFO - __main__ -   Batch Number = 64
01/07/2022 12:53:43 - INFO - __main__ -   Batch Number = 65
Evaluating:  42%|     | 65/156 [00:01<00:02, 44.38it/s]01/07/2022 12:53:43 - INFO - __main__ -   Batch Number = 66
01/07/2022 12:53:43 - INFO - __main__ -   Batch Number = 67
01/07/2022 12:53:43 - INFO - __main__ -   Batch Number = 68
01/07/2022 12:53:43 - INFO - __main__ -   Batch Number = 69
01/07/2022 12:53:43 - INFO - __main__ -   Batch Number = 70
Evaluating:  45%|     | 70/156 [00:01<00:01, 44.43it/s]01/07/2022 12:53:43 - INFO - __main__ -   Batch Number = 71
01/07/2022 12:53:43 - INFO - __main__ -   Batch Number = 72
01/07/2022 12:53:43 - INFO - __main__ -   Batch Number = 73
01/07/2022 12:53:43 - INFO - __main__ -   Batch Number = 74
01/07/2022 12:53:43 - INFO - __main__ -   Batch Number = 75
Evaluating:  48%|     | 75/156 [00:01<00:01, 44.42it/s]01/07/2022 12:53:43 - INFO - __main__ -   Batch Number = 76
01/07/2022 12:53:43 - INFO - __main__ -   Batch Number = 77
01/07/2022 12:53:43 - INFO - __main__ -   Batch Number = 78
01/07/2022 12:53:43 - INFO - __main__ -   Batch Number = 79
01/07/2022 12:53:43 - INFO - __main__ -   Batch Number = 80
Evaluating:  51%|    | 80/156 [00:01<00:01, 44.28it/s]01/07/2022 12:53:43 - INFO - __main__ -   Batch Number = 81
01/07/2022 12:53:43 - INFO - __main__ -   Batch Number = 82
01/07/2022 12:53:43 - INFO - __main__ -   Batch Number = 83
01/07/2022 12:53:43 - INFO - __main__ -   Batch Number = 84
01/07/2022 12:53:43 - INFO - __main__ -   Batch Number = 85
Evaluating:  54%|    | 85/156 [00:01<00:01, 44.37it/s]01/07/2022 12:53:43 - INFO - __main__ -   Batch Number = 86
01/07/2022 12:53:43 - INFO - __main__ -   Batch Number = 87
01/07/2022 12:53:43 - INFO - __main__ -   Batch Number = 88
01/07/2022 12:53:43 - INFO - __main__ -   Batch Number = 89
01/07/2022 12:53:43 - INFO - __main__ -   Batch Number = 90
Evaluating:  58%|    | 90/156 [00:02<00:01, 44.23it/s]01/07/2022 12:53:43 - INFO - __main__ -   Batch Number = 91
01/07/2022 12:53:43 - INFO - __main__ -   Batch Number = 92
01/07/2022 12:53:43 - INFO - __main__ -   Batch Number = 93
01/07/2022 12:53:43 - INFO - __main__ -   Batch Number = 94
01/07/2022 12:53:43 - INFO - __main__ -   Batch Number = 95
Evaluating:  61%|    | 95/156 [00:02<00:01, 44.31it/s]01/07/2022 12:53:43 - INFO - __main__ -   Batch Number = 96
01/07/2022 12:53:43 - INFO - __main__ -   Batch Number = 97
01/07/2022 12:53:43 - INFO - __main__ -   Batch Number = 98
01/07/2022 12:53:43 - INFO - __main__ -   Batch Number = 99
01/07/2022 12:53:43 - INFO - __main__ -   Batch Number = 100
Evaluating:  64%|   | 100/156 [00:02<00:01, 44.31it/s]01/07/2022 12:53:43 - INFO - __main__ -   Batch Number = 101
01/07/2022 12:53:44 - INFO - __main__ -   Batch Number = 102
01/07/2022 12:53:44 - INFO - __main__ -   Batch Number = 103
01/07/2022 12:53:44 - INFO - __main__ -   Batch Number = 104
01/07/2022 12:53:44 - INFO - __main__ -   Batch Number = 105
Evaluating:  67%|   | 105/156 [00:02<00:01, 44.46it/s]01/07/2022 12:53:44 - INFO - __main__ -   Batch Number = 106
01/07/2022 12:53:44 - INFO - __main__ -   Batch Number = 107
01/07/2022 12:53:44 - INFO - __main__ -   Batch Number = 108
01/07/2022 12:53:44 - INFO - __main__ -   Batch Number = 109
01/07/2022 12:53:44 - INFO - __main__ -   Batch Number = 110
Evaluating:  71%|   | 110/156 [00:02<00:01, 44.33it/s]01/07/2022 12:53:44 - INFO - __main__ -   Batch Number = 111
01/07/2022 12:53:44 - INFO - __main__ -   Batch Number = 112
01/07/2022 12:53:44 - INFO - __main__ -   Batch Number = 113
01/07/2022 12:53:44 - INFO - __main__ -   Batch Number = 114
01/07/2022 12:53:44 - INFO - __main__ -   Batch Number = 115
Evaluating:  74%|  | 115/156 [00:02<00:00, 44.42it/s]01/07/2022 12:53:44 - INFO - __main__ -   Batch Number = 116
01/07/2022 12:53:44 - INFO - __main__ -   Batch Number = 117
01/07/2022 12:53:44 - INFO - __main__ -   Batch Number = 118
01/07/2022 12:53:44 - INFO - __main__ -   Batch Number = 119
01/07/2022 12:53:44 - INFO - __main__ -   Batch Number = 120
Evaluating:  77%|  | 120/156 [00:02<00:00, 44.49it/s]01/07/2022 12:53:44 - INFO - __main__ -   Batch Number = 121
01/07/2022 12:53:44 - INFO - __main__ -   Batch Number = 122
01/07/2022 12:53:44 - INFO - __main__ -   Batch Number = 123
01/07/2022 12:53:44 - INFO - __main__ -   Batch Number = 124
01/07/2022 12:53:44 - INFO - __main__ -   Batch Number = 125
Evaluating:  80%|  | 125/156 [00:02<00:00, 44.37it/s]01/07/2022 12:53:44 - INFO - __main__ -   Batch Number = 126
01/07/2022 12:53:44 - INFO - __main__ -   Batch Number = 127
01/07/2022 12:53:44 - INFO - __main__ -   Batch Number = 128
01/07/2022 12:53:44 - INFO - __main__ -   Batch Number = 129
01/07/2022 12:53:44 - INFO - __main__ -   Batch Number = 130
Evaluating:  83%| | 130/156 [00:03<00:00, 44.50it/s]01/07/2022 12:53:44 - INFO - __main__ -   Batch Number = 131
01/07/2022 12:53:44 - INFO - __main__ -   Batch Number = 132
01/07/2022 12:53:44 - INFO - __main__ -   Batch Number = 133
01/07/2022 12:53:44 - INFO - __main__ -   Batch Number = 134
01/07/2022 12:53:44 - INFO - __main__ -   Batch Number = 135
Evaluating:  87%| | 135/156 [00:03<00:00, 44.45it/s]01/07/2022 12:53:44 - INFO - __main__ -   Batch Number = 136
01/07/2022 12:53:44 - INFO - __main__ -   Batch Number = 137
01/07/2022 12:53:44 - INFO - __main__ -   Batch Number = 138
01/07/2022 12:53:44 - INFO - __main__ -   Batch Number = 139
01/07/2022 12:53:44 - INFO - __main__ -   Batch Number = 140
Evaluating:  90%| | 140/156 [00:03<00:00, 42.91it/s]01/07/2022 12:53:44 - INFO - __main__ -   Batch Number = 141
01/07/2022 12:53:44 - INFO - __main__ -   Batch Number = 142
01/07/2022 12:53:44 - INFO - __main__ -   Batch Number = 143
01/07/2022 12:53:44 - INFO - __main__ -   Batch Number = 144
01/07/2022 12:53:44 - INFO - __main__ -   Batch Number = 145
Evaluating:  93%|| 145/156 [00:03<00:00, 43.44it/s]01/07/2022 12:53:45 - INFO - __main__ -   Batch Number = 146
01/07/2022 12:53:45 - INFO - __main__ -   Batch Number = 147
01/07/2022 12:53:45 - INFO - __main__ -   Batch Number = 148
01/07/2022 12:53:45 - INFO - __main__ -   Batch Number = 149
01/07/2022 12:53:45 - INFO - __main__ -   Batch Number = 150
Evaluating:  96%|| 150/156 [00:03<00:00, 43.77it/s]01/07/2022 12:53:45 - INFO - __main__ -   Batch Number = 151
01/07/2022 12:53:45 - INFO - __main__ -   Batch Number = 152
01/07/2022 12:53:45 - INFO - __main__ -   Batch Number = 153
01/07/2022 12:53:45 - INFO - __main__ -   Batch Number = 154
01/07/2022 12:53:45 - INFO - __main__ -   Batch Number = 155
Evaluating:  99%|| 155/156 [00:03<00:00, 43.90it/s]01/07/2022 12:53:45 - INFO - __main__ -   Batch Number = 156
Evaluating: 100%|| 156/156 [00:03<00:00, 43.28it/s]
01/07/2022 12:53:45 - INFO - __main__ -     Evaluation done in total 3.605219 secs (0.002893 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/predictions_zh_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/nbest_predictions_zh_.json
01/07/2022 12:54:06 - INFO - __main__ -   Results = OrderedDict([('exact', 44.20168067226891), ('f1', 52.34156692980218), ('total', 1190), ('HasAns_exact', 44.20168067226891), ('HasAns_f1', 52.34156692980218), ('HasAns_total', 1190), ('best_exact', 44.20168067226891), ('best_exact_thresh', 0.0), ('best_f1', 52.34156692980218), ('best_f1_thresh', 0.0)])
PyTorch version 1.10.0+cu111 available.
01/07/2022 12:54:08 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/07/2022 12:54:08 - INFO - root -   save model
loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /root/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6
Model config XLMRobertaConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /root/.cache/torch/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8
loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/07/2022 12:54:17 - INFO - __main__ -   lang2id = None
01/07/2022 12:54:21 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='xlm-roberta-base', model_type='xlm-roberta', config_name=None, tokenizer_name=None, cache_dir=None, data_dir='/root/Desktop/cloud-emea-copy/data//xquad', output_dir='/root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/', max_seq_length=384, train_file='/root/Desktop/cloud-emea-copy/data//xquad/train-v1.1.json', predict_file='/root/Desktop/cloud-emea-copy/data//xquad/dev-v1.1.json', eval_test_set=False, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=4, per_gpu_eval_batch_size=8, gradient_accumulation_steps=4, learning_rate=0.0001, weight_decay=0.0001, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=15.0, max_steps=-1, save_steps=1000, warmup_steps=500, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', eval_lang='en', predict_langs='ar,de,el,es,hi,ru,th,tr,vi,zh', train_lang='en', log_file='/root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='qna', threads=8, version_2_with_negative=False, verbose_logging=False, n_best_size=20, max_query_length=64, max_answer_length=30, doc_stride=128, null_score_diff_threshold=0.0, predict_task_adapter='output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s3/checkpoint-best/qna', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix=None, en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/07/2022 12:54:21 - INFO - __main__ -   Results: {}
loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /root/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6
Model config XLMRobertaConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /root/.cache/torch/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8
loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/07/2022 12:54:30 - INFO - __main__ -   lang2id = None
01/07/2022 12:54:30 - INFO - __main__ -   Evaluating the model on the test set of all languages specified
01/07/2022 12:54:30 - INFO - __main__ -   Adapter will be loaded from this path: output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s3/checkpoint-best/qna
01/07/2022 12:54:30 - INFO - root -   Trying to decide if add adapter
01/07/2022 12:54:30 - INFO - root -   loading task adapter
Loading module configuration from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s3/checkpoint-best/qna/adapter_config.json
Adding adapter 'qna' of type 'text_task'.
Loading module weights from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s3/checkpoint-best/qna/pytorch_adapter.bin
Loading module configuration from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s3/checkpoint-best/qna/head_config.json
Loading module weights from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s3/checkpoint-best/qna/pytorch_model_head.bin
01/07/2022 12:54:30 - INFO - root -   loading lang adpater en/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/xlm-roberta-base/pfeiffer/en_relu_2.zip.
Loading module configuration from /root/.cache/torch/adapters/3e9c8c800af886cd1c684a3acf4929990b35f3ac75e050e942de2ff2ecf895fe-caabba7772816577124631d5b84810c4d32f49afcea4fca637162ec7f432174f-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
Loading module weights from /root/.cache/torch/adapters/3e9c8c800af886cd1c684a3acf4929990b35f3ac75e050e942de2ff2ecf895fe-caabba7772816577124631d5b84810c4d32f49afcea4fca637162ec7f432174f-extracted/pytorch_adapter.bin
No matching prediction head found in '/root/.cache/torch/adapters/3e9c8c800af886cd1c684a3acf4929990b35f3ac75e050e942de2ff2ecf895fe-caabba7772816577124631d5b84810c4d32f49afcea4fca637162ec7f432174f-extracted'
01/07/2022 12:54:32 - INFO - __main__ -   Language adapter for ar not found, using en instead
01/07/2022 12:54:32 - INFO - __main__ -   Set active language adapter to en
01/07/2022 12:54:32 - INFO - __main__ -   Args Adapter Weight = None
01/07/2022 12:54:32 - INFO - __main__ -   Adapter Languages = ['en']
01/07/2022 12:54:32 - INFO - __main__ -   Predict File = xquad.ar.json
01/07/2022 12:54:32 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
en en/wiki@ukp
  0%|          | 0/48 [00:00<?, ?it/s] 33%|      | 16/48 [00:00<00:00, 139.23it/s] 62%|   | 30/48 [00:00<00:00, 133.01it/s] 94%|| 45/48 [00:00<00:00, 139.99it/s]100%|| 48/48 [00:00<00:00, 141.43it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<05:01,  3.94it/s]convert squad examples to features:  32%|      | 385/1190 [00:00<00:01, 638.81it/s]convert squad examples to features: 100%|| 1190/1190 [00:00<00:00, 1643.83it/s]/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|| 1190/1190 [00:00<00:00, 461679.93it/s]
01/07/2022 12:54:34 - INFO - __main__ -   Local Rank = -1
01/07/2022 12:54:34 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.ar.json_xlm-roberta-base_384_ar
01/07/2022 12:54:35 - INFO - __main__ -   ***** Running evaluation  *****
01/07/2022 12:54:35 - INFO - __main__ -     Num examples = 1318
01/07/2022 12:54:35 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/165 [00:00<?, ?it/s]01/07/2022 12:54:35 - INFO - __main__ -   Batch Number = 1
01/07/2022 12:54:35 - INFO - __main__ -   Batch Number = 2
Evaluating:   1%|          | 2/165 [00:00<00:11, 13.89it/s]01/07/2022 12:54:35 - INFO - __main__ -   Batch Number = 3
01/07/2022 12:54:35 - INFO - __main__ -   Batch Number = 4
Evaluating:   2%|         | 4/165 [00:00<00:09, 16.56it/s]01/07/2022 12:54:35 - INFO - __main__ -   Batch Number = 5
01/07/2022 12:54:35 - INFO - __main__ -   Batch Number = 6
Evaluating:   4%|         | 6/165 [00:00<00:09, 17.64it/s]01/07/2022 12:54:35 - INFO - __main__ -   Batch Number = 7
01/07/2022 12:54:35 - INFO - __main__ -   Batch Number = 8
Evaluating:   5%|         | 8/165 [00:00<00:08, 18.12it/s]01/07/2022 12:54:35 - INFO - __main__ -   Batch Number = 9
01/07/2022 12:54:36 - INFO - __main__ -   Batch Number = 10
Evaluating:   6%|         | 10/165 [00:00<00:08, 18.46it/s]01/07/2022 12:54:36 - INFO - __main__ -   Batch Number = 11
01/07/2022 12:54:36 - INFO - __main__ -   Batch Number = 12
Evaluating:   7%|         | 12/165 [00:00<00:08, 18.71it/s]01/07/2022 12:54:36 - INFO - __main__ -   Batch Number = 13
01/07/2022 12:54:36 - INFO - __main__ -   Batch Number = 14
Evaluating:   8%|         | 14/165 [00:00<00:08, 18.81it/s]01/07/2022 12:54:36 - INFO - __main__ -   Batch Number = 15
01/07/2022 12:54:36 - INFO - __main__ -   Batch Number = 16
Evaluating:  10%|         | 16/165 [00:00<00:08, 18.60it/s]01/07/2022 12:54:36 - INFO - __main__ -   Batch Number = 17
01/07/2022 12:54:36 - INFO - __main__ -   Batch Number = 18
Evaluating:  11%|         | 18/165 [00:00<00:07, 18.77it/s]01/07/2022 12:54:36 - INFO - __main__ -   Batch Number = 19
01/07/2022 12:54:36 - INFO - __main__ -   Batch Number = 20
Evaluating:  12%|        | 20/165 [00:01<00:07, 18.86it/s]01/07/2022 12:54:36 - INFO - __main__ -   Batch Number = 21
01/07/2022 12:54:36 - INFO - __main__ -   Batch Number = 22
Evaluating:  13%|        | 22/165 [00:01<00:07, 18.80it/s]01/07/2022 12:54:36 - INFO - __main__ -   Batch Number = 23
01/07/2022 12:54:36 - INFO - __main__ -   Batch Number = 24
Evaluating:  15%|        | 24/165 [00:01<00:07, 18.64it/s]01/07/2022 12:54:36 - INFO - __main__ -   Batch Number = 25
01/07/2022 12:54:36 - INFO - __main__ -   Batch Number = 26
Evaluating:  16%|        | 26/165 [00:01<00:07, 18.81it/s]01/07/2022 12:54:36 - INFO - __main__ -   Batch Number = 27
01/07/2022 12:54:36 - INFO - __main__ -   Batch Number = 28
Evaluating:  17%|        | 28/165 [00:01<00:07, 18.75it/s]01/07/2022 12:54:37 - INFO - __main__ -   Batch Number = 29
01/07/2022 12:54:37 - INFO - __main__ -   Batch Number = 30
Evaluating:  18%|        | 30/165 [00:01<00:07, 18.61it/s]01/07/2022 12:54:37 - INFO - __main__ -   Batch Number = 31
01/07/2022 12:54:37 - INFO - __main__ -   Batch Number = 32
Evaluating:  19%|        | 32/165 [00:01<00:07, 18.61it/s]01/07/2022 12:54:37 - INFO - __main__ -   Batch Number = 33
01/07/2022 12:54:37 - INFO - __main__ -   Batch Number = 34
Evaluating:  21%|        | 34/165 [00:01<00:07, 18.47it/s]01/07/2022 12:54:37 - INFO - __main__ -   Batch Number = 35
01/07/2022 12:54:37 - INFO - __main__ -   Batch Number = 36
Evaluating:  22%|       | 36/165 [00:01<00:07, 18.37it/s]01/07/2022 12:54:37 - INFO - __main__ -   Batch Number = 37
01/07/2022 12:54:37 - INFO - __main__ -   Batch Number = 38
Evaluating:  23%|       | 38/165 [00:02<00:06, 18.31it/s]01/07/2022 12:54:37 - INFO - __main__ -   Batch Number = 39
01/07/2022 12:54:37 - INFO - __main__ -   Batch Number = 40
Evaluating:  24%|       | 40/165 [00:02<00:06, 18.28it/s]01/07/2022 12:54:37 - INFO - __main__ -   Batch Number = 41
01/07/2022 12:54:37 - INFO - __main__ -   Batch Number = 42
Evaluating:  25%|       | 42/165 [00:02<00:06, 18.48it/s]01/07/2022 12:54:37 - INFO - __main__ -   Batch Number = 43
01/07/2022 12:54:37 - INFO - __main__ -   Batch Number = 44
Evaluating:  27%|       | 44/165 [00:02<00:06, 18.59it/s]01/07/2022 12:54:37 - INFO - __main__ -   Batch Number = 45
01/07/2022 12:54:37 - INFO - __main__ -   Batch Number = 46
Evaluating:  28%|       | 46/165 [00:02<00:06, 18.95it/s]01/07/2022 12:54:37 - INFO - __main__ -   Batch Number = 47
01/07/2022 12:54:38 - INFO - __main__ -   Batch Number = 48
Evaluating:  29%|       | 48/165 [00:02<00:06, 18.87it/s]01/07/2022 12:54:38 - INFO - __main__ -   Batch Number = 49
01/07/2022 12:54:38 - INFO - __main__ -   Batch Number = 50
Evaluating:  30%|       | 50/165 [00:02<00:06, 18.73it/s]01/07/2022 12:54:38 - INFO - __main__ -   Batch Number = 51
01/07/2022 12:54:38 - INFO - __main__ -   Batch Number = 52
Evaluating:  32%|      | 52/165 [00:02<00:06, 18.63it/s]01/07/2022 12:54:38 - INFO - __main__ -   Batch Number = 53
01/07/2022 12:54:38 - INFO - __main__ -   Batch Number = 54
Evaluating:  33%|      | 54/165 [00:02<00:06, 18.46it/s]01/07/2022 12:54:38 - INFO - __main__ -   Batch Number = 55
01/07/2022 12:54:38 - INFO - __main__ -   Batch Number = 56
Evaluating:  34%|      | 56/165 [00:03<00:05, 18.46it/s]01/07/2022 12:54:38 - INFO - __main__ -   Batch Number = 57
01/07/2022 12:54:38 - INFO - __main__ -   Batch Number = 58
Evaluating:  35%|      | 58/165 [00:03<00:05, 18.52it/s]01/07/2022 12:54:38 - INFO - __main__ -   Batch Number = 59
01/07/2022 12:54:38 - INFO - __main__ -   Batch Number = 60
Evaluating:  36%|      | 60/165 [00:03<00:07, 13.49it/s]01/07/2022 12:54:38 - INFO - __main__ -   Batch Number = 61
01/07/2022 12:54:38 - INFO - __main__ -   Batch Number = 62
01/07/2022 12:54:38 - INFO - __main__ -   Batch Number = 63
Evaluating:  38%|      | 63/165 [00:03<00:06, 15.91it/s]01/07/2022 12:54:39 - INFO - __main__ -   Batch Number = 64
01/07/2022 12:54:39 - INFO - __main__ -   Batch Number = 65
Evaluating:  39%|      | 65/165 [00:03<00:06, 16.66it/s]01/07/2022 12:54:39 - INFO - __main__ -   Batch Number = 66
01/07/2022 12:54:39 - INFO - __main__ -   Batch Number = 67
Evaluating:  41%|      | 67/165 [00:03<00:05, 17.22it/s]01/07/2022 12:54:39 - INFO - __main__ -   Batch Number = 68
01/07/2022 12:54:39 - INFO - __main__ -   Batch Number = 69
Evaluating:  42%|     | 69/165 [00:03<00:05, 17.43it/s]01/07/2022 12:54:39 - INFO - __main__ -   Batch Number = 70
01/07/2022 12:54:39 - INFO - __main__ -   Batch Number = 71
Evaluating:  43%|     | 71/165 [00:03<00:05, 17.68it/s]01/07/2022 12:54:39 - INFO - __main__ -   Batch Number = 72
01/07/2022 12:54:39 - INFO - __main__ -   Batch Number = 73
Evaluating:  44%|     | 73/165 [00:04<00:05, 17.93it/s]01/07/2022 12:54:39 - INFO - __main__ -   Batch Number = 74
01/07/2022 12:54:39 - INFO - __main__ -   Batch Number = 75
Evaluating:  45%|     | 75/165 [00:04<00:04, 18.18it/s]01/07/2022 12:54:39 - INFO - __main__ -   Batch Number = 76
01/07/2022 12:54:39 - INFO - __main__ -   Batch Number = 77
Evaluating:  47%|     | 77/165 [00:04<00:04, 18.50it/s]01/07/2022 12:54:39 - INFO - __main__ -   Batch Number = 78
01/07/2022 12:54:39 - INFO - __main__ -   Batch Number = 79
Evaluating:  48%|     | 79/165 [00:04<00:04, 18.68it/s]01/07/2022 12:54:39 - INFO - __main__ -   Batch Number = 80
01/07/2022 12:54:39 - INFO - __main__ -   Batch Number = 81
Evaluating:  49%|     | 81/165 [00:04<00:04, 18.80it/s]01/07/2022 12:54:39 - INFO - __main__ -   Batch Number = 82
01/07/2022 12:54:40 - INFO - __main__ -   Batch Number = 83
Evaluating:  50%|     | 83/165 [00:04<00:04, 19.11it/s]01/07/2022 12:54:40 - INFO - __main__ -   Batch Number = 84
01/07/2022 12:54:40 - INFO - __main__ -   Batch Number = 85
01/07/2022 12:54:40 - INFO - __main__ -   Batch Number = 86
01/07/2022 12:54:40 - INFO - __main__ -   Batch Number = 87
01/07/2022 12:54:40 - INFO - __main__ -   Batch Number = 88
Evaluating:  53%|    | 88/165 [00:04<00:02, 27.01it/s]01/07/2022 12:54:40 - INFO - __main__ -   Batch Number = 89
01/07/2022 12:54:40 - INFO - __main__ -   Batch Number = 90
01/07/2022 12:54:40 - INFO - __main__ -   Batch Number = 91
01/07/2022 12:54:40 - INFO - __main__ -   Batch Number = 92
01/07/2022 12:54:40 - INFO - __main__ -   Batch Number = 93
Evaluating:  56%|    | 93/165 [00:04<00:02, 32.32it/s]01/07/2022 12:54:40 - INFO - __main__ -   Batch Number = 94
01/07/2022 12:54:40 - INFO - __main__ -   Batch Number = 95
01/07/2022 12:54:40 - INFO - __main__ -   Batch Number = 96
01/07/2022 12:54:40 - INFO - __main__ -   Batch Number = 97
01/07/2022 12:54:40 - INFO - __main__ -   Batch Number = 98
Evaluating:  59%|    | 98/165 [00:04<00:01, 35.82it/s]01/07/2022 12:54:40 - INFO - __main__ -   Batch Number = 99
01/07/2022 12:54:40 - INFO - __main__ -   Batch Number = 100
01/07/2022 12:54:40 - INFO - __main__ -   Batch Number = 101
01/07/2022 12:54:40 - INFO - __main__ -   Batch Number = 102
01/07/2022 12:54:40 - INFO - __main__ -   Batch Number = 103
Evaluating:  62%|   | 103/165 [00:05<00:01, 38.42it/s]01/07/2022 12:54:40 - INFO - __main__ -   Batch Number = 104
01/07/2022 12:54:40 - INFO - __main__ -   Batch Number = 105
01/07/2022 12:54:40 - INFO - __main__ -   Batch Number = 106
01/07/2022 12:54:40 - INFO - __main__ -   Batch Number = 107
01/07/2022 12:54:40 - INFO - __main__ -   Batch Number = 108
Evaluating:  65%|   | 108/165 [00:05<00:01, 40.32it/s]01/07/2022 12:54:40 - INFO - __main__ -   Batch Number = 109
01/07/2022 12:54:40 - INFO - __main__ -   Batch Number = 110
01/07/2022 12:54:40 - INFO - __main__ -   Batch Number = 111
01/07/2022 12:54:40 - INFO - __main__ -   Batch Number = 112
01/07/2022 12:54:40 - INFO - __main__ -   Batch Number = 113
Evaluating:  68%|   | 113/165 [00:05<00:01, 41.67it/s]01/07/2022 12:54:40 - INFO - __main__ -   Batch Number = 114
01/07/2022 12:54:40 - INFO - __main__ -   Batch Number = 115
01/07/2022 12:54:40 - INFO - __main__ -   Batch Number = 116
01/07/2022 12:54:40 - INFO - __main__ -   Batch Number = 117
01/07/2022 12:54:40 - INFO - __main__ -   Batch Number = 118
Evaluating:  72%|  | 118/165 [00:05<00:01, 42.57it/s]01/07/2022 12:54:40 - INFO - __main__ -   Batch Number = 119
01/07/2022 12:54:40 - INFO - __main__ -   Batch Number = 120
01/07/2022 12:54:40 - INFO - __main__ -   Batch Number = 121
01/07/2022 12:54:40 - INFO - __main__ -   Batch Number = 122
01/07/2022 12:54:40 - INFO - __main__ -   Batch Number = 123
Evaluating:  75%|  | 123/165 [00:05<00:00, 43.28it/s]01/07/2022 12:54:40 - INFO - __main__ -   Batch Number = 124
01/07/2022 12:54:41 - INFO - __main__ -   Batch Number = 125
01/07/2022 12:54:41 - INFO - __main__ -   Batch Number = 126
01/07/2022 12:54:41 - INFO - __main__ -   Batch Number = 127
01/07/2022 12:54:41 - INFO - __main__ -   Batch Number = 128
Evaluating:  78%|  | 128/165 [00:05<00:00, 43.86it/s]01/07/2022 12:54:41 - INFO - __main__ -   Batch Number = 129
01/07/2022 12:54:41 - INFO - __main__ -   Batch Number = 130
01/07/2022 12:54:41 - INFO - __main__ -   Batch Number = 131
01/07/2022 12:54:41 - INFO - __main__ -   Batch Number = 132
01/07/2022 12:54:41 - INFO - __main__ -   Batch Number = 133
Evaluating:  81%|  | 133/165 [00:05<00:00, 43.99it/s]01/07/2022 12:54:41 - INFO - __main__ -   Batch Number = 134
01/07/2022 12:54:41 - INFO - __main__ -   Batch Number = 135
01/07/2022 12:54:41 - INFO - __main__ -   Batch Number = 136
01/07/2022 12:54:41 - INFO - __main__ -   Batch Number = 137
01/07/2022 12:54:41 - INFO - __main__ -   Batch Number = 138
Evaluating:  84%| | 138/165 [00:05<00:00, 44.28it/s]01/07/2022 12:54:41 - INFO - __main__ -   Batch Number = 139
01/07/2022 12:54:41 - INFO - __main__ -   Batch Number = 140
01/07/2022 12:54:41 - INFO - __main__ -   Batch Number = 141
01/07/2022 12:54:41 - INFO - __main__ -   Batch Number = 142
01/07/2022 12:54:41 - INFO - __main__ -   Batch Number = 143
Evaluating:  87%| | 143/165 [00:05<00:00, 44.52it/s]01/07/2022 12:54:41 - INFO - __main__ -   Batch Number = 144
01/07/2022 12:54:41 - INFO - __main__ -   Batch Number = 145
01/07/2022 12:54:41 - INFO - __main__ -   Batch Number = 146
01/07/2022 12:54:41 - INFO - __main__ -   Batch Number = 147
01/07/2022 12:54:41 - INFO - __main__ -   Batch Number = 148
Evaluating:  90%| | 148/165 [00:06<00:00, 44.57it/s]01/07/2022 12:54:41 - INFO - __main__ -   Batch Number = 149
01/07/2022 12:54:41 - INFO - __main__ -   Batch Number = 150
01/07/2022 12:54:41 - INFO - __main__ -   Batch Number = 151
01/07/2022 12:54:41 - INFO - __main__ -   Batch Number = 152
01/07/2022 12:54:41 - INFO - __main__ -   Batch Number = 153
Evaluating:  93%|| 153/165 [00:06<00:00, 44.63it/s]01/07/2022 12:54:41 - INFO - __main__ -   Batch Number = 154
01/07/2022 12:54:41 - INFO - __main__ -   Batch Number = 155
01/07/2022 12:54:41 - INFO - __main__ -   Batch Number = 156
01/07/2022 12:54:41 - INFO - __main__ -   Batch Number = 157
01/07/2022 12:54:41 - INFO - __main__ -   Batch Number = 158
Evaluating:  96%|| 158/165 [00:06<00:00, 44.68it/s]01/07/2022 12:54:41 - INFO - __main__ -   Batch Number = 159
01/07/2022 12:54:41 - INFO - __main__ -   Batch Number = 160
01/07/2022 12:54:41 - INFO - __main__ -   Batch Number = 161
01/07/2022 12:54:41 - INFO - __main__ -   Batch Number = 162
01/07/2022 12:54:41 - INFO - __main__ -   Batch Number = 163
Evaluating:  99%|| 163/165 [00:06<00:00, 43.47it/s]01/07/2022 12:54:41 - INFO - __main__ -   Batch Number = 164
01/07/2022 12:54:41 - INFO - __main__ -   Batch Number = 165
Evaluating: 100%|| 165/165 [00:06<00:00, 25.70it/s]
01/07/2022 12:54:41 - INFO - __main__ -     Evaluation done in total 6.422137 secs (0.004873 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/predictions_ar_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/nbest_predictions_ar_.json
01/07/2022 12:54:45 - INFO - __main__ -   Results = OrderedDict([('exact', 42.773109243697476), ('f1', 60.406954949090995), ('total', 1190), ('HasAns_exact', 42.773109243697476), ('HasAns_f1', 60.406954949090995), ('HasAns_total', 1190), ('best_exact', 42.773109243697476), ('best_exact_thresh', 0.0), ('best_f1', 60.406954949090995), ('best_f1_thresh', 0.0)])
01/07/2022 12:54:45 - INFO - __main__ -   Language adapter for de not found, using en instead
01/07/2022 12:54:45 - INFO - __main__ -   Set active language adapter to en
01/07/2022 12:54:45 - INFO - __main__ -   Args Adapter Weight = None
01/07/2022 12:54:45 - INFO - __main__ -   Adapter Languages = ['en']
01/07/2022 12:54:45 - INFO - __main__ -   Predict File = xquad.de.json
01/07/2022 12:54:45 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
  0%|          | 0/48 [00:00<?, ?it/s] 33%|      | 16/48 [00:00<00:00, 123.56it/s] 60%|    | 29/48 [00:00<00:00, 111.10it/s] 92%|| 44/48 [00:00<00:00, 123.12it/s]100%|| 48/48 [00:00<00:00, 122.70it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<03:59,  4.97it/s]convert squad examples to features:  24%|       | 289/1190 [00:00<00:00, 1138.77it/s]convert squad examples to features:  37%|      | 440/1190 [00:00<00:01, 660.48it/s] convert squad examples to features:  92%|| 1089/1190 [00:00<00:00, 1673.17it/s]convert squad examples to features: 100%|| 1190/1190 [00:00<00:00, 1415.24it/s]/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|| 1190/1190 [00:00<00:00, 670502.65it/s]
01/07/2022 12:54:47 - INFO - __main__ -   Local Rank = -1
01/07/2022 12:54:47 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.de.json_xlm-roberta-base_384_de
01/07/2022 12:54:48 - INFO - __main__ -   ***** Running evaluation  *****
01/07/2022 12:54:48 - INFO - __main__ -     Num examples = 1303
01/07/2022 12:54:48 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/163 [00:00<?, ?it/s]01/07/2022 12:54:48 - INFO - __main__ -   Batch Number = 1
01/07/2022 12:54:48 - INFO - __main__ -   Batch Number = 2
01/07/2022 12:54:48 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|         | 3/163 [00:00<00:05, 29.43it/s]01/07/2022 12:54:48 - INFO - __main__ -   Batch Number = 4
01/07/2022 12:54:48 - INFO - __main__ -   Batch Number = 5
01/07/2022 12:54:48 - INFO - __main__ -   Batch Number = 6
01/07/2022 12:54:48 - INFO - __main__ -   Batch Number = 7
Evaluating:   4%|         | 7/163 [00:00<00:04, 33.34it/s]01/07/2022 12:54:48 - INFO - __main__ -   Batch Number = 8
01/07/2022 12:54:48 - INFO - __main__ -   Batch Number = 9
01/07/2022 12:54:48 - INFO - __main__ -   Batch Number = 10
01/07/2022 12:54:48 - INFO - __main__ -   Batch Number = 11
Evaluating:   7%|         | 11/163 [00:00<00:04, 35.01it/s]01/07/2022 12:54:48 - INFO - __main__ -   Batch Number = 12
01/07/2022 12:54:48 - INFO - __main__ -   Batch Number = 13
01/07/2022 12:54:48 - INFO - __main__ -   Batch Number = 14
01/07/2022 12:54:48 - INFO - __main__ -   Batch Number = 15
01/07/2022 12:54:48 - INFO - __main__ -   Batch Number = 16
Evaluating:  10%|         | 16/163 [00:00<00:03, 39.02it/s]01/07/2022 12:54:48 - INFO - __main__ -   Batch Number = 17
01/07/2022 12:54:48 - INFO - __main__ -   Batch Number = 18
01/07/2022 12:54:48 - INFO - __main__ -   Batch Number = 19
01/07/2022 12:54:48 - INFO - __main__ -   Batch Number = 20
Evaluating:  12%|        | 20/163 [00:00<00:03, 38.87it/s]01/07/2022 12:54:48 - INFO - __main__ -   Batch Number = 21
01/07/2022 12:54:48 - INFO - __main__ -   Batch Number = 22
01/07/2022 12:54:48 - INFO - __main__ -   Batch Number = 23
01/07/2022 12:54:48 - INFO - __main__ -   Batch Number = 24
01/07/2022 12:54:48 - INFO - __main__ -   Batch Number = 25
Evaluating:  15%|        | 25/163 [00:00<00:03, 41.05it/s]01/07/2022 12:54:48 - INFO - __main__ -   Batch Number = 26
01/07/2022 12:54:48 - INFO - __main__ -   Batch Number = 27
01/07/2022 12:54:48 - INFO - __main__ -   Batch Number = 28
01/07/2022 12:54:48 - INFO - __main__ -   Batch Number = 29
01/07/2022 12:54:48 - INFO - __main__ -   Batch Number = 30
Evaluating:  18%|        | 30/163 [00:00<00:03, 42.01it/s]01/07/2022 12:54:49 - INFO - __main__ -   Batch Number = 31
01/07/2022 12:54:49 - INFO - __main__ -   Batch Number = 32
01/07/2022 12:54:49 - INFO - __main__ -   Batch Number = 33
01/07/2022 12:54:49 - INFO - __main__ -   Batch Number = 34
01/07/2022 12:54:49 - INFO - __main__ -   Batch Number = 35
Evaluating:  21%|       | 35/163 [00:00<00:03, 33.50it/s]01/07/2022 12:54:49 - INFO - __main__ -   Batch Number = 36
01/07/2022 12:54:49 - INFO - __main__ -   Batch Number = 37
01/07/2022 12:54:49 - INFO - __main__ -   Batch Number = 38
01/07/2022 12:54:49 - INFO - __main__ -   Batch Number = 39
01/07/2022 12:54:49 - INFO - __main__ -   Batch Number = 40
Evaluating:  25%|       | 40/163 [00:01<00:03, 36.16it/s]01/07/2022 12:54:49 - INFO - __main__ -   Batch Number = 41
01/07/2022 12:54:49 - INFO - __main__ -   Batch Number = 42
01/07/2022 12:54:49 - INFO - __main__ -   Batch Number = 43
01/07/2022 12:54:49 - INFO - __main__ -   Batch Number = 44
01/07/2022 12:54:49 - INFO - __main__ -   Batch Number = 45
Evaluating:  28%|       | 45/163 [00:01<00:03, 38.26it/s]01/07/2022 12:54:49 - INFO - __main__ -   Batch Number = 46
01/07/2022 12:54:49 - INFO - __main__ -   Batch Number = 47
01/07/2022 12:54:49 - INFO - __main__ -   Batch Number = 48
01/07/2022 12:54:49 - INFO - __main__ -   Batch Number = 49
01/07/2022 12:54:49 - INFO - __main__ -   Batch Number = 50
Evaluating:  31%|       | 50/163 [00:01<00:02, 39.69it/s]01/07/2022 12:54:49 - INFO - __main__ -   Batch Number = 51
01/07/2022 12:54:49 - INFO - __main__ -   Batch Number = 52
01/07/2022 12:54:49 - INFO - __main__ -   Batch Number = 53
01/07/2022 12:54:49 - INFO - __main__ -   Batch Number = 54
01/07/2022 12:54:49 - INFO - __main__ -   Batch Number = 55
Evaluating:  34%|      | 55/163 [00:01<00:02, 40.81it/s]01/07/2022 12:54:49 - INFO - __main__ -   Batch Number = 56
01/07/2022 12:54:49 - INFO - __main__ -   Batch Number = 57
01/07/2022 12:54:49 - INFO - __main__ -   Batch Number = 58
01/07/2022 12:54:49 - INFO - __main__ -   Batch Number = 59
01/07/2022 12:54:49 - INFO - __main__ -   Batch Number = 60
Evaluating:  37%|      | 60/163 [00:01<00:02, 41.61it/s]01/07/2022 12:54:49 - INFO - __main__ -   Batch Number = 61
01/07/2022 12:54:49 - INFO - __main__ -   Batch Number = 62
01/07/2022 12:54:49 - INFO - __main__ -   Batch Number = 63
01/07/2022 12:54:49 - INFO - __main__ -   Batch Number = 64
01/07/2022 12:54:49 - INFO - __main__ -   Batch Number = 65
Evaluating:  40%|      | 65/163 [00:01<00:02, 42.06it/s]01/07/2022 12:54:49 - INFO - __main__ -   Batch Number = 66
01/07/2022 12:54:49 - INFO - __main__ -   Batch Number = 67
01/07/2022 12:54:49 - INFO - __main__ -   Batch Number = 68
01/07/2022 12:54:49 - INFO - __main__ -   Batch Number = 69
01/07/2022 12:54:49 - INFO - __main__ -   Batch Number = 70
Evaluating:  43%|     | 70/163 [00:01<00:02, 42.18it/s]01/07/2022 12:54:50 - INFO - __main__ -   Batch Number = 71
01/07/2022 12:54:50 - INFO - __main__ -   Batch Number = 72
01/07/2022 12:54:50 - INFO - __main__ -   Batch Number = 73
01/07/2022 12:54:50 - INFO - __main__ -   Batch Number = 74
01/07/2022 12:54:50 - INFO - __main__ -   Batch Number = 75
Evaluating:  46%|     | 75/163 [00:01<00:02, 42.34it/s]01/07/2022 12:54:50 - INFO - __main__ -   Batch Number = 76
01/07/2022 12:54:50 - INFO - __main__ -   Batch Number = 77
01/07/2022 12:54:50 - INFO - __main__ -   Batch Number = 78
01/07/2022 12:54:50 - INFO - __main__ -   Batch Number = 79
01/07/2022 12:54:50 - INFO - __main__ -   Batch Number = 80
Evaluating:  49%|     | 80/163 [00:02<00:01, 42.56it/s]01/07/2022 12:54:50 - INFO - __main__ -   Batch Number = 81
01/07/2022 12:54:50 - INFO - __main__ -   Batch Number = 82
01/07/2022 12:54:50 - INFO - __main__ -   Batch Number = 83
01/07/2022 12:54:50 - INFO - __main__ -   Batch Number = 84
01/07/2022 12:54:50 - INFO - __main__ -   Batch Number = 85
Evaluating:  52%|    | 85/163 [00:02<00:01, 42.77it/s]01/07/2022 12:54:50 - INFO - __main__ -   Batch Number = 86
01/07/2022 12:54:50 - INFO - __main__ -   Batch Number = 87
01/07/2022 12:54:50 - INFO - __main__ -   Batch Number = 88
01/07/2022 12:54:50 - INFO - __main__ -   Batch Number = 89
01/07/2022 12:54:50 - INFO - __main__ -   Batch Number = 90
Evaluating:  55%|    | 90/163 [00:02<00:01, 43.00it/s]01/07/2022 12:54:50 - INFO - __main__ -   Batch Number = 91
01/07/2022 12:54:50 - INFO - __main__ -   Batch Number = 92
01/07/2022 12:54:50 - INFO - __main__ -   Batch Number = 93
01/07/2022 12:54:50 - INFO - __main__ -   Batch Number = 94
01/07/2022 12:54:50 - INFO - __main__ -   Batch Number = 95
Evaluating:  58%|    | 95/163 [00:02<00:01, 43.05it/s]01/07/2022 12:54:50 - INFO - __main__ -   Batch Number = 96
01/07/2022 12:54:50 - INFO - __main__ -   Batch Number = 97
01/07/2022 12:54:50 - INFO - __main__ -   Batch Number = 98
01/07/2022 12:54:50 - INFO - __main__ -   Batch Number = 99
01/07/2022 12:54:50 - INFO - __main__ -   Batch Number = 100
Evaluating:  61%|   | 100/163 [00:02<00:01, 43.10it/s]01/07/2022 12:54:50 - INFO - __main__ -   Batch Number = 101
01/07/2022 12:54:50 - INFO - __main__ -   Batch Number = 102
01/07/2022 12:54:50 - INFO - __main__ -   Batch Number = 103
01/07/2022 12:54:50 - INFO - __main__ -   Batch Number = 104
01/07/2022 12:54:50 - INFO - __main__ -   Batch Number = 105
Evaluating:  64%|   | 105/163 [00:02<00:01, 43.10it/s]01/07/2022 12:54:50 - INFO - __main__ -   Batch Number = 106
01/07/2022 12:54:50 - INFO - __main__ -   Batch Number = 107
01/07/2022 12:54:50 - INFO - __main__ -   Batch Number = 108
01/07/2022 12:54:50 - INFO - __main__ -   Batch Number = 109
01/07/2022 12:54:50 - INFO - __main__ -   Batch Number = 110
Evaluating:  67%|   | 110/163 [00:02<00:01, 43.17it/s]01/07/2022 12:54:50 - INFO - __main__ -   Batch Number = 111
01/07/2022 12:54:50 - INFO - __main__ -   Batch Number = 112
01/07/2022 12:54:50 - INFO - __main__ -   Batch Number = 113
01/07/2022 12:54:51 - INFO - __main__ -   Batch Number = 114
01/07/2022 12:54:51 - INFO - __main__ -   Batch Number = 115
Evaluating:  71%|   | 115/163 [00:02<00:01, 43.27it/s]01/07/2022 12:54:51 - INFO - __main__ -   Batch Number = 116
01/07/2022 12:54:51 - INFO - __main__ -   Batch Number = 117
01/07/2022 12:54:51 - INFO - __main__ -   Batch Number = 118
01/07/2022 12:54:51 - INFO - __main__ -   Batch Number = 119
01/07/2022 12:54:51 - INFO - __main__ -   Batch Number = 120
Evaluating:  74%|  | 120/163 [00:02<00:00, 43.24it/s]01/07/2022 12:54:51 - INFO - __main__ -   Batch Number = 121
01/07/2022 12:54:51 - INFO - __main__ -   Batch Number = 122
01/07/2022 12:54:51 - INFO - __main__ -   Batch Number = 123
01/07/2022 12:54:51 - INFO - __main__ -   Batch Number = 124
01/07/2022 12:54:51 - INFO - __main__ -   Batch Number = 125
Evaluating:  77%|  | 125/163 [00:03<00:00, 43.17it/s]01/07/2022 12:54:51 - INFO - __main__ -   Batch Number = 126
01/07/2022 12:54:51 - INFO - __main__ -   Batch Number = 127
01/07/2022 12:54:51 - INFO - __main__ -   Batch Number = 128
01/07/2022 12:54:51 - INFO - __main__ -   Batch Number = 129
01/07/2022 12:54:51 - INFO - __main__ -   Batch Number = 130
Evaluating:  80%|  | 130/163 [00:03<00:00, 43.14it/s]01/07/2022 12:54:51 - INFO - __main__ -   Batch Number = 131
01/07/2022 12:54:51 - INFO - __main__ -   Batch Number = 132
01/07/2022 12:54:51 - INFO - __main__ -   Batch Number = 133
01/07/2022 12:54:51 - INFO - __main__ -   Batch Number = 134
01/07/2022 12:54:51 - INFO - __main__ -   Batch Number = 135
Evaluating:  83%| | 135/163 [00:03<00:00, 42.88it/s]01/07/2022 12:54:51 - INFO - __main__ -   Batch Number = 136
01/07/2022 12:54:51 - INFO - __main__ -   Batch Number = 137
01/07/2022 12:54:51 - INFO - __main__ -   Batch Number = 138
01/07/2022 12:54:51 - INFO - __main__ -   Batch Number = 139
01/07/2022 12:54:51 - INFO - __main__ -   Batch Number = 140
Evaluating:  86%| | 140/163 [00:03<00:00, 42.79it/s]01/07/2022 12:54:51 - INFO - __main__ -   Batch Number = 141
01/07/2022 12:54:51 - INFO - __main__ -   Batch Number = 142
01/07/2022 12:54:51 - INFO - __main__ -   Batch Number = 143
01/07/2022 12:54:51 - INFO - __main__ -   Batch Number = 144
01/07/2022 12:54:51 - INFO - __main__ -   Batch Number = 145
Evaluating:  89%| | 145/163 [00:03<00:00, 42.85it/s]01/07/2022 12:54:51 - INFO - __main__ -   Batch Number = 146
01/07/2022 12:54:51 - INFO - __main__ -   Batch Number = 147
01/07/2022 12:54:51 - INFO - __main__ -   Batch Number = 148
01/07/2022 12:54:51 - INFO - __main__ -   Batch Number = 149
01/07/2022 12:54:51 - INFO - __main__ -   Batch Number = 150
Evaluating:  92%|| 150/163 [00:03<00:00, 42.91it/s]01/07/2022 12:54:51 - INFO - __main__ -   Batch Number = 151
01/07/2022 12:54:51 - INFO - __main__ -   Batch Number = 152
01/07/2022 12:54:51 - INFO - __main__ -   Batch Number = 153
01/07/2022 12:54:51 - INFO - __main__ -   Batch Number = 154
01/07/2022 12:54:51 - INFO - __main__ -   Batch Number = 155
Evaluating:  95%|| 155/163 [00:03<00:00, 42.86it/s]01/07/2022 12:54:51 - INFO - __main__ -   Batch Number = 156
01/07/2022 12:54:52 - INFO - __main__ -   Batch Number = 157
01/07/2022 12:54:52 - INFO - __main__ -   Batch Number = 158
01/07/2022 12:54:52 - INFO - __main__ -   Batch Number = 159
01/07/2022 12:54:52 - INFO - __main__ -   Batch Number = 160
Evaluating:  98%|| 160/163 [00:03<00:00, 42.83it/s]01/07/2022 12:54:52 - INFO - __main__ -   Batch Number = 161
01/07/2022 12:54:52 - INFO - __main__ -   Batch Number = 162
01/07/2022 12:54:52 - INFO - __main__ -   Batch Number = 163
Evaluating: 100%|| 163/163 [00:03<00:00, 41.41it/s]
01/07/2022 12:54:52 - INFO - __main__ -     Evaluation done in total 3.936461 secs (0.003021 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/predictions_de_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/nbest_predictions_de_.json
01/07/2022 12:54:55 - INFO - __main__ -   Results = OrderedDict([('exact', 53.78151260504202), ('f1', 70.57634576478708), ('total', 1190), ('HasAns_exact', 53.78151260504202), ('HasAns_f1', 70.57634576478708), ('HasAns_total', 1190), ('best_exact', 53.78151260504202), ('best_exact_thresh', 0.0), ('best_f1', 70.57634576478708), ('best_f1_thresh', 0.0)])
01/07/2022 12:54:55 - INFO - __main__ -   Language adapter for el not found, using en instead
01/07/2022 12:54:55 - INFO - __main__ -   Set active language adapter to en
01/07/2022 12:54:55 - INFO - __main__ -   Args Adapter Weight = None
01/07/2022 12:54:55 - INFO - __main__ -   Adapter Languages = ['en']
01/07/2022 12:54:55 - INFO - __main__ -   Predict File = xquad.el.json
01/07/2022 12:54:55 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
  0%|          | 0/48 [00:00<?, ?it/s] 29%|       | 14/48 [00:00<00:00, 136.37it/s] 58%|    | 28/48 [00:00<00:00, 103.94it/s] 83%| | 40/48 [00:00<00:00, 109.18it/s]100%|| 48/48 [00:00<00:00, 113.87it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<03:33,  5.56it/s]convert squad examples to features:  24%|       | 289/1190 [00:00<00:00, 1024.57it/s]convert squad examples to features:  33%|      | 398/1190 [00:00<00:01, 402.71it/s] convert squad examples to features:  97%|| 1153/1190 [00:01<00:00, 1485.85it/s]convert squad examples to features: 100%|| 1190/1190 [00:01<00:00, 1160.32it/s]/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|| 1190/1190 [00:00<00:00, 671946.93it/s]
01/07/2022 12:54:57 - INFO - __main__ -   Local Rank = -1
01/07/2022 12:54:57 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.el.json_xlm-roberta-base_384_el
01/07/2022 12:54:58 - INFO - __main__ -   ***** Running evaluation  *****
01/07/2022 12:54:58 - INFO - __main__ -     Num examples = 1488
01/07/2022 12:54:58 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/186 [00:00<?, ?it/s]01/07/2022 12:54:58 - INFO - __main__ -   Batch Number = 1
01/07/2022 12:54:59 - INFO - __main__ -   Batch Number = 2
01/07/2022 12:54:59 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|         | 3/186 [00:00<00:06, 29.32it/s]01/07/2022 12:54:59 - INFO - __main__ -   Batch Number = 4
01/07/2022 12:54:59 - INFO - __main__ -   Batch Number = 5
01/07/2022 12:54:59 - INFO - __main__ -   Batch Number = 6
Evaluating:   3%|         | 6/186 [00:00<00:06, 29.67it/s]01/07/2022 12:54:59 - INFO - __main__ -   Batch Number = 7
01/07/2022 12:54:59 - INFO - __main__ -   Batch Number = 8
01/07/2022 12:54:59 - INFO - __main__ -   Batch Number = 9
01/07/2022 12:54:59 - INFO - __main__ -   Batch Number = 10
Evaluating:   5%|         | 10/186 [00:00<00:05, 31.66it/s]01/07/2022 12:54:59 - INFO - __main__ -   Batch Number = 11
01/07/2022 12:54:59 - INFO - __main__ -   Batch Number = 12
01/07/2022 12:54:59 - INFO - __main__ -   Batch Number = 13
01/07/2022 12:54:59 - INFO - __main__ -   Batch Number = 14
01/07/2022 12:54:59 - INFO - __main__ -   Batch Number = 15
Evaluating:   8%|         | 15/186 [00:00<00:04, 36.87it/s]01/07/2022 12:54:59 - INFO - __main__ -   Batch Number = 16
01/07/2022 12:54:59 - INFO - __main__ -   Batch Number = 17
01/07/2022 12:54:59 - INFO - __main__ -   Batch Number = 18
01/07/2022 12:54:59 - INFO - __main__ -   Batch Number = 19
01/07/2022 12:54:59 - INFO - __main__ -   Batch Number = 20
Evaluating:  11%|         | 20/186 [00:00<00:04, 39.08it/s]01/07/2022 12:54:59 - INFO - __main__ -   Batch Number = 21
01/07/2022 12:54:59 - INFO - __main__ -   Batch Number = 22
01/07/2022 12:54:59 - INFO - __main__ -   Batch Number = 23
01/07/2022 12:54:59 - INFO - __main__ -   Batch Number = 24
01/07/2022 12:54:59 - INFO - __main__ -   Batch Number = 25
Evaluating:  13%|        | 25/186 [00:00<00:03, 41.16it/s]01/07/2022 12:54:59 - INFO - __main__ -   Batch Number = 26
01/07/2022 12:54:59 - INFO - __main__ -   Batch Number = 27
01/07/2022 12:54:59 - INFO - __main__ -   Batch Number = 28
01/07/2022 12:54:59 - INFO - __main__ -   Batch Number = 29
01/07/2022 12:54:59 - INFO - __main__ -   Batch Number = 30
Evaluating:  16%|        | 30/186 [00:00<00:03, 42.29it/s]01/07/2022 12:54:59 - INFO - __main__ -   Batch Number = 31
01/07/2022 12:54:59 - INFO - __main__ -   Batch Number = 32
01/07/2022 12:54:59 - INFO - __main__ -   Batch Number = 33
01/07/2022 12:54:59 - INFO - __main__ -   Batch Number = 34
01/07/2022 12:54:59 - INFO - __main__ -   Batch Number = 35
Evaluating:  19%|        | 35/186 [00:00<00:03, 43.13it/s]01/07/2022 12:54:59 - INFO - __main__ -   Batch Number = 36
01/07/2022 12:54:59 - INFO - __main__ -   Batch Number = 37
01/07/2022 12:54:59 - INFO - __main__ -   Batch Number = 38
01/07/2022 12:54:59 - INFO - __main__ -   Batch Number = 39
01/07/2022 12:54:59 - INFO - __main__ -   Batch Number = 40
Evaluating:  22%|       | 40/186 [00:00<00:03, 43.64it/s]01/07/2022 12:54:59 - INFO - __main__ -   Batch Number = 41
01/07/2022 12:54:59 - INFO - __main__ -   Batch Number = 42
01/07/2022 12:55:00 - INFO - __main__ -   Batch Number = 43
01/07/2022 12:55:00 - INFO - __main__ -   Batch Number = 44
01/07/2022 12:55:00 - INFO - __main__ -   Batch Number = 45
Evaluating:  24%|       | 45/186 [00:01<00:03, 44.13it/s]01/07/2022 12:55:00 - INFO - __main__ -   Batch Number = 46
01/07/2022 12:55:00 - INFO - __main__ -   Batch Number = 47
01/07/2022 12:55:00 - INFO - __main__ -   Batch Number = 48
01/07/2022 12:55:00 - INFO - __main__ -   Batch Number = 49
01/07/2022 12:55:00 - INFO - __main__ -   Batch Number = 50
Evaluating:  27%|       | 50/186 [00:01<00:03, 44.14it/s]01/07/2022 12:55:00 - INFO - __main__ -   Batch Number = 51
01/07/2022 12:55:00 - INFO - __main__ -   Batch Number = 52
01/07/2022 12:55:00 - INFO - __main__ -   Batch Number = 53
01/07/2022 12:55:00 - INFO - __main__ -   Batch Number = 54
01/07/2022 12:55:00 - INFO - __main__ -   Batch Number = 55
Evaluating:  30%|       | 55/186 [00:01<00:02, 44.40it/s]01/07/2022 12:55:00 - INFO - __main__ -   Batch Number = 56
01/07/2022 12:55:00 - INFO - __main__ -   Batch Number = 57
01/07/2022 12:55:00 - INFO - __main__ -   Batch Number = 58
01/07/2022 12:55:00 - INFO - __main__ -   Batch Number = 59
01/07/2022 12:55:00 - INFO - __main__ -   Batch Number = 60
Evaluating:  32%|      | 60/186 [00:01<00:02, 44.34it/s]01/07/2022 12:55:00 - INFO - __main__ -   Batch Number = 61
01/07/2022 12:55:00 - INFO - __main__ -   Batch Number = 62
01/07/2022 12:55:00 - INFO - __main__ -   Batch Number = 63
01/07/2022 12:55:00 - INFO - __main__ -   Batch Number = 64
01/07/2022 12:55:00 - INFO - __main__ -   Batch Number = 65
Evaluating:  35%|      | 65/186 [00:01<00:02, 44.29it/s]01/07/2022 12:55:00 - INFO - __main__ -   Batch Number = 66
01/07/2022 12:55:00 - INFO - __main__ -   Batch Number = 67
01/07/2022 12:55:00 - INFO - __main__ -   Batch Number = 68
01/07/2022 12:55:00 - INFO - __main__ -   Batch Number = 69
01/07/2022 12:55:00 - INFO - __main__ -   Batch Number = 70
Evaluating:  38%|      | 70/186 [00:01<00:02, 44.38it/s]01/07/2022 12:55:00 - INFO - __main__ -   Batch Number = 71
01/07/2022 12:55:00 - INFO - __main__ -   Batch Number = 72
01/07/2022 12:55:00 - INFO - __main__ -   Batch Number = 73
01/07/2022 12:55:00 - INFO - __main__ -   Batch Number = 74
01/07/2022 12:55:00 - INFO - __main__ -   Batch Number = 75
Evaluating:  40%|      | 75/186 [00:01<00:02, 44.49it/s]01/07/2022 12:55:00 - INFO - __main__ -   Batch Number = 76
01/07/2022 12:55:00 - INFO - __main__ -   Batch Number = 77
01/07/2022 12:55:00 - INFO - __main__ -   Batch Number = 78
01/07/2022 12:55:00 - INFO - __main__ -   Batch Number = 79
01/07/2022 12:55:00 - INFO - __main__ -   Batch Number = 80
Evaluating:  43%|     | 80/186 [00:01<00:02, 44.43it/s]01/07/2022 12:55:00 - INFO - __main__ -   Batch Number = 81
01/07/2022 12:55:00 - INFO - __main__ -   Batch Number = 82
01/07/2022 12:55:00 - INFO - __main__ -   Batch Number = 83
01/07/2022 12:55:00 - INFO - __main__ -   Batch Number = 84
01/07/2022 12:55:00 - INFO - __main__ -   Batch Number = 85
Evaluating:  46%|     | 85/186 [00:02<00:02, 44.43it/s]01/07/2022 12:55:00 - INFO - __main__ -   Batch Number = 86
01/07/2022 12:55:00 - INFO - __main__ -   Batch Number = 87
01/07/2022 12:55:01 - INFO - __main__ -   Batch Number = 88
01/07/2022 12:55:01 - INFO - __main__ -   Batch Number = 89
01/07/2022 12:55:01 - INFO - __main__ -   Batch Number = 90
Evaluating:  48%|     | 90/186 [00:02<00:02, 44.66it/s]01/07/2022 12:55:01 - INFO - __main__ -   Batch Number = 91
01/07/2022 12:55:01 - INFO - __main__ -   Batch Number = 92
01/07/2022 12:55:01 - INFO - __main__ -   Batch Number = 93
01/07/2022 12:55:01 - INFO - __main__ -   Batch Number = 94
01/07/2022 12:55:01 - INFO - __main__ -   Batch Number = 95
Evaluating:  51%|     | 95/186 [00:02<00:02, 44.42it/s]01/07/2022 12:55:01 - INFO - __main__ -   Batch Number = 96
01/07/2022 12:55:01 - INFO - __main__ -   Batch Number = 97
01/07/2022 12:55:01 - INFO - __main__ -   Batch Number = 98
01/07/2022 12:55:01 - INFO - __main__ -   Batch Number = 99
01/07/2022 12:55:01 - INFO - __main__ -   Batch Number = 100
Evaluating:  54%|    | 100/186 [00:02<00:01, 44.52it/s]01/07/2022 12:55:01 - INFO - __main__ -   Batch Number = 101
01/07/2022 12:55:01 - INFO - __main__ -   Batch Number = 102
01/07/2022 12:55:01 - INFO - __main__ -   Batch Number = 103
01/07/2022 12:55:01 - INFO - __main__ -   Batch Number = 104
01/07/2022 12:55:01 - INFO - __main__ -   Batch Number = 105
Evaluating:  56%|    | 105/186 [00:02<00:01, 44.38it/s]01/07/2022 12:55:01 - INFO - __main__ -   Batch Number = 106
01/07/2022 12:55:01 - INFO - __main__ -   Batch Number = 107
01/07/2022 12:55:01 - INFO - __main__ -   Batch Number = 108
01/07/2022 12:55:01 - INFO - __main__ -   Batch Number = 109
01/07/2022 12:55:01 - INFO - __main__ -   Batch Number = 110
Evaluating:  59%|    | 110/186 [00:02<00:01, 40.51it/s]01/07/2022 12:55:01 - INFO - __main__ -   Batch Number = 111
01/07/2022 12:55:01 - INFO - __main__ -   Batch Number = 112
01/07/2022 12:55:01 - INFO - __main__ -   Batch Number = 113
01/07/2022 12:55:01 - INFO - __main__ -   Batch Number = 114
01/07/2022 12:55:01 - INFO - __main__ -   Batch Number = 115
Evaluating:  62%|   | 115/186 [00:02<00:01, 41.59it/s]01/07/2022 12:55:01 - INFO - __main__ -   Batch Number = 116
01/07/2022 12:55:01 - INFO - __main__ -   Batch Number = 117
01/07/2022 12:55:01 - INFO - __main__ -   Batch Number = 118
01/07/2022 12:55:01 - INFO - __main__ -   Batch Number = 119
01/07/2022 12:55:01 - INFO - __main__ -   Batch Number = 120
Evaluating:  65%|   | 120/186 [00:02<00:01, 42.60it/s]01/07/2022 12:55:01 - INFO - __main__ -   Batch Number = 121
01/07/2022 12:55:01 - INFO - __main__ -   Batch Number = 122
01/07/2022 12:55:01 - INFO - __main__ -   Batch Number = 123
01/07/2022 12:55:01 - INFO - __main__ -   Batch Number = 124
01/07/2022 12:55:01 - INFO - __main__ -   Batch Number = 125
Evaluating:  67%|   | 125/186 [00:02<00:01, 41.64it/s]01/07/2022 12:55:01 - INFO - __main__ -   Batch Number = 126
01/07/2022 12:55:01 - INFO - __main__ -   Batch Number = 127
01/07/2022 12:55:01 - INFO - __main__ -   Batch Number = 128
01/07/2022 12:55:01 - INFO - __main__ -   Batch Number = 129
01/07/2022 12:55:02 - INFO - __main__ -   Batch Number = 130
Evaluating:  70%|   | 130/186 [00:03<00:01, 42.49it/s]01/07/2022 12:55:02 - INFO - __main__ -   Batch Number = 131
01/07/2022 12:55:02 - INFO - __main__ -   Batch Number = 132
01/07/2022 12:55:02 - INFO - __main__ -   Batch Number = 133
01/07/2022 12:55:02 - INFO - __main__ -   Batch Number = 134
01/07/2022 12:55:02 - INFO - __main__ -   Batch Number = 135
Evaluating:  73%|  | 135/186 [00:03<00:01, 43.00it/s]01/07/2022 12:55:02 - INFO - __main__ -   Batch Number = 136
01/07/2022 12:55:02 - INFO - __main__ -   Batch Number = 137
01/07/2022 12:55:02 - INFO - __main__ -   Batch Number = 138
01/07/2022 12:55:02 - INFO - __main__ -   Batch Number = 139
01/07/2022 12:55:02 - INFO - __main__ -   Batch Number = 140
Evaluating:  75%|  | 140/186 [00:03<00:01, 43.40it/s]01/07/2022 12:55:02 - INFO - __main__ -   Batch Number = 141
01/07/2022 12:55:02 - INFO - __main__ -   Batch Number = 142
01/07/2022 12:55:02 - INFO - __main__ -   Batch Number = 143
01/07/2022 12:55:02 - INFO - __main__ -   Batch Number = 144
01/07/2022 12:55:02 - INFO - __main__ -   Batch Number = 145
Evaluating:  78%|  | 145/186 [00:03<00:00, 43.66it/s]01/07/2022 12:55:02 - INFO - __main__ -   Batch Number = 146
01/07/2022 12:55:02 - INFO - __main__ -   Batch Number = 147
01/07/2022 12:55:02 - INFO - __main__ -   Batch Number = 148
01/07/2022 12:55:02 - INFO - __main__ -   Batch Number = 149
01/07/2022 12:55:02 - INFO - __main__ -   Batch Number = 150
Evaluating:  81%|  | 150/186 [00:03<00:00, 43.91it/s]01/07/2022 12:55:02 - INFO - __main__ -   Batch Number = 151
01/07/2022 12:55:02 - INFO - __main__ -   Batch Number = 152
01/07/2022 12:55:02 - INFO - __main__ -   Batch Number = 153
01/07/2022 12:55:02 - INFO - __main__ -   Batch Number = 154
01/07/2022 12:55:02 - INFO - __main__ -   Batch Number = 155
Evaluating:  83%| | 155/186 [00:03<00:00, 44.00it/s]01/07/2022 12:55:02 - INFO - __main__ -   Batch Number = 156
01/07/2022 12:55:02 - INFO - __main__ -   Batch Number = 157
01/07/2022 12:55:02 - INFO - __main__ -   Batch Number = 158
01/07/2022 12:55:02 - INFO - __main__ -   Batch Number = 159
01/07/2022 12:55:02 - INFO - __main__ -   Batch Number = 160
Evaluating:  86%| | 160/186 [00:03<00:00, 44.05it/s]01/07/2022 12:55:02 - INFO - __main__ -   Batch Number = 161
01/07/2022 12:55:02 - INFO - __main__ -   Batch Number = 162
01/07/2022 12:55:02 - INFO - __main__ -   Batch Number = 163
01/07/2022 12:55:02 - INFO - __main__ -   Batch Number = 164
01/07/2022 12:55:02 - INFO - __main__ -   Batch Number = 165
Evaluating:  89%| | 165/186 [00:03<00:00, 44.23it/s]01/07/2022 12:55:02 - INFO - __main__ -   Batch Number = 166
01/07/2022 12:55:02 - INFO - __main__ -   Batch Number = 167
01/07/2022 12:55:02 - INFO - __main__ -   Batch Number = 168
01/07/2022 12:55:02 - INFO - __main__ -   Batch Number = 169
01/07/2022 12:55:02 - INFO - __main__ -   Batch Number = 170
Evaluating:  91%|| 170/186 [00:03<00:00, 44.16it/s]01/07/2022 12:55:02 - INFO - __main__ -   Batch Number = 171
01/07/2022 12:55:02 - INFO - __main__ -   Batch Number = 172
01/07/2022 12:55:02 - INFO - __main__ -   Batch Number = 173
01/07/2022 12:55:03 - INFO - __main__ -   Batch Number = 174
01/07/2022 12:55:03 - INFO - __main__ -   Batch Number = 175
Evaluating:  94%|| 175/186 [00:04<00:00, 43.55it/s]01/07/2022 12:55:03 - INFO - __main__ -   Batch Number = 176
01/07/2022 12:55:03 - INFO - __main__ -   Batch Number = 177
01/07/2022 12:55:03 - INFO - __main__ -   Batch Number = 178
01/07/2022 12:55:03 - INFO - __main__ -   Batch Number = 179
01/07/2022 12:55:03 - INFO - __main__ -   Batch Number = 180
Evaluating:  97%|| 180/186 [00:04<00:00, 39.33it/s]01/07/2022 12:55:03 - INFO - __main__ -   Batch Number = 181
01/07/2022 12:55:03 - INFO - __main__ -   Batch Number = 182
01/07/2022 12:55:03 - INFO - __main__ -   Batch Number = 183
01/07/2022 12:55:03 - INFO - __main__ -   Batch Number = 184
01/07/2022 12:55:03 - INFO - __main__ -   Batch Number = 185
Evaluating:  99%|| 185/186 [00:04<00:00, 38.57it/s]01/07/2022 12:55:03 - INFO - __main__ -   Batch Number = 186
Evaluating: 100%|| 186/186 [00:04<00:00, 42.26it/s]
01/07/2022 12:55:03 - INFO - __main__ -     Evaluation done in total 4.402189 secs (0.002958 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/predictions_el_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/nbest_predictions_el_.json
01/07/2022 12:55:06 - INFO - __main__ -   Results = OrderedDict([('exact', 51.9327731092437), ('f1', 69.64843702744083), ('total', 1190), ('HasAns_exact', 51.9327731092437), ('HasAns_f1', 69.64843702744083), ('HasAns_total', 1190), ('best_exact', 51.9327731092437), ('best_exact_thresh', 0.0), ('best_f1', 69.64843702744083), ('best_f1_thresh', 0.0)])
01/07/2022 12:55:06 - INFO - __main__ -   Language adapter for es not found, using en instead
01/07/2022 12:55:06 - INFO - __main__ -   Set active language adapter to en
01/07/2022 12:55:06 - INFO - __main__ -   Args Adapter Weight = None
01/07/2022 12:55:06 - INFO - __main__ -   Adapter Languages = ['en']
01/07/2022 12:55:06 - INFO - __main__ -   Predict File = xquad.es.json
01/07/2022 12:55:06 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
  0%|          | 0/48 [00:00<?, ?it/s] 33%|      | 16/48 [00:00<00:00, 125.20it/s] 60%|    | 29/48 [00:00<00:00, 114.64it/s] 92%|| 44/48 [00:00<00:00, 123.38it/s]100%|| 48/48 [00:00<00:00, 124.06it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<04:21,  4.54it/s]convert squad examples to features:  32%|      | 385/1190 [00:00<00:01, 632.77it/s]convert squad examples to features:  89%| | 1057/1190 [00:00<00:00, 1600.29it/s]convert squad examples to features: 100%|| 1190/1190 [00:00<00:00, 1339.73it/s]/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|| 1190/1190 [00:00<00:00, 747524.60it/s]
01/07/2022 12:55:08 - INFO - __main__ -   Local Rank = -1
01/07/2022 12:55:08 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.es.json_xlm-roberta-base_384_es
01/07/2022 12:55:09 - INFO - __main__ -   ***** Running evaluation  *****
01/07/2022 12:55:09 - INFO - __main__ -     Num examples = 1304
01/07/2022 12:55:09 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/163 [00:00<?, ?it/s]01/07/2022 12:55:09 - INFO - __main__ -   Batch Number = 1
01/07/2022 12:55:09 - INFO - __main__ -   Batch Number = 2
01/07/2022 12:55:09 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|         | 3/163 [00:00<00:05, 29.78it/s]01/07/2022 12:55:09 - INFO - __main__ -   Batch Number = 4
01/07/2022 12:55:09 - INFO - __main__ -   Batch Number = 5
01/07/2022 12:55:09 - INFO - __main__ -   Batch Number = 6
01/07/2022 12:55:09 - INFO - __main__ -   Batch Number = 7
Evaluating:   4%|         | 7/163 [00:00<00:05, 30.28it/s]01/07/2022 12:55:09 - INFO - __main__ -   Batch Number = 8
01/07/2022 12:55:09 - INFO - __main__ -   Batch Number = 9
01/07/2022 12:55:09 - INFO - __main__ -   Batch Number = 10
01/07/2022 12:55:10 - INFO - __main__ -   Batch Number = 11
01/07/2022 12:55:10 - INFO - __main__ -   Batch Number = 12
Evaluating:   7%|         | 12/163 [00:00<00:04, 36.70it/s]01/07/2022 12:55:10 - INFO - __main__ -   Batch Number = 13
01/07/2022 12:55:10 - INFO - __main__ -   Batch Number = 14
01/07/2022 12:55:10 - INFO - __main__ -   Batch Number = 15
01/07/2022 12:55:10 - INFO - __main__ -   Batch Number = 16
01/07/2022 12:55:10 - INFO - __main__ -   Batch Number = 17
Evaluating:  10%|         | 17/163 [00:00<00:03, 39.48it/s]01/07/2022 12:55:10 - INFO - __main__ -   Batch Number = 18
01/07/2022 12:55:10 - INFO - __main__ -   Batch Number = 19
01/07/2022 12:55:10 - INFO - __main__ -   Batch Number = 20
01/07/2022 12:55:10 - INFO - __main__ -   Batch Number = 21
01/07/2022 12:55:10 - INFO - __main__ -   Batch Number = 22
Evaluating:  13%|        | 22/163 [00:00<00:03, 41.38it/s]01/07/2022 12:55:10 - INFO - __main__ -   Batch Number = 23
01/07/2022 12:55:10 - INFO - __main__ -   Batch Number = 24
01/07/2022 12:55:10 - INFO - __main__ -   Batch Number = 25
01/07/2022 12:55:10 - INFO - __main__ -   Batch Number = 26
01/07/2022 12:55:10 - INFO - __main__ -   Batch Number = 27
Evaluating:  17%|        | 27/163 [00:00<00:03, 42.56it/s]01/07/2022 12:55:10 - INFO - __main__ -   Batch Number = 28
01/07/2022 12:55:10 - INFO - __main__ -   Batch Number = 29
01/07/2022 12:55:10 - INFO - __main__ -   Batch Number = 30
01/07/2022 12:55:10 - INFO - __main__ -   Batch Number = 31
01/07/2022 12:55:10 - INFO - __main__ -   Batch Number = 32
Evaluating:  20%|        | 32/163 [00:00<00:03, 43.23it/s]01/07/2022 12:55:10 - INFO - __main__ -   Batch Number = 33
01/07/2022 12:55:10 - INFO - __main__ -   Batch Number = 34
01/07/2022 12:55:10 - INFO - __main__ -   Batch Number = 35
01/07/2022 12:55:10 - INFO - __main__ -   Batch Number = 36
01/07/2022 12:55:10 - INFO - __main__ -   Batch Number = 37
Evaluating:  23%|       | 37/163 [00:00<00:02, 43.69it/s]01/07/2022 12:55:10 - INFO - __main__ -   Batch Number = 38
01/07/2022 12:55:10 - INFO - __main__ -   Batch Number = 39
01/07/2022 12:55:10 - INFO - __main__ -   Batch Number = 40
01/07/2022 12:55:10 - INFO - __main__ -   Batch Number = 41
01/07/2022 12:55:10 - INFO - __main__ -   Batch Number = 42
Evaluating:  26%|       | 42/163 [00:01<00:02, 43.93it/s]01/07/2022 12:55:10 - INFO - __main__ -   Batch Number = 43
01/07/2022 12:55:10 - INFO - __main__ -   Batch Number = 44
01/07/2022 12:55:10 - INFO - __main__ -   Batch Number = 45
01/07/2022 12:55:10 - INFO - __main__ -   Batch Number = 46
01/07/2022 12:55:10 - INFO - __main__ -   Batch Number = 47
Evaluating:  29%|       | 47/163 [00:01<00:02, 44.07it/s]01/07/2022 12:55:10 - INFO - __main__ -   Batch Number = 48
01/07/2022 12:55:10 - INFO - __main__ -   Batch Number = 49
01/07/2022 12:55:10 - INFO - __main__ -   Batch Number = 50
01/07/2022 12:55:10 - INFO - __main__ -   Batch Number = 51
01/07/2022 12:55:10 - INFO - __main__ -   Batch Number = 52
Evaluating:  32%|      | 52/163 [00:01<00:02, 44.22it/s]01/07/2022 12:55:10 - INFO - __main__ -   Batch Number = 53
01/07/2022 12:55:10 - INFO - __main__ -   Batch Number = 54
01/07/2022 12:55:10 - INFO - __main__ -   Batch Number = 55
01/07/2022 12:55:11 - INFO - __main__ -   Batch Number = 56
01/07/2022 12:55:11 - INFO - __main__ -   Batch Number = 57
Evaluating:  35%|      | 57/163 [00:01<00:02, 44.32it/s]01/07/2022 12:55:11 - INFO - __main__ -   Batch Number = 58
01/07/2022 12:55:11 - INFO - __main__ -   Batch Number = 59
01/07/2022 12:55:11 - INFO - __main__ -   Batch Number = 60
01/07/2022 12:55:11 - INFO - __main__ -   Batch Number = 61
01/07/2022 12:55:11 - INFO - __main__ -   Batch Number = 62
Evaluating:  38%|      | 62/163 [00:01<00:02, 44.16it/s]01/07/2022 12:55:11 - INFO - __main__ -   Batch Number = 63
01/07/2022 12:55:11 - INFO - __main__ -   Batch Number = 64
01/07/2022 12:55:11 - INFO - __main__ -   Batch Number = 65
01/07/2022 12:55:11 - INFO - __main__ -   Batch Number = 66
01/07/2022 12:55:11 - INFO - __main__ -   Batch Number = 67
Evaluating:  41%|      | 67/163 [00:01<00:02, 44.41it/s]01/07/2022 12:55:11 - INFO - __main__ -   Batch Number = 68
01/07/2022 12:55:11 - INFO - __main__ -   Batch Number = 69
01/07/2022 12:55:11 - INFO - __main__ -   Batch Number = 70
01/07/2022 12:55:11 - INFO - __main__ -   Batch Number = 71
01/07/2022 12:55:11 - INFO - __main__ -   Batch Number = 72
Evaluating:  44%|     | 72/163 [00:01<00:02, 44.37it/s]01/07/2022 12:55:11 - INFO - __main__ -   Batch Number = 73
01/07/2022 12:55:11 - INFO - __main__ -   Batch Number = 74
01/07/2022 12:55:11 - INFO - __main__ -   Batch Number = 75
01/07/2022 12:55:11 - INFO - __main__ -   Batch Number = 76
01/07/2022 12:55:11 - INFO - __main__ -   Batch Number = 77
Evaluating:  47%|     | 77/163 [00:01<00:02, 42.84it/s]01/07/2022 12:55:11 - INFO - __main__ -   Batch Number = 78
01/07/2022 12:55:11 - INFO - __main__ -   Batch Number = 79
01/07/2022 12:55:11 - INFO - __main__ -   Batch Number = 80
01/07/2022 12:55:11 - INFO - __main__ -   Batch Number = 81
01/07/2022 12:55:11 - INFO - __main__ -   Batch Number = 82
Evaluating:  50%|     | 82/163 [00:01<00:01, 43.25it/s]01/07/2022 12:55:11 - INFO - __main__ -   Batch Number = 83
01/07/2022 12:55:11 - INFO - __main__ -   Batch Number = 84
01/07/2022 12:55:11 - INFO - __main__ -   Batch Number = 85
01/07/2022 12:55:11 - INFO - __main__ -   Batch Number = 86
01/07/2022 12:55:11 - INFO - __main__ -   Batch Number = 87
Evaluating:  53%|    | 87/163 [00:02<00:01, 43.69it/s]01/07/2022 12:55:11 - INFO - __main__ -   Batch Number = 88
01/07/2022 12:55:11 - INFO - __main__ -   Batch Number = 89
01/07/2022 12:55:11 - INFO - __main__ -   Batch Number = 90
01/07/2022 12:55:11 - INFO - __main__ -   Batch Number = 91
01/07/2022 12:55:11 - INFO - __main__ -   Batch Number = 92
Evaluating:  56%|    | 92/163 [00:02<00:01, 43.86it/s]01/07/2022 12:55:11 - INFO - __main__ -   Batch Number = 93
01/07/2022 12:55:11 - INFO - __main__ -   Batch Number = 94
01/07/2022 12:55:11 - INFO - __main__ -   Batch Number = 95
01/07/2022 12:55:11 - INFO - __main__ -   Batch Number = 96
01/07/2022 12:55:11 - INFO - __main__ -   Batch Number = 97
Evaluating:  60%|    | 97/163 [00:02<00:01, 44.23it/s]01/07/2022 12:55:11 - INFO - __main__ -   Batch Number = 98
01/07/2022 12:55:11 - INFO - __main__ -   Batch Number = 99
01/07/2022 12:55:12 - INFO - __main__ -   Batch Number = 100
01/07/2022 12:55:12 - INFO - __main__ -   Batch Number = 101
01/07/2022 12:55:12 - INFO - __main__ -   Batch Number = 102
Evaluating:  63%|   | 102/163 [00:02<00:01, 44.28it/s]01/07/2022 12:55:12 - INFO - __main__ -   Batch Number = 103
01/07/2022 12:55:12 - INFO - __main__ -   Batch Number = 104
01/07/2022 12:55:12 - INFO - __main__ -   Batch Number = 105
01/07/2022 12:55:12 - INFO - __main__ -   Batch Number = 106
01/07/2022 12:55:12 - INFO - __main__ -   Batch Number = 107
Evaluating:  66%|   | 107/163 [00:02<00:01, 44.31it/s]01/07/2022 12:55:12 - INFO - __main__ -   Batch Number = 108
01/07/2022 12:55:12 - INFO - __main__ -   Batch Number = 109
01/07/2022 12:55:12 - INFO - __main__ -   Batch Number = 110
01/07/2022 12:55:12 - INFO - __main__ -   Batch Number = 111
01/07/2022 12:55:12 - INFO - __main__ -   Batch Number = 112
Evaluating:  69%|   | 112/163 [00:02<00:01, 44.42it/s]01/07/2022 12:55:12 - INFO - __main__ -   Batch Number = 113
01/07/2022 12:55:12 - INFO - __main__ -   Batch Number = 114
01/07/2022 12:55:12 - INFO - __main__ -   Batch Number = 115
01/07/2022 12:55:12 - INFO - __main__ -   Batch Number = 116
01/07/2022 12:55:12 - INFO - __main__ -   Batch Number = 117
Evaluating:  72%|  | 117/163 [00:02<00:01, 44.57it/s]01/07/2022 12:55:12 - INFO - __main__ -   Batch Number = 118
01/07/2022 12:55:12 - INFO - __main__ -   Batch Number = 119
01/07/2022 12:55:12 - INFO - __main__ -   Batch Number = 120
01/07/2022 12:55:12 - INFO - __main__ -   Batch Number = 121
01/07/2022 12:55:12 - INFO - __main__ -   Batch Number = 122
Evaluating:  75%|  | 122/163 [00:02<00:00, 44.49it/s]01/07/2022 12:55:12 - INFO - __main__ -   Batch Number = 123
01/07/2022 12:55:12 - INFO - __main__ -   Batch Number = 124
01/07/2022 12:55:12 - INFO - __main__ -   Batch Number = 125
01/07/2022 12:55:12 - INFO - __main__ -   Batch Number = 126
01/07/2022 12:55:12 - INFO - __main__ -   Batch Number = 127
Evaluating:  78%|  | 127/163 [00:02<00:00, 44.46it/s]01/07/2022 12:55:12 - INFO - __main__ -   Batch Number = 128
01/07/2022 12:55:12 - INFO - __main__ -   Batch Number = 129
01/07/2022 12:55:12 - INFO - __main__ -   Batch Number = 130
01/07/2022 12:55:12 - INFO - __main__ -   Batch Number = 131
01/07/2022 12:55:12 - INFO - __main__ -   Batch Number = 132
Evaluating:  81%|  | 132/163 [00:03<00:00, 44.55it/s]01/07/2022 12:55:12 - INFO - __main__ -   Batch Number = 133
01/07/2022 12:55:12 - INFO - __main__ -   Batch Number = 134
01/07/2022 12:55:12 - INFO - __main__ -   Batch Number = 135
01/07/2022 12:55:12 - INFO - __main__ -   Batch Number = 136
01/07/2022 12:55:12 - INFO - __main__ -   Batch Number = 137
Evaluating:  84%| | 137/163 [00:03<00:00, 44.49it/s]01/07/2022 12:55:12 - INFO - __main__ -   Batch Number = 138
01/07/2022 12:55:12 - INFO - __main__ -   Batch Number = 139
01/07/2022 12:55:12 - INFO - __main__ -   Batch Number = 140
01/07/2022 12:55:12 - INFO - __main__ -   Batch Number = 141
01/07/2022 12:55:12 - INFO - __main__ -   Batch Number = 142
Evaluating:  87%| | 142/163 [00:03<00:00, 41.62it/s]01/07/2022 12:55:13 - INFO - __main__ -   Batch Number = 143
01/07/2022 12:55:13 - INFO - __main__ -   Batch Number = 144
01/07/2022 12:55:13 - INFO - __main__ -   Batch Number = 145
01/07/2022 12:55:13 - INFO - __main__ -   Batch Number = 146
01/07/2022 12:55:13 - INFO - __main__ -   Batch Number = 147
Evaluating:  90%| | 147/163 [00:03<00:00, 42.29it/s]01/07/2022 12:55:13 - INFO - __main__ -   Batch Number = 148
01/07/2022 12:55:13 - INFO - __main__ -   Batch Number = 149
01/07/2022 12:55:13 - INFO - __main__ -   Batch Number = 150
01/07/2022 12:55:13 - INFO - __main__ -   Batch Number = 151
01/07/2022 12:55:13 - INFO - __main__ -   Batch Number = 152
Evaluating:  93%|| 152/163 [00:03<00:00, 42.68it/s]01/07/2022 12:55:13 - INFO - __main__ -   Batch Number = 153
01/07/2022 12:55:13 - INFO - __main__ -   Batch Number = 154
01/07/2022 12:55:13 - INFO - __main__ -   Batch Number = 155
01/07/2022 12:55:13 - INFO - __main__ -   Batch Number = 156
01/07/2022 12:55:13 - INFO - __main__ -   Batch Number = 157
Evaluating:  96%|| 157/163 [00:03<00:00, 43.27it/s]01/07/2022 12:55:13 - INFO - __main__ -   Batch Number = 158
01/07/2022 12:55:13 - INFO - __main__ -   Batch Number = 159
01/07/2022 12:55:13 - INFO - __main__ -   Batch Number = 160
01/07/2022 12:55:13 - INFO - __main__ -   Batch Number = 161
01/07/2022 12:55:13 - INFO - __main__ -   Batch Number = 162
Evaluating:  99%|| 162/163 [00:03<00:00, 43.37it/s]01/07/2022 12:55:13 - INFO - __main__ -   Batch Number = 163
Evaluating: 100%|| 163/163 [00:03<00:00, 43.11it/s]
01/07/2022 12:55:13 - INFO - __main__ -     Evaluation done in total 3.781695 secs (0.002900 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/predictions_es_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/nbest_predictions_es_.json
01/07/2022 12:55:16 - INFO - __main__ -   Results = OrderedDict([('exact', 57.05882352941177), ('f1', 74.31497909198899), ('total', 1190), ('HasAns_exact', 57.05882352941177), ('HasAns_f1', 74.31497909198899), ('HasAns_total', 1190), ('best_exact', 57.05882352941177), ('best_exact_thresh', 0.0), ('best_f1', 74.31497909198899), ('best_f1_thresh', 0.0)])
01/07/2022 12:55:16 - INFO - __main__ -   Language adapter for hi not found, using en instead
01/07/2022 12:55:16 - INFO - __main__ -   Set active language adapter to en
01/07/2022 12:55:16 - INFO - __main__ -   Args Adapter Weight = None
01/07/2022 12:55:16 - INFO - __main__ -   Adapter Languages = ['en']
01/07/2022 12:55:16 - INFO - __main__ -   Predict File = xquad.hi.json
01/07/2022 12:55:16 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
  0%|          | 0/48 [00:00<?, ?it/s] 33%|      | 16/48 [00:00<00:00, 128.82it/s] 60%|    | 29/48 [00:00<00:00, 120.79it/s] 90%| | 43/48 [00:00<00:00, 126.38it/s]100%|| 48/48 [00:00<00:00, 128.01it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<03:39,  5.41it/s]convert squad examples to features:  32%|      | 385/1190 [00:00<00:01, 728.96it/s]convert squad examples to features:  92%|| 1089/1190 [00:00<00:00, 1955.17it/s]convert squad examples to features: 100%|| 1190/1190 [00:00<00:00, 1526.49it/s]/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|| 1190/1190 [00:00<00:00, 597250.42it/s]
01/07/2022 12:55:18 - INFO - __main__ -   Local Rank = -1
01/07/2022 12:55:18 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.hi.json_xlm-roberta-base_384_hi
01/07/2022 12:55:20 - INFO - __main__ -   ***** Running evaluation  *****
01/07/2022 12:55:20 - INFO - __main__ -     Num examples = 1382
01/07/2022 12:55:20 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/173 [00:00<?, ?it/s]01/07/2022 12:55:20 - INFO - __main__ -   Batch Number = 1
01/07/2022 12:55:20 - INFO - __main__ -   Batch Number = 2
01/07/2022 12:55:20 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|         | 3/173 [00:00<00:05, 29.10it/s]01/07/2022 12:55:20 - INFO - __main__ -   Batch Number = 4
01/07/2022 12:55:20 - INFO - __main__ -   Batch Number = 5
01/07/2022 12:55:20 - INFO - __main__ -   Batch Number = 6
01/07/2022 12:55:20 - INFO - __main__ -   Batch Number = 7
Evaluating:   4%|         | 7/173 [00:00<00:05, 32.24it/s]01/07/2022 12:55:20 - INFO - __main__ -   Batch Number = 8
01/07/2022 12:55:20 - INFO - __main__ -   Batch Number = 9
01/07/2022 12:55:20 - INFO - __main__ -   Batch Number = 10
01/07/2022 12:55:20 - INFO - __main__ -   Batch Number = 11
01/07/2022 12:55:20 - INFO - __main__ -   Batch Number = 12
Evaluating:   7%|         | 12/173 [00:00<00:04, 37.90it/s]01/07/2022 12:55:20 - INFO - __main__ -   Batch Number = 13
01/07/2022 12:55:20 - INFO - __main__ -   Batch Number = 14
01/07/2022 12:55:20 - INFO - __main__ -   Batch Number = 15
01/07/2022 12:55:20 - INFO - __main__ -   Batch Number = 16
01/07/2022 12:55:20 - INFO - __main__ -   Batch Number = 17
Evaluating:  10%|         | 17/173 [00:00<00:04, 38.29it/s]01/07/2022 12:55:20 - INFO - __main__ -   Batch Number = 18
01/07/2022 12:55:20 - INFO - __main__ -   Batch Number = 19
01/07/2022 12:55:20 - INFO - __main__ -   Batch Number = 20
01/07/2022 12:55:20 - INFO - __main__ -   Batch Number = 21
01/07/2022 12:55:20 - INFO - __main__ -   Batch Number = 22
Evaluating:  13%|        | 22/173 [00:00<00:03, 40.41it/s]01/07/2022 12:55:20 - INFO - __main__ -   Batch Number = 23
01/07/2022 12:55:20 - INFO - __main__ -   Batch Number = 24
01/07/2022 12:55:20 - INFO - __main__ -   Batch Number = 25
01/07/2022 12:55:20 - INFO - __main__ -   Batch Number = 26
01/07/2022 12:55:20 - INFO - __main__ -   Batch Number = 27
Evaluating:  16%|        | 27/173 [00:00<00:03, 41.78it/s]01/07/2022 12:55:20 - INFO - __main__ -   Batch Number = 28
01/07/2022 12:55:20 - INFO - __main__ -   Batch Number = 29
01/07/2022 12:55:20 - INFO - __main__ -   Batch Number = 30
01/07/2022 12:55:20 - INFO - __main__ -   Batch Number = 31
01/07/2022 12:55:20 - INFO - __main__ -   Batch Number = 32
Evaluating:  18%|        | 32/173 [00:00<00:03, 42.56it/s]01/07/2022 12:55:20 - INFO - __main__ -   Batch Number = 33
01/07/2022 12:55:20 - INFO - __main__ -   Batch Number = 34
01/07/2022 12:55:20 - INFO - __main__ -   Batch Number = 35
01/07/2022 12:55:20 - INFO - __main__ -   Batch Number = 36
01/07/2022 12:55:20 - INFO - __main__ -   Batch Number = 37
Evaluating:  21%|       | 37/173 [00:00<00:03, 43.23it/s]01/07/2022 12:55:20 - INFO - __main__ -   Batch Number = 38
01/07/2022 12:55:20 - INFO - __main__ -   Batch Number = 39
01/07/2022 12:55:21 - INFO - __main__ -   Batch Number = 40
01/07/2022 12:55:21 - INFO - __main__ -   Batch Number = 41
01/07/2022 12:55:21 - INFO - __main__ -   Batch Number = 42
Evaluating:  24%|       | 42/173 [00:01<00:04, 31.96it/s]01/07/2022 12:55:21 - INFO - __main__ -   Batch Number = 43
01/07/2022 12:55:21 - INFO - __main__ -   Batch Number = 44
01/07/2022 12:55:21 - INFO - __main__ -   Batch Number = 45
01/07/2022 12:55:21 - INFO - __main__ -   Batch Number = 46
Evaluating:  27%|       | 46/173 [00:01<00:04, 27.21it/s]01/07/2022 12:55:21 - INFO - __main__ -   Batch Number = 47
01/07/2022 12:55:21 - INFO - __main__ -   Batch Number = 48
01/07/2022 12:55:21 - INFO - __main__ -   Batch Number = 49
01/07/2022 12:55:21 - INFO - __main__ -   Batch Number = 50
Evaluating:  29%|       | 50/173 [00:01<00:05, 23.15it/s]01/07/2022 12:55:21 - INFO - __main__ -   Batch Number = 51
01/07/2022 12:55:21 - INFO - __main__ -   Batch Number = 52
01/07/2022 12:55:21 - INFO - __main__ -   Batch Number = 53
Evaluating:  31%|       | 53/173 [00:01<00:05, 21.99it/s]01/07/2022 12:55:21 - INFO - __main__ -   Batch Number = 54
01/07/2022 12:55:21 - INFO - __main__ -   Batch Number = 55
01/07/2022 12:55:21 - INFO - __main__ -   Batch Number = 56
Evaluating:  32%|      | 56/173 [00:01<00:05, 21.25it/s]01/07/2022 12:55:21 - INFO - __main__ -   Batch Number = 57
01/07/2022 12:55:21 - INFO - __main__ -   Batch Number = 58
01/07/2022 12:55:22 - INFO - __main__ -   Batch Number = 59
Evaluating:  34%|      | 59/173 [00:02<00:05, 20.68it/s]01/07/2022 12:55:22 - INFO - __main__ -   Batch Number = 60
01/07/2022 12:55:22 - INFO - __main__ -   Batch Number = 61
01/07/2022 12:55:22 - INFO - __main__ -   Batch Number = 62
Evaluating:  36%|      | 62/173 [00:02<00:05, 20.24it/s]01/07/2022 12:55:22 - INFO - __main__ -   Batch Number = 63
01/07/2022 12:55:22 - INFO - __main__ -   Batch Number = 64
01/07/2022 12:55:22 - INFO - __main__ -   Batch Number = 65
Evaluating:  38%|      | 65/173 [00:02<00:05, 19.96it/s]01/07/2022 12:55:22 - INFO - __main__ -   Batch Number = 66
01/07/2022 12:55:22 - INFO - __main__ -   Batch Number = 67
01/07/2022 12:55:22 - INFO - __main__ -   Batch Number = 68
Evaluating:  39%|      | 68/173 [00:02<00:05, 19.92it/s]01/07/2022 12:55:22 - INFO - __main__ -   Batch Number = 69
01/07/2022 12:55:22 - INFO - __main__ -   Batch Number = 70
01/07/2022 12:55:22 - INFO - __main__ -   Batch Number = 71
Evaluating:  41%|      | 71/173 [00:02<00:05, 19.63it/s]01/07/2022 12:55:22 - INFO - __main__ -   Batch Number = 72
01/07/2022 12:55:22 - INFO - __main__ -   Batch Number = 73
Evaluating:  42%|     | 73/173 [00:02<00:05, 19.47it/s]01/07/2022 12:55:22 - INFO - __main__ -   Batch Number = 74
01/07/2022 12:55:22 - INFO - __main__ -   Batch Number = 75
Evaluating:  43%|     | 75/173 [00:02<00:05, 19.29it/s]01/07/2022 12:55:22 - INFO - __main__ -   Batch Number = 76
01/07/2022 12:55:22 - INFO - __main__ -   Batch Number = 77
Evaluating:  45%|     | 77/173 [00:03<00:05, 19.16it/s]01/07/2022 12:55:23 - INFO - __main__ -   Batch Number = 78
01/07/2022 12:55:23 - INFO - __main__ -   Batch Number = 79
Evaluating:  46%|     | 79/173 [00:03<00:04, 19.08it/s]01/07/2022 12:55:23 - INFO - __main__ -   Batch Number = 80
01/07/2022 12:55:23 - INFO - __main__ -   Batch Number = 81
Evaluating:  47%|     | 81/173 [00:03<00:04, 19.09it/s]01/07/2022 12:55:23 - INFO - __main__ -   Batch Number = 82
01/07/2022 12:55:23 - INFO - __main__ -   Batch Number = 83
01/07/2022 12:55:23 - INFO - __main__ -   Batch Number = 84
01/07/2022 12:55:23 - INFO - __main__ -   Batch Number = 85
01/07/2022 12:55:23 - INFO - __main__ -   Batch Number = 86
Evaluating:  50%|     | 86/173 [00:03<00:03, 26.71it/s]01/07/2022 12:55:23 - INFO - __main__ -   Batch Number = 87
01/07/2022 12:55:23 - INFO - __main__ -   Batch Number = 88
01/07/2022 12:55:23 - INFO - __main__ -   Batch Number = 89
Evaluating:  51%|    | 89/173 [00:03<00:03, 25.72it/s]01/07/2022 12:55:23 - INFO - __main__ -   Batch Number = 90
01/07/2022 12:55:23 - INFO - __main__ -   Batch Number = 91
01/07/2022 12:55:23 - INFO - __main__ -   Batch Number = 92
Evaluating:  53%|    | 92/173 [00:03<00:03, 23.15it/s]01/07/2022 12:55:23 - INFO - __main__ -   Batch Number = 93
01/07/2022 12:55:23 - INFO - __main__ -   Batch Number = 94
01/07/2022 12:55:23 - INFO - __main__ -   Batch Number = 95
Evaluating:  55%|    | 95/173 [00:03<00:03, 21.68it/s]01/07/2022 12:55:23 - INFO - __main__ -   Batch Number = 96
01/07/2022 12:55:23 - INFO - __main__ -   Batch Number = 97
01/07/2022 12:55:23 - INFO - __main__ -   Batch Number = 98
Evaluating:  57%|    | 98/173 [00:03<00:03, 20.68it/s]01/07/2022 12:55:23 - INFO - __main__ -   Batch Number = 99
01/07/2022 12:55:24 - INFO - __main__ -   Batch Number = 100
01/07/2022 12:55:24 - INFO - __main__ -   Batch Number = 101
Evaluating:  58%|    | 101/173 [00:04<00:03, 20.18it/s]01/07/2022 12:55:24 - INFO - __main__ -   Batch Number = 102
01/07/2022 12:55:24 - INFO - __main__ -   Batch Number = 103
01/07/2022 12:55:24 - INFO - __main__ -   Batch Number = 104
Evaluating:  60%|    | 104/173 [00:04<00:03, 19.80it/s]01/07/2022 12:55:24 - INFO - __main__ -   Batch Number = 105
01/07/2022 12:55:24 - INFO - __main__ -   Batch Number = 106
01/07/2022 12:55:24 - INFO - __main__ -   Batch Number = 107
Evaluating:  62%|   | 107/173 [00:04<00:03, 18.57it/s]01/07/2022 12:55:24 - INFO - __main__ -   Batch Number = 108
01/07/2022 12:55:24 - INFO - __main__ -   Batch Number = 109
Evaluating:  63%|   | 109/173 [00:04<00:03, 18.68it/s]01/07/2022 12:55:24 - INFO - __main__ -   Batch Number = 110
01/07/2022 12:55:24 - INFO - __main__ -   Batch Number = 111
Evaluating:  64%|   | 111/173 [00:04<00:03, 18.61it/s]01/07/2022 12:55:24 - INFO - __main__ -   Batch Number = 112
01/07/2022 12:55:24 - INFO - __main__ -   Batch Number = 113
Evaluating:  65%|   | 113/173 [00:04<00:03, 18.68it/s]01/07/2022 12:55:24 - INFO - __main__ -   Batch Number = 114
01/07/2022 12:55:24 - INFO - __main__ -   Batch Number = 115
Evaluating:  66%|   | 115/173 [00:04<00:03, 18.79it/s]01/07/2022 12:55:24 - INFO - __main__ -   Batch Number = 116
01/07/2022 12:55:24 - INFO - __main__ -   Batch Number = 117
Evaluating:  68%|   | 117/173 [00:04<00:03, 18.62it/s]01/07/2022 12:55:25 - INFO - __main__ -   Batch Number = 118
01/07/2022 12:55:25 - INFO - __main__ -   Batch Number = 119
Evaluating:  69%|   | 119/173 [00:05<00:02, 18.58it/s]01/07/2022 12:55:25 - INFO - __main__ -   Batch Number = 120
01/07/2022 12:55:25 - INFO - __main__ -   Batch Number = 121
Evaluating:  70%|   | 121/173 [00:05<00:02, 18.56it/s]01/07/2022 12:55:25 - INFO - __main__ -   Batch Number = 122
01/07/2022 12:55:25 - INFO - __main__ -   Batch Number = 123
Evaluating:  71%|   | 123/173 [00:05<00:02, 18.60it/s]01/07/2022 12:55:25 - INFO - __main__ -   Batch Number = 124
01/07/2022 12:55:25 - INFO - __main__ -   Batch Number = 125
Evaluating:  72%|  | 125/173 [00:05<00:02, 18.55it/s]01/07/2022 12:55:25 - INFO - __main__ -   Batch Number = 126
01/07/2022 12:55:25 - INFO - __main__ -   Batch Number = 127
Evaluating:  73%|  | 127/173 [00:05<00:02, 18.46it/s]01/07/2022 12:55:25 - INFO - __main__ -   Batch Number = 128
01/07/2022 12:55:25 - INFO - __main__ -   Batch Number = 129
Evaluating:  75%|  | 129/173 [00:05<00:02, 18.49it/s]01/07/2022 12:55:25 - INFO - __main__ -   Batch Number = 130
01/07/2022 12:55:25 - INFO - __main__ -   Batch Number = 131
Evaluating:  76%|  | 131/173 [00:05<00:02, 18.65it/s]01/07/2022 12:55:25 - INFO - __main__ -   Batch Number = 132
01/07/2022 12:55:25 - INFO - __main__ -   Batch Number = 133
Evaluating:  77%|  | 133/173 [00:05<00:02, 18.46it/s]01/07/2022 12:55:25 - INFO - __main__ -   Batch Number = 134
01/07/2022 12:55:25 - INFO - __main__ -   Batch Number = 135
Evaluating:  78%|  | 135/173 [00:05<00:02, 18.46it/s]01/07/2022 12:55:25 - INFO - __main__ -   Batch Number = 136
01/07/2022 12:55:26 - INFO - __main__ -   Batch Number = 137
Evaluating:  79%|  | 137/173 [00:06<00:01, 18.60it/s]01/07/2022 12:55:26 - INFO - __main__ -   Batch Number = 138
01/07/2022 12:55:26 - INFO - __main__ -   Batch Number = 139
Evaluating:  80%|  | 139/173 [00:06<00:01, 18.55it/s]01/07/2022 12:55:26 - INFO - __main__ -   Batch Number = 140
01/07/2022 12:55:26 - INFO - __main__ -   Batch Number = 141
Evaluating:  82%| | 141/173 [00:06<00:01, 17.57it/s]01/07/2022 12:55:26 - INFO - __main__ -   Batch Number = 142
01/07/2022 12:55:26 - INFO - __main__ -   Batch Number = 143
Evaluating:  83%| | 143/173 [00:06<00:01, 17.60it/s]01/07/2022 12:55:26 - INFO - __main__ -   Batch Number = 144
01/07/2022 12:55:26 - INFO - __main__ -   Batch Number = 145
Evaluating:  84%| | 145/173 [00:06<00:01, 17.89it/s]01/07/2022 12:55:26 - INFO - __main__ -   Batch Number = 146
01/07/2022 12:55:26 - INFO - __main__ -   Batch Number = 147
Evaluating:  85%| | 147/173 [00:06<00:01, 18.16it/s]01/07/2022 12:55:26 - INFO - __main__ -   Batch Number = 148
01/07/2022 12:55:26 - INFO - __main__ -   Batch Number = 149
Evaluating:  86%| | 149/173 [00:06<00:01, 18.46it/s]01/07/2022 12:55:26 - INFO - __main__ -   Batch Number = 150
01/07/2022 12:55:26 - INFO - __main__ -   Batch Number = 151
Evaluating:  87%| | 151/173 [00:06<00:01, 18.59it/s]01/07/2022 12:55:26 - INFO - __main__ -   Batch Number = 152
01/07/2022 12:55:26 - INFO - __main__ -   Batch Number = 153
Evaluating:  88%| | 153/173 [00:06<00:01, 18.63it/s]01/07/2022 12:55:26 - INFO - __main__ -   Batch Number = 154
01/07/2022 12:55:27 - INFO - __main__ -   Batch Number = 155
Evaluating:  90%| | 155/173 [00:07<00:00, 18.74it/s]01/07/2022 12:55:27 - INFO - __main__ -   Batch Number = 156
01/07/2022 12:55:27 - INFO - __main__ -   Batch Number = 157
Evaluating:  91%| | 157/173 [00:07<00:00, 18.82it/s]01/07/2022 12:55:27 - INFO - __main__ -   Batch Number = 158
01/07/2022 12:55:27 - INFO - __main__ -   Batch Number = 159
Evaluating:  92%|| 159/173 [00:07<00:00, 18.89it/s]01/07/2022 12:55:27 - INFO - __main__ -   Batch Number = 160
01/07/2022 12:55:27 - INFO - __main__ -   Batch Number = 161
Evaluating:  93%|| 161/173 [00:07<00:00, 19.06it/s]01/07/2022 12:55:27 - INFO - __main__ -   Batch Number = 162
01/07/2022 12:55:27 - INFO - __main__ -   Batch Number = 163
Evaluating:  94%|| 163/173 [00:07<00:00, 19.01it/s]01/07/2022 12:55:27 - INFO - __main__ -   Batch Number = 164
01/07/2022 12:55:27 - INFO - __main__ -   Batch Number = 165
Evaluating:  95%|| 165/173 [00:07<00:00, 18.99it/s]01/07/2022 12:55:27 - INFO - __main__ -   Batch Number = 166
01/07/2022 12:55:27 - INFO - __main__ -   Batch Number = 167
Evaluating:  97%|| 167/173 [00:07<00:00, 18.87it/s]01/07/2022 12:55:27 - INFO - __main__ -   Batch Number = 168
01/07/2022 12:55:27 - INFO - __main__ -   Batch Number = 169
Evaluating:  98%|| 169/173 [00:07<00:00, 18.94it/s]01/07/2022 12:55:27 - INFO - __main__ -   Batch Number = 170
01/07/2022 12:55:27 - INFO - __main__ -   Batch Number = 171
Evaluating:  99%|| 171/173 [00:07<00:00, 18.75it/s]01/07/2022 12:55:27 - INFO - __main__ -   Batch Number = 172
01/07/2022 12:55:27 - INFO - __main__ -   Batch Number = 173
Evaluating: 100%|| 173/173 [00:07<00:00, 21.65it/s]
01/07/2022 12:55:28 - INFO - __main__ -     Evaluation done in total 7.989576 secs (0.005781 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/predictions_hi_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/nbest_predictions_hi_.json
01/07/2022 12:55:30 - INFO - __main__ -   Results = OrderedDict([('exact', 48.15126050420168), ('f1', 64.89924936591798), ('total', 1190), ('HasAns_exact', 48.15126050420168), ('HasAns_f1', 64.89924936591798), ('HasAns_total', 1190), ('best_exact', 48.15126050420168), ('best_exact_thresh', 0.0), ('best_f1', 64.89924936591798), ('best_f1_thresh', 0.0)])
01/07/2022 12:55:31 - INFO - __main__ -   Language adapter for ru not found, using en instead
01/07/2022 12:55:31 - INFO - __main__ -   Set active language adapter to en
01/07/2022 12:55:31 - INFO - __main__ -   Args Adapter Weight = None
01/07/2022 12:55:31 - INFO - __main__ -   Adapter Languages = ['en']
01/07/2022 12:55:31 - INFO - __main__ -   Predict File = xquad.ru.json
01/07/2022 12:55:31 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
  0%|          | 0/48 [00:00<?, ?it/s] 31%|      | 15/48 [00:00<00:00, 146.11it/s] 62%|   | 30/48 [00:00<00:00, 108.01it/s] 92%|| 44/48 [00:00<00:00, 117.37it/s]100%|| 48/48 [00:00<00:00, 119.67it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<05:36,  3.53it/s]convert squad examples to features:   0%|          | 2/1190 [00:00<03:31,  5.62it/s]convert squad examples to features:  32%|      | 385/1190 [00:00<00:01, 703.85it/s]convert squad examples to features:  94%|| 1121/1190 [00:00<00:00, 1802.67it/s]convert squad examples to features: 100%|| 1190/1190 [00:00<00:00, 1325.70it/s]/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|| 1190/1190 [00:00<00:00, 685984.30it/s]
01/07/2022 12:55:33 - INFO - __main__ -   Local Rank = -1
01/07/2022 12:55:33 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.ru.json_xlm-roberta-base_384_ru
01/07/2022 12:55:34 - INFO - __main__ -   ***** Running evaluation  *****
01/07/2022 12:55:34 - INFO - __main__ -     Num examples = 1332
01/07/2022 12:55:34 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/167 [00:00<?, ?it/s]01/07/2022 12:55:34 - INFO - __main__ -   Batch Number = 1
01/07/2022 12:55:34 - INFO - __main__ -   Batch Number = 2
01/07/2022 12:55:34 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|         | 3/167 [00:00<00:06, 26.63it/s]01/07/2022 12:55:34 - INFO - __main__ -   Batch Number = 4
01/07/2022 12:55:34 - INFO - __main__ -   Batch Number = 5
01/07/2022 12:55:34 - INFO - __main__ -   Batch Number = 6
01/07/2022 12:55:34 - INFO - __main__ -   Batch Number = 7
Evaluating:   4%|         | 7/167 [00:00<00:05, 30.73it/s]01/07/2022 12:55:34 - INFO - __main__ -   Batch Number = 8
01/07/2022 12:55:34 - INFO - __main__ -   Batch Number = 9
01/07/2022 12:55:34 - INFO - __main__ -   Batch Number = 10
01/07/2022 12:55:34 - INFO - __main__ -   Batch Number = 11
01/07/2022 12:55:34 - INFO - __main__ -   Batch Number = 12
Evaluating:   7%|         | 12/167 [00:00<00:04, 37.02it/s]01/07/2022 12:55:34 - INFO - __main__ -   Batch Number = 13
01/07/2022 12:55:34 - INFO - __main__ -   Batch Number = 14
01/07/2022 12:55:34 - INFO - __main__ -   Batch Number = 15
01/07/2022 12:55:34 - INFO - __main__ -   Batch Number = 16
01/07/2022 12:55:34 - INFO - __main__ -   Batch Number = 17
Evaluating:  10%|         | 17/167 [00:00<00:03, 39.83it/s]01/07/2022 12:55:34 - INFO - __main__ -   Batch Number = 18
01/07/2022 12:55:34 - INFO - __main__ -   Batch Number = 19
01/07/2022 12:55:34 - INFO - __main__ -   Batch Number = 20
01/07/2022 12:55:34 - INFO - __main__ -   Batch Number = 21
01/07/2022 12:55:34 - INFO - __main__ -   Batch Number = 22
Evaluating:  13%|        | 22/167 [00:00<00:03, 41.35it/s]01/07/2022 12:55:34 - INFO - __main__ -   Batch Number = 23
01/07/2022 12:55:34 - INFO - __main__ -   Batch Number = 24
01/07/2022 12:55:34 - INFO - __main__ -   Batch Number = 25
01/07/2022 12:55:34 - INFO - __main__ -   Batch Number = 26
01/07/2022 12:55:34 - INFO - __main__ -   Batch Number = 27
Evaluating:  16%|        | 27/167 [00:00<00:03, 42.47it/s]01/07/2022 12:55:34 - INFO - __main__ -   Batch Number = 28
01/07/2022 12:55:34 - INFO - __main__ -   Batch Number = 29
01/07/2022 12:55:34 - INFO - __main__ -   Batch Number = 30
01/07/2022 12:55:34 - INFO - __main__ -   Batch Number = 31
01/07/2022 12:55:34 - INFO - __main__ -   Batch Number = 32
Evaluating:  19%|        | 32/167 [00:00<00:03, 43.05it/s]01/07/2022 12:55:34 - INFO - __main__ -   Batch Number = 33
01/07/2022 12:55:34 - INFO - __main__ -   Batch Number = 34
01/07/2022 12:55:34 - INFO - __main__ -   Batch Number = 35
01/07/2022 12:55:34 - INFO - __main__ -   Batch Number = 36
01/07/2022 12:55:34 - INFO - __main__ -   Batch Number = 37
Evaluating:  22%|       | 37/167 [00:00<00:02, 43.48it/s]01/07/2022 12:55:35 - INFO - __main__ -   Batch Number = 38
01/07/2022 12:55:35 - INFO - __main__ -   Batch Number = 39
01/07/2022 12:55:35 - INFO - __main__ -   Batch Number = 40
01/07/2022 12:55:35 - INFO - __main__ -   Batch Number = 41
01/07/2022 12:55:35 - INFO - __main__ -   Batch Number = 42
Evaluating:  25%|       | 42/167 [00:01<00:02, 43.84it/s]01/07/2022 12:55:35 - INFO - __main__ -   Batch Number = 43
01/07/2022 12:55:35 - INFO - __main__ -   Batch Number = 44
01/07/2022 12:55:35 - INFO - __main__ -   Batch Number = 45
01/07/2022 12:55:35 - INFO - __main__ -   Batch Number = 46
01/07/2022 12:55:35 - INFO - __main__ -   Batch Number = 47
Evaluating:  28%|       | 47/167 [00:01<00:02, 43.92it/s]01/07/2022 12:55:35 - INFO - __main__ -   Batch Number = 48
01/07/2022 12:55:35 - INFO - __main__ -   Batch Number = 49
01/07/2022 12:55:35 - INFO - __main__ -   Batch Number = 50
01/07/2022 12:55:35 - INFO - __main__ -   Batch Number = 51
01/07/2022 12:55:35 - INFO - __main__ -   Batch Number = 52
Evaluating:  31%|       | 52/167 [00:01<00:02, 44.24it/s]01/07/2022 12:55:35 - INFO - __main__ -   Batch Number = 53
01/07/2022 12:55:35 - INFO - __main__ -   Batch Number = 54
01/07/2022 12:55:35 - INFO - __main__ -   Batch Number = 55
01/07/2022 12:55:35 - INFO - __main__ -   Batch Number = 56
01/07/2022 12:55:35 - INFO - __main__ -   Batch Number = 57
Evaluating:  34%|      | 57/167 [00:01<00:02, 44.34it/s]01/07/2022 12:55:35 - INFO - __main__ -   Batch Number = 58
01/07/2022 12:55:35 - INFO - __main__ -   Batch Number = 59
01/07/2022 12:55:35 - INFO - __main__ -   Batch Number = 60
01/07/2022 12:55:35 - INFO - __main__ -   Batch Number = 61
01/07/2022 12:55:35 - INFO - __main__ -   Batch Number = 62
Evaluating:  37%|      | 62/167 [00:01<00:02, 44.39it/s]01/07/2022 12:55:35 - INFO - __main__ -   Batch Number = 63
01/07/2022 12:55:35 - INFO - __main__ -   Batch Number = 64
01/07/2022 12:55:35 - INFO - __main__ -   Batch Number = 65
01/07/2022 12:55:35 - INFO - __main__ -   Batch Number = 66
01/07/2022 12:55:35 - INFO - __main__ -   Batch Number = 67
Evaluating:  40%|      | 67/167 [00:01<00:02, 44.23it/s]01/07/2022 12:55:35 - INFO - __main__ -   Batch Number = 68
01/07/2022 12:55:35 - INFO - __main__ -   Batch Number = 69
01/07/2022 12:55:35 - INFO - __main__ -   Batch Number = 70
01/07/2022 12:55:35 - INFO - __main__ -   Batch Number = 71
01/07/2022 12:55:35 - INFO - __main__ -   Batch Number = 72
Evaluating:  43%|     | 72/167 [00:01<00:02, 44.34it/s]01/07/2022 12:55:35 - INFO - __main__ -   Batch Number = 73
01/07/2022 12:55:35 - INFO - __main__ -   Batch Number = 74
01/07/2022 12:55:35 - INFO - __main__ -   Batch Number = 75
01/07/2022 12:55:35 - INFO - __main__ -   Batch Number = 76
01/07/2022 12:55:35 - INFO - __main__ -   Batch Number = 77
Evaluating:  46%|     | 77/167 [00:01<00:02, 44.27it/s]01/07/2022 12:55:35 - INFO - __main__ -   Batch Number = 78
01/07/2022 12:55:35 - INFO - __main__ -   Batch Number = 79
01/07/2022 12:55:35 - INFO - __main__ -   Batch Number = 80
01/07/2022 12:55:35 - INFO - __main__ -   Batch Number = 81
01/07/2022 12:55:36 - INFO - __main__ -   Batch Number = 82
Evaluating:  49%|     | 82/167 [00:01<00:01, 44.32it/s]01/07/2022 12:55:36 - INFO - __main__ -   Batch Number = 83
01/07/2022 12:55:36 - INFO - __main__ -   Batch Number = 84
01/07/2022 12:55:36 - INFO - __main__ -   Batch Number = 85
01/07/2022 12:55:36 - INFO - __main__ -   Batch Number = 86
01/07/2022 12:55:36 - INFO - __main__ -   Batch Number = 87
Evaluating:  52%|    | 87/167 [00:02<00:01, 44.31it/s]01/07/2022 12:55:36 - INFO - __main__ -   Batch Number = 88
01/07/2022 12:55:36 - INFO - __main__ -   Batch Number = 89
01/07/2022 12:55:36 - INFO - __main__ -   Batch Number = 90
01/07/2022 12:55:36 - INFO - __main__ -   Batch Number = 91
01/07/2022 12:55:36 - INFO - __main__ -   Batch Number = 92
Evaluating:  55%|    | 92/167 [00:02<00:01, 44.16it/s]01/07/2022 12:55:36 - INFO - __main__ -   Batch Number = 93
01/07/2022 12:55:36 - INFO - __main__ -   Batch Number = 94
01/07/2022 12:55:36 - INFO - __main__ -   Batch Number = 95
01/07/2022 12:55:36 - INFO - __main__ -   Batch Number = 96
01/07/2022 12:55:36 - INFO - __main__ -   Batch Number = 97
Evaluating:  58%|    | 97/167 [00:02<00:01, 44.29it/s]01/07/2022 12:55:36 - INFO - __main__ -   Batch Number = 98
01/07/2022 12:55:36 - INFO - __main__ -   Batch Number = 99
01/07/2022 12:55:36 - INFO - __main__ -   Batch Number = 100
01/07/2022 12:55:36 - INFO - __main__ -   Batch Number = 101
01/07/2022 12:55:36 - INFO - __main__ -   Batch Number = 102
Evaluating:  61%|    | 102/167 [00:02<00:01, 44.31it/s]01/07/2022 12:55:36 - INFO - __main__ -   Batch Number = 103
01/07/2022 12:55:36 - INFO - __main__ -   Batch Number = 104
01/07/2022 12:55:36 - INFO - __main__ -   Batch Number = 105
01/07/2022 12:55:36 - INFO - __main__ -   Batch Number = 106
01/07/2022 12:55:36 - INFO - __main__ -   Batch Number = 107
Evaluating:  64%|   | 107/167 [00:02<00:01, 44.16it/s]01/07/2022 12:55:36 - INFO - __main__ -   Batch Number = 108
01/07/2022 12:55:36 - INFO - __main__ -   Batch Number = 109
01/07/2022 12:55:36 - INFO - __main__ -   Batch Number = 110
01/07/2022 12:55:36 - INFO - __main__ -   Batch Number = 111
01/07/2022 12:55:36 - INFO - __main__ -   Batch Number = 112
Evaluating:  67%|   | 112/167 [00:02<00:01, 44.26it/s]01/07/2022 12:55:36 - INFO - __main__ -   Batch Number = 113
01/07/2022 12:55:36 - INFO - __main__ -   Batch Number = 114
01/07/2022 12:55:36 - INFO - __main__ -   Batch Number = 115
01/07/2022 12:55:36 - INFO - __main__ -   Batch Number = 116
01/07/2022 12:55:36 - INFO - __main__ -   Batch Number = 117
Evaluating:  70%|   | 117/167 [00:02<00:01, 44.23it/s]01/07/2022 12:55:36 - INFO - __main__ -   Batch Number = 118
01/07/2022 12:55:36 - INFO - __main__ -   Batch Number = 119
01/07/2022 12:55:36 - INFO - __main__ -   Batch Number = 120
01/07/2022 12:55:36 - INFO - __main__ -   Batch Number = 121
01/07/2022 12:55:36 - INFO - __main__ -   Batch Number = 122
Evaluating:  73%|  | 122/167 [00:02<00:01, 44.24it/s]01/07/2022 12:55:36 - INFO - __main__ -   Batch Number = 123
01/07/2022 12:55:36 - INFO - __main__ -   Batch Number = 124
01/07/2022 12:55:36 - INFO - __main__ -   Batch Number = 125
01/07/2022 12:55:37 - INFO - __main__ -   Batch Number = 126
01/07/2022 12:55:37 - INFO - __main__ -   Batch Number = 127
Evaluating:  76%|  | 127/167 [00:02<00:00, 44.19it/s]01/07/2022 12:55:37 - INFO - __main__ -   Batch Number = 128
01/07/2022 12:55:37 - INFO - __main__ -   Batch Number = 129
01/07/2022 12:55:37 - INFO - __main__ -   Batch Number = 130
01/07/2022 12:55:37 - INFO - __main__ -   Batch Number = 131
01/07/2022 12:55:37 - INFO - __main__ -   Batch Number = 132
Evaluating:  79%|  | 132/167 [00:03<00:00, 44.11it/s]01/07/2022 12:55:37 - INFO - __main__ -   Batch Number = 133
01/07/2022 12:55:37 - INFO - __main__ -   Batch Number = 134
01/07/2022 12:55:37 - INFO - __main__ -   Batch Number = 135
01/07/2022 12:55:37 - INFO - __main__ -   Batch Number = 136
01/07/2022 12:55:37 - INFO - __main__ -   Batch Number = 137
Evaluating:  82%| | 137/167 [00:03<00:00, 42.42it/s]01/07/2022 12:55:37 - INFO - __main__ -   Batch Number = 138
01/07/2022 12:55:37 - INFO - __main__ -   Batch Number = 139
01/07/2022 12:55:37 - INFO - __main__ -   Batch Number = 140
01/07/2022 12:55:37 - INFO - __main__ -   Batch Number = 141
01/07/2022 12:55:37 - INFO - __main__ -   Batch Number = 142
Evaluating:  85%| | 142/167 [00:03<00:00, 43.04it/s]01/07/2022 12:55:37 - INFO - __main__ -   Batch Number = 143
01/07/2022 12:55:37 - INFO - __main__ -   Batch Number = 144
01/07/2022 12:55:37 - INFO - __main__ -   Batch Number = 145
01/07/2022 12:55:37 - INFO - __main__ -   Batch Number = 146
01/07/2022 12:55:37 - INFO - __main__ -   Batch Number = 147
Evaluating:  88%| | 147/167 [00:03<00:00, 43.38it/s]01/07/2022 12:55:37 - INFO - __main__ -   Batch Number = 148
01/07/2022 12:55:37 - INFO - __main__ -   Batch Number = 149
01/07/2022 12:55:37 - INFO - __main__ -   Batch Number = 150
01/07/2022 12:55:37 - INFO - __main__ -   Batch Number = 151
01/07/2022 12:55:37 - INFO - __main__ -   Batch Number = 152
Evaluating:  91%| | 152/167 [00:03<00:00, 43.48it/s]01/07/2022 12:55:37 - INFO - __main__ -   Batch Number = 153
01/07/2022 12:55:37 - INFO - __main__ -   Batch Number = 154
01/07/2022 12:55:37 - INFO - __main__ -   Batch Number = 155
01/07/2022 12:55:37 - INFO - __main__ -   Batch Number = 156
01/07/2022 12:55:37 - INFO - __main__ -   Batch Number = 157
Evaluating:  94%|| 157/167 [00:03<00:00, 43.87it/s]01/07/2022 12:55:37 - INFO - __main__ -   Batch Number = 158
01/07/2022 12:55:37 - INFO - __main__ -   Batch Number = 159
01/07/2022 12:55:37 - INFO - __main__ -   Batch Number = 160
01/07/2022 12:55:37 - INFO - __main__ -   Batch Number = 161
01/07/2022 12:55:37 - INFO - __main__ -   Batch Number = 162
Evaluating:  97%|| 162/167 [00:03<00:00, 44.01it/s]01/07/2022 12:55:37 - INFO - __main__ -   Batch Number = 163
01/07/2022 12:55:37 - INFO - __main__ -   Batch Number = 164
01/07/2022 12:55:37 - INFO - __main__ -   Batch Number = 165
01/07/2022 12:55:37 - INFO - __main__ -   Batch Number = 166
01/07/2022 12:55:37 - INFO - __main__ -   Batch Number = 167
Evaluating: 100%|| 167/167 [00:03<00:00, 44.86it/s]Evaluating: 100%|| 167/167 [00:03<00:00, 43.35it/s]
01/07/2022 12:55:37 - INFO - __main__ -     Evaluation done in total 3.852696 secs (0.002892 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/predictions_ru_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/nbest_predictions_ru_.json
01/07/2022 12:55:41 - INFO - __main__ -   Results = OrderedDict([('exact', 56.38655462184874), ('f1', 72.15910521363318), ('total', 1190), ('HasAns_exact', 56.38655462184874), ('HasAns_f1', 72.15910521363318), ('HasAns_total', 1190), ('best_exact', 56.38655462184874), ('best_exact_thresh', 0.0), ('best_f1', 72.15910521363318), ('best_f1_thresh', 0.0)])
01/07/2022 12:55:41 - INFO - __main__ -   Language adapter for th not found, using en instead
01/07/2022 12:55:41 - INFO - __main__ -   Set active language adapter to en
01/07/2022 12:55:41 - INFO - __main__ -   Args Adapter Weight = None
01/07/2022 12:55:41 - INFO - __main__ -   Adapter Languages = ['en']
01/07/2022 12:55:41 - INFO - __main__ -   Predict File = xquad.th.json
01/07/2022 12:55:41 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
  0%|          | 0/48 [00:00<?, ?it/s] 33%|      | 16/48 [00:00<00:00, 126.73it/s] 60%|    | 29/48 [00:00<00:00, 102.12it/s] 92%|| 44/48 [00:00<00:00, 117.80it/s]100%|| 48/48 [00:00<00:00, 117.57it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<06:25,  3.08it/s]convert squad examples to features:  32%|      | 385/1190 [00:00<00:00, 903.69it/s]convert squad examples to features:  57%|    | 673/1190 [00:00<00:00, 1392.04it/s]convert squad examples to features:  73%|  | 865/1190 [00:00<00:00, 1510.13it/s]convert squad examples to features:  92%|| 1089/1190 [00:00<00:00, 1669.76it/s]convert squad examples to features: 100%|| 1190/1190 [00:00<00:00, 1399.10it/s]/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|| 1190/1190 [00:00<00:00, 683354.57it/s]
01/07/2022 12:55:43 - INFO - __main__ -   Local Rank = -1
01/07/2022 12:55:43 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.th.json_xlm-roberta-base_384_th
01/07/2022 12:55:44 - INFO - __main__ -   ***** Running evaluation  *****
01/07/2022 12:55:44 - INFO - __main__ -     Num examples = 1314
01/07/2022 12:55:44 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/165 [00:00<?, ?it/s]01/07/2022 12:55:44 - INFO - __main__ -   Batch Number = 1
01/07/2022 12:55:44 - INFO - __main__ -   Batch Number = 2
01/07/2022 12:55:44 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|         | 3/165 [00:00<00:06, 23.68it/s]01/07/2022 12:55:44 - INFO - __main__ -   Batch Number = 4
01/07/2022 12:55:44 - INFO - __main__ -   Batch Number = 5
01/07/2022 12:55:44 - INFO - __main__ -   Batch Number = 6
01/07/2022 12:55:44 - INFO - __main__ -   Batch Number = 7
Evaluating:   4%|         | 7/165 [00:00<00:05, 28.94it/s]01/07/2022 12:55:44 - INFO - __main__ -   Batch Number = 8
01/07/2022 12:55:44 - INFO - __main__ -   Batch Number = 9
01/07/2022 12:55:44 - INFO - __main__ -   Batch Number = 10
01/07/2022 12:55:44 - INFO - __main__ -   Batch Number = 11
Evaluating:   7%|         | 11/165 [00:00<00:04, 31.33it/s]01/07/2022 12:55:44 - INFO - __main__ -   Batch Number = 12
01/07/2022 12:55:44 - INFO - __main__ -   Batch Number = 13
01/07/2022 12:55:44 - INFO - __main__ -   Batch Number = 14
01/07/2022 12:55:44 - INFO - __main__ -   Batch Number = 15
01/07/2022 12:55:44 - INFO - __main__ -   Batch Number = 16
Evaluating:  10%|         | 16/165 [00:00<00:04, 35.10it/s]01/07/2022 12:55:44 - INFO - __main__ -   Batch Number = 17
01/07/2022 12:55:45 - INFO - __main__ -   Batch Number = 18
01/07/2022 12:55:45 - INFO - __main__ -   Batch Number = 19
01/07/2022 12:55:45 - INFO - __main__ -   Batch Number = 20
01/07/2022 12:55:45 - INFO - __main__ -   Batch Number = 21
Evaluating:  13%|        | 21/165 [00:00<00:03, 37.34it/s]01/07/2022 12:55:45 - INFO - __main__ -   Batch Number = 22
01/07/2022 12:55:45 - INFO - __main__ -   Batch Number = 23
01/07/2022 12:55:45 - INFO - __main__ -   Batch Number = 24
01/07/2022 12:55:45 - INFO - __main__ -   Batch Number = 25
01/07/2022 12:55:45 - INFO - __main__ -   Batch Number = 26
Evaluating:  16%|        | 26/165 [00:00<00:03, 38.04it/s]01/07/2022 12:55:45 - INFO - __main__ -   Batch Number = 27
01/07/2022 12:55:45 - INFO - __main__ -   Batch Number = 28
01/07/2022 12:55:45 - INFO - __main__ -   Batch Number = 29
01/07/2022 12:55:45 - INFO - __main__ -   Batch Number = 30
Evaluating:  18%|        | 30/165 [00:00<00:03, 38.36it/s]01/07/2022 12:55:45 - INFO - __main__ -   Batch Number = 31
01/07/2022 12:55:45 - INFO - __main__ -   Batch Number = 32
01/07/2022 12:55:45 - INFO - __main__ -   Batch Number = 33
01/07/2022 12:55:45 - INFO - __main__ -   Batch Number = 34
Evaluating:  21%|        | 34/165 [00:00<00:03, 38.81it/s]01/07/2022 12:55:45 - INFO - __main__ -   Batch Number = 35
01/07/2022 12:55:45 - INFO - __main__ -   Batch Number = 36
01/07/2022 12:55:45 - INFO - __main__ -   Batch Number = 37
01/07/2022 12:55:45 - INFO - __main__ -   Batch Number = 38
01/07/2022 12:55:45 - INFO - __main__ -   Batch Number = 39
Evaluating:  24%|       | 39/165 [00:01<00:03, 39.50it/s]01/07/2022 12:55:45 - INFO - __main__ -   Batch Number = 40
01/07/2022 12:55:45 - INFO - __main__ -   Batch Number = 41
01/07/2022 12:55:45 - INFO - __main__ -   Batch Number = 42
01/07/2022 12:55:45 - INFO - __main__ -   Batch Number = 43
Evaluating:  26%|       | 43/165 [00:01<00:03, 39.24it/s]01/07/2022 12:55:45 - INFO - __main__ -   Batch Number = 44
01/07/2022 12:55:45 - INFO - __main__ -   Batch Number = 45
01/07/2022 12:55:45 - INFO - __main__ -   Batch Number = 46
01/07/2022 12:55:45 - INFO - __main__ -   Batch Number = 47
Evaluating:  28%|       | 47/165 [00:01<00:02, 39.38it/s]01/07/2022 12:55:45 - INFO - __main__ -   Batch Number = 48
01/07/2022 12:55:45 - INFO - __main__ -   Batch Number = 49
01/07/2022 12:55:45 - INFO - __main__ -   Batch Number = 50
01/07/2022 12:55:45 - INFO - __main__ -   Batch Number = 51
01/07/2022 12:55:45 - INFO - __main__ -   Batch Number = 52
Evaluating:  32%|      | 52/165 [00:01<00:02, 39.50it/s]01/07/2022 12:55:45 - INFO - __main__ -   Batch Number = 53
01/07/2022 12:55:45 - INFO - __main__ -   Batch Number = 54
01/07/2022 12:55:45 - INFO - __main__ -   Batch Number = 55
01/07/2022 12:55:45 - INFO - __main__ -   Batch Number = 56
Evaluating:  34%|      | 56/165 [00:01<00:02, 38.09it/s]01/07/2022 12:55:46 - INFO - __main__ -   Batch Number = 57
01/07/2022 12:55:46 - INFO - __main__ -   Batch Number = 58
01/07/2022 12:55:46 - INFO - __main__ -   Batch Number = 59
01/07/2022 12:55:46 - INFO - __main__ -   Batch Number = 60
Evaluating:  36%|      | 60/165 [00:01<00:03, 34.66it/s]01/07/2022 12:55:46 - INFO - __main__ -   Batch Number = 61
01/07/2022 12:55:46 - INFO - __main__ -   Batch Number = 62
01/07/2022 12:55:46 - INFO - __main__ -   Batch Number = 63
01/07/2022 12:55:46 - INFO - __main__ -   Batch Number = 64
Evaluating:  39%|      | 64/165 [00:01<00:02, 35.35it/s]01/07/2022 12:55:46 - INFO - __main__ -   Batch Number = 65
01/07/2022 12:55:46 - INFO - __main__ -   Batch Number = 66
01/07/2022 12:55:46 - INFO - __main__ -   Batch Number = 67
01/07/2022 12:55:46 - INFO - __main__ -   Batch Number = 68
Evaluating:  41%|      | 68/165 [00:01<00:02, 35.92it/s]01/07/2022 12:55:46 - INFO - __main__ -   Batch Number = 69
01/07/2022 12:55:46 - INFO - __main__ -   Batch Number = 70
01/07/2022 12:55:46 - INFO - __main__ -   Batch Number = 71
01/07/2022 12:55:46 - INFO - __main__ -   Batch Number = 72
Evaluating:  44%|     | 72/165 [00:01<00:02, 36.89it/s]01/07/2022 12:55:46 - INFO - __main__ -   Batch Number = 73
01/07/2022 12:55:46 - INFO - __main__ -   Batch Number = 74
01/07/2022 12:55:46 - INFO - __main__ -   Batch Number = 75
01/07/2022 12:55:46 - INFO - __main__ -   Batch Number = 76
Evaluating:  46%|     | 76/165 [00:02<00:02, 36.95it/s]01/07/2022 12:55:46 - INFO - __main__ -   Batch Number = 77
01/07/2022 12:55:46 - INFO - __main__ -   Batch Number = 78
01/07/2022 12:55:46 - INFO - __main__ -   Batch Number = 79
01/07/2022 12:55:46 - INFO - __main__ -   Batch Number = 80
Evaluating:  48%|     | 80/165 [00:02<00:02, 36.84it/s]01/07/2022 12:55:46 - INFO - __main__ -   Batch Number = 81
01/07/2022 12:55:46 - INFO - __main__ -   Batch Number = 82
01/07/2022 12:55:46 - INFO - __main__ -   Batch Number = 83
01/07/2022 12:55:46 - INFO - __main__ -   Batch Number = 84
Evaluating:  51%|     | 84/165 [00:02<00:02, 37.28it/s]01/07/2022 12:55:46 - INFO - __main__ -   Batch Number = 85
01/07/2022 12:55:46 - INFO - __main__ -   Batch Number = 86
01/07/2022 12:55:46 - INFO - __main__ -   Batch Number = 87
01/07/2022 12:55:46 - INFO - __main__ -   Batch Number = 88
Evaluating:  53%|    | 88/165 [00:02<00:02, 37.35it/s]01/07/2022 12:55:46 - INFO - __main__ -   Batch Number = 89
01/07/2022 12:55:46 - INFO - __main__ -   Batch Number = 90
01/07/2022 12:55:46 - INFO - __main__ -   Batch Number = 91
01/07/2022 12:55:46 - INFO - __main__ -   Batch Number = 92
Evaluating:  56%|    | 92/165 [00:02<00:01, 37.53it/s]01/07/2022 12:55:46 - INFO - __main__ -   Batch Number = 93
01/07/2022 12:55:47 - INFO - __main__ -   Batch Number = 94
01/07/2022 12:55:47 - INFO - __main__ -   Batch Number = 95
01/07/2022 12:55:47 - INFO - __main__ -   Batch Number = 96
Evaluating:  58%|    | 96/165 [00:02<00:01, 37.55it/s]01/07/2022 12:55:47 - INFO - __main__ -   Batch Number = 97
01/07/2022 12:55:47 - INFO - __main__ -   Batch Number = 98
01/07/2022 12:55:47 - INFO - __main__ -   Batch Number = 99
01/07/2022 12:55:47 - INFO - __main__ -   Batch Number = 100
Evaluating:  61%|    | 100/165 [00:02<00:01, 37.54it/s]01/07/2022 12:55:47 - INFO - __main__ -   Batch Number = 101
01/07/2022 12:55:47 - INFO - __main__ -   Batch Number = 102
01/07/2022 12:55:47 - INFO - __main__ -   Batch Number = 103
01/07/2022 12:55:47 - INFO - __main__ -   Batch Number = 104
Evaluating:  63%|   | 104/165 [00:02<00:01, 37.98it/s]01/07/2022 12:55:47 - INFO - __main__ -   Batch Number = 105
01/07/2022 12:55:47 - INFO - __main__ -   Batch Number = 106
01/07/2022 12:55:47 - INFO - __main__ -   Batch Number = 107
01/07/2022 12:55:47 - INFO - __main__ -   Batch Number = 108
Evaluating:  65%|   | 108/165 [00:02<00:01, 36.43it/s]01/07/2022 12:55:47 - INFO - __main__ -   Batch Number = 109
01/07/2022 12:55:47 - INFO - __main__ -   Batch Number = 110
01/07/2022 12:55:47 - INFO - __main__ -   Batch Number = 111
01/07/2022 12:55:47 - INFO - __main__ -   Batch Number = 112
Evaluating:  68%|   | 112/165 [00:03<00:01, 36.67it/s]01/07/2022 12:55:47 - INFO - __main__ -   Batch Number = 113
01/07/2022 12:55:47 - INFO - __main__ -   Batch Number = 114
01/07/2022 12:55:47 - INFO - __main__ -   Batch Number = 115
01/07/2022 12:55:47 - INFO - __main__ -   Batch Number = 116
Evaluating:  70%|   | 116/165 [00:03<00:01, 37.21it/s]01/07/2022 12:55:47 - INFO - __main__ -   Batch Number = 117
01/07/2022 12:55:47 - INFO - __main__ -   Batch Number = 118
01/07/2022 12:55:47 - INFO - __main__ -   Batch Number = 119
01/07/2022 12:55:47 - INFO - __main__ -   Batch Number = 120
Evaluating:  73%|  | 120/165 [00:03<00:01, 37.02it/s]01/07/2022 12:55:47 - INFO - __main__ -   Batch Number = 121
01/07/2022 12:55:47 - INFO - __main__ -   Batch Number = 122
01/07/2022 12:55:47 - INFO - __main__ -   Batch Number = 123
01/07/2022 12:55:47 - INFO - __main__ -   Batch Number = 124
Evaluating:  75%|  | 124/165 [00:03<00:01, 37.47it/s]01/07/2022 12:55:47 - INFO - __main__ -   Batch Number = 125
01/07/2022 12:55:47 - INFO - __main__ -   Batch Number = 126
01/07/2022 12:55:47 - INFO - __main__ -   Batch Number = 127
01/07/2022 12:55:47 - INFO - __main__ -   Batch Number = 128
Evaluating:  78%|  | 128/165 [00:03<00:00, 37.13it/s]01/07/2022 12:55:47 - INFO - __main__ -   Batch Number = 129
01/07/2022 12:55:47 - INFO - __main__ -   Batch Number = 130
01/07/2022 12:55:48 - INFO - __main__ -   Batch Number = 131
01/07/2022 12:55:48 - INFO - __main__ -   Batch Number = 132
Evaluating:  80%|  | 132/165 [00:03<00:00, 37.49it/s]01/07/2022 12:55:48 - INFO - __main__ -   Batch Number = 133
01/07/2022 12:55:48 - INFO - __main__ -   Batch Number = 134
01/07/2022 12:55:48 - INFO - __main__ -   Batch Number = 135
01/07/2022 12:55:48 - INFO - __main__ -   Batch Number = 136
Evaluating:  82%| | 136/165 [00:03<00:00, 37.24it/s]01/07/2022 12:55:48 - INFO - __main__ -   Batch Number = 137
01/07/2022 12:55:48 - INFO - __main__ -   Batch Number = 138
01/07/2022 12:55:48 - INFO - __main__ -   Batch Number = 139
01/07/2022 12:55:48 - INFO - __main__ -   Batch Number = 140
01/07/2022 12:55:48 - INFO - __main__ -   Batch Number = 141
Evaluating:  85%| | 141/165 [00:03<00:00, 38.70it/s]01/07/2022 12:55:48 - INFO - __main__ -   Batch Number = 142
01/07/2022 12:55:48 - INFO - __main__ -   Batch Number = 143
01/07/2022 12:55:48 - INFO - __main__ -   Batch Number = 144
01/07/2022 12:55:48 - INFO - __main__ -   Batch Number = 145
Evaluating:  88%| | 145/165 [00:03<00:00, 37.67it/s]01/07/2022 12:55:48 - INFO - __main__ -   Batch Number = 146
01/07/2022 12:55:48 - INFO - __main__ -   Batch Number = 147
01/07/2022 12:55:48 - INFO - __main__ -   Batch Number = 148
01/07/2022 12:55:48 - INFO - __main__ -   Batch Number = 149
Evaluating:  90%| | 149/165 [00:04<00:00, 36.72it/s]01/07/2022 12:55:48 - INFO - __main__ -   Batch Number = 150
01/07/2022 12:55:48 - INFO - __main__ -   Batch Number = 151
01/07/2022 12:55:48 - INFO - __main__ -   Batch Number = 152
01/07/2022 12:55:48 - INFO - __main__ -   Batch Number = 153
01/07/2022 12:55:48 - INFO - __main__ -   Batch Number = 154
Evaluating:  93%|| 154/165 [00:04<00:00, 39.06it/s]01/07/2022 12:55:48 - INFO - __main__ -   Batch Number = 155
01/07/2022 12:55:48 - INFO - __main__ -   Batch Number = 156
01/07/2022 12:55:48 - INFO - __main__ -   Batch Number = 157
01/07/2022 12:55:48 - INFO - __main__ -   Batch Number = 158
01/07/2022 12:55:48 - INFO - __main__ -   Batch Number = 159
Evaluating:  96%|| 159/165 [00:04<00:00, 40.71it/s]01/07/2022 12:55:48 - INFO - __main__ -   Batch Number = 160
01/07/2022 12:55:48 - INFO - __main__ -   Batch Number = 161
01/07/2022 12:55:48 - INFO - __main__ -   Batch Number = 162
01/07/2022 12:55:48 - INFO - __main__ -   Batch Number = 163
01/07/2022 12:55:48 - INFO - __main__ -   Batch Number = 164
Evaluating:  99%|| 164/165 [00:04<00:00, 41.99it/s]01/07/2022 12:55:48 - INFO - __main__ -   Batch Number = 165
Evaluating: 100%|| 165/165 [00:04<00:00, 37.67it/s]
01/07/2022 12:55:48 - INFO - __main__ -     Evaluation done in total 4.380430 secs (0.003334 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/predictions_th_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/nbest_predictions_th_.json
01/07/2022 12:55:53 - INFO - __main__ -   Results = OrderedDict([('exact', 53.36134453781513), ('f1', 65.49749596808411), ('total', 1190), ('HasAns_exact', 53.36134453781513), ('HasAns_f1', 65.49749596808411), ('HasAns_total', 1190), ('best_exact', 53.36134453781513), ('best_exact_thresh', 0.0), ('best_f1', 65.49749596808411), ('best_f1_thresh', 0.0)])
01/07/2022 12:55:53 - INFO - __main__ -   Language adapter for tr not found, using en instead
01/07/2022 12:55:53 - INFO - __main__ -   Set active language adapter to en
01/07/2022 12:55:53 - INFO - __main__ -   Args Adapter Weight = None
01/07/2022 12:55:53 - INFO - __main__ -   Adapter Languages = ['en']
01/07/2022 12:55:53 - INFO - __main__ -   Predict File = xquad.tr.json
01/07/2022 12:55:53 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
  0%|          | 0/48 [00:00<?, ?it/s] 33%|      | 16/48 [00:00<00:00, 135.29it/s] 62%|   | 30/48 [00:00<00:00, 127.96it/s] 96%|| 46/48 [00:00<00:00, 139.46it/s]100%|| 48/48 [00:00<00:00, 138.19it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<06:11,  3.20it/s]convert squad examples to features:  32%|      | 385/1190 [00:00<00:01, 621.33it/s]convert squad examples to features: 100%|| 1190/1190 [00:00<00:00, 1543.18it/s]/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|| 1190/1190 [00:00<00:00, 700325.77it/s]
01/07/2022 12:55:55 - INFO - __main__ -   Local Rank = -1
01/07/2022 12:55:55 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.tr.json_xlm-roberta-base_384_tr
01/07/2022 12:55:56 - INFO - __main__ -   ***** Running evaluation  *****
01/07/2022 12:55:56 - INFO - __main__ -     Num examples = 1274
01/07/2022 12:55:56 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/160 [00:00<?, ?it/s]01/07/2022 12:55:56 - INFO - __main__ -   Batch Number = 1
01/07/2022 12:55:56 - INFO - __main__ -   Batch Number = 2
01/07/2022 12:55:56 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|         | 3/160 [00:00<00:05, 29.47it/s]01/07/2022 12:55:56 - INFO - __main__ -   Batch Number = 4
01/07/2022 12:55:56 - INFO - __main__ -   Batch Number = 5
01/07/2022 12:55:56 - INFO - __main__ -   Batch Number = 6
Evaluating:   4%|         | 6/160 [00:00<00:05, 28.23it/s]01/07/2022 12:55:56 - INFO - __main__ -   Batch Number = 7
01/07/2022 12:55:56 - INFO - __main__ -   Batch Number = 8
01/07/2022 12:55:56 - INFO - __main__ -   Batch Number = 9
01/07/2022 12:55:56 - INFO - __main__ -   Batch Number = 10
01/07/2022 12:55:57 - INFO - __main__ -   Batch Number = 11
Evaluating:   7%|         | 11/160 [00:00<00:04, 35.97it/s]01/07/2022 12:55:57 - INFO - __main__ -   Batch Number = 12
01/07/2022 12:55:57 - INFO - __main__ -   Batch Number = 13
01/07/2022 12:55:57 - INFO - __main__ -   Batch Number = 14
01/07/2022 12:55:57 - INFO - __main__ -   Batch Number = 15
01/07/2022 12:55:57 - INFO - __main__ -   Batch Number = 16
Evaluating:  10%|         | 16/160 [00:00<00:03, 39.46it/s]01/07/2022 12:55:57 - INFO - __main__ -   Batch Number = 17
01/07/2022 12:55:57 - INFO - __main__ -   Batch Number = 18
01/07/2022 12:55:57 - INFO - __main__ -   Batch Number = 19
01/07/2022 12:55:57 - INFO - __main__ -   Batch Number = 20
01/07/2022 12:55:57 - INFO - __main__ -   Batch Number = 21
Evaluating:  13%|        | 21/160 [00:00<00:03, 41.07it/s]01/07/2022 12:55:57 - INFO - __main__ -   Batch Number = 22
01/07/2022 12:55:57 - INFO - __main__ -   Batch Number = 23
01/07/2022 12:55:57 - INFO - __main__ -   Batch Number = 24
01/07/2022 12:55:57 - INFO - __main__ -   Batch Number = 25
01/07/2022 12:55:57 - INFO - __main__ -   Batch Number = 26
Evaluating:  16%|        | 26/160 [00:00<00:03, 42.24it/s]01/07/2022 12:55:57 - INFO - __main__ -   Batch Number = 27
01/07/2022 12:55:57 - INFO - __main__ -   Batch Number = 28
01/07/2022 12:55:57 - INFO - __main__ -   Batch Number = 29
01/07/2022 12:55:57 - INFO - __main__ -   Batch Number = 30
01/07/2022 12:55:57 - INFO - __main__ -   Batch Number = 31
Evaluating:  19%|        | 31/160 [00:00<00:02, 43.03it/s]01/07/2022 12:55:57 - INFO - __main__ -   Batch Number = 32
01/07/2022 12:55:57 - INFO - __main__ -   Batch Number = 33
01/07/2022 12:55:57 - INFO - __main__ -   Batch Number = 34
01/07/2022 12:55:57 - INFO - __main__ -   Batch Number = 35
01/07/2022 12:55:57 - INFO - __main__ -   Batch Number = 36
Evaluating:  22%|       | 36/160 [00:00<00:02, 43.54it/s]01/07/2022 12:55:57 - INFO - __main__ -   Batch Number = 37
01/07/2022 12:55:57 - INFO - __main__ -   Batch Number = 38
01/07/2022 12:55:57 - INFO - __main__ -   Batch Number = 39
01/07/2022 12:55:57 - INFO - __main__ -   Batch Number = 40
01/07/2022 12:55:57 - INFO - __main__ -   Batch Number = 41
Evaluating:  26%|       | 41/160 [00:00<00:02, 43.86it/s]01/07/2022 12:55:57 - INFO - __main__ -   Batch Number = 42
01/07/2022 12:55:57 - INFO - __main__ -   Batch Number = 43
01/07/2022 12:55:57 - INFO - __main__ -   Batch Number = 44
01/07/2022 12:55:57 - INFO - __main__ -   Batch Number = 45
01/07/2022 12:55:57 - INFO - __main__ -   Batch Number = 46
Evaluating:  29%|       | 46/160 [00:01<00:02, 44.13it/s]01/07/2022 12:55:57 - INFO - __main__ -   Batch Number = 47
01/07/2022 12:55:57 - INFO - __main__ -   Batch Number = 48
01/07/2022 12:55:57 - INFO - __main__ -   Batch Number = 49
01/07/2022 12:55:57 - INFO - __main__ -   Batch Number = 50
01/07/2022 12:55:57 - INFO - __main__ -   Batch Number = 51
Evaluating:  32%|      | 51/160 [00:01<00:02, 43.86it/s]01/07/2022 12:55:57 - INFO - __main__ -   Batch Number = 52
01/07/2022 12:55:57 - INFO - __main__ -   Batch Number = 53
01/07/2022 12:55:57 - INFO - __main__ -   Batch Number = 54
01/07/2022 12:55:58 - INFO - __main__ -   Batch Number = 55
01/07/2022 12:55:58 - INFO - __main__ -   Batch Number = 56
Evaluating:  35%|      | 56/160 [00:01<00:02, 43.74it/s]01/07/2022 12:55:58 - INFO - __main__ -   Batch Number = 57
01/07/2022 12:55:58 - INFO - __main__ -   Batch Number = 58
01/07/2022 12:55:58 - INFO - __main__ -   Batch Number = 59
01/07/2022 12:55:58 - INFO - __main__ -   Batch Number = 60
01/07/2022 12:55:58 - INFO - __main__ -   Batch Number = 61
Evaluating:  38%|      | 61/160 [00:01<00:02, 38.35it/s]01/07/2022 12:55:58 - INFO - __main__ -   Batch Number = 62
01/07/2022 12:55:58 - INFO - __main__ -   Batch Number = 63
01/07/2022 12:55:58 - INFO - __main__ -   Batch Number = 64
01/07/2022 12:55:58 - INFO - __main__ -   Batch Number = 65
01/07/2022 12:55:58 - INFO - __main__ -   Batch Number = 66
Evaluating:  41%|     | 66/160 [00:01<00:02, 40.07it/s]01/07/2022 12:55:58 - INFO - __main__ -   Batch Number = 67
01/07/2022 12:55:58 - INFO - __main__ -   Batch Number = 68
01/07/2022 12:55:58 - INFO - __main__ -   Batch Number = 69
01/07/2022 12:55:58 - INFO - __main__ -   Batch Number = 70
01/07/2022 12:55:58 - INFO - __main__ -   Batch Number = 71
Evaluating:  44%|     | 71/160 [00:01<00:02, 41.35it/s]01/07/2022 12:55:58 - INFO - __main__ -   Batch Number = 72
01/07/2022 12:55:58 - INFO - __main__ -   Batch Number = 73
01/07/2022 12:55:58 - INFO - __main__ -   Batch Number = 74
01/07/2022 12:55:58 - INFO - __main__ -   Batch Number = 75
01/07/2022 12:55:58 - INFO - __main__ -   Batch Number = 76
Evaluating:  48%|     | 76/160 [00:01<00:01, 42.31it/s]01/07/2022 12:55:58 - INFO - __main__ -   Batch Number = 77
01/07/2022 12:55:58 - INFO - __main__ -   Batch Number = 78
01/07/2022 12:55:58 - INFO - __main__ -   Batch Number = 79
01/07/2022 12:55:58 - INFO - __main__ -   Batch Number = 80
01/07/2022 12:55:58 - INFO - __main__ -   Batch Number = 81
Evaluating:  51%|     | 81/160 [00:01<00:01, 42.85it/s]01/07/2022 12:55:58 - INFO - __main__ -   Batch Number = 82
01/07/2022 12:55:58 - INFO - __main__ -   Batch Number = 83
01/07/2022 12:55:58 - INFO - __main__ -   Batch Number = 84
01/07/2022 12:55:58 - INFO - __main__ -   Batch Number = 85
01/07/2022 12:55:58 - INFO - __main__ -   Batch Number = 86
Evaluating:  54%|    | 86/160 [00:02<00:01, 43.54it/s]01/07/2022 12:55:58 - INFO - __main__ -   Batch Number = 87
01/07/2022 12:55:58 - INFO - __main__ -   Batch Number = 88
01/07/2022 12:55:58 - INFO - __main__ -   Batch Number = 89
01/07/2022 12:55:58 - INFO - __main__ -   Batch Number = 90
01/07/2022 12:55:58 - INFO - __main__ -   Batch Number = 91
Evaluating:  57%|    | 91/160 [00:02<00:01, 43.86it/s]01/07/2022 12:55:58 - INFO - __main__ -   Batch Number = 92
01/07/2022 12:55:58 - INFO - __main__ -   Batch Number = 93
01/07/2022 12:55:58 - INFO - __main__ -   Batch Number = 94
01/07/2022 12:55:58 - INFO - __main__ -   Batch Number = 95
01/07/2022 12:55:58 - INFO - __main__ -   Batch Number = 96
Evaluating:  60%|    | 96/160 [00:02<00:01, 42.53it/s]01/07/2022 12:55:59 - INFO - __main__ -   Batch Number = 97
01/07/2022 12:55:59 - INFO - __main__ -   Batch Number = 98
01/07/2022 12:55:59 - INFO - __main__ -   Batch Number = 99
01/07/2022 12:55:59 - INFO - __main__ -   Batch Number = 100
01/07/2022 12:55:59 - INFO - __main__ -   Batch Number = 101
Evaluating:  63%|   | 101/160 [00:02<00:01, 43.16it/s]01/07/2022 12:55:59 - INFO - __main__ -   Batch Number = 102
01/07/2022 12:55:59 - INFO - __main__ -   Batch Number = 103
01/07/2022 12:55:59 - INFO - __main__ -   Batch Number = 104
01/07/2022 12:55:59 - INFO - __main__ -   Batch Number = 105
01/07/2022 12:55:59 - INFO - __main__ -   Batch Number = 106
Evaluating:  66%|   | 106/160 [00:02<00:01, 43.59it/s]01/07/2022 12:55:59 - INFO - __main__ -   Batch Number = 107
01/07/2022 12:55:59 - INFO - __main__ -   Batch Number = 108
01/07/2022 12:55:59 - INFO - __main__ -   Batch Number = 109
01/07/2022 12:55:59 - INFO - __main__ -   Batch Number = 110
01/07/2022 12:55:59 - INFO - __main__ -   Batch Number = 111
Evaluating:  69%|   | 111/160 [00:02<00:01, 43.83it/s]01/07/2022 12:55:59 - INFO - __main__ -   Batch Number = 112
01/07/2022 12:55:59 - INFO - __main__ -   Batch Number = 113
01/07/2022 12:55:59 - INFO - __main__ -   Batch Number = 114
01/07/2022 12:55:59 - INFO - __main__ -   Batch Number = 115
01/07/2022 12:55:59 - INFO - __main__ -   Batch Number = 116
Evaluating:  72%|  | 116/160 [00:02<00:00, 44.05it/s]01/07/2022 12:55:59 - INFO - __main__ -   Batch Number = 117
01/07/2022 12:55:59 - INFO - __main__ -   Batch Number = 118
01/07/2022 12:55:59 - INFO - __main__ -   Batch Number = 119
01/07/2022 12:55:59 - INFO - __main__ -   Batch Number = 120
01/07/2022 12:55:59 - INFO - __main__ -   Batch Number = 121
Evaluating:  76%|  | 121/160 [00:02<00:00, 44.32it/s]01/07/2022 12:55:59 - INFO - __main__ -   Batch Number = 122
01/07/2022 12:55:59 - INFO - __main__ -   Batch Number = 123
01/07/2022 12:55:59 - INFO - __main__ -   Batch Number = 124
01/07/2022 12:55:59 - INFO - __main__ -   Batch Number = 125
01/07/2022 12:55:59 - INFO - __main__ -   Batch Number = 126
Evaluating:  79%|  | 126/160 [00:02<00:00, 44.35it/s]01/07/2022 12:55:59 - INFO - __main__ -   Batch Number = 127
01/07/2022 12:55:59 - INFO - __main__ -   Batch Number = 128
01/07/2022 12:55:59 - INFO - __main__ -   Batch Number = 129
01/07/2022 12:55:59 - INFO - __main__ -   Batch Number = 130
01/07/2022 12:55:59 - INFO - __main__ -   Batch Number = 131
Evaluating:  82%| | 131/160 [00:03<00:00, 44.29it/s]01/07/2022 12:55:59 - INFO - __main__ -   Batch Number = 132
01/07/2022 12:55:59 - INFO - __main__ -   Batch Number = 133
01/07/2022 12:55:59 - INFO - __main__ -   Batch Number = 134
01/07/2022 12:55:59 - INFO - __main__ -   Batch Number = 135
01/07/2022 12:55:59 - INFO - __main__ -   Batch Number = 136
Evaluating:  85%| | 136/160 [00:03<00:00, 44.11it/s]01/07/2022 12:55:59 - INFO - __main__ -   Batch Number = 137
01/07/2022 12:55:59 - INFO - __main__ -   Batch Number = 138
01/07/2022 12:55:59 - INFO - __main__ -   Batch Number = 139
01/07/2022 12:55:59 - INFO - __main__ -   Batch Number = 140
01/07/2022 12:56:00 - INFO - __main__ -   Batch Number = 141
Evaluating:  88%| | 141/160 [00:03<00:00, 43.71it/s]01/07/2022 12:56:00 - INFO - __main__ -   Batch Number = 142
01/07/2022 12:56:00 - INFO - __main__ -   Batch Number = 143
01/07/2022 12:56:00 - INFO - __main__ -   Batch Number = 144
01/07/2022 12:56:00 - INFO - __main__ -   Batch Number = 145
01/07/2022 12:56:00 - INFO - __main__ -   Batch Number = 146
Evaluating:  91%|| 146/160 [00:03<00:00, 43.04it/s]01/07/2022 12:56:00 - INFO - __main__ -   Batch Number = 147
01/07/2022 12:56:00 - INFO - __main__ -   Batch Number = 148
01/07/2022 12:56:00 - INFO - __main__ -   Batch Number = 149
01/07/2022 12:56:00 - INFO - __main__ -   Batch Number = 150
01/07/2022 12:56:00 - INFO - __main__ -   Batch Number = 151
Evaluating:  94%|| 151/160 [00:03<00:00, 41.81it/s]01/07/2022 12:56:00 - INFO - __main__ -   Batch Number = 152
01/07/2022 12:56:00 - INFO - __main__ -   Batch Number = 153
01/07/2022 12:56:00 - INFO - __main__ -   Batch Number = 154
01/07/2022 12:56:00 - INFO - __main__ -   Batch Number = 155
01/07/2022 12:56:00 - INFO - __main__ -   Batch Number = 156
Evaluating:  98%|| 156/160 [00:03<00:00, 40.98it/s]01/07/2022 12:56:00 - INFO - __main__ -   Batch Number = 157
01/07/2022 12:56:00 - INFO - __main__ -   Batch Number = 158
01/07/2022 12:56:00 - INFO - __main__ -   Batch Number = 159
01/07/2022 12:56:00 - INFO - __main__ -   Batch Number = 160
Evaluating: 100%|| 160/160 [00:03<00:00, 42.11it/s]
01/07/2022 12:56:00 - INFO - __main__ -     Evaluation done in total 3.799692 secs (0.002982 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/predictions_tr_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/nbest_predictions_tr_.json
01/07/2022 12:56:03 - INFO - __main__ -   Results = OrderedDict([('exact', 50.67226890756302), ('f1', 66.67699846057094), ('total', 1190), ('HasAns_exact', 50.67226890756302), ('HasAns_f1', 66.67699846057094), ('HasAns_total', 1190), ('best_exact', 50.67226890756302), ('best_exact_thresh', 0.0), ('best_f1', 66.67699846057094), ('best_f1_thresh', 0.0)])
01/07/2022 12:56:03 - INFO - __main__ -   Language adapter for vi not found, using en instead
01/07/2022 12:56:03 - INFO - __main__ -   Set active language adapter to en
01/07/2022 12:56:03 - INFO - __main__ -   Args Adapter Weight = None
01/07/2022 12:56:03 - INFO - __main__ -   Adapter Languages = ['en']
01/07/2022 12:56:03 - INFO - __main__ -   Predict File = xquad.vi.json
01/07/2022 12:56:03 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
  0%|          | 0/48 [00:00<?, ?it/s] 33%|      | 16/48 [00:00<00:00, 136.95it/s] 62%|   | 30/48 [00:00<00:00, 126.07it/s] 90%| | 43/48 [00:00<00:00, 109.97it/s]100%|| 48/48 [00:00<00:00, 118.63it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<04:22,  4.52it/s]convert squad examples to features:  32%|      | 385/1190 [00:00<00:01, 593.44it/s]convert squad examples to features:  92%|| 1089/1190 [00:00<00:00, 1679.51it/s]convert squad examples to features: 100%|| 1190/1190 [00:00<00:00, 1335.56it/s]/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|| 1190/1190 [00:00<00:00, 728220.27it/s]
01/07/2022 12:56:05 - INFO - __main__ -   Local Rank = -1
01/07/2022 12:56:05 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.vi.json_xlm-roberta-base_384_vi
01/07/2022 12:56:07 - INFO - __main__ -   ***** Running evaluation  *****
01/07/2022 12:56:07 - INFO - __main__ -     Num examples = 1314
01/07/2022 12:56:07 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/165 [00:00<?, ?it/s]01/07/2022 12:56:07 - INFO - __main__ -   Batch Number = 1
01/07/2022 12:56:07 - INFO - __main__ -   Batch Number = 2
Evaluating:   1%|          | 2/165 [00:00<00:08, 18.83it/s]01/07/2022 12:56:07 - INFO - __main__ -   Batch Number = 3
01/07/2022 12:56:07 - INFO - __main__ -   Batch Number = 4
Evaluating:   2%|         | 4/165 [00:00<00:08, 18.92it/s]01/07/2022 12:56:07 - INFO - __main__ -   Batch Number = 5
01/07/2022 12:56:07 - INFO - __main__ -   Batch Number = 6
Evaluating:   4%|         | 6/165 [00:00<00:08, 18.97it/s]01/07/2022 12:56:07 - INFO - __main__ -   Batch Number = 7
01/07/2022 12:56:07 - INFO - __main__ -   Batch Number = 8
01/07/2022 12:56:07 - INFO - __main__ -   Batch Number = 9
Evaluating:   5%|         | 9/165 [00:00<00:07, 20.83it/s]01/07/2022 12:56:07 - INFO - __main__ -   Batch Number = 10
01/07/2022 12:56:07 - INFO - __main__ -   Batch Number = 11
01/07/2022 12:56:07 - INFO - __main__ -   Batch Number = 12
Evaluating:   7%|         | 12/165 [00:00<00:07, 20.07it/s]01/07/2022 12:56:07 - INFO - __main__ -   Batch Number = 13
01/07/2022 12:56:07 - INFO - __main__ -   Batch Number = 14
01/07/2022 12:56:07 - INFO - __main__ -   Batch Number = 15
Evaluating:   9%|         | 15/165 [00:00<00:07, 19.43it/s]01/07/2022 12:56:07 - INFO - __main__ -   Batch Number = 16
01/07/2022 12:56:07 - INFO - __main__ -   Batch Number = 17
Evaluating:  10%|         | 17/165 [00:00<00:07, 19.43it/s]01/07/2022 12:56:07 - INFO - __main__ -   Batch Number = 18
01/07/2022 12:56:07 - INFO - __main__ -   Batch Number = 19
Evaluating:  12%|        | 19/165 [00:00<00:07, 19.40it/s]01/07/2022 12:56:08 - INFO - __main__ -   Batch Number = 20
01/07/2022 12:56:08 - INFO - __main__ -   Batch Number = 21
Evaluating:  13%|        | 21/165 [00:01<00:07, 19.34it/s]01/07/2022 12:56:08 - INFO - __main__ -   Batch Number = 22
01/07/2022 12:56:08 - INFO - __main__ -   Batch Number = 23
Evaluating:  14%|        | 23/165 [00:01<00:07, 19.34it/s]01/07/2022 12:56:08 - INFO - __main__ -   Batch Number = 24
01/07/2022 12:56:08 - INFO - __main__ -   Batch Number = 25
01/07/2022 12:56:08 - INFO - __main__ -   Batch Number = 26
01/07/2022 12:56:08 - INFO - __main__ -   Batch Number = 27
Evaluating:  16%|        | 27/165 [00:01<00:05, 24.01it/s]01/07/2022 12:56:08 - INFO - __main__ -   Batch Number = 28
01/07/2022 12:56:08 - INFO - __main__ -   Batch Number = 29
01/07/2022 12:56:08 - INFO - __main__ -   Batch Number = 30
01/07/2022 12:56:08 - INFO - __main__ -   Batch Number = 31
01/07/2022 12:56:08 - INFO - __main__ -   Batch Number = 32
Evaluating:  19%|        | 32/165 [00:01<00:04, 30.06it/s]01/07/2022 12:56:08 - INFO - __main__ -   Batch Number = 33
01/07/2022 12:56:08 - INFO - __main__ -   Batch Number = 34
01/07/2022 12:56:08 - INFO - __main__ -   Batch Number = 35
01/07/2022 12:56:08 - INFO - __main__ -   Batch Number = 36
01/07/2022 12:56:08 - INFO - __main__ -   Batch Number = 37
Evaluating:  22%|       | 37/165 [00:01<00:03, 34.46it/s]01/07/2022 12:56:08 - INFO - __main__ -   Batch Number = 38
01/07/2022 12:56:08 - INFO - __main__ -   Batch Number = 39
01/07/2022 12:56:08 - INFO - __main__ -   Batch Number = 40
01/07/2022 12:56:08 - INFO - __main__ -   Batch Number = 41
01/07/2022 12:56:08 - INFO - __main__ -   Batch Number = 42
Evaluating:  25%|       | 42/165 [00:01<00:03, 37.34it/s]01/07/2022 12:56:08 - INFO - __main__ -   Batch Number = 43
01/07/2022 12:56:08 - INFO - __main__ -   Batch Number = 44
01/07/2022 12:56:08 - INFO - __main__ -   Batch Number = 45
01/07/2022 12:56:08 - INFO - __main__ -   Batch Number = 46
01/07/2022 12:56:08 - INFO - __main__ -   Batch Number = 47
Evaluating:  28%|       | 47/165 [00:01<00:03, 38.00it/s]01/07/2022 12:56:08 - INFO - __main__ -   Batch Number = 48
01/07/2022 12:56:08 - INFO - __main__ -   Batch Number = 49
01/07/2022 12:56:08 - INFO - __main__ -   Batch Number = 50
01/07/2022 12:56:08 - INFO - __main__ -   Batch Number = 51
01/07/2022 12:56:08 - INFO - __main__ -   Batch Number = 52
Evaluating:  32%|      | 52/165 [00:01<00:02, 39.93it/s]01/07/2022 12:56:08 - INFO - __main__ -   Batch Number = 53
01/07/2022 12:56:08 - INFO - __main__ -   Batch Number = 54
01/07/2022 12:56:08 - INFO - __main__ -   Batch Number = 55
01/07/2022 12:56:08 - INFO - __main__ -   Batch Number = 56
01/07/2022 12:56:09 - INFO - __main__ -   Batch Number = 57
Evaluating:  35%|      | 57/165 [00:01<00:02, 41.34it/s]01/07/2022 12:56:09 - INFO - __main__ -   Batch Number = 58
01/07/2022 12:56:09 - INFO - __main__ -   Batch Number = 59
01/07/2022 12:56:09 - INFO - __main__ -   Batch Number = 60
01/07/2022 12:56:09 - INFO - __main__ -   Batch Number = 61
01/07/2022 12:56:09 - INFO - __main__ -   Batch Number = 62
Evaluating:  38%|      | 62/165 [00:02<00:02, 42.08it/s]01/07/2022 12:56:09 - INFO - __main__ -   Batch Number = 63
01/07/2022 12:56:09 - INFO - __main__ -   Batch Number = 64
01/07/2022 12:56:09 - INFO - __main__ -   Batch Number = 65
01/07/2022 12:56:09 - INFO - __main__ -   Batch Number = 66
01/07/2022 12:56:09 - INFO - __main__ -   Batch Number = 67
Evaluating:  41%|      | 67/165 [00:02<00:02, 42.89it/s]01/07/2022 12:56:09 - INFO - __main__ -   Batch Number = 68
01/07/2022 12:56:09 - INFO - __main__ -   Batch Number = 69
01/07/2022 12:56:09 - INFO - __main__ -   Batch Number = 70
01/07/2022 12:56:09 - INFO - __main__ -   Batch Number = 71
01/07/2022 12:56:09 - INFO - __main__ -   Batch Number = 72
Evaluating:  44%|     | 72/165 [00:02<00:02, 43.45it/s]01/07/2022 12:56:09 - INFO - __main__ -   Batch Number = 73
01/07/2022 12:56:09 - INFO - __main__ -   Batch Number = 74
01/07/2022 12:56:09 - INFO - __main__ -   Batch Number = 75
01/07/2022 12:56:09 - INFO - __main__ -   Batch Number = 76
01/07/2022 12:56:09 - INFO - __main__ -   Batch Number = 77
Evaluating:  47%|     | 77/165 [00:02<00:02, 43.74it/s]01/07/2022 12:56:09 - INFO - __main__ -   Batch Number = 78
01/07/2022 12:56:09 - INFO - __main__ -   Batch Number = 79
01/07/2022 12:56:09 - INFO - __main__ -   Batch Number = 80
01/07/2022 12:56:09 - INFO - __main__ -   Batch Number = 81
01/07/2022 12:56:09 - INFO - __main__ -   Batch Number = 82
Evaluating:  50%|     | 82/165 [00:02<00:01, 43.90it/s]01/07/2022 12:56:09 - INFO - __main__ -   Batch Number = 83
01/07/2022 12:56:09 - INFO - __main__ -   Batch Number = 84
01/07/2022 12:56:09 - INFO - __main__ -   Batch Number = 85
01/07/2022 12:56:09 - INFO - __main__ -   Batch Number = 86
01/07/2022 12:56:09 - INFO - __main__ -   Batch Number = 87
Evaluating:  53%|    | 87/165 [00:02<00:01, 43.86it/s]01/07/2022 12:56:09 - INFO - __main__ -   Batch Number = 88
01/07/2022 12:56:09 - INFO - __main__ -   Batch Number = 89
01/07/2022 12:56:09 - INFO - __main__ -   Batch Number = 90
01/07/2022 12:56:09 - INFO - __main__ -   Batch Number = 91
01/07/2022 12:56:09 - INFO - __main__ -   Batch Number = 92
Evaluating:  56%|    | 92/165 [00:02<00:01, 43.91it/s]01/07/2022 12:56:09 - INFO - __main__ -   Batch Number = 93
01/07/2022 12:56:09 - INFO - __main__ -   Batch Number = 94
01/07/2022 12:56:09 - INFO - __main__ -   Batch Number = 95
01/07/2022 12:56:09 - INFO - __main__ -   Batch Number = 96
01/07/2022 12:56:09 - INFO - __main__ -   Batch Number = 97
Evaluating:  59%|    | 97/165 [00:02<00:01, 43.92it/s]01/07/2022 12:56:09 - INFO - __main__ -   Batch Number = 98
01/07/2022 12:56:09 - INFO - __main__ -   Batch Number = 99
01/07/2022 12:56:09 - INFO - __main__ -   Batch Number = 100
01/07/2022 12:56:09 - INFO - __main__ -   Batch Number = 101
01/07/2022 12:56:10 - INFO - __main__ -   Batch Number = 102
Evaluating:  62%|   | 102/165 [00:03<00:01, 44.13it/s]01/07/2022 12:56:10 - INFO - __main__ -   Batch Number = 103
01/07/2022 12:56:10 - INFO - __main__ -   Batch Number = 104
01/07/2022 12:56:10 - INFO - __main__ -   Batch Number = 105
01/07/2022 12:56:10 - INFO - __main__ -   Batch Number = 106
01/07/2022 12:56:10 - INFO - __main__ -   Batch Number = 107
Evaluating:  65%|   | 107/165 [00:03<00:01, 44.19it/s]01/07/2022 12:56:10 - INFO - __main__ -   Batch Number = 108
01/07/2022 12:56:10 - INFO - __main__ -   Batch Number = 109
01/07/2022 12:56:10 - INFO - __main__ -   Batch Number = 110
01/07/2022 12:56:10 - INFO - __main__ -   Batch Number = 111
01/07/2022 12:56:10 - INFO - __main__ -   Batch Number = 112
Evaluating:  68%|   | 112/165 [00:03<00:01, 44.31it/s]01/07/2022 12:56:10 - INFO - __main__ -   Batch Number = 113
01/07/2022 12:56:10 - INFO - __main__ -   Batch Number = 114
01/07/2022 12:56:10 - INFO - __main__ -   Batch Number = 115
01/07/2022 12:56:10 - INFO - __main__ -   Batch Number = 116
01/07/2022 12:56:10 - INFO - __main__ -   Batch Number = 117
Evaluating:  71%|   | 117/165 [00:03<00:01, 44.49it/s]01/07/2022 12:56:10 - INFO - __main__ -   Batch Number = 118
01/07/2022 12:56:10 - INFO - __main__ -   Batch Number = 119
01/07/2022 12:56:10 - INFO - __main__ -   Batch Number = 120
01/07/2022 12:56:10 - INFO - __main__ -   Batch Number = 121
01/07/2022 12:56:10 - INFO - __main__ -   Batch Number = 122
Evaluating:  74%|  | 122/165 [00:03<00:00, 44.41it/s]01/07/2022 12:56:10 - INFO - __main__ -   Batch Number = 123
01/07/2022 12:56:10 - INFO - __main__ -   Batch Number = 124
01/07/2022 12:56:10 - INFO - __main__ -   Batch Number = 125
01/07/2022 12:56:10 - INFO - __main__ -   Batch Number = 126
01/07/2022 12:56:10 - INFO - __main__ -   Batch Number = 127
Evaluating:  77%|  | 127/165 [00:03<00:00, 44.47it/s]01/07/2022 12:56:10 - INFO - __main__ -   Batch Number = 128
01/07/2022 12:56:10 - INFO - __main__ -   Batch Number = 129
01/07/2022 12:56:10 - INFO - __main__ -   Batch Number = 130
01/07/2022 12:56:10 - INFO - __main__ -   Batch Number = 131
01/07/2022 12:56:10 - INFO - __main__ -   Batch Number = 132
Evaluating:  80%|  | 132/165 [00:03<00:00, 44.31it/s]01/07/2022 12:56:10 - INFO - __main__ -   Batch Number = 133
01/07/2022 12:56:10 - INFO - __main__ -   Batch Number = 134
01/07/2022 12:56:10 - INFO - __main__ -   Batch Number = 135
01/07/2022 12:56:10 - INFO - __main__ -   Batch Number = 136
01/07/2022 12:56:10 - INFO - __main__ -   Batch Number = 137
Evaluating:  83%| | 137/165 [00:03<00:00, 44.25it/s]01/07/2022 12:56:10 - INFO - __main__ -   Batch Number = 138
01/07/2022 12:56:10 - INFO - __main__ -   Batch Number = 139
01/07/2022 12:56:10 - INFO - __main__ -   Batch Number = 140
01/07/2022 12:56:10 - INFO - __main__ -   Batch Number = 141
01/07/2022 12:56:10 - INFO - __main__ -   Batch Number = 142
Evaluating:  86%| | 142/165 [00:03<00:00, 44.33it/s]01/07/2022 12:56:10 - INFO - __main__ -   Batch Number = 143
01/07/2022 12:56:10 - INFO - __main__ -   Batch Number = 144
01/07/2022 12:56:10 - INFO - __main__ -   Batch Number = 145
01/07/2022 12:56:11 - INFO - __main__ -   Batch Number = 146
01/07/2022 12:56:11 - INFO - __main__ -   Batch Number = 147
Evaluating:  89%| | 147/165 [00:04<00:00, 44.46it/s]01/07/2022 12:56:11 - INFO - __main__ -   Batch Number = 148
01/07/2022 12:56:11 - INFO - __main__ -   Batch Number = 149
01/07/2022 12:56:11 - INFO - __main__ -   Batch Number = 150
01/07/2022 12:56:11 - INFO - __main__ -   Batch Number = 151
01/07/2022 12:56:11 - INFO - __main__ -   Batch Number = 152
Evaluating:  92%|| 152/165 [00:04<00:00, 44.31it/s]01/07/2022 12:56:11 - INFO - __main__ -   Batch Number = 153
01/07/2022 12:56:11 - INFO - __main__ -   Batch Number = 154
01/07/2022 12:56:11 - INFO - __main__ -   Batch Number = 155
01/07/2022 12:56:11 - INFO - __main__ -   Batch Number = 156
01/07/2022 12:56:11 - INFO - __main__ -   Batch Number = 157
Evaluating:  95%|| 157/165 [00:04<00:00, 44.36it/s]01/07/2022 12:56:11 - INFO - __main__ -   Batch Number = 158
01/07/2022 12:56:11 - INFO - __main__ -   Batch Number = 159
01/07/2022 12:56:11 - INFO - __main__ -   Batch Number = 160
01/07/2022 12:56:11 - INFO - __main__ -   Batch Number = 161
01/07/2022 12:56:11 - INFO - __main__ -   Batch Number = 162
Evaluating:  98%|| 162/165 [00:04<00:00, 44.50it/s]01/07/2022 12:56:11 - INFO - __main__ -   Batch Number = 163
01/07/2022 12:56:11 - INFO - __main__ -   Batch Number = 164
01/07/2022 12:56:11 - INFO - __main__ -   Batch Number = 165
Evaluating: 100%|| 165/165 [00:04<00:00, 37.36it/s]
01/07/2022 12:56:11 - INFO - __main__ -     Evaluation done in total 4.417500 secs (0.003362 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/predictions_vi_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/nbest_predictions_vi_.json
01/07/2022 12:56:14 - INFO - __main__ -   Results = OrderedDict([('exact', 52.60504201680672), ('f1', 71.68785662627492), ('total', 1190), ('HasAns_exact', 52.60504201680672), ('HasAns_f1', 71.68785662627492), ('HasAns_total', 1190), ('best_exact', 52.60504201680672), ('best_exact_thresh', 0.0), ('best_f1', 71.68785662627492), ('best_f1_thresh', 0.0)])
01/07/2022 12:56:14 - INFO - __main__ -   Language adapter for zh not found, using en instead
01/07/2022 12:56:14 - INFO - __main__ -   Set active language adapter to en
01/07/2022 12:56:14 - INFO - __main__ -   Args Adapter Weight = None
01/07/2022 12:56:14 - INFO - __main__ -   Adapter Languages = ['en']
01/07/2022 12:56:14 - INFO - __main__ -   Predict File = xquad.zh.json
01/07/2022 12:56:14 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
  0%|          | 0/48 [00:00<?, ?it/s] 35%|      | 17/48 [00:00<00:00, 169.68it/s]100%|| 48/48 [00:00<00:00, 266.65it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<03:10,  6.24it/s]convert squad examples to features:  32%|      | 385/1190 [00:00<00:00, 1126.46it/s]convert squad examples to features: 100%|| 1190/1190 [00:00<00:00, 2827.42it/s]/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|| 1190/1190 [00:00<00:00, 742741.33it/s]
01/07/2022 12:56:15 - INFO - __main__ -   Local Rank = -1
01/07/2022 12:56:15 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.zh.json_xlm-roberta-base_384_zh
01/07/2022 12:56:16 - INFO - __main__ -   ***** Running evaluation  *****
01/07/2022 12:56:16 - INFO - __main__ -     Num examples = 1246
01/07/2022 12:56:16 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/156 [00:00<?, ?it/s]01/07/2022 12:56:16 - INFO - __main__ -   Batch Number = 1
01/07/2022 12:56:16 - INFO - __main__ -   Batch Number = 2
01/07/2022 12:56:16 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|         | 3/156 [00:00<00:05, 29.26it/s]01/07/2022 12:56:17 - INFO - __main__ -   Batch Number = 4
01/07/2022 12:56:17 - INFO - __main__ -   Batch Number = 5
01/07/2022 12:56:17 - INFO - __main__ -   Batch Number = 6
Evaluating:   4%|         | 6/156 [00:00<00:05, 29.60it/s]01/07/2022 12:56:17 - INFO - __main__ -   Batch Number = 7
01/07/2022 12:56:17 - INFO - __main__ -   Batch Number = 8
01/07/2022 12:56:17 - INFO - __main__ -   Batch Number = 9
01/07/2022 12:56:17 - INFO - __main__ -   Batch Number = 10
01/07/2022 12:56:17 - INFO - __main__ -   Batch Number = 11
Evaluating:   7%|         | 11/156 [00:00<00:03, 36.53it/s]01/07/2022 12:56:17 - INFO - __main__ -   Batch Number = 12
01/07/2022 12:56:17 - INFO - __main__ -   Batch Number = 13
01/07/2022 12:56:17 - INFO - __main__ -   Batch Number = 14
01/07/2022 12:56:17 - INFO - __main__ -   Batch Number = 15
01/07/2022 12:56:17 - INFO - __main__ -   Batch Number = 16
Evaluating:  10%|         | 16/156 [00:00<00:03, 39.54it/s]01/07/2022 12:56:17 - INFO - __main__ -   Batch Number = 17
01/07/2022 12:56:17 - INFO - __main__ -   Batch Number = 18
01/07/2022 12:56:17 - INFO - __main__ -   Batch Number = 19
01/07/2022 12:56:17 - INFO - __main__ -   Batch Number = 20
Evaluating:  13%|        | 20/156 [00:00<00:03, 38.99it/s]01/07/2022 12:56:17 - INFO - __main__ -   Batch Number = 21
01/07/2022 12:56:17 - INFO - __main__ -   Batch Number = 22
01/07/2022 12:56:17 - INFO - __main__ -   Batch Number = 23
01/07/2022 12:56:17 - INFO - __main__ -   Batch Number = 24
01/07/2022 12:56:17 - INFO - __main__ -   Batch Number = 25
Evaluating:  16%|        | 25/156 [00:00<00:03, 40.82it/s]01/07/2022 12:56:17 - INFO - __main__ -   Batch Number = 26
01/07/2022 12:56:17 - INFO - __main__ -   Batch Number = 27
01/07/2022 12:56:17 - INFO - __main__ -   Batch Number = 28
01/07/2022 12:56:17 - INFO - __main__ -   Batch Number = 29
01/07/2022 12:56:17 - INFO - __main__ -   Batch Number = 30
Evaluating:  19%|        | 30/156 [00:00<00:03, 41.99it/s]01/07/2022 12:56:17 - INFO - __main__ -   Batch Number = 31
01/07/2022 12:56:17 - INFO - __main__ -   Batch Number = 32
01/07/2022 12:56:17 - INFO - __main__ -   Batch Number = 33
01/07/2022 12:56:17 - INFO - __main__ -   Batch Number = 34
01/07/2022 12:56:17 - INFO - __main__ -   Batch Number = 35
Evaluating:  22%|       | 35/156 [00:00<00:02, 42.61it/s]01/07/2022 12:56:17 - INFO - __main__ -   Batch Number = 36
01/07/2022 12:56:17 - INFO - __main__ -   Batch Number = 37
01/07/2022 12:56:17 - INFO - __main__ -   Batch Number = 38
01/07/2022 12:56:17 - INFO - __main__ -   Batch Number = 39
01/07/2022 12:56:17 - INFO - __main__ -   Batch Number = 40
Evaluating:  26%|       | 40/156 [00:00<00:02, 43.15it/s]01/07/2022 12:56:17 - INFO - __main__ -   Batch Number = 41
01/07/2022 12:56:17 - INFO - __main__ -   Batch Number = 42
01/07/2022 12:56:17 - INFO - __main__ -   Batch Number = 43
01/07/2022 12:56:17 - INFO - __main__ -   Batch Number = 44
01/07/2022 12:56:17 - INFO - __main__ -   Batch Number = 45
Evaluating:  29%|       | 45/156 [00:01<00:02, 43.64it/s]01/07/2022 12:56:18 - INFO - __main__ -   Batch Number = 46
01/07/2022 12:56:18 - INFO - __main__ -   Batch Number = 47
01/07/2022 12:56:18 - INFO - __main__ -   Batch Number = 48
01/07/2022 12:56:18 - INFO - __main__ -   Batch Number = 49
01/07/2022 12:56:18 - INFO - __main__ -   Batch Number = 50
Evaluating:  32%|      | 50/156 [00:01<00:02, 43.72it/s]01/07/2022 12:56:18 - INFO - __main__ -   Batch Number = 51
01/07/2022 12:56:18 - INFO - __main__ -   Batch Number = 52
01/07/2022 12:56:18 - INFO - __main__ -   Batch Number = 53
01/07/2022 12:56:18 - INFO - __main__ -   Batch Number = 54
01/07/2022 12:56:18 - INFO - __main__ -   Batch Number = 55
Evaluating:  35%|      | 55/156 [00:01<00:02, 43.82it/s]01/07/2022 12:56:18 - INFO - __main__ -   Batch Number = 56
01/07/2022 12:56:18 - INFO - __main__ -   Batch Number = 57
01/07/2022 12:56:18 - INFO - __main__ -   Batch Number = 58
01/07/2022 12:56:18 - INFO - __main__ -   Batch Number = 59
01/07/2022 12:56:18 - INFO - __main__ -   Batch Number = 60
Evaluating:  38%|      | 60/156 [00:01<00:02, 44.09it/s]01/07/2022 12:56:18 - INFO - __main__ -   Batch Number = 61
01/07/2022 12:56:18 - INFO - __main__ -   Batch Number = 62
01/07/2022 12:56:18 - INFO - __main__ -   Batch Number = 63
01/07/2022 12:56:18 - INFO - __main__ -   Batch Number = 64
01/07/2022 12:56:18 - INFO - __main__ -   Batch Number = 65
Evaluating:  42%|     | 65/156 [00:01<00:02, 43.89it/s]01/07/2022 12:56:18 - INFO - __main__ -   Batch Number = 66
01/07/2022 12:56:18 - INFO - __main__ -   Batch Number = 67
01/07/2022 12:56:18 - INFO - __main__ -   Batch Number = 68
01/07/2022 12:56:18 - INFO - __main__ -   Batch Number = 69
01/07/2022 12:56:18 - INFO - __main__ -   Batch Number = 70
Evaluating:  45%|     | 70/156 [00:01<00:02, 41.80it/s]01/07/2022 12:56:18 - INFO - __main__ -   Batch Number = 71
01/07/2022 12:56:18 - INFO - __main__ -   Batch Number = 72
01/07/2022 12:56:18 - INFO - __main__ -   Batch Number = 73
01/07/2022 12:56:18 - INFO - __main__ -   Batch Number = 74
01/07/2022 12:56:18 - INFO - __main__ -   Batch Number = 75
Evaluating:  48%|     | 75/156 [00:01<00:01, 40.96it/s]01/07/2022 12:56:18 - INFO - __main__ -   Batch Number = 76
01/07/2022 12:56:18 - INFO - __main__ -   Batch Number = 77
01/07/2022 12:56:18 - INFO - __main__ -   Batch Number = 78
01/07/2022 12:56:18 - INFO - __main__ -   Batch Number = 79
01/07/2022 12:56:18 - INFO - __main__ -   Batch Number = 80
Evaluating:  51%|    | 80/156 [00:01<00:01, 41.05it/s]01/07/2022 12:56:18 - INFO - __main__ -   Batch Number = 81
01/07/2022 12:56:18 - INFO - __main__ -   Batch Number = 82
01/07/2022 12:56:18 - INFO - __main__ -   Batch Number = 83
01/07/2022 12:56:18 - INFO - __main__ -   Batch Number = 84
01/07/2022 12:56:18 - INFO - __main__ -   Batch Number = 85
Evaluating:  54%|    | 85/156 [00:02<00:01, 42.05it/s]01/07/2022 12:56:18 - INFO - __main__ -   Batch Number = 86
01/07/2022 12:56:18 - INFO - __main__ -   Batch Number = 87
01/07/2022 12:56:18 - INFO - __main__ -   Batch Number = 88
01/07/2022 12:56:19 - INFO - __main__ -   Batch Number = 89
01/07/2022 12:56:19 - INFO - __main__ -   Batch Number = 90
Evaluating:  58%|    | 90/156 [00:02<00:01, 42.81it/s]01/07/2022 12:56:19 - INFO - __main__ -   Batch Number = 91
01/07/2022 12:56:19 - INFO - __main__ -   Batch Number = 92
01/07/2022 12:56:19 - INFO - __main__ -   Batch Number = 93
01/07/2022 12:56:19 - INFO - __main__ -   Batch Number = 94
01/07/2022 12:56:19 - INFO - __main__ -   Batch Number = 95
Evaluating:  61%|    | 95/156 [00:02<00:01, 43.34it/s]01/07/2022 12:56:19 - INFO - __main__ -   Batch Number = 96
01/07/2022 12:56:19 - INFO - __main__ -   Batch Number = 97
01/07/2022 12:56:19 - INFO - __main__ -   Batch Number = 98
01/07/2022 12:56:19 - INFO - __main__ -   Batch Number = 99
01/07/2022 12:56:19 - INFO - __main__ -   Batch Number = 100
Evaluating:  64%|   | 100/156 [00:02<00:01, 43.69it/s]01/07/2022 12:56:19 - INFO - __main__ -   Batch Number = 101
01/07/2022 12:56:19 - INFO - __main__ -   Batch Number = 102
01/07/2022 12:56:19 - INFO - __main__ -   Batch Number = 103
01/07/2022 12:56:19 - INFO - __main__ -   Batch Number = 104
01/07/2022 12:56:19 - INFO - __main__ -   Batch Number = 105
Evaluating:  67%|   | 105/156 [00:02<00:01, 44.08it/s]01/07/2022 12:56:19 - INFO - __main__ -   Batch Number = 106
01/07/2022 12:56:19 - INFO - __main__ -   Batch Number = 107
01/07/2022 12:56:19 - INFO - __main__ -   Batch Number = 108
01/07/2022 12:56:19 - INFO - __main__ -   Batch Number = 109
01/07/2022 12:56:19 - INFO - __main__ -   Batch Number = 110
Evaluating:  71%|   | 110/156 [00:02<00:01, 43.99it/s]01/07/2022 12:56:19 - INFO - __main__ -   Batch Number = 111
01/07/2022 12:56:19 - INFO - __main__ -   Batch Number = 112
01/07/2022 12:56:19 - INFO - __main__ -   Batch Number = 113
01/07/2022 12:56:19 - INFO - __main__ -   Batch Number = 114
01/07/2022 12:56:19 - INFO - __main__ -   Batch Number = 115
Evaluating:  74%|  | 115/156 [00:02<00:00, 44.26it/s]01/07/2022 12:56:19 - INFO - __main__ -   Batch Number = 116
01/07/2022 12:56:19 - INFO - __main__ -   Batch Number = 117
01/07/2022 12:56:19 - INFO - __main__ -   Batch Number = 118
01/07/2022 12:56:19 - INFO - __main__ -   Batch Number = 119
01/07/2022 12:56:19 - INFO - __main__ -   Batch Number = 120
Evaluating:  77%|  | 120/156 [00:02<00:00, 44.31it/s]01/07/2022 12:56:19 - INFO - __main__ -   Batch Number = 121
01/07/2022 12:56:19 - INFO - __main__ -   Batch Number = 122
01/07/2022 12:56:19 - INFO - __main__ -   Batch Number = 123
01/07/2022 12:56:19 - INFO - __main__ -   Batch Number = 124
01/07/2022 12:56:19 - INFO - __main__ -   Batch Number = 125
Evaluating:  80%|  | 125/156 [00:02<00:00, 44.33it/s]01/07/2022 12:56:19 - INFO - __main__ -   Batch Number = 126
01/07/2022 12:56:19 - INFO - __main__ -   Batch Number = 127
01/07/2022 12:56:19 - INFO - __main__ -   Batch Number = 128
01/07/2022 12:56:19 - INFO - __main__ -   Batch Number = 129
01/07/2022 12:56:19 - INFO - __main__ -   Batch Number = 130
Evaluating:  83%| | 130/156 [00:03<00:00, 44.46it/s]01/07/2022 12:56:19 - INFO - __main__ -   Batch Number = 131
01/07/2022 12:56:19 - INFO - __main__ -   Batch Number = 132
01/07/2022 12:56:20 - INFO - __main__ -   Batch Number = 133
01/07/2022 12:56:20 - INFO - __main__ -   Batch Number = 134
01/07/2022 12:56:20 - INFO - __main__ -   Batch Number = 135
Evaluating:  87%| | 135/156 [00:03<00:00, 44.38it/s]01/07/2022 12:56:20 - INFO - __main__ -   Batch Number = 136
01/07/2022 12:56:20 - INFO - __main__ -   Batch Number = 137
01/07/2022 12:56:20 - INFO - __main__ -   Batch Number = 138
01/07/2022 12:56:20 - INFO - __main__ -   Batch Number = 139
01/07/2022 12:56:20 - INFO - __main__ -   Batch Number = 140
Evaluating:  90%| | 140/156 [00:03<00:00, 44.31it/s]01/07/2022 12:56:20 - INFO - __main__ -   Batch Number = 141
01/07/2022 12:56:20 - INFO - __main__ -   Batch Number = 142
01/07/2022 12:56:20 - INFO - __main__ -   Batch Number = 143
01/07/2022 12:56:20 - INFO - __main__ -   Batch Number = 144
01/07/2022 12:56:20 - INFO - __main__ -   Batch Number = 145
Evaluating:  93%|| 145/156 [00:03<00:00, 44.33it/s]01/07/2022 12:56:20 - INFO - __main__ -   Batch Number = 146
01/07/2022 12:56:20 - INFO - __main__ -   Batch Number = 147
01/07/2022 12:56:20 - INFO - __main__ -   Batch Number = 148
01/07/2022 12:56:20 - INFO - __main__ -   Batch Number = 149
01/07/2022 12:56:20 - INFO - __main__ -   Batch Number = 150
Evaluating:  96%|| 150/156 [00:03<00:00, 44.44it/s]01/07/2022 12:56:20 - INFO - __main__ -   Batch Number = 151
01/07/2022 12:56:20 - INFO - __main__ -   Batch Number = 152
01/07/2022 12:56:20 - INFO - __main__ -   Batch Number = 153
01/07/2022 12:56:20 - INFO - __main__ -   Batch Number = 154
01/07/2022 12:56:20 - INFO - __main__ -   Batch Number = 155
Evaluating:  99%|| 155/156 [00:03<00:00, 44.29it/s]01/07/2022 12:56:20 - INFO - __main__ -   Batch Number = 156
Evaluating: 100%|| 156/156 [00:03<00:00, 42.85it/s]
01/07/2022 12:56:20 - INFO - __main__ -     Evaluation done in total 3.641355 secs (0.002922 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/predictions_zh_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/nbest_predictions_zh_.json
01/07/2022 12:56:44 - INFO - __main__ -   Results = OrderedDict([('exact', 36.97478991596638), ('f1', 49.835497835497826), ('total', 1190), ('HasAns_exact', 36.97478991596638), ('HasAns_f1', 49.835497835497826), ('HasAns_total', 1190), ('best_exact', 36.97478991596638), ('best_exact_thresh', 0.0), ('best_f1', 49.835497835497826), ('best_f1_thresh', 0.0)])
PyTorch version 1.10.0+cu111 available.
01/07/2022 13:02:42 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/07/2022 13:02:42 - INFO - root -   save model
loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /root/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6
Model config XLMRobertaConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /root/.cache/torch/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8
loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/07/2022 13:02:51 - INFO - __main__ -   lang2id = None
01/07/2022 13:02:54 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='xlm-roberta-base', model_type='xlm-roberta', config_name=None, tokenizer_name=None, cache_dir=None, data_dir='/root/Desktop/cloud-emea-copy/data//xquad', output_dir='/root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/', max_seq_length=384, train_file='/root/Desktop/cloud-emea-copy/data//xquad/train-v1.1.json', predict_file='/root/Desktop/cloud-emea-copy/data//xquad/dev-v1.1.json', eval_test_set=False, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=4, per_gpu_eval_batch_size=8, gradient_accumulation_steps=4, learning_rate=0.0001, weight_decay=0.0001, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=15.0, max_steps=-1, save_steps=1000, warmup_steps=500, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', eval_lang='en', predict_langs='en', train_lang='en', log_file='/root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='qna', threads=8, version_2_with_negative=False, verbose_logging=False, n_best_size=20, max_query_length=64, max_answer_length=30, doc_stride=128, null_score_diff_threshold=0.0, predict_task_adapter='output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s1/checkpoint-best/qna', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix=None, en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/07/2022 13:02:54 - INFO - __main__ -   Results: {}
loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /root/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6
Model config XLMRobertaConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /root/.cache/torch/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8
loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/07/2022 13:03:03 - INFO - __main__ -   lang2id = None
01/07/2022 13:03:03 - INFO - __main__ -   Evaluating the model on the test set of all languages specified
01/07/2022 13:03:03 - INFO - __main__ -   Adapter will be loaded from this path: output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s1/checkpoint-best/qna
01/07/2022 13:03:03 - INFO - root -   Trying to decide if add adapter
01/07/2022 13:03:03 - INFO - root -   loading task adapter
Loading module configuration from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s1/checkpoint-best/qna/adapter_config.json
Adding adapter 'qna' of type 'text_task'.
Loading module weights from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s1/checkpoint-best/qna/pytorch_adapter.bin
Loading module configuration from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s1/checkpoint-best/qna/head_config.json
Loading module weights from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s1/checkpoint-best/qna/pytorch_model_head.bin
01/07/2022 13:03:03 - INFO - root -   loading lang adpater en/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/xlm-roberta-base/pfeiffer/en_relu_2.zip.
Loading module configuration from /root/.cache/torch/adapters/3e9c8c800af886cd1c684a3acf4929990b35f3ac75e050e942de2ff2ecf895fe-caabba7772816577124631d5b84810c4d32f49afcea4fca637162ec7f432174f-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
Loading module weights from /root/.cache/torch/adapters/3e9c8c800af886cd1c684a3acf4929990b35f3ac75e050e942de2ff2ecf895fe-caabba7772816577124631d5b84810c4d32f49afcea4fca637162ec7f432174f-extracted/pytorch_adapter.bin
No matching prediction head found in '/root/.cache/torch/adapters/3e9c8c800af886cd1c684a3acf4929990b35f3ac75e050e942de2ff2ecf895fe-caabba7772816577124631d5b84810c4d32f49afcea4fca637162ec7f432174f-extracted'
01/07/2022 13:03:10 - INFO - __main__ -   Language adapter for en found
01/07/2022 13:03:10 - INFO - __main__ -   Set active language adapter to en
01/07/2022 13:03:10 - INFO - __main__ -   Args Adapter Weight = None
01/07/2022 13:03:10 - INFO - __main__ -   Adapter Languages = ['en']
01/07/2022 13:03:10 - INFO - __main__ -   Predict File = xquad.en.json
01/07/2022 13:03:10 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
en en/wiki@ukp
  0%|          | 0/48 [00:00<?, ?it/s] 33%|      | 16/48 [00:00<00:00, 133.87it/s] 62%|   | 30/48 [00:00<00:00, 125.42it/s] 94%|| 45/48 [00:00<00:00, 133.44it/s]100%|| 48/48 [00:00<00:00, 134.36it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<04:02,  4.91it/s]convert squad examples to features:  32%|      | 385/1190 [00:00<00:01, 792.09it/s]convert squad examples to features:  94%|| 1121/1190 [00:00<00:00, 2208.42it/s]convert squad examples to features: 100%|| 1190/1190 [00:00<00:00, 1511.80it/s]/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|| 1190/1190 [00:00<00:00, 732602.64it/s]
01/07/2022 13:03:12 - INFO - __main__ -   Local Rank = -1
01/07/2022 13:03:12 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.en.json_xlm-roberta-base_384_en
01/07/2022 13:03:13 - INFO - __main__ -   ***** Running evaluation  *****
01/07/2022 13:03:13 - INFO - __main__ -     Num examples = 1270
01/07/2022 13:03:13 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/159 [00:00<?, ?it/s]01/07/2022 13:03:13 - INFO - __main__ -   Batch Number = 1
01/07/2022 13:03:13 - INFO - __main__ -   Batch Number = 2
01/07/2022 13:03:13 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|         | 3/159 [00:00<00:05, 26.95it/s]01/07/2022 13:03:13 - INFO - __main__ -   Batch Number = 4
01/07/2022 13:03:13 - INFO - __main__ -   Batch Number = 5
01/07/2022 13:03:13 - INFO - __main__ -   Batch Number = 6
01/07/2022 13:03:13 - INFO - __main__ -   Batch Number = 7
Evaluating:   4%|         | 7/159 [00:00<00:04, 33.03it/s]01/07/2022 13:03:13 - INFO - __main__ -   Batch Number = 8
01/07/2022 13:03:13 - INFO - __main__ -   Batch Number = 9
01/07/2022 13:03:13 - INFO - __main__ -   Batch Number = 10
01/07/2022 13:03:13 - INFO - __main__ -   Batch Number = 11
01/07/2022 13:03:13 - INFO - __main__ -   Batch Number = 12
Evaluating:   8%|         | 12/159 [00:00<00:03, 36.84it/s]01/07/2022 13:03:14 - INFO - __main__ -   Batch Number = 13
01/07/2022 13:03:14 - INFO - __main__ -   Batch Number = 14
01/07/2022 13:03:14 - INFO - __main__ -   Batch Number = 15
01/07/2022 13:03:14 - INFO - __main__ -   Batch Number = 16
01/07/2022 13:03:14 - INFO - __main__ -   Batch Number = 17
Evaluating:  11%|         | 17/159 [00:00<00:03, 39.74it/s]01/07/2022 13:03:14 - INFO - __main__ -   Batch Number = 18
01/07/2022 13:03:14 - INFO - __main__ -   Batch Number = 19
01/07/2022 13:03:14 - INFO - __main__ -   Batch Number = 20
01/07/2022 13:03:14 - INFO - __main__ -   Batch Number = 21
01/07/2022 13:03:14 - INFO - __main__ -   Batch Number = 22
Evaluating:  14%|        | 22/159 [00:00<00:03, 41.55it/s]01/07/2022 13:03:14 - INFO - __main__ -   Batch Number = 23
01/07/2022 13:03:14 - INFO - __main__ -   Batch Number = 24
01/07/2022 13:03:14 - INFO - __main__ -   Batch Number = 25
01/07/2022 13:03:14 - INFO - __main__ -   Batch Number = 26
01/07/2022 13:03:14 - INFO - __main__ -   Batch Number = 27
Evaluating:  17%|        | 27/159 [00:00<00:03, 42.76it/s]01/07/2022 13:03:14 - INFO - __main__ -   Batch Number = 28
01/07/2022 13:03:14 - INFO - __main__ -   Batch Number = 29
01/07/2022 13:03:14 - INFO - __main__ -   Batch Number = 30
01/07/2022 13:03:14 - INFO - __main__ -   Batch Number = 31
01/07/2022 13:03:14 - INFO - __main__ -   Batch Number = 32
Evaluating:  20%|        | 32/159 [00:00<00:02, 43.46it/s]01/07/2022 13:03:14 - INFO - __main__ -   Batch Number = 33
01/07/2022 13:03:14 - INFO - __main__ -   Batch Number = 34
01/07/2022 13:03:14 - INFO - __main__ -   Batch Number = 35
01/07/2022 13:03:14 - INFO - __main__ -   Batch Number = 36
01/07/2022 13:03:14 - INFO - __main__ -   Batch Number = 37
Evaluating:  23%|       | 37/159 [00:00<00:02, 43.98it/s]01/07/2022 13:03:14 - INFO - __main__ -   Batch Number = 38
01/07/2022 13:03:14 - INFO - __main__ -   Batch Number = 39
01/07/2022 13:03:14 - INFO - __main__ -   Batch Number = 40
01/07/2022 13:03:14 - INFO - __main__ -   Batch Number = 41
01/07/2022 13:03:14 - INFO - __main__ -   Batch Number = 42
Evaluating:  26%|       | 42/159 [00:01<00:02, 44.39it/s]01/07/2022 13:03:14 - INFO - __main__ -   Batch Number = 43
01/07/2022 13:03:14 - INFO - __main__ -   Batch Number = 44
01/07/2022 13:03:14 - INFO - __main__ -   Batch Number = 45
01/07/2022 13:03:14 - INFO - __main__ -   Batch Number = 46
01/07/2022 13:03:14 - INFO - __main__ -   Batch Number = 47
Evaluating:  30%|       | 47/159 [00:01<00:02, 42.90it/s]01/07/2022 13:03:14 - INFO - __main__ -   Batch Number = 48
01/07/2022 13:03:14 - INFO - __main__ -   Batch Number = 49
01/07/2022 13:03:14 - INFO - __main__ -   Batch Number = 50
01/07/2022 13:03:14 - INFO - __main__ -   Batch Number = 51
01/07/2022 13:03:14 - INFO - __main__ -   Batch Number = 52
Evaluating:  33%|      | 52/159 [00:01<00:02, 43.52it/s]01/07/2022 13:03:14 - INFO - __main__ -   Batch Number = 53
01/07/2022 13:03:14 - INFO - __main__ -   Batch Number = 54
01/07/2022 13:03:14 - INFO - __main__ -   Batch Number = 55
01/07/2022 13:03:14 - INFO - __main__ -   Batch Number = 56
01/07/2022 13:03:15 - INFO - __main__ -   Batch Number = 57
Evaluating:  36%|      | 57/159 [00:01<00:02, 43.99it/s]01/07/2022 13:03:15 - INFO - __main__ -   Batch Number = 58
01/07/2022 13:03:15 - INFO - __main__ -   Batch Number = 59
01/07/2022 13:03:15 - INFO - __main__ -   Batch Number = 60
01/07/2022 13:03:15 - INFO - __main__ -   Batch Number = 61
01/07/2022 13:03:15 - INFO - __main__ -   Batch Number = 62
Evaluating:  39%|      | 62/159 [00:01<00:03, 31.78it/s]01/07/2022 13:03:15 - INFO - __main__ -   Batch Number = 63
01/07/2022 13:03:15 - INFO - __main__ -   Batch Number = 64
01/07/2022 13:03:15 - INFO - __main__ -   Batch Number = 65
01/07/2022 13:03:15 - INFO - __main__ -   Batch Number = 66
01/07/2022 13:03:15 - INFO - __main__ -   Batch Number = 67
Evaluating:  42%|     | 67/159 [00:01<00:02, 34.77it/s]01/07/2022 13:03:15 - INFO - __main__ -   Batch Number = 68
01/07/2022 13:03:15 - INFO - __main__ -   Batch Number = 69
01/07/2022 13:03:15 - INFO - __main__ -   Batch Number = 70
01/07/2022 13:03:15 - INFO - __main__ -   Batch Number = 71
01/07/2022 13:03:15 - INFO - __main__ -   Batch Number = 72
Evaluating:  45%|     | 72/159 [00:01<00:02, 37.15it/s]01/07/2022 13:03:15 - INFO - __main__ -   Batch Number = 73
01/07/2022 13:03:15 - INFO - __main__ -   Batch Number = 74
01/07/2022 13:03:15 - INFO - __main__ -   Batch Number = 75
01/07/2022 13:03:15 - INFO - __main__ -   Batch Number = 76
01/07/2022 13:03:15 - INFO - __main__ -   Batch Number = 77
Evaluating:  48%|     | 77/159 [00:01<00:02, 38.63it/s]01/07/2022 13:03:15 - INFO - __main__ -   Batch Number = 78
01/07/2022 13:03:15 - INFO - __main__ -   Batch Number = 79
01/07/2022 13:03:15 - INFO - __main__ -   Batch Number = 80
01/07/2022 13:03:15 - INFO - __main__ -   Batch Number = 81
01/07/2022 13:03:15 - INFO - __main__ -   Batch Number = 82
Evaluating:  52%|    | 82/159 [00:02<00:01, 40.16it/s]01/07/2022 13:03:15 - INFO - __main__ -   Batch Number = 83
01/07/2022 13:03:15 - INFO - __main__ -   Batch Number = 84
01/07/2022 13:03:15 - INFO - __main__ -   Batch Number = 85
01/07/2022 13:03:15 - INFO - __main__ -   Batch Number = 86
01/07/2022 13:03:15 - INFO - __main__ -   Batch Number = 87
Evaluating:  55%|    | 87/159 [00:02<00:01, 41.56it/s]01/07/2022 13:03:15 - INFO - __main__ -   Batch Number = 88
01/07/2022 13:03:15 - INFO - __main__ -   Batch Number = 89
01/07/2022 13:03:15 - INFO - __main__ -   Batch Number = 90
01/07/2022 13:03:15 - INFO - __main__ -   Batch Number = 91
01/07/2022 13:03:15 - INFO - __main__ -   Batch Number = 92
Evaluating:  58%|    | 92/159 [00:02<00:01, 42.32it/s]01/07/2022 13:03:15 - INFO - __main__ -   Batch Number = 93
01/07/2022 13:03:15 - INFO - __main__ -   Batch Number = 94
01/07/2022 13:03:16 - INFO - __main__ -   Batch Number = 95
01/07/2022 13:03:16 - INFO - __main__ -   Batch Number = 96
01/07/2022 13:03:16 - INFO - __main__ -   Batch Number = 97
Evaluating:  61%|    | 97/159 [00:02<00:01, 43.10it/s]01/07/2022 13:03:16 - INFO - __main__ -   Batch Number = 98
01/07/2022 13:03:16 - INFO - __main__ -   Batch Number = 99
01/07/2022 13:03:16 - INFO - __main__ -   Batch Number = 100
01/07/2022 13:03:16 - INFO - __main__ -   Batch Number = 101
01/07/2022 13:03:16 - INFO - __main__ -   Batch Number = 102
Evaluating:  64%|   | 102/159 [00:02<00:01, 43.60it/s]01/07/2022 13:03:16 - INFO - __main__ -   Batch Number = 103
01/07/2022 13:03:16 - INFO - __main__ -   Batch Number = 104
01/07/2022 13:03:16 - INFO - __main__ -   Batch Number = 105
01/07/2022 13:03:16 - INFO - __main__ -   Batch Number = 106
01/07/2022 13:03:16 - INFO - __main__ -   Batch Number = 107
Evaluating:  67%|   | 107/159 [00:02<00:01, 43.89it/s]01/07/2022 13:03:16 - INFO - __main__ -   Batch Number = 108
01/07/2022 13:03:16 - INFO - __main__ -   Batch Number = 109
01/07/2022 13:03:16 - INFO - __main__ -   Batch Number = 110
01/07/2022 13:03:16 - INFO - __main__ -   Batch Number = 111
01/07/2022 13:03:16 - INFO - __main__ -   Batch Number = 112
Evaluating:  70%|   | 112/159 [00:02<00:01, 44.15it/s]01/07/2022 13:03:16 - INFO - __main__ -   Batch Number = 113
01/07/2022 13:03:16 - INFO - __main__ -   Batch Number = 114
01/07/2022 13:03:16 - INFO - __main__ -   Batch Number = 115
01/07/2022 13:03:16 - INFO - __main__ -   Batch Number = 116
01/07/2022 13:03:16 - INFO - __main__ -   Batch Number = 117
Evaluating:  74%|  | 117/159 [00:02<00:00, 44.46it/s]01/07/2022 13:03:16 - INFO - __main__ -   Batch Number = 118
01/07/2022 13:03:16 - INFO - __main__ -   Batch Number = 119
01/07/2022 13:03:16 - INFO - __main__ -   Batch Number = 120
01/07/2022 13:03:16 - INFO - __main__ -   Batch Number = 121
01/07/2022 13:03:16 - INFO - __main__ -   Batch Number = 122
Evaluating:  77%|  | 122/159 [00:02<00:00, 44.42it/s]01/07/2022 13:03:16 - INFO - __main__ -   Batch Number = 123
01/07/2022 13:03:16 - INFO - __main__ -   Batch Number = 124
01/07/2022 13:03:16 - INFO - __main__ -   Batch Number = 125
01/07/2022 13:03:16 - INFO - __main__ -   Batch Number = 126
01/07/2022 13:03:16 - INFO - __main__ -   Batch Number = 127
Evaluating:  80%|  | 127/159 [00:03<00:00, 44.46it/s]01/07/2022 13:03:16 - INFO - __main__ -   Batch Number = 128
01/07/2022 13:03:16 - INFO - __main__ -   Batch Number = 129
01/07/2022 13:03:16 - INFO - __main__ -   Batch Number = 130
01/07/2022 13:03:16 - INFO - __main__ -   Batch Number = 131
01/07/2022 13:03:16 - INFO - __main__ -   Batch Number = 132
Evaluating:  83%| | 132/159 [00:03<00:00, 44.55it/s]01/07/2022 13:03:16 - INFO - __main__ -   Batch Number = 133
01/07/2022 13:03:16 - INFO - __main__ -   Batch Number = 134
01/07/2022 13:03:16 - INFO - __main__ -   Batch Number = 135
01/07/2022 13:03:16 - INFO - __main__ -   Batch Number = 136
01/07/2022 13:03:16 - INFO - __main__ -   Batch Number = 137
Evaluating:  86%| | 137/159 [00:03<00:00, 44.58it/s]01/07/2022 13:03:16 - INFO - __main__ -   Batch Number = 138
01/07/2022 13:03:16 - INFO - __main__ -   Batch Number = 139
01/07/2022 13:03:17 - INFO - __main__ -   Batch Number = 140
01/07/2022 13:03:17 - INFO - __main__ -   Batch Number = 141
01/07/2022 13:03:17 - INFO - __main__ -   Batch Number = 142
Evaluating:  89%| | 142/159 [00:03<00:00, 44.63it/s]01/07/2022 13:03:17 - INFO - __main__ -   Batch Number = 143
01/07/2022 13:03:17 - INFO - __main__ -   Batch Number = 144
01/07/2022 13:03:17 - INFO - __main__ -   Batch Number = 145
01/07/2022 13:03:17 - INFO - __main__ -   Batch Number = 146
01/07/2022 13:03:17 - INFO - __main__ -   Batch Number = 147
Evaluating:  92%|| 147/159 [00:03<00:00, 44.64it/s]01/07/2022 13:03:17 - INFO - __main__ -   Batch Number = 148
01/07/2022 13:03:17 - INFO - __main__ -   Batch Number = 149
01/07/2022 13:03:17 - INFO - __main__ -   Batch Number = 150
01/07/2022 13:03:17 - INFO - __main__ -   Batch Number = 151
01/07/2022 13:03:17 - INFO - __main__ -   Batch Number = 152
Evaluating:  96%|| 152/159 [00:03<00:00, 44.60it/s]01/07/2022 13:03:17 - INFO - __main__ -   Batch Number = 153
01/07/2022 13:03:17 - INFO - __main__ -   Batch Number = 154
01/07/2022 13:03:17 - INFO - __main__ -   Batch Number = 155
01/07/2022 13:03:17 - INFO - __main__ -   Batch Number = 156
01/07/2022 13:03:17 - INFO - __main__ -   Batch Number = 157
Evaluating:  99%|| 157/159 [00:03<00:00, 44.66it/s]01/07/2022 13:03:17 - INFO - __main__ -   Batch Number = 158
01/07/2022 13:03:17 - INFO - __main__ -   Batch Number = 159
Evaluating: 100%|| 159/159 [00:03<00:00, 42.00it/s]
01/07/2022 13:03:17 - INFO - __main__ -     Evaluation done in total 3.786341 secs (0.002981 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/predictions_en_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/nbest_predictions_en_.json
01/07/2022 13:03:20 - INFO - __main__ -   Results = OrderedDict([('exact', 70.75630252100841), ('f1', 82.91804895129187), ('total', 1190), ('HasAns_exact', 70.75630252100841), ('HasAns_f1', 82.91804895129187), ('HasAns_total', 1190), ('best_exact', 70.75630252100841), ('best_exact_thresh', 0.0), ('best_f1', 82.91804895129187), ('best_f1_thresh', 0.0)])
PyTorch version 1.10.0+cu111 available.
01/07/2022 13:03:22 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/07/2022 13:03:22 - INFO - root -   save model
loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /root/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6
Model config XLMRobertaConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /root/.cache/torch/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8
loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/07/2022 13:03:32 - INFO - __main__ -   lang2id = None
01/07/2022 13:03:35 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='xlm-roberta-base', model_type='xlm-roberta', config_name=None, tokenizer_name=None, cache_dir=None, data_dir='/root/Desktop/cloud-emea-copy/data//xquad', output_dir='/root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/', max_seq_length=384, train_file='/root/Desktop/cloud-emea-copy/data//xquad/train-v1.1.json', predict_file='/root/Desktop/cloud-emea-copy/data//xquad/dev-v1.1.json', eval_test_set=False, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=4, per_gpu_eval_batch_size=8, gradient_accumulation_steps=4, learning_rate=0.0001, weight_decay=0.0001, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=15.0, max_steps=-1, save_steps=1000, warmup_steps=500, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', eval_lang='en', predict_langs='en', train_lang='en', log_file='/root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='qna', threads=8, version_2_with_negative=False, verbose_logging=False, n_best_size=20, max_query_length=64, max_answer_length=30, doc_stride=128, null_score_diff_threshold=0.0, predict_task_adapter='output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s2/checkpoint-best/qna', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix=None, en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/07/2022 13:03:35 - INFO - __main__ -   Results: {}
loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /root/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6
Model config XLMRobertaConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /root/.cache/torch/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8
loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/07/2022 13:03:44 - INFO - __main__ -   lang2id = None
01/07/2022 13:03:44 - INFO - __main__ -   Evaluating the model on the test set of all languages specified
01/07/2022 13:03:44 - INFO - __main__ -   Adapter will be loaded from this path: output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s2/checkpoint-best/qna
01/07/2022 13:03:44 - INFO - root -   Trying to decide if add adapter
01/07/2022 13:03:44 - INFO - root -   loading task adapter
Loading module configuration from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s2/checkpoint-best/qna/adapter_config.json
Adding adapter 'qna' of type 'text_task'.
Loading module weights from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s2/checkpoint-best/qna/pytorch_adapter.bin
Loading module configuration from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s2/checkpoint-best/qna/head_config.json
Loading module weights from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s2/checkpoint-best/qna/pytorch_model_head.bin
01/07/2022 13:03:44 - INFO - root -   loading lang adpater en/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/xlm-roberta-base/pfeiffer/en_relu_2.zip.
Loading module configuration from /root/.cache/torch/adapters/3e9c8c800af886cd1c684a3acf4929990b35f3ac75e050e942de2ff2ecf895fe-caabba7772816577124631d5b84810c4d32f49afcea4fca637162ec7f432174f-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
Loading module weights from /root/.cache/torch/adapters/3e9c8c800af886cd1c684a3acf4929990b35f3ac75e050e942de2ff2ecf895fe-caabba7772816577124631d5b84810c4d32f49afcea4fca637162ec7f432174f-extracted/pytorch_adapter.bin
No matching prediction head found in '/root/.cache/torch/adapters/3e9c8c800af886cd1c684a3acf4929990b35f3ac75e050e942de2ff2ecf895fe-caabba7772816577124631d5b84810c4d32f49afcea4fca637162ec7f432174f-extracted'
01/07/2022 13:03:46 - INFO - __main__ -   Language adapter for en found
01/07/2022 13:03:46 - INFO - __main__ -   Set active language adapter to en
01/07/2022 13:03:46 - INFO - __main__ -   Args Adapter Weight = None
01/07/2022 13:03:46 - INFO - __main__ -   Adapter Languages = ['en']
01/07/2022 13:03:46 - INFO - __main__ -   Predict File = xquad.en.json
01/07/2022 13:03:46 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
en en/wiki@ukp
  0%|          | 0/48 [00:00<?, ?it/s] 33%|      | 16/48 [00:00<00:00, 134.82it/s] 62%|   | 30/48 [00:00<00:00, 126.42it/s] 94%|| 45/48 [00:00<00:00, 133.54it/s]100%|| 48/48 [00:00<00:00, 134.75it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<02:51,  6.92it/s]convert squad examples to features:  22%|       | 257/1190 [00:00<00:00, 1020.89it/s]convert squad examples to features:  32%|      | 385/1190 [00:00<00:01, 694.61it/s] convert squad examples to features:  94%|| 1121/1190 [00:00<00:00, 2127.56it/s]convert squad examples to features: 100%|| 1190/1190 [00:00<00:00, 1517.69it/s]/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|| 1190/1190 [00:00<00:00, 699442.51it/s]
01/07/2022 13:03:48 - INFO - __main__ -   Local Rank = -1
01/07/2022 13:03:48 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.en.json_xlm-roberta-base_384_en
01/07/2022 13:03:49 - INFO - __main__ -   ***** Running evaluation  *****
01/07/2022 13:03:49 - INFO - __main__ -     Num examples = 1270
01/07/2022 13:03:49 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/159 [00:00<?, ?it/s]01/07/2022 13:03:49 - INFO - __main__ -   Batch Number = 1
01/07/2022 13:03:49 - INFO - __main__ -   Batch Number = 2
01/07/2022 13:03:49 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|         | 3/159 [00:00<00:05, 26.72it/s]01/07/2022 13:03:49 - INFO - __main__ -   Batch Number = 4
01/07/2022 13:03:49 - INFO - __main__ -   Batch Number = 5
01/07/2022 13:03:49 - INFO - __main__ -   Batch Number = 6
01/07/2022 13:03:49 - INFO - __main__ -   Batch Number = 7
Evaluating:   4%|         | 7/159 [00:00<00:04, 32.35it/s]01/07/2022 13:03:49 - INFO - __main__ -   Batch Number = 8
01/07/2022 13:03:49 - INFO - __main__ -   Batch Number = 9
01/07/2022 13:03:49 - INFO - __main__ -   Batch Number = 10
01/07/2022 13:03:49 - INFO - __main__ -   Batch Number = 11
01/07/2022 13:03:50 - INFO - __main__ -   Batch Number = 12
Evaluating:   8%|         | 12/159 [00:00<00:03, 38.14it/s]01/07/2022 13:03:50 - INFO - __main__ -   Batch Number = 13
01/07/2022 13:03:50 - INFO - __main__ -   Batch Number = 14
01/07/2022 13:03:50 - INFO - __main__ -   Batch Number = 15
01/07/2022 13:03:50 - INFO - __main__ -   Batch Number = 16
01/07/2022 13:03:50 - INFO - __main__ -   Batch Number = 17
Evaluating:  11%|         | 17/159 [00:00<00:03, 40.65it/s]01/07/2022 13:03:50 - INFO - __main__ -   Batch Number = 18
01/07/2022 13:03:50 - INFO - __main__ -   Batch Number = 19
01/07/2022 13:03:50 - INFO - __main__ -   Batch Number = 20
01/07/2022 13:03:50 - INFO - __main__ -   Batch Number = 21
01/07/2022 13:03:50 - INFO - __main__ -   Batch Number = 22
Evaluating:  14%|        | 22/159 [00:00<00:03, 42.26it/s]01/07/2022 13:03:50 - INFO - __main__ -   Batch Number = 23
01/07/2022 13:03:50 - INFO - __main__ -   Batch Number = 24
01/07/2022 13:03:50 - INFO - __main__ -   Batch Number = 25
01/07/2022 13:03:50 - INFO - __main__ -   Batch Number = 26
01/07/2022 13:03:50 - INFO - __main__ -   Batch Number = 27
Evaluating:  17%|        | 27/159 [00:00<00:03, 43.16it/s]01/07/2022 13:03:50 - INFO - __main__ -   Batch Number = 28
01/07/2022 13:03:50 - INFO - __main__ -   Batch Number = 29
01/07/2022 13:03:50 - INFO - __main__ -   Batch Number = 30
01/07/2022 13:03:50 - INFO - __main__ -   Batch Number = 31
01/07/2022 13:03:50 - INFO - __main__ -   Batch Number = 32
Evaluating:  20%|        | 32/159 [00:00<00:02, 43.74it/s]01/07/2022 13:03:50 - INFO - __main__ -   Batch Number = 33
01/07/2022 13:03:50 - INFO - __main__ -   Batch Number = 34
01/07/2022 13:03:50 - INFO - __main__ -   Batch Number = 35
01/07/2022 13:03:50 - INFO - __main__ -   Batch Number = 36
01/07/2022 13:03:50 - INFO - __main__ -   Batch Number = 37
Evaluating:  23%|       | 37/159 [00:00<00:02, 44.21it/s]01/07/2022 13:03:50 - INFO - __main__ -   Batch Number = 38
01/07/2022 13:03:50 - INFO - __main__ -   Batch Number = 39
01/07/2022 13:03:50 - INFO - __main__ -   Batch Number = 40
01/07/2022 13:03:50 - INFO - __main__ -   Batch Number = 41
01/07/2022 13:03:50 - INFO - __main__ -   Batch Number = 42
Evaluating:  26%|       | 42/159 [00:01<00:02, 44.53it/s]01/07/2022 13:03:50 - INFO - __main__ -   Batch Number = 43
01/07/2022 13:03:50 - INFO - __main__ -   Batch Number = 44
01/07/2022 13:03:50 - INFO - __main__ -   Batch Number = 45
01/07/2022 13:03:50 - INFO - __main__ -   Batch Number = 46
01/07/2022 13:03:50 - INFO - __main__ -   Batch Number = 47
Evaluating:  30%|       | 47/159 [00:01<00:02, 43.08it/s]01/07/2022 13:03:50 - INFO - __main__ -   Batch Number = 48
01/07/2022 13:03:50 - INFO - __main__ -   Batch Number = 49
01/07/2022 13:03:50 - INFO - __main__ -   Batch Number = 50
01/07/2022 13:03:50 - INFO - __main__ -   Batch Number = 51
01/07/2022 13:03:50 - INFO - __main__ -   Batch Number = 52
Evaluating:  33%|      | 52/159 [00:01<00:02, 43.62it/s]01/07/2022 13:03:50 - INFO - __main__ -   Batch Number = 53
01/07/2022 13:03:50 - INFO - __main__ -   Batch Number = 54
01/07/2022 13:03:50 - INFO - __main__ -   Batch Number = 55
01/07/2022 13:03:50 - INFO - __main__ -   Batch Number = 56
01/07/2022 13:03:51 - INFO - __main__ -   Batch Number = 57
Evaluating:  36%|      | 57/159 [00:01<00:02, 44.04it/s]01/07/2022 13:03:51 - INFO - __main__ -   Batch Number = 58
01/07/2022 13:03:51 - INFO - __main__ -   Batch Number = 59
01/07/2022 13:03:51 - INFO - __main__ -   Batch Number = 60
01/07/2022 13:03:51 - INFO - __main__ -   Batch Number = 61
01/07/2022 13:03:51 - INFO - __main__ -   Batch Number = 62
Evaluating:  39%|      | 62/159 [00:01<00:03, 32.09it/s]01/07/2022 13:03:51 - INFO - __main__ -   Batch Number = 63
01/07/2022 13:03:51 - INFO - __main__ -   Batch Number = 64
01/07/2022 13:03:51 - INFO - __main__ -   Batch Number = 65
01/07/2022 13:03:51 - INFO - __main__ -   Batch Number = 66
01/07/2022 13:03:51 - INFO - __main__ -   Batch Number = 67
Evaluating:  42%|     | 67/159 [00:01<00:02, 35.04it/s]01/07/2022 13:03:51 - INFO - __main__ -   Batch Number = 68
01/07/2022 13:03:51 - INFO - __main__ -   Batch Number = 69
01/07/2022 13:03:51 - INFO - __main__ -   Batch Number = 70
01/07/2022 13:03:51 - INFO - __main__ -   Batch Number = 71
01/07/2022 13:03:51 - INFO - __main__ -   Batch Number = 72
Evaluating:  45%|     | 72/159 [00:01<00:02, 37.42it/s]01/07/2022 13:03:51 - INFO - __main__ -   Batch Number = 73
01/07/2022 13:03:51 - INFO - __main__ -   Batch Number = 74
01/07/2022 13:03:51 - INFO - __main__ -   Batch Number = 75
01/07/2022 13:03:51 - INFO - __main__ -   Batch Number = 76
01/07/2022 13:03:51 - INFO - __main__ -   Batch Number = 77
Evaluating:  48%|     | 77/159 [00:01<00:02, 39.37it/s]01/07/2022 13:03:51 - INFO - __main__ -   Batch Number = 78
01/07/2022 13:03:51 - INFO - __main__ -   Batch Number = 79
01/07/2022 13:03:51 - INFO - __main__ -   Batch Number = 80
01/07/2022 13:03:51 - INFO - __main__ -   Batch Number = 81
01/07/2022 13:03:51 - INFO - __main__ -   Batch Number = 82
Evaluating:  52%|    | 82/159 [00:02<00:01, 40.81it/s]01/07/2022 13:03:51 - INFO - __main__ -   Batch Number = 83
01/07/2022 13:03:51 - INFO - __main__ -   Batch Number = 84
01/07/2022 13:03:51 - INFO - __main__ -   Batch Number = 85
01/07/2022 13:03:51 - INFO - __main__ -   Batch Number = 86
01/07/2022 13:03:51 - INFO - __main__ -   Batch Number = 87
Evaluating:  55%|    | 87/159 [00:02<00:01, 41.99it/s]01/07/2022 13:03:51 - INFO - __main__ -   Batch Number = 88
01/07/2022 13:03:51 - INFO - __main__ -   Batch Number = 89
01/07/2022 13:03:51 - INFO - __main__ -   Batch Number = 90
01/07/2022 13:03:51 - INFO - __main__ -   Batch Number = 91
01/07/2022 13:03:51 - INFO - __main__ -   Batch Number = 92
Evaluating:  58%|    | 92/159 [00:02<00:01, 42.70it/s]01/07/2022 13:03:51 - INFO - __main__ -   Batch Number = 93
01/07/2022 13:03:51 - INFO - __main__ -   Batch Number = 94
01/07/2022 13:03:52 - INFO - __main__ -   Batch Number = 95
01/07/2022 13:03:52 - INFO - __main__ -   Batch Number = 96
01/07/2022 13:03:52 - INFO - __main__ -   Batch Number = 97
Evaluating:  61%|    | 97/159 [00:02<00:01, 43.34it/s]01/07/2022 13:03:52 - INFO - __main__ -   Batch Number = 98
01/07/2022 13:03:52 - INFO - __main__ -   Batch Number = 99
01/07/2022 13:03:52 - INFO - __main__ -   Batch Number = 100
01/07/2022 13:03:52 - INFO - __main__ -   Batch Number = 101
01/07/2022 13:03:52 - INFO - __main__ -   Batch Number = 102
Evaluating:  64%|   | 102/159 [00:02<00:01, 43.73it/s]01/07/2022 13:03:52 - INFO - __main__ -   Batch Number = 103
01/07/2022 13:03:52 - INFO - __main__ -   Batch Number = 104
01/07/2022 13:03:52 - INFO - __main__ -   Batch Number = 105
01/07/2022 13:03:52 - INFO - __main__ -   Batch Number = 106
01/07/2022 13:03:52 - INFO - __main__ -   Batch Number = 107
Evaluating:  67%|   | 107/159 [00:02<00:01, 43.90it/s]01/07/2022 13:03:52 - INFO - __main__ -   Batch Number = 108
01/07/2022 13:03:52 - INFO - __main__ -   Batch Number = 109
01/07/2022 13:03:52 - INFO - __main__ -   Batch Number = 110
01/07/2022 13:03:52 - INFO - __main__ -   Batch Number = 111
01/07/2022 13:03:52 - INFO - __main__ -   Batch Number = 112
Evaluating:  70%|   | 112/159 [00:02<00:01, 44.20it/s]01/07/2022 13:03:52 - INFO - __main__ -   Batch Number = 113
01/07/2022 13:03:52 - INFO - __main__ -   Batch Number = 114
01/07/2022 13:03:52 - INFO - __main__ -   Batch Number = 115
01/07/2022 13:03:52 - INFO - __main__ -   Batch Number = 116
01/07/2022 13:03:52 - INFO - __main__ -   Batch Number = 117
Evaluating:  74%|  | 117/159 [00:02<00:00, 44.39it/s]01/07/2022 13:03:52 - INFO - __main__ -   Batch Number = 118
01/07/2022 13:03:52 - INFO - __main__ -   Batch Number = 119
01/07/2022 13:03:52 - INFO - __main__ -   Batch Number = 120
01/07/2022 13:03:52 - INFO - __main__ -   Batch Number = 121
01/07/2022 13:03:52 - INFO - __main__ -   Batch Number = 122
Evaluating:  77%|  | 122/159 [00:02<00:00, 44.43it/s]01/07/2022 13:03:52 - INFO - __main__ -   Batch Number = 123
01/07/2022 13:03:52 - INFO - __main__ -   Batch Number = 124
01/07/2022 13:03:52 - INFO - __main__ -   Batch Number = 125
01/07/2022 13:03:52 - INFO - __main__ -   Batch Number = 126
01/07/2022 13:03:52 - INFO - __main__ -   Batch Number = 127
Evaluating:  80%|  | 127/159 [00:03<00:00, 44.39it/s]01/07/2022 13:03:52 - INFO - __main__ -   Batch Number = 128
01/07/2022 13:03:52 - INFO - __main__ -   Batch Number = 129
01/07/2022 13:03:52 - INFO - __main__ -   Batch Number = 130
01/07/2022 13:03:52 - INFO - __main__ -   Batch Number = 131
01/07/2022 13:03:52 - INFO - __main__ -   Batch Number = 132
Evaluating:  83%| | 132/159 [00:03<00:00, 44.57it/s]01/07/2022 13:03:52 - INFO - __main__ -   Batch Number = 133
01/07/2022 13:03:52 - INFO - __main__ -   Batch Number = 134
01/07/2022 13:03:52 - INFO - __main__ -   Batch Number = 135
01/07/2022 13:03:52 - INFO - __main__ -   Batch Number = 136
01/07/2022 13:03:52 - INFO - __main__ -   Batch Number = 137
Evaluating:  86%| | 137/159 [00:03<00:00, 44.51it/s]01/07/2022 13:03:52 - INFO - __main__ -   Batch Number = 138
01/07/2022 13:03:52 - INFO - __main__ -   Batch Number = 139
01/07/2022 13:03:53 - INFO - __main__ -   Batch Number = 140
01/07/2022 13:03:53 - INFO - __main__ -   Batch Number = 141
01/07/2022 13:03:53 - INFO - __main__ -   Batch Number = 142
Evaluating:  89%| | 142/159 [00:03<00:00, 44.70it/s]01/07/2022 13:03:53 - INFO - __main__ -   Batch Number = 143
01/07/2022 13:03:53 - INFO - __main__ -   Batch Number = 144
01/07/2022 13:03:53 - INFO - __main__ -   Batch Number = 145
01/07/2022 13:03:53 - INFO - __main__ -   Batch Number = 146
01/07/2022 13:03:53 - INFO - __main__ -   Batch Number = 147
Evaluating:  92%|| 147/159 [00:03<00:00, 44.66it/s]01/07/2022 13:03:53 - INFO - __main__ -   Batch Number = 148
01/07/2022 13:03:53 - INFO - __main__ -   Batch Number = 149
01/07/2022 13:03:53 - INFO - __main__ -   Batch Number = 150
01/07/2022 13:03:53 - INFO - __main__ -   Batch Number = 151
01/07/2022 13:03:53 - INFO - __main__ -   Batch Number = 152
Evaluating:  96%|| 152/159 [00:03<00:00, 44.64it/s]01/07/2022 13:03:53 - INFO - __main__ -   Batch Number = 153
01/07/2022 13:03:53 - INFO - __main__ -   Batch Number = 154
01/07/2022 13:03:53 - INFO - __main__ -   Batch Number = 155
01/07/2022 13:03:53 - INFO - __main__ -   Batch Number = 156
01/07/2022 13:03:53 - INFO - __main__ -   Batch Number = 157
Evaluating:  99%|| 157/159 [00:03<00:00, 44.62it/s]01/07/2022 13:03:53 - INFO - __main__ -   Batch Number = 158
01/07/2022 13:03:53 - INFO - __main__ -   Batch Number = 159
Evaluating: 100%|| 159/159 [00:03<00:00, 42.21it/s]
01/07/2022 13:03:53 - INFO - __main__ -     Evaluation done in total 3.767300 secs (0.002966 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/predictions_en_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/nbest_predictions_en_.json
01/07/2022 13:03:56 - INFO - __main__ -   Results = OrderedDict([('exact', 72.18487394957984), ('f1', 83.57563939574104), ('total', 1190), ('HasAns_exact', 72.18487394957984), ('HasAns_f1', 83.57563939574104), ('HasAns_total', 1190), ('best_exact', 72.18487394957984), ('best_exact_thresh', 0.0), ('best_f1', 83.57563939574104), ('best_f1_thresh', 0.0)])
PyTorch version 1.10.0+cu111 available.
01/07/2022 13:03:58 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/07/2022 13:03:58 - INFO - root -   save model
loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /root/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6
Model config XLMRobertaConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /root/.cache/torch/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8
loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/07/2022 13:04:08 - INFO - __main__ -   lang2id = None
01/07/2022 13:04:11 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='xlm-roberta-base', model_type='xlm-roberta', config_name=None, tokenizer_name=None, cache_dir=None, data_dir='/root/Desktop/cloud-emea-copy/data//xquad', output_dir='/root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/', max_seq_length=384, train_file='/root/Desktop/cloud-emea-copy/data//xquad/train-v1.1.json', predict_file='/root/Desktop/cloud-emea-copy/data//xquad/dev-v1.1.json', eval_test_set=False, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=4, per_gpu_eval_batch_size=8, gradient_accumulation_steps=4, learning_rate=0.0001, weight_decay=0.0001, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=15.0, max_steps=-1, save_steps=1000, warmup_steps=500, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', eval_lang='en', predict_langs='en', train_lang='en', log_file='/root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='qna', threads=8, version_2_with_negative=False, verbose_logging=False, n_best_size=20, max_query_length=64, max_answer_length=30, doc_stride=128, null_score_diff_threshold=0.0, predict_task_adapter='output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s3/checkpoint-best/qna', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix=None, en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/07/2022 13:04:11 - INFO - __main__ -   Results: {}
loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /root/.cache/torch/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6
Model config XLMRobertaConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 250002
}

loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /root/.cache/torch/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8
loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/07/2022 13:04:21 - INFO - __main__ -   lang2id = None
01/07/2022 13:04:21 - INFO - __main__ -   Evaluating the model on the test set of all languages specified
01/07/2022 13:04:21 - INFO - __main__ -   Adapter will be loaded from this path: output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s3/checkpoint-best/qna
01/07/2022 13:04:21 - INFO - root -   Trying to decide if add adapter
01/07/2022 13:04:21 - INFO - root -   loading task adapter
Loading module configuration from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s3/checkpoint-best/qna/adapter_config.json
Adding adapter 'qna' of type 'text_task'.
Loading module weights from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s3/checkpoint-best/qna/pytorch_adapter.bin
Loading module configuration from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s3/checkpoint-best/qna/head_config.json
Loading module weights from output/squad/my_xlm-roberta-base_LR1e-4_EPOCH15_maxlen384_batchsize4_gradacc4_s3/checkpoint-best/qna/pytorch_model_head.bin
01/07/2022 13:04:21 - INFO - root -   loading lang adpater en/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/xlm-roberta-base/pfeiffer/en_relu_2.zip.
Loading module configuration from /root/.cache/torch/adapters/3e9c8c800af886cd1c684a3acf4929990b35f3ac75e050e942de2ff2ecf895fe-caabba7772816577124631d5b84810c4d32f49afcea4fca637162ec7f432174f-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
Loading module weights from /root/.cache/torch/adapters/3e9c8c800af886cd1c684a3acf4929990b35f3ac75e050e942de2ff2ecf895fe-caabba7772816577124631d5b84810c4d32f49afcea4fca637162ec7f432174f-extracted/pytorch_adapter.bin
No matching prediction head found in '/root/.cache/torch/adapters/3e9c8c800af886cd1c684a3acf4929990b35f3ac75e050e942de2ff2ecf895fe-caabba7772816577124631d5b84810c4d32f49afcea4fca637162ec7f432174f-extracted'
01/07/2022 13:04:23 - INFO - __main__ -   Language adapter for en found
01/07/2022 13:04:23 - INFO - __main__ -   Set active language adapter to en
01/07/2022 13:04:23 - INFO - __main__ -   Args Adapter Weight = None
01/07/2022 13:04:23 - INFO - __main__ -   Adapter Languages = ['en']
01/07/2022 13:04:23 - INFO - __main__ -   Predict File = xquad.en.json
01/07/2022 13:04:23 - INFO - __main__ -   Creating features from dataset file at /root/Desktop/cloud-emea-copy/data//xquad
en en/wiki@ukp
  0%|          | 0/48 [00:00<?, ?it/s] 33%|      | 16/48 [00:00<00:00, 132.47it/s] 62%|   | 30/48 [00:00<00:00, 124.46it/s] 94%|| 45/48 [00:00<00:00, 132.05it/s]100%|| 48/48 [00:00<00:00, 133.22it/s]
convert squad examples to features:   0%|          | 0/1190 [00:00<?, ?it/s]convert squad examples to features:   0%|          | 1/1190 [00:00<03:16,  6.06it/s]convert squad examples to features:  30%|       | 353/1190 [00:00<00:00, 1205.41it/s]convert squad examples to features:  40%|      | 471/1190 [00:00<00:00, 887.18it/s] convert squad examples to features:  94%|| 1121/1190 [00:00<00:00, 2034.47it/s]convert squad examples to features: 100%|| 1190/1190 [00:00<00:00, 1657.30it/s]/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:1421: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.
  warnings.warn(
/root/Desktop/cloud-emea-copy/src/transformers/tokenization_utils_base.py:2016: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

add example index and unique id:   0%|          | 0/1190 [00:00<?, ?it/s]add example index and unique id: 100%|| 1190/1190 [00:00<00:00, 730993.23it/s]
01/07/2022 13:04:25 - INFO - __main__ -   Local Rank = -1
01/07/2022 13:04:25 - INFO - __main__ -   Saving features into cached file /root/Desktop/cloud-emea-copy/data//xquad/cached_xquad.en.json_xlm-roberta-base_384_en
01/07/2022 13:04:26 - INFO - __main__ -   ***** Running evaluation  *****
01/07/2022 13:04:26 - INFO - __main__ -     Num examples = 1270
01/07/2022 13:04:26 - INFO - __main__ -     Batch size = 8
Evaluating:   0%|          | 0/159 [00:00<?, ?it/s]01/07/2022 13:04:26 - INFO - __main__ -   Batch Number = 1
01/07/2022 13:04:26 - INFO - __main__ -   Batch Number = 2
01/07/2022 13:04:26 - INFO - __main__ -   Batch Number = 3
Evaluating:   2%|         | 3/159 [00:00<00:05, 26.54it/s]01/07/2022 13:04:26 - INFO - __main__ -   Batch Number = 4
01/07/2022 13:04:26 - INFO - __main__ -   Batch Number = 5
01/07/2022 13:04:26 - INFO - __main__ -   Batch Number = 6
01/07/2022 13:04:26 - INFO - __main__ -   Batch Number = 7
Evaluating:   4%|         | 7/159 [00:00<00:04, 32.49it/s]01/07/2022 13:04:26 - INFO - __main__ -   Batch Number = 8
01/07/2022 13:04:26 - INFO - __main__ -   Batch Number = 9
01/07/2022 13:04:26 - INFO - __main__ -   Batch Number = 10
01/07/2022 13:04:26 - INFO - __main__ -   Batch Number = 11
01/07/2022 13:04:26 - INFO - __main__ -   Batch Number = 12
Evaluating:   8%|         | 12/159 [00:00<00:03, 37.14it/s]01/07/2022 13:04:26 - INFO - __main__ -   Batch Number = 13
01/07/2022 13:04:26 - INFO - __main__ -   Batch Number = 14
01/07/2022 13:04:26 - INFO - __main__ -   Batch Number = 15
01/07/2022 13:04:26 - INFO - __main__ -   Batch Number = 16
01/07/2022 13:04:26 - INFO - __main__ -   Batch Number = 17
Evaluating:  11%|         | 17/159 [00:00<00:03, 39.60it/s]01/07/2022 13:04:26 - INFO - __main__ -   Batch Number = 18
01/07/2022 13:04:26 - INFO - __main__ -   Batch Number = 19
01/07/2022 13:04:26 - INFO - __main__ -   Batch Number = 20
01/07/2022 13:04:26 - INFO - __main__ -   Batch Number = 21
01/07/2022 13:04:26 - INFO - __main__ -   Batch Number = 22
Evaluating:  14%|        | 22/159 [00:00<00:03, 41.22it/s]01/07/2022 13:04:26 - INFO - __main__ -   Batch Number = 23
01/07/2022 13:04:26 - INFO - __main__ -   Batch Number = 24
01/07/2022 13:04:26 - INFO - __main__ -   Batch Number = 25
01/07/2022 13:04:26 - INFO - __main__ -   Batch Number = 26
01/07/2022 13:04:26 - INFO - __main__ -   Batch Number = 27
Evaluating:  17%|        | 27/159 [00:00<00:03, 42.22it/s]01/07/2022 13:04:27 - INFO - __main__ -   Batch Number = 28
01/07/2022 13:04:27 - INFO - __main__ -   Batch Number = 29
01/07/2022 13:04:27 - INFO - __main__ -   Batch Number = 30
01/07/2022 13:04:27 - INFO - __main__ -   Batch Number = 31
01/07/2022 13:04:27 - INFO - __main__ -   Batch Number = 32
Evaluating:  20%|        | 32/159 [00:00<00:02, 42.60it/s]01/07/2022 13:04:27 - INFO - __main__ -   Batch Number = 33
01/07/2022 13:04:27 - INFO - __main__ -   Batch Number = 34
01/07/2022 13:04:27 - INFO - __main__ -   Batch Number = 35
01/07/2022 13:04:27 - INFO - __main__ -   Batch Number = 36
01/07/2022 13:04:27 - INFO - __main__ -   Batch Number = 37
Evaluating:  23%|       | 37/159 [00:00<00:02, 43.03it/s]01/07/2022 13:04:27 - INFO - __main__ -   Batch Number = 38
01/07/2022 13:04:27 - INFO - __main__ -   Batch Number = 39
01/07/2022 13:04:27 - INFO - __main__ -   Batch Number = 40
01/07/2022 13:04:27 - INFO - __main__ -   Batch Number = 41
01/07/2022 13:04:27 - INFO - __main__ -   Batch Number = 42
Evaluating:  26%|       | 42/159 [00:01<00:02, 43.36it/s]01/07/2022 13:04:27 - INFO - __main__ -   Batch Number = 43
01/07/2022 13:04:27 - INFO - __main__ -   Batch Number = 44
01/07/2022 13:04:27 - INFO - __main__ -   Batch Number = 45
01/07/2022 13:04:27 - INFO - __main__ -   Batch Number = 46
01/07/2022 13:04:27 - INFO - __main__ -   Batch Number = 47
Evaluating:  30%|       | 47/159 [00:01<00:02, 41.73it/s]01/07/2022 13:04:27 - INFO - __main__ -   Batch Number = 48
01/07/2022 13:04:27 - INFO - __main__ -   Batch Number = 49
01/07/2022 13:04:27 - INFO - __main__ -   Batch Number = 50
01/07/2022 13:04:27 - INFO - __main__ -   Batch Number = 51
01/07/2022 13:04:27 - INFO - __main__ -   Batch Number = 52
Evaluating:  33%|      | 52/159 [00:01<00:02, 42.50it/s]01/07/2022 13:04:27 - INFO - __main__ -   Batch Number = 53
01/07/2022 13:04:27 - INFO - __main__ -   Batch Number = 54
01/07/2022 13:04:27 - INFO - __main__ -   Batch Number = 55
01/07/2022 13:04:27 - INFO - __main__ -   Batch Number = 56
01/07/2022 13:04:27 - INFO - __main__ -   Batch Number = 57
Evaluating:  36%|      | 57/159 [00:01<00:02, 42.96it/s]01/07/2022 13:04:27 - INFO - __main__ -   Batch Number = 58
01/07/2022 13:04:27 - INFO - __main__ -   Batch Number = 59
01/07/2022 13:04:27 - INFO - __main__ -   Batch Number = 60
01/07/2022 13:04:27 - INFO - __main__ -   Batch Number = 61
01/07/2022 13:04:27 - INFO - __main__ -   Batch Number = 62
Evaluating:  39%|      | 62/159 [00:01<00:03, 30.95it/s]01/07/2022 13:04:27 - INFO - __main__ -   Batch Number = 63
01/07/2022 13:04:28 - INFO - __main__ -   Batch Number = 64
01/07/2022 13:04:28 - INFO - __main__ -   Batch Number = 65
01/07/2022 13:04:28 - INFO - __main__ -   Batch Number = 66
01/07/2022 13:04:28 - INFO - __main__ -   Batch Number = 67
Evaluating:  42%|     | 67/159 [00:01<00:02, 34.05it/s]01/07/2022 13:04:28 - INFO - __main__ -   Batch Number = 68
01/07/2022 13:04:28 - INFO - __main__ -   Batch Number = 69
01/07/2022 13:04:28 - INFO - __main__ -   Batch Number = 70
01/07/2022 13:04:28 - INFO - __main__ -   Batch Number = 71
01/07/2022 13:04:28 - INFO - __main__ -   Batch Number = 72
Evaluating:  45%|     | 72/159 [00:01<00:02, 36.35it/s]01/07/2022 13:04:28 - INFO - __main__ -   Batch Number = 73
01/07/2022 13:04:28 - INFO - __main__ -   Batch Number = 74
01/07/2022 13:04:28 - INFO - __main__ -   Batch Number = 75
01/07/2022 13:04:28 - INFO - __main__ -   Batch Number = 76
01/07/2022 13:04:28 - INFO - __main__ -   Batch Number = 77
Evaluating:  48%|     | 77/159 [00:01<00:02, 37.96it/s]01/07/2022 13:04:28 - INFO - __main__ -   Batch Number = 78
01/07/2022 13:04:28 - INFO - __main__ -   Batch Number = 79
01/07/2022 13:04:28 - INFO - __main__ -   Batch Number = 80
01/07/2022 13:04:28 - INFO - __main__ -   Batch Number = 81
01/07/2022 13:04:28 - INFO - __main__ -   Batch Number = 82
Evaluating:  52%|    | 82/159 [00:02<00:01, 39.13it/s]01/07/2022 13:04:28 - INFO - __main__ -   Batch Number = 83
01/07/2022 13:04:28 - INFO - __main__ -   Batch Number = 84
01/07/2022 13:04:28 - INFO - __main__ -   Batch Number = 85
01/07/2022 13:04:28 - INFO - __main__ -   Batch Number = 86
01/07/2022 13:04:28 - INFO - __main__ -   Batch Number = 87
Evaluating:  55%|    | 87/159 [00:02<00:01, 39.97it/s]01/07/2022 13:04:28 - INFO - __main__ -   Batch Number = 88
01/07/2022 13:04:28 - INFO - __main__ -   Batch Number = 89
01/07/2022 13:04:28 - INFO - __main__ -   Batch Number = 90
01/07/2022 13:04:28 - INFO - __main__ -   Batch Number = 91
01/07/2022 13:04:28 - INFO - __main__ -   Batch Number = 92
Evaluating:  58%|    | 92/159 [00:02<00:01, 40.40it/s]01/07/2022 13:04:28 - INFO - __main__ -   Batch Number = 93
01/07/2022 13:04:28 - INFO - __main__ -   Batch Number = 94
01/07/2022 13:04:28 - INFO - __main__ -   Batch Number = 95
01/07/2022 13:04:28 - INFO - __main__ -   Batch Number = 96
01/07/2022 13:04:28 - INFO - __main__ -   Batch Number = 97
Evaluating:  61%|    | 97/159 [00:02<00:01, 40.78it/s]01/07/2022 13:04:28 - INFO - __main__ -   Batch Number = 98
01/07/2022 13:04:28 - INFO - __main__ -   Batch Number = 99
01/07/2022 13:04:28 - INFO - __main__ -   Batch Number = 100
01/07/2022 13:04:28 - INFO - __main__ -   Batch Number = 101
01/07/2022 13:04:28 - INFO - __main__ -   Batch Number = 102
Evaluating:  64%|   | 102/159 [00:02<00:01, 40.79it/s]01/07/2022 13:04:28 - INFO - __main__ -   Batch Number = 103
01/07/2022 13:04:28 - INFO - __main__ -   Batch Number = 104
01/07/2022 13:04:29 - INFO - __main__ -   Batch Number = 105
01/07/2022 13:04:29 - INFO - __main__ -   Batch Number = 106
01/07/2022 13:04:29 - INFO - __main__ -   Batch Number = 107
Evaluating:  67%|   | 107/159 [00:02<00:01, 36.18it/s]01/07/2022 13:04:29 - INFO - __main__ -   Batch Number = 108
01/07/2022 13:04:29 - INFO - __main__ -   Batch Number = 109
01/07/2022 13:04:29 - INFO - __main__ -   Batch Number = 110
01/07/2022 13:04:29 - INFO - __main__ -   Batch Number = 111
01/07/2022 13:04:29 - INFO - __main__ -   Batch Number = 112
Evaluating:  70%|   | 112/159 [00:02<00:01, 38.35it/s]01/07/2022 13:04:29 - INFO - __main__ -   Batch Number = 113
01/07/2022 13:04:29 - INFO - __main__ -   Batch Number = 114
01/07/2022 13:04:29 - INFO - __main__ -   Batch Number = 115
01/07/2022 13:04:29 - INFO - __main__ -   Batch Number = 116
01/07/2022 13:04:29 - INFO - __main__ -   Batch Number = 117
Evaluating:  74%|  | 117/159 [00:02<00:01, 40.19it/s]01/07/2022 13:04:29 - INFO - __main__ -   Batch Number = 118
01/07/2022 13:04:29 - INFO - __main__ -   Batch Number = 119
01/07/2022 13:04:29 - INFO - __main__ -   Batch Number = 120
01/07/2022 13:04:29 - INFO - __main__ -   Batch Number = 121
01/07/2022 13:04:29 - INFO - __main__ -   Batch Number = 122
Evaluating:  77%|  | 122/159 [00:03<00:00, 41.37it/s]01/07/2022 13:04:29 - INFO - __main__ -   Batch Number = 123
01/07/2022 13:04:29 - INFO - __main__ -   Batch Number = 124
01/07/2022 13:04:29 - INFO - __main__ -   Batch Number = 125
01/07/2022 13:04:29 - INFO - __main__ -   Batch Number = 126
01/07/2022 13:04:29 - INFO - __main__ -   Batch Number = 127
Evaluating:  80%|  | 127/159 [00:03<00:00, 42.04it/s]01/07/2022 13:04:29 - INFO - __main__ -   Batch Number = 128
01/07/2022 13:04:29 - INFO - __main__ -   Batch Number = 129
01/07/2022 13:04:29 - INFO - __main__ -   Batch Number = 130
01/07/2022 13:04:29 - INFO - __main__ -   Batch Number = 131
01/07/2022 13:04:29 - INFO - __main__ -   Batch Number = 132
Evaluating:  83%| | 132/159 [00:03<00:00, 42.92it/s]01/07/2022 13:04:29 - INFO - __main__ -   Batch Number = 133
01/07/2022 13:04:29 - INFO - __main__ -   Batch Number = 134
01/07/2022 13:04:29 - INFO - __main__ -   Batch Number = 135
01/07/2022 13:04:29 - INFO - __main__ -   Batch Number = 136
01/07/2022 13:04:29 - INFO - __main__ -   Batch Number = 137
Evaluating:  86%| | 137/159 [00:03<00:00, 33.55it/s]01/07/2022 13:04:29 - INFO - __main__ -   Batch Number = 138
01/07/2022 13:04:29 - INFO - __main__ -   Batch Number = 139
01/07/2022 13:04:29 - INFO - __main__ -   Batch Number = 140
01/07/2022 13:04:29 - INFO - __main__ -   Batch Number = 141
01/07/2022 13:04:29 - INFO - __main__ -   Batch Number = 142
Evaluating:  89%| | 142/159 [00:03<00:00, 36.21it/s]01/07/2022 13:04:29 - INFO - __main__ -   Batch Number = 143
01/07/2022 13:04:30 - INFO - __main__ -   Batch Number = 144
01/07/2022 13:04:30 - INFO - __main__ -   Batch Number = 145
01/07/2022 13:04:30 - INFO - __main__ -   Batch Number = 146
01/07/2022 13:04:30 - INFO - __main__ -   Batch Number = 147
Evaluating:  92%|| 147/159 [00:03<00:00, 38.52it/s]01/07/2022 13:04:30 - INFO - __main__ -   Batch Number = 148
01/07/2022 13:04:30 - INFO - __main__ -   Batch Number = 149
01/07/2022 13:04:30 - INFO - __main__ -   Batch Number = 150
01/07/2022 13:04:30 - INFO - __main__ -   Batch Number = 151
01/07/2022 13:04:30 - INFO - __main__ -   Batch Number = 152
Evaluating:  96%|| 152/159 [00:03<00:00, 40.12it/s]01/07/2022 13:04:30 - INFO - __main__ -   Batch Number = 153
01/07/2022 13:04:30 - INFO - __main__ -   Batch Number = 154
01/07/2022 13:04:30 - INFO - __main__ -   Batch Number = 155
01/07/2022 13:04:30 - INFO - __main__ -   Batch Number = 156
01/07/2022 13:04:30 - INFO - __main__ -   Batch Number = 157
Evaluating:  99%|| 157/159 [00:04<00:00, 41.45it/s]01/07/2022 13:04:30 - INFO - __main__ -   Batch Number = 158
01/07/2022 13:04:30 - INFO - __main__ -   Batch Number = 159
Evaluating: 100%|| 159/159 [00:04<00:00, 39.36it/s]
01/07/2022 13:04:30 - INFO - __main__ -     Evaluation done in total 4.040130 secs (0.003181 sec per example)
Writing predictions to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/predictions_en_.json
Writing nbest to: /root/Desktop/cloud-emea-copy/outputs//squad/my_xlm-roberta-base_maxlen384_qna_en/nbest_predictions_en_.json
01/07/2022 13:04:33 - INFO - __main__ -   Results = OrderedDict([('exact', 72.3529411764706), ('f1', 83.2098296965675), ('total', 1190), ('HasAns_exact', 72.3529411764706), ('HasAns_f1', 83.2098296965675), ('HasAns_total', 1190), ('best_exact', 72.3529411764706), ('best_exact_thresh', 0.0), ('best_f1', 83.2098296965675), ('best_f1_thresh', 0.0)])
