PyTorch version 1.9.0+cu102 available.
11/21/2021 11:10:15 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ar', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/21/2021 11:10:15 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/21/2021 11:10:15 - INFO - __main__ -   Seed = 1
11/21/2021 11:10:15 - INFO - root -   save model
11/21/2021 11:10:15 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ar', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/21/2021 11:10:15 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/21/2021 11:10:17 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/21/2021 11:10:23 - INFO - __main__ -   Using lang2id = None
11/21/2021 11:10:23 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/21/2021 11:10:23 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
11/21/2021 11:10:23 - INFO - root -   Trying to decide if add adapter
11/21/2021 11:10:23 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_model_head.bin
11/21/2021 11:10:23 - INFO - root -   loading lang adpater am/wiki@ukp
11/21/2021 11:10:23 - INFO - __main__ -   Adapter Languages : ['am'], Length : 1
11/21/2021 11:10:23 - INFO - __main__ -   Adapter Names ['am/wiki@ukp'], Length : 1
11/21/2021 11:10:23 - INFO - __main__ -   Language = am
11/21/2021 11:10:23 - INFO - __main__ -   Adapter Name = am/wiki@ukp
Found matching adapter at: adapters/ukp/bert-base-multilingual-cased_am_wiki_pfeiffer.json
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/am/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_am_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/adapter_config.json
Adding adapter 'am' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/head_config.json
11/21/2021 11:10:28 - INFO - __main__ -   Language adapter for ar not found, using am instead
11/21/2021 11:10:28 - INFO - __main__ -   Set active language adapter to am
11/21/2021 11:10:28 - INFO - __main__ -   Args Adapter Weight = None
11/21/2021 11:10:28 - INFO - __main__ -   Adapter Languages = ['am']
11/21/2021 11:10:28 - INFO - __main__ -   all languages = ar
11/21/2021 11:10:28 - INFO - __main__ -   Creating features from dataset file at /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/ar/test.bert-base-multilingual-cased in language ar
11/21/2021 11:10:28 - INFO - utils_tag -   lang_id=0, lang=ar, lang2id=None
11/21/2021 11:10:28 - INFO - utils_tag -   Writing example 0 of 1784
11/21/2021 11:10:28 - INFO - utils_tag -   *** Example ***
11/21/2021 11:10:28 - INFO - utils_tag -   guid: ar-1
11/21/2021 11:10:28 - INFO - utils_tag -   tokens: [CLS] سوريا : تعد ##يل و ##زار ##ي واسع ي ##شم ##ل 8 حق ##ا ##ئب [SEP]
11/21/2021 11:10:28 - INFO - utils_tag -   input_ids: 101 61829 131 83160 15951 791 34648 10461 101771 793 71604 10961 129 46540 10429 52054 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/21/2021 11:10:28 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/21/2021 11:10:28 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/21/2021 11:10:28 - INFO - utils_tag -   label_ids: -100 17 13 8 -100 1 -100 -100 1 16 -100 -100 9 8 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
11/21/2021 11:10:28 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
11/21/2021 11:10:28 - INFO - utils_tag -   *** Example ***
11/21/2021 11:10:28 - INFO - utils_tag -   guid: ar-2
11/21/2021 11:10:28 - INFO - utils_tag -   tokens: [CLS] دمشق ( و ##كا ##لات ال ##ان ##باء ) - ا ##جر ##ى الرئيس السوري ب ##شار ال ##اس ##د تعد ##يل ##ا ح ##كو ##مي ##اً واسع ##ا تم ب ##مو ##جب ##ه إ ##قالة وزير ##ي الداخلية والا ##علام عن منصب ##يها في حين ظل محمد ن ##اج ##ي ال ##ع ##طر ##ي رئيس ##اً ل ##لح ##كو ##مة . [SEP]
11/21/2021 11:10:28 - INFO - utils_tag -   input_ids: 101 54435 113 791 37423 40475 59901 10765 58267 114 118 763 24618 11832 43012 107744 764 48607 59901 15995 10658 83160 15951 10429 769 39494 28474 11870 101771 10429 14500 764 54037 50576 10388 761 106946 58658 10461 69264 60424 89754 11749 56741 90391 10210 29735 104846 13247 789 24728 10461 59901 11693 48227 10461 26880 11870 787 87536 39494 16526 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/21/2021 11:10:28 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/21/2021 11:10:28 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/21/2021 11:10:28 - INFO - utils_tag -   label_ids: -100 17 13 8 -100 -100 8 -100 -100 13 13 16 -100 -100 8 1 17 -100 17 -100 -100 8 -100 -100 1 -100 -100 -100 1 -100 16 11 -100 -100 -100 8 -100 8 -100 1 8 -100 2 8 -100 2 2 16 17 17 -100 -100 17 -100 -100 -100 8 -100 8 -100 -100 -100 13 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
11/21/2021 11:10:28 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
11/21/2021 11:10:28 - INFO - utils_tag -   *** Example ***
11/21/2021 11:10:28 - INFO - utils_tag -   guid: ar-3
11/21/2021 11:10:28 - INFO - utils_tag -   tokens: [CLS] و ##اض ##افت ال ##م ##صادر ان مه ##دي دخل الله رئيس ت ##حرير صحيفة الحزب ال ##حاكم و ##ال ##ليب ##رال ##ي ال ##توجه ##ات ت ##سل ##م منصب وزير ال ##اع ##لام خ ##لف ##ا لا ##حمد الحسن فيما ت ##سل ##م ال ##لو ##اء غ ##از ##ي ك ##نع ##ان رئيس ش ##عبة ال ##ام ##ن السياسي منصب وزير الداخلية . [SEP]
11/21/2021 11:10:28 - INFO - utils_tag -   input_ids: 101 791 34201 109536 59901 10700 60401 14269 77010 18914 32480 15764 26880 766 98157 87446 57808 59901 89214 791 13154 105312 89364 10461 59901 109179 10564 766 32219 10700 56741 58658 59901 21337 36334 770 42472 10429 13879 63151 78959 37792 766 32219 10700 59901 29426 12700 782 20688 10461 786 50643 10765 26880 776 94977 59901 13367 10582 79239 56741 58658 69264 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/21/2021 11:10:28 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/21/2021 11:10:28 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/21/2021 11:10:28 - INFO - utils_tag -   label_ids: -100 5 -100 -100 8 -100 -100 14 17 -100 17 17 8 8 -100 8 8 8 -100 1 -100 -100 -100 -100 8 -100 -100 16 -100 -100 8 8 8 -100 -100 8 -100 -100 8 -100 17 5 16 -100 -100 8 -100 -100 17 -100 -100 17 -100 -100 8 8 -100 8 -100 -100 1 8 8 1 13 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
11/21/2021 11:10:28 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
11/21/2021 11:10:28 - INFO - utils_tag -   *** Example ***
11/21/2021 11:10:28 - INFO - utils_tag -   guid: ar-4
11/21/2021 11:10:28 - INFO - utils_tag -   tokens: [CLS] و ##ذكر ##ت و ##كالة ال ##ان ##باء السورية ان ال ##ت ##عد ##يل ش ##مل ثم ##اني حق ##ا ##ئب بين ##ها وزارت ##ا الداخلية والا ##قت ##صاد . [SEP]
11/21/2021 11:10:28 - INFO - utils_tag -   input_ids: 101 791 42172 10502 791 97311 59901 10765 58267 88316 14269 59901 10502 34575 15951 776 33061 15902 25706 46540 10429 52054 12827 10742 54179 10429 69264 60424 39053 68595 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/21/2021 11:10:28 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/21/2021 11:10:28 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/21/2021 11:10:28 - INFO - utils_tag -   label_ids: -100 5 -100 -100 8 -100 8 -100 -100 1 14 8 -100 -100 -100 16 -100 9 -100 8 -100 -100 11 -100 8 -100 1 8 -100 -100 13 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
11/21/2021 11:10:28 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
11/21/2021 11:10:28 - INFO - utils_tag -   *** Example ***
11/21/2021 11:10:28 - INFO - utils_tag -   guid: ar-5
11/21/2021 11:10:28 - INFO - utils_tag -   tokens: [CLS] و ##عين ال ##لو ##اء ك ##نع ##ان الذي كان رئيس ##ا ل ##جه ##از ال ##ام ##ن السياسي وزير ##ا ل ##لد ##اخ ##لية خ ##لف ##ا ل ##لو ##اء علي ح ##مو ##د . [SEP]
11/21/2021 11:10:28 - INFO - utils_tag -   input_ids: 101 791 34163 59901 29426 12700 786 50643 10765 13121 12326 26880 10429 787 28437 20688 59901 13367 10582 79239 58658 10429 787 48649 66085 29985 770 42472 10429 787 29426 12700 19163 769 54037 10658 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/21/2021 11:10:28 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/21/2021 11:10:28 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/21/2021 11:10:28 - INFO - utils_tag -   label_ids: -100 5 -100 17 -100 -100 17 -100 -100 17 4 8 -100 8 -100 -100 8 -100 -100 1 8 -100 1 -100 -100 -100 8 -100 -100 8 -100 -100 17 17 -100 -100 13 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
11/21/2021 11:10:28 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
11/21/2021 11:10:30 - INFO - __main__ -   Saving features into cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_ar_bert-base-multilingual-cased_128, len(features)=1784
11/21/2021 11:10:30 - INFO - __main__ -   ***** Running evaluation  in ar *****
11/21/2021 11:10:30 - INFO - __main__ -     Num examples = 1784
11/21/2021 11:10:30 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/56 [00:00<?, ?it/s]11/21/2021 11:10:30 - INFO - __main__ -   Batch number = 1
Evaluating:   2%|▏         | 1/56 [00:00<00:07,  7.04it/s]11/21/2021 11:10:31 - INFO - __main__ -   Batch number = 2
Evaluating:   4%|▎         | 2/56 [00:00<00:07,  7.40it/s]11/21/2021 11:10:31 - INFO - __main__ -   Batch number = 3
Evaluating:   5%|▌         | 3/56 [00:00<00:06,  7.65it/s]11/21/2021 11:10:31 - INFO - __main__ -   Batch number = 4
Evaluating:   7%|▋         | 4/56 [00:00<00:06,  7.84it/s]11/21/2021 11:10:31 - INFO - __main__ -   Batch number = 5
Evaluating:   9%|▉         | 5/56 [00:00<00:06,  7.93it/s]11/21/2021 11:10:31 - INFO - __main__ -   Batch number = 6
Evaluating:  11%|█         | 6/56 [00:00<00:06,  7.98it/s]11/21/2021 11:10:31 - INFO - __main__ -   Batch number = 7
Evaluating:  12%|█▎        | 7/56 [00:00<00:06,  8.00it/s]11/21/2021 11:10:31 - INFO - __main__ -   Batch number = 8
Evaluating:  14%|█▍        | 8/56 [00:01<00:05,  8.01it/s]11/21/2021 11:10:31 - INFO - __main__ -   Batch number = 9
Evaluating:  16%|█▌        | 9/56 [00:01<00:05,  8.03it/s]11/21/2021 11:10:32 - INFO - __main__ -   Batch number = 10
Evaluating:  18%|█▊        | 10/56 [00:01<00:05,  8.03it/s]11/21/2021 11:10:32 - INFO - __main__ -   Batch number = 11
Evaluating:  20%|█▉        | 11/56 [00:01<00:05,  8.00it/s]11/21/2021 11:10:32 - INFO - __main__ -   Batch number = 12
Evaluating:  21%|██▏       | 12/56 [00:01<00:05,  8.02it/s]11/21/2021 11:10:32 - INFO - __main__ -   Batch number = 13
Evaluating:  23%|██▎       | 13/56 [00:01<00:05,  8.02it/s]11/21/2021 11:10:32 - INFO - __main__ -   Batch number = 14
Evaluating:  25%|██▌       | 14/56 [00:01<00:05,  8.02it/s]11/21/2021 11:10:32 - INFO - __main__ -   Batch number = 15
Evaluating:  27%|██▋       | 15/56 [00:01<00:05,  8.02it/s]11/21/2021 11:10:32 - INFO - __main__ -   Batch number = 16
Evaluating:  29%|██▊       | 16/56 [00:02<00:04,  8.02it/s]11/21/2021 11:10:32 - INFO - __main__ -   Batch number = 17
Evaluating:  30%|███       | 17/56 [00:02<00:04,  8.00it/s]11/21/2021 11:10:33 - INFO - __main__ -   Batch number = 18
Evaluating:  32%|███▏      | 18/56 [00:02<00:04,  8.01it/s]11/21/2021 11:10:33 - INFO - __main__ -   Batch number = 19
Evaluating:  34%|███▍      | 19/56 [00:02<00:05,  6.92it/s]11/21/2021 11:10:33 - INFO - __main__ -   Batch number = 20
Evaluating:  36%|███▌      | 20/56 [00:02<00:05,  7.20it/s]11/21/2021 11:10:33 - INFO - __main__ -   Batch number = 21
Evaluating:  38%|███▊      | 21/56 [00:02<00:04,  7.42it/s]11/21/2021 11:10:33 - INFO - __main__ -   Batch number = 22
Evaluating:  39%|███▉      | 22/56 [00:02<00:04,  7.59it/s]11/21/2021 11:10:33 - INFO - __main__ -   Batch number = 23
Evaluating:  41%|████      | 23/56 [00:02<00:04,  7.69it/s]11/21/2021 11:10:33 - INFO - __main__ -   Batch number = 24
Evaluating:  43%|████▎     | 24/56 [00:03<00:04,  7.77it/s]11/21/2021 11:10:33 - INFO - __main__ -   Batch number = 25
Evaluating:  45%|████▍     | 25/56 [00:03<00:03,  7.83it/s]11/21/2021 11:10:34 - INFO - __main__ -   Batch number = 26
Evaluating:  46%|████▋     | 26/56 [00:03<00:03,  7.85it/s]11/21/2021 11:10:34 - INFO - __main__ -   Batch number = 27
Evaluating:  48%|████▊     | 27/56 [00:03<00:03,  7.82it/s]11/21/2021 11:10:34 - INFO - __main__ -   Batch number = 28
Evaluating:  50%|█████     | 28/56 [00:03<00:03,  7.84it/s]11/21/2021 11:10:34 - INFO - __main__ -   Batch number = 29
Evaluating:  52%|█████▏    | 29/56 [00:03<00:03,  7.85it/s]11/21/2021 11:10:34 - INFO - __main__ -   Batch number = 30
Evaluating:  54%|█████▎    | 30/56 [00:03<00:03,  7.87it/s]11/21/2021 11:10:34 - INFO - __main__ -   Batch number = 31
Evaluating:  55%|█████▌    | 31/56 [00:03<00:03,  7.90it/s]11/21/2021 11:10:34 - INFO - __main__ -   Batch number = 32
Evaluating:  57%|█████▋    | 32/56 [00:04<00:03,  7.92it/s]11/21/2021 11:10:34 - INFO - __main__ -   Batch number = 33
Evaluating:  59%|█████▉    | 33/56 [00:04<00:02,  7.94it/s]11/21/2021 11:10:35 - INFO - __main__ -   Batch number = 34
Evaluating:  61%|██████    | 34/56 [00:04<00:02,  7.96it/s]11/21/2021 11:10:35 - INFO - __main__ -   Batch number = 35
Evaluating:  62%|██████▎   | 35/56 [00:04<00:02,  7.94it/s]11/21/2021 11:10:35 - INFO - __main__ -   Batch number = 36
Evaluating:  64%|██████▍   | 36/56 [00:04<00:02,  7.95it/s]11/21/2021 11:10:35 - INFO - __main__ -   Batch number = 37
Evaluating:  66%|██████▌   | 37/56 [00:04<00:02,  7.96it/s]11/21/2021 11:10:35 - INFO - __main__ -   Batch number = 38
Evaluating:  68%|██████▊   | 38/56 [00:04<00:02,  7.94it/s]11/21/2021 11:10:35 - INFO - __main__ -   Batch number = 39
Evaluating:  70%|██████▉   | 39/56 [00:04<00:02,  7.95it/s]11/21/2021 11:10:35 - INFO - __main__ -   Batch number = 40
Evaluating:  71%|███████▏  | 40/56 [00:05<00:02,  7.94it/s]11/21/2021 11:10:36 - INFO - __main__ -   Batch number = 41
Evaluating:  73%|███████▎  | 41/56 [00:05<00:01,  7.90it/s]11/21/2021 11:10:36 - INFO - __main__ -   Batch number = 42
Evaluating:  75%|███████▌  | 42/56 [00:05<00:01,  7.91it/s]11/21/2021 11:10:36 - INFO - __main__ -   Batch number = 43
Evaluating:  77%|███████▋  | 43/56 [00:05<00:01,  7.91it/s]11/21/2021 11:10:36 - INFO - __main__ -   Batch number = 44
Evaluating:  79%|███████▊  | 44/56 [00:05<00:01,  7.89it/s]11/21/2021 11:10:36 - INFO - __main__ -   Batch number = 45
Evaluating:  80%|████████  | 45/56 [00:05<00:01,  7.89it/s]11/21/2021 11:10:36 - INFO - __main__ -   Batch number = 46
Evaluating:  82%|████████▏ | 46/56 [00:05<00:01,  7.89it/s]11/21/2021 11:10:36 - INFO - __main__ -   Batch number = 47
Evaluating:  84%|████████▍ | 47/56 [00:05<00:01,  7.88it/s]11/21/2021 11:10:36 - INFO - __main__ -   Batch number = 48
Evaluating:  86%|████████▌ | 48/56 [00:06<00:01,  7.88it/s]11/21/2021 11:10:37 - INFO - __main__ -   Batch number = 49
Evaluating:  88%|████████▊ | 49/56 [00:06<00:00,  7.87it/s]11/21/2021 11:10:37 - INFO - __main__ -   Batch number = 50
Evaluating:  89%|████████▉ | 50/56 [00:06<00:00,  7.85it/s]11/21/2021 11:10:37 - INFO - __main__ -   Batch number = 51
Evaluating:  91%|█████████ | 51/56 [00:06<00:00,  7.86it/s]11/21/2021 11:10:37 - INFO - __main__ -   Batch number = 52
Evaluating:  93%|█████████▎| 52/56 [00:06<00:00,  7.87it/s]11/21/2021 11:10:37 - INFO - __main__ -   Batch number = 53
Evaluating:  95%|█████████▍| 53/56 [00:06<00:00,  7.86it/s]11/21/2021 11:10:37 - INFO - __main__ -   Batch number = 54
Evaluating:  96%|█████████▋| 54/56 [00:06<00:00,  7.85it/s]11/21/2021 11:10:37 - INFO - __main__ -   Batch number = 55
Evaluating:  98%|█████████▊| 55/56 [00:07<00:00,  7.85it/s]11/21/2021 11:10:37 - INFO - __main__ -   Batch number = 56
Evaluating: 100%|██████████| 56/56 [00:07<00:00,  8.38it/s]Evaluating: 100%|██████████| 56/56 [00:07<00:00,  7.88it/s]
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/21/2021 11:10:39 - INFO - __main__ -   ***** Evaluation result  in ar *****
11/21/2021 11:10:39 - INFO - __main__ -     f1 = 0.5603497188837882
11/21/2021 11:10:39 - INFO - __main__ -     loss = 1.5195540519697326
11/21/2021 11:10:39 - INFO - __main__ -     precision = 0.5488695212833143
11/21/2021 11:10:39 - INFO - __main__ -     recall = 0.5723204172483735
24.49user 10.46system 0:25.85elapsed 135%CPU (0avgtext+0avgdata 3823156maxresident)k
0inputs+6576outputs (0major+1159594minor)pagefaults 0swaps
PyTorch version 1.9.0+cu102 available.
11/21/2021 11:10:40 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ar', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/21/2021 11:10:40 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/21/2021 11:10:40 - INFO - __main__ -   Seed = 2
11/21/2021 11:10:40 - INFO - root -   save model
11/21/2021 11:10:40 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ar', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/21/2021 11:10:40 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/21/2021 11:10:43 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/21/2021 11:10:48 - INFO - __main__ -   Using lang2id = None
11/21/2021 11:10:48 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/21/2021 11:10:48 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
11/21/2021 11:10:48 - INFO - root -   Trying to decide if add adapter
11/21/2021 11:10:48 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_model_head.bin
11/21/2021 11:10:48 - INFO - root -   loading lang adpater am/wiki@ukp
11/21/2021 11:10:48 - INFO - __main__ -   Adapter Languages : ['am'], Length : 1
11/21/2021 11:10:48 - INFO - __main__ -   Adapter Names ['am/wiki@ukp'], Length : 1
11/21/2021 11:10:48 - INFO - __main__ -   Language = am
11/21/2021 11:10:48 - INFO - __main__ -   Adapter Name = am/wiki@ukp
Found matching adapter at: adapters/ukp/bert-base-multilingual-cased_am_wiki_pfeiffer.json
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/am/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_am_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/adapter_config.json
Adding adapter 'am' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/head_config.json
11/21/2021 11:10:53 - INFO - __main__ -   Language adapter for ar not found, using am instead
11/21/2021 11:10:53 - INFO - __main__ -   Set active language adapter to am
11/21/2021 11:10:53 - INFO - __main__ -   Args Adapter Weight = None
11/21/2021 11:10:53 - INFO - __main__ -   Adapter Languages = ['am']
11/21/2021 11:10:53 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_ar_bert-base-multilingual-cased_128
11/21/2021 11:10:53 - INFO - __main__ -   ***** Running evaluation  in ar *****
11/21/2021 11:10:53 - INFO - __main__ -     Num examples = 1784
11/21/2021 11:10:53 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/56 [00:00<?, ?it/s]11/21/2021 11:10:53 - INFO - __main__ -   Batch number = 1
Evaluating:   2%|▏         | 1/56 [00:00<00:07,  7.09it/s]11/21/2021 11:10:53 - INFO - __main__ -   Batch number = 2
Evaluating:   4%|▎         | 2/56 [00:00<00:07,  7.44it/s]11/21/2021 11:10:54 - INFO - __main__ -   Batch number = 3
Evaluating:   5%|▌         | 3/56 [00:00<00:06,  7.71it/s]11/21/2021 11:10:54 - INFO - __main__ -   Batch number = 4
Evaluating:   7%|▋         | 4/56 [00:00<00:06,  7.85it/s]11/21/2021 11:10:54 - INFO - __main__ -   Batch number = 5
Evaluating:   9%|▉         | 5/56 [00:00<00:06,  7.89it/s]11/21/2021 11:10:54 - INFO - __main__ -   Batch number = 6
Evaluating:  11%|█         | 6/56 [00:00<00:06,  7.94it/s]11/21/2021 11:10:54 - INFO - __main__ -   Batch number = 7
Evaluating:  12%|█▎        | 7/56 [00:00<00:06,  7.95it/s]11/21/2021 11:10:54 - INFO - __main__ -   Batch number = 8
Evaluating:  14%|█▍        | 8/56 [00:01<00:06,  7.96it/s]11/21/2021 11:10:54 - INFO - __main__ -   Batch number = 9
Evaluating:  16%|█▌        | 9/56 [00:01<00:05,  7.99it/s]11/21/2021 11:10:54 - INFO - __main__ -   Batch number = 10
Evaluating:  18%|█▊        | 10/56 [00:01<00:05,  7.98it/s]11/21/2021 11:10:55 - INFO - __main__ -   Batch number = 11
Evaluating:  20%|█▉        | 11/56 [00:01<00:05,  8.00it/s]11/21/2021 11:10:55 - INFO - __main__ -   Batch number = 12
Evaluating:  21%|██▏       | 12/56 [00:01<00:05,  7.99it/s]11/21/2021 11:10:55 - INFO - __main__ -   Batch number = 13
Evaluating:  23%|██▎       | 13/56 [00:01<00:05,  7.97it/s]11/21/2021 11:10:55 - INFO - __main__ -   Batch number = 14
Evaluating:  25%|██▌       | 14/56 [00:01<00:05,  7.97it/s]11/21/2021 11:10:55 - INFO - __main__ -   Batch number = 15
Evaluating:  27%|██▋       | 15/56 [00:01<00:05,  7.89it/s]11/21/2021 11:10:55 - INFO - __main__ -   Batch number = 16
Evaluating:  29%|██▊       | 16/56 [00:02<00:05,  7.89it/s]11/21/2021 11:10:55 - INFO - __main__ -   Batch number = 17
Evaluating:  30%|███       | 17/56 [00:02<00:04,  7.91it/s]11/21/2021 11:10:55 - INFO - __main__ -   Batch number = 18
Evaluating:  32%|███▏      | 18/56 [00:02<00:04,  7.92it/s]11/21/2021 11:10:56 - INFO - __main__ -   Batch number = 19
Evaluating:  34%|███▍      | 19/56 [00:02<00:04,  7.94it/s]11/21/2021 11:10:56 - INFO - __main__ -   Batch number = 20
Evaluating:  36%|███▌      | 20/56 [00:02<00:04,  7.93it/s]11/21/2021 11:10:56 - INFO - __main__ -   Batch number = 21
Evaluating:  38%|███▊      | 21/56 [00:02<00:04,  7.94it/s]11/21/2021 11:10:56 - INFO - __main__ -   Batch number = 22
Evaluating:  39%|███▉      | 22/56 [00:02<00:04,  7.95it/s]11/21/2021 11:10:56 - INFO - __main__ -   Batch number = 23
Evaluating:  41%|████      | 23/56 [00:02<00:04,  7.93it/s]11/21/2021 11:10:56 - INFO - __main__ -   Batch number = 24
Evaluating:  43%|████▎     | 24/56 [00:03<00:04,  7.91it/s]11/21/2021 11:10:56 - INFO - __main__ -   Batch number = 25
Evaluating:  45%|████▍     | 25/56 [00:03<00:03,  7.91it/s]11/21/2021 11:10:56 - INFO - __main__ -   Batch number = 26
Evaluating:  46%|████▋     | 26/56 [00:03<00:03,  7.90it/s]11/21/2021 11:10:57 - INFO - __main__ -   Batch number = 27
Evaluating:  48%|████▊     | 27/56 [00:03<00:03,  7.91it/s]11/21/2021 11:10:57 - INFO - __main__ -   Batch number = 28
Evaluating:  50%|█████     | 28/56 [00:03<00:03,  7.92it/s]11/21/2021 11:10:57 - INFO - __main__ -   Batch number = 29
Evaluating:  52%|█████▏    | 29/56 [00:03<00:03,  7.93it/s]11/21/2021 11:10:57 - INFO - __main__ -   Batch number = 30
Evaluating:  54%|█████▎    | 30/56 [00:03<00:03,  7.93it/s]11/21/2021 11:10:57 - INFO - __main__ -   Batch number = 31
Evaluating:  55%|█████▌    | 31/56 [00:03<00:03,  7.93it/s]11/21/2021 11:10:57 - INFO - __main__ -   Batch number = 32
Evaluating:  57%|█████▋    | 32/56 [00:04<00:03,  7.92it/s]11/21/2021 11:10:57 - INFO - __main__ -   Batch number = 33
Evaluating:  59%|█████▉    | 33/56 [00:04<00:02,  7.90it/s]11/21/2021 11:10:57 - INFO - __main__ -   Batch number = 34
Evaluating:  61%|██████    | 34/56 [00:04<00:02,  7.90it/s]11/21/2021 11:10:58 - INFO - __main__ -   Batch number = 35
Evaluating:  62%|██████▎   | 35/56 [00:04<00:02,  7.89it/s]11/21/2021 11:10:58 - INFO - __main__ -   Batch number = 36
Evaluating:  64%|██████▍   | 36/56 [00:04<00:02,  7.89it/s]11/21/2021 11:10:58 - INFO - __main__ -   Batch number = 37
Evaluating:  66%|██████▌   | 37/56 [00:04<00:02,  7.88it/s]11/21/2021 11:10:58 - INFO - __main__ -   Batch number = 38
Evaluating:  68%|██████▊   | 38/56 [00:04<00:02,  7.09it/s]11/21/2021 11:10:58 - INFO - __main__ -   Batch number = 39
Evaluating:  70%|██████▉   | 39/56 [00:04<00:02,  7.33it/s]11/21/2021 11:10:58 - INFO - __main__ -   Batch number = 40
Evaluating:  71%|███████▏  | 40/56 [00:05<00:02,  7.48it/s]11/21/2021 11:10:58 - INFO - __main__ -   Batch number = 41
Evaluating:  73%|███████▎  | 41/56 [00:05<00:01,  7.59it/s]11/21/2021 11:10:59 - INFO - __main__ -   Batch number = 42
Evaluating:  75%|███████▌  | 42/56 [00:05<00:01,  7.65it/s]11/21/2021 11:10:59 - INFO - __main__ -   Batch number = 43
Evaluating:  77%|███████▋  | 43/56 [00:05<00:01,  7.72it/s]11/21/2021 11:10:59 - INFO - __main__ -   Batch number = 44
Evaluating:  79%|███████▊  | 44/56 [00:05<00:01,  7.76it/s]11/21/2021 11:10:59 - INFO - __main__ -   Batch number = 45
Evaluating:  80%|████████  | 45/56 [00:05<00:01,  7.78it/s]11/21/2021 11:10:59 - INFO - __main__ -   Batch number = 46
Evaluating:  82%|████████▏ | 46/56 [00:05<00:01,  7.80it/s]11/21/2021 11:10:59 - INFO - __main__ -   Batch number = 47
Evaluating:  84%|████████▍ | 47/56 [00:05<00:01,  7.82it/s]11/21/2021 11:10:59 - INFO - __main__ -   Batch number = 48
Evaluating:  86%|████████▌ | 48/56 [00:06<00:01,  7.81it/s]11/21/2021 11:10:59 - INFO - __main__ -   Batch number = 49
Evaluating:  88%|████████▊ | 49/56 [00:06<00:00,  7.79it/s]11/21/2021 11:11:00 - INFO - __main__ -   Batch number = 50
Evaluating:  89%|████████▉ | 50/56 [00:06<00:00,  7.78it/s]11/21/2021 11:11:00 - INFO - __main__ -   Batch number = 51
Evaluating:  91%|█████████ | 51/56 [00:06<00:00,  7.77it/s]11/21/2021 11:11:00 - INFO - __main__ -   Batch number = 52
Evaluating:  93%|█████████▎| 52/56 [00:06<00:00,  7.76it/s]11/21/2021 11:11:00 - INFO - __main__ -   Batch number = 53
Evaluating:  95%|█████████▍| 53/56 [00:06<00:00,  7.70it/s]11/21/2021 11:11:00 - INFO - __main__ -   Batch number = 54
Evaluating:  96%|█████████▋| 54/56 [00:06<00:00,  7.68it/s]11/21/2021 11:11:00 - INFO - __main__ -   Batch number = 55
Evaluating:  98%|█████████▊| 55/56 [00:07<00:00,  7.71it/s]11/21/2021 11:11:00 - INFO - __main__ -   Batch number = 56
Evaluating: 100%|██████████| 56/56 [00:07<00:00,  7.85it/s]
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/21/2021 11:11:01 - INFO - __main__ -   ***** Evaluation result  in ar *****
11/21/2021 11:11:01 - INFO - __main__ -     f1 = 0.5552432766933966
11/21/2021 11:11:01 - INFO - __main__ -     loss = 1.682669228741101
11/21/2021 11:11:01 - INFO - __main__ -     precision = 0.543746685857132
11/21/2021 11:11:01 - INFO - __main__ -     recall = 0.567236519769249
21.88user 10.53system 0:22.69elapsed 142%CPU (0avgtext+0avgdata 3838424maxresident)k
0inputs+496outputs (0major+1132813minor)pagefaults 0swaps
PyTorch version 1.9.0+cu102 available.
11/21/2021 11:11:03 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ar', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/21/2021 11:11:03 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/21/2021 11:11:03 - INFO - __main__ -   Seed = 3
11/21/2021 11:11:03 - INFO - root -   save model
11/21/2021 11:11:03 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ar', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/21/2021 11:11:03 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/21/2021 11:11:06 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/21/2021 11:11:11 - INFO - __main__ -   Using lang2id = None
11/21/2021 11:11:11 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/21/2021 11:11:11 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
11/21/2021 11:11:11 - INFO - root -   Trying to decide if add adapter
11/21/2021 11:11:11 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_model_head.bin
11/21/2021 11:11:11 - INFO - root -   loading lang adpater am/wiki@ukp
11/21/2021 11:11:11 - INFO - __main__ -   Adapter Languages : ['am'], Length : 1
11/21/2021 11:11:11 - INFO - __main__ -   Adapter Names ['am/wiki@ukp'], Length : 1
11/21/2021 11:11:11 - INFO - __main__ -   Language = am
11/21/2021 11:11:11 - INFO - __main__ -   Adapter Name = am/wiki@ukp
Found matching adapter at: adapters/ukp/bert-base-multilingual-cased_am_wiki_pfeiffer.json
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/am/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_am_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/adapter_config.json
Adding adapter 'am' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/head_config.json
11/21/2021 11:11:16 - INFO - __main__ -   Language adapter for ar not found, using am instead
11/21/2021 11:11:16 - INFO - __main__ -   Set active language adapter to am
11/21/2021 11:11:16 - INFO - __main__ -   Args Adapter Weight = None
11/21/2021 11:11:16 - INFO - __main__ -   Adapter Languages = ['am']
11/21/2021 11:11:16 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_ar_bert-base-multilingual-cased_128
11/21/2021 11:11:16 - INFO - __main__ -   ***** Running evaluation  in ar *****
11/21/2021 11:11:16 - INFO - __main__ -     Num examples = 1784
11/21/2021 11:11:16 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/56 [00:00<?, ?it/s]11/21/2021 11:11:16 - INFO - __main__ -   Batch number = 1
Evaluating:   2%|▏         | 1/56 [00:00<00:06,  7.94it/s]11/21/2021 11:11:16 - INFO - __main__ -   Batch number = 2
Evaluating:   4%|▎         | 2/56 [00:00<00:06,  7.99it/s]11/21/2021 11:11:16 - INFO - __main__ -   Batch number = 3
Evaluating:   5%|▌         | 3/56 [00:00<00:06,  8.01it/s]11/21/2021 11:11:17 - INFO - __main__ -   Batch number = 4
Evaluating:   7%|▋         | 4/56 [00:00<00:06,  8.03it/s]11/21/2021 11:11:17 - INFO - __main__ -   Batch number = 5
Evaluating:   9%|▉         | 5/56 [00:00<00:06,  8.02it/s]11/21/2021 11:11:17 - INFO - __main__ -   Batch number = 6
Evaluating:  11%|█         | 6/56 [00:00<00:06,  8.01it/s]11/21/2021 11:11:17 - INFO - __main__ -   Batch number = 7
Evaluating:  12%|█▎        | 7/56 [00:00<00:06,  8.00it/s]11/21/2021 11:11:17 - INFO - __main__ -   Batch number = 8
Evaluating:  14%|█▍        | 8/56 [00:01<00:06,  7.95it/s]11/21/2021 11:11:17 - INFO - __main__ -   Batch number = 9
Evaluating:  16%|█▌        | 9/56 [00:01<00:05,  7.93it/s]11/21/2021 11:11:17 - INFO - __main__ -   Batch number = 10
Evaluating:  18%|█▊        | 10/56 [00:01<00:05,  7.94it/s]11/21/2021 11:11:17 - INFO - __main__ -   Batch number = 11
Evaluating:  20%|█▉        | 11/56 [00:01<00:05,  7.90it/s]11/21/2021 11:11:18 - INFO - __main__ -   Batch number = 12
Evaluating:  21%|██▏       | 12/56 [00:01<00:05,  7.89it/s]11/21/2021 11:11:18 - INFO - __main__ -   Batch number = 13
Evaluating:  23%|██▎       | 13/56 [00:01<00:05,  7.91it/s]11/21/2021 11:11:18 - INFO - __main__ -   Batch number = 14
Evaluating:  25%|██▌       | 14/56 [00:01<00:05,  7.92it/s]11/21/2021 11:11:18 - INFO - __main__ -   Batch number = 15
Evaluating:  27%|██▋       | 15/56 [00:01<00:05,  7.92it/s]11/21/2021 11:11:18 - INFO - __main__ -   Batch number = 16
Evaluating:  29%|██▊       | 16/56 [00:02<00:05,  7.92it/s]11/21/2021 11:11:18 - INFO - __main__ -   Batch number = 17
Evaluating:  30%|███       | 17/56 [00:02<00:04,  7.93it/s]11/21/2021 11:11:18 - INFO - __main__ -   Batch number = 18
Evaluating:  32%|███▏      | 18/56 [00:02<00:04,  7.90it/s]11/21/2021 11:11:18 - INFO - __main__ -   Batch number = 19
Evaluating:  34%|███▍      | 19/56 [00:02<00:04,  7.91it/s]11/21/2021 11:11:19 - INFO - __main__ -   Batch number = 20
Evaluating:  36%|███▌      | 20/56 [00:02<00:04,  7.92it/s]11/21/2021 11:11:19 - INFO - __main__ -   Batch number = 21
Evaluating:  38%|███▊      | 21/56 [00:02<00:04,  7.89it/s]11/21/2021 11:11:19 - INFO - __main__ -   Batch number = 22
Evaluating:  39%|███▉      | 22/56 [00:02<00:04,  7.91it/s]11/21/2021 11:11:19 - INFO - __main__ -   Batch number = 23
Evaluating:  41%|████      | 23/56 [00:02<00:04,  7.90it/s]11/21/2021 11:11:19 - INFO - __main__ -   Batch number = 24
Evaluating:  43%|████▎     | 24/56 [00:03<00:04,  7.89it/s]11/21/2021 11:11:19 - INFO - __main__ -   Batch number = 25
Evaluating:  45%|████▍     | 25/56 [00:03<00:03,  7.89it/s]11/21/2021 11:11:19 - INFO - __main__ -   Batch number = 26
Evaluating:  46%|████▋     | 26/56 [00:03<00:03,  7.88it/s]11/21/2021 11:11:19 - INFO - __main__ -   Batch number = 27
Evaluating:  48%|████▊     | 27/56 [00:03<00:03,  7.86it/s]11/21/2021 11:11:20 - INFO - __main__ -   Batch number = 28
Evaluating:  50%|█████     | 28/56 [00:03<00:03,  7.87it/s]11/21/2021 11:11:20 - INFO - __main__ -   Batch number = 29
Evaluating:  52%|█████▏    | 29/56 [00:03<00:03,  7.86it/s]11/21/2021 11:11:20 - INFO - __main__ -   Batch number = 30
Evaluating:  54%|█████▎    | 30/56 [00:03<00:03,  7.84it/s]11/21/2021 11:11:20 - INFO - __main__ -   Batch number = 31
Evaluating:  55%|█████▌    | 31/56 [00:03<00:03,  7.85it/s]11/21/2021 11:11:20 - INFO - __main__ -   Batch number = 32
Evaluating:  57%|█████▋    | 32/56 [00:04<00:03,  7.86it/s]11/21/2021 11:11:20 - INFO - __main__ -   Batch number = 33
Evaluating:  59%|█████▉    | 33/56 [00:04<00:02,  7.85it/s]11/21/2021 11:11:20 - INFO - __main__ -   Batch number = 34
Evaluating:  61%|██████    | 34/56 [00:04<00:02,  7.83it/s]11/21/2021 11:11:20 - INFO - __main__ -   Batch number = 35
Evaluating:  62%|██████▎   | 35/56 [00:04<00:02,  7.85it/s]11/21/2021 11:11:21 - INFO - __main__ -   Batch number = 36
Evaluating:  64%|██████▍   | 36/56 [00:04<00:02,  7.83it/s]11/21/2021 11:11:21 - INFO - __main__ -   Batch number = 37
Evaluating:  66%|██████▌   | 37/56 [00:04<00:02,  7.38it/s]11/21/2021 11:11:21 - INFO - __main__ -   Batch number = 38
Evaluating:  68%|██████▊   | 38/56 [00:04<00:02,  7.43it/s]11/21/2021 11:11:21 - INFO - __main__ -   Batch number = 39
Evaluating:  70%|██████▉   | 39/56 [00:04<00:02,  7.52it/s]11/21/2021 11:11:21 - INFO - __main__ -   Batch number = 40
Evaluating:  71%|███████▏  | 40/56 [00:05<00:02,  7.63it/s]11/21/2021 11:11:21 - INFO - __main__ -   Batch number = 41
Evaluating:  73%|███████▎  | 41/56 [00:05<00:01,  7.68it/s]11/21/2021 11:11:21 - INFO - __main__ -   Batch number = 42
Evaluating:  75%|███████▌  | 42/56 [00:05<00:01,  7.71it/s]11/21/2021 11:11:22 - INFO - __main__ -   Batch number = 43
Evaluating:  77%|███████▋  | 43/56 [00:05<00:01,  7.75it/s]11/21/2021 11:11:22 - INFO - __main__ -   Batch number = 44
Evaluating:  79%|███████▊  | 44/56 [00:05<00:01,  7.76it/s]11/21/2021 11:11:22 - INFO - __main__ -   Batch number = 45
Evaluating:  80%|████████  | 45/56 [00:05<00:01,  7.78it/s]11/21/2021 11:11:22 - INFO - __main__ -   Batch number = 46
Evaluating:  82%|████████▏ | 46/56 [00:05<00:01,  7.79it/s]11/21/2021 11:11:22 - INFO - __main__ -   Batch number = 47
Evaluating:  84%|████████▍ | 47/56 [00:06<00:01,  7.68it/s]11/21/2021 11:11:22 - INFO - __main__ -   Batch number = 48
Evaluating:  86%|████████▌ | 48/56 [00:06<00:01,  7.66it/s]11/21/2021 11:11:22 - INFO - __main__ -   Batch number = 49
Evaluating:  88%|████████▊ | 49/56 [00:06<00:00,  7.72it/s]11/21/2021 11:11:22 - INFO - __main__ -   Batch number = 50
Evaluating:  89%|████████▉ | 50/56 [00:06<00:00,  7.74it/s]11/21/2021 11:11:23 - INFO - __main__ -   Batch number = 51
Evaluating:  91%|█████████ | 51/56 [00:06<00:00,  7.74it/s]11/21/2021 11:11:23 - INFO - __main__ -   Batch number = 52
Evaluating:  93%|█████████▎| 52/56 [00:06<00:00,  7.76it/s]11/21/2021 11:11:23 - INFO - __main__ -   Batch number = 53
Evaluating:  95%|█████████▍| 53/56 [00:06<00:00,  7.78it/s]11/21/2021 11:11:23 - INFO - __main__ -   Batch number = 54
Evaluating:  96%|█████████▋| 54/56 [00:06<00:00,  7.73it/s]11/21/2021 11:11:23 - INFO - __main__ -   Batch number = 55
Evaluating:  98%|█████████▊| 55/56 [00:07<00:00,  7.73it/s]11/21/2021 11:11:23 - INFO - __main__ -   Batch number = 56
Evaluating: 100%|██████████| 56/56 [00:07<00:00,  8.28it/s]Evaluating: 100%|██████████| 56/56 [00:07<00:00,  7.85it/s]
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/21/2021 11:11:24 - INFO - __main__ -   ***** Evaluation result  in ar *****
11/21/2021 11:11:24 - INFO - __main__ -     f1 = 0.5559433804304427
11/21/2021 11:11:24 - INFO - __main__ -     loss = 1.5234386803848403
11/21/2021 11:11:24 - INFO - __main__ -     precision = 0.5439286254347497
11/21/2021 11:11:24 - INFO - __main__ -     recall = 0.5685009087796012
21.61user 10.06system 0:23.01elapsed 137%CPU (0avgtext+0avgdata 3823764maxresident)k
0inputs+528outputs (0major+1146369minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 00:58:13 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ru', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 00:58:13 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 00:58:13 - INFO - __main__ -   Seed = 1
11/28/2021 00:58:13 - INFO - root -   save model
11/28/2021 00:58:13 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ru', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 00:58:13 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 00:58:16 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 00:58:22 - INFO - __main__ -   Using lang2id = None
11/28/2021 00:58:22 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 00:58:22 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
11/28/2021 00:58:22 - INFO - root -   Trying to decide if add adapter
11/28/2021 00:58:22 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 00:58:22 - INFO - root -   loading lang adpater am/wiki@ukp
11/28/2021 00:58:22 - INFO - __main__ -   Adapter Languages : ['am'], Length : 1
11/28/2021 00:58:22 - INFO - __main__ -   Adapter Names ['am/wiki@ukp'], Length : 1
11/28/2021 00:58:22 - INFO - __main__ -   Language = am
11/28/2021 00:58:22 - INFO - __main__ -   Adapter Name = am/wiki@ukp
Found matching adapter at: adapters/ukp/bert-base-multilingual-cased_am_wiki_pfeiffer.json
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/am/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_am_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/adapter_config.json
Adding adapter 'am' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/head_config.json
11/28/2021 00:58:28 - INFO - __main__ -   Language adapter for ru not found, using am instead
11/28/2021 00:58:28 - INFO - __main__ -   Set active language adapter to am
11/28/2021 00:58:28 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 00:58:28 - INFO - __main__ -   Adapter Languages = ['am']
11/28/2021 00:58:28 - INFO - __main__ -   all languages = ru
11/28/2021 00:58:28 - INFO - __main__ -   Creating features from dataset file at /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/ru/test.bert-base-multilingual-cased in language ru
11/28/2021 00:58:28 - INFO - utils_tag -   lang_id=0, lang=ru, lang2id=None
11/28/2021 00:58:29 - INFO - utils_tag -   Writing example 0 of 8995
11/28/2021 00:58:29 - INFO - utils_tag -   *** Example ***
11/28/2021 00:58:29 - INFO - utils_tag -   guid: ru-1
11/28/2021 00:58:29 - INFO - utils_tag -   tokens: [CLS] Бил ##ли начал играть за р ##езе ##рв ##ный состав [UNK] [UNK] Ч ##ер ##ка & # 39 ; & # 39 ; в возрасте 16 лет , а через пар ##у сезон ##ов был при ##гла ##шён в основной состав . [SEP]
11/28/2021 00:58:29 - INFO - utils_tag -   input_ids: 101 109719 10783 23137 66796 10234 557 109990 47153 11092 13701 100 100 532 11977 10521 111 108 11303 132 111 108 11303 132 543 30451 10250 12773 117 541 12798 66457 10227 22140 10433 10702 10913 42805 108111 543 46864 13701 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 00:58:29 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 00:58:29 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 00:58:29 - INFO - utils_tag -   label_ids: -100 12 -100 16 16 2 1 -100 -100 -100 8 13 -100 12 -100 -100 13 -100 -100 -100 -100 -100 -100 -100 2 8 9 8 13 5 2 8 -100 8 -100 4 16 -100 -100 2 1 8 13 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
11/28/2021 00:58:29 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
11/28/2021 00:58:29 - INFO - utils_tag -   *** Example ***
11/28/2021 00:58:29 - INFO - utils_tag -   guid: ru-2
11/28/2021 00:58:29 - INFO - utils_tag -   tokens: [CLS] С ##то ##имость про ##езда с 5 января 2013 года - - 15 рублей , движение осуществляется с 6 . 00 до 00 . 20 . [SEP]
11/28/2021 00:58:29 - INFO - utils_tag -   input_ids: 101 526 10752 73434 12709 64860 558 126 13548 10207 10334 118 118 10208 54298 117 37767 91411 558 127 119 11025 10344 11025 119 10197 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 00:58:29 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 00:58:29 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 00:58:29 - INFO - utils_tag -   label_ids: -100 8 -100 -100 8 -100 2 1 8 1 8 13 -100 9 8 13 8 16 2 9 -100 -100 2 9 -100 -100 13 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
11/28/2021 00:58:29 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
11/28/2021 00:58:29 - INFO - utils_tag -   *** Example ***
11/28/2021 00:58:29 - INFO - utils_tag -   guid: ru-3
11/28/2021 00:58:29 - INFO - utils_tag -   tokens: [CLS] С ##тал членом секретар ##иа ##та общественной безопасности Мексики ( SS ##P , Secretaría de Seguridad Pública ) и с ##пе ##циальным у ##по ##лно ##мо ##чен ##ным Ф ##еде ##рал ##ьной полиции Мексики ( P ##F ##P , Policía Federal Pre ##venti ##va ) . [SEP]
11/28/2021 00:58:29 - INFO - utils_tag -   input_ids: 101 526 44422 23003 46230 97843 10367 100871 41779 105551 113 13945 11127 117 84328 10104 92605 48948 114 549 558 19820 75420 560 53204 22789 19900 16580 11692 529 87097 31326 21284 74724 105551 113 153 11565 11127 117 84688 14492 35248 83692 10362 114 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 00:58:29 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 00:58:29 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 00:58:29 - INFO - utils_tag -   label_ids: -100 16 -100 8 8 -100 -100 1 8 12 13 17 -100 13 17 17 17 17 13 5 1 -100 -100 8 -100 -100 -100 -100 -100 1 -100 -100 -100 8 12 13 17 -100 -100 13 17 17 17 -100 -100 13 13 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
11/28/2021 00:58:29 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
11/28/2021 00:58:29 - INFO - utils_tag -   *** Example ***
11/28/2021 00:58:29 - INFO - utils_tag -   guid: ru-4
11/28/2021 00:58:29 - INFO - utils_tag -   tokens: [CLS] Официальный код округа AT ##12 ##5 . [SEP]
11/28/2021 00:58:29 - INFO - utils_tag -   input_ids: 101 50214 15296 19134 30554 24747 11166 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 00:58:29 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 00:58:29 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 00:58:29 - INFO - utils_tag -   label_ids: -100 1 8 8 17 -100 -100 13 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
11/28/2021 00:58:29 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
11/28/2021 00:58:29 - INFO - utils_tag -   *** Example ***
11/28/2021 00:58:29 - INFO - utils_tag -   guid: ru-5
11/28/2021 00:58:29 - INFO - utils_tag -   tokens: [CLS] Здесь об ##ита ##ет несколько де ##сят ##ков видов птиц . [SEP]
11/28/2021 00:58:29 - INFO - utils_tag -   input_ids: 101 48140 13248 33227 11613 15981 11323 83125 13036 32436 84799 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 00:58:29 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 00:58:29 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 00:58:29 - INFO - utils_tag -   label_ids: -100 3 16 -100 -100 9 8 -100 -100 8 8 13 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
11/28/2021 00:58:29 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
11/28/2021 00:58:37 - INFO - __main__ -   Saving features into cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_ru_bert-base-multilingual-cased_128, len(features)=8995
11/28/2021 00:58:39 - INFO - __main__ -   ***** Running evaluation  in ru *****
11/28/2021 00:58:39 - INFO - __main__ -     Num examples = 8995
11/28/2021 00:58:39 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/282 [00:00<?, ?it/s]11/28/2021 00:58:39 - INFO - __main__ -   Batch number = 1
Evaluating:   0%|          | 1/282 [00:00<00:41,  6.76it/s]11/28/2021 00:58:39 - INFO - __main__ -   Batch number = 2
Evaluating:   1%|          | 2/282 [00:00<00:39,  7.18it/s]11/28/2021 00:58:39 - INFO - __main__ -   Batch number = 3
Evaluating:   1%|          | 3/282 [00:00<00:37,  7.48it/s]11/28/2021 00:58:40 - INFO - __main__ -   Batch number = 4
Evaluating:   1%|▏         | 4/282 [00:00<00:36,  7.65it/s]11/28/2021 00:58:40 - INFO - __main__ -   Batch number = 5
Evaluating:   2%|▏         | 5/282 [00:00<00:35,  7.72it/s]11/28/2021 00:58:40 - INFO - __main__ -   Batch number = 6
Evaluating:   2%|▏         | 6/282 [00:00<00:35,  7.77it/s]11/28/2021 00:58:40 - INFO - __main__ -   Batch number = 7
Evaluating:   2%|▏         | 7/282 [00:00<00:35,  7.81it/s]11/28/2021 00:58:40 - INFO - __main__ -   Batch number = 8
Evaluating:   3%|▎         | 8/282 [00:01<00:34,  7.83it/s]11/28/2021 00:58:40 - INFO - __main__ -   Batch number = 9
Evaluating:   3%|▎         | 9/282 [00:01<00:34,  7.84it/s]11/28/2021 00:58:40 - INFO - __main__ -   Batch number = 10
Evaluating:   4%|▎         | 10/282 [00:01<00:34,  7.84it/s]11/28/2021 00:58:40 - INFO - __main__ -   Batch number = 11
Evaluating:   4%|▍         | 11/282 [00:01<00:34,  7.84it/s]11/28/2021 00:58:41 - INFO - __main__ -   Batch number = 12
Evaluating:   4%|▍         | 12/282 [00:01<00:34,  7.84it/s]11/28/2021 00:58:41 - INFO - __main__ -   Batch number = 13
Evaluating:   5%|▍         | 13/282 [00:01<00:34,  7.82it/s]11/28/2021 00:58:41 - INFO - __main__ -   Batch number = 14
Evaluating:   5%|▍         | 14/282 [00:01<00:34,  7.80it/s]11/28/2021 00:58:41 - INFO - __main__ -   Batch number = 15
Evaluating:   5%|▌         | 15/282 [00:01<00:34,  7.80it/s]11/28/2021 00:58:41 - INFO - __main__ -   Batch number = 16
Evaluating:   6%|▌         | 16/282 [00:02<00:34,  7.82it/s]11/28/2021 00:58:41 - INFO - __main__ -   Batch number = 17
Evaluating:   6%|▌         | 17/282 [00:02<00:33,  7.81it/s]11/28/2021 00:58:41 - INFO - __main__ -   Batch number = 18
Evaluating:   6%|▋         | 18/282 [00:02<00:33,  7.82it/s]11/28/2021 00:58:41 - INFO - __main__ -   Batch number = 19
Evaluating:   7%|▋         | 19/282 [00:02<00:33,  7.81it/s]11/28/2021 00:58:42 - INFO - __main__ -   Batch number = 20
Evaluating:   7%|▋         | 20/282 [00:02<00:33,  7.82it/s]11/28/2021 00:58:42 - INFO - __main__ -   Batch number = 21
Evaluating:   7%|▋         | 21/282 [00:02<00:33,  7.82it/s]11/28/2021 00:58:42 - INFO - __main__ -   Batch number = 22
Evaluating:   8%|▊         | 22/282 [00:02<00:33,  7.81it/s]11/28/2021 00:58:42 - INFO - __main__ -   Batch number = 23
Evaluating:   8%|▊         | 23/282 [00:02<00:33,  7.79it/s]11/28/2021 00:58:42 - INFO - __main__ -   Batch number = 24
Evaluating:   9%|▊         | 24/282 [00:03<00:33,  7.74it/s]11/28/2021 00:58:42 - INFO - __main__ -   Batch number = 25
Evaluating:   9%|▉         | 25/282 [00:03<00:33,  7.69it/s]11/28/2021 00:58:42 - INFO - __main__ -   Batch number = 26
Evaluating:   9%|▉         | 26/282 [00:03<00:33,  7.71it/s]11/28/2021 00:58:42 - INFO - __main__ -   Batch number = 27
Evaluating:  10%|▉         | 27/282 [00:03<00:32,  7.73it/s]11/28/2021 00:58:43 - INFO - __main__ -   Batch number = 28
Evaluating:  10%|▉         | 28/282 [00:03<00:32,  7.73it/s]11/28/2021 00:58:43 - INFO - __main__ -   Batch number = 29
Evaluating:  10%|█         | 29/282 [00:03<00:32,  7.74it/s]11/28/2021 00:58:43 - INFO - __main__ -   Batch number = 30
Evaluating:  11%|█         | 30/282 [00:03<00:32,  7.74it/s]11/28/2021 00:58:43 - INFO - __main__ -   Batch number = 31
Evaluating:  11%|█         | 31/282 [00:03<00:32,  7.74it/s]11/28/2021 00:58:43 - INFO - __main__ -   Batch number = 32
Evaluating:  11%|█▏        | 32/282 [00:04<00:32,  7.75it/s]11/28/2021 00:58:43 - INFO - __main__ -   Batch number = 33
Evaluating:  12%|█▏        | 33/282 [00:04<00:32,  7.74it/s]11/28/2021 00:58:43 - INFO - __main__ -   Batch number = 34
Evaluating:  12%|█▏        | 34/282 [00:04<00:32,  7.71it/s]11/28/2021 00:58:44 - INFO - __main__ -   Batch number = 35
Evaluating:  12%|█▏        | 35/282 [00:04<00:31,  7.73it/s]11/28/2021 00:58:44 - INFO - __main__ -   Batch number = 36
Evaluating:  13%|█▎        | 36/282 [00:04<00:31,  7.73it/s]11/28/2021 00:58:44 - INFO - __main__ -   Batch number = 37
Evaluating:  13%|█▎        | 37/282 [00:04<00:31,  7.71it/s]11/28/2021 00:58:44 - INFO - __main__ -   Batch number = 38
Evaluating:  13%|█▎        | 38/282 [00:04<00:31,  7.71it/s]11/28/2021 00:58:44 - INFO - __main__ -   Batch number = 39
Evaluating:  14%|█▍        | 39/282 [00:05<00:31,  7.72it/s]11/28/2021 00:58:44 - INFO - __main__ -   Batch number = 40
Evaluating:  14%|█▍        | 40/282 [00:05<00:31,  7.71it/s]11/28/2021 00:58:44 - INFO - __main__ -   Batch number = 41
Evaluating:  15%|█▍        | 41/282 [00:05<00:31,  7.72it/s]11/28/2021 00:58:44 - INFO - __main__ -   Batch number = 42
Evaluating:  15%|█▍        | 42/282 [00:05<00:31,  7.71it/s]11/28/2021 00:58:45 - INFO - __main__ -   Batch number = 43
Evaluating:  15%|█▌        | 43/282 [00:05<00:31,  7.68it/s]11/28/2021 00:58:45 - INFO - __main__ -   Batch number = 44
Evaluating:  16%|█▌        | 44/282 [00:05<00:30,  7.70it/s]11/28/2021 00:58:45 - INFO - __main__ -   Batch number = 45
Evaluating:  16%|█▌        | 45/282 [00:05<00:30,  7.70it/s]11/28/2021 00:58:45 - INFO - __main__ -   Batch number = 46
Evaluating:  16%|█▋        | 46/282 [00:05<00:30,  7.69it/s]11/28/2021 00:58:45 - INFO - __main__ -   Batch number = 47
Evaluating:  17%|█▋        | 47/282 [00:06<00:30,  7.69it/s]11/28/2021 00:58:45 - INFO - __main__ -   Batch number = 48
Evaluating:  17%|█▋        | 48/282 [00:06<00:30,  7.69it/s]11/28/2021 00:58:45 - INFO - __main__ -   Batch number = 49
Evaluating:  17%|█▋        | 49/282 [00:06<00:30,  7.66it/s]11/28/2021 00:58:45 - INFO - __main__ -   Batch number = 50
Evaluating:  18%|█▊        | 50/282 [00:06<00:30,  7.64it/s]11/28/2021 00:58:46 - INFO - __main__ -   Batch number = 51
Evaluating:  18%|█▊        | 51/282 [00:06<00:30,  7.65it/s]11/28/2021 00:58:46 - INFO - __main__ -   Batch number = 52
Evaluating:  18%|█▊        | 52/282 [00:06<00:30,  7.64it/s]11/28/2021 00:58:46 - INFO - __main__ -   Batch number = 53
Evaluating:  19%|█▉        | 53/282 [00:06<00:29,  7.65it/s]11/28/2021 00:58:46 - INFO - __main__ -   Batch number = 54
Evaluating:  19%|█▉        | 54/282 [00:06<00:29,  7.66it/s]11/28/2021 00:58:46 - INFO - __main__ -   Batch number = 55
Evaluating:  20%|█▉        | 55/282 [00:07<00:29,  7.64it/s]11/28/2021 00:58:46 - INFO - __main__ -   Batch number = 56
Evaluating:  20%|█▉        | 56/282 [00:07<00:29,  7.64it/s]11/28/2021 00:58:46 - INFO - __main__ -   Batch number = 57
Evaluating:  20%|██        | 57/282 [00:07<00:29,  7.64it/s]11/28/2021 00:58:47 - INFO - __main__ -   Batch number = 58
Evaluating:  21%|██        | 58/282 [00:07<00:29,  7.63it/s]11/28/2021 00:58:47 - INFO - __main__ -   Batch number = 59
Evaluating:  21%|██        | 59/282 [00:07<00:29,  7.63it/s]11/28/2021 00:58:47 - INFO - __main__ -   Batch number = 60
Evaluating:  21%|██▏       | 60/282 [00:07<00:29,  7.62it/s]11/28/2021 00:58:47 - INFO - __main__ -   Batch number = 61
Evaluating:  22%|██▏       | 61/282 [00:07<00:29,  7.61it/s]11/28/2021 00:58:47 - INFO - __main__ -   Batch number = 62
Evaluating:  22%|██▏       | 62/282 [00:08<00:28,  7.61it/s]11/28/2021 00:58:47 - INFO - __main__ -   Batch number = 63
Evaluating:  22%|██▏       | 63/282 [00:08<00:28,  7.61it/s]11/28/2021 00:58:47 - INFO - __main__ -   Batch number = 64
Evaluating:  23%|██▎       | 64/282 [00:08<00:28,  7.59it/s]11/28/2021 00:58:47 - INFO - __main__ -   Batch number = 65
Evaluating:  23%|██▎       | 65/282 [00:08<00:28,  7.60it/s]11/28/2021 00:58:48 - INFO - __main__ -   Batch number = 66
Evaluating:  23%|██▎       | 66/282 [00:08<00:28,  7.59it/s]11/28/2021 00:58:48 - INFO - __main__ -   Batch number = 67
Evaluating:  24%|██▍       | 67/282 [00:08<00:28,  7.56it/s]11/28/2021 00:58:48 - INFO - __main__ -   Batch number = 68
Evaluating:  24%|██▍       | 68/282 [00:08<00:28,  7.44it/s]11/28/2021 00:58:48 - INFO - __main__ -   Batch number = 69
Evaluating:  24%|██▍       | 69/282 [00:08<00:28,  7.48it/s]11/28/2021 00:58:48 - INFO - __main__ -   Batch number = 70
Evaluating:  25%|██▍       | 70/282 [00:09<00:28,  7.51it/s]11/28/2021 00:58:48 - INFO - __main__ -   Batch number = 71
Evaluating:  25%|██▌       | 71/282 [00:09<00:28,  7.53it/s]11/28/2021 00:58:48 - INFO - __main__ -   Batch number = 72
Evaluating:  26%|██▌       | 72/282 [00:09<00:27,  7.52it/s]11/28/2021 00:58:48 - INFO - __main__ -   Batch number = 73
Evaluating:  26%|██▌       | 73/282 [00:09<00:27,  7.51it/s]11/28/2021 00:58:49 - INFO - __main__ -   Batch number = 74
Evaluating:  26%|██▌       | 74/282 [00:09<00:27,  7.51it/s]11/28/2021 00:58:49 - INFO - __main__ -   Batch number = 75
Evaluating:  27%|██▋       | 75/282 [00:09<00:27,  7.52it/s]11/28/2021 00:58:49 - INFO - __main__ -   Batch number = 76
Evaluating:  27%|██▋       | 76/282 [00:09<00:27,  7.48it/s]11/28/2021 00:58:49 - INFO - __main__ -   Batch number = 77
Evaluating:  27%|██▋       | 77/282 [00:10<00:27,  7.50it/s]11/28/2021 00:58:49 - INFO - __main__ -   Batch number = 78
Evaluating:  28%|██▊       | 78/282 [00:10<00:30,  6.72it/s]11/28/2021 00:58:49 - INFO - __main__ -   Batch number = 79
Evaluating:  28%|██▊       | 79/282 [00:10<00:29,  6.94it/s]11/28/2021 00:58:49 - INFO - __main__ -   Batch number = 80
Evaluating:  28%|██▊       | 80/282 [00:10<00:28,  7.10it/s]11/28/2021 00:58:50 - INFO - __main__ -   Batch number = 81
Evaluating:  29%|██▊       | 81/282 [00:10<00:27,  7.21it/s]11/28/2021 00:58:50 - INFO - __main__ -   Batch number = 82
Evaluating:  29%|██▉       | 82/282 [00:10<00:27,  7.30it/s]11/28/2021 00:58:50 - INFO - __main__ -   Batch number = 83
Evaluating:  29%|██▉       | 83/282 [00:10<00:27,  7.37it/s]11/28/2021 00:58:50 - INFO - __main__ -   Batch number = 84
Evaluating:  30%|██▉       | 84/282 [00:11<00:27,  7.15it/s]11/28/2021 00:58:50 - INFO - __main__ -   Batch number = 85
Evaluating:  30%|███       | 85/282 [00:11<00:27,  7.25it/s]11/28/2021 00:58:50 - INFO - __main__ -   Batch number = 86
Evaluating:  30%|███       | 86/282 [00:11<00:26,  7.31it/s]11/28/2021 00:58:50 - INFO - __main__ -   Batch number = 87
Evaluating:  31%|███       | 87/282 [00:11<00:26,  7.36it/s]11/28/2021 00:58:51 - INFO - __main__ -   Batch number = 88
Evaluating:  31%|███       | 88/282 [00:11<00:26,  7.39it/s]11/28/2021 00:58:51 - INFO - __main__ -   Batch number = 89
Evaluating:  32%|███▏      | 89/282 [00:11<00:26,  7.41it/s]11/28/2021 00:58:51 - INFO - __main__ -   Batch number = 90
Evaluating:  32%|███▏      | 90/282 [00:11<00:25,  7.40it/s]11/28/2021 00:58:51 - INFO - __main__ -   Batch number = 91
Evaluating:  32%|███▏      | 91/282 [00:11<00:25,  7.43it/s]11/28/2021 00:58:51 - INFO - __main__ -   Batch number = 92
Evaluating:  33%|███▎      | 92/282 [00:12<00:25,  7.44it/s]11/28/2021 00:58:51 - INFO - __main__ -   Batch number = 93
Evaluating:  33%|███▎      | 93/282 [00:12<00:25,  7.45it/s]11/28/2021 00:58:51 - INFO - __main__ -   Batch number = 94
Evaluating:  33%|███▎      | 94/282 [00:12<00:25,  7.45it/s]11/28/2021 00:58:52 - INFO - __main__ -   Batch number = 95
Evaluating:  34%|███▎      | 95/282 [00:12<00:25,  7.44it/s]11/28/2021 00:58:52 - INFO - __main__ -   Batch number = 96
Evaluating:  34%|███▍      | 96/282 [00:12<00:25,  7.44it/s]11/28/2021 00:58:52 - INFO - __main__ -   Batch number = 97
Evaluating:  34%|███▍      | 97/282 [00:12<00:24,  7.43it/s]11/28/2021 00:58:52 - INFO - __main__ -   Batch number = 98
Evaluating:  35%|███▍      | 98/282 [00:12<00:24,  7.44it/s]11/28/2021 00:58:52 - INFO - __main__ -   Batch number = 99
Evaluating:  35%|███▌      | 99/282 [00:13<00:24,  7.42it/s]11/28/2021 00:58:52 - INFO - __main__ -   Batch number = 100
Evaluating:  35%|███▌      | 100/282 [00:13<00:24,  7.42it/s]11/28/2021 00:58:52 - INFO - __main__ -   Batch number = 101
Evaluating:  36%|███▌      | 101/282 [00:13<00:24,  7.27it/s]11/28/2021 00:58:52 - INFO - __main__ -   Batch number = 102
Evaluating:  36%|███▌      | 102/282 [00:13<00:24,  7.26it/s]11/28/2021 00:58:53 - INFO - __main__ -   Batch number = 103
Evaluating:  37%|███▋      | 103/282 [00:13<00:24,  7.28it/s]11/28/2021 00:58:53 - INFO - __main__ -   Batch number = 104
Evaluating:  37%|███▋      | 104/282 [00:13<00:24,  7.32it/s]11/28/2021 00:58:53 - INFO - __main__ -   Batch number = 105
Evaluating:  37%|███▋      | 105/282 [00:13<00:24,  7.35it/s]11/28/2021 00:58:53 - INFO - __main__ -   Batch number = 106
Evaluating:  38%|███▊      | 106/282 [00:14<00:23,  7.36it/s]11/28/2021 00:58:53 - INFO - __main__ -   Batch number = 107
Evaluating:  38%|███▊      | 107/282 [00:14<00:23,  7.37it/s]11/28/2021 00:58:53 - INFO - __main__ -   Batch number = 108
Evaluating:  38%|███▊      | 108/282 [00:14<00:23,  7.38it/s]11/28/2021 00:58:53 - INFO - __main__ -   Batch number = 109
Evaluating:  39%|███▊      | 109/282 [00:14<00:23,  7.37it/s]11/28/2021 00:58:54 - INFO - __main__ -   Batch number = 110
Evaluating:  39%|███▉      | 110/282 [00:14<00:23,  7.37it/s]11/28/2021 00:58:54 - INFO - __main__ -   Batch number = 111
Evaluating:  39%|███▉      | 111/282 [00:14<00:23,  7.36it/s]11/28/2021 00:58:54 - INFO - __main__ -   Batch number = 112
Evaluating:  40%|███▉      | 112/282 [00:14<00:23,  7.36it/s]11/28/2021 00:58:54 - INFO - __main__ -   Batch number = 113
Evaluating:  40%|████      | 113/282 [00:14<00:22,  7.35it/s]11/28/2021 00:58:54 - INFO - __main__ -   Batch number = 114
Evaluating:  40%|████      | 114/282 [00:15<00:26,  6.31it/s]11/28/2021 00:58:54 - INFO - __main__ -   Batch number = 115
Evaluating:  41%|████      | 115/282 [00:15<00:25,  6.60it/s]11/28/2021 00:58:54 - INFO - __main__ -   Batch number = 116
Evaluating:  41%|████      | 116/282 [00:15<00:24,  6.81it/s]11/28/2021 00:58:55 - INFO - __main__ -   Batch number = 117
Evaluating:  41%|████▏     | 117/282 [00:15<00:23,  6.95it/s]11/28/2021 00:58:55 - INFO - __main__ -   Batch number = 118
Evaluating:  42%|████▏     | 118/282 [00:15<00:23,  7.07it/s]11/28/2021 00:58:55 - INFO - __main__ -   Batch number = 119
Evaluating:  42%|████▏     | 119/282 [00:15<00:22,  7.14it/s]11/28/2021 00:58:55 - INFO - __main__ -   Batch number = 120
Evaluating:  43%|████▎     | 120/282 [00:15<00:22,  7.21it/s]11/28/2021 00:58:55 - INFO - __main__ -   Batch number = 121
Evaluating:  43%|████▎     | 121/282 [00:16<00:22,  7.24it/s]11/28/2021 00:58:55 - INFO - __main__ -   Batch number = 122
Evaluating:  43%|████▎     | 122/282 [00:16<00:22,  7.26it/s]11/28/2021 00:58:55 - INFO - __main__ -   Batch number = 123
Evaluating:  44%|████▎     | 123/282 [00:16<00:21,  7.28it/s]11/28/2021 00:58:56 - INFO - __main__ -   Batch number = 124
Evaluating:  44%|████▍     | 124/282 [00:16<00:21,  7.30it/s]11/28/2021 00:58:56 - INFO - __main__ -   Batch number = 125
Evaluating:  44%|████▍     | 125/282 [00:16<00:21,  7.29it/s]11/28/2021 00:58:56 - INFO - __main__ -   Batch number = 126
Evaluating:  45%|████▍     | 126/282 [00:16<00:21,  7.30it/s]11/28/2021 00:58:56 - INFO - __main__ -   Batch number = 127
Evaluating:  45%|████▌     | 127/282 [00:16<00:21,  7.29it/s]11/28/2021 00:58:56 - INFO - __main__ -   Batch number = 128
Evaluating:  45%|████▌     | 128/282 [00:17<00:21,  7.31it/s]11/28/2021 00:58:56 - INFO - __main__ -   Batch number = 129
Evaluating:  46%|████▌     | 129/282 [00:17<00:20,  7.29it/s]11/28/2021 00:58:56 - INFO - __main__ -   Batch number = 130
Evaluating:  46%|████▌     | 130/282 [00:17<00:20,  7.30it/s]11/28/2021 00:58:56 - INFO - __main__ -   Batch number = 131
Evaluating:  46%|████▋     | 131/282 [00:17<00:20,  7.29it/s]11/28/2021 00:58:57 - INFO - __main__ -   Batch number = 132
Evaluating:  47%|████▋     | 132/282 [00:17<00:20,  7.30it/s]11/28/2021 00:58:57 - INFO - __main__ -   Batch number = 133
Evaluating:  47%|████▋     | 133/282 [00:17<00:20,  7.29it/s]11/28/2021 00:58:57 - INFO - __main__ -   Batch number = 134
Evaluating:  48%|████▊     | 134/282 [00:17<00:20,  7.28it/s]11/28/2021 00:58:57 - INFO - __main__ -   Batch number = 135
Evaluating:  48%|████▊     | 135/282 [00:18<00:20,  7.27it/s]11/28/2021 00:58:57 - INFO - __main__ -   Batch number = 136
Evaluating:  48%|████▊     | 136/282 [00:18<00:20,  7.28it/s]11/28/2021 00:58:57 - INFO - __main__ -   Batch number = 137
Evaluating:  49%|████▊     | 137/282 [00:18<00:19,  7.27it/s]11/28/2021 00:58:57 - INFO - __main__ -   Batch number = 138
Evaluating:  49%|████▉     | 138/282 [00:18<00:19,  7.26it/s]11/28/2021 00:58:58 - INFO - __main__ -   Batch number = 139
Evaluating:  49%|████▉     | 139/282 [00:18<00:19,  7.25it/s]11/28/2021 00:58:58 - INFO - __main__ -   Batch number = 140
Evaluating:  50%|████▉     | 140/282 [00:18<00:19,  7.25it/s]11/28/2021 00:58:58 - INFO - __main__ -   Batch number = 141
Evaluating:  50%|█████     | 141/282 [00:18<00:19,  7.24it/s]11/28/2021 00:58:58 - INFO - __main__ -   Batch number = 142
Evaluating:  50%|█████     | 142/282 [00:19<00:19,  7.25it/s]11/28/2021 00:58:58 - INFO - __main__ -   Batch number = 143
Evaluating:  51%|█████     | 143/282 [00:19<00:19,  7.24it/s]11/28/2021 00:58:58 - INFO - __main__ -   Batch number = 144
Evaluating:  51%|█████     | 144/282 [00:19<00:19,  7.13it/s]11/28/2021 00:58:58 - INFO - __main__ -   Batch number = 145
Evaluating:  51%|█████▏    | 145/282 [00:19<00:19,  7.17it/s]11/28/2021 00:58:59 - INFO - __main__ -   Batch number = 146
Evaluating:  52%|█████▏    | 146/282 [00:19<00:18,  7.19it/s]11/28/2021 00:58:59 - INFO - __main__ -   Batch number = 147
Evaluating:  52%|█████▏    | 147/282 [00:19<00:19,  7.08it/s]11/28/2021 00:58:59 - INFO - __main__ -   Batch number = 148
Evaluating:  52%|█████▏    | 148/282 [00:19<00:18,  7.12it/s]11/28/2021 00:58:59 - INFO - __main__ -   Batch number = 149
Evaluating:  53%|█████▎    | 149/282 [00:19<00:18,  7.14it/s]11/28/2021 00:58:59 - INFO - __main__ -   Batch number = 150
Evaluating:  53%|█████▎    | 150/282 [00:20<00:21,  6.09it/s]11/28/2021 00:58:59 - INFO - __main__ -   Batch number = 151
Evaluating:  54%|█████▎    | 151/282 [00:20<00:20,  6.38it/s]11/28/2021 00:58:59 - INFO - __main__ -   Batch number = 152
Evaluating:  54%|█████▍    | 152/282 [00:20<00:19,  6.60it/s]11/28/2021 00:59:00 - INFO - __main__ -   Batch number = 153
Evaluating:  54%|█████▍    | 153/282 [00:20<00:19,  6.66it/s]11/28/2021 00:59:00 - INFO - __main__ -   Batch number = 154
Evaluating:  55%|█████▍    | 154/282 [00:20<00:18,  6.82it/s]11/28/2021 00:59:00 - INFO - __main__ -   Batch number = 155
Evaluating:  55%|█████▍    | 155/282 [00:20<00:18,  6.82it/s]11/28/2021 00:59:00 - INFO - __main__ -   Batch number = 156
Evaluating:  55%|█████▌    | 156/282 [00:21<00:18,  6.93it/s]11/28/2021 00:59:00 - INFO - __main__ -   Batch number = 157
Evaluating:  56%|█████▌    | 157/282 [00:21<00:18,  6.89it/s]11/28/2021 00:59:00 - INFO - __main__ -   Batch number = 158
Evaluating:  56%|█████▌    | 158/282 [00:21<00:17,  6.97it/s]11/28/2021 00:59:00 - INFO - __main__ -   Batch number = 159
Evaluating:  56%|█████▋    | 159/282 [00:21<00:17,  6.91it/s]11/28/2021 00:59:01 - INFO - __main__ -   Batch number = 160
Evaluating:  57%|█████▋    | 160/282 [00:21<00:17,  6.99it/s]11/28/2021 00:59:01 - INFO - __main__ -   Batch number = 161
Evaluating:  57%|█████▋    | 161/282 [00:21<00:17,  6.93it/s]11/28/2021 00:59:01 - INFO - __main__ -   Batch number = 162
Evaluating:  57%|█████▋    | 162/282 [00:21<00:17,  7.00it/s]11/28/2021 00:59:01 - INFO - __main__ -   Batch number = 163
Evaluating:  58%|█████▊    | 163/282 [00:22<00:17,  6.92it/s]11/28/2021 00:59:01 - INFO - __main__ -   Batch number = 164
Evaluating:  58%|█████▊    | 164/282 [00:22<00:16,  6.99it/s]11/28/2021 00:59:01 - INFO - __main__ -   Batch number = 165
Evaluating:  59%|█████▊    | 165/282 [00:22<00:16,  6.91it/s]11/28/2021 00:59:01 - INFO - __main__ -   Batch number = 166
Evaluating:  59%|█████▉    | 166/282 [00:22<00:16,  6.94it/s]11/28/2021 00:59:02 - INFO - __main__ -   Batch number = 167
Evaluating:  59%|█████▉    | 167/282 [00:22<00:16,  6.87it/s]11/28/2021 00:59:02 - INFO - __main__ -   Batch number = 168
Evaluating:  60%|█████▉    | 168/282 [00:22<00:16,  6.95it/s]11/28/2021 00:59:02 - INFO - __main__ -   Batch number = 169
Evaluating:  60%|█████▉    | 169/282 [00:22<00:16,  6.87it/s]11/28/2021 00:59:02 - INFO - __main__ -   Batch number = 170
Evaluating:  60%|██████    | 170/282 [00:23<00:16,  6.96it/s]11/28/2021 00:59:02 - INFO - __main__ -   Batch number = 171
Evaluating:  61%|██████    | 171/282 [00:23<00:16,  6.88it/s]11/28/2021 00:59:02 - INFO - __main__ -   Batch number = 172
Evaluating:  61%|██████    | 172/282 [00:23<00:15,  6.95it/s]11/28/2021 00:59:02 - INFO - __main__ -   Batch number = 173
Evaluating:  61%|██████▏   | 173/282 [00:23<00:15,  6.86it/s]11/28/2021 00:59:03 - INFO - __main__ -   Batch number = 174
Evaluating:  62%|██████▏   | 174/282 [00:23<00:15,  6.94it/s]11/28/2021 00:59:03 - INFO - __main__ -   Batch number = 175
Evaluating:  62%|██████▏   | 175/282 [00:23<00:15,  6.86it/s]11/28/2021 00:59:03 - INFO - __main__ -   Batch number = 176
Evaluating:  62%|██████▏   | 176/282 [00:23<00:15,  6.92it/s]11/28/2021 00:59:03 - INFO - __main__ -   Batch number = 177
Evaluating:  63%|██████▎   | 177/282 [00:24<00:15,  6.85it/s]11/28/2021 00:59:03 - INFO - __main__ -   Batch number = 178
Evaluating:  63%|██████▎   | 178/282 [00:24<00:14,  6.94it/s]11/28/2021 00:59:03 - INFO - __main__ -   Batch number = 179
Evaluating:  63%|██████▎   | 179/282 [00:24<00:15,  6.86it/s]11/28/2021 00:59:04 - INFO - __main__ -   Batch number = 180
Evaluating:  64%|██████▍   | 180/282 [00:24<00:14,  6.93it/s]11/28/2021 00:59:04 - INFO - __main__ -   Batch number = 181
Evaluating:  64%|██████▍   | 181/282 [00:24<00:14,  6.84it/s]11/28/2021 00:59:04 - INFO - __main__ -   Batch number = 182
Evaluating:  65%|██████▍   | 182/282 [00:24<00:14,  6.92it/s]11/28/2021 00:59:04 - INFO - __main__ -   Batch number = 183
Evaluating:  65%|██████▍   | 183/282 [00:24<00:14,  6.83it/s]11/28/2021 00:59:04 - INFO - __main__ -   Batch number = 184
Evaluating:  65%|██████▌   | 184/282 [00:25<00:15,  6.48it/s]11/28/2021 00:59:04 - INFO - __main__ -   Batch number = 185
Evaluating:  66%|██████▌   | 185/282 [00:25<00:14,  6.54it/s]11/28/2021 00:59:04 - INFO - __main__ -   Batch number = 186
Evaluating:  66%|██████▌   | 186/282 [00:25<00:14,  6.70it/s]11/28/2021 00:59:05 - INFO - __main__ -   Batch number = 187
Evaluating:  66%|██████▋   | 187/282 [00:25<00:14,  6.68it/s]11/28/2021 00:59:05 - INFO - __main__ -   Batch number = 188
Evaluating:  67%|██████▋   | 188/282 [00:25<00:13,  6.80it/s]11/28/2021 00:59:05 - INFO - __main__ -   Batch number = 189
Evaluating:  67%|██████▋   | 189/282 [00:25<00:13,  6.75it/s]11/28/2021 00:59:05 - INFO - __main__ -   Batch number = 190
Evaluating:  67%|██████▋   | 190/282 [00:26<00:13,  6.84it/s]11/28/2021 00:59:05 - INFO - __main__ -   Batch number = 191
Evaluating:  68%|██████▊   | 191/282 [00:26<00:13,  6.77it/s]11/28/2021 00:59:05 - INFO - __main__ -   Batch number = 192
Evaluating:  68%|██████▊   | 192/282 [00:26<00:13,  6.86it/s]11/28/2021 00:59:05 - INFO - __main__ -   Batch number = 193
Evaluating:  68%|██████▊   | 193/282 [00:26<00:13,  6.80it/s]11/28/2021 00:59:06 - INFO - __main__ -   Batch number = 194
Evaluating:  69%|██████▉   | 194/282 [00:26<00:12,  6.88it/s]11/28/2021 00:59:06 - INFO - __main__ -   Batch number = 195
Evaluating:  69%|██████▉   | 195/282 [00:26<00:12,  6.79it/s]11/28/2021 00:59:06 - INFO - __main__ -   Batch number = 196
Evaluating:  70%|██████▉   | 196/282 [00:26<00:12,  6.86it/s]11/28/2021 00:59:06 - INFO - __main__ -   Batch number = 197
Evaluating:  70%|██████▉   | 197/282 [00:27<00:12,  6.77it/s]11/28/2021 00:59:06 - INFO - __main__ -   Batch number = 198
Evaluating:  70%|███████   | 198/282 [00:27<00:12,  6.83it/s]11/28/2021 00:59:06 - INFO - __main__ -   Batch number = 199
Evaluating:  71%|███████   | 199/282 [00:27<00:12,  6.74it/s]11/28/2021 00:59:06 - INFO - __main__ -   Batch number = 200
Evaluating:  71%|███████   | 200/282 [00:27<00:11,  6.84it/s]11/28/2021 00:59:07 - INFO - __main__ -   Batch number = 201
Evaluating:  71%|███████▏  | 201/282 [00:27<00:11,  6.76it/s]11/28/2021 00:59:07 - INFO - __main__ -   Batch number = 202
Evaluating:  72%|███████▏  | 202/282 [00:27<00:11,  6.84it/s]11/28/2021 00:59:07 - INFO - __main__ -   Batch number = 203
Evaluating:  72%|███████▏  | 203/282 [00:27<00:11,  6.74it/s]11/28/2021 00:59:07 - INFO - __main__ -   Batch number = 204
Evaluating:  72%|███████▏  | 204/282 [00:28<00:11,  6.83it/s]11/28/2021 00:59:07 - INFO - __main__ -   Batch number = 205
Evaluating:  73%|███████▎  | 205/282 [00:28<00:11,  6.72it/s]11/28/2021 00:59:07 - INFO - __main__ -   Batch number = 206
Evaluating:  73%|███████▎  | 206/282 [00:28<00:11,  6.81it/s]11/28/2021 00:59:07 - INFO - __main__ -   Batch number = 207
Evaluating:  73%|███████▎  | 207/282 [00:28<00:11,  6.74it/s]11/28/2021 00:59:08 - INFO - __main__ -   Batch number = 208
Evaluating:  74%|███████▍  | 208/282 [00:28<00:10,  6.82it/s]11/28/2021 00:59:08 - INFO - __main__ -   Batch number = 209
Evaluating:  74%|███████▍  | 209/282 [00:28<00:10,  6.72it/s]11/28/2021 00:59:08 - INFO - __main__ -   Batch number = 210
Evaluating:  74%|███████▍  | 210/282 [00:28<00:10,  6.80it/s]11/28/2021 00:59:08 - INFO - __main__ -   Batch number = 211
Evaluating:  75%|███████▍  | 211/282 [00:29<00:10,  6.70it/s]11/28/2021 00:59:08 - INFO - __main__ -   Batch number = 212
Evaluating:  75%|███████▌  | 212/282 [00:29<00:10,  6.80it/s]11/28/2021 00:59:08 - INFO - __main__ -   Batch number = 213
Evaluating:  76%|███████▌  | 213/282 [00:29<00:10,  6.69it/s]11/28/2021 00:59:09 - INFO - __main__ -   Batch number = 214
Evaluating:  76%|███████▌  | 214/282 [00:29<00:10,  6.75it/s]11/28/2021 00:59:09 - INFO - __main__ -   Batch number = 215
Evaluating:  76%|███████▌  | 215/282 [00:29<00:10,  6.68it/s]11/28/2021 00:59:09 - INFO - __main__ -   Batch number = 216
Evaluating:  77%|███████▋  | 216/282 [00:29<00:09,  6.76it/s]11/28/2021 00:59:09 - INFO - __main__ -   Batch number = 217
Evaluating:  77%|███████▋  | 217/282 [00:30<00:09,  6.68it/s]11/28/2021 00:59:09 - INFO - __main__ -   Batch number = 218
Evaluating:  77%|███████▋  | 218/282 [00:30<00:09,  6.56it/s]11/28/2021 00:59:09 - INFO - __main__ -   Batch number = 219
Evaluating:  78%|███████▊  | 219/282 [00:30<00:09,  6.53it/s]11/28/2021 00:59:09 - INFO - __main__ -   Batch number = 220
Evaluating:  78%|███████▊  | 220/282 [00:30<00:09,  6.64it/s]11/28/2021 00:59:10 - INFO - __main__ -   Batch number = 221
Evaluating:  78%|███████▊  | 221/282 [00:30<00:09,  6.60it/s]11/28/2021 00:59:10 - INFO - __main__ -   Batch number = 222
Evaluating:  79%|███████▊  | 222/282 [00:30<00:08,  6.70it/s]11/28/2021 00:59:10 - INFO - __main__ -   Batch number = 223
Evaluating:  79%|███████▉  | 223/282 [00:30<00:08,  6.61it/s]11/28/2021 00:59:10 - INFO - __main__ -   Batch number = 224
Evaluating:  79%|███████▉  | 224/282 [00:31<00:08,  6.72it/s]11/28/2021 00:59:10 - INFO - __main__ -   Batch number = 225
Evaluating:  80%|███████▉  | 225/282 [00:31<00:08,  6.63it/s]11/28/2021 00:59:10 - INFO - __main__ -   Batch number = 226
Evaluating:  80%|████████  | 226/282 [00:31<00:08,  6.73it/s]11/28/2021 00:59:10 - INFO - __main__ -   Batch number = 227
Evaluating:  80%|████████  | 227/282 [00:31<00:08,  6.63it/s]11/28/2021 00:59:11 - INFO - __main__ -   Batch number = 228
Evaluating:  81%|████████  | 228/282 [00:31<00:08,  6.72it/s]11/28/2021 00:59:11 - INFO - __main__ -   Batch number = 229
Evaluating:  81%|████████  | 229/282 [00:31<00:08,  6.62it/s]11/28/2021 00:59:11 - INFO - __main__ -   Batch number = 230
Evaluating:  82%|████████▏ | 230/282 [00:31<00:07,  6.72it/s]11/28/2021 00:59:11 - INFO - __main__ -   Batch number = 231
Evaluating:  82%|████████▏ | 231/282 [00:32<00:07,  6.62it/s]11/28/2021 00:59:11 - INFO - __main__ -   Batch number = 232
Evaluating:  82%|████████▏ | 232/282 [00:32<00:07,  6.71it/s]11/28/2021 00:59:11 - INFO - __main__ -   Batch number = 233
Evaluating:  83%|████████▎ | 233/282 [00:32<00:07,  6.62it/s]11/28/2021 00:59:12 - INFO - __main__ -   Batch number = 234
Evaluating:  83%|████████▎ | 234/282 [00:32<00:07,  6.71it/s]11/28/2021 00:59:12 - INFO - __main__ -   Batch number = 235
Evaluating:  83%|████████▎ | 235/282 [00:32<00:07,  6.61it/s]11/28/2021 00:59:12 - INFO - __main__ -   Batch number = 236
Evaluating:  84%|████████▎ | 236/282 [00:32<00:06,  6.70it/s]11/28/2021 00:59:12 - INFO - __main__ -   Batch number = 237
Evaluating:  84%|████████▍ | 237/282 [00:33<00:06,  6.61it/s]11/28/2021 00:59:12 - INFO - __main__ -   Batch number = 238
Evaluating:  84%|████████▍ | 238/282 [00:33<00:06,  6.71it/s]11/28/2021 00:59:12 - INFO - __main__ -   Batch number = 239
Evaluating:  85%|████████▍ | 239/282 [00:33<00:06,  6.61it/s]11/28/2021 00:59:12 - INFO - __main__ -   Batch number = 240
Evaluating:  85%|████████▌ | 240/282 [00:33<00:06,  6.68it/s]11/28/2021 00:59:13 - INFO - __main__ -   Batch number = 241
Evaluating:  85%|████████▌ | 241/282 [00:33<00:06,  6.58it/s]11/28/2021 00:59:13 - INFO - __main__ -   Batch number = 242
Evaluating:  86%|████████▌ | 242/282 [00:33<00:06,  6.66it/s]11/28/2021 00:59:13 - INFO - __main__ -   Batch number = 243
Evaluating:  86%|████████▌ | 243/282 [00:33<00:06,  6.50it/s]11/28/2021 00:59:13 - INFO - __main__ -   Batch number = 244
Evaluating:  87%|████████▋ | 244/282 [00:34<00:05,  6.46it/s]11/28/2021 00:59:13 - INFO - __main__ -   Batch number = 245
Evaluating:  87%|████████▋ | 245/282 [00:34<00:05,  6.42it/s]11/28/2021 00:59:13 - INFO - __main__ -   Batch number = 246
Evaluating:  87%|████████▋ | 246/282 [00:34<00:05,  6.56it/s]11/28/2021 00:59:14 - INFO - __main__ -   Batch number = 247
Evaluating:  88%|████████▊ | 247/282 [00:34<00:05,  6.50it/s]11/28/2021 00:59:14 - INFO - __main__ -   Batch number = 248
Evaluating:  88%|████████▊ | 248/282 [00:34<00:05,  6.60it/s]11/28/2021 00:59:14 - INFO - __main__ -   Batch number = 249
Evaluating:  88%|████████▊ | 249/282 [00:34<00:05,  6.52it/s]11/28/2021 00:59:14 - INFO - __main__ -   Batch number = 250
Evaluating:  89%|████████▊ | 250/282 [00:35<00:04,  6.61it/s]11/28/2021 00:59:14 - INFO - __main__ -   Batch number = 251
Evaluating:  89%|████████▉ | 251/282 [00:35<00:04,  6.53it/s]11/28/2021 00:59:14 - INFO - __main__ -   Batch number = 252
Evaluating:  89%|████████▉ | 252/282 [00:35<00:04,  6.64it/s]11/28/2021 00:59:14 - INFO - __main__ -   Batch number = 253
Evaluating:  90%|████████▉ | 253/282 [00:35<00:04,  6.54it/s]11/28/2021 00:59:15 - INFO - __main__ -   Batch number = 254
Evaluating:  90%|█████████ | 254/282 [00:35<00:04,  6.64it/s]11/28/2021 00:59:15 - INFO - __main__ -   Batch number = 255
Evaluating:  90%|█████████ | 255/282 [00:35<00:04,  6.53it/s]11/28/2021 00:59:15 - INFO - __main__ -   Batch number = 256
Evaluating:  91%|█████████ | 256/282 [00:35<00:03,  6.63it/s]11/28/2021 00:59:15 - INFO - __main__ -   Batch number = 257
Evaluating:  91%|█████████ | 257/282 [00:36<00:03,  6.53it/s]11/28/2021 00:59:15 - INFO - __main__ -   Batch number = 258
Evaluating:  91%|█████████▏| 258/282 [00:36<00:03,  6.63it/s]11/28/2021 00:59:15 - INFO - __main__ -   Batch number = 259
Evaluating:  92%|█████████▏| 259/282 [00:36<00:03,  6.52it/s]11/28/2021 00:59:16 - INFO - __main__ -   Batch number = 260
Evaluating:  92%|█████████▏| 260/282 [00:36<00:03,  6.62it/s]11/28/2021 00:59:16 - INFO - __main__ -   Batch number = 261
Evaluating:  93%|█████████▎| 261/282 [00:36<00:03,  6.50it/s]11/28/2021 00:59:16 - INFO - __main__ -   Batch number = 262
Evaluating:  93%|█████████▎| 262/282 [00:36<00:03,  6.60it/s]11/28/2021 00:59:16 - INFO - __main__ -   Batch number = 263
Evaluating:  93%|█████████▎| 263/282 [00:36<00:02,  6.50it/s]11/28/2021 00:59:16 - INFO - __main__ -   Batch number = 264
Evaluating:  94%|█████████▎| 264/282 [00:37<00:02,  6.61it/s]11/28/2021 00:59:16 - INFO - __main__ -   Batch number = 265
Evaluating:  94%|█████████▍| 265/282 [00:37<00:02,  6.50it/s]11/28/2021 00:59:16 - INFO - __main__ -   Batch number = 266
Evaluating:  94%|█████████▍| 266/282 [00:37<00:02,  6.59it/s]11/28/2021 00:59:17 - INFO - __main__ -   Batch number = 267
Evaluating:  95%|█████████▍| 267/282 [00:37<00:02,  6.47it/s]11/28/2021 00:59:17 - INFO - __main__ -   Batch number = 268
Evaluating:  95%|█████████▌| 268/282 [00:37<00:02,  6.58it/s]11/28/2021 00:59:17 - INFO - __main__ -   Batch number = 269
Evaluating:  95%|█████████▌| 269/282 [00:37<00:02,  6.47it/s]11/28/2021 00:59:17 - INFO - __main__ -   Batch number = 270
Evaluating:  96%|█████████▌| 270/282 [00:38<00:01,  6.57it/s]11/28/2021 00:59:17 - INFO - __main__ -   Batch number = 271
Evaluating:  96%|█████████▌| 271/282 [00:38<00:01,  6.46it/s]11/28/2021 00:59:17 - INFO - __main__ -   Batch number = 272
Evaluating:  96%|█████████▋| 272/282 [00:38<00:01,  6.56it/s]11/28/2021 00:59:17 - INFO - __main__ -   Batch number = 273
Evaluating:  97%|█████████▋| 273/282 [00:38<00:01,  6.31it/s]11/28/2021 00:59:18 - INFO - __main__ -   Batch number = 274
Evaluating:  97%|█████████▋| 274/282 [00:38<00:01,  6.45it/s]11/28/2021 00:59:18 - INFO - __main__ -   Batch number = 275
Evaluating:  98%|█████████▊| 275/282 [00:38<00:01,  6.37it/s]11/28/2021 00:59:18 - INFO - __main__ -   Batch number = 276
Evaluating:  98%|█████████▊| 276/282 [00:38<00:00,  6.49it/s]11/28/2021 00:59:18 - INFO - __main__ -   Batch number = 277
Evaluating:  98%|█████████▊| 277/282 [00:39<00:00,  6.38it/s]11/28/2021 00:59:18 - INFO - __main__ -   Batch number = 278
Evaluating:  99%|█████████▊| 278/282 [00:39<00:00,  6.49it/s]11/28/2021 00:59:18 - INFO - __main__ -   Batch number = 279
Evaluating:  99%|█████████▉| 279/282 [00:39<00:00,  6.40it/s]11/28/2021 00:59:19 - INFO - __main__ -   Batch number = 280
Evaluating:  99%|█████████▉| 280/282 [00:39<00:00,  6.51it/s]11/28/2021 00:59:19 - INFO - __main__ -   Batch number = 281
Evaluating: 100%|█████████▉| 281/282 [00:39<00:00,  6.40it/s]11/28/2021 00:59:19 - INFO - __main__ -   Batch number = 282
Evaluating: 100%|██████████| 282/282 [00:39<00:00,  7.08it/s]
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 00:59:23 - INFO - __main__ -   ***** Evaluation result  in ru *****
11/28/2021 00:59:23 - INFO - __main__ -     f1 = 0.8639662697250804
11/28/2021 00:59:23 - INFO - __main__ -     loss = 0.5335670350291205
11/28/2021 00:59:23 - INFO - __main__ -     precision = 0.8676274223565958
11/28/2021 00:59:23 - INFO - __main__ -     recall = 0.8603358854038035
56.94user 14.61system 1:12.04elapsed 99%CPU (0avgtext+0avgdata 3954512maxresident)k
6432inputs+32016outputs (0major+1862599minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 00:59:25 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ru', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 00:59:25 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 00:59:25 - INFO - __main__ -   Seed = 2
11/28/2021 00:59:25 - INFO - root -   save model
11/28/2021 00:59:25 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ru', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 00:59:25 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 00:59:28 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 00:59:34 - INFO - __main__ -   Using lang2id = None
11/28/2021 00:59:34 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 00:59:34 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
11/28/2021 00:59:34 - INFO - root -   Trying to decide if add adapter
11/28/2021 00:59:34 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 00:59:34 - INFO - root -   loading lang adpater am/wiki@ukp
11/28/2021 00:59:34 - INFO - __main__ -   Adapter Languages : ['am'], Length : 1
11/28/2021 00:59:34 - INFO - __main__ -   Adapter Names ['am/wiki@ukp'], Length : 1
11/28/2021 00:59:34 - INFO - __main__ -   Language = am
11/28/2021 00:59:34 - INFO - __main__ -   Adapter Name = am/wiki@ukp
Found matching adapter at: adapters/ukp/bert-base-multilingual-cased_am_wiki_pfeiffer.json
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/am/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_am_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/adapter_config.json
Adding adapter 'am' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/head_config.json
11/28/2021 00:59:40 - INFO - __main__ -   Language adapter for ru not found, using am instead
11/28/2021 00:59:40 - INFO - __main__ -   Set active language adapter to am
11/28/2021 00:59:40 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 00:59:40 - INFO - __main__ -   Adapter Languages = ['am']
11/28/2021 00:59:40 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_ru_bert-base-multilingual-cased_128
11/28/2021 00:59:41 - INFO - __main__ -   ***** Running evaluation  in ru *****
11/28/2021 00:59:41 - INFO - __main__ -     Num examples = 8995
11/28/2021 00:59:41 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/282 [00:00<?, ?it/s]11/28/2021 00:59:41 - INFO - __main__ -   Batch number = 1
Evaluating:   0%|          | 1/282 [00:00<00:44,  6.31it/s]11/28/2021 00:59:41 - INFO - __main__ -   Batch number = 2
Evaluating:   1%|          | 2/282 [00:00<00:40,  6.97it/s]11/28/2021 00:59:41 - INFO - __main__ -   Batch number = 3
Evaluating:   1%|          | 3/282 [00:00<00:38,  7.32it/s]11/28/2021 00:59:41 - INFO - __main__ -   Batch number = 4
Evaluating:   1%|▏         | 4/282 [00:00<00:36,  7.51it/s]11/28/2021 00:59:42 - INFO - __main__ -   Batch number = 5
Evaluating:   2%|▏         | 5/282 [00:00<00:36,  7.60it/s]11/28/2021 00:59:42 - INFO - __main__ -   Batch number = 6
Evaluating:   2%|▏         | 6/282 [00:00<00:36,  7.64it/s]11/28/2021 00:59:42 - INFO - __main__ -   Batch number = 7
Evaluating:   2%|▏         | 7/282 [00:00<00:35,  7.65it/s]11/28/2021 00:59:42 - INFO - __main__ -   Batch number = 8
Evaluating:   3%|▎         | 8/282 [00:01<00:35,  7.68it/s]11/28/2021 00:59:42 - INFO - __main__ -   Batch number = 9
Evaluating:   3%|▎         | 9/282 [00:01<00:35,  7.69it/s]11/28/2021 00:59:42 - INFO - __main__ -   Batch number = 10
Evaluating:   4%|▎         | 10/282 [00:01<00:35,  7.67it/s]11/28/2021 00:59:42 - INFO - __main__ -   Batch number = 11
Evaluating:   4%|▍         | 11/282 [00:01<00:35,  7.65it/s]11/28/2021 00:59:42 - INFO - __main__ -   Batch number = 12
Evaluating:   4%|▍         | 12/282 [00:01<00:35,  7.68it/s]11/28/2021 00:59:43 - INFO - __main__ -   Batch number = 13
Evaluating:   5%|▍         | 13/282 [00:01<00:34,  7.69it/s]11/28/2021 00:59:43 - INFO - __main__ -   Batch number = 14
Evaluating:   5%|▍         | 14/282 [00:01<00:34,  7.70it/s]11/28/2021 00:59:43 - INFO - __main__ -   Batch number = 15
Evaluating:   5%|▌         | 15/282 [00:01<00:34,  7.69it/s]11/28/2021 00:59:43 - INFO - __main__ -   Batch number = 16
Evaluating:   6%|▌         | 16/282 [00:02<00:34,  7.70it/s]11/28/2021 00:59:43 - INFO - __main__ -   Batch number = 17
Evaluating:   6%|▌         | 17/282 [00:02<00:34,  7.68it/s]11/28/2021 00:59:43 - INFO - __main__ -   Batch number = 18
Evaluating:   6%|▋         | 18/282 [00:02<00:34,  7.67it/s]11/28/2021 00:59:43 - INFO - __main__ -   Batch number = 19
Evaluating:   7%|▋         | 19/282 [00:02<00:34,  7.66it/s]11/28/2021 00:59:44 - INFO - __main__ -   Batch number = 20
Evaluating:   7%|▋         | 20/282 [00:02<00:34,  7.66it/s]11/28/2021 00:59:44 - INFO - __main__ -   Batch number = 21
Evaluating:   7%|▋         | 21/282 [00:02<00:34,  7.65it/s]11/28/2021 00:59:44 - INFO - __main__ -   Batch number = 22
Evaluating:   8%|▊         | 22/282 [00:02<00:34,  7.64it/s]11/28/2021 00:59:44 - INFO - __main__ -   Batch number = 23
Evaluating:   8%|▊         | 23/282 [00:03<00:33,  7.62it/s]11/28/2021 00:59:44 - INFO - __main__ -   Batch number = 24
Evaluating:   9%|▊         | 24/282 [00:03<00:33,  7.64it/s]11/28/2021 00:59:44 - INFO - __main__ -   Batch number = 25
Evaluating:   9%|▉         | 25/282 [00:03<00:33,  7.65it/s]11/28/2021 00:59:44 - INFO - __main__ -   Batch number = 26
Evaluating:   9%|▉         | 26/282 [00:03<00:33,  7.64it/s]11/28/2021 00:59:44 - INFO - __main__ -   Batch number = 27
Evaluating:  10%|▉         | 27/282 [00:03<00:33,  7.64it/s]11/28/2021 00:59:45 - INFO - __main__ -   Batch number = 28
Evaluating:  10%|▉         | 28/282 [00:03<00:33,  7.63it/s]11/28/2021 00:59:45 - INFO - __main__ -   Batch number = 29
Evaluating:  10%|█         | 29/282 [00:03<00:33,  7.61it/s]11/28/2021 00:59:45 - INFO - __main__ -   Batch number = 30
Evaluating:  11%|█         | 30/282 [00:03<00:33,  7.62it/s]11/28/2021 00:59:45 - INFO - __main__ -   Batch number = 31
Evaluating:  11%|█         | 31/282 [00:04<00:32,  7.61it/s]11/28/2021 00:59:45 - INFO - __main__ -   Batch number = 32
Evaluating:  11%|█▏        | 32/282 [00:04<00:32,  7.59it/s]11/28/2021 00:59:45 - INFO - __main__ -   Batch number = 33
Evaluating:  12%|█▏        | 33/282 [00:04<00:32,  7.60it/s]11/28/2021 00:59:45 - INFO - __main__ -   Batch number = 34
Evaluating:  12%|█▏        | 34/282 [00:04<00:32,  7.61it/s]11/28/2021 00:59:45 - INFO - __main__ -   Batch number = 35
Evaluating:  12%|█▏        | 35/282 [00:04<00:32,  7.59it/s]11/28/2021 00:59:46 - INFO - __main__ -   Batch number = 36
Evaluating:  13%|█▎        | 36/282 [00:04<00:32,  7.58it/s]11/28/2021 00:59:46 - INFO - __main__ -   Batch number = 37
Evaluating:  13%|█▎        | 37/282 [00:04<00:32,  7.59it/s]11/28/2021 00:59:46 - INFO - __main__ -   Batch number = 38
Evaluating:  13%|█▎        | 38/282 [00:04<00:32,  7.59it/s]11/28/2021 00:59:46 - INFO - __main__ -   Batch number = 39
Evaluating:  14%|█▍        | 39/282 [00:05<00:32,  7.58it/s]11/28/2021 00:59:46 - INFO - __main__ -   Batch number = 40
Evaluating:  14%|█▍        | 40/282 [00:05<00:31,  7.59it/s]11/28/2021 00:59:46 - INFO - __main__ -   Batch number = 41
Evaluating:  15%|█▍        | 41/282 [00:05<00:32,  7.49it/s]11/28/2021 00:59:46 - INFO - __main__ -   Batch number = 42
Evaluating:  15%|█▍        | 42/282 [00:05<00:32,  7.47it/s]11/28/2021 00:59:47 - INFO - __main__ -   Batch number = 43
Evaluating:  15%|█▌        | 43/282 [00:05<00:31,  7.51it/s]11/28/2021 00:59:47 - INFO - __main__ -   Batch number = 44
Evaluating:  16%|█▌        | 44/282 [00:05<00:31,  7.50it/s]11/28/2021 00:59:47 - INFO - __main__ -   Batch number = 45
Evaluating:  16%|█▌        | 45/282 [00:05<00:31,  7.52it/s]11/28/2021 00:59:47 - INFO - __main__ -   Batch number = 46
Evaluating:  16%|█▋        | 46/282 [00:06<00:31,  7.52it/s]11/28/2021 00:59:47 - INFO - __main__ -   Batch number = 47
Evaluating:  17%|█▋        | 47/282 [00:06<00:31,  7.51it/s]11/28/2021 00:59:47 - INFO - __main__ -   Batch number = 48
Evaluating:  17%|█▋        | 48/282 [00:06<00:31,  7.52it/s]11/28/2021 00:59:47 - INFO - __main__ -   Batch number = 49
Evaluating:  17%|█▋        | 49/282 [00:06<00:30,  7.53it/s]11/28/2021 00:59:47 - INFO - __main__ -   Batch number = 50
Evaluating:  18%|█▊        | 50/282 [00:06<00:30,  7.50it/s]11/28/2021 00:59:48 - INFO - __main__ -   Batch number = 51
Evaluating:  18%|█▊        | 51/282 [00:06<00:30,  7.50it/s]11/28/2021 00:59:48 - INFO - __main__ -   Batch number = 52
Evaluating:  18%|█▊        | 52/282 [00:06<00:30,  7.51it/s]11/28/2021 00:59:48 - INFO - __main__ -   Batch number = 53
Evaluating:  19%|█▉        | 53/282 [00:06<00:30,  7.49it/s]11/28/2021 00:59:48 - INFO - __main__ -   Batch number = 54
Evaluating:  19%|█▉        | 54/282 [00:07<00:30,  7.46it/s]11/28/2021 00:59:48 - INFO - __main__ -   Batch number = 55
Evaluating:  20%|█▉        | 55/282 [00:07<00:30,  7.47it/s]11/28/2021 00:59:48 - INFO - __main__ -   Batch number = 56
Evaluating:  20%|█▉        | 56/282 [00:07<00:30,  7.45it/s]11/28/2021 00:59:48 - INFO - __main__ -   Batch number = 57
Evaluating:  20%|██        | 57/282 [00:07<00:30,  7.48it/s]11/28/2021 00:59:49 - INFO - __main__ -   Batch number = 58
Evaluating:  21%|██        | 58/282 [00:07<00:29,  7.48it/s]11/28/2021 00:59:49 - INFO - __main__ -   Batch number = 59
Evaluating:  21%|██        | 59/282 [00:07<00:29,  7.48it/s]11/28/2021 00:59:49 - INFO - __main__ -   Batch number = 60
Evaluating:  21%|██▏       | 60/282 [00:07<00:29,  7.49it/s]11/28/2021 00:59:49 - INFO - __main__ -   Batch number = 61
Evaluating:  22%|██▏       | 61/282 [00:08<00:29,  7.47it/s]11/28/2021 00:59:49 - INFO - __main__ -   Batch number = 62
Evaluating:  22%|██▏       | 62/282 [00:08<00:35,  6.20it/s]11/28/2021 00:59:49 - INFO - __main__ -   Batch number = 63
Evaluating:  22%|██▏       | 63/282 [00:08<00:33,  6.52it/s]11/28/2021 00:59:49 - INFO - __main__ -   Batch number = 64
Evaluating:  23%|██▎       | 64/282 [00:08<00:32,  6.79it/s]11/28/2021 00:59:50 - INFO - __main__ -   Batch number = 65
Evaluating:  23%|██▎       | 65/282 [00:08<00:31,  6.96it/s]11/28/2021 00:59:50 - INFO - __main__ -   Batch number = 66
Evaluating:  23%|██▎       | 66/282 [00:08<00:30,  7.10it/s]11/28/2021 00:59:50 - INFO - __main__ -   Batch number = 67
Evaluating:  24%|██▍       | 67/282 [00:08<00:29,  7.20it/s]11/28/2021 00:59:50 - INFO - __main__ -   Batch number = 68
Evaluating:  24%|██▍       | 68/282 [00:09<00:29,  7.25it/s]11/28/2021 00:59:50 - INFO - __main__ -   Batch number = 69
Evaluating:  24%|██▍       | 69/282 [00:09<00:29,  7.31it/s]11/28/2021 00:59:50 - INFO - __main__ -   Batch number = 70
Evaluating:  25%|██▍       | 70/282 [00:09<00:28,  7.35it/s]11/28/2021 00:59:50 - INFO - __main__ -   Batch number = 71
Evaluating:  25%|██▌       | 71/282 [00:09<00:28,  7.38it/s]11/28/2021 00:59:51 - INFO - __main__ -   Batch number = 72
Evaluating:  26%|██▌       | 72/282 [00:09<00:28,  7.39it/s]11/28/2021 00:59:51 - INFO - __main__ -   Batch number = 73
Evaluating:  26%|██▌       | 73/282 [00:09<00:28,  7.39it/s]11/28/2021 00:59:51 - INFO - __main__ -   Batch number = 74
Evaluating:  26%|██▌       | 74/282 [00:09<00:28,  7.38it/s]11/28/2021 00:59:51 - INFO - __main__ -   Batch number = 75
Evaluating:  27%|██▋       | 75/282 [00:10<00:27,  7.40it/s]11/28/2021 00:59:51 - INFO - __main__ -   Batch number = 76
Evaluating:  27%|██▋       | 76/282 [00:10<00:27,  7.41it/s]11/28/2021 00:59:51 - INFO - __main__ -   Batch number = 77
Evaluating:  27%|██▋       | 77/282 [00:10<00:27,  7.40it/s]11/28/2021 00:59:51 - INFO - __main__ -   Batch number = 78
Evaluating:  28%|██▊       | 78/282 [00:10<00:27,  7.41it/s]11/28/2021 00:59:51 - INFO - __main__ -   Batch number = 79
Evaluating:  28%|██▊       | 79/282 [00:10<00:27,  7.39it/s]11/28/2021 00:59:52 - INFO - __main__ -   Batch number = 80
Evaluating:  28%|██▊       | 80/282 [00:10<00:27,  7.41it/s]11/28/2021 00:59:52 - INFO - __main__ -   Batch number = 81
Evaluating:  29%|██▊       | 81/282 [00:10<00:27,  7.42it/s]11/28/2021 00:59:52 - INFO - __main__ -   Batch number = 82
Evaluating:  29%|██▉       | 82/282 [00:10<00:27,  7.39it/s]11/28/2021 00:59:52 - INFO - __main__ -   Batch number = 83
Evaluating:  29%|██▉       | 83/282 [00:11<00:26,  7.39it/s]11/28/2021 00:59:52 - INFO - __main__ -   Batch number = 84
Evaluating:  30%|██▉       | 84/282 [00:11<00:26,  7.39it/s]11/28/2021 00:59:52 - INFO - __main__ -   Batch number = 85
Evaluating:  30%|███       | 85/282 [00:11<00:27,  7.29it/s]11/28/2021 00:59:52 - INFO - __main__ -   Batch number = 86
Evaluating:  30%|███       | 86/282 [00:11<00:26,  7.28it/s]11/28/2021 00:59:53 - INFO - __main__ -   Batch number = 87
Evaluating:  31%|███       | 87/282 [00:11<00:26,  7.28it/s]11/28/2021 00:59:53 - INFO - __main__ -   Batch number = 88
Evaluating:  31%|███       | 88/282 [00:11<00:26,  7.26it/s]11/28/2021 00:59:53 - INFO - __main__ -   Batch number = 89
Evaluating:  32%|███▏      | 89/282 [00:11<00:26,  7.26it/s]11/28/2021 00:59:53 - INFO - __main__ -   Batch number = 90
Evaluating:  32%|███▏      | 90/282 [00:12<00:26,  7.21it/s]11/28/2021 00:59:53 - INFO - __main__ -   Batch number = 91
Evaluating:  32%|███▏      | 91/282 [00:12<00:26,  7.19it/s]11/28/2021 00:59:53 - INFO - __main__ -   Batch number = 92
Evaluating:  33%|███▎      | 92/282 [00:12<00:26,  7.16it/s]11/28/2021 00:59:53 - INFO - __main__ -   Batch number = 93
Evaluating:  33%|███▎      | 93/282 [00:12<00:26,  7.16it/s]11/28/2021 00:59:54 - INFO - __main__ -   Batch number = 94
Evaluating:  33%|███▎      | 94/282 [00:12<00:26,  7.16it/s]11/28/2021 00:59:54 - INFO - __main__ -   Batch number = 95
Evaluating:  34%|███▎      | 95/282 [00:12<00:26,  7.16it/s]11/28/2021 00:59:54 - INFO - __main__ -   Batch number = 96
Evaluating:  34%|███▍      | 96/282 [00:12<00:26,  7.14it/s]11/28/2021 00:59:54 - INFO - __main__ -   Batch number = 97
Evaluating:  34%|███▍      | 97/282 [00:13<00:25,  7.12it/s]11/28/2021 00:59:54 - INFO - __main__ -   Batch number = 98
Evaluating:  35%|███▍      | 98/282 [00:13<00:30,  6.00it/s]11/28/2021 00:59:54 - INFO - __main__ -   Batch number = 99
Evaluating:  35%|███▌      | 99/282 [00:13<00:29,  6.29it/s]11/28/2021 00:59:54 - INFO - __main__ -   Batch number = 100
Evaluating:  35%|███▌      | 100/282 [00:13<00:28,  6.50it/s]11/28/2021 00:59:55 - INFO - __main__ -   Batch number = 101
Evaluating:  36%|███▌      | 101/282 [00:13<00:27,  6.66it/s]11/28/2021 00:59:55 - INFO - __main__ -   Batch number = 102
Evaluating:  36%|███▌      | 102/282 [00:13<00:26,  6.78it/s]11/28/2021 00:59:55 - INFO - __main__ -   Batch number = 103
Evaluating:  37%|███▋      | 103/282 [00:14<00:26,  6.86it/s]11/28/2021 00:59:55 - INFO - __main__ -   Batch number = 104
Evaluating:  37%|███▋      | 104/282 [00:14<00:25,  6.92it/s]11/28/2021 00:59:55 - INFO - __main__ -   Batch number = 105
Evaluating:  37%|███▋      | 105/282 [00:14<00:25,  6.96it/s]11/28/2021 00:59:55 - INFO - __main__ -   Batch number = 106
Evaluating:  38%|███▊      | 106/282 [00:14<00:25,  6.97it/s]11/28/2021 00:59:55 - INFO - __main__ -   Batch number = 107
Evaluating:  38%|███▊      | 107/282 [00:14<00:25,  6.98it/s]11/28/2021 00:59:56 - INFO - __main__ -   Batch number = 108
Evaluating:  38%|███▊      | 108/282 [00:14<00:24,  6.99it/s]11/28/2021 00:59:56 - INFO - __main__ -   Batch number = 109
Evaluating:  39%|███▊      | 109/282 [00:14<00:24,  7.00it/s]11/28/2021 00:59:56 - INFO - __main__ -   Batch number = 110
Evaluating:  39%|███▉      | 110/282 [00:15<00:24,  7.01it/s]11/28/2021 00:59:56 - INFO - __main__ -   Batch number = 111
Evaluating:  39%|███▉      | 111/282 [00:15<00:24,  7.02it/s]11/28/2021 00:59:56 - INFO - __main__ -   Batch number = 112
Evaluating:  40%|███▉      | 112/282 [00:15<00:24,  6.97it/s]11/28/2021 00:59:56 - INFO - __main__ -   Batch number = 113
Evaluating:  40%|████      | 113/282 [00:15<00:24,  6.96it/s]11/28/2021 00:59:56 - INFO - __main__ -   Batch number = 114
Evaluating:  40%|████      | 114/282 [00:15<00:24,  6.97it/s]11/28/2021 00:59:57 - INFO - __main__ -   Batch number = 115
Evaluating:  41%|████      | 115/282 [00:15<00:23,  6.96it/s]11/28/2021 00:59:57 - INFO - __main__ -   Batch number = 116
Evaluating:  41%|████      | 116/282 [00:15<00:23,  6.97it/s]11/28/2021 00:59:57 - INFO - __main__ -   Batch number = 117
Evaluating:  41%|████▏     | 117/282 [00:16<00:23,  6.96it/s]11/28/2021 00:59:57 - INFO - __main__ -   Batch number = 118
Evaluating:  42%|████▏     | 118/282 [00:16<00:23,  6.96it/s]11/28/2021 00:59:57 - INFO - __main__ -   Batch number = 119
Evaluating:  42%|████▏     | 119/282 [00:16<00:23,  6.95it/s]11/28/2021 00:59:57 - INFO - __main__ -   Batch number = 120
Evaluating:  43%|████▎     | 120/282 [00:16<00:23,  6.93it/s]11/28/2021 00:59:57 - INFO - __main__ -   Batch number = 121
Evaluating:  43%|████▎     | 121/282 [00:16<00:23,  6.91it/s]11/28/2021 00:59:58 - INFO - __main__ -   Batch number = 122
Evaluating:  43%|████▎     | 122/282 [00:16<00:23,  6.91it/s]11/28/2021 00:59:58 - INFO - __main__ -   Batch number = 123
Evaluating:  44%|████▎     | 123/282 [00:16<00:23,  6.91it/s]11/28/2021 00:59:58 - INFO - __main__ -   Batch number = 124
Evaluating:  44%|████▍     | 124/282 [00:17<00:22,  6.91it/s]11/28/2021 00:59:58 - INFO - __main__ -   Batch number = 125
Evaluating:  44%|████▍     | 125/282 [00:17<00:22,  6.90it/s]11/28/2021 00:59:58 - INFO - __main__ -   Batch number = 126
Evaluating:  45%|████▍     | 126/282 [00:17<00:22,  6.90it/s]11/28/2021 00:59:58 - INFO - __main__ -   Batch number = 127
Evaluating:  45%|████▌     | 127/282 [00:17<00:22,  6.89it/s]11/28/2021 00:59:58 - INFO - __main__ -   Batch number = 128
Evaluating:  45%|████▌     | 128/282 [00:17<00:22,  6.89it/s]11/28/2021 00:59:59 - INFO - __main__ -   Batch number = 129
Evaluating:  46%|████▌     | 129/282 [00:17<00:22,  6.88it/s]11/28/2021 00:59:59 - INFO - __main__ -   Batch number = 130
Evaluating:  46%|████▌     | 130/282 [00:17<00:22,  6.87it/s]11/28/2021 00:59:59 - INFO - __main__ -   Batch number = 131
Evaluating:  46%|████▋     | 131/282 [00:18<00:21,  6.87it/s]11/28/2021 00:59:59 - INFO - __main__ -   Batch number = 132
Evaluating:  47%|████▋     | 132/282 [00:18<00:21,  6.87it/s]11/28/2021 00:59:59 - INFO - __main__ -   Batch number = 133
Evaluating:  47%|████▋     | 133/282 [00:18<00:21,  6.86it/s]11/28/2021 00:59:59 - INFO - __main__ -   Batch number = 134
Evaluating:  48%|████▊     | 134/282 [00:18<00:21,  6.86it/s]11/28/2021 00:59:59 - INFO - __main__ -   Batch number = 135
Evaluating:  48%|████▊     | 135/282 [00:18<00:21,  6.85it/s]11/28/2021 01:00:00 - INFO - __main__ -   Batch number = 136
Evaluating:  48%|████▊     | 136/282 [00:18<00:21,  6.84it/s]11/28/2021 01:00:00 - INFO - __main__ -   Batch number = 137
Evaluating:  49%|████▊     | 137/282 [00:18<00:21,  6.84it/s]11/28/2021 01:00:00 - INFO - __main__ -   Batch number = 138
Evaluating:  49%|████▉     | 138/282 [00:19<00:21,  6.82it/s]11/28/2021 01:00:00 - INFO - __main__ -   Batch number = 139
Evaluating:  49%|████▉     | 139/282 [00:19<00:20,  6.83it/s]11/28/2021 01:00:00 - INFO - __main__ -   Batch number = 140
Evaluating:  50%|████▉     | 140/282 [00:19<00:20,  6.82it/s]11/28/2021 01:00:00 - INFO - __main__ -   Batch number = 141
Evaluating:  50%|█████     | 141/282 [00:19<00:20,  6.81it/s]11/28/2021 01:00:01 - INFO - __main__ -   Batch number = 142
Evaluating:  50%|█████     | 142/282 [00:19<00:20,  6.81it/s]11/28/2021 01:00:01 - INFO - __main__ -   Batch number = 143
Evaluating:  51%|█████     | 143/282 [00:19<00:20,  6.82it/s]11/28/2021 01:00:01 - INFO - __main__ -   Batch number = 144
Evaluating:  51%|█████     | 144/282 [00:19<00:20,  6.79it/s]11/28/2021 01:00:01 - INFO - __main__ -   Batch number = 145
Evaluating:  51%|█████▏    | 145/282 [00:20<00:20,  6.79it/s]11/28/2021 01:00:01 - INFO - __main__ -   Batch number = 146
Evaluating:  52%|█████▏    | 146/282 [00:20<00:20,  6.79it/s]11/28/2021 01:00:01 - INFO - __main__ -   Batch number = 147
Evaluating:  52%|█████▏    | 147/282 [00:20<00:19,  6.79it/s]11/28/2021 01:00:01 - INFO - __main__ -   Batch number = 148
Evaluating:  52%|█████▏    | 148/282 [00:20<00:19,  6.79it/s]11/28/2021 01:00:02 - INFO - __main__ -   Batch number = 149
Evaluating:  53%|█████▎    | 149/282 [00:20<00:19,  6.77it/s]11/28/2021 01:00:02 - INFO - __main__ -   Batch number = 150
Evaluating:  53%|█████▎    | 150/282 [00:20<00:19,  6.77it/s]11/28/2021 01:00:02 - INFO - __main__ -   Batch number = 151
Evaluating:  54%|█████▎    | 151/282 [00:20<00:19,  6.76it/s]11/28/2021 01:00:02 - INFO - __main__ -   Batch number = 152
Evaluating:  54%|█████▍    | 152/282 [00:21<00:19,  6.75it/s]11/28/2021 01:00:02 - INFO - __main__ -   Batch number = 153
Evaluating:  54%|█████▍    | 153/282 [00:21<00:19,  6.75it/s]11/28/2021 01:00:02 - INFO - __main__ -   Batch number = 154
Evaluating:  55%|█████▍    | 154/282 [00:21<00:18,  6.75it/s]11/28/2021 01:00:02 - INFO - __main__ -   Batch number = 155
Evaluating:  55%|█████▍    | 155/282 [00:21<00:18,  6.75it/s]11/28/2021 01:00:03 - INFO - __main__ -   Batch number = 156
Evaluating:  55%|█████▌    | 156/282 [00:21<00:18,  6.74it/s]11/28/2021 01:00:03 - INFO - __main__ -   Batch number = 157
Evaluating:  56%|█████▌    | 157/282 [00:21<00:18,  6.72it/s]11/28/2021 01:00:03 - INFO - __main__ -   Batch number = 158
Evaluating:  56%|█████▌    | 158/282 [00:22<00:18,  6.72it/s]11/28/2021 01:00:03 - INFO - __main__ -   Batch number = 159
Evaluating:  56%|█████▋    | 159/282 [00:22<00:18,  6.70it/s]11/28/2021 01:00:03 - INFO - __main__ -   Batch number = 160
Evaluating:  57%|█████▋    | 160/282 [00:22<00:18,  6.70it/s]11/28/2021 01:00:03 - INFO - __main__ -   Batch number = 161
Evaluating:  57%|█████▋    | 161/282 [00:22<00:18,  6.70it/s]11/28/2021 01:00:03 - INFO - __main__ -   Batch number = 162
Evaluating:  57%|█████▋    | 162/282 [00:22<00:17,  6.71it/s]11/28/2021 01:00:04 - INFO - __main__ -   Batch number = 163
Evaluating:  58%|█████▊    | 163/282 [00:22<00:17,  6.70it/s]11/28/2021 01:00:04 - INFO - __main__ -   Batch number = 164
Evaluating:  58%|█████▊    | 164/282 [00:22<00:17,  6.69it/s]11/28/2021 01:00:04 - INFO - __main__ -   Batch number = 165
Evaluating:  59%|█████▊    | 165/282 [00:23<00:17,  6.68it/s]11/28/2021 01:00:04 - INFO - __main__ -   Batch number = 166
Evaluating:  59%|█████▉    | 166/282 [00:23<00:17,  6.64it/s]11/28/2021 01:00:04 - INFO - __main__ -   Batch number = 167
Evaluating:  59%|█████▉    | 167/282 [00:23<00:17,  6.65it/s]11/28/2021 01:00:04 - INFO - __main__ -   Batch number = 168
Evaluating:  60%|█████▉    | 168/282 [00:23<00:17,  6.66it/s]11/28/2021 01:00:05 - INFO - __main__ -   Batch number = 169
Evaluating:  60%|█████▉    | 169/282 [00:23<00:16,  6.65it/s]11/28/2021 01:00:05 - INFO - __main__ -   Batch number = 170
Evaluating:  60%|██████    | 170/282 [00:23<00:16,  6.65it/s]11/28/2021 01:00:05 - INFO - __main__ -   Batch number = 171
Evaluating:  61%|██████    | 171/282 [00:23<00:16,  6.65it/s]11/28/2021 01:00:05 - INFO - __main__ -   Batch number = 172
Evaluating:  61%|██████    | 172/282 [00:24<00:19,  5.69it/s]11/28/2021 01:00:05 - INFO - __main__ -   Batch number = 173
Evaluating:  61%|██████▏   | 173/282 [00:24<00:18,  5.94it/s]11/28/2021 01:00:05 - INFO - __main__ -   Batch number = 174
Evaluating:  62%|██████▏   | 174/282 [00:24<00:17,  6.13it/s]11/28/2021 01:00:06 - INFO - __main__ -   Batch number = 175
Evaluating:  62%|██████▏   | 175/282 [00:24<00:17,  6.28it/s]11/28/2021 01:00:06 - INFO - __main__ -   Batch number = 176
Evaluating:  62%|██████▏   | 176/282 [00:24<00:16,  6.38it/s]11/28/2021 01:00:06 - INFO - __main__ -   Batch number = 177
Evaluating:  63%|██████▎   | 177/282 [00:24<00:16,  6.45it/s]11/28/2021 01:00:06 - INFO - __main__ -   Batch number = 178
Evaluating:  63%|██████▎   | 178/282 [00:25<00:15,  6.50it/s]11/28/2021 01:00:06 - INFO - __main__ -   Batch number = 179
Evaluating:  63%|██████▎   | 179/282 [00:25<00:15,  6.54it/s]11/28/2021 01:00:06 - INFO - __main__ -   Batch number = 180
Evaluating:  64%|██████▍   | 180/282 [00:25<00:15,  6.55it/s]11/28/2021 01:00:06 - INFO - __main__ -   Batch number = 181
Evaluating:  64%|██████▍   | 181/282 [00:25<00:15,  6.54it/s]11/28/2021 01:00:07 - INFO - __main__ -   Batch number = 182
Evaluating:  65%|██████▍   | 182/282 [00:25<00:15,  6.55it/s]11/28/2021 01:00:07 - INFO - __main__ -   Batch number = 183
Evaluating:  65%|██████▍   | 183/282 [00:25<00:15,  6.57it/s]11/28/2021 01:00:07 - INFO - __main__ -   Batch number = 184
Evaluating:  65%|██████▌   | 184/282 [00:26<00:14,  6.57it/s]11/28/2021 01:00:07 - INFO - __main__ -   Batch number = 185
Evaluating:  66%|██████▌   | 185/282 [00:26<00:14,  6.58it/s]11/28/2021 01:00:07 - INFO - __main__ -   Batch number = 186
Evaluating:  66%|██████▌   | 186/282 [00:26<00:14,  6.57it/s]11/28/2021 01:00:07 - INFO - __main__ -   Batch number = 187
Evaluating:  66%|██████▋   | 187/282 [00:26<00:14,  6.47it/s]11/28/2021 01:00:08 - INFO - __main__ -   Batch number = 188
Evaluating:  67%|██████▋   | 188/282 [00:26<00:14,  6.49it/s]11/28/2021 01:00:08 - INFO - __main__ -   Batch number = 189
Evaluating:  67%|██████▋   | 189/282 [00:26<00:14,  6.51it/s]11/28/2021 01:00:08 - INFO - __main__ -   Batch number = 190
Evaluating:  67%|██████▋   | 190/282 [00:26<00:14,  6.52it/s]11/28/2021 01:00:08 - INFO - __main__ -   Batch number = 191
Evaluating:  68%|██████▊   | 191/282 [00:27<00:13,  6.51it/s]11/28/2021 01:00:08 - INFO - __main__ -   Batch number = 192
Evaluating:  68%|██████▊   | 192/282 [00:27<00:13,  6.51it/s]11/28/2021 01:00:08 - INFO - __main__ -   Batch number = 193
Evaluating:  68%|██████▊   | 193/282 [00:27<00:13,  6.52it/s]11/28/2021 01:00:08 - INFO - __main__ -   Batch number = 194
Evaluating:  69%|██████▉   | 194/282 [00:27<00:13,  6.52it/s]11/28/2021 01:00:09 - INFO - __main__ -   Batch number = 195
Evaluating:  69%|██████▉   | 195/282 [00:27<00:13,  6.52it/s]11/28/2021 01:00:09 - INFO - __main__ -   Batch number = 196
PyTorch version 1.10.0+cu102 available.
Evaluating:  70%|██████▉   | 196/282 [00:27<00:13,  6.50it/s]11/28/2021 01:00:09 - INFO - __main__ -   Batch number = 197
Evaluating:  70%|██████▉   | 197/282 [00:28<00:13,  6.52it/s]11/28/2021 01:00:09 - INFO - __main__ -   Batch number = 198
11/28/2021 01:00:09 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='hi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:00:09 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:00:09 - INFO - __main__ -   Seed = 1
11/28/2021 01:00:09 - INFO - root -   save model
11/28/2021 01:00:09 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='hi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:00:09 - INFO - __main__ -   Loading pretrained model and tokenizer
Evaluating:  70%|███████   | 198/282 [00:28<00:12,  6.52it/s]11/28/2021 01:00:09 - INFO - __main__ -   Batch number = 199
Evaluating:  71%|███████   | 199/282 [00:28<00:12,  6.52it/s]11/28/2021 01:00:09 - INFO - __main__ -   Batch number = 200
Evaluating:  71%|███████   | 200/282 [00:28<00:12,  6.51it/s]11/28/2021 01:00:09 - INFO - __main__ -   Batch number = 201
Evaluating:  71%|███████▏  | 201/282 [00:28<00:12,  6.50it/s]11/28/2021 01:00:10 - INFO - __main__ -   Batch number = 202
Evaluating:  72%|███████▏  | 202/282 [00:28<00:12,  6.49it/s]11/28/2021 01:00:10 - INFO - __main__ -   Batch number = 203
Evaluating:  72%|███████▏  | 203/282 [00:28<00:12,  6.48it/s]11/28/2021 01:00:10 - INFO - __main__ -   Batch number = 204
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  72%|███████▏  | 204/282 [00:29<00:12,  6.48it/s]11/28/2021 01:00:10 - INFO - __main__ -   Batch number = 205
Evaluating:  73%|███████▎  | 205/282 [00:29<00:11,  6.46it/s]11/28/2021 01:00:10 - INFO - __main__ -   Batch number = 206
Evaluating:  73%|███████▎  | 206/282 [00:29<00:11,  6.46it/s]11/28/2021 01:00:10 - INFO - __main__ -   Batch number = 207
Evaluating:  73%|███████▎  | 207/282 [00:29<00:11,  6.47it/s]11/28/2021 01:00:11 - INFO - __main__ -   Batch number = 208
Evaluating:  74%|███████▍  | 208/282 [00:29<00:11,  6.47it/s]11/28/2021 01:00:11 - INFO - __main__ -   Batch number = 209
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  74%|███████▍  | 209/282 [00:29<00:11,  6.47it/s]11/28/2021 01:00:11 - INFO - __main__ -   Batch number = 210
Evaluating:  74%|███████▍  | 210/282 [00:30<00:11,  6.45it/s]11/28/2021 01:00:11 - INFO - __main__ -   Batch number = 211
Evaluating:  75%|███████▍  | 211/282 [00:30<00:11,  6.44it/s]11/28/2021 01:00:11 - INFO - __main__ -   Batch number = 212
Evaluating:  75%|███████▌  | 212/282 [00:30<00:10,  6.44it/s]11/28/2021 01:00:11 - INFO - __main__ -   Batch number = 213
Evaluating:  76%|███████▌  | 213/282 [00:30<00:10,  6.44it/s]11/28/2021 01:00:12 - INFO - __main__ -   Batch number = 214
Evaluating:  76%|███████▌  | 214/282 [00:30<00:10,  6.43it/s]11/28/2021 01:00:12 - INFO - __main__ -   Batch number = 215
loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
Evaluating:  76%|███████▌  | 215/282 [00:30<00:10,  6.43it/s]11/28/2021 01:00:12 - INFO - __main__ -   Batch number = 216
11/28/2021 01:00:12 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
Evaluating:  77%|███████▋  | 216/282 [00:30<00:10,  6.42it/s]11/28/2021 01:00:12 - INFO - __main__ -   Batch number = 217
Evaluating:  77%|███████▋  | 217/282 [00:31<00:10,  6.41it/s]11/28/2021 01:00:12 - INFO - __main__ -   Batch number = 218
Evaluating:  77%|███████▋  | 218/282 [00:31<00:09,  6.42it/s]11/28/2021 01:00:12 - INFO - __main__ -   Batch number = 219
Evaluating:  78%|███████▊  | 219/282 [00:31<00:09,  6.41it/s]11/28/2021 01:00:12 - INFO - __main__ -   Batch number = 220
Evaluating:  78%|███████▊  | 220/282 [00:31<00:09,  6.41it/s]11/28/2021 01:00:13 - INFO - __main__ -   Batch number = 221
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  78%|███████▊  | 221/282 [00:31<00:09,  6.41it/s]11/28/2021 01:00:13 - INFO - __main__ -   Batch number = 222
Evaluating:  79%|███████▊  | 222/282 [00:31<00:09,  6.41it/s]11/28/2021 01:00:13 - INFO - __main__ -   Batch number = 223
Evaluating:  79%|███████▉  | 223/282 [00:32<00:09,  6.40it/s]11/28/2021 01:00:13 - INFO - __main__ -   Batch number = 224
Evaluating:  79%|███████▉  | 224/282 [00:32<00:09,  6.39it/s]11/28/2021 01:00:13 - INFO - __main__ -   Batch number = 225
Evaluating:  80%|███████▉  | 225/282 [00:32<00:08,  6.40it/s]11/28/2021 01:00:13 - INFO - __main__ -   Batch number = 226
Evaluating:  80%|████████  | 226/282 [00:32<00:08,  6.38it/s]11/28/2021 01:00:14 - INFO - __main__ -   Batch number = 227
Evaluating:  80%|████████  | 227/282 [00:32<00:08,  6.37it/s]11/28/2021 01:00:14 - INFO - __main__ -   Batch number = 228
Evaluating:  81%|████████  | 228/282 [00:32<00:08,  6.36it/s]11/28/2021 01:00:14 - INFO - __main__ -   Batch number = 229
Evaluating:  81%|████████  | 229/282 [00:33<00:08,  6.37it/s]11/28/2021 01:00:14 - INFO - __main__ -   Batch number = 230
Evaluating:  82%|████████▏ | 230/282 [00:33<00:08,  6.34it/s]11/28/2021 01:00:14 - INFO - __main__ -   Batch number = 231
Evaluating:  82%|████████▏ | 231/282 [00:33<00:08,  6.32it/s]11/28/2021 01:00:14 - INFO - __main__ -   Batch number = 232
Evaluating:  82%|████████▏ | 232/282 [00:33<00:07,  6.32it/s]11/28/2021 01:00:14 - INFO - __main__ -   Batch number = 233
Evaluating:  83%|████████▎ | 233/282 [00:33<00:07,  6.32it/s]11/28/2021 01:00:15 - INFO - __main__ -   Batch number = 234
Evaluating:  83%|████████▎ | 234/282 [00:33<00:07,  6.31it/s]11/28/2021 01:00:15 - INFO - __main__ -   Batch number = 235
Evaluating:  83%|████████▎ | 235/282 [00:33<00:07,  6.31it/s]11/28/2021 01:00:15 - INFO - __main__ -   Batch number = 236
Evaluating:  84%|████████▎ | 236/282 [00:34<00:07,  6.31it/s]11/28/2021 01:00:15 - INFO - __main__ -   Batch number = 237
Evaluating:  84%|████████▍ | 237/282 [00:34<00:07,  6.31it/s]11/28/2021 01:00:15 - INFO - __main__ -   Batch number = 238
Evaluating:  84%|████████▍ | 238/282 [00:34<00:06,  6.29it/s]11/28/2021 01:00:15 - INFO - __main__ -   Batch number = 239
Evaluating:  85%|████████▍ | 239/282 [00:34<00:06,  6.29it/s]11/28/2021 01:00:16 - INFO - __main__ -   Batch number = 240
Evaluating:  85%|████████▌ | 240/282 [00:34<00:06,  6.26it/s]11/28/2021 01:00:16 - INFO - __main__ -   Batch number = 241
Evaluating:  85%|████████▌ | 241/282 [00:34<00:06,  6.25it/s]11/28/2021 01:00:16 - INFO - __main__ -   Batch number = 242
Evaluating:  86%|████████▌ | 242/282 [00:35<00:06,  6.26it/s]11/28/2021 01:00:16 - INFO - __main__ -   Batch number = 243
Evaluating:  86%|████████▌ | 243/282 [00:35<00:06,  6.26it/s]11/28/2021 01:00:16 - INFO - __main__ -   Batch number = 244
Evaluating:  87%|████████▋ | 244/282 [00:35<00:06,  6.26it/s]11/28/2021 01:00:16 - INFO - __main__ -   Batch number = 245
Evaluating:  87%|████████▋ | 245/282 [00:35<00:05,  6.26it/s]11/28/2021 01:00:17 - INFO - __main__ -   Batch number = 246
Evaluating:  87%|████████▋ | 246/282 [00:35<00:05,  6.26it/s]11/28/2021 01:00:17 - INFO - __main__ -   Batch number = 247
Evaluating:  88%|████████▊ | 247/282 [00:35<00:05,  6.16it/s]11/28/2021 01:00:17 - INFO - __main__ -   Batch number = 248
Evaluating:  88%|████████▊ | 248/282 [00:36<00:05,  6.03it/s]11/28/2021 01:00:17 - INFO - __main__ -   Batch number = 249
Evaluating:  88%|████████▊ | 249/282 [00:36<00:05,  5.95it/s]11/28/2021 01:00:17 - INFO - __main__ -   Batch number = 250
Evaluating:  89%|████████▊ | 250/282 [00:36<00:05,  5.83it/s]11/28/2021 01:00:17 - INFO - __main__ -   Batch number = 251
Evaluating:  89%|████████▉ | 251/282 [00:36<00:05,  5.81it/s]11/28/2021 01:00:18 - INFO - __main__ -   Batch number = 252
Evaluating:  89%|████████▉ | 252/282 [00:36<00:05,  5.82it/s]11/28/2021 01:00:18 - INFO - __main__ -   Batch number = 253
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:00:18 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:00:18 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:00:18 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
11/28/2021 01:00:18 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:00:18 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:00:18 - INFO - root -   loading lang adpater am/wiki@ukp
11/28/2021 01:00:18 - INFO - __main__ -   Adapter Languages : ['am'], Length : 1
11/28/2021 01:00:18 - INFO - __main__ -   Adapter Names ['am/wiki@ukp'], Length : 1
11/28/2021 01:00:18 - INFO - __main__ -   Language = am
11/28/2021 01:00:18 - INFO - __main__ -   Adapter Name = am/wiki@ukp
Evaluating:  90%|████████▉ | 253/282 [00:36<00:04,  5.85it/s]11/28/2021 01:00:18 - INFO - __main__ -   Batch number = 254
Evaluating:  90%|█████████ | 254/282 [00:37<00:04,  5.89it/s]11/28/2021 01:00:18 - INFO - __main__ -   Batch number = 255
Found matching adapter at: adapters/ukp/bert-base-multilingual-cased_am_wiki_pfeiffer.json
Evaluating:  90%|█████████ | 255/282 [00:37<00:04,  5.93it/s]11/28/2021 01:00:18 - INFO - __main__ -   Batch number = 256
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/am/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_am_wiki_pfeiffer.zip.
Evaluating:  91%|█████████ | 256/282 [00:37<00:04,  5.99it/s]11/28/2021 01:00:18 - INFO - __main__ -   Batch number = 257
Evaluating:  91%|█████████ | 257/282 [00:37<00:04,  6.05it/s]11/28/2021 01:00:19 - INFO - __main__ -   Batch number = 258
Evaluating:  91%|█████████▏| 258/282 [00:37<00:03,  6.08it/s]11/28/2021 01:00:19 - INFO - __main__ -   Batch number = 259
Evaluating:  92%|█████████▏| 259/282 [00:37<00:03,  6.11it/s]11/28/2021 01:00:19 - INFO - __main__ -   Batch number = 260
Evaluating:  92%|█████████▏| 260/282 [00:38<00:03,  6.13it/s]11/28/2021 01:00:19 - INFO - __main__ -   Batch number = 261
Evaluating:  93%|█████████▎| 261/282 [00:38<00:03,  6.15it/s]11/28/2021 01:00:19 - INFO - __main__ -   Batch number = 262
Evaluating:  93%|█████████▎| 262/282 [00:38<00:03,  6.11it/s]11/28/2021 01:00:19 - INFO - __main__ -   Batch number = 263
Evaluating:  93%|█████████▎| 263/282 [00:38<00:03,  5.98it/s]11/28/2021 01:00:20 - INFO - __main__ -   Batch number = 264
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/adapter_config.json
Adding adapter 'am' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/head_config.json
Evaluating:  94%|█████████▎| 264/282 [00:38<00:03,  5.87it/s]11/28/2021 01:00:20 - INFO - __main__ -   Batch number = 265
Evaluating:  94%|█████████▍| 265/282 [00:38<00:02,  5.77it/s]11/28/2021 01:00:20 - INFO - __main__ -   Batch number = 266
Evaluating:  94%|█████████▍| 266/282 [00:39<00:02,  5.67it/s]11/28/2021 01:00:20 - INFO - __main__ -   Batch number = 267
Evaluating:  95%|█████████▍| 267/282 [00:39<00:02,  5.67it/s]11/28/2021 01:00:20 - INFO - __main__ -   Batch number = 268
Evaluating:  95%|█████████▌| 268/282 [00:39<00:02,  5.64it/s]11/28/2021 01:00:20 - INFO - __main__ -   Batch number = 269
Evaluating:  95%|█████████▌| 269/282 [00:39<00:02,  5.63it/s]11/28/2021 01:00:21 - INFO - __main__ -   Batch number = 270
Evaluating:  96%|█████████▌| 270/282 [00:39<00:02,  5.65it/s]11/28/2021 01:00:21 - INFO - __main__ -   Batch number = 271
Evaluating:  96%|█████████▌| 271/282 [00:39<00:01,  5.72it/s]11/28/2021 01:00:21 - INFO - __main__ -   Batch number = 272
Evaluating:  96%|█████████▋| 272/282 [00:40<00:01,  5.73it/s]11/28/2021 01:00:21 - INFO - __main__ -   Batch number = 273
Evaluating:  97%|█████████▋| 273/282 [00:40<00:01,  5.75it/s]11/28/2021 01:00:21 - INFO - __main__ -   Batch number = 274
Evaluating:  97%|█████████▋| 274/282 [00:40<00:01,  5.76it/s]11/28/2021 01:00:22 - INFO - __main__ -   Batch number = 275
Evaluating:  98%|█████████▊| 275/282 [00:40<00:01,  5.75it/s]11/28/2021 01:00:22 - INFO - __main__ -   Batch number = 276
Evaluating:  98%|█████████▊| 276/282 [00:40<00:01,  5.78it/s]11/28/2021 01:00:22 - INFO - __main__ -   Batch number = 277
Evaluating:  98%|█████████▊| 277/282 [00:41<00:00,  5.79it/s]11/28/2021 01:00:22 - INFO - __main__ -   Batch number = 278
Evaluating:  99%|█████████▊| 278/282 [00:41<00:00,  5.85it/s]11/28/2021 01:00:22 - INFO - __main__ -   Batch number = 279
Evaluating:  99%|█████████▉| 279/282 [00:41<00:00,  5.88it/s]11/28/2021 01:00:22 - INFO - __main__ -   Batch number = 280
Evaluating:  99%|█████████▉| 280/282 [00:41<00:00,  5.87it/s]11/28/2021 01:00:23 - INFO - __main__ -   Batch number = 281
Evaluating: 100%|█████████▉| 281/282 [00:41<00:00,  5.87it/s]11/28/2021 01:00:23 - INFO - __main__ -   Batch number = 282
Evaluating: 100%|██████████| 282/282 [00:41<00:00,  6.75it/s]11/28/2021 01:00:25 - INFO - __main__ -   Language adapter for hi not found, using am instead
11/28/2021 01:00:25 - INFO - __main__ -   Set active language adapter to am
11/28/2021 01:00:25 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:00:25 - INFO - __main__ -   Adapter Languages = ['am']
11/28/2021 01:00:25 - INFO - __main__ -   all languages = hi
11/28/2021 01:00:25 - INFO - __main__ -   Creating features from dataset file at /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/hi/test.bert-base-multilingual-cased in language hi
11/28/2021 01:00:25 - INFO - utils_tag -   lang_id=0, lang=hi, lang2id=None
11/28/2021 01:00:25 - INFO - utils_tag -   Writing example 0 of 2685
11/28/2021 01:00:25 - INFO - utils_tag -   *** Example ***
11/28/2021 01:00:25 - INFO - utils_tag -   guid: hi-1
11/28/2021 01:00:25 - INFO - utils_tag -   tokens: [CLS] इसके अतिरिक्त ग ##ुग ##्ग ##ुल क ##ु ##ंड , भी ##म ग ##ु ##फा तथा भी ##म ##श ##िला भी दर्शन ##ीय स ##्थल हैं । [SEP]
11/28/2021 01:00:25 - INFO - utils_tag -   input_ids: 101 23801 79704 867 78660 65536 52736 865 14070 29841 117 13286 13841 867 14070 89761 14862 13286 13841 21835 33156 13286 96417 21571 898 79379 11716 920 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:00:25 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:00:25 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:00:25 - INFO - utils_tag -   label_ids: -100 11 2 12 -100 -100 -100 12 -100 -100 13 12 -100 12 -100 -100 5 12 -100 -100 -100 10 1 -100 8 -100 4 13 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
11/28/2021 01:00:25 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
11/28/2021 01:00:25 - INFO - utils_tag -   *** Example ***
11/28/2021 01:00:25 - INFO - utils_tag -   guid: hi-2
11/28/2021 01:00:25 - INFO - utils_tag -   tokens: [CLS] आ ##धा किमी की दूर ##ी पर भ ##ैर ##वन ##ा ##थ मंदिर है , जहाँ केवल के ##दार ##नाथ के प ##ट ख ##ुल ##ने और ब ##ंद होने के दिन ही पू ##जन किया जाता है । [SEP]
11/28/2021 01:00:25 - INFO - utils_tag -   input_ids: 101 852 38380 77159 10826 79010 10914 12213 888 89595 45052 11208 30534 43799 10569 117 79977 46605 10412 55904 87006 10412 885 14835 866 52736 13466 10977 887 52768 27697 10412 34578 14080 77826 36667 13016 15425 10569 920 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:00:25 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:00:25 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:00:25 - INFO - utils_tag -   label_ids: -100 6 -100 8 2 8 -100 2 12 -100 -100 -100 -100 8 4 13 11 10 12 -100 -100 2 8 -100 16 -100 -100 5 1 -100 16 2 8 10 8 -100 16 4 4 13 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
11/28/2021 01:00:25 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
11/28/2021 01:00:25 - INFO - utils_tag -   *** Example ***
11/28/2021 01:00:25 - INFO - utils_tag -   guid: hi-3
11/28/2021 01:00:25 - INFO - utils_tag -   tokens: [CLS] भ ##ैर ##व का स्थान उत्तर ##ा ##ख ##ंड में क्षेत्र ##पा ##ल अथवा भूमि ##देव के रूप में महत्वपूर्ण है । [SEP]
11/28/2021 01:00:25 - INFO - utils_tag -   input_ids: 101 888 89595 15070 11081 36724 17297 11208 27841 29841 10532 21582 42035 11714 48012 90775 94928 10412 15236 10532 57136 10569 920 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:00:25 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:00:25 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:00:25 - INFO - utils_tag -   label_ids: -100 12 -100 -100 2 8 12 -100 -100 -100 2 12 -100 -100 5 12 -100 2 2 2 1 4 13 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
11/28/2021 01:00:25 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
11/28/2021 01:00:25 - INFO - utils_tag -   *** Example ***
11/28/2021 01:00:25 - INFO - utils_tag -   guid: hi-4
11/28/2021 01:00:25 - INFO - utils_tag -   tokens: [CLS] यह स ##ोन ##प ##्र ##या ##ग से 5 किमी आगे और के ##दार ##नाथ में 6 किमी पहले ( प ##ै ##दल ) प ##ड़ ##ने वाला एक अ ##त्य ##ंत महत्वपूर्ण ती ##र्थ एवं वि ##श ##्राम स ##्थल है । [SEP]
11/28/2021 01:00:25 - INFO - utils_tag -   input_ids: 101 13525 898 73354 18187 18321 15168 19741 11072 126 77159 90240 10977 10412 55904 87006 10532 127 77159 29474 113 885 18438 99989 114 885 43733 13466 62332 11186 851 91618 24786 57136 68613 86025 21850 55190 21835 66695 898 79379 10569 920 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:00:25 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:00:25 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:00:25 - INFO - utils_tag -   label_ids: -100 11 12 -100 -100 -100 -100 -100 2 9 8 2 5 12 -100 -100 2 9 8 2 13 8 -100 -100 13 16 -100 -100 2 9 3 -100 -100 1 8 -100 5 8 -100 -100 8 -100 4 13 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
11/28/2021 01:00:25 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
11/28/2021 01:00:25 - INFO - utils_tag -   *** Example ***
11/28/2021 01:00:25 - INFO - utils_tag -   guid: hi-5
11/28/2021 01:00:25 - INFO - utils_tag -   tokens: [CLS] इसकी ऊ ##ँ ##चा ##ई केवल 1982 मीटर है । [SEP]
11/28/2021 01:00:25 - INFO - utils_tag -   input_ids: 101 49683 856 28462 22078 15801 46605 10642 95790 10569 920 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:00:25 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:00:25 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:00:25 - INFO - utils_tag -   label_ids: -100 11 8 -100 -100 -100 10 9 8 4 13 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
11/28/2021 01:00:25 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]

/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:00:27 - INFO - __main__ -   ***** Evaluation result  in ru *****
11/28/2021 01:00:27 - INFO - __main__ -     f1 = 0.8598014246968125
11/28/2021 01:00:27 - INFO - __main__ -     loss = 0.5538920300438049
11/28/2021 01:00:27 - INFO - __main__ -     precision = 0.8638532440461764
11/28/2021 01:00:27 - INFO - __main__ -     recall = 0.8557874372272989
49.80user 17.88system 1:04.17elapsed 105%CPU (0avgtext+0avgdata 3933364maxresident)k
0inputs+1776outputs (0major+1828598minor)pagefaults 0swaps
11/28/2021 01:00:28 - INFO - __main__ -   Saving features into cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_hi_bert-base-multilingual-cased_128, len(features)=2685
11/28/2021 01:00:28 - INFO - __main__ -   ***** Running evaluation  in hi *****
11/28/2021 01:00:28 - INFO - __main__ -     Num examples = 2685
11/28/2021 01:00:28 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/84 [00:00<?, ?it/s]11/28/2021 01:00:28 - INFO - __main__ -   Batch number = 1
Evaluating:   1%|          | 1/84 [00:00<00:12,  6.90it/s]11/28/2021 01:00:29 - INFO - __main__ -   Batch number = 2
Evaluating:   2%|▏         | 2/84 [00:00<00:11,  7.26it/s]11/28/2021 01:00:29 - INFO - __main__ -   Batch number = 3
Evaluating:   4%|▎         | 3/84 [00:00<00:10,  7.46it/s]11/28/2021 01:00:29 - INFO - __main__ -   Batch number = 4
Evaluating:   5%|▍         | 4/84 [00:00<00:10,  7.55it/s]11/28/2021 01:00:29 - INFO - __main__ -   Batch number = 5
Evaluating:   6%|▌         | 5/84 [00:00<00:10,  7.56it/s]11/28/2021 01:00:29 - INFO - __main__ -   Batch number = 6
PyTorch version 1.10.0+cu102 available.
Evaluating:   7%|▋         | 6/84 [00:00<00:10,  7.58it/s]11/28/2021 01:00:29 - INFO - __main__ -   Batch number = 7
Evaluating:   8%|▊         | 7/84 [00:00<00:10,  7.58it/s]11/28/2021 01:00:29 - INFO - __main__ -   Batch number = 8
Evaluating:  10%|▉         | 8/84 [00:01<00:09,  7.60it/s]11/28/2021 01:00:30 - INFO - __main__ -   Batch number = 9
11/28/2021 01:00:30 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ru', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:00:30 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:00:30 - INFO - __main__ -   Seed = 3
11/28/2021 01:00:30 - INFO - root -   save model
11/28/2021 01:00:30 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ru', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:00:30 - INFO - __main__ -   Loading pretrained model and tokenizer
Evaluating:  11%|█         | 9/84 [00:01<00:09,  7.62it/s]11/28/2021 01:00:30 - INFO - __main__ -   Batch number = 10
Evaluating:  12%|█▏        | 10/84 [00:01<00:09,  7.60it/s]11/28/2021 01:00:30 - INFO - __main__ -   Batch number = 11
Evaluating:  13%|█▎        | 11/84 [00:01<00:09,  7.57it/s]11/28/2021 01:00:30 - INFO - __main__ -   Batch number = 12
Evaluating:  14%|█▍        | 12/84 [00:01<00:09,  7.57it/s]11/28/2021 01:00:30 - INFO - __main__ -   Batch number = 13
Evaluating:  15%|█▌        | 13/84 [00:01<00:10,  6.48it/s]11/28/2021 01:00:30 - INFO - __main__ -   Batch number = 14
Evaluating:  17%|█▋        | 14/84 [00:01<00:10,  6.76it/s]11/28/2021 01:00:30 - INFO - __main__ -   Batch number = 15
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  18%|█▊        | 15/84 [00:02<00:09,  7.00it/s]11/28/2021 01:00:31 - INFO - __main__ -   Batch number = 16
Evaluating:  19%|█▉        | 16/84 [00:02<00:09,  7.17it/s]11/28/2021 01:00:31 - INFO - __main__ -   Batch number = 17
Evaluating:  20%|██        | 17/84 [00:02<00:09,  7.27it/s]11/28/2021 01:00:31 - INFO - __main__ -   Batch number = 18
Evaluating:  21%|██▏       | 18/84 [00:02<00:08,  7.37it/s]11/28/2021 01:00:31 - INFO - __main__ -   Batch number = 19
Evaluating:  23%|██▎       | 19/84 [00:02<00:08,  7.42it/s]11/28/2021 01:00:31 - INFO - __main__ -   Batch number = 20
Evaluating:  24%|██▍       | 20/84 [00:02<00:08,  7.45it/s]11/28/2021 01:00:31 - INFO - __main__ -   Batch number = 21
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  25%|██▌       | 21/84 [00:02<00:08,  7.47it/s]11/28/2021 01:00:31 - INFO - __main__ -   Batch number = 22
Evaluating:  26%|██▌       | 22/84 [00:02<00:08,  7.51it/s]11/28/2021 01:00:31 - INFO - __main__ -   Batch number = 23
Evaluating:  27%|██▋       | 23/84 [00:03<00:08,  7.49it/s]11/28/2021 01:00:32 - INFO - __main__ -   Batch number = 24
Evaluating:  29%|██▊       | 24/84 [00:03<00:07,  7.51it/s]11/28/2021 01:00:32 - INFO - __main__ -   Batch number = 25
Evaluating:  30%|██▉       | 25/84 [00:03<00:07,  7.52it/s]11/28/2021 01:00:32 - INFO - __main__ -   Batch number = 26
Evaluating:  31%|███       | 26/84 [00:03<00:07,  7.49it/s]11/28/2021 01:00:32 - INFO - __main__ -   Batch number = 27
Evaluating:  32%|███▏      | 27/84 [00:03<00:07,  7.51it/s]11/28/2021 01:00:32 - INFO - __main__ -   Batch number = 28
loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
Evaluating:  33%|███▎      | 28/84 [00:03<00:07,  7.53it/s]11/28/2021 01:00:32 - INFO - __main__ -   Batch number = 29
11/28/2021 01:00:32 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
Evaluating:  35%|███▍      | 29/84 [00:03<00:07,  7.52it/s]11/28/2021 01:00:32 - INFO - __main__ -   Batch number = 30
Evaluating:  36%|███▌      | 30/84 [00:04<00:07,  7.53it/s]11/28/2021 01:00:32 - INFO - __main__ -   Batch number = 31
Evaluating:  37%|███▋      | 31/84 [00:04<00:07,  7.53it/s]11/28/2021 01:00:33 - INFO - __main__ -   Batch number = 32
Evaluating:  38%|███▊      | 32/84 [00:04<00:06,  7.43it/s]11/28/2021 01:00:33 - INFO - __main__ -   Batch number = 33
Evaluating:  39%|███▉      | 33/84 [00:04<00:06,  7.40it/s]11/28/2021 01:00:33 - INFO - __main__ -   Batch number = 34
Evaluating:  40%|████      | 34/84 [00:04<00:06,  7.43it/s]11/28/2021 01:00:33 - INFO - __main__ -   Batch number = 35
Evaluating:  42%|████▏     | 35/84 [00:04<00:06,  7.43it/s]11/28/2021 01:00:33 - INFO - __main__ -   Batch number = 36
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  43%|████▎     | 36/84 [00:04<00:06,  7.45it/s]11/28/2021 01:00:33 - INFO - __main__ -   Batch number = 37
Evaluating:  44%|████▍     | 37/84 [00:04<00:06,  7.46it/s]11/28/2021 01:00:33 - INFO - __main__ -   Batch number = 38
Evaluating:  45%|████▌     | 38/84 [00:05<00:06,  7.46it/s]11/28/2021 01:00:34 - INFO - __main__ -   Batch number = 39
Evaluating:  46%|████▋     | 39/84 [00:05<00:06,  7.46it/s]11/28/2021 01:00:34 - INFO - __main__ -   Batch number = 40
Evaluating:  48%|████▊     | 40/84 [00:05<00:05,  7.45it/s]11/28/2021 01:00:34 - INFO - __main__ -   Batch number = 41
Evaluating:  49%|████▉     | 41/84 [00:05<00:05,  7.45it/s]11/28/2021 01:00:34 - INFO - __main__ -   Batch number = 42
Evaluating:  50%|█████     | 42/84 [00:05<00:05,  7.44it/s]11/28/2021 01:00:34 - INFO - __main__ -   Batch number = 43
Evaluating:  51%|█████     | 43/84 [00:05<00:05,  7.44it/s]11/28/2021 01:00:34 - INFO - __main__ -   Batch number = 44
Evaluating:  52%|█████▏    | 44/84 [00:05<00:05,  7.41it/s]11/28/2021 01:00:34 - INFO - __main__ -   Batch number = 45
Evaluating:  54%|█████▎    | 45/84 [00:06<00:05,  7.39it/s]11/28/2021 01:00:35 - INFO - __main__ -   Batch number = 46
Evaluating:  55%|█████▍    | 46/84 [00:06<00:05,  7.37it/s]11/28/2021 01:00:35 - INFO - __main__ -   Batch number = 47
Evaluating:  56%|█████▌    | 47/84 [00:06<00:05,  7.32it/s]11/28/2021 01:00:35 - INFO - __main__ -   Batch number = 48
Evaluating:  57%|█████▋    | 48/84 [00:06<00:04,  7.31it/s]11/28/2021 01:00:35 - INFO - __main__ -   Batch number = 49
Evaluating:  58%|█████▊    | 49/84 [00:06<00:04,  7.21it/s]11/28/2021 01:00:35 - INFO - __main__ -   Batch number = 50
Evaluating:  60%|█████▉    | 50/84 [00:06<00:05,  6.28it/s]11/28/2021 01:00:35 - INFO - __main__ -   Batch number = 51
Evaluating:  61%|██████    | 51/84 [00:06<00:05,  6.58it/s]11/28/2021 01:00:35 - INFO - __main__ -   Batch number = 52
Evaluating:  62%|██████▏   | 52/84 [00:07<00:04,  6.82it/s]11/28/2021 01:00:36 - INFO - __main__ -   Batch number = 53
Evaluating:  63%|██████▎   | 53/84 [00:07<00:04,  6.96it/s]11/28/2021 01:00:36 - INFO - __main__ -   Batch number = 54
Evaluating:  64%|██████▍   | 54/84 [00:07<00:04,  7.07it/s]11/28/2021 01:00:36 - INFO - __main__ -   Batch number = 55
Evaluating:  65%|██████▌   | 55/84 [00:07<00:04,  7.17it/s]11/28/2021 01:00:36 - INFO - __main__ -   Batch number = 56
Evaluating:  67%|██████▋   | 56/84 [00:07<00:03,  7.22it/s]11/28/2021 01:00:36 - INFO - __main__ -   Batch number = 57
Evaluating:  68%|██████▊   | 57/84 [00:07<00:03,  7.27it/s]11/28/2021 01:00:36 - INFO - __main__ -   Batch number = 58
Evaluating:  69%|██████▉   | 58/84 [00:07<00:03,  7.29it/s]11/28/2021 01:00:36 - INFO - __main__ -   Batch number = 59
Evaluating:  70%|███████   | 59/84 [00:08<00:03,  7.29it/s]11/28/2021 01:00:37 - INFO - __main__ -   Batch number = 60
Evaluating:  71%|███████▏  | 60/84 [00:08<00:03,  7.31it/s]11/28/2021 01:00:37 - INFO - __main__ -   Batch number = 61
Evaluating:  73%|███████▎  | 61/84 [00:08<00:03,  7.31it/s]11/28/2021 01:00:37 - INFO - __main__ -   Batch number = 62
Evaluating:  74%|███████▍  | 62/84 [00:08<00:03,  7.32it/s]11/28/2021 01:00:37 - INFO - __main__ -   Batch number = 63
Evaluating:  75%|███████▌  | 63/84 [00:08<00:02,  7.33it/s]11/28/2021 01:00:37 - INFO - __main__ -   Batch number = 64
Evaluating:  76%|███████▌  | 64/84 [00:08<00:02,  7.35it/s]11/28/2021 01:00:37 - INFO - __main__ -   Batch number = 65
Evaluating:  77%|███████▋  | 65/84 [00:08<00:02,  7.35it/s]11/28/2021 01:00:37 - INFO - __main__ -   Batch number = 66
Evaluating:  79%|███████▊  | 66/84 [00:09<00:02,  7.35it/s]11/28/2021 01:00:37 - INFO - __main__ -   Batch number = 67
Evaluating:  80%|███████▉  | 67/84 [00:09<00:02,  7.35it/s]11/28/2021 01:00:38 - INFO - __main__ -   Batch number = 68
Evaluating:  81%|████████  | 68/84 [00:09<00:02,  7.34it/s]11/28/2021 01:00:38 - INFO - __main__ -   Batch number = 69
Evaluating:  82%|████████▏ | 69/84 [00:09<00:02,  7.33it/s]11/28/2021 01:00:38 - INFO - __main__ -   Batch number = 70
Evaluating:  83%|████████▎ | 70/84 [00:09<00:01,  7.32it/s]11/28/2021 01:00:38 - INFO - __main__ -   Batch number = 71
Evaluating:  85%|████████▍ | 71/84 [00:09<00:01,  7.30it/s]11/28/2021 01:00:38 - INFO - __main__ -   Batch number = 72
Evaluating:  86%|████████▌ | 72/84 [00:09<00:01,  7.30it/s]11/28/2021 01:00:38 - INFO - __main__ -   Batch number = 73
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Evaluating:  87%|████████▋ | 73/84 [00:09<00:01,  7.32it/s]11/28/2021 01:00:38 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:00:38 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:00:38 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
11/28/2021 01:00:38 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:00:38 - INFO - __main__ -   Batch number = 74
11/28/2021 01:00:38 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:00:38 - INFO - root -   loading lang adpater am/wiki@ukp
11/28/2021 01:00:38 - INFO - __main__ -   Adapter Languages : ['am'], Length : 1
11/28/2021 01:00:38 - INFO - __main__ -   Adapter Names ['am/wiki@ukp'], Length : 1
11/28/2021 01:00:38 - INFO - __main__ -   Language = am
11/28/2021 01:00:38 - INFO - __main__ -   Adapter Name = am/wiki@ukp
Found matching adapter at: adapters/ukp/bert-base-multilingual-cased_am_wiki_pfeiffer.json
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/am/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_am_wiki_pfeiffer.zip.
Evaluating:  88%|████████▊ | 74/84 [00:10<00:01,  7.29it/s]11/28/2021 01:00:39 - INFO - __main__ -   Batch number = 75
Evaluating:  89%|████████▉ | 75/84 [00:10<00:01,  7.29it/s]11/28/2021 01:00:39 - INFO - __main__ -   Batch number = 76
Evaluating:  90%|█████████ | 76/84 [00:10<00:01,  7.29it/s]11/28/2021 01:00:39 - INFO - __main__ -   Batch number = 77
Evaluating:  92%|█████████▏| 77/84 [00:10<00:00,  7.32it/s]11/28/2021 01:00:39 - INFO - __main__ -   Batch number = 78
Evaluating:  93%|█████████▎| 78/84 [00:10<00:00,  7.32it/s]11/28/2021 01:00:39 - INFO - __main__ -   Batch number = 79
Evaluating:  94%|█████████▍| 79/84 [00:10<00:00,  7.30it/s]11/28/2021 01:00:39 - INFO - __main__ -   Batch number = 80
Evaluating:  95%|█████████▌| 80/84 [00:10<00:00,  7.30it/s]11/28/2021 01:00:39 - INFO - __main__ -   Batch number = 81
Evaluating:  96%|█████████▋| 81/84 [00:11<00:00,  7.31it/s]11/28/2021 01:00:40 - INFO - __main__ -   Batch number = 82
Evaluating:  98%|█████████▊| 82/84 [00:11<00:00,  7.31it/s]11/28/2021 01:00:40 - INFO - __main__ -   Batch number = 83
Evaluating:  99%|█████████▉| 83/84 [00:11<00:00,  7.29it/s]11/28/2021 01:00:40 - INFO - __main__ -   Batch number = 84
Evaluating: 100%|██████████| 84/84 [00:11<00:00,  7.40it/s]Evaluating: 100%|██████████| 84/84 [00:11<00:00,  7.32it/s]Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/adapter_config.json
Adding adapter 'am' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/head_config.json

/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:00:41 - INFO - __main__ -   ***** Evaluation result  in hi *****
11/28/2021 01:00:41 - INFO - __main__ -     f1 = 0.6409164988975169
11/28/2021 01:00:41 - INFO - __main__ -     loss = 1.1971660795665922
11/28/2021 01:00:41 - INFO - __main__ -     precision = 0.6416177204499213
11/28/2021 01:00:41 - INFO - __main__ -     recall = 0.6402168083965372
28.42user 10.00system 0:34.78elapsed 110%CPU (0avgtext+0avgdata 3931688maxresident)k
2528inputs+9712outputs (0major+1534891minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:00:44 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='hi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:00:44 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:00:44 - INFO - __main__ -   Seed = 2
11/28/2021 01:00:44 - INFO - root -   save model
11/28/2021 01:00:44 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='hi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:00:44 - INFO - __main__ -   Loading pretrained model and tokenizer
11/28/2021 01:00:45 - INFO - __main__ -   Language adapter for ru not found, using am instead
11/28/2021 01:00:45 - INFO - __main__ -   Set active language adapter to am
11/28/2021 01:00:45 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:00:45 - INFO - __main__ -   Adapter Languages = ['am']
11/28/2021 01:00:45 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_ru_bert-base-multilingual-cased_128
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

11/28/2021 01:00:46 - INFO - __main__ -   ***** Running evaluation  in ru *****
11/28/2021 01:00:46 - INFO - __main__ -     Num examples = 8995
11/28/2021 01:00:46 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/282 [00:00<?, ?it/s]11/28/2021 01:00:46 - INFO - __main__ -   Batch number = 1
Evaluating:   0%|          | 1/282 [00:00<00:38,  7.23it/s]11/28/2021 01:00:46 - INFO - __main__ -   Batch number = 2
Evaluating:   1%|          | 2/282 [00:00<00:37,  7.50it/s]11/28/2021 01:00:46 - INFO - __main__ -   Batch number = 3
loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
Evaluating:   1%|          | 3/282 [00:00<00:36,  7.57it/s]11/28/2021 01:00:46 - INFO - __main__ -   Batch number = 4
Evaluating:   1%|▏         | 4/282 [00:00<00:36,  7.61it/s]11/28/2021 01:00:47 - INFO - __main__ -   Batch number = 5
11/28/2021 01:00:47 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
Evaluating:   2%|▏         | 5/282 [00:00<00:36,  7.62it/s]11/28/2021 01:00:47 - INFO - __main__ -   Batch number = 6
Evaluating:   2%|▏         | 6/282 [00:00<00:36,  7.64it/s]11/28/2021 01:00:47 - INFO - __main__ -   Batch number = 7
Evaluating:   2%|▏         | 7/282 [00:00<00:36,  7.61it/s]11/28/2021 01:00:47 - INFO - __main__ -   Batch number = 8
Evaluating:   3%|▎         | 8/282 [00:01<00:36,  7.57it/s]11/28/2021 01:00:47 - INFO - __main__ -   Batch number = 9
Evaluating:   3%|▎         | 9/282 [00:01<00:36,  7.58it/s]11/28/2021 01:00:47 - INFO - __main__ -   Batch number = 10
Evaluating:   4%|▎         | 10/282 [00:01<00:35,  7.58it/s]11/28/2021 01:00:47 - INFO - __main__ -   Batch number = 11
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:   4%|▍         | 11/282 [00:01<00:35,  7.59it/s]11/28/2021 01:00:48 - INFO - __main__ -   Batch number = 12
Evaluating:   4%|▍         | 12/282 [00:01<00:35,  7.61it/s]11/28/2021 01:00:48 - INFO - __main__ -   Batch number = 13
Evaluating:   5%|▍         | 13/282 [00:01<00:35,  7.61it/s]11/28/2021 01:00:48 - INFO - __main__ -   Batch number = 14
Evaluating:   5%|▍         | 14/282 [00:01<00:35,  7.62it/s]11/28/2021 01:00:48 - INFO - __main__ -   Batch number = 15
Evaluating:   5%|▌         | 15/282 [00:01<00:35,  7.59it/s]11/28/2021 01:00:48 - INFO - __main__ -   Batch number = 16
Evaluating:   6%|▌         | 16/282 [00:02<00:35,  7.55it/s]11/28/2021 01:00:48 - INFO - __main__ -   Batch number = 17
Evaluating:   6%|▌         | 17/282 [00:02<00:35,  7.56it/s]11/28/2021 01:00:48 - INFO - __main__ -   Batch number = 18
Evaluating:   6%|▋         | 18/282 [00:02<00:34,  7.55it/s]11/28/2021 01:00:48 - INFO - __main__ -   Batch number = 19
Evaluating:   7%|▋         | 19/282 [00:02<00:34,  7.53it/s]11/28/2021 01:00:49 - INFO - __main__ -   Batch number = 20
Evaluating:   7%|▋         | 20/282 [00:02<00:34,  7.52it/s]11/28/2021 01:00:49 - INFO - __main__ -   Batch number = 21
Evaluating:   7%|▋         | 21/282 [00:02<00:34,  7.52it/s]11/28/2021 01:00:49 - INFO - __main__ -   Batch number = 22
Evaluating:   8%|▊         | 22/282 [00:02<00:34,  7.49it/s]11/28/2021 01:00:49 - INFO - __main__ -   Batch number = 23
Evaluating:   8%|▊         | 23/282 [00:03<00:34,  7.48it/s]11/28/2021 01:00:49 - INFO - __main__ -   Batch number = 24
Evaluating:   9%|▊         | 24/282 [00:03<00:34,  7.50it/s]11/28/2021 01:00:49 - INFO - __main__ -   Batch number = 25
Evaluating:   9%|▉         | 25/282 [00:03<00:34,  7.49it/s]11/28/2021 01:00:49 - INFO - __main__ -   Batch number = 26
Evaluating:   9%|▉         | 26/282 [00:03<00:34,  7.47it/s]11/28/2021 01:00:50 - INFO - __main__ -   Batch number = 27
Evaluating:  10%|▉         | 27/282 [00:03<00:34,  7.48it/s]11/28/2021 01:00:50 - INFO - __main__ -   Batch number = 28
Evaluating:  10%|▉         | 28/282 [00:03<00:34,  7.47it/s]11/28/2021 01:00:50 - INFO - __main__ -   Batch number = 29
Evaluating:  10%|█         | 29/282 [00:03<00:34,  7.37it/s]11/28/2021 01:00:50 - INFO - __main__ -   Batch number = 30
Evaluating:  11%|█         | 30/282 [00:03<00:34,  7.35it/s]11/28/2021 01:00:50 - INFO - __main__ -   Batch number = 31
Evaluating:  11%|█         | 31/282 [00:04<00:33,  7.39it/s]11/28/2021 01:00:50 - INFO - __main__ -   Batch number = 32
Evaluating:  11%|█▏        | 32/282 [00:04<00:33,  7.40it/s]11/28/2021 01:00:50 - INFO - __main__ -   Batch number = 33
Evaluating:  12%|█▏        | 33/282 [00:04<00:33,  7.43it/s]11/28/2021 01:00:50 - INFO - __main__ -   Batch number = 34
Evaluating:  12%|█▏        | 34/282 [00:04<00:33,  7.43it/s]11/28/2021 01:00:51 - INFO - __main__ -   Batch number = 35
Evaluating:  12%|█▏        | 35/282 [00:04<00:33,  7.42it/s]11/28/2021 01:00:51 - INFO - __main__ -   Batch number = 36
Evaluating:  13%|█▎        | 36/282 [00:04<00:33,  7.44it/s]11/28/2021 01:00:51 - INFO - __main__ -   Batch number = 37
Evaluating:  13%|█▎        | 37/282 [00:04<00:32,  7.45it/s]11/28/2021 01:00:51 - INFO - __main__ -   Batch number = 38
Evaluating:  13%|█▎        | 38/282 [00:05<00:32,  7.45it/s]11/28/2021 01:00:51 - INFO - __main__ -   Batch number = 39
Evaluating:  14%|█▍        | 39/282 [00:05<00:32,  7.46it/s]11/28/2021 01:00:51 - INFO - __main__ -   Batch number = 40
Evaluating:  14%|█▍        | 40/282 [00:05<00:32,  7.47it/s]11/28/2021 01:00:51 - INFO - __main__ -   Batch number = 41
Evaluating:  15%|█▍        | 41/282 [00:05<00:32,  7.45it/s]11/28/2021 01:00:52 - INFO - __main__ -   Batch number = 42
Evaluating:  15%|█▍        | 42/282 [00:05<00:32,  7.46it/s]11/28/2021 01:00:52 - INFO - __main__ -   Batch number = 43
Evaluating:  15%|█▌        | 43/282 [00:05<00:32,  7.47it/s]11/28/2021 01:00:52 - INFO - __main__ -   Batch number = 44
Evaluating:  16%|█▌        | 44/282 [00:05<00:32,  7.42it/s]11/28/2021 01:00:52 - INFO - __main__ -   Batch number = 45
Evaluating:  16%|█▌        | 45/282 [00:06<00:31,  7.41it/s]11/28/2021 01:00:52 - INFO - __main__ -   Batch number = 46
Evaluating:  16%|█▋        | 46/282 [00:06<00:31,  7.43it/s]11/28/2021 01:00:52 - INFO - __main__ -   Batch number = 47
Evaluating:  17%|█▋        | 47/282 [00:06<00:31,  7.41it/s]11/28/2021 01:00:52 - INFO - __main__ -   Batch number = 48
Evaluating:  17%|█▋        | 48/282 [00:06<00:31,  7.42it/s]11/28/2021 01:00:52 - INFO - __main__ -   Batch number = 49
Evaluating:  17%|█▋        | 49/282 [00:06<00:31,  7.41it/s]11/28/2021 01:00:53 - INFO - __main__ -   Batch number = 50
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:00:53 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:00:53 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:00:53 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
11/28/2021 01:00:53 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:00:53 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:00:53 - INFO - root -   loading lang adpater am/wiki@ukp
11/28/2021 01:00:53 - INFO - __main__ -   Adapter Languages : ['am'], Length : 1
11/28/2021 01:00:53 - INFO - __main__ -   Adapter Names ['am/wiki@ukp'], Length : 1
11/28/2021 01:00:53 - INFO - __main__ -   Language = am
11/28/2021 01:00:53 - INFO - __main__ -   Adapter Name = am/wiki@ukp
Found matching adapter at: adapters/ukp/bert-base-multilingual-cased_am_wiki_pfeiffer.json
Evaluating:  18%|█▊        | 50/282 [00:06<00:31,  7.41it/s]11/28/2021 01:00:53 - INFO - __main__ -   Batch number = 51
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/am/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_am_wiki_pfeiffer.zip.
Evaluating:  18%|█▊        | 51/282 [00:06<00:31,  7.43it/s]11/28/2021 01:00:53 - INFO - __main__ -   Batch number = 52
Evaluating:  18%|█▊        | 52/282 [00:06<00:31,  7.41it/s]11/28/2021 01:00:53 - INFO - __main__ -   Batch number = 53
Evaluating:  19%|█▉        | 53/282 [00:07<00:30,  7.40it/s]11/28/2021 01:00:53 - INFO - __main__ -   Batch number = 54
Evaluating:  19%|█▉        | 54/282 [00:07<00:30,  7.40it/s]11/28/2021 01:00:53 - INFO - __main__ -   Batch number = 55
Evaluating:  20%|█▉        | 55/282 [00:07<00:30,  7.39it/s]11/28/2021 01:00:53 - INFO - __main__ -   Batch number = 56
Evaluating:  20%|█▉        | 56/282 [00:07<00:30,  7.38it/s]11/28/2021 01:00:54 - INFO - __main__ -   Batch number = 57
Evaluating:  20%|██        | 57/282 [00:07<00:30,  7.37it/s]11/28/2021 01:00:54 - INFO - __main__ -   Batch number = 58
Evaluating:  21%|██        | 58/282 [00:07<00:30,  7.37it/s]11/28/2021 01:00:54 - INFO - __main__ -   Batch number = 59
Evaluating:  21%|██        | 59/282 [00:07<00:30,  7.35it/s]11/28/2021 01:00:54 - INFO - __main__ -   Batch number = 60
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/adapter_config.json
Adding adapter 'am' of type 'text_lang'.
Evaluating:  21%|██▏       | 60/282 [00:08<00:30,  7.36it/s]11/28/2021 01:00:54 - INFO - __main__ -   Batch number = 61
Loading module weights from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/head_config.json
Evaluating:  22%|██▏       | 61/282 [00:08<00:30,  7.31it/s]11/28/2021 01:00:54 - INFO - __main__ -   Batch number = 62
Evaluating:  22%|██▏       | 62/282 [00:08<00:30,  7.20it/s]11/28/2021 01:00:54 - INFO - __main__ -   Batch number = 63
Evaluating:  22%|██▏       | 63/282 [00:08<00:30,  7.24it/s]11/28/2021 01:00:55 - INFO - __main__ -   Batch number = 64
Evaluating:  23%|██▎       | 64/282 [00:08<00:30,  7.16it/s]11/28/2021 01:00:55 - INFO - __main__ -   Batch number = 65
Evaluating:  23%|██▎       | 65/282 [00:08<00:30,  7.11it/s]11/28/2021 01:00:55 - INFO - __main__ -   Batch number = 66
Evaluating:  23%|██▎       | 66/282 [00:08<00:30,  7.09it/s]11/28/2021 01:00:55 - INFO - __main__ -   Batch number = 67
Evaluating:  24%|██▍       | 67/282 [00:09<00:30,  7.08it/s]11/28/2021 01:00:55 - INFO - __main__ -   Batch number = 68
Evaluating:  24%|██▍       | 68/282 [00:09<00:45,  4.74it/s]11/28/2021 01:00:55 - INFO - __main__ -   Batch number = 69
Evaluating:  24%|██▍       | 69/282 [00:09<00:40,  5.25it/s]11/28/2021 01:00:56 - INFO - __main__ -   Batch number = 70
Evaluating:  25%|██▍       | 70/282 [00:09<00:37,  5.67it/s]11/28/2021 01:00:56 - INFO - __main__ -   Batch number = 71
Evaluating:  25%|██▌       | 71/282 [00:09<00:34,  6.03it/s]11/28/2021 01:00:56 - INFO - __main__ -   Batch number = 72
Evaluating:  26%|██▌       | 72/282 [00:09<00:33,  6.32it/s]11/28/2021 01:00:56 - INFO - __main__ -   Batch number = 73
Evaluating:  26%|██▌       | 73/282 [00:10<00:31,  6.56it/s]11/28/2021 01:00:56 - INFO - __main__ -   Batch number = 74
Evaluating:  26%|██▌       | 74/282 [00:10<00:31,  6.70it/s]11/28/2021 01:00:56 - INFO - __main__ -   Batch number = 75
Evaluating:  27%|██▋       | 75/282 [00:10<00:30,  6.80it/s]11/28/2021 01:00:56 - INFO - __main__ -   Batch number = 76
Evaluating:  27%|██▋       | 76/282 [00:10<00:30,  6.86it/s]11/28/2021 01:00:57 - INFO - __main__ -   Batch number = 77
Evaluating:  27%|██▋       | 77/282 [00:10<00:29,  6.90it/s]11/28/2021 01:00:57 - INFO - __main__ -   Batch number = 78
Evaluating:  28%|██▊       | 78/282 [00:10<00:29,  6.93it/s]11/28/2021 01:00:57 - INFO - __main__ -   Batch number = 79
Evaluating:  28%|██▊       | 79/282 [00:10<00:29,  6.95it/s]11/28/2021 01:00:57 - INFO - __main__ -   Batch number = 80
Evaluating:  28%|██▊       | 80/282 [00:11<00:29,  6.92it/s]11/28/2021 01:00:57 - INFO - __main__ -   Batch number = 81
Evaluating:  29%|██▊       | 81/282 [00:11<00:29,  6.93it/s]11/28/2021 01:00:57 - INFO - __main__ -   Batch number = 82
Evaluating:  29%|██▉       | 82/282 [00:11<00:28,  7.00it/s]11/28/2021 01:00:57 - INFO - __main__ -   Batch number = 83
Evaluating:  29%|██▉       | 83/282 [00:11<00:28,  6.96it/s]11/28/2021 01:00:58 - INFO - __main__ -   Batch number = 84
Evaluating:  30%|██▉       | 84/282 [00:11<00:28,  6.92it/s]11/28/2021 01:00:58 - INFO - __main__ -   Batch number = 85
Evaluating:  30%|███       | 85/282 [00:11<00:28,  6.97it/s]11/28/2021 01:00:58 - INFO - __main__ -   Batch number = 86
Evaluating:  30%|███       | 86/282 [00:11<00:27,  7.02it/s]11/28/2021 01:00:58 - INFO - __main__ -   Batch number = 87
Evaluating:  31%|███       | 87/282 [00:12<00:27,  6.99it/s]11/28/2021 01:00:58 - INFO - __main__ -   Batch number = 88
Evaluating:  31%|███       | 88/282 [00:12<00:27,  7.00it/s]11/28/2021 01:00:58 - INFO - __main__ -   Batch number = 89
Evaluating:  32%|███▏      | 89/282 [00:12<00:27,  6.97it/s]11/28/2021 01:00:58 - INFO - __main__ -   Batch number = 90
Evaluating:  32%|███▏      | 90/282 [00:12<00:27,  7.01it/s]11/28/2021 01:00:59 - INFO - __main__ -   Batch number = 91
Evaluating:  32%|███▏      | 91/282 [00:12<00:27,  6.93it/s]11/28/2021 01:00:59 - INFO - __main__ -   Batch number = 92
Evaluating:  33%|███▎      | 92/282 [00:12<00:27,  6.92it/s]11/28/2021 01:00:59 - INFO - __main__ -   Batch number = 93
Evaluating:  33%|███▎      | 93/282 [00:12<00:27,  6.92it/s]11/28/2021 01:00:59 - INFO - __main__ -   Batch number = 94
Evaluating:  33%|███▎      | 94/282 [00:13<00:27,  6.95it/s]11/28/2021 01:00:59 - INFO - __main__ -   Batch number = 95
Evaluating:  34%|███▎      | 95/282 [00:13<00:26,  6.99it/s]11/28/2021 01:00:59 - INFO - __main__ -   Batch number = 96
Evaluating:  34%|███▍      | 96/282 [00:13<00:26,  7.00it/s]11/28/2021 01:00:59 - INFO - __main__ -   Batch number = 97
Evaluating:  34%|███▍      | 97/282 [00:13<00:26,  7.05it/s]11/28/2021 01:01:00 - INFO - __main__ -   Batch number = 98
Evaluating:  35%|███▍      | 98/282 [00:13<00:26,  7.06it/s]11/28/2021 01:01:00 - INFO - __main__ -   Batch number = 99
Evaluating:  35%|███▌      | 99/282 [00:13<00:26,  6.98it/s]11/28/2021 01:01:00 - INFO - __main__ -   Batch number = 100
Evaluating:  35%|███▌      | 100/282 [00:13<00:26,  6.91it/s]11/28/2021 01:01:00 - INFO - __main__ -   Batch number = 101
Evaluating:  36%|███▌      | 101/282 [00:14<00:26,  6.87it/s]11/28/2021 01:01:00 - INFO - __main__ -   Batch number = 102
11/28/2021 01:01:00 - INFO - __main__ -   Language adapter for hi not found, using am instead
11/28/2021 01:01:00 - INFO - __main__ -   Set active language adapter to am
11/28/2021 01:01:00 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:01:00 - INFO - __main__ -   Adapter Languages = ['am']
11/28/2021 01:01:00 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_hi_bert-base-multilingual-cased_128
Evaluating:  36%|███▌      | 102/282 [00:14<00:27,  6.57it/s]11/28/2021 01:01:00 - INFO - __main__ -   Batch number = 103
Evaluating:  37%|███▋      | 103/282 [00:14<00:26,  6.75it/s]11/28/2021 01:01:01 - INFO - __main__ -   Batch number = 104
Evaluating:  37%|███▋      | 104/282 [00:14<00:25,  6.87it/s]11/28/2021 01:01:01 - INFO - __main__ -   Batch number = 105
11/28/2021 01:01:01 - INFO - __main__ -   ***** Running evaluation  in hi *****
11/28/2021 01:01:01 - INFO - __main__ -     Num examples = 2685
11/28/2021 01:01:01 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/84 [00:00<?, ?it/s]11/28/2021 01:01:01 - INFO - __main__ -   Batch number = 1
Evaluating:  37%|███▋      | 105/282 [00:14<00:25,  6.95it/s]11/28/2021 01:01:01 - INFO - __main__ -   Batch number = 106
Evaluating:   1%|          | 1/84 [00:00<00:23,  3.56it/s]11/28/2021 01:01:01 - INFO - __main__ -   Batch number = 2
Evaluating:  38%|███▊      | 106/282 [00:15<00:33,  5.29it/s]11/28/2021 01:01:01 - INFO - __main__ -   Batch number = 107
Evaluating:   2%|▏         | 2/84 [00:00<00:23,  3.54it/s]11/28/2021 01:01:01 - INFO - __main__ -   Batch number = 3
Evaluating:  38%|███▊      | 107/282 [00:15<00:38,  4.51it/s]11/28/2021 01:01:01 - INFO - __main__ -   Batch number = 108
Evaluating:   4%|▎         | 3/84 [00:00<00:22,  3.53it/s]11/28/2021 01:01:02 - INFO - __main__ -   Batch number = 4
Evaluating:  38%|███▊      | 108/282 [00:15<00:42,  4.07it/s]11/28/2021 01:01:02 - INFO - __main__ -   Batch number = 109
Evaluating:   5%|▍         | 4/84 [00:01<00:22,  3.48it/s]11/28/2021 01:01:02 - INFO - __main__ -   Batch number = 5
Evaluating:  39%|███▊      | 109/282 [00:15<00:45,  3.78it/s]11/28/2021 01:01:02 - INFO - __main__ -   Batch number = 110
Evaluating:   6%|▌         | 5/84 [00:01<00:23,  3.43it/s]11/28/2021 01:01:02 - INFO - __main__ -   Batch number = 6
Evaluating:  39%|███▉      | 110/282 [00:16<00:49,  3.49it/s]11/28/2021 01:01:02 - INFO - __main__ -   Batch number = 111
Evaluating:   7%|▋         | 6/84 [00:01<00:22,  3.52it/s]11/28/2021 01:01:02 - INFO - __main__ -   Batch number = 7
Evaluating:  39%|███▉      | 111/282 [00:16<00:50,  3.40it/s]11/28/2021 01:01:03 - INFO - __main__ -   Batch number = 112
Evaluating:   8%|▊         | 7/84 [00:02<00:22,  3.46it/s]11/28/2021 01:01:03 - INFO - __main__ -   Batch number = 8
Evaluating:  40%|███▉      | 112/282 [00:16<00:51,  3.33it/s]11/28/2021 01:01:03 - INFO - __main__ -   Batch number = 113
Evaluating:  10%|▉         | 8/84 [00:02<00:22,  3.43it/s]11/28/2021 01:01:03 - INFO - __main__ -   Batch number = 9
Evaluating:  40%|████      | 113/282 [00:17<00:51,  3.28it/s]11/28/2021 01:01:03 - INFO - __main__ -   Batch number = 114
Evaluating:  11%|█         | 9/84 [00:02<00:21,  3.41it/s]11/28/2021 01:01:03 - INFO - __main__ -   Batch number = 10
Evaluating:  40%|████      | 114/282 [00:17<00:51,  3.24it/s]11/28/2021 01:01:04 - INFO - __main__ -   Batch number = 115
Evaluating:  12%|█▏        | 10/84 [00:02<00:21,  3.42it/s]11/28/2021 01:01:04 - INFO - __main__ -   Batch number = 11
Evaluating:  41%|████      | 115/282 [00:17<00:51,  3.23it/s]11/28/2021 01:01:04 - INFO - __main__ -   Batch number = 116
Evaluating:  13%|█▎        | 11/84 [00:03<00:21,  3.40it/s]11/28/2021 01:01:04 - INFO - __main__ -   Batch number = 12
Evaluating:  41%|████      | 116/282 [00:18<00:51,  3.22it/s]11/28/2021 01:01:04 - INFO - __main__ -   Batch number = 117
Evaluating:  14%|█▍        | 12/84 [00:03<00:21,  3.40it/s]11/28/2021 01:01:04 - INFO - __main__ -   Batch number = 13
Evaluating:  41%|████▏     | 117/282 [00:18<00:51,  3.20it/s]11/28/2021 01:01:05 - INFO - __main__ -   Batch number = 118
Evaluating:  15%|█▌        | 13/84 [00:03<00:20,  3.39it/s]11/28/2021 01:01:05 - INFO - __main__ -   Batch number = 14
Evaluating:  42%|████▏     | 118/282 [00:18<00:51,  3.20it/s]11/28/2021 01:01:05 - INFO - __main__ -   Batch number = 119
Evaluating:  17%|█▋        | 14/84 [00:04<00:20,  3.38it/s]11/28/2021 01:01:05 - INFO - __main__ -   Batch number = 15
Evaluating:  42%|████▏     | 119/282 [00:19<00:51,  3.19it/s]Evaluating:  18%|█▊        | 15/84 [00:04<00:20,  3.37it/s]11/28/2021 01:01:05 - INFO - __main__ -   Batch number = 16
11/28/2021 01:01:05 - INFO - __main__ -   Batch number = 120
Evaluating:  19%|█▉        | 16/84 [00:04<00:21,  3.21it/s]11/28/2021 01:01:06 - INFO - __main__ -   Batch number = 17
Evaluating:  43%|████▎     | 120/282 [00:19<00:53,  3.04it/s]11/28/2021 01:01:06 - INFO - __main__ -   Batch number = 121
Evaluating:  20%|██        | 17/84 [00:05<00:20,  3.32it/s]11/28/2021 01:01:06 - INFO - __main__ -   Batch number = 18
Evaluating:  43%|████▎     | 121/282 [00:19<00:51,  3.13it/s]11/28/2021 01:01:06 - INFO - __main__ -   Batch number = 122
Evaluating:  21%|██▏       | 18/84 [00:05<00:19,  3.38it/s]11/28/2021 01:01:06 - INFO - __main__ -   Batch number = 19
Evaluating:  43%|████▎     | 122/282 [00:20<00:50,  3.18it/s]11/28/2021 01:01:06 - INFO - __main__ -   Batch number = 123
Evaluating:  23%|██▎       | 19/84 [00:05<00:19,  3.40it/s]11/28/2021 01:01:06 - INFO - __main__ -   Batch number = 20
Evaluating:  44%|████▎     | 123/282 [00:20<00:49,  3.21it/s]11/28/2021 01:01:06 - INFO - __main__ -   Batch number = 124
Evaluating:  24%|██▍       | 20/84 [00:05<00:18,  3.40it/s]11/28/2021 01:01:07 - INFO - __main__ -   Batch number = 21
Evaluating:  44%|████▍     | 124/282 [00:20<00:48,  3.23it/s]11/28/2021 01:01:07 - INFO - __main__ -   Batch number = 125
Evaluating:  25%|██▌       | 21/84 [00:06<00:18,  3.39it/s]11/28/2021 01:01:07 - INFO - __main__ -   Batch number = 22
Evaluating:  44%|████▍     | 125/282 [00:20<00:48,  3.22it/s]11/28/2021 01:01:07 - INFO - __main__ -   Batch number = 126
Evaluating:  26%|██▌       | 22/84 [00:06<00:18,  3.38it/s]11/28/2021 01:01:07 - INFO - __main__ -   Batch number = 23
Evaluating:  45%|████▍     | 126/282 [00:21<00:48,  3.22it/s]11/28/2021 01:01:07 - INFO - __main__ -   Batch number = 127
Evaluating:  27%|██▋       | 23/84 [00:06<00:18,  3.36it/s]11/28/2021 01:01:08 - INFO - __main__ -   Batch number = 24
Evaluating:  45%|████▌     | 127/282 [00:21<00:48,  3.21it/s]11/28/2021 01:01:08 - INFO - __main__ -   Batch number = 128
Evaluating:  29%|██▊       | 24/84 [00:07<00:17,  3.37it/s]11/28/2021 01:01:08 - INFO - __main__ -   Batch number = 25
Evaluating:  45%|████▌     | 128/282 [00:21<00:47,  3.21it/s]11/28/2021 01:01:08 - INFO - __main__ -   Batch number = 129
Evaluating:  30%|██▉       | 25/84 [00:07<00:17,  3.36it/s]11/28/2021 01:01:08 - INFO - __main__ -   Batch number = 26
Evaluating:  46%|████▌     | 129/282 [00:22<00:47,  3.20it/s]11/28/2021 01:01:08 - INFO - __main__ -   Batch number = 130
Evaluating:  31%|███       | 26/84 [00:07<00:17,  3.36it/s]11/28/2021 01:01:08 - INFO - __main__ -   Batch number = 27
Evaluating:  46%|████▌     | 130/282 [00:22<00:47,  3.21it/s]11/28/2021 01:01:09 - INFO - __main__ -   Batch number = 131
Evaluating:  32%|███▏      | 27/84 [00:07<00:16,  3.35it/s]11/28/2021 01:01:09 - INFO - __main__ -   Batch number = 28
Evaluating:  46%|████▋     | 131/282 [00:22<00:47,  3.20it/s]11/28/2021 01:01:09 - INFO - __main__ -   Batch number = 132
Evaluating:  33%|███▎      | 28/84 [00:08<00:16,  3.35it/s]11/28/2021 01:01:09 - INFO - __main__ -   Batch number = 29
Evaluating:  47%|████▋     | 132/282 [00:23<00:46,  3.21it/s]11/28/2021 01:01:09 - INFO - __main__ -   Batch number = 133
Evaluating:  35%|███▍      | 29/84 [00:08<00:16,  3.35it/s]11/28/2021 01:01:09 - INFO - __main__ -   Batch number = 30
Evaluating:  47%|████▋     | 133/282 [00:23<00:46,  3.21it/s]11/28/2021 01:01:10 - INFO - __main__ -   Batch number = 134
Evaluating:  36%|███▌      | 30/84 [00:08<00:16,  3.35it/s]11/28/2021 01:01:10 - INFO - __main__ -   Batch number = 31
Evaluating:  48%|████▊     | 134/282 [00:23<00:46,  3.17it/s]11/28/2021 01:01:10 - INFO - __main__ -   Batch number = 135
Evaluating:  37%|███▋      | 31/84 [00:09<00:15,  3.40it/s]11/28/2021 01:01:10 - INFO - __main__ -   Batch number = 32
Evaluating:  48%|████▊     | 135/282 [00:24<00:46,  3.14it/s]Evaluating:  38%|███▊      | 32/84 [00:09<00:15,  3.44it/s]11/28/2021 01:01:10 - INFO - __main__ -   Batch number = 33
11/28/2021 01:01:10 - INFO - __main__ -   Batch number = 136
Evaluating:  39%|███▉      | 33/84 [00:09<00:15,  3.29it/s]11/28/2021 01:01:11 - INFO - __main__ -   Batch number = 34
Evaluating:  48%|████▊     | 136/282 [00:24<00:48,  3.02it/s]11/28/2021 01:01:11 - INFO - __main__ -   Batch number = 137
Evaluating:  40%|████      | 34/84 [00:10<00:15,  3.32it/s]11/28/2021 01:01:11 - INFO - __main__ -   Batch number = 35
Evaluating:  49%|████▊     | 137/282 [00:24<00:47,  3.06it/s]11/28/2021 01:01:11 - INFO - __main__ -   Batch number = 138
Evaluating:  42%|████▏     | 35/84 [00:10<00:14,  3.34it/s]11/28/2021 01:01:11 - INFO - __main__ -   Batch number = 36
Evaluating:  49%|████▉     | 138/282 [00:25<00:46,  3.10it/s]11/28/2021 01:01:11 - INFO - __main__ -   Batch number = 139
Evaluating:  43%|████▎     | 36/84 [00:10<00:14,  3.34it/s]11/28/2021 01:01:11 - INFO - __main__ -   Batch number = 37
Evaluating:  49%|████▉     | 139/282 [00:25<00:45,  3.12it/s]11/28/2021 01:01:11 - INFO - __main__ -   Batch number = 140
Evaluating:  44%|████▍     | 37/84 [00:10<00:14,  3.34it/s]11/28/2021 01:01:12 - INFO - __main__ -   Batch number = 38
Evaluating:  50%|████▉     | 140/282 [00:25<00:44,  3.16it/s]11/28/2021 01:01:12 - INFO - __main__ -   Batch number = 141
Evaluating:  45%|████▌     | 38/84 [00:11<00:13,  3.33it/s]11/28/2021 01:01:12 - INFO - __main__ -   Batch number = 39
Evaluating:  50%|█████     | 141/282 [00:26<00:44,  3.18it/s]11/28/2021 01:01:12 - INFO - __main__ -   Batch number = 142
Evaluating:  46%|████▋     | 39/84 [00:11<00:13,  3.34it/s]11/28/2021 01:01:12 - INFO - __main__ -   Batch number = 40
Evaluating:  50%|█████     | 142/282 [00:26<00:43,  3.19it/s]11/28/2021 01:01:12 - INFO - __main__ -   Batch number = 143
Evaluating:  48%|████▊     | 40/84 [00:11<00:13,  3.34it/s]11/28/2021 01:01:13 - INFO - __main__ -   Batch number = 41
Evaluating:  51%|█████     | 143/282 [00:26<00:43,  3.19it/s]11/28/2021 01:01:13 - INFO - __main__ -   Batch number = 144
Evaluating:  49%|████▉     | 41/84 [00:12<00:12,  3.35it/s]11/28/2021 01:01:13 - INFO - __main__ -   Batch number = 42
Evaluating:  51%|█████     | 144/282 [00:26<00:42,  3.21it/s]11/28/2021 01:01:13 - INFO - __main__ -   Batch number = 145
Evaluating:  50%|█████     | 42/84 [00:12<00:12,  3.36it/s]11/28/2021 01:01:13 - INFO - __main__ -   Batch number = 43
Evaluating:  51%|█████▏    | 145/282 [00:27<00:42,  3.21it/s]11/28/2021 01:01:13 - INFO - __main__ -   Batch number = 146
Evaluating:  51%|█████     | 43/84 [00:12<00:12,  3.36it/s]11/28/2021 01:01:14 - INFO - __main__ -   Batch number = 44
Evaluating:  52%|█████▏    | 146/282 [00:27<00:42,  3.18it/s]11/28/2021 01:01:14 - INFO - __main__ -   Batch number = 147
Evaluating:  52%|█████▏    | 44/84 [00:13<00:11,  3.39it/s]11/28/2021 01:01:14 - INFO - __main__ -   Batch number = 45
Evaluating:  52%|█████▏    | 147/282 [00:27<00:42,  3.19it/s]11/28/2021 01:01:14 - INFO - __main__ -   Batch number = 148
Evaluating:  54%|█████▎    | 45/84 [00:13<00:11,  3.40it/s]11/28/2021 01:01:14 - INFO - __main__ -   Batch number = 46
Evaluating:  52%|█████▏    | 148/282 [00:28<00:41,  3.21it/s]11/28/2021 01:01:14 - INFO - __main__ -   Batch number = 149
Evaluating:  55%|█████▍    | 46/84 [00:13<00:11,  3.40it/s]11/28/2021 01:01:14 - INFO - __main__ -   Batch number = 47
Evaluating:  53%|█████▎    | 149/282 [00:28<00:41,  3.21it/s]11/28/2021 01:01:15 - INFO - __main__ -   Batch number = 150
Evaluating:  56%|█████▌    | 47/84 [00:13<00:10,  3.39it/s]11/28/2021 01:01:15 - INFO - __main__ -   Batch number = 48
Evaluating:  53%|█████▎    | 150/282 [00:28<00:41,  3.21it/s]11/28/2021 01:01:15 - INFO - __main__ -   Batch number = 151
Evaluating:  57%|█████▋    | 48/84 [00:14<00:10,  3.37it/s]11/28/2021 01:01:15 - INFO - __main__ -   Batch number = 49
Evaluating:  54%|█████▎    | 151/282 [00:29<00:40,  3.22it/s]11/28/2021 01:01:15 - INFO - __main__ -   Batch number = 152
Evaluating:  58%|█████▊    | 49/84 [00:14<00:10,  3.46it/s]11/28/2021 01:01:15 - INFO - __main__ -   Batch number = 50
Evaluating:  60%|█████▉    | 50/84 [00:14<00:09,  3.42it/s]11/28/2021 01:01:16 - INFO - __main__ -   Batch number = 51
Evaluating:  54%|█████▍    | 152/282 [00:29<00:41,  3.14it/s]11/28/2021 01:01:16 - INFO - __main__ -   Batch number = 153
Evaluating:  61%|██████    | 51/84 [00:15<00:09,  3.41it/s]11/28/2021 01:01:16 - INFO - __main__ -   Batch number = 52
Evaluating:  54%|█████▍    | 153/282 [00:29<00:40,  3.17it/s]11/28/2021 01:01:16 - INFO - __main__ -   Batch number = 154
Evaluating:  62%|██████▏   | 52/84 [00:15<00:09,  3.42it/s]11/28/2021 01:01:16 - INFO - __main__ -   Batch number = 53
Evaluating:  55%|█████▍    | 154/282 [00:30<00:40,  3.17it/s]11/28/2021 01:01:16 - INFO - __main__ -   Batch number = 155
Evaluating:  63%|██████▎   | 53/84 [00:15<00:08,  3.45it/s]11/28/2021 01:01:16 - INFO - __main__ -   Batch number = 54
Evaluating:  55%|█████▍    | 155/282 [00:30<00:39,  3.20it/s]11/28/2021 01:01:16 - INFO - __main__ -   Batch number = 156
Evaluating:  64%|██████▍   | 54/84 [00:15<00:08,  3.43it/s]11/28/2021 01:01:17 - INFO - __main__ -   Batch number = 55
Evaluating:  55%|█████▌    | 156/282 [00:30<00:39,  3.17it/s]11/28/2021 01:01:17 - INFO - __main__ -   Batch number = 157
Evaluating:  65%|██████▌   | 55/84 [00:16<00:08,  3.44it/s]11/28/2021 01:01:17 - INFO - __main__ -   Batch number = 56
Evaluating:  56%|█████▌    | 157/282 [00:31<00:39,  3.19it/s]11/28/2021 01:01:17 - INFO - __main__ -   Batch number = 158
Evaluating:  67%|██████▋   | 56/84 [00:16<00:08,  3.42it/s]11/28/2021 01:01:17 - INFO - __main__ -   Batch number = 57
Evaluating:  56%|█████▌    | 158/282 [00:31<00:38,  3.19it/s]11/28/2021 01:01:17 - INFO - __main__ -   Batch number = 159
Evaluating:  68%|██████▊   | 57/84 [00:16<00:07,  3.46it/s]11/28/2021 01:01:18 - INFO - __main__ -   Batch number = 58
Evaluating:  56%|█████▋    | 159/282 [00:31<00:38,  3.17it/s]11/28/2021 01:01:18 - INFO - __main__ -   Batch number = 160
Evaluating:  69%|██████▉   | 58/84 [00:17<00:07,  3.50it/s]11/28/2021 01:01:18 - INFO - __main__ -   Batch number = 59
Evaluating:  57%|█████▋    | 160/282 [00:31<00:38,  3.17it/s]11/28/2021 01:01:18 - INFO - __main__ -   Batch number = 161
Evaluating:  70%|███████   | 59/84 [00:17<00:07,  3.51it/s]11/28/2021 01:01:18 - INFO - __main__ -   Batch number = 60
Evaluating:  57%|█████▋    | 161/282 [00:32<00:37,  3.19it/s]11/28/2021 01:01:18 - INFO - __main__ -   Batch number = 162
Evaluating:  71%|███████▏  | 60/84 [00:17<00:06,  3.47it/s]11/28/2021 01:01:18 - INFO - __main__ -   Batch number = 61
Evaluating:  57%|█████▋    | 162/282 [00:32<00:37,  3.17it/s]11/28/2021 01:01:19 - INFO - __main__ -   Batch number = 163
Evaluating:  73%|███████▎  | 61/84 [00:17<00:06,  3.47it/s]11/28/2021 01:01:19 - INFO - __main__ -   Batch number = 62
Evaluating:  58%|█████▊    | 163/282 [00:32<00:37,  3.20it/s]11/28/2021 01:01:19 - INFO - __main__ -   Batch number = 164
Evaluating:  74%|███████▍  | 62/84 [00:18<00:06,  3.46it/s]11/28/2021 01:01:19 - INFO - __main__ -   Batch number = 63
Evaluating:  58%|█████▊    | 164/282 [00:33<00:36,  3.21it/s]Evaluating:  75%|███████▌  | 63/84 [00:18<00:06,  3.46it/s]11/28/2021 01:01:19 - INFO - __main__ -   Batch number = 165
11/28/2021 01:01:19 - INFO - __main__ -   Batch number = 64
Evaluating:  76%|███████▌  | 64/84 [00:18<00:05,  3.44it/s]11/28/2021 01:01:20 - INFO - __main__ -   Batch number = 65
Evaluating:  59%|█████▊    | 165/282 [00:33<00:35,  3.26it/s]11/28/2021 01:01:20 - INFO - __main__ -   Batch number = 166
Evaluating:  77%|███████▋  | 65/84 [00:19<00:05,  3.47it/s]11/28/2021 01:01:20 - INFO - __main__ -   Batch number = 66
Evaluating:  59%|█████▉    | 166/282 [00:33<00:35,  3.26it/s]11/28/2021 01:01:20 - INFO - __main__ -   Batch number = 167
Evaluating:  79%|███████▊  | 66/84 [00:19<00:05,  3.53it/s]11/28/2021 01:01:20 - INFO - __main__ -   Batch number = 67
Evaluating:  59%|█████▉    | 167/282 [00:34<00:35,  3.29it/s]11/28/2021 01:01:20 - INFO - __main__ -   Batch number = 168
Evaluating:  80%|███████▉  | 67/84 [00:19<00:04,  3.50it/s]11/28/2021 01:01:20 - INFO - __main__ -   Batch number = 68
Evaluating:  60%|█████▉    | 168/282 [00:34<00:35,  3.24it/s]11/28/2021 01:01:21 - INFO - __main__ -   Batch number = 169
Evaluating:  81%|████████  | 68/84 [00:19<00:04,  3.51it/s]11/28/2021 01:01:21 - INFO - __main__ -   Batch number = 69
Evaluating:  60%|█████▉    | 169/282 [00:34<00:34,  3.24it/s]11/28/2021 01:01:21 - INFO - __main__ -   Batch number = 170
Evaluating:  82%|████████▏ | 69/84 [00:20<00:04,  3.47it/s]11/28/2021 01:01:21 - INFO - __main__ -   Batch number = 70
Evaluating:  60%|██████    | 170/282 [00:35<00:35,  3.19it/s]11/28/2021 01:01:21 - INFO - __main__ -   Batch number = 171
Evaluating:  83%|████████▎ | 70/84 [00:20<00:04,  3.48it/s]11/28/2021 01:01:21 - INFO - __main__ -   Batch number = 71
Evaluating:  61%|██████    | 171/282 [00:35<00:34,  3.19it/s]11/28/2021 01:01:21 - INFO - __main__ -   Batch number = 172
Evaluating:  85%|████████▍ | 71/84 [00:20<00:03,  3.45it/s]11/28/2021 01:01:22 - INFO - __main__ -   Batch number = 72
Evaluating:  61%|██████    | 172/282 [00:35<00:34,  3.15it/s]11/28/2021 01:01:22 - INFO - __main__ -   Batch number = 173
Evaluating:  86%|████████▌ | 72/84 [00:21<00:03,  3.56it/s]11/28/2021 01:01:22 - INFO - __main__ -   Batch number = 73
Evaluating:  61%|██████▏   | 173/282 [00:36<00:34,  3.19it/s]11/28/2021 01:01:22 - INFO - __main__ -   Batch number = 174
Evaluating:  87%|████████▋ | 73/84 [00:21<00:03,  3.53it/s]11/28/2021 01:01:22 - INFO - __main__ -   Batch number = 74
Evaluating:  62%|██████▏   | 174/282 [00:36<00:34,  3.16it/s]11/28/2021 01:01:22 - INFO - __main__ -   Batch number = 175
Evaluating:  88%|████████▊ | 74/84 [00:21<00:02,  3.52it/s]11/28/2021 01:01:22 - INFO - __main__ -   Batch number = 75
Evaluating:  62%|██████▏   | 175/282 [00:36<00:33,  3.19it/s]11/28/2021 01:01:23 - INFO - __main__ -   Batch number = 176
Evaluating:  89%|████████▉ | 75/84 [00:21<00:02,  3.45it/s]11/28/2021 01:01:23 - INFO - __main__ -   Batch number = 76
Evaluating:  90%|█████████ | 76/84 [00:22<00:02,  3.41it/s]11/28/2021 01:01:23 - INFO - __main__ -   Batch number = 77
Evaluating:  62%|██████▏   | 176/282 [00:36<00:33,  3.19it/s]11/28/2021 01:01:23 - INFO - __main__ -   Batch number = 177
Evaluating:  92%|█████████▏| 77/84 [00:22<00:02,  3.43it/s]11/28/2021 01:01:23 - INFO - __main__ -   Batch number = 78
Evaluating:  63%|██████▎   | 177/282 [00:37<00:32,  3.22it/s]11/28/2021 01:01:23 - INFO - __main__ -   Batch number = 178
Evaluating:  93%|█████████▎| 78/84 [00:22<00:01,  3.48it/s]11/28/2021 01:01:24 - INFO - __main__ -   Batch number = 79
Evaluating:  63%|██████▎   | 178/282 [00:37<00:32,  3.23it/s]11/28/2021 01:01:24 - INFO - __main__ -   Batch number = 179
Evaluating:  94%|█████████▍| 79/84 [00:23<00:01,  3.52it/s]11/28/2021 01:01:24 - INFO - __main__ -   Batch number = 80
Evaluating:  63%|██████▎   | 179/282 [00:37<00:31,  3.26it/s]11/28/2021 01:01:24 - INFO - __main__ -   Batch number = 180
Evaluating:  95%|█████████▌| 80/84 [00:23<00:01,  3.48it/s]11/28/2021 01:01:24 - INFO - __main__ -   Batch number = 81
Evaluating:  64%|██████▍   | 180/282 [00:38<00:31,  3.23it/s]11/28/2021 01:01:24 - INFO - __main__ -   Batch number = 181
Evaluating:  96%|█████████▋| 81/84 [00:23<00:00,  3.50it/s]11/28/2021 01:01:24 - INFO - __main__ -   Batch number = 82
Evaluating:  64%|██████▍   | 181/282 [00:38<00:31,  3.24it/s]11/28/2021 01:01:25 - INFO - __main__ -   Batch number = 182
Evaluating:  98%|█████████▊| 82/84 [00:23<00:00,  3.46it/s]11/28/2021 01:01:25 - INFO - __main__ -   Batch number = 83
Evaluating:  65%|██████▍   | 182/282 [00:38<00:31,  3.20it/s]11/28/2021 01:01:25 - INFO - __main__ -   Batch number = 183
Evaluating:  99%|█████████▉| 83/84 [00:24<00:00,  3.46it/s]11/28/2021 01:01:25 - INFO - __main__ -   Batch number = 84
Evaluating:  65%|██████▍   | 183/282 [00:39<00:30,  3.21it/s]11/28/2021 01:01:25 - INFO - __main__ -   Batch number = 184
Evaluating: 100%|██████████| 84/84 [00:24<00:00,  3.51it/s]Evaluating: 100%|██████████| 84/84 [00:24<00:00,  3.42it/s]Evaluating:  65%|██████▌   | 184/282 [00:39<00:27,  3.51it/s]11/28/2021 01:01:25 - INFO - __main__ -   Batch number = 185
Evaluating:  66%|██████▌   | 185/282 [00:39<00:23,  4.10it/s]11/28/2021 01:01:26 - INFO - __main__ -   Batch number = 186
Evaluating:  66%|██████▌   | 186/282 [00:39<00:20,  4.59it/s]11/28/2021 01:01:26 - INFO - __main__ -   Batch number = 187
Evaluating:  66%|██████▋   | 187/282 [00:39<00:18,  5.04it/s]11/28/2021 01:01:26 - INFO - __main__ -   Batch number = 188
Evaluating:  67%|██████▋   | 188/282 [00:39<00:17,  5.32it/s]11/28/2021 01:01:26 - INFO - __main__ -   Batch number = 189
Evaluating:  67%|██████▋   | 189/282 [00:40<00:16,  5.65it/s]11/28/2021 01:01:26 - INFO - __main__ -   Batch number = 190
Evaluating:  67%|██████▋   | 190/282 [00:40<00:15,  5.85it/s]11/28/2021 01:01:26 - INFO - __main__ -   Batch number = 191
Evaluating:  68%|██████▊   | 191/282 [00:40<00:14,  6.13it/s]11/28/2021 01:01:27 - INFO - __main__ -   Batch number = 192
Evaluating:  68%|██████▊   | 192/282 [00:40<00:14,  6.20it/s]11/28/2021 01:01:27 - INFO - __main__ -   Batch number = 193

/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:01:27 - INFO - __main__ -   ***** Evaluation result  in hi *****
11/28/2021 01:01:27 - INFO - __main__ -     f1 = 0.6024851911165996
11/28/2021 01:01:27 - INFO - __main__ -     loss = 1.333756909483955
11/28/2021 01:01:27 - INFO - __main__ -     precision = 0.6069942848256289
11/28/2021 01:01:27 - INFO - __main__ -     recall = 0.5980425955718992
Evaluating:  68%|██████▊   | 193/282 [00:40<00:13,  6.37it/s]11/28/2021 01:01:27 - INFO - __main__ -   Batch number = 194
Evaluating:  69%|██████▉   | 194/282 [00:40<00:13,  6.37it/s]11/28/2021 01:01:27 - INFO - __main__ -   Batch number = 195
Evaluating:  69%|██████▉   | 195/282 [00:41<00:13,  6.51it/s]11/28/2021 01:01:27 - INFO - __main__ -   Batch number = 196
Evaluating:  70%|██████▉   | 196/282 [00:41<00:13,  6.49it/s]11/28/2021 01:01:27 - INFO - __main__ -   Batch number = 197
Evaluating:  70%|██████▉   | 197/282 [00:41<00:12,  6.62it/s]11/28/2021 01:01:27 - INFO - __main__ -   Batch number = 198
35.53user 13.87system 0:45.44elapsed 108%CPU (0avgtext+0avgdata 3937108maxresident)k
0inputs+688outputs (0major+1665498minor)pagefaults 0swaps
Evaluating:  70%|███████   | 198/282 [00:41<00:12,  6.55it/s]11/28/2021 01:01:28 - INFO - __main__ -   Batch number = 199
Evaluating:  71%|███████   | 199/282 [00:41<00:12,  6.50it/s]11/28/2021 01:01:28 - INFO - __main__ -   Batch number = 200
Evaluating:  71%|███████   | 200/282 [00:41<00:12,  6.34it/s]11/28/2021 01:01:28 - INFO - __main__ -   Batch number = 201
Evaluating:  71%|███████▏  | 201/282 [00:41<00:12,  6.48it/s]11/28/2021 01:01:28 - INFO - __main__ -   Batch number = 202
Evaluating:  72%|███████▏  | 202/282 [00:42<00:12,  6.48it/s]11/28/2021 01:01:28 - INFO - __main__ -   Batch number = 203
Evaluating:  72%|███████▏  | 203/282 [00:42<00:12,  6.57it/s]11/28/2021 01:01:28 - INFO - __main__ -   Batch number = 204
Evaluating:  72%|███████▏  | 204/282 [00:42<00:11,  6.53it/s]11/28/2021 01:01:29 - INFO - __main__ -   Batch number = 205
Evaluating:  73%|███████▎  | 205/282 [00:42<00:11,  6.62it/s]11/28/2021 01:01:29 - INFO - __main__ -   Batch number = 206
Evaluating:  73%|███████▎  | 206/282 [00:42<00:11,  6.56it/s]11/28/2021 01:01:29 - INFO - __main__ -   Batch number = 207
Evaluating:  73%|███████▎  | 207/282 [00:42<00:11,  6.66it/s]11/28/2021 01:01:29 - INFO - __main__ -   Batch number = 208
PyTorch version 1.10.0+cu102 available.
Evaluating:  74%|███████▍  | 208/282 [00:43<00:11,  6.57it/s]11/28/2021 01:01:29 - INFO - __main__ -   Batch number = 209
Evaluating:  74%|███████▍  | 209/282 [00:43<00:10,  6.66it/s]11/28/2021 01:01:29 - INFO - __main__ -   Batch number = 210
Evaluating:  74%|███████▍  | 210/282 [00:43<00:10,  6.57it/s]11/28/2021 01:01:29 - INFO - __main__ -   Batch number = 211
11/28/2021 01:01:29 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='hi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:01:29 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:01:29 - INFO - __main__ -   Seed = 3
11/28/2021 01:01:29 - INFO - root -   save model
11/28/2021 01:01:29 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='hi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:01:29 - INFO - __main__ -   Loading pretrained model and tokenizer
Evaluating:  75%|███████▍  | 211/282 [00:43<00:10,  6.67it/s]11/28/2021 01:01:30 - INFO - __main__ -   Batch number = 212
Evaluating:  75%|███████▌  | 212/282 [00:43<00:10,  6.56it/s]11/28/2021 01:01:30 - INFO - __main__ -   Batch number = 213
Evaluating:  76%|███████▌  | 213/282 [00:43<00:10,  6.65it/s]11/28/2021 01:01:30 - INFO - __main__ -   Batch number = 214
Evaluating:  76%|███████▌  | 214/282 [00:43<00:10,  6.56it/s]11/28/2021 01:01:30 - INFO - __main__ -   Batch number = 215
Evaluating:  76%|███████▌  | 215/282 [00:44<00:10,  6.65it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
11/28/2021 01:01:30 - INFO - __main__ -   Batch number = 216
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  77%|███████▋  | 216/282 [00:44<00:12,  5.16it/s]11/28/2021 01:01:30 - INFO - __main__ -   Batch number = 217
Evaluating:  77%|███████▋  | 217/282 [00:44<00:11,  5.57it/s]11/28/2021 01:01:31 - INFO - __main__ -   Batch number = 218
Evaluating:  77%|███████▋  | 218/282 [00:44<00:11,  5.78it/s]11/28/2021 01:01:31 - INFO - __main__ -   Batch number = 219
Evaluating:  78%|███████▊  | 219/282 [00:44<00:10,  6.06it/s]11/28/2021 01:01:31 - INFO - __main__ -   Batch number = 220
Evaluating:  78%|███████▊  | 220/282 [00:44<00:10,  6.13it/s]11/28/2021 01:01:31 - INFO - __main__ -   Batch number = 221
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  78%|███████▊  | 221/282 [00:45<00:09,  6.25it/s]11/28/2021 01:01:31 - INFO - __main__ -   Batch number = 222
Evaluating:  79%|███████▊  | 222/282 [00:45<00:09,  6.16it/s]11/28/2021 01:01:31 - INFO - __main__ -   Batch number = 223
Evaluating:  79%|███████▉  | 223/282 [00:45<00:09,  6.33it/s]11/28/2021 01:01:32 - INFO - __main__ -   Batch number = 224
Evaluating:  79%|███████▉  | 224/282 [00:45<00:09,  6.33it/s]11/28/2021 01:01:32 - INFO - __main__ -   Batch number = 225
Evaluating:  80%|███████▉  | 225/282 [00:45<00:08,  6.47it/s]11/28/2021 01:01:32 - INFO - __main__ -   Batch number = 226
loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
Evaluating:  80%|████████  | 226/282 [00:45<00:08,  6.40it/s]11/28/2021 01:01:32 - INFO - __main__ -   Batch number = 227
Evaluating:  80%|████████  | 227/282 [00:46<00:08,  6.52it/s]11/28/2021 01:01:32 - INFO - __main__ -   Batch number = 228
11/28/2021 01:01:32 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
Evaluating:  81%|████████  | 228/282 [00:46<00:08,  6.45it/s]11/28/2021 01:01:32 - INFO - __main__ -   Batch number = 229
Evaluating:  81%|████████  | 229/282 [00:46<00:08,  6.56it/s]11/28/2021 01:01:32 - INFO - __main__ -   Batch number = 230
Evaluating:  82%|████████▏ | 230/282 [00:46<00:08,  6.48it/s]11/28/2021 01:01:33 - INFO - __main__ -   Batch number = 231
Evaluating:  82%|████████▏ | 231/282 [00:46<00:07,  6.58it/s]11/28/2021 01:01:33 - INFO - __main__ -   Batch number = 232
Evaluating:  82%|████████▏ | 232/282 [00:46<00:07,  6.49it/s]11/28/2021 01:01:33 - INFO - __main__ -   Batch number = 233
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  83%|████████▎ | 233/282 [00:46<00:07,  6.57it/s]11/28/2021 01:01:33 - INFO - __main__ -   Batch number = 234
Evaluating:  83%|████████▎ | 234/282 [00:47<00:07,  6.46it/s]11/28/2021 01:01:33 - INFO - __main__ -   Batch number = 235
Evaluating:  83%|████████▎ | 235/282 [00:47<00:07,  6.56it/s]11/28/2021 01:01:33 - INFO - __main__ -   Batch number = 236
Evaluating:  84%|████████▎ | 236/282 [00:47<00:07,  6.47it/s]11/28/2021 01:01:34 - INFO - __main__ -   Batch number = 237
Evaluating:  84%|████████▍ | 237/282 [00:47<00:06,  6.56it/s]11/28/2021 01:01:34 - INFO - __main__ -   Batch number = 238
Evaluating:  84%|████████▍ | 238/282 [00:47<00:06,  6.44it/s]11/28/2021 01:01:34 - INFO - __main__ -   Batch number = 239
Evaluating:  85%|████████▍ | 239/282 [00:47<00:06,  6.53it/s]11/28/2021 01:01:34 - INFO - __main__ -   Batch number = 240
Evaluating:  85%|████████▌ | 240/282 [00:48<00:06,  6.43it/s]11/28/2021 01:01:34 - INFO - __main__ -   Batch number = 241
Evaluating:  85%|████████▌ | 241/282 [00:48<00:06,  6.53it/s]11/28/2021 01:01:34 - INFO - __main__ -   Batch number = 242
Evaluating:  86%|████████▌ | 242/282 [00:48<00:06,  6.43it/s]11/28/2021 01:01:34 - INFO - __main__ -   Batch number = 243
Evaluating:  86%|████████▌ | 243/282 [00:48<00:06,  6.48it/s]11/28/2021 01:01:35 - INFO - __main__ -   Batch number = 244
Evaluating:  87%|████████▋ | 244/282 [00:48<00:05,  6.41it/s]11/28/2021 01:01:35 - INFO - __main__ -   Batch number = 245
Evaluating:  87%|████████▋ | 245/282 [00:48<00:05,  6.51it/s]11/28/2021 01:01:35 - INFO - __main__ -   Batch number = 246
Evaluating:  87%|████████▋ | 246/282 [00:49<00:05,  6.42it/s]11/28/2021 01:01:35 - INFO - __main__ -   Batch number = 247
Evaluating:  88%|████████▊ | 247/282 [00:49<00:06,  5.30it/s]11/28/2021 01:01:35 - INFO - __main__ -   Batch number = 248
Evaluating:  88%|████████▊ | 248/282 [00:49<00:06,  5.43it/s]11/28/2021 01:01:36 - INFO - __main__ -   Batch number = 249
Evaluating:  88%|████████▊ | 249/282 [00:49<00:05,  5.60it/s]11/28/2021 01:01:36 - INFO - __main__ -   Batch number = 250
Evaluating:  89%|████████▊ | 250/282 [00:49<00:05,  5.77it/s]11/28/2021 01:01:36 - INFO - __main__ -   Batch number = 251
Evaluating:  89%|████████▉ | 251/282 [00:49<00:05,  6.04it/s]11/28/2021 01:01:36 - INFO - __main__ -   Batch number = 252
Evaluating:  89%|████████▉ | 252/282 [00:50<00:04,  6.07it/s]11/28/2021 01:01:36 - INFO - __main__ -   Batch number = 253
Evaluating:  90%|████████▉ | 253/282 [00:50<00:04,  6.26it/s]11/28/2021 01:01:36 - INFO - __main__ -   Batch number = 254
Evaluating:  90%|█████████ | 254/282 [00:50<00:04,  6.23it/s]11/28/2021 01:01:36 - INFO - __main__ -   Batch number = 255
Evaluating:  90%|█████████ | 255/282 [00:50<00:04,  6.38it/s]11/28/2021 01:01:37 - INFO - __main__ -   Batch number = 256
Evaluating:  91%|█████████ | 256/282 [00:50<00:04,  6.31it/s]11/28/2021 01:01:37 - INFO - __main__ -   Batch number = 257
Evaluating:  91%|█████████ | 257/282 [00:50<00:03,  6.44it/s]11/28/2021 01:01:37 - INFO - __main__ -   Batch number = 258
Evaluating:  91%|█████████▏| 258/282 [00:51<00:03,  6.35it/s]11/28/2021 01:01:37 - INFO - __main__ -   Batch number = 259
Evaluating:  92%|█████████▏| 259/282 [00:51<00:03,  6.31it/s]11/28/2021 01:01:37 - INFO - __main__ -   Batch number = 260
Evaluating:  92%|█████████▏| 260/282 [00:51<00:03,  6.26it/s]11/28/2021 01:01:37 - INFO - __main__ -   Batch number = 261
Evaluating:  93%|█████████▎| 261/282 [00:51<00:03,  6.27it/s]11/28/2021 01:01:38 - INFO - __main__ -   Batch number = 262
Evaluating:  93%|█████████▎| 262/282 [00:51<00:03,  6.17it/s]11/28/2021 01:01:38 - INFO - __main__ -   Batch number = 263
Evaluating:  93%|█████████▎| 263/282 [00:51<00:03,  6.33it/s]11/28/2021 01:01:38 - INFO - __main__ -   Batch number = 264
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:01:38 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:01:38 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:01:38 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
11/28/2021 01:01:38 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:01:38 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:01:38 - INFO - root -   loading lang adpater am/wiki@ukp
11/28/2021 01:01:38 - INFO - __main__ -   Adapter Languages : ['am'], Length : 1
11/28/2021 01:01:38 - INFO - __main__ -   Adapter Names ['am/wiki@ukp'], Length : 1
11/28/2021 01:01:38 - INFO - __main__ -   Language = am
11/28/2021 01:01:38 - INFO - __main__ -   Adapter Name = am/wiki@ukp
Found matching adapter at: adapters/ukp/bert-base-multilingual-cased_am_wiki_pfeiffer.json
Evaluating:  94%|█████████▎| 264/282 [00:51<00:02,  6.26it/s]11/28/2021 01:01:38 - INFO - __main__ -   Batch number = 265
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/am/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_am_wiki_pfeiffer.zip.
Evaluating:  94%|█████████▍| 265/282 [00:52<00:02,  6.38it/s]11/28/2021 01:01:38 - INFO - __main__ -   Batch number = 266
Evaluating:  94%|█████████▍| 266/282 [00:52<00:02,  6.30it/s]11/28/2021 01:01:38 - INFO - __main__ -   Batch number = 267
Evaluating:  95%|█████████▍| 267/282 [00:52<00:02,  6.43it/s]11/28/2021 01:01:39 - INFO - __main__ -   Batch number = 268
Evaluating:  95%|█████████▌| 268/282 [00:52<00:02,  6.34it/s]11/28/2021 01:01:39 - INFO - __main__ -   Batch number = 269
Evaluating:  95%|█████████▌| 269/282 [00:52<00:02,  6.44it/s]11/28/2021 01:01:39 - INFO - __main__ -   Batch number = 270
Evaluating:  96%|█████████▌| 270/282 [00:52<00:01,  6.29it/s]11/28/2021 01:01:39 - INFO - __main__ -   Batch number = 271
Evaluating:  96%|█████████▌| 271/282 [00:53<00:01,  6.40it/s]11/28/2021 01:01:39 - INFO - __main__ -   Batch number = 272
Evaluating:  96%|█████████▋| 272/282 [00:53<00:01,  6.30it/s]11/28/2021 01:01:39 - INFO - __main__ -   Batch number = 273
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/adapter_config.json
Adding adapter 'am' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/head_config.json
Evaluating:  97%|█████████▋| 273/282 [00:53<00:01,  6.34it/s]11/28/2021 01:01:39 - INFO - __main__ -   Batch number = 274
Evaluating:  97%|█████████▋| 274/282 [00:53<00:01,  6.13it/s]11/28/2021 01:01:40 - INFO - __main__ -   Batch number = 275
Evaluating:  98%|█████████▊| 275/282 [00:53<00:01,  6.14it/s]11/28/2021 01:01:40 - INFO - __main__ -   Batch number = 276
Evaluating:  98%|█████████▊| 276/282 [00:53<00:00,  6.04it/s]11/28/2021 01:01:40 - INFO - __main__ -   Batch number = 277
Evaluating:  98%|█████████▊| 277/282 [00:54<00:00,  6.16it/s]11/28/2021 01:01:40 - INFO - __main__ -   Batch number = 278
Evaluating:  99%|█████████▊| 278/282 [00:54<00:00,  4.35it/s]11/28/2021 01:01:41 - INFO - __main__ -   Batch number = 279
Evaluating:  99%|█████████▉| 279/282 [00:54<00:00,  4.82it/s]11/28/2021 01:01:41 - INFO - __main__ -   Batch number = 280
Evaluating:  99%|█████████▉| 280/282 [00:54<00:00,  5.08it/s]11/28/2021 01:01:41 - INFO - __main__ -   Batch number = 281
Evaluating: 100%|█████████▉| 281/282 [00:54<00:00,  5.44it/s]11/28/2021 01:01:41 - INFO - __main__ -   Batch number = 282
Evaluating: 100%|██████████| 282/282 [00:54<00:00,  5.13it/s]11/28/2021 01:01:44 - INFO - __main__ -   Language adapter for hi not found, using am instead
11/28/2021 01:01:44 - INFO - __main__ -   Set active language adapter to am
11/28/2021 01:01:44 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:01:44 - INFO - __main__ -   Adapter Languages = ['am']
11/28/2021 01:01:44 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_hi_bert-base-multilingual-cased_128
11/28/2021 01:01:45 - INFO - __main__ -   ***** Running evaluation  in hi *****
11/28/2021 01:01:45 - INFO - __main__ -     Num examples = 2685
11/28/2021 01:01:45 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/84 [00:00<?, ?it/s]11/28/2021 01:01:45 - INFO - __main__ -   Batch number = 1
Evaluating:   1%|          | 1/84 [00:00<00:11,  7.12it/s]11/28/2021 01:01:45 - INFO - __main__ -   Batch number = 2
Evaluating:   2%|▏         | 2/84 [00:00<00:11,  7.43it/s]11/28/2021 01:01:45 - INFO - __main__ -   Batch number = 3
Evaluating:   4%|▎         | 3/84 [00:00<00:10,  7.46it/s]11/28/2021 01:01:45 - INFO - __main__ -   Batch number = 4

/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:01:45 - INFO - __main__ -   ***** Evaluation result  in ru *****
11/28/2021 01:01:45 - INFO - __main__ -     f1 = 0.8681977068869231
11/28/2021 01:01:45 - INFO - __main__ -     loss = 0.49390519494917373
11/28/2021 01:01:45 - INFO - __main__ -     precision = 0.8725350258449855
11/28/2021 01:01:45 - INFO - __main__ -     recall = 0.8639032957383167
Evaluating:   5%|▍         | 4/84 [00:00<00:10,  7.43it/s]11/28/2021 01:01:45 - INFO - __main__ -   Batch number = 5
Evaluating:   6%|▌         | 5/84 [00:00<00:10,  7.39it/s]11/28/2021 01:01:45 - INFO - __main__ -   Batch number = 6
Evaluating:   7%|▋         | 6/84 [00:00<00:10,  7.36it/s]11/28/2021 01:01:45 - INFO - __main__ -   Batch number = 7
Evaluating:   8%|▊         | 7/84 [00:00<00:10,  7.32it/s]11/28/2021 01:01:45 - INFO - __main__ -   Batch number = 8
Evaluating:  10%|▉         | 8/84 [00:01<00:10,  7.28it/s]11/28/2021 01:01:46 - INFO - __main__ -   Batch number = 9
Evaluating:  11%|█         | 9/84 [00:01<00:10,  7.24it/s]11/28/2021 01:01:46 - INFO - __main__ -   Batch number = 10
Evaluating:  12%|█▏        | 10/84 [00:01<00:10,  7.19it/s]11/28/2021 01:01:46 - INFO - __main__ -   Batch number = 11
59.63user 20.60system 1:18.26elapsed 102%CPU (0avgtext+0avgdata 3941868maxresident)k
0inputs+1744outputs (0major+1944241minor)pagefaults 0swaps
Evaluating:  13%|█▎        | 11/84 [00:01<00:10,  7.15it/s]11/28/2021 01:01:46 - INFO - __main__ -   Batch number = 12
Evaluating:  14%|█▍        | 12/84 [00:01<00:10,  7.14it/s]11/28/2021 01:01:46 - INFO - __main__ -   Batch number = 13
Evaluating:  15%|█▌        | 13/84 [00:01<00:09,  7.14it/s]11/28/2021 01:01:46 - INFO - __main__ -   Batch number = 14
Evaluating:  17%|█▋        | 14/84 [00:01<00:09,  7.12it/s]11/28/2021 01:01:46 - INFO - __main__ -   Batch number = 15
Evaluating:  18%|█▊        | 15/84 [00:02<00:09,  7.13it/s]11/28/2021 01:01:47 - INFO - __main__ -   Batch number = 16
Evaluating:  19%|█▉        | 16/84 [00:02<00:09,  7.13it/s]11/28/2021 01:01:47 - INFO - __main__ -   Batch number = 17
Evaluating:  20%|██        | 17/84 [00:02<00:09,  7.11it/s]11/28/2021 01:01:47 - INFO - __main__ -   Batch number = 18
Evaluating:  21%|██▏       | 18/84 [00:02<00:09,  7.12it/s]11/28/2021 01:01:47 - INFO - __main__ -   Batch number = 19
Evaluating:  23%|██▎       | 19/84 [00:02<00:09,  7.09it/s]11/28/2021 01:01:47 - INFO - __main__ -   Batch number = 20
Evaluating:  24%|██▍       | 20/84 [00:02<00:09,  7.04it/s]11/28/2021 01:01:47 - INFO - __main__ -   Batch number = 21
Evaluating:  25%|██▌       | 21/84 [00:02<00:08,  7.01it/s]11/28/2021 01:01:47 - INFO - __main__ -   Batch number = 22
Evaluating:  26%|██▌       | 22/84 [00:03<00:08,  6.93it/s]11/28/2021 01:01:48 - INFO - __main__ -   Batch number = 23
Evaluating:  27%|██▋       | 23/84 [00:03<00:08,  6.91it/s]11/28/2021 01:01:48 - INFO - __main__ -   Batch number = 24
Evaluating:  29%|██▊       | 24/84 [00:03<00:08,  6.94it/s]11/28/2021 01:01:48 - INFO - __main__ -   Batch number = 25
Evaluating:  30%|██▉       | 25/84 [00:03<00:08,  6.96it/s]11/28/2021 01:01:48 - INFO - __main__ -   Batch number = 26
Evaluating:  31%|███       | 26/84 [00:03<00:08,  6.97it/s]11/28/2021 01:01:48 - INFO - __main__ -   Batch number = 27
Evaluating:  32%|███▏      | 27/84 [00:03<00:08,  7.01it/s]11/28/2021 01:01:48 - INFO - __main__ -   Batch number = 28
Evaluating:  33%|███▎      | 28/84 [00:03<00:07,  7.02it/s]11/28/2021 01:01:48 - INFO - __main__ -   Batch number = 29
Evaluating:  35%|███▍      | 29/84 [00:04<00:07,  6.99it/s]11/28/2021 01:01:49 - INFO - __main__ -   Batch number = 30
Evaluating:  36%|███▌      | 30/84 [00:04<00:07,  6.96it/s]11/28/2021 01:01:49 - INFO - __main__ -   Batch number = 31
Evaluating:  37%|███▋      | 31/84 [00:04<00:07,  6.96it/s]11/28/2021 01:01:49 - INFO - __main__ -   Batch number = 32
Evaluating:  38%|███▊      | 32/84 [00:04<00:07,  6.96it/s]11/28/2021 01:01:49 - INFO - __main__ -   Batch number = 33
Evaluating:  39%|███▉      | 33/84 [00:04<00:07,  6.98it/s]11/28/2021 01:01:49 - INFO - __main__ -   Batch number = 34
Evaluating:  40%|████      | 34/84 [00:04<00:07,  7.01it/s]11/28/2021 01:01:49 - INFO - __main__ -   Batch number = 35
Evaluating:  42%|████▏     | 35/84 [00:04<00:06,  7.02it/s]11/28/2021 01:01:49 - INFO - __main__ -   Batch number = 36
Evaluating:  43%|████▎     | 36/84 [00:05<00:06,  7.04it/s]11/28/2021 01:01:50 - INFO - __main__ -   Batch number = 37
Evaluating:  44%|████▍     | 37/84 [00:05<00:06,  7.02it/s]11/28/2021 01:01:50 - INFO - __main__ -   Batch number = 38
Evaluating:  45%|████▌     | 38/84 [00:05<00:06,  6.98it/s]11/28/2021 01:01:50 - INFO - __main__ -   Batch number = 39
Evaluating:  46%|████▋     | 39/84 [00:05<00:06,  6.96it/s]11/28/2021 01:01:50 - INFO - __main__ -   Batch number = 40
Evaluating:  48%|████▊     | 40/84 [00:05<00:06,  6.93it/s]11/28/2021 01:01:50 - INFO - __main__ -   Batch number = 41
Evaluating:  49%|████▉     | 41/84 [00:05<00:06,  6.39it/s]11/28/2021 01:01:50 - INFO - __main__ -   Batch number = 42
Evaluating:  50%|█████     | 42/84 [00:05<00:06,  6.67it/s]11/28/2021 01:01:50 - INFO - __main__ -   Batch number = 43
Evaluating:  51%|█████     | 43/84 [00:06<00:05,  6.86it/s]11/28/2021 01:01:51 - INFO - __main__ -   Batch number = 44
Evaluating:  52%|█████▏    | 44/84 [00:06<00:05,  6.97it/s]11/28/2021 01:01:51 - INFO - __main__ -   Batch number = 45
Evaluating:  54%|█████▎    | 45/84 [00:06<00:05,  7.08it/s]11/28/2021 01:01:51 - INFO - __main__ -   Batch number = 46
Evaluating:  55%|█████▍    | 46/84 [00:06<00:05,  7.13it/s]11/28/2021 01:01:51 - INFO - __main__ -   Batch number = 47
Evaluating:  56%|█████▌    | 47/84 [00:06<00:05,  7.16it/s]11/28/2021 01:01:51 - INFO - __main__ -   Batch number = 48
Evaluating:  57%|█████▋    | 48/84 [00:06<00:04,  7.22it/s]11/28/2021 01:01:51 - INFO - __main__ -   Batch number = 49
Evaluating:  58%|█████▊    | 49/84 [00:06<00:04,  7.21it/s]11/28/2021 01:01:51 - INFO - __main__ -   Batch number = 50
Evaluating:  60%|█████▉    | 50/84 [00:07<00:04,  7.19it/s]11/28/2021 01:01:52 - INFO - __main__ -   Batch number = 51
Evaluating:  61%|██████    | 51/84 [00:07<00:04,  7.19it/s]11/28/2021 01:01:52 - INFO - __main__ -   Batch number = 52
Evaluating:  62%|██████▏   | 52/84 [00:07<00:04,  7.15it/s]11/28/2021 01:01:52 - INFO - __main__ -   Batch number = 53
Evaluating:  63%|██████▎   | 53/84 [00:07<00:04,  7.10it/s]11/28/2021 01:01:52 - INFO - __main__ -   Batch number = 54
Evaluating:  64%|██████▍   | 54/84 [00:07<00:04,  7.10it/s]11/28/2021 01:01:52 - INFO - __main__ -   Batch number = 55
Evaluating:  65%|██████▌   | 55/84 [00:07<00:04,  7.06it/s]11/28/2021 01:01:52 - INFO - __main__ -   Batch number = 56
Evaluating:  67%|██████▋   | 56/84 [00:07<00:03,  7.04it/s]11/28/2021 01:01:52 - INFO - __main__ -   Batch number = 57
Evaluating:  68%|██████▊   | 57/84 [00:08<00:03,  7.02it/s]11/28/2021 01:01:53 - INFO - __main__ -   Batch number = 58
Evaluating:  69%|██████▉   | 58/84 [00:08<00:03,  7.00it/s]11/28/2021 01:01:53 - INFO - __main__ -   Batch number = 59
Evaluating:  70%|███████   | 59/84 [00:08<00:03,  6.97it/s]11/28/2021 01:01:53 - INFO - __main__ -   Batch number = 60
Evaluating:  71%|███████▏  | 60/84 [00:08<00:03,  6.96it/s]11/28/2021 01:01:53 - INFO - __main__ -   Batch number = 61
Evaluating:  73%|███████▎  | 61/84 [00:08<00:03,  6.95it/s]11/28/2021 01:01:53 - INFO - __main__ -   Batch number = 62
Evaluating:  74%|███████▍  | 62/84 [00:08<00:03,  6.74it/s]11/28/2021 01:01:53 - INFO - __main__ -   Batch number = 63
Evaluating:  75%|███████▌  | 63/84 [00:08<00:03,  6.74it/s]11/28/2021 01:01:53 - INFO - __main__ -   Batch number = 64
Evaluating:  76%|███████▌  | 64/84 [00:09<00:02,  6.88it/s]11/28/2021 01:01:54 - INFO - __main__ -   Batch number = 65
Evaluating:  77%|███████▋  | 65/84 [00:09<00:02,  7.02it/s]11/28/2021 01:01:54 - INFO - __main__ -   Batch number = 66
Evaluating:  79%|███████▊  | 66/84 [00:09<00:02,  7.12it/s]11/28/2021 01:01:54 - INFO - __main__ -   Batch number = 67
Evaluating:  80%|███████▉  | 67/84 [00:09<00:02,  7.18it/s]11/28/2021 01:01:54 - INFO - __main__ -   Batch number = 68
Evaluating:  81%|████████  | 68/84 [00:09<00:02,  7.23it/s]11/28/2021 01:01:54 - INFO - __main__ -   Batch number = 69
Evaluating:  82%|████████▏ | 69/84 [00:09<00:02,  7.26it/s]11/28/2021 01:01:54 - INFO - __main__ -   Batch number = 70
Evaluating:  83%|████████▎ | 70/84 [00:09<00:01,  7.28it/s]11/28/2021 01:01:54 - INFO - __main__ -   Batch number = 71
Evaluating:  85%|████████▍ | 71/84 [00:10<00:01,  7.31it/s]11/28/2021 01:01:55 - INFO - __main__ -   Batch number = 72
Evaluating:  86%|████████▌ | 72/84 [00:10<00:01,  7.32it/s]11/28/2021 01:01:55 - INFO - __main__ -   Batch number = 73
Evaluating:  87%|████████▋ | 73/84 [00:10<00:01,  7.32it/s]11/28/2021 01:01:55 - INFO - __main__ -   Batch number = 74
Evaluating:  88%|████████▊ | 74/84 [00:10<00:01,  7.30it/s]11/28/2021 01:01:55 - INFO - __main__ -   Batch number = 75
Evaluating:  89%|████████▉ | 75/84 [00:10<00:01,  7.31it/s]11/28/2021 01:01:55 - INFO - __main__ -   Batch number = 76
Evaluating:  90%|█████████ | 76/84 [00:10<00:01,  7.32it/s]11/28/2021 01:01:55 - INFO - __main__ -   Batch number = 77
Evaluating:  92%|█████████▏| 77/84 [00:10<00:00,  7.03it/s]11/28/2021 01:01:55 - INFO - __main__ -   Batch number = 78
Evaluating:  93%|█████████▎| 78/84 [00:11<00:00,  7.08it/s]11/28/2021 01:01:56 - INFO - __main__ -   Batch number = 79
Evaluating:  94%|█████████▍| 79/84 [00:11<00:00,  7.12it/s]11/28/2021 01:01:56 - INFO - __main__ -   Batch number = 80
Evaluating:  95%|█████████▌| 80/84 [00:11<00:00,  7.16it/s]11/28/2021 01:01:56 - INFO - __main__ -   Batch number = 81
Evaluating:  96%|█████████▋| 81/84 [00:11<00:00,  7.17it/s]11/28/2021 01:01:56 - INFO - __main__ -   Batch number = 82
Evaluating:  98%|█████████▊| 82/84 [00:11<00:00,  7.16it/s]11/28/2021 01:01:56 - INFO - __main__ -   Batch number = 83
Evaluating:  99%|█████████▉| 83/84 [00:11<00:00,  7.17it/s]11/28/2021 01:01:56 - INFO - __main__ -   Batch number = 84
Evaluating: 100%|██████████| 84/84 [00:11<00:00,  7.23it/s]Evaluating: 100%|██████████| 84/84 [00:11<00:00,  7.09it/s]
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:01:58 - INFO - __main__ -   ***** Evaluation result  in hi *****
11/28/2021 01:01:58 - INFO - __main__ -     f1 = 0.6357986803744053
11/28/2021 01:01:58 - INFO - __main__ -     loss = 1.2881216761611758
11/28/2021 01:01:58 - INFO - __main__ -     precision = 0.6367268536304265
11/28/2021 01:01:58 - INFO - __main__ -     recall = 0.6348732092239332
23.92user 7.93system 0:30.85elapsed 103%CPU (0avgtext+0avgdata 3935392maxresident)k
0inputs+672outputs (0major+1430344minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:03:01 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='cdo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:03:01 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:03:01 - INFO - __main__ -   Seed = 1
11/28/2021 01:03:01 - INFO - root -   save model
11/28/2021 01:03:01 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='cdo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:03:01 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:03:04 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:03:09 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:03:09 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:03:09 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
11/28/2021 01:03:09 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:03:09 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:03:09 - INFO - root -   loading lang adpater am/wiki@ukp
11/28/2021 01:03:09 - INFO - __main__ -   Adapter Languages : ['am'], Length : 1
11/28/2021 01:03:09 - INFO - __main__ -   Adapter Names ['am/wiki@ukp'], Length : 1
11/28/2021 01:03:09 - INFO - __main__ -   Language = am
11/28/2021 01:03:09 - INFO - __main__ -   Adapter Name = am/wiki@ukp
Found matching adapter at: adapters/ukp/bert-base-multilingual-cased_am_wiki_pfeiffer.json
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/am/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_am_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/adapter_config.json
Adding adapter 'am' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/head_config.json
11/28/2021 01:03:17 - INFO - __main__ -   Language cdo, split test does not exist
14.06user 5.98system 0:18.24elapsed 109%CPU (0avgtext+0avgdata 3939928maxresident)k
0inputs+56outputs (0major+1498152minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:03:19 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='cdo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:03:19 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:03:19 - INFO - __main__ -   Seed = 2
11/28/2021 01:03:19 - INFO - root -   save model
11/28/2021 01:03:19 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='cdo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:03:19 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:03:22 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:03:28 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:03:28 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:03:28 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
11/28/2021 01:03:28 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:03:28 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:03:28 - INFO - root -   loading lang adpater am/wiki@ukp
11/28/2021 01:03:28 - INFO - __main__ -   Adapter Languages : ['am'], Length : 1
11/28/2021 01:03:28 - INFO - __main__ -   Adapter Names ['am/wiki@ukp'], Length : 1
11/28/2021 01:03:28 - INFO - __main__ -   Language = am
11/28/2021 01:03:28 - INFO - __main__ -   Adapter Name = am/wiki@ukp
Found matching adapter at: adapters/ukp/bert-base-multilingual-cased_am_wiki_pfeiffer.json
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/am/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_am_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/adapter_config.json
Adding adapter 'am' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/head_config.json
11/28/2021 01:03:36 - INFO - __main__ -   Language cdo, split test does not exist
14.76user 5.60system 0:19.63elapsed 103%CPU (0avgtext+0avgdata 3935576maxresident)k
0inputs+56outputs (0major+1471375minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:03:39 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='cdo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:03:39 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:03:39 - INFO - __main__ -   Seed = 3
11/28/2021 01:03:39 - INFO - root -   save model
11/28/2021 01:03:39 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='cdo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:03:39 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:03:41 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:03:47 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:03:47 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:03:47 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
11/28/2021 01:03:47 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:03:47 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:03:47 - INFO - root -   loading lang adpater am/wiki@ukp
11/28/2021 01:03:47 - INFO - __main__ -   Adapter Languages : ['am'], Length : 1
11/28/2021 01:03:47 - INFO - __main__ -   Adapter Names ['am/wiki@ukp'], Length : 1
11/28/2021 01:03:47 - INFO - __main__ -   Language = am
11/28/2021 01:03:47 - INFO - __main__ -   Adapter Name = am/wiki@ukp
Found matching adapter at: adapters/ukp/bert-base-multilingual-cased_am_wiki_pfeiffer.json
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/am/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_am_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/adapter_config.json
Adding adapter 'am' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/head_config.json
11/28/2021 01:03:57 - INFO - __main__ -   Language cdo, split test does not exist
15.55user 6.02system 0:21.19elapsed 101%CPU (0avgtext+0avgdata 3934712maxresident)k
0inputs+32outputs (0major+1457787minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:04:07 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='gn', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:04:07 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:04:07 - INFO - __main__ -   Seed = 1
11/28/2021 01:04:07 - INFO - root -   save model
11/28/2021 01:04:07 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='gn', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:04:07 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:04:10 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:04:16 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:04:16 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:04:16 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
11/28/2021 01:04:16 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:04:16 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:04:16 - INFO - root -   loading lang adpater am/wiki@ukp
11/28/2021 01:04:16 - INFO - __main__ -   Adapter Languages : ['am'], Length : 1
11/28/2021 01:04:16 - INFO - __main__ -   Adapter Names ['am/wiki@ukp'], Length : 1
11/28/2021 01:04:16 - INFO - __main__ -   Language = am
11/28/2021 01:04:16 - INFO - __main__ -   Adapter Name = am/wiki@ukp
Found matching adapter at: adapters/ukp/bert-base-multilingual-cased_am_wiki_pfeiffer.json
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/am/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_am_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/adapter_config.json
Adding adapter 'am' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/head_config.json
11/28/2021 01:04:23 - INFO - __main__ -   Language gn, split test does not exist
14.30user 5.35system 0:18.82elapsed 104%CPU (0avgtext+0avgdata 3940864maxresident)k
0inputs+56outputs (0major+1336950minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:04:26 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='gn', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:04:26 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:04:26 - INFO - __main__ -   Seed = 2
11/28/2021 01:04:26 - INFO - root -   save model
11/28/2021 01:04:26 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='gn', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:04:26 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:04:29 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:04:34 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:04:34 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:04:34 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
11/28/2021 01:04:34 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:04:34 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:04:34 - INFO - root -   loading lang adpater am/wiki@ukp
11/28/2021 01:04:34 - INFO - __main__ -   Adapter Languages : ['am'], Length : 1
11/28/2021 01:04:34 - INFO - __main__ -   Adapter Names ['am/wiki@ukp'], Length : 1
11/28/2021 01:04:34 - INFO - __main__ -   Language = am
11/28/2021 01:04:34 - INFO - __main__ -   Adapter Name = am/wiki@ukp
Found matching adapter at: adapters/ukp/bert-base-multilingual-cased_am_wiki_pfeiffer.json
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/am/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_am_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/adapter_config.json
Adding adapter 'am' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/head_config.json
11/28/2021 01:04:41 - INFO - __main__ -   Language gn, split test does not exist
13.60user 5.54system 0:17.84elapsed 107%CPU (0avgtext+0avgdata 3939740maxresident)k
0inputs+56outputs (0major+1334359minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:04:44 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='gn', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:04:44 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:04:44 - INFO - __main__ -   Seed = 3
11/28/2021 01:04:44 - INFO - root -   save model
11/28/2021 01:04:44 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='gn', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:04:44 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:04:46 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:04:52 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:04:52 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:04:52 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
11/28/2021 01:04:52 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:04:52 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:04:52 - INFO - root -   loading lang adpater am/wiki@ukp
11/28/2021 01:04:52 - INFO - __main__ -   Adapter Languages : ['am'], Length : 1
11/28/2021 01:04:52 - INFO - __main__ -   Adapter Names ['am/wiki@ukp'], Length : 1
11/28/2021 01:04:52 - INFO - __main__ -   Language = am
11/28/2021 01:04:52 - INFO - __main__ -   Adapter Name = am/wiki@ukp
Found matching adapter at: adapters/ukp/bert-base-multilingual-cased_am_wiki_pfeiffer.json
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/am/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_am_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/adapter_config.json
Adding adapter 'am' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/head_config.json
11/28/2021 01:05:00 - INFO - __main__ -   Language gn, split test does not exist
14.01user 5.58system 0:18.66elapsed 105%CPU (0avgtext+0avgdata 3949020maxresident)k
0inputs+48outputs (0major+1343693minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:06:09 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:06:09 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:06:09 - INFO - __main__ -   Seed = 1
11/28/2021 01:06:09 - INFO - root -   save model
11/28/2021 01:06:09 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:06:09 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:06:12 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:06:18 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:06:18 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:06:18 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
11/28/2021 01:06:18 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:06:18 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:06:18 - INFO - root -   loading lang adpater am/wiki@ukp
11/28/2021 01:06:18 - INFO - __main__ -   Adapter Languages : ['am'], Length : 1
11/28/2021 01:06:18 - INFO - __main__ -   Adapter Names ['am/wiki@ukp'], Length : 1
11/28/2021 01:06:18 - INFO - __main__ -   Language = am
11/28/2021 01:06:18 - INFO - __main__ -   Adapter Name = am/wiki@ukp
Found matching adapter at: adapters/ukp/bert-base-multilingual-cased_am_wiki_pfeiffer.json
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/am/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_am_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/adapter_config.json
Adding adapter 'am' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/head_config.json
11/28/2021 01:06:27 - INFO - __main__ -   Language mi, split test does not exist
15.52user 6.29system 0:20.56elapsed 106%CPU (0avgtext+0avgdata 3937284maxresident)k
0inputs+48outputs (0major+1331496minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:06:30 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:06:30 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:06:30 - INFO - __main__ -   Seed = 2
11/28/2021 01:06:30 - INFO - root -   save model
11/28/2021 01:06:30 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:06:30 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:06:33 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:06:39 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:06:39 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:06:39 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
11/28/2021 01:06:39 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:06:39 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:06:39 - INFO - root -   loading lang adpater am/wiki@ukp
11/28/2021 01:06:39 - INFO - __main__ -   Adapter Languages : ['am'], Length : 1
11/28/2021 01:06:39 - INFO - __main__ -   Adapter Names ['am/wiki@ukp'], Length : 1
11/28/2021 01:06:39 - INFO - __main__ -   Language = am
11/28/2021 01:06:39 - INFO - __main__ -   Adapter Name = am/wiki@ukp
Found matching adapter at: adapters/ukp/bert-base-multilingual-cased_am_wiki_pfeiffer.json
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/am/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_am_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/adapter_config.json
Adding adapter 'am' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/head_config.json
11/28/2021 01:06:49 - INFO - __main__ -   Language mi, split test does not exist
17.70user 7.86system 0:21.86elapsed 116%CPU (0avgtext+0avgdata 3936448maxresident)k
0inputs+56outputs (0major+1566218minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:06:52 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:06:52 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:06:52 - INFO - __main__ -   Seed = 3
11/28/2021 01:06:52 - INFO - root -   save model
11/28/2021 01:06:52 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:06:52 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:06:54 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:07:01 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:07:01 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:07:01 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
11/28/2021 01:07:01 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:07:01 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:07:01 - INFO - root -   loading lang adpater am/wiki@ukp
11/28/2021 01:07:01 - INFO - __main__ -   Adapter Languages : ['am'], Length : 1
11/28/2021 01:07:01 - INFO - __main__ -   Adapter Names ['am/wiki@ukp'], Length : 1
11/28/2021 01:07:01 - INFO - __main__ -   Language = am
11/28/2021 01:07:01 - INFO - __main__ -   Adapter Name = am/wiki@ukp
Found matching adapter at: adapters/ukp/bert-base-multilingual-cased_am_wiki_pfeiffer.json
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/am/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_am_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/adapter_config.json
Adding adapter 'am' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/head_config.json
11/28/2021 01:07:09 - INFO - __main__ -   Language mi, split test does not exist
13.57user 6.01system 0:19.45elapsed 100%CPU (0avgtext+0avgdata 3936900maxresident)k
0inputs+40outputs (0major+1606754minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:08:13 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='es', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:08:13 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:08:13 - INFO - __main__ -   Seed = 1
11/28/2021 01:08:13 - INFO - root -   save model
11/28/2021 01:08:13 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='es', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:08:13 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:08:16 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:08:22 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:08:22 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:08:22 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
11/28/2021 01:08:22 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:08:22 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:08:22 - INFO - root -   loading lang adpater am/wiki@ukp
11/28/2021 01:08:22 - INFO - __main__ -   Adapter Languages : ['am'], Length : 1
11/28/2021 01:08:22 - INFO - __main__ -   Adapter Names ['am/wiki@ukp'], Length : 1
11/28/2021 01:08:22 - INFO - __main__ -   Language = am
11/28/2021 01:08:22 - INFO - __main__ -   Adapter Name = am/wiki@ukp
Found matching adapter at: adapters/ukp/bert-base-multilingual-cased_am_wiki_pfeiffer.json
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/am/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_am_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/adapter_config.json
Adding adapter 'am' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/head_config.json
11/28/2021 01:08:30 - INFO - __main__ -   Language adapter for es not found, using am instead
11/28/2021 01:08:30 - INFO - __main__ -   Set active language adapter to am
11/28/2021 01:08:30 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:08:30 - INFO - __main__ -   Adapter Languages = ['am']
11/28/2021 01:08:30 - INFO - __main__ -   all languages = es
11/28/2021 01:08:30 - INFO - __main__ -   Creating features from dataset file at /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/es/test.bert-base-multilingual-cased in language es
11/28/2021 01:08:30 - INFO - utils_tag -   lang_id=0, lang=es, lang2id=None
11/28/2021 01:08:31 - INFO - utils_tag -   Writing example 0 of 3154
11/28/2021 01:08:31 - INFO - utils_tag -   *** Example ***
11/28/2021 01:08:31 - INFO - utils_tag -   guid: es-1
11/28/2021 01:08:31 - INFO - utils_tag -   tokens: [CLS] Parti ##dari ##o de la " per ##estro ##ika " de Mi ##jai ##l Go ##rba ##chov en la Unión Soviética , en 1989 entró en conflicto con Y ##ív ##kov , líder durante 35 años del Partido Comunista y del Estado b ##úl ##garo , y le acu ##só en una carta abierta de utilizar métodos poco demo ##crático ##s de gobierno . [SEP]
11/28/2021 01:08:31 - INFO - utils_tag -   input_ids: 101 19644 91902 10133 10104 10109 107 10178 87410 13060 107 10104 19803 26685 10161 14439 61494 109230 10110 10109 19406 37078 117 10110 10524 49750 10110 56405 10173 162 29244 16433 117 21770 11047 10803 11278 10127 17084 41231 193 10127 14359 170 49227 49572 117 193 10141 81886 31355 10110 10153 24044 77780 10104 34569 44116 14445 30776 70385 10107 10104 16339 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:08:31 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:08:31 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:08:31 - INFO - utils_tag -   label_ids: -100 1 -100 -100 2 6 13 8 -100 -100 13 2 12 -100 -100 12 -100 -100 2 6 12 12 13 2 8 16 2 8 2 12 -100 -100 13 8 2 9 8 2 12 12 5 2 12 1 -100 -100 13 5 11 16 -100 2 6 8 1 2 16 8 3 1 -100 -100 2 8 13 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
11/28/2021 01:08:31 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
11/28/2021 01:08:31 - INFO - utils_tag -   *** Example ***
11/28/2021 01:08:31 - INFO - utils_tag -   guid: es-2
11/28/2021 01:08:31 - INFO - utils_tag -   tokens: [CLS] Día ##s después , el 10 de noviembre de 1989 , organi ##zó junto con otros miembros del Ga ##bine ##te la des ##titución de Y ##ív ##kov , lo que provocó la caída del régimen comunista en el país , y dos meses más tarde fue elegido presidente de la Bulgaria plu ##rip ##arti ##dista . [SEP]
11/28/2021 01:08:31 - INFO - utils_tag -   input_ids: 101 52819 10107 12884 117 10125 10150 10104 16269 10104 10524 117 91969 16993 13597 10173 12525 19525 10127 69699 36848 10216 10109 10139 104361 10104 162 29244 16433 117 10406 10121 76469 10109 56097 10127 55339 49228 10110 10125 12115 117 193 10398 17359 10543 14002 10553 35847 13103 10104 10109 21935 13651 68228 61115 57250 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:08:31 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:08:31 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:08:31 - INFO - utils_tag -   label_ids: -100 8 -100 3 13 6 9 2 8 2 9 13 16 -100 1 2 6 8 2 12 -100 -100 6 8 -100 2 12 -100 -100 13 11 11 16 6 8 2 8 1 2 6 8 13 5 9 8 3 3 4 16 8 2 6 12 1 -100 -100 -100 13 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
11/28/2021 01:08:31 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
11/28/2021 01:08:31 - INFO - utils_tag -   *** Example ***
11/28/2021 01:08:31 - INFO - utils_tag -   guid: es-3
11/28/2021 01:08:31 - INFO - utils_tag -   tokens: [CLS] Después del triunfo electoral de los socialista ##s ( ex comunistas ) en las primeras elecciones demo ##c ##rática ##s del 10 de junio de 1990 , la der ##ech ##ista Unión de Fuerzas Democrática ##s ( SD ##S ) acu ##só a M ##lad ##én ##ov de haber amenaza ##do con " sac ##ar los carros de combate " el 14 de diciembre de 1989 . [SEP]
11/28/2021 01:08:31 - INFO - utils_tag -   input_ids: 101 19849 10127 71531 29125 10104 10182 47981 10107 113 11419 105677 114 10110 10285 30221 27982 30776 10350 106387 10107 10127 10150 10104 15870 10104 10420 117 10109 10118 16200 11298 19406 10104 100373 69261 10107 113 27589 10731 114 81886 31355 169 150 19505 13632 11024 10104 19086 85170 10317 10173 107 109436 10354 10182 81844 10104 26628 107 10125 10247 10104 16185 10104 10524 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:08:31 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:08:31 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:08:31 - INFO - utils_tag -   label_ids: -100 3 2 8 1 2 6 8 -100 13 1 8 13 2 6 1 8 1 -100 -100 -100 2 9 2 8 2 9 13 6 1 -100 -100 12 2 12 12 -100 13 12 -100 13 16 -100 2 12 -100 -100 -100 2 4 16 -100 2 13 16 -100 6 8 2 8 13 6 9 2 8 2 9 13 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
11/28/2021 01:08:31 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
11/28/2021 01:08:31 - INFO - utils_tag -   *** Example ***
11/28/2021 01:08:31 - INFO - utils_tag -   guid: es-4
11/28/2021 01:08:31 - INFO - utils_tag -   tokens: [CLS] Des ##con ##tent ##o por la falta de apoyo del Partido Socialista B ##úl ##garo ( BS ##P , ex comunista ) , M ##lad ##én ##ov , que desde hacía tiempo su ##fr ##ía problemas card ##ía ##cos , di ##mit ##ió el 6 de julio de 1990 y , por decisión del Parlamento fue sus ##tituido en la Presiden ##cia por Ye ##liu Y ##él ##ev , entonces líder de la SD ##S . [SEP]
11/28/2021 01:08:31 - INFO - utils_tag -   input_ids: 101 13810 23486 51294 10133 10183 10109 23821 10104 30390 10127 17084 37768 139 49227 49572 113 43436 11127 117 11419 49228 114 117 150 19505 13632 11024 117 10121 11392 62621 14104 10198 71843 10836 20088 23050 10836 15023 117 10120 15772 12343 10125 127 10104 15761 10104 10420 193 117 10183 41183 10127 33350 10553 10846 109055 10110 10109 33382 11462 10183 20567 79737 162 24817 15705 117 16720 21770 10104 10109 27589 10731 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:08:31 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:08:31 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:08:31 - INFO - utils_tag -   label_ids: -100 1 -100 -100 -100 2 6 8 2 8 2 12 12 12 -100 -100 13 12 -100 13 1 1 13 13 12 -100 -100 -100 13 11 2 16 8 16 -100 -100 8 1 -100 -100 13 16 -100 -100 6 9 2 8 2 9 5 13 2 8 2 12 4 16 -100 2 6 12 -100 2 12 -100 12 -100 -100 13 3 8 2 6 12 -100 13 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
11/28/2021 01:08:31 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
11/28/2021 01:08:31 - INFO - utils_tag -   *** Example ***
11/28/2021 01:08:31 - INFO - utils_tag -   guid: es-5
11/28/2021 01:08:31 - INFO - utils_tag -   tokens: [CLS] El Ministerio chino de Asuntos Exteriores defend ##ió hoy el resultado de las elecciones presidencial ##es celebrada ##s en Perú y of ##reció su apoyo al nuevo gobierno del presidente Alberto Fuji ##mori . [SEP]
11/28/2021 01:08:31 - INFO - utils_tag -   input_ids: 101 10224 27700 73755 10104 110586 98227 60041 12343 25235 10125 20229 10104 10285 27982 66176 10171 88689 10107 10110 20874 193 10108 89529 10198 30390 10164 15249 16339 10127 13103 15796 53509 72315 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:08:31 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:08:31 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:08:31 - INFO - utils_tag -   label_ids: -100 6 12 1 2 12 12 16 -100 3 6 8 2 6 8 1 -100 1 -100 2 12 5 16 -100 6 8 2 1 8 2 8 12 12 -100 13 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
11/28/2021 01:08:31 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
11/28/2021 01:08:35 - INFO - __main__ -   Saving features into cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_es_bert-base-multilingual-cased_128, len(features)=3154
11/28/2021 01:08:36 - INFO - __main__ -   ***** Running evaluation  in es *****
11/28/2021 01:08:36 - INFO - __main__ -     Num examples = 3154
11/28/2021 01:08:36 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/99 [00:00<?, ?it/s]11/28/2021 01:08:36 - INFO - __main__ -   Batch number = 1
Evaluating:   1%|          | 1/99 [00:00<00:30,  3.26it/s]11/28/2021 01:08:36 - INFO - __main__ -   Batch number = 2
Evaluating:   2%|▏         | 2/99 [00:00<00:29,  3.27it/s]11/28/2021 01:08:36 - INFO - __main__ -   Batch number = 3
Evaluating:   3%|▎         | 3/99 [00:00<00:29,  3.28it/s]11/28/2021 01:08:37 - INFO - __main__ -   Batch number = 4
Evaluating:   4%|▍         | 4/99 [00:01<00:28,  3.29it/s]11/28/2021 01:08:37 - INFO - __main__ -   Batch number = 5
Evaluating:   5%|▌         | 5/99 [00:01<00:32,  2.90it/s]11/28/2021 01:08:37 - INFO - __main__ -   Batch number = 6
Evaluating:   6%|▌         | 6/99 [00:01<00:30,  3.08it/s]11/28/2021 01:08:38 - INFO - __main__ -   Batch number = 7
Evaluating:   7%|▋         | 7/99 [00:02<00:28,  3.18it/s]11/28/2021 01:08:38 - INFO - __main__ -   Batch number = 8
Evaluating:   8%|▊         | 8/99 [00:02<00:28,  3.24it/s]11/28/2021 01:08:38 - INFO - __main__ -   Batch number = 9
Evaluating:   9%|▉         | 9/99 [00:02<00:27,  3.26it/s]11/28/2021 01:08:39 - INFO - __main__ -   Batch number = 10
Evaluating:  10%|█         | 10/99 [00:03<00:27,  3.25it/s]11/28/2021 01:08:39 - INFO - __main__ -   Batch number = 11
Evaluating:  11%|█         | 11/99 [00:03<00:27,  3.25it/s]11/28/2021 01:08:39 - INFO - __main__ -   Batch number = 12
Evaluating:  12%|█▏        | 12/99 [00:03<00:26,  3.25it/s]11/28/2021 01:08:40 - INFO - __main__ -   Batch number = 13
Evaluating:  13%|█▎        | 13/99 [00:04<00:26,  3.26it/s]11/28/2021 01:08:40 - INFO - __main__ -   Batch number = 14
Evaluating:  14%|█▍        | 14/99 [00:04<00:26,  3.25it/s]11/28/2021 01:08:40 - INFO - __main__ -   Batch number = 15
Evaluating:  15%|█▌        | 15/99 [00:04<00:25,  3.24it/s]11/28/2021 01:08:41 - INFO - __main__ -   Batch number = 16
Evaluating:  16%|█▌        | 16/99 [00:04<00:25,  3.23it/s]11/28/2021 01:08:41 - INFO - __main__ -   Batch number = 17
Evaluating:  17%|█▋        | 17/99 [00:05<00:25,  3.24it/s]11/28/2021 01:08:41 - INFO - __main__ -   Batch number = 18
Evaluating:  18%|█▊        | 18/99 [00:05<00:25,  3.21it/s]11/28/2021 01:08:41 - INFO - __main__ -   Batch number = 19
Evaluating:  19%|█▉        | 19/99 [00:05<00:24,  3.24it/s]11/28/2021 01:08:42 - INFO - __main__ -   Batch number = 20
Evaluating:  20%|██        | 20/99 [00:06<00:24,  3.25it/s]11/28/2021 01:08:42 - INFO - __main__ -   Batch number = 21
Evaluating:  21%|██        | 21/99 [00:06<00:25,  3.10it/s]11/28/2021 01:08:42 - INFO - __main__ -   Batch number = 22
Evaluating:  22%|██▏       | 22/99 [00:06<00:23,  3.22it/s]11/28/2021 01:08:43 - INFO - __main__ -   Batch number = 23
Evaluating:  23%|██▎       | 23/99 [00:07<00:23,  3.29it/s]11/28/2021 01:08:43 - INFO - __main__ -   Batch number = 24
Evaluating:  24%|██▍       | 24/99 [00:07<00:22,  3.31it/s]11/28/2021 01:08:43 - INFO - __main__ -   Batch number = 25
Evaluating:  25%|██▌       | 25/99 [00:07<00:22,  3.30it/s]11/28/2021 01:08:44 - INFO - __main__ -   Batch number = 26
Evaluating:  26%|██▋       | 26/99 [00:08<00:22,  3.29it/s]11/28/2021 01:08:44 - INFO - __main__ -   Batch number = 27
Evaluating:  27%|██▋       | 27/99 [00:08<00:21,  3.29it/s]11/28/2021 01:08:44 - INFO - __main__ -   Batch number = 28
Evaluating:  28%|██▊       | 28/99 [00:08<00:21,  3.27it/s]11/28/2021 01:08:44 - INFO - __main__ -   Batch number = 29
Evaluating:  29%|██▉       | 29/99 [00:08<00:21,  3.27it/s]11/28/2021 01:08:45 - INFO - __main__ -   Batch number = 30
Evaluating:  30%|███       | 30/99 [00:09<00:21,  3.26it/s]11/28/2021 01:08:45 - INFO - __main__ -   Batch number = 31
Evaluating:  31%|███▏      | 31/99 [00:09<00:20,  3.26it/s]11/28/2021 01:08:45 - INFO - __main__ -   Batch number = 32
Evaluating:  32%|███▏      | 32/99 [00:09<00:20,  3.27it/s]11/28/2021 01:08:46 - INFO - __main__ -   Batch number = 33
Evaluating:  33%|███▎      | 33/99 [00:10<00:20,  3.25it/s]11/28/2021 01:08:46 - INFO - __main__ -   Batch number = 34
Evaluating:  34%|███▍      | 34/99 [00:10<00:20,  3.21it/s]11/28/2021 01:08:46 - INFO - __main__ -   Batch number = 35
Evaluating:  35%|███▌      | 35/99 [00:10<00:19,  3.23it/s]11/28/2021 01:08:47 - INFO - __main__ -   Batch number = 36
Evaluating:  36%|███▋      | 36/99 [00:11<00:19,  3.24it/s]11/28/2021 01:08:47 - INFO - __main__ -   Batch number = 37
Evaluating:  37%|███▋      | 37/99 [00:11<00:19,  3.25it/s]11/28/2021 01:08:47 - INFO - __main__ -   Batch number = 38
Evaluating:  38%|███▊      | 38/99 [00:11<00:18,  3.25it/s]11/28/2021 01:08:48 - INFO - __main__ -   Batch number = 39
Evaluating:  39%|███▉      | 39/99 [00:12<00:18,  3.27it/s]11/28/2021 01:08:48 - INFO - __main__ -   Batch number = 40
Evaluating:  40%|████      | 40/99 [00:12<00:18,  3.27it/s]11/28/2021 01:08:48 - INFO - __main__ -   Batch number = 41
Evaluating:  41%|████▏     | 41/99 [00:12<00:17,  3.26it/s]11/28/2021 01:08:48 - INFO - __main__ -   Batch number = 42
Evaluating:  42%|████▏     | 42/99 [00:12<00:17,  3.23it/s]11/28/2021 01:08:49 - INFO - __main__ -   Batch number = 43
Evaluating:  43%|████▎     | 43/99 [00:13<00:17,  3.26it/s]11/28/2021 01:08:49 - INFO - __main__ -   Batch number = 44
Evaluating:  44%|████▍     | 44/99 [00:13<00:16,  3.28it/s]11/28/2021 01:08:49 - INFO - __main__ -   Batch number = 45
Evaluating:  45%|████▌     | 45/99 [00:13<00:16,  3.32it/s]11/28/2021 01:08:50 - INFO - __main__ -   Batch number = 46
Evaluating:  46%|████▋     | 46/99 [00:14<00:16,  3.27it/s]11/28/2021 01:08:50 - INFO - __main__ -   Batch number = 47
Evaluating:  47%|████▋     | 47/99 [00:14<00:16,  3.23it/s]11/28/2021 01:08:50 - INFO - __main__ -   Batch number = 48
Evaluating:  48%|████▊     | 48/99 [00:14<00:15,  3.28it/s]11/28/2021 01:08:51 - INFO - __main__ -   Batch number = 49
Evaluating:  49%|████▉     | 49/99 [00:15<00:15,  3.29it/s]11/28/2021 01:08:51 - INFO - __main__ -   Batch number = 50
Evaluating:  51%|█████     | 50/99 [00:15<00:14,  3.27it/s]11/28/2021 01:08:51 - INFO - __main__ -   Batch number = 51
Evaluating:  52%|█████▏    | 51/99 [00:15<00:14,  3.29it/s]11/28/2021 01:08:52 - INFO - __main__ -   Batch number = 52
Evaluating:  53%|█████▎    | 52/99 [00:16<00:14,  3.32it/s]11/28/2021 01:08:52 - INFO - __main__ -   Batch number = 53
Evaluating:  54%|█████▎    | 53/99 [00:16<00:13,  3.47it/s]11/28/2021 01:08:52 - INFO - __main__ -   Batch number = 54
Evaluating:  55%|█████▍    | 54/99 [00:16<00:17,  2.60it/s]11/28/2021 01:08:53 - INFO - __main__ -   Batch number = 55
Evaluating:  56%|█████▌    | 55/99 [00:17<00:15,  2.82it/s]11/28/2021 01:08:53 - INFO - __main__ -   Batch number = 56
Evaluating:  57%|█████▋    | 56/99 [00:17<00:14,  3.01it/s]11/28/2021 01:08:53 - INFO - __main__ -   Batch number = 57
Evaluating:  58%|█████▊    | 57/99 [00:17<00:13,  3.16it/s]11/28/2021 01:08:54 - INFO - __main__ -   Batch number = 58
Evaluating:  59%|█████▊    | 58/99 [00:18<00:12,  3.23it/s]11/28/2021 01:08:54 - INFO - __main__ -   Batch number = 59
Evaluating:  60%|█████▉    | 59/99 [00:18<00:12,  3.26it/s]11/28/2021 01:08:54 - INFO - __main__ -   Batch number = 60
Evaluating:  61%|██████    | 60/99 [00:18<00:11,  3.26it/s]11/28/2021 01:08:54 - INFO - __main__ -   Batch number = 61
Evaluating:  62%|██████▏   | 61/99 [00:18<00:11,  3.26it/s]11/28/2021 01:08:55 - INFO - __main__ -   Batch number = 62
Evaluating:  63%|██████▎   | 62/99 [00:19<00:11,  3.25it/s]11/28/2021 01:08:55 - INFO - __main__ -   Batch number = 63
Evaluating:  64%|██████▎   | 63/99 [00:19<00:11,  3.25it/s]11/28/2021 01:08:55 - INFO - __main__ -   Batch number = 64
Evaluating:  65%|██████▍   | 64/99 [00:19<00:10,  3.22it/s]11/28/2021 01:08:56 - INFO - __main__ -   Batch number = 65
Evaluating:  66%|██████▌   | 65/99 [00:20<00:10,  3.27it/s]11/28/2021 01:08:56 - INFO - __main__ -   Batch number = 66
Evaluating:  67%|██████▋   | 66/99 [00:20<00:10,  3.30it/s]11/28/2021 01:08:56 - INFO - __main__ -   Batch number = 67
Evaluating:  68%|██████▊   | 67/99 [00:20<00:09,  3.27it/s]11/28/2021 01:08:57 - INFO - __main__ -   Batch number = 68
Evaluating:  69%|██████▊   | 68/99 [00:21<00:09,  3.25it/s]11/28/2021 01:08:57 - INFO - __main__ -   Batch number = 69
Evaluating:  70%|██████▉   | 69/99 [00:21<00:09,  3.24it/s]11/28/2021 01:08:57 - INFO - __main__ -   Batch number = 70
Evaluating:  71%|███████   | 70/99 [00:21<00:08,  3.23it/s]11/28/2021 01:08:58 - INFO - __main__ -   Batch number = 71
Evaluating:  72%|███████▏  | 71/99 [00:22<00:08,  3.23it/s]11/28/2021 01:08:58 - INFO - __main__ -   Batch number = 72
Evaluating:  73%|███████▎  | 72/99 [00:22<00:08,  3.22it/s]11/28/2021 01:08:58 - INFO - __main__ -   Batch number = 73
Evaluating:  74%|███████▎  | 73/99 [00:22<00:08,  3.23it/s]11/28/2021 01:08:58 - INFO - __main__ -   Batch number = 74
Evaluating:  75%|███████▍  | 74/99 [00:22<00:07,  3.23it/s]11/28/2021 01:08:59 - INFO - __main__ -   Batch number = 75
Evaluating:  76%|███████▌  | 75/99 [00:23<00:07,  3.23it/s]11/28/2021 01:08:59 - INFO - __main__ -   Batch number = 76
Evaluating:  77%|███████▋  | 76/99 [00:23<00:07,  3.23it/s]11/28/2021 01:08:59 - INFO - __main__ -   Batch number = 77
Evaluating:  78%|███████▊  | 77/99 [00:23<00:05,  3.83it/s]11/28/2021 01:09:00 - INFO - __main__ -   Batch number = 78
Evaluating:  79%|███████▉  | 78/99 [00:23<00:04,  4.40it/s]11/28/2021 01:09:00 - INFO - __main__ -   Batch number = 79
Evaluating:  80%|███████▉  | 79/99 [00:24<00:04,  4.83it/s]11/28/2021 01:09:00 - INFO - __main__ -   Batch number = 80
Evaluating:  81%|████████  | 80/99 [00:24<00:03,  5.23it/s]11/28/2021 01:09:00 - INFO - __main__ -   Batch number = 81
Evaluating:  82%|████████▏ | 81/99 [00:24<00:03,  4.91it/s]11/28/2021 01:09:00 - INFO - __main__ -   Batch number = 82
Evaluating:  83%|████████▎ | 82/99 [00:24<00:04,  4.22it/s]11/28/2021 01:09:01 - INFO - __main__ -   Batch number = 83
Evaluating:  84%|████████▍ | 83/99 [00:25<00:04,  3.84it/s]11/28/2021 01:09:01 - INFO - __main__ -   Batch number = 84
Evaluating:  85%|████████▍ | 84/99 [00:25<00:04,  3.62it/s]11/28/2021 01:09:01 - INFO - __main__ -   Batch number = 85
Evaluating:  86%|████████▌ | 85/99 [00:25<00:04,  3.48it/s]11/28/2021 01:09:01 - INFO - __main__ -   Batch number = 86
Evaluating:  87%|████████▋ | 86/99 [00:25<00:03,  3.42it/s]11/28/2021 01:09:02 - INFO - __main__ -   Batch number = 87
Evaluating:  88%|████████▊ | 87/99 [00:26<00:03,  3.36it/s]11/28/2021 01:09:02 - INFO - __main__ -   Batch number = 88
Evaluating:  89%|████████▉ | 88/99 [00:26<00:03,  3.24it/s]11/28/2021 01:09:02 - INFO - __main__ -   Batch number = 89
Evaluating:  90%|████████▉ | 89/99 [00:26<00:03,  3.22it/s]11/28/2021 01:09:03 - INFO - __main__ -   Batch number = 90
Evaluating:  91%|█████████ | 90/99 [00:27<00:02,  3.21it/s]11/28/2021 01:09:03 - INFO - __main__ -   Batch number = 91
Evaluating:  92%|█████████▏| 91/99 [00:27<00:02,  3.22it/s]11/28/2021 01:09:03 - INFO - __main__ -   Batch number = 92
Evaluating:  93%|█████████▎| 92/99 [00:27<00:02,  3.21it/s]11/28/2021 01:09:04 - INFO - __main__ -   Batch number = 93
Evaluating:  94%|█████████▍| 93/99 [00:28<00:01,  3.20it/s]11/28/2021 01:09:04 - INFO - __main__ -   Batch number = 94
Evaluating:  95%|█████████▍| 94/99 [00:28<00:01,  3.18it/s]11/28/2021 01:09:04 - INFO - __main__ -   Batch number = 95
Evaluating:  96%|█████████▌| 95/99 [00:28<00:01,  3.16it/s]11/28/2021 01:09:05 - INFO - __main__ -   Batch number = 96
Evaluating:  97%|█████████▋| 96/99 [00:29<00:00,  3.15it/s]11/28/2021 01:09:05 - INFO - __main__ -   Batch number = 97
Evaluating:  98%|█████████▊| 97/99 [00:29<00:00,  3.08it/s]11/28/2021 01:09:05 - INFO - __main__ -   Batch number = 98
Evaluating:  99%|█████████▉| 98/99 [00:29<00:00,  3.11it/s]11/28/2021 01:09:06 - INFO - __main__ -   Batch number = 99
Evaluating: 100%|██████████| 99/99 [00:29<00:00,  3.54it/s]Evaluating: 100%|██████████| 99/99 [00:29<00:00,  3.30it/s]
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:09:08 - INFO - __main__ -   ***** Evaluation result  in es *****
11/28/2021 01:09:08 - INFO - __main__ -     f1 = 0.8731506332426527
11/28/2021 01:09:08 - INFO - __main__ -     loss = 0.39907390481293803
11/28/2021 01:09:08 - INFO - __main__ -     precision = 0.8827184800531588
11/28/2021 01:09:08 - INFO - __main__ -     recall = 0.8637879754837622
45.03user 13.00system 0:57.64elapsed 100%CPU (0avgtext+0avgdata 3949808maxresident)k
2552inputs+11424outputs (0major+1588451minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:09:11 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='es', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:09:11 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:09:11 - INFO - __main__ -   Seed = 2
11/28/2021 01:09:11 - INFO - root -   save model
11/28/2021 01:09:11 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='es', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:09:11 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:09:14 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:09:20 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:09:20 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:09:20 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
11/28/2021 01:09:20 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:09:20 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:09:20 - INFO - root -   loading lang adpater am/wiki@ukp
11/28/2021 01:09:20 - INFO - __main__ -   Adapter Languages : ['am'], Length : 1
11/28/2021 01:09:20 - INFO - __main__ -   Adapter Names ['am/wiki@ukp'], Length : 1
11/28/2021 01:09:20 - INFO - __main__ -   Language = am
11/28/2021 01:09:20 - INFO - __main__ -   Adapter Name = am/wiki@ukp
Found matching adapter at: adapters/ukp/bert-base-multilingual-cased_am_wiki_pfeiffer.json
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/am/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_am_wiki_pfeiffer.zip.
PyTorch version 1.10.0+cu102 available.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/adapter_config.json
Adding adapter 'am' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/head_config.json
11/28/2021 01:09:22 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='eu', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:09:22 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:09:22 - INFO - __main__ -   Seed = 1
11/28/2021 01:09:22 - INFO - root -   save model
11/28/2021 01:09:22 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='eu', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:09:22 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:09:25 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:09:30 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:09:30 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:09:30 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
11/28/2021 01:09:30 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:09:30 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:09:30 - INFO - root -   loading lang adpater am/wiki@ukp
11/28/2021 01:09:30 - INFO - __main__ -   Adapter Languages : ['am'], Length : 1
11/28/2021 01:09:30 - INFO - __main__ -   Adapter Names ['am/wiki@ukp'], Length : 1
11/28/2021 01:09:30 - INFO - __main__ -   Language = am
11/28/2021 01:09:30 - INFO - __main__ -   Adapter Name = am/wiki@ukp
Found matching adapter at: adapters/ukp/bert-base-multilingual-cased_am_wiki_pfeiffer.json
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/am/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_am_wiki_pfeiffer.zip.
11/28/2021 01:09:31 - INFO - __main__ -   Language adapter for es not found, using am instead
11/28/2021 01:09:31 - INFO - __main__ -   Set active language adapter to am
11/28/2021 01:09:31 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:09:31 - INFO - __main__ -   Adapter Languages = ['am']
11/28/2021 01:09:31 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_es_bert-base-multilingual-cased_128
11/28/2021 01:09:31 - INFO - __main__ -   ***** Running evaluation  in es *****
11/28/2021 01:09:31 - INFO - __main__ -     Num examples = 3154
11/28/2021 01:09:31 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/99 [00:00<?, ?it/s]11/28/2021 01:09:31 - INFO - __main__ -   Batch number = 1
Evaluating:   1%|          | 1/99 [00:00<00:45,  2.18it/s]11/28/2021 01:09:32 - INFO - __main__ -   Batch number = 2
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/adapter_config.json
Adding adapter 'am' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/head_config.json
Evaluating:   2%|▏         | 2/99 [00:00<00:43,  2.23it/s]11/28/2021 01:09:32 - INFO - __main__ -   Batch number = 3
Evaluating:   3%|▎         | 3/99 [00:01<00:44,  2.15it/s]11/28/2021 01:09:33 - INFO - __main__ -   Batch number = 4
Evaluating:   4%|▍         | 4/99 [00:01<00:43,  2.19it/s]11/28/2021 01:09:33 - INFO - __main__ -   Batch number = 5
Evaluating:   5%|▌         | 5/99 [00:02<00:42,  2.20it/s]11/28/2021 01:09:34 - INFO - __main__ -   Batch number = 6
Evaluating:   6%|▌         | 6/99 [00:02<00:42,  2.20it/s]11/28/2021 01:09:34 - INFO - __main__ -   Batch number = 7
Evaluating:   7%|▋         | 7/99 [00:03<00:41,  2.21it/s]11/28/2021 01:09:34 - INFO - __main__ -   Batch number = 8
Evaluating:   8%|▊         | 8/99 [00:03<00:41,  2.21it/s]11/28/2021 01:09:35 - INFO - __main__ -   Batch number = 9
Evaluating:   9%|▉         | 9/99 [00:04<00:41,  2.19it/s]11/28/2021 01:09:35 - INFO - __main__ -   Batch number = 10
Evaluating:  10%|█         | 10/99 [00:04<00:40,  2.20it/s]11/28/2021 01:09:36 - INFO - __main__ -   Batch number = 11
Evaluating:  11%|█         | 11/99 [00:04<00:39,  2.21it/s]11/28/2021 01:09:36 - INFO - __main__ -   Batch number = 12
Evaluating:  12%|█▏        | 12/99 [00:05<00:39,  2.22it/s]11/28/2021 01:09:37 - INFO - __main__ -   Batch number = 13
Evaluating:  13%|█▎        | 13/99 [00:05<00:38,  2.21it/s]11/28/2021 01:09:37 - INFO - __main__ -   Language adapter for eu not found, using am instead
11/28/2021 01:09:37 - INFO - __main__ -   Set active language adapter to am
11/28/2021 01:09:37 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:09:37 - INFO - __main__ -   Adapter Languages = ['am']
11/28/2021 01:09:37 - INFO - __main__ -   all languages = eu
11/28/2021 01:09:37 - INFO - __main__ -   Creating features from dataset file at /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/eu/test.bert-base-multilingual-cased in language eu
11/28/2021 01:09:37 - INFO - utils_tag -   lang_id=0, lang=eu, lang2id=None
11/28/2021 01:09:37 - INFO - __main__ -   Batch number = 14
11/28/2021 01:09:37 - INFO - utils_tag -   Writing example 0 of 1799
11/28/2021 01:09:37 - INFO - utils_tag -   *** Example ***
11/28/2021 01:09:37 - INFO - utils_tag -   guid: eu-1
11/28/2021 01:09:37 - INFO - utils_tag -   tokens: [CLS] Familia ##n , aldiz , ez da in ##ola ##ko ha ##zku ##nder ##ik ant ##zem ##aten eus ##kara ##ren era ##bile ##ran , ele ##bid ##un gaz ##teen ##en guraso ##ak er ##dal ##dun ##ak dire ##lako ora ##ind ##ik . [SEP]
11/28/2021 01:09:37 - INFO - utils_tag -   input_ids: 101 35728 10115 117 74083 117 13112 10143 10106 15154 10440 10228 74850 16497 10896 14917 25464 27151 14223 33066 10969 10411 15642 12111 117 12637 78489 11107 34055 20156 10136 38600 10710 10163 14555 44904 10710 19263 106382 14480 32524 10896 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:09:37 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:09:37 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:09:37 - INFO - utils_tag -   label_ids: -100 8 -100 13 5 13 10 4 1 -100 -100 8 -100 -100 -100 16 -100 -100 8 -100 -100 8 -100 -100 13 8 -100 -100 1 -100 -100 8 -100 1 -100 -100 -100 4 -100 3 -100 -100 13 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
11/28/2021 01:09:37 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
11/28/2021 01:09:37 - INFO - utils_tag -   *** Example ***
11/28/2021 01:09:37 - INFO - utils_tag -   guid: eu-2
11/28/2021 01:09:37 - INFO - utils_tag -   tokens: [CLS] Lux ##enburg ##oko ag ##inta ##ri politiko ##ek zi ##urt ##atu zuten es ##kari hau ona ##rt ##zeko pre ##st ze ##ude ##la eta he ##ga ##zki ##n t ##xi ##ki bat za ##in ze ##go ##ela Find ##elek ##o aire ##port ##uan . [SEP]
11/28/2021 01:09:37 - INFO - utils_tag -   input_ids: 101 89497 29012 20954 16942 42948 10401 97806 10707 57087 30546 19003 13635 10196 40028 20091 15546 10976 23462 12229 10562 10941 17756 10330 10408 10261 10483 26480 10115 188 20572 10506 11519 10339 10245 10941 10797 15108 50738 102500 10133 20989 15520 16093 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:09:37 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:09:37 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:09:37 - INFO - utils_tag -   label_ids: -100 12 -100 -100 8 -100 -100 1 -100 16 -100 -100 4 8 -100 6 16 -100 -100 1 -100 16 -100 -100 5 8 -100 -100 -100 1 -100 -100 9 3 -100 16 -100 -100 12 -100 -100 8 -100 -100 13 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
11/28/2021 01:09:37 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
11/28/2021 01:09:37 - INFO - utils_tag -   *** Example ***
11/28/2021 01:09:37 - INFO - utils_tag -   guid: eu-3
11/28/2021 01:09:37 - INFO - utils_tag -   tokens: [CLS] Ara ##zoa da ez daude ##la ins ##tit ##uzi ##oak ara ##u horiek bet ##etzen diren be ##rmat ##zeko . [SEP]
11/28/2021 01:09:37 - INFO - utils_tag -   input_ids: 101 54789 45152 10143 13112 40909 10330 15498 23516 55485 38724 13785 10138 46903 13009 57810 45715 10347 103286 23462 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:09:37 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:09:37 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:09:37 - INFO - utils_tag -   label_ids: -100 8 -100 16 10 16 -100 8 -100 -100 -100 8 -100 6 16 -100 4 16 -100 -100 13 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
11/28/2021 01:09:37 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
11/28/2021 01:09:37 - INFO - utils_tag -   *** Example ***
11/28/2021 01:09:37 - INFO - utils_tag -   guid: eu-4
11/28/2021 01:09:37 - INFO - utils_tag -   tokens: [CLS] I ##pu ##in poli ##t baten bu ##ka ##era da hau , zo ##rit ##xar ##rez . [SEP]
11/28/2021 01:09:37 - INFO - utils_tag -   input_ids: 101 146 17490 10245 91929 10123 20814 11499 10371 12015 10143 20091 117 12555 16598 30019 34305 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:09:37 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:09:37 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:09:37 - INFO - utils_tag -   label_ids: -100 8 -100 -100 1 -100 9 8 -100 -100 4 6 13 3 -100 -100 -100 13 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
11/28/2021 01:09:37 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
11/28/2021 01:09:37 - INFO - utils_tag -   *** Example ***
11/28/2021 01:09:37 - INFO - utils_tag -   guid: eu-5
11/28/2021 01:09:37 - INFO - utils_tag -   tokens: [CLS] Lehen sari ##a 50 . 000 pez ##etako ##a izan ##en da , bigarren ##a 25 . 000 ##koa eta hiru ##garren ##a 15 . 000 ##koa . [SEP]
11/28/2021 01:09:37 - INFO - utils_tag -   input_ids: 101 59951 76450 10113 10462 119 10259 89764 35895 10113 12790 10136 10143 117 26809 10113 10258 119 10259 24521 10408 25621 89228 10113 10208 119 10259 24521 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:09:37 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:09:37 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:09:37 - INFO - utils_tag -   label_ids: -100 9 8 -100 9 -100 -100 8 -100 -100 4 -100 4 13 9 -100 9 -100 -100 -100 5 9 -100 -100 9 -100 -100 -100 13 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
11/28/2021 01:09:37 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Evaluating:  14%|█▍        | 14/99 [00:06<00:41,  2.07it/s]11/28/2021 01:09:38 - INFO - __main__ -   Batch number = 15
Evaluating:  15%|█▌        | 15/99 [00:06<00:39,  2.10it/s]11/28/2021 01:09:38 - INFO - __main__ -   Batch number = 16
Evaluating:  16%|█▌        | 16/99 [00:07<00:38,  2.13it/s]11/28/2021 01:09:39 - INFO - __main__ -   Batch number = 17
11/28/2021 01:09:39 - INFO - __main__ -   Saving features into cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_eu_bert-base-multilingual-cased_128, len(features)=1799
Evaluating:  17%|█▋        | 17/99 [00:07<00:38,  2.15it/s]11/28/2021 01:09:39 - INFO - __main__ -   Batch number = 18
11/28/2021 01:09:39 - INFO - __main__ -   ***** Running evaluation  in eu *****
11/28/2021 01:09:39 - INFO - __main__ -     Num examples = 1799
11/28/2021 01:09:39 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/57 [00:00<?, ?it/s]11/28/2021 01:09:39 - INFO - __main__ -   Batch number = 1
Evaluating:   2%|▏         | 1/57 [00:00<00:08,  6.56it/s]11/28/2021 01:09:40 - INFO - __main__ -   Batch number = 2
Evaluating:  18%|█▊        | 18/99 [00:08<00:37,  2.16it/s]11/28/2021 01:09:40 - INFO - __main__ -   Batch number = 19
Evaluating:   4%|▎         | 2/57 [00:00<00:07,  7.00it/s]11/28/2021 01:09:40 - INFO - __main__ -   Batch number = 3
Evaluating:   5%|▌         | 3/57 [00:00<00:07,  7.27it/s]11/28/2021 01:09:40 - INFO - __main__ -   Batch number = 4
Evaluating:   7%|▋         | 4/57 [00:00<00:07,  7.48it/s]11/28/2021 01:09:40 - INFO - __main__ -   Batch number = 5
Evaluating:  19%|█▉        | 19/99 [00:08<00:36,  2.17it/s]11/28/2021 01:09:40 - INFO - __main__ -   Batch number = 20
Evaluating:   9%|▉         | 5/57 [00:00<00:06,  7.60it/s]11/28/2021 01:09:40 - INFO - __main__ -   Batch number = 6
Evaluating:  11%|█         | 6/57 [00:00<00:06,  7.67it/s]11/28/2021 01:09:40 - INFO - __main__ -   Batch number = 7
Evaluating:  12%|█▏        | 7/57 [00:00<00:06,  7.67it/s]11/28/2021 01:09:40 - INFO - __main__ -   Batch number = 8
Evaluating:  14%|█▍        | 8/57 [00:01<00:06,  7.70it/s]11/28/2021 01:09:40 - INFO - __main__ -   Batch number = 9
Evaluating:  20%|██        | 20/99 [00:09<00:36,  2.19it/s]11/28/2021 01:09:40 - INFO - __main__ -   Batch number = 21
Evaluating:  16%|█▌        | 9/57 [00:01<00:06,  7.70it/s]11/28/2021 01:09:41 - INFO - __main__ -   Batch number = 10
Evaluating:  18%|█▊        | 10/57 [00:01<00:06,  7.70it/s]11/28/2021 01:09:41 - INFO - __main__ -   Batch number = 11
Evaluating:  19%|█▉        | 11/57 [00:01<00:05,  7.68it/s]11/28/2021 01:09:41 - INFO - __main__ -   Batch number = 12
Evaluating:  21%|██        | 12/57 [00:01<00:05,  7.68it/s]11/28/2021 01:09:41 - INFO - __main__ -   Batch number = 13
Evaluating:  21%|██        | 21/99 [00:09<00:36,  2.16it/s]11/28/2021 01:09:41 - INFO - __main__ -   Batch number = 22
Evaluating:  23%|██▎       | 13/57 [00:01<00:05,  7.70it/s]11/28/2021 01:09:41 - INFO - __main__ -   Batch number = 14
Evaluating:  25%|██▍       | 14/57 [00:01<00:05,  7.69it/s]11/28/2021 01:09:41 - INFO - __main__ -   Batch number = 15
Evaluating:  26%|██▋       | 15/57 [00:01<00:05,  7.69it/s]11/28/2021 01:09:41 - INFO - __main__ -   Batch number = 16
Evaluating:  22%|██▏       | 22/99 [00:10<00:35,  2.18it/s]11/28/2021 01:09:41 - INFO - __main__ -   Batch number = 23
Evaluating:  28%|██▊       | 16/57 [00:02<00:05,  7.68it/s]11/28/2021 01:09:41 - INFO - __main__ -   Batch number = 17
Evaluating:  30%|██▉       | 17/57 [00:02<00:05,  7.58it/s]11/28/2021 01:09:42 - INFO - __main__ -   Batch number = 18
Evaluating:  32%|███▏      | 18/57 [00:02<00:05,  7.59it/s]11/28/2021 01:09:42 - INFO - __main__ -   Batch number = 19
Evaluating:  23%|██▎       | 23/99 [00:10<00:34,  2.22it/s]11/28/2021 01:09:42 - INFO - __main__ -   Batch number = 24
Evaluating:  33%|███▎      | 19/57 [00:02<00:04,  7.62it/s]11/28/2021 01:09:42 - INFO - __main__ -   Batch number = 20
Evaluating:  35%|███▌      | 20/57 [00:02<00:04,  7.62it/s]11/28/2021 01:09:42 - INFO - __main__ -   Batch number = 21
Evaluating:  37%|███▋      | 21/57 [00:02<00:04,  7.54it/s]11/28/2021 01:09:42 - INFO - __main__ -   Batch number = 22
Evaluating:  39%|███▊      | 22/57 [00:02<00:04,  7.54it/s]11/28/2021 01:09:42 - INFO - __main__ -   Batch number = 23
Evaluating:  24%|██▍       | 24/99 [00:10<00:33,  2.22it/s]11/28/2021 01:09:42 - INFO - __main__ -   Batch number = 25
Evaluating:  40%|████      | 23/57 [00:03<00:04,  7.57it/s]11/28/2021 01:09:42 - INFO - __main__ -   Batch number = 24
Evaluating:  42%|████▏     | 24/57 [00:03<00:04,  7.61it/s]11/28/2021 01:09:43 - INFO - __main__ -   Batch number = 25
Evaluating:  44%|████▍     | 25/57 [00:03<00:04,  7.50it/s]11/28/2021 01:09:43 - INFO - __main__ -   Batch number = 26
Evaluating:  25%|██▌       | 25/99 [00:11<00:33,  2.22it/s]11/28/2021 01:09:43 - INFO - __main__ -   Batch number = 26
Evaluating:  46%|████▌     | 26/57 [00:03<00:04,  7.55it/s]11/28/2021 01:09:43 - INFO - __main__ -   Batch number = 27
Evaluating:  47%|████▋     | 27/57 [00:03<00:03,  7.59it/s]11/28/2021 01:09:43 - INFO - __main__ -   Batch number = 28
Evaluating:  49%|████▉     | 28/57 [00:03<00:03,  7.63it/s]11/28/2021 01:09:43 - INFO - __main__ -   Batch number = 29
Evaluating:  51%|█████     | 29/57 [00:03<00:03,  7.65it/s]11/28/2021 01:09:43 - INFO - __main__ -   Batch number = 30
Evaluating:  26%|██▋       | 26/99 [00:11<00:32,  2.22it/s]11/28/2021 01:09:43 - INFO - __main__ -   Batch number = 27
Evaluating:  53%|█████▎    | 30/57 [00:03<00:03,  7.67it/s]11/28/2021 01:09:43 - INFO - __main__ -   Batch number = 31
Evaluating:  54%|█████▍    | 31/57 [00:04<00:03,  7.69it/s]11/28/2021 01:09:43 - INFO - __main__ -   Batch number = 32
Evaluating:  56%|█████▌    | 32/57 [00:04<00:03,  7.69it/s]11/28/2021 01:09:44 - INFO - __main__ -   Batch number = 33
Evaluating:  27%|██▋       | 27/99 [00:12<00:32,  2.22it/s]11/28/2021 01:09:44 - INFO - __main__ -   Batch number = 28
Evaluating:  58%|█████▊    | 33/57 [00:04<00:03,  7.70it/s]11/28/2021 01:09:44 - INFO - __main__ -   Batch number = 34
Evaluating:  60%|█████▉    | 34/57 [00:04<00:02,  7.72it/s]11/28/2021 01:09:44 - INFO - __main__ -   Batch number = 35
Evaluating:  61%|██████▏   | 35/57 [00:04<00:02,  7.69it/s]11/28/2021 01:09:44 - INFO - __main__ -   Batch number = 36
Evaluating:  63%|██████▎   | 36/57 [00:04<00:02,  7.64it/s]11/28/2021 01:09:44 - INFO - __main__ -   Batch number = 37
Evaluating:  28%|██▊       | 28/99 [00:12<00:32,  2.20it/s]11/28/2021 01:09:44 - INFO - __main__ -   Batch number = 29
Evaluating:  65%|██████▍   | 37/57 [00:04<00:02,  7.63it/s]11/28/2021 01:09:44 - INFO - __main__ -   Batch number = 38
Evaluating:  67%|██████▋   | 38/57 [00:04<00:02,  7.59it/s]11/28/2021 01:09:44 - INFO - __main__ -   Batch number = 39
Evaluating:  68%|██████▊   | 39/57 [00:05<00:02,  7.59it/s]11/28/2021 01:09:45 - INFO - __main__ -   Batch number = 40
Evaluating:  29%|██▉       | 29/99 [00:13<00:31,  2.19it/s]11/28/2021 01:09:45 - INFO - __main__ -   Batch number = 30
Evaluating:  70%|███████   | 40/57 [00:05<00:02,  7.59it/s]11/28/2021 01:09:45 - INFO - __main__ -   Batch number = 41
Evaluating:  72%|███████▏  | 41/57 [00:05<00:02,  7.58it/s]11/28/2021 01:09:45 - INFO - __main__ -   Batch number = 42
Evaluating:  74%|███████▎  | 42/57 [00:05<00:01,  7.58it/s]11/28/2021 01:09:45 - INFO - __main__ -   Batch number = 43
Evaluating:  75%|███████▌  | 43/57 [00:05<00:01,  7.57it/s]11/28/2021 01:09:45 - INFO - __main__ -   Batch number = 44
Evaluating:  30%|███       | 30/99 [00:13<00:31,  2.17it/s]11/28/2021 01:09:45 - INFO - __main__ -   Batch number = 31
Evaluating:  77%|███████▋  | 44/57 [00:05<00:01,  7.56it/s]11/28/2021 01:09:45 - INFO - __main__ -   Batch number = 45
Evaluating:  79%|███████▉  | 45/57 [00:05<00:01,  7.55it/s]11/28/2021 01:09:45 - INFO - __main__ -   Batch number = 46
Evaluating:  81%|████████  | 46/57 [00:06<00:01,  7.55it/s]11/28/2021 01:09:45 - INFO - __main__ -   Batch number = 47
Evaluating:  31%|███▏      | 31/99 [00:14<00:31,  2.14it/s]11/28/2021 01:09:46 - INFO - __main__ -   Batch number = 32
Evaluating:  82%|████████▏ | 47/57 [00:06<00:01,  7.51it/s]11/28/2021 01:09:46 - INFO - __main__ -   Batch number = 48
Evaluating:  84%|████████▍ | 48/57 [00:06<00:01,  7.51it/s]11/28/2021 01:09:46 - INFO - __main__ -   Batch number = 49
Evaluating:  86%|████████▌ | 49/57 [00:06<00:01,  7.54it/s]11/28/2021 01:09:46 - INFO - __main__ -   Batch number = 50
Evaluating:  88%|████████▊ | 50/57 [00:06<00:00,  7.54it/s]11/28/2021 01:09:46 - INFO - __main__ -   Batch number = 51
Evaluating:  32%|███▏      | 32/99 [00:14<00:31,  2.15it/s]11/28/2021 01:09:46 - INFO - __main__ -   Batch number = 33
Evaluating:  89%|████████▉ | 51/57 [00:06<00:00,  7.59it/s]11/28/2021 01:09:46 - INFO - __main__ -   Batch number = 52
Evaluating:  91%|█████████ | 52/57 [00:06<00:00,  7.59it/s]11/28/2021 01:09:46 - INFO - __main__ -   Batch number = 53
Evaluating:  93%|█████████▎| 53/57 [00:06<00:00,  7.58it/s]11/28/2021 01:09:46 - INFO - __main__ -   Batch number = 54
Evaluating:  33%|███▎      | 33/99 [00:15<00:30,  2.15it/s]11/28/2021 01:09:46 - INFO - __main__ -   Batch number = 34
Evaluating:  95%|█████████▍| 54/57 [00:07<00:00,  7.58it/s]11/28/2021 01:09:46 - INFO - __main__ -   Batch number = 55
Evaluating:  96%|█████████▋| 55/57 [00:07<00:00,  7.59it/s]11/28/2021 01:09:47 - INFO - __main__ -   Batch number = 56
Evaluating:  98%|█████████▊| 56/57 [00:07<00:00,  7.53it/s]11/28/2021 01:09:47 - INFO - __main__ -   Batch number = 57
Evaluating: 100%|██████████| 57/57 [00:07<00:00,  7.68it/s]Evaluating:  34%|███▍      | 34/99 [00:15<00:30,  2.15it/s]11/28/2021 01:09:47 - INFO - __main__ -   Batch number = 35
Evaluating:  35%|███▌      | 35/99 [00:16<00:29,  2.15it/s]
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:09:47 - INFO - __main__ -   ***** Evaluation result  in eu *****
11/28/2021 01:09:47 - INFO - __main__ -     f1 = 0.5825942890556477
11/28/2021 01:09:47 - INFO - __main__ -     loss = 1.353027435771206
11/28/2021 01:09:47 - INFO - __main__ -   Batch number = 36
11/28/2021 01:09:47 - INFO - __main__ -     precision = 0.6233251655283875
11/28/2021 01:09:47 - INFO - __main__ -     recall = 0.5468599917669121
Evaluating:  36%|███▋      | 36/99 [00:16<00:29,  2.14it/s]11/28/2021 01:09:48 - INFO - __main__ -   Batch number = 37
Evaluating:  37%|███▋      | 37/99 [00:17<00:29,  2.14it/s]11/28/2021 01:09:48 - INFO - __main__ -   Batch number = 38
21.74user 7.75system 0:28.44elapsed 103%CPU (0avgtext+0avgdata 3936476maxresident)k
784inputs+6456outputs (0major+1419576minor)pagefaults 0swaps
Evaluating:  38%|███▊      | 38/99 [00:17<00:27,  2.24it/s]11/28/2021 01:09:49 - INFO - __main__ -   Batch number = 39
Evaluating:  39%|███▉      | 39/99 [00:17<00:26,  2.26it/s]11/28/2021 01:09:49 - INFO - __main__ -   Batch number = 40
Evaluating:  40%|████      | 40/99 [00:18<00:26,  2.22it/s]11/28/2021 01:09:50 - INFO - __main__ -   Batch number = 41
PyTorch version 1.10.0+cu102 available.
Evaluating:  41%|████▏     | 41/99 [00:18<00:26,  2.22it/s]11/28/2021 01:09:50 - INFO - __main__ -   Batch number = 42
11/28/2021 01:09:50 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='eu', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:09:50 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:09:50 - INFO - __main__ -   Seed = 2
11/28/2021 01:09:50 - INFO - root -   save model
11/28/2021 01:09:50 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='eu', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:09:50 - INFO - __main__ -   Loading pretrained model and tokenizer
Evaluating:  42%|████▏     | 42/99 [00:19<00:25,  2.21it/s]11/28/2021 01:09:51 - INFO - __main__ -   Batch number = 43
Evaluating:  43%|████▎     | 43/99 [00:19<00:25,  2.21it/s]11/28/2021 01:09:51 - INFO - __main__ -   Batch number = 44
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  44%|████▍     | 44/99 [00:20<00:24,  2.21it/s]11/28/2021 01:09:51 - INFO - __main__ -   Batch number = 45
Evaluating:  45%|████▌     | 45/99 [00:20<00:24,  2.21it/s]11/28/2021 01:09:52 - INFO - __main__ -   Batch number = 46
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  46%|████▋     | 46/99 [00:21<00:24,  2.19it/s]11/28/2021 01:09:53 - INFO - __main__ -   Batch number = 47
loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:09:53 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
Evaluating:  47%|████▋     | 47/99 [00:21<00:26,  1.98it/s]11/28/2021 01:09:53 - INFO - __main__ -   Batch number = 48
Evaluating:  48%|████▊     | 48/99 [00:22<00:24,  2.07it/s]11/28/2021 01:09:53 - INFO - __main__ -   Batch number = 49
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  49%|████▉     | 49/99 [00:22<00:23,  2.12it/s]11/28/2021 01:09:54 - INFO - __main__ -   Batch number = 50
Evaluating:  51%|█████     | 50/99 [00:22<00:22,  2.16it/s]11/28/2021 01:09:54 - INFO - __main__ -   Batch number = 51
Evaluating:  52%|█████▏    | 51/99 [00:23<00:22,  2.17it/s]11/28/2021 01:09:55 - INFO - __main__ -   Batch number = 52
Evaluating:  53%|█████▎    | 52/99 [00:23<00:21,  2.18it/s]11/28/2021 01:09:55 - INFO - __main__ -   Batch number = 53
Evaluating:  54%|█████▎    | 53/99 [00:24<00:21,  2.18it/s]11/28/2021 01:09:56 - INFO - __main__ -   Batch number = 54
Evaluating:  55%|█████▍    | 54/99 [00:24<00:20,  2.19it/s]11/28/2021 01:09:56 - INFO - __main__ -   Batch number = 55
Evaluating:  56%|█████▌    | 55/99 [00:25<00:20,  2.18it/s]11/28/2021 01:09:57 - INFO - __main__ -   Batch number = 56
Evaluating:  57%|█████▋    | 56/99 [00:25<00:19,  2.16it/s]11/28/2021 01:09:57 - INFO - __main__ -   Batch number = 57
Evaluating:  58%|█████▊    | 57/99 [00:26<00:19,  2.18it/s]11/28/2021 01:09:58 - INFO - __main__ -   Batch number = 58
Evaluating:  59%|█████▊    | 58/99 [00:26<00:18,  2.20it/s]11/28/2021 01:09:58 - INFO - __main__ -   Batch number = 59
Evaluating:  60%|█████▉    | 59/99 [00:26<00:16,  2.48it/s]11/28/2021 01:09:58 - INFO - __main__ -   Batch number = 60
Evaluating:  61%|██████    | 60/99 [00:27<00:14,  2.70it/s]11/28/2021 01:09:59 - INFO - __main__ -   Batch number = 61
Evaluating:  62%|██████▏   | 61/99 [00:27<00:12,  2.93it/s]11/28/2021 01:09:59 - INFO - __main__ -   Batch number = 62
Evaluating:  63%|██████▎   | 62/99 [00:27<00:12,  3.05it/s]11/28/2021 01:09:59 - INFO - __main__ -   Batch number = 63
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:09:59 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:09:59 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:09:59 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
11/28/2021 01:09:59 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:09:59 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:09:59 - INFO - root -   loading lang adpater am/wiki@ukp
11/28/2021 01:09:59 - INFO - __main__ -   Adapter Languages : ['am'], Length : 1
11/28/2021 01:09:59 - INFO - __main__ -   Adapter Names ['am/wiki@ukp'], Length : 1
11/28/2021 01:09:59 - INFO - __main__ -   Language = am
11/28/2021 01:09:59 - INFO - __main__ -   Adapter Name = am/wiki@ukp
Found matching adapter at: adapters/ukp/bert-base-multilingual-cased_am_wiki_pfeiffer.json
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/am/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_am_wiki_pfeiffer.zip.
Evaluating:  64%|██████▎   | 63/99 [00:28<00:11,  3.11it/s]11/28/2021 01:09:59 - INFO - __main__ -   Batch number = 64
Evaluating:  65%|██████▍   | 64/99 [00:28<00:10,  3.19it/s]11/28/2021 01:10:00 - INFO - __main__ -   Batch number = 65
Evaluating:  66%|██████▌   | 65/99 [00:28<00:10,  3.24it/s]11/28/2021 01:10:00 - INFO - __main__ -   Batch number = 66
Evaluating:  67%|██████▋   | 66/99 [00:28<00:10,  3.30it/s]11/28/2021 01:10:00 - INFO - __main__ -   Batch number = 67
Evaluating:  68%|██████▊   | 67/99 [00:29<00:09,  3.27it/s]11/28/2021 01:10:01 - INFO - __main__ -   Batch number = 68
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/adapter_config.json
Adding adapter 'am' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/head_config.json
Evaluating:  69%|██████▊   | 68/99 [00:29<00:09,  3.30it/s]11/28/2021 01:10:01 - INFO - __main__ -   Batch number = 69
Evaluating:  70%|██████▉   | 69/99 [00:29<00:09,  3.32it/s]11/28/2021 01:10:01 - INFO - __main__ -   Batch number = 70
Evaluating:  71%|███████   | 70/99 [00:30<00:08,  3.38it/s]11/28/2021 01:10:01 - INFO - __main__ -   Batch number = 71
Evaluating:  72%|███████▏  | 71/99 [00:30<00:08,  3.38it/s]11/28/2021 01:10:02 - INFO - __main__ -   Batch number = 72
Evaluating:  73%|███████▎  | 72/99 [00:30<00:08,  3.31it/s]11/28/2021 01:10:02 - INFO - __main__ -   Batch number = 73
Evaluating:  74%|███████▎  | 73/99 [00:31<00:07,  3.34it/s]11/28/2021 01:10:02 - INFO - __main__ -   Batch number = 74
Evaluating:  75%|███████▍  | 74/99 [00:31<00:07,  3.32it/s]11/28/2021 01:10:03 - INFO - __main__ -   Batch number = 75
Evaluating:  76%|███████▌  | 75/99 [00:31<00:07,  3.37it/s]11/28/2021 01:10:03 - INFO - __main__ -   Batch number = 76
Evaluating:  77%|███████▋  | 76/99 [00:31<00:06,  3.36it/s]11/28/2021 01:10:03 - INFO - __main__ -   Batch number = 77
Evaluating:  78%|███████▊  | 77/99 [00:32<00:06,  3.38it/s]11/28/2021 01:10:04 - INFO - __main__ -   Batch number = 78
Evaluating:  79%|███████▉  | 78/99 [00:32<00:06,  3.35it/s]11/28/2021 01:10:04 - INFO - __main__ -   Batch number = 79
Evaluating:  80%|███████▉  | 79/99 [00:32<00:05,  3.34it/s]11/28/2021 01:10:04 - INFO - __main__ -   Batch number = 80
Evaluating:  81%|████████  | 80/99 [00:33<00:05,  3.26it/s]11/28/2021 01:10:04 - INFO - __main__ -   Batch number = 81
Evaluating:  82%|████████▏ | 81/99 [00:33<00:05,  3.23it/s]11/28/2021 01:10:05 - INFO - __main__ -   Batch number = 82
Evaluating:  83%|████████▎ | 82/99 [00:33<00:05,  3.32it/s]11/28/2021 01:10:05 - INFO - __main__ -   Batch number = 83
11/28/2021 01:10:05 - INFO - __main__ -   Language adapter for eu not found, using am instead
11/28/2021 01:10:05 - INFO - __main__ -   Set active language adapter to am
11/28/2021 01:10:05 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:10:05 - INFO - __main__ -   Adapter Languages = ['am']
11/28/2021 01:10:05 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_eu_bert-base-multilingual-cased_128
Evaluating:  84%|████████▍ | 83/99 [00:34<00:04,  3.33it/s]11/28/2021 01:10:05 - INFO - __main__ -   Batch number = 84
11/28/2021 01:10:06 - INFO - __main__ -   ***** Running evaluation  in eu *****
11/28/2021 01:10:06 - INFO - __main__ -     Num examples = 1799
11/28/2021 01:10:06 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/57 [00:00<?, ?it/s]11/28/2021 01:10:06 - INFO - __main__ -   Batch number = 1
Evaluating:  85%|████████▍ | 84/99 [00:34<00:04,  3.36it/s]11/28/2021 01:10:06 - INFO - __main__ -   Batch number = 85
Evaluating:   2%|▏         | 1/57 [00:00<00:08,  6.73it/s]11/28/2021 01:10:06 - INFO - __main__ -   Batch number = 2
Evaluating:   4%|▎         | 2/57 [00:00<00:07,  7.14it/s]11/28/2021 01:10:06 - INFO - __main__ -   Batch number = 3
Evaluating:  86%|████████▌ | 85/99 [00:34<00:04,  3.35it/s]11/28/2021 01:10:06 - INFO - __main__ -   Batch number = 86
Evaluating:   5%|▌         | 3/57 [00:00<00:07,  7.40it/s]11/28/2021 01:10:06 - INFO - __main__ -   Batch number = 4
Evaluating:   7%|▋         | 4/57 [00:00<00:07,  7.53it/s]11/28/2021 01:10:06 - INFO - __main__ -   Batch number = 5
Evaluating:  87%|████████▋ | 86/99 [00:34<00:03,  3.32it/s]11/28/2021 01:10:06 - INFO - __main__ -   Batch number = 87
Evaluating:   9%|▉         | 5/57 [00:00<00:06,  7.60it/s]11/28/2021 01:10:06 - INFO - __main__ -   Batch number = 6
Evaluating:  11%|█         | 6/57 [00:00<00:06,  7.65it/s]11/28/2021 01:10:06 - INFO - __main__ -   Batch number = 7
Evaluating:  12%|█▏        | 7/57 [00:00<00:06,  7.69it/s]11/28/2021 01:10:07 - INFO - __main__ -   Batch number = 8
Evaluating:  88%|████████▊ | 87/99 [00:35<00:03,  3.33it/s]11/28/2021 01:10:07 - INFO - __main__ -   Batch number = 88
Evaluating:  14%|█▍        | 8/57 [00:01<00:06,  7.72it/s]11/28/2021 01:10:07 - INFO - __main__ -   Batch number = 9
Evaluating:  16%|█▌        | 9/57 [00:01<00:06,  7.73it/s]11/28/2021 01:10:07 - INFO - __main__ -   Batch number = 10
Evaluating:  89%|████████▉ | 88/99 [00:35<00:03,  3.33it/s]11/28/2021 01:10:07 - INFO - __main__ -   Batch number = 89
Evaluating:  18%|█▊        | 10/57 [00:01<00:06,  7.71it/s]11/28/2021 01:10:07 - INFO - __main__ -   Batch number = 11
Evaluating:  19%|█▉        | 11/57 [00:01<00:05,  7.69it/s]11/28/2021 01:10:07 - INFO - __main__ -   Batch number = 12
Evaluating:  90%|████████▉ | 89/99 [00:35<00:03,  3.26it/s]11/28/2021 01:10:07 - INFO - __main__ -   Batch number = 90
Evaluating:  21%|██        | 12/57 [00:01<00:05,  7.69it/s]11/28/2021 01:10:07 - INFO - __main__ -   Batch number = 13
Evaluating:  23%|██▎       | 13/57 [00:01<00:05,  7.69it/s]11/28/2021 01:10:07 - INFO - __main__ -   Batch number = 14
Evaluating:  25%|██▍       | 14/57 [00:01<00:05,  7.67it/s]Evaluating:  91%|█████████ | 90/99 [00:36<00:02,  3.28it/s]11/28/2021 01:10:08 - INFO - __main__ -   Batch number = 15
11/28/2021 01:10:08 - INFO - __main__ -   Batch number = 91
Evaluating:  26%|██▋       | 15/57 [00:02<00:07,  5.55it/s]11/28/2021 01:10:08 - INFO - __main__ -   Batch number = 16
Evaluating:  28%|██▊       | 16/57 [00:02<00:06,  6.05it/s]11/28/2021 01:10:08 - INFO - __main__ -   Batch number = 17
Evaluating:  92%|█████████▏| 91/99 [00:36<00:02,  2.89it/s]11/28/2021 01:10:08 - INFO - __main__ -   Batch number = 92
Evaluating:  30%|██▉       | 17/57 [00:02<00:06,  6.47it/s]11/28/2021 01:10:08 - INFO - __main__ -   Batch number = 18
Evaluating:  32%|███▏      | 18/57 [00:02<00:05,  6.78it/s]11/28/2021 01:10:08 - INFO - __main__ -   Batch number = 19
Evaluating:  93%|█████████▎| 92/99 [00:36<00:02,  2.98it/s]11/28/2021 01:10:08 - INFO - __main__ -   Batch number = 93
Evaluating:  33%|███▎      | 19/57 [00:02<00:05,  7.03it/s]11/28/2021 01:10:08 - INFO - __main__ -   Batch number = 20
Evaluating:  35%|███▌      | 20/57 [00:02<00:05,  7.24it/s]11/28/2021 01:10:08 - INFO - __main__ -   Batch number = 21
Evaluating:  94%|█████████▍| 93/99 [00:37<00:01,  3.12it/s]11/28/2021 01:10:09 - INFO - __main__ -   Batch number = 94
Evaluating:  37%|███▋      | 21/57 [00:02<00:04,  7.36it/s]11/28/2021 01:10:09 - INFO - __main__ -   Batch number = 22
Evaluating:  39%|███▊      | 22/57 [00:03<00:04,  7.46it/s]11/28/2021 01:10:09 - INFO - __main__ -   Batch number = 23
Evaluating:  40%|████      | 23/57 [00:03<00:04,  7.54it/s]11/28/2021 01:10:09 - INFO - __main__ -   Batch number = 24
Evaluating:  95%|█████████▍| 94/99 [00:37<00:01,  3.24it/s]11/28/2021 01:10:09 - INFO - __main__ -   Batch number = 95
Evaluating:  42%|████▏     | 24/57 [00:03<00:04,  7.56it/s]11/28/2021 01:10:09 - INFO - __main__ -   Batch number = 25
Evaluating:  44%|████▍     | 25/57 [00:03<00:04,  7.59it/s]11/28/2021 01:10:09 - INFO - __main__ -   Batch number = 26
Evaluating:  96%|█████████▌| 95/99 [00:37<00:01,  3.37it/s]11/28/2021 01:10:09 - INFO - __main__ -   Batch number = 96
Evaluating:  46%|████▌     | 26/57 [00:03<00:04,  7.62it/s]11/28/2021 01:10:09 - INFO - __main__ -   Batch number = 27
Evaluating:  47%|████▋     | 27/57 [00:03<00:03,  7.61it/s]11/28/2021 01:10:09 - INFO - __main__ -   Batch number = 28
Evaluating:  97%|█████████▋| 96/99 [00:38<00:00,  3.41it/s]11/28/2021 01:10:09 - INFO - __main__ -   Batch number = 97
Evaluating:  49%|████▉     | 28/57 [00:03<00:03,  7.63it/s]11/28/2021 01:10:09 - INFO - __main__ -   Batch number = 29
Evaluating:  51%|█████     | 29/57 [00:03<00:03,  7.64it/s]11/28/2021 01:10:10 - INFO - __main__ -   Batch number = 30
Evaluating:  98%|█████████▊| 97/99 [00:38<00:00,  3.49it/s]11/28/2021 01:10:10 - INFO - __main__ -   Batch number = 98
Evaluating:  53%|█████▎    | 30/57 [00:04<00:03,  7.62it/s]11/28/2021 01:10:10 - INFO - __main__ -   Batch number = 31
Evaluating:  54%|█████▍    | 31/57 [00:04<00:03,  7.60it/s]11/28/2021 01:10:10 - INFO - __main__ -   Batch number = 32
Evaluating:  99%|█████████▉| 98/99 [00:38<00:00,  3.47it/s]11/28/2021 01:10:10 - INFO - __main__ -   Batch number = 99
Evaluating:  56%|█████▌    | 32/57 [00:04<00:03,  7.61it/s]11/28/2021 01:10:10 - INFO - __main__ -   Batch number = 33
Evaluating:  58%|█████▊    | 33/57 [00:04<00:03,  7.60it/s]11/28/2021 01:10:10 - INFO - __main__ -   Batch number = 34
Evaluating: 100%|██████████| 99/99 [00:38<00:00,  3.78it/s]Evaluating: 100%|██████████| 99/99 [00:38<00:00,  2.55it/s]Evaluating:  60%|█████▉    | 34/57 [00:04<00:03,  7.61it/s]11/28/2021 01:10:10 - INFO - __main__ -   Batch number = 35
Evaluating:  61%|██████▏   | 35/57 [00:04<00:02,  7.61it/s]11/28/2021 01:10:10 - INFO - __main__ -   Batch number = 36
Evaluating:  63%|██████▎   | 36/57 [00:04<00:02,  7.58it/s]11/28/2021 01:10:11 - INFO - __main__ -   Batch number = 37
Evaluating:  65%|██████▍   | 37/57 [00:05<00:02,  7.58it/s]11/28/2021 01:10:11 - INFO - __main__ -   Batch number = 38
Evaluating:  67%|██████▋   | 38/57 [00:05<00:02,  7.60it/s]11/28/2021 01:10:11 - INFO - __main__ -   Batch number = 39
Evaluating:  68%|██████▊   | 39/57 [00:05<00:02,  7.57it/s]11/28/2021 01:10:11 - INFO - __main__ -   Batch number = 40
Evaluating:  70%|███████   | 40/57 [00:05<00:02,  7.55it/s]11/28/2021 01:10:11 - INFO - __main__ -   Batch number = 41
Evaluating:  72%|███████▏  | 41/57 [00:05<00:02,  7.54it/s]11/28/2021 01:10:11 - INFO - __main__ -   Batch number = 42
Evaluating:  74%|███████▎  | 42/57 [00:05<00:01,  7.53it/s]11/28/2021 01:10:11 - INFO - __main__ -   Batch number = 43
Evaluating:  75%|███████▌  | 43/57 [00:05<00:01,  7.55it/s]11/28/2021 01:10:11 - INFO - __main__ -   Batch number = 44
Evaluating:  77%|███████▋  | 44/57 [00:05<00:01,  7.57it/s]11/28/2021 01:10:12 - INFO - __main__ -   Batch number = 45
Evaluating:  79%|███████▉  | 45/57 [00:06<00:01,  7.55it/s]11/28/2021 01:10:12 - INFO - __main__ -   Batch number = 46
Evaluating:  81%|████████  | 46/57 [00:06<00:01,  7.55it/s]11/28/2021 01:10:12 - INFO - __main__ -   Batch number = 47
Evaluating:  82%|████████▏ | 47/57 [00:06<00:01,  7.36it/s]11/28/2021 01:10:12 - INFO - __main__ -   Batch number = 48
Evaluating:  84%|████████▍ | 48/57 [00:06<00:01,  7.34it/s]11/28/2021 01:10:12 - INFO - __main__ -   Batch number = 49
Evaluating:  86%|████████▌ | 49/57 [00:06<00:01,  7.39it/s]11/28/2021 01:10:12 - INFO - __main__ -   Batch number = 50

/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:10:12 - INFO - __main__ -   ***** Evaluation result  in es *****
11/28/2021 01:10:12 - INFO - __main__ -     f1 = 0.8594352591943744
11/28/2021 01:10:12 - INFO - __main__ -     loss = 0.4554084428031035
11/28/2021 01:10:12 - INFO - __main__ -     precision = 0.8684155531242701
11/28/2021 01:10:12 - INFO - __main__ -     recall = 0.8506387948992739
Evaluating:  88%|████████▊ | 50/57 [00:06<00:00,  7.43it/s]11/28/2021 01:10:12 - INFO - __main__ -   Batch number = 51
Evaluating:  89%|████████▉ | 51/57 [00:06<00:00,  7.45it/s]11/28/2021 01:10:13 - INFO - __main__ -   Batch number = 52
Evaluating:  91%|█████████ | 52/57 [00:07<00:00,  7.45it/s]11/28/2021 01:10:13 - INFO - __main__ -   Batch number = 53
Evaluating:  93%|█████████▎| 53/57 [00:07<00:00,  7.47it/s]11/28/2021 01:10:13 - INFO - __main__ -   Batch number = 54
Evaluating:  95%|█████████▍| 54/57 [00:07<00:00,  7.46it/s]11/28/2021 01:10:13 - INFO - __main__ -   Batch number = 55
Evaluating:  96%|█████████▋| 55/57 [00:07<00:00,  7.48it/s]11/28/2021 01:10:13 - INFO - __main__ -   Batch number = 56
Evaluating:  98%|█████████▊| 56/57 [00:07<00:00,  7.50it/s]11/28/2021 01:10:13 - INFO - __main__ -   Batch number = 57
Evaluating: 100%|██████████| 57/57 [00:07<00:00,  7.51it/s]46.63user 17.19system 1:04.69elapsed 98%CPU (0avgtext+0avgdata 3943712maxresident)k
8inputs+952outputs (0major+1928888minor)pagefaults 0swaps

/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:10:14 - INFO - __main__ -   ***** Evaluation result  in eu *****
11/28/2021 01:10:14 - INFO - __main__ -     f1 = 0.6035157193219685
11/28/2021 01:10:14 - INFO - __main__ -     loss = 1.2215935870220787
11/28/2021 01:10:14 - INFO - __main__ -     precision = 0.639200040918623
11/28/2021 01:10:14 - INFO - __main__ -     recall = 0.5716049947399716
19.82user 6.79system 0:26.30elapsed 101%CPU (0avgtext+0avgdata 3943456maxresident)k
0inputs+336outputs (0major+1623232minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:10:15 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='es', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:10:15 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:10:15 - INFO - __main__ -   Seed = 3
11/28/2021 01:10:15 - INFO - root -   save model
11/28/2021 01:10:15 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='es', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:10:15 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

PyTorch version 1.10.0+cu102 available.
11/28/2021 01:10:17 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='eu', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:10:17 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:10:17 - INFO - __main__ -   Seed = 3
11/28/2021 01:10:17 - INFO - root -   save model
11/28/2021 01:10:17 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='eu', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:10:17 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:10:18 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:10:19 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:10:24 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:10:24 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:10:24 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
11/28/2021 01:10:24 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:10:24 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:10:25 - INFO - root -   loading lang adpater am/wiki@ukp
11/28/2021 01:10:25 - INFO - __main__ -   Adapter Languages : ['am'], Length : 1
11/28/2021 01:10:25 - INFO - __main__ -   Adapter Names ['am/wiki@ukp'], Length : 1
11/28/2021 01:10:25 - INFO - __main__ -   Language = am
11/28/2021 01:10:25 - INFO - __main__ -   Adapter Name = am/wiki@ukp
Found matching adapter at: adapters/ukp/bert-base-multilingual-cased_am_wiki_pfeiffer.json
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/am/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_am_wiki_pfeiffer.zip.
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:10:26 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:10:26 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:10:26 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
11/28/2021 01:10:26 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:10:26 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:10:26 - INFO - root -   loading lang adpater am/wiki@ukp
11/28/2021 01:10:26 - INFO - __main__ -   Adapter Languages : ['am'], Length : 1
11/28/2021 01:10:26 - INFO - __main__ -   Adapter Names ['am/wiki@ukp'], Length : 1
11/28/2021 01:10:26 - INFO - __main__ -   Language = am
11/28/2021 01:10:26 - INFO - __main__ -   Adapter Name = am/wiki@ukp
Found matching adapter at: adapters/ukp/bert-base-multilingual-cased_am_wiki_pfeiffer.json
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/am/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_am_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/adapter_config.json
Adding adapter 'am' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/head_config.json
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/adapter_config.json
Adding adapter 'am' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/head_config.json
11/28/2021 01:10:32 - INFO - __main__ -   Language adapter for eu not found, using am instead
11/28/2021 01:10:32 - INFO - __main__ -   Set active language adapter to am
11/28/2021 01:10:32 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:10:32 - INFO - __main__ -   Adapter Languages = ['am']
11/28/2021 01:10:32 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_eu_bert-base-multilingual-cased_128
11/28/2021 01:10:33 - INFO - __main__ -   ***** Running evaluation  in eu *****
11/28/2021 01:10:33 - INFO - __main__ -     Num examples = 1799
11/28/2021 01:10:33 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/57 [00:00<?, ?it/s]11/28/2021 01:10:33 - INFO - __main__ -   Batch number = 1
Evaluating:   2%|▏         | 1/57 [00:00<00:07,  7.55it/s]11/28/2021 01:10:33 - INFO - __main__ -   Batch number = 2
Evaluating:   4%|▎         | 2/57 [00:00<00:07,  7.66it/s]11/28/2021 01:10:33 - INFO - __main__ -   Batch number = 3
Evaluating:   5%|▌         | 3/57 [00:00<00:07,  7.70it/s]11/28/2021 01:10:33 - INFO - __main__ -   Batch number = 4
Evaluating:   7%|▋         | 4/57 [00:00<00:06,  7.71it/s]11/28/2021 01:10:33 - INFO - __main__ -   Batch number = 5
Evaluating:   9%|▉         | 5/57 [00:00<00:06,  7.71it/s]11/28/2021 01:10:33 - INFO - __main__ -   Batch number = 6
Evaluating:  11%|█         | 6/57 [00:00<00:06,  7.73it/s]11/28/2021 01:10:33 - INFO - __main__ -   Batch number = 7
Evaluating:  12%|█▏        | 7/57 [00:00<00:06,  7.73it/s]11/28/2021 01:10:33 - INFO - __main__ -   Batch number = 8
Evaluating:  14%|█▍        | 8/57 [00:01<00:06,  7.73it/s]11/28/2021 01:10:34 - INFO - __main__ -   Batch number = 9
Evaluating:  16%|█▌        | 9/57 [00:01<00:07,  6.78it/s]11/28/2021 01:10:34 - INFO - __main__ -   Batch number = 10
Evaluating:  18%|█▊        | 10/57 [00:01<00:06,  7.03it/s]11/28/2021 01:10:34 - INFO - __main__ -   Batch number = 11
11/28/2021 01:10:34 - INFO - __main__ -   Language adapter for es not found, using am instead
11/28/2021 01:10:34 - INFO - __main__ -   Set active language adapter to am
11/28/2021 01:10:34 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:10:34 - INFO - __main__ -   Adapter Languages = ['am']
11/28/2021 01:10:34 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_es_bert-base-multilingual-cased_128
Evaluating:  19%|█▉        | 11/57 [00:01<00:06,  7.18it/s]11/28/2021 01:10:34 - INFO - __main__ -   Batch number = 12
Evaluating:  21%|██        | 12/57 [00:01<00:06,  7.25it/s]11/28/2021 01:10:34 - INFO - __main__ -   Batch number = 13
Evaluating:  23%|██▎       | 13/57 [00:01<00:05,  7.37it/s]11/28/2021 01:10:34 - INFO - __main__ -   Batch number = 14
Evaluating:  25%|██▍       | 14/57 [00:01<00:05,  7.47it/s]11/28/2021 01:10:34 - INFO - __main__ -   Batch number = 15
Evaluating:  26%|██▋       | 15/57 [00:02<00:05,  7.50it/s]11/28/2021 01:10:35 - INFO - __main__ -   Batch number = 16
11/28/2021 01:10:35 - INFO - __main__ -   ***** Running evaluation  in es *****
11/28/2021 01:10:35 - INFO - __main__ -     Num examples = 3154
11/28/2021 01:10:35 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/99 [00:00<?, ?it/s]11/28/2021 01:10:35 - INFO - __main__ -   Batch number = 1
Evaluating:  28%|██▊       | 16/57 [00:02<00:05,  7.56it/s]11/28/2021 01:10:35 - INFO - __main__ -   Batch number = 17
Evaluating:   1%|          | 1/99 [00:00<00:17,  5.71it/s]11/28/2021 01:10:35 - INFO - __main__ -   Batch number = 2
Evaluating:  30%|██▉       | 17/57 [00:02<00:05,  7.60it/s]11/28/2021 01:10:35 - INFO - __main__ -   Batch number = 18
Evaluating:   2%|▏         | 2/99 [00:00<00:15,  6.23it/s]11/28/2021 01:10:35 - INFO - __main__ -   Batch number = 3
Evaluating:  32%|███▏      | 18/57 [00:02<00:05,  7.60it/s]11/28/2021 01:10:35 - INFO - __main__ -   Batch number = 19
Evaluating:  33%|███▎      | 19/57 [00:02<00:04,  7.63it/s]11/28/2021 01:10:35 - INFO - __main__ -   Batch number = 20
Evaluating:   3%|▎         | 3/99 [00:00<00:14,  6.41it/s]11/28/2021 01:10:35 - INFO - __main__ -   Batch number = 4
Evaluating:  35%|███▌      | 20/57 [00:02<00:04,  7.64it/s]11/28/2021 01:10:35 - INFO - __main__ -   Batch number = 21
Evaluating:   4%|▍         | 4/99 [00:00<00:14,  6.53it/s]11/28/2021 01:10:35 - INFO - __main__ -   Batch number = 5
Evaluating:  37%|███▋      | 21/57 [00:02<00:04,  7.64it/s]11/28/2021 01:10:35 - INFO - __main__ -   Batch number = 22
Evaluating:   5%|▌         | 5/99 [00:00<00:14,  6.61it/s]11/28/2021 01:10:35 - INFO - __main__ -   Batch number = 6
Evaluating:  39%|███▊      | 22/57 [00:02<00:04,  7.63it/s]11/28/2021 01:10:35 - INFO - __main__ -   Batch number = 23
Evaluating:   6%|▌         | 6/99 [00:00<00:14,  6.58it/s]11/28/2021 01:10:36 - INFO - __main__ -   Batch number = 7
Evaluating:  40%|████      | 23/57 [00:03<00:04,  7.63it/s]11/28/2021 01:10:36 - INFO - __main__ -   Batch number = 24
Evaluating:   7%|▋         | 7/99 [00:01<00:13,  6.64it/s]11/28/2021 01:10:36 - INFO - __main__ -   Batch number = 8
Evaluating:  42%|████▏     | 24/57 [00:03<00:04,  7.62it/s]11/28/2021 01:10:36 - INFO - __main__ -   Batch number = 25
Evaluating:   8%|▊         | 8/99 [00:01<00:13,  6.57it/s]11/28/2021 01:10:36 - INFO - __main__ -   Batch number = 9
Evaluating:  44%|████▍     | 25/57 [00:03<00:04,  7.64it/s]11/28/2021 01:10:36 - INFO - __main__ -   Batch number = 26
Evaluating:  46%|████▌     | 26/57 [00:03<00:04,  7.62it/s]11/28/2021 01:10:36 - INFO - __main__ -   Batch number = 27
Evaluating:   9%|▉         | 9/99 [00:01<00:13,  6.54it/s]11/28/2021 01:10:36 - INFO - __main__ -   Batch number = 10
Evaluating:  47%|████▋     | 27/57 [00:03<00:03,  7.61it/s]11/28/2021 01:10:36 - INFO - __main__ -   Batch number = 28
Evaluating:  10%|█         | 10/99 [00:01<00:13,  6.60it/s]11/28/2021 01:10:36 - INFO - __main__ -   Batch number = 11
Evaluating:  49%|████▉     | 28/57 [00:03<00:03,  7.62it/s]11/28/2021 01:10:36 - INFO - __main__ -   Batch number = 29
Evaluating:  11%|█         | 11/99 [00:01<00:13,  6.61it/s]11/28/2021 01:10:36 - INFO - __main__ -   Batch number = 12
Evaluating:  51%|█████     | 29/57 [00:03<00:03,  7.62it/s]11/28/2021 01:10:36 - INFO - __main__ -   Batch number = 30
Evaluating:  12%|█▏        | 12/99 [00:01<00:13,  6.65it/s]11/28/2021 01:10:36 - INFO - __main__ -   Batch number = 13
Evaluating:  53%|█████▎    | 30/57 [00:03<00:03,  7.57it/s]11/28/2021 01:10:36 - INFO - __main__ -   Batch number = 31
Evaluating:  13%|█▎        | 13/99 [00:01<00:12,  6.68it/s]11/28/2021 01:10:37 - INFO - __main__ -   Batch number = 14
Evaluating:  54%|█████▍    | 31/57 [00:04<00:03,  7.57it/s]11/28/2021 01:10:37 - INFO - __main__ -   Batch number = 32
Evaluating:  14%|█▍        | 14/99 [00:02<00:12,  6.59it/s]11/28/2021 01:10:37 - INFO - __main__ -   Batch number = 15
Evaluating:  56%|█████▌    | 32/57 [00:04<00:03,  7.57it/s]11/28/2021 01:10:37 - INFO - __main__ -   Batch number = 33
Evaluating:  15%|█▌        | 15/99 [00:02<00:12,  6.58it/s]11/28/2021 01:10:37 - INFO - __main__ -   Batch number = 16
Evaluating:  58%|█████▊    | 33/57 [00:04<00:03,  7.58it/s]11/28/2021 01:10:37 - INFO - __main__ -   Batch number = 34
Evaluating:  60%|█████▉    | 34/57 [00:04<00:03,  7.58it/s]11/28/2021 01:10:37 - INFO - __main__ -   Batch number = 35
Evaluating:  16%|█▌        | 16/99 [00:02<00:12,  6.60it/s]11/28/2021 01:10:37 - INFO - __main__ -   Batch number = 17
Evaluating:  61%|██████▏   | 35/57 [00:04<00:02,  7.59it/s]11/28/2021 01:10:37 - INFO - __main__ -   Batch number = 36
Evaluating:  17%|█▋        | 17/99 [00:02<00:12,  6.59it/s]11/28/2021 01:10:37 - INFO - __main__ -   Batch number = 18
Evaluating:  63%|██████▎   | 36/57 [00:04<00:02,  7.58it/s]11/28/2021 01:10:37 - INFO - __main__ -   Batch number = 37
Evaluating:  18%|█▊        | 18/99 [00:02<00:12,  6.57it/s]11/28/2021 01:10:37 - INFO - __main__ -   Batch number = 19
Evaluating:  65%|██████▍   | 37/57 [00:04<00:02,  7.58it/s]11/28/2021 01:10:37 - INFO - __main__ -   Batch number = 38
Evaluating:  19%|█▉        | 19/99 [00:02<00:12,  6.60it/s]11/28/2021 01:10:37 - INFO - __main__ -   Batch number = 20
Evaluating:  67%|██████▋   | 38/57 [00:05<00:02,  7.57it/s]11/28/2021 01:10:38 - INFO - __main__ -   Batch number = 39
Evaluating:  20%|██        | 20/99 [00:03<00:11,  6.63it/s]11/28/2021 01:10:38 - INFO - __main__ -   Batch number = 21
Evaluating:  68%|██████▊   | 39/57 [00:05<00:02,  7.54it/s]11/28/2021 01:10:38 - INFO - __main__ -   Batch number = 40
Evaluating:  21%|██        | 21/99 [00:03<00:11,  6.65it/s]11/28/2021 01:10:38 - INFO - __main__ -   Batch number = 22
Evaluating:  70%|███████   | 40/57 [00:05<00:02,  7.56it/s]11/28/2021 01:10:38 - INFO - __main__ -   Batch number = 41
Evaluating:  22%|██▏       | 22/99 [00:03<00:11,  6.68it/s]11/28/2021 01:10:38 - INFO - __main__ -   Batch number = 23
Evaluating:  72%|███████▏  | 41/57 [00:05<00:02,  7.57it/s]11/28/2021 01:10:38 - INFO - __main__ -   Batch number = 42
Evaluating:  23%|██▎       | 23/99 [00:03<00:11,  6.71it/s]11/28/2021 01:10:38 - INFO - __main__ -   Batch number = 24
Evaluating:  74%|███████▎  | 42/57 [00:05<00:01,  7.55it/s]11/28/2021 01:10:38 - INFO - __main__ -   Batch number = 43
Evaluating:  75%|███████▌  | 43/57 [00:05<00:01,  7.54it/s]11/28/2021 01:10:38 - INFO - __main__ -   Batch number = 44
Evaluating:  24%|██▍       | 24/99 [00:03<00:11,  6.61it/s]11/28/2021 01:10:38 - INFO - __main__ -   Batch number = 25
Evaluating:  77%|███████▋  | 44/57 [00:05<00:01,  7.56it/s]11/28/2021 01:10:38 - INFO - __main__ -   Batch number = 45
Evaluating:  25%|██▌       | 25/99 [00:03<00:11,  6.58it/s]11/28/2021 01:10:38 - INFO - __main__ -   Batch number = 26
Evaluating:  79%|███████▉  | 45/57 [00:05<00:01,  7.54it/s]11/28/2021 01:10:38 - INFO - __main__ -   Batch number = 46
Evaluating:  26%|██▋       | 26/99 [00:03<00:11,  6.56it/s]11/28/2021 01:10:39 - INFO - __main__ -   Batch number = 27
Evaluating:  81%|████████  | 46/57 [00:06<00:01,  7.54it/s]11/28/2021 01:10:39 - INFO - __main__ -   Batch number = 47
Evaluating:  27%|██▋       | 27/99 [00:04<00:11,  6.54it/s]11/28/2021 01:10:39 - INFO - __main__ -   Batch number = 28
Evaluating:  82%|████████▏ | 47/57 [00:06<00:01,  6.86it/s]11/28/2021 01:10:39 - INFO - __main__ -   Batch number = 48
Evaluating:  28%|██▊       | 28/99 [00:04<00:10,  6.61it/s]11/28/2021 01:10:39 - INFO - __main__ -   Batch number = 29
Evaluating:  84%|████████▍ | 48/57 [00:06<00:01,  7.02it/s]11/28/2021 01:10:39 - INFO - __main__ -   Batch number = 49
Evaluating:  29%|██▉       | 29/99 [00:04<00:10,  6.68it/s]11/28/2021 01:10:39 - INFO - __main__ -   Batch number = 30
Evaluating:  86%|████████▌ | 49/57 [00:06<00:01,  7.17it/s]11/28/2021 01:10:39 - INFO - __main__ -   Batch number = 50
Evaluating:  30%|███       | 30/99 [00:04<00:10,  6.69it/s]11/28/2021 01:10:39 - INFO - __main__ -   Batch number = 31
Evaluating:  88%|████████▊ | 50/57 [00:06<00:00,  7.28it/s]11/28/2021 01:10:39 - INFO - __main__ -   Batch number = 51
Evaluating:  31%|███▏      | 31/99 [00:04<00:10,  6.74it/s]11/28/2021 01:10:39 - INFO - __main__ -   Batch number = 32
Evaluating:  89%|████████▉ | 51/57 [00:06<00:00,  7.34it/s]11/28/2021 01:10:39 - INFO - __main__ -   Batch number = 52
Evaluating:  32%|███▏      | 32/99 [00:04<00:09,  6.85it/s]11/28/2021 01:10:39 - INFO - __main__ -   Batch number = 33
Evaluating:  91%|█████████ | 52/57 [00:06<00:00,  7.40it/s]11/28/2021 01:10:39 - INFO - __main__ -   Batch number = 53
Evaluating:  33%|███▎      | 33/99 [00:04<00:09,  6.88it/s]11/28/2021 01:10:40 - INFO - __main__ -   Batch number = 34
Evaluating:  93%|█████████▎| 53/57 [00:07<00:00,  7.45it/s]11/28/2021 01:10:40 - INFO - __main__ -   Batch number = 54
Evaluating:  34%|███▍      | 34/99 [00:05<00:09,  6.96it/s]11/28/2021 01:10:40 - INFO - __main__ -   Batch number = 35
Evaluating:  95%|█████████▍| 54/57 [00:07<00:00,  7.48it/s]11/28/2021 01:10:40 - INFO - __main__ -   Batch number = 55
Evaluating:  96%|█████████▋| 55/57 [00:07<00:00,  7.51it/s]11/28/2021 01:10:40 - INFO - __main__ -   Batch number = 56
Evaluating:  35%|███▌      | 35/99 [00:05<00:09,  6.95it/s]11/28/2021 01:10:40 - INFO - __main__ -   Batch number = 36
Evaluating:  98%|█████████▊| 56/57 [00:07<00:00,  7.50it/s]11/28/2021 01:10:40 - INFO - __main__ -   Batch number = 57
Evaluating:  36%|███▋      | 36/99 [00:05<00:09,  6.91it/s]11/28/2021 01:10:40 - INFO - __main__ -   Batch number = 37
Evaluating: 100%|██████████| 57/57 [00:07<00:00,  7.59it/s]Evaluating:  37%|███▋      | 37/99 [00:05<00:08,  6.89it/s]11/28/2021 01:10:40 - INFO - __main__ -   Batch number = 38
Evaluating:  38%|███▊      | 38/99 [00:05<00:09,  6.76it/s]11/28/2021 01:10:40 - INFO - __main__ -   Batch number = 39
Evaluating:  39%|███▉      | 39/99 [00:05<00:08,  6.71it/s]11/28/2021 01:10:40 - INFO - __main__ -   Batch number = 40
PyTorch version 1.10.0+cu102 available.
Evaluating:  40%|████      | 40/99 [00:06<00:08,  6.64it/s]11/28/2021 01:10:41 - INFO - __main__ -   Batch number = 41

/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:10:41 - INFO - __main__ -   ***** Evaluation result  in eu *****
11/28/2021 01:10:41 - INFO - __main__ -     f1 = 0.5693087489899361
11/28/2021 01:10:41 - INFO - __main__ -     loss = 1.3632794242156179
11/28/2021 01:10:41 - INFO - __main__ -     precision = 0.6126159359190556
11/28/2021 01:10:41 - INFO - __main__ -     recall = 0.5317202579700865
Evaluating:  41%|████▏     | 41/99 [00:06<00:08,  6.60it/s]11/28/2021 01:10:41 - INFO - __main__ -   Batch number = 42
11/28/2021 01:10:41 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='wo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:10:41 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:10:41 - INFO - __main__ -   Seed = 1
11/28/2021 01:10:41 - INFO - root -   save model
11/28/2021 01:10:41 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='wo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:10:41 - INFO - __main__ -   Loading pretrained model and tokenizer
Evaluating:  42%|████▏     | 42/99 [00:06<00:08,  6.59it/s]11/28/2021 01:10:41 - INFO - __main__ -   Batch number = 43
Evaluating:  43%|████▎     | 43/99 [00:06<00:08,  6.57it/s]11/28/2021 01:10:41 - INFO - __main__ -   Batch number = 44
Evaluating:  44%|████▍     | 44/99 [00:06<00:08,  6.59it/s]11/28/2021 01:10:41 - INFO - __main__ -   Batch number = 45
Evaluating:  45%|████▌     | 45/99 [00:06<00:08,  6.54it/s]11/28/2021 01:10:41 - INFO - __main__ -   Batch number = 46
19.30user 6.39system 0:26.64elapsed 96%CPU (0avgtext+0avgdata 3938116maxresident)k
0inputs+320outputs (0major+1469946minor)pagefaults 0swaps
Evaluating:  46%|████▋     | 46/99 [00:06<00:08,  6.54it/s]11/28/2021 01:10:42 - INFO - __main__ -   Batch number = 47
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  47%|████▋     | 47/99 [00:07<00:07,  6.51it/s]11/28/2021 01:10:42 - INFO - __main__ -   Batch number = 48
Evaluating:  48%|████▊     | 48/99 [00:07<00:07,  6.41it/s]11/28/2021 01:10:42 - INFO - __main__ -   Batch number = 49
Evaluating:  49%|████▉     | 49/99 [00:07<00:07,  6.37it/s]11/28/2021 01:10:42 - INFO - __main__ -   Batch number = 50
Evaluating:  51%|█████     | 50/99 [00:07<00:07,  6.46it/s]11/28/2021 01:10:42 - INFO - __main__ -   Batch number = 51
Evaluating:  52%|█████▏    | 51/99 [00:07<00:07,  6.53it/s]11/28/2021 01:10:42 - INFO - __main__ -   Batch number = 52
Evaluating:  53%|█████▎    | 52/99 [00:07<00:07,  6.56it/s]11/28/2021 01:10:42 - INFO - __main__ -   Batch number = 53
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  54%|█████▎    | 53/99 [00:08<00:07,  6.56it/s]11/28/2021 01:10:43 - INFO - __main__ -   Batch number = 54
Evaluating:  55%|█████▍    | 54/99 [00:08<00:06,  6.61it/s]11/28/2021 01:10:43 - INFO - __main__ -   Batch number = 55
Evaluating:  56%|█████▌    | 55/99 [00:08<00:06,  6.65it/s]11/28/2021 01:10:43 - INFO - __main__ -   Batch number = 56
Evaluating:  57%|█████▋    | 56/99 [00:08<00:06,  6.70it/s]11/28/2021 01:10:43 - INFO - __main__ -   Batch number = 57
Evaluating:  58%|█████▊    | 57/99 [00:08<00:06,  6.73it/s]11/28/2021 01:10:43 - INFO - __main__ -   Batch number = 58
loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
Evaluating:  59%|█████▊    | 58/99 [00:08<00:06,  6.73it/s]11/28/2021 01:10:43 - INFO - __main__ -   Batch number = 59
11/28/2021 01:10:43 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
Evaluating:  60%|█████▉    | 59/99 [00:08<00:05,  6.69it/s]11/28/2021 01:10:43 - INFO - __main__ -   Batch number = 60
Evaluating:  61%|██████    | 60/99 [00:09<00:05,  6.67it/s]11/28/2021 01:10:44 - INFO - __main__ -   Batch number = 61
Evaluating:  62%|██████▏   | 61/99 [00:09<00:05,  6.64it/s]11/28/2021 01:10:44 - INFO - __main__ -   Batch number = 62
Evaluating:  63%|██████▎   | 62/99 [00:09<00:05,  6.68it/s]11/28/2021 01:10:44 - INFO - __main__ -   Batch number = 63
Evaluating:  64%|██████▎   | 63/99 [00:09<00:05,  6.66it/s]11/28/2021 01:10:44 - INFO - __main__ -   Batch number = 64
Evaluating:  65%|██████▍   | 64/99 [00:09<00:05,  6.75it/s]11/28/2021 01:10:44 - INFO - __main__ -   Batch number = 65
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  66%|██████▌   | 65/99 [00:09<00:05,  6.73it/s]11/28/2021 01:10:44 - INFO - __main__ -   Batch number = 66
Evaluating:  67%|██████▋   | 66/99 [00:09<00:04,  6.82it/s]11/28/2021 01:10:45 - INFO - __main__ -   Batch number = 67
Evaluating:  68%|██████▊   | 67/99 [00:10<00:04,  6.81it/s]11/28/2021 01:10:45 - INFO - __main__ -   Batch number = 68
Evaluating:  69%|██████▊   | 68/99 [00:10<00:04,  6.83it/s]11/28/2021 01:10:45 - INFO - __main__ -   Batch number = 69
Evaluating:  70%|██████▉   | 69/99 [00:10<00:04,  6.84it/s]11/28/2021 01:10:45 - INFO - __main__ -   Batch number = 70
Evaluating:  71%|███████   | 70/99 [00:10<00:04,  6.85it/s]11/28/2021 01:10:45 - INFO - __main__ -   Batch number = 71
Evaluating:  72%|███████▏  | 71/99 [00:10<00:04,  6.55it/s]11/28/2021 01:10:45 - INFO - __main__ -   Batch number = 72
Evaluating:  73%|███████▎  | 72/99 [00:11<00:05,  5.01it/s]11/28/2021 01:10:46 - INFO - __main__ -   Batch number = 73
Evaluating:  74%|███████▎  | 73/99 [00:11<00:06,  4.28it/s]11/28/2021 01:10:46 - INFO - __main__ -   Batch number = 74
Evaluating:  75%|███████▍  | 74/99 [00:11<00:06,  3.86it/s]11/28/2021 01:10:46 - INFO - __main__ -   Batch number = 75
Evaluating:  76%|███████▌  | 75/99 [00:11<00:06,  3.62it/s]11/28/2021 01:10:47 - INFO - __main__ -   Batch number = 76
Evaluating:  77%|███████▋  | 76/99 [00:12<00:06,  3.47it/s]11/28/2021 01:10:47 - INFO - __main__ -   Batch number = 77
Evaluating:  78%|███████▊  | 77/99 [00:12<00:06,  3.38it/s]11/28/2021 01:10:47 - INFO - __main__ -   Batch number = 78
Evaluating:  79%|███████▉  | 78/99 [00:12<00:06,  3.31it/s]11/28/2021 01:10:47 - INFO - __main__ -   Batch number = 79
Evaluating:  80%|███████▉  | 79/99 [00:13<00:06,  3.25it/s]11/28/2021 01:10:48 - INFO - __main__ -   Batch number = 80
Evaluating:  81%|████████  | 80/99 [00:13<00:05,  3.22it/s]11/28/2021 01:10:48 - INFO - __main__ -   Batch number = 81
Evaluating:  82%|████████▏ | 81/99 [00:13<00:05,  3.18it/s]11/28/2021 01:10:48 - INFO - __main__ -   Batch number = 82
Evaluating:  83%|████████▎ | 82/99 [00:14<00:05,  3.15it/s]11/28/2021 01:10:49 - INFO - __main__ -   Batch number = 83
Evaluating:  84%|████████▍ | 83/99 [00:14<00:05,  3.13it/s]11/28/2021 01:10:49 - INFO - __main__ -   Batch number = 84
Evaluating:  85%|████████▍ | 84/99 [00:14<00:04,  3.12it/s]11/28/2021 01:10:49 - INFO - __main__ -   Batch number = 85
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:10:50 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:10:50 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:10:50 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
11/28/2021 01:10:50 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:10:50 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:10:50 - INFO - root -   loading lang adpater am/wiki@ukp
11/28/2021 01:10:50 - INFO - __main__ -   Adapter Languages : ['am'], Length : 1
11/28/2021 01:10:50 - INFO - __main__ -   Adapter Names ['am/wiki@ukp'], Length : 1
11/28/2021 01:10:50 - INFO - __main__ -   Language = am
11/28/2021 01:10:50 - INFO - __main__ -   Adapter Name = am/wiki@ukp
Found matching adapter at: adapters/ukp/bert-base-multilingual-cased_am_wiki_pfeiffer.json
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/am/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_am_wiki_pfeiffer.zip.
Evaluating:  86%|████████▌ | 85/99 [00:15<00:04,  3.11it/s]11/28/2021 01:10:50 - INFO - __main__ -   Batch number = 86
Evaluating:  87%|████████▋ | 86/99 [00:15<00:04,  3.10it/s]11/28/2021 01:10:50 - INFO - __main__ -   Batch number = 87
Evaluating:  88%|████████▊ | 87/99 [00:15<00:03,  3.09it/s]11/28/2021 01:10:50 - INFO - __main__ -   Batch number = 88
Evaluating:  89%|████████▉ | 88/99 [00:16<00:03,  3.09it/s]11/28/2021 01:10:51 - INFO - __main__ -   Batch number = 89
Evaluating:  90%|████████▉ | 89/99 [00:16<00:03,  3.09it/s]11/28/2021 01:10:51 - INFO - __main__ -   Batch number = 90
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/adapter_config.json
Adding adapter 'am' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/head_config.json
Evaluating:  91%|█████████ | 90/99 [00:16<00:02,  3.09it/s]11/28/2021 01:10:51 - INFO - __main__ -   Batch number = 91
Evaluating:  92%|█████████▏| 91/99 [00:17<00:02,  3.09it/s]11/28/2021 01:10:52 - INFO - __main__ -   Batch number = 92
Evaluating:  93%|█████████▎| 92/99 [00:17<00:02,  3.09it/s]11/28/2021 01:10:52 - INFO - __main__ -   Batch number = 93
Evaluating:  94%|█████████▍| 93/99 [00:17<00:01,  3.10it/s]11/28/2021 01:10:52 - INFO - __main__ -   Batch number = 94
Evaluating:  95%|█████████▍| 94/99 [00:18<00:01,  3.09it/s]11/28/2021 01:10:53 - INFO - __main__ -   Batch number = 95
Evaluating:  96%|█████████▌| 95/99 [00:18<00:01,  3.10it/s]11/28/2021 01:10:53 - INFO - __main__ -   Batch number = 96
Evaluating:  97%|█████████▋| 96/99 [00:18<00:00,  3.10it/s]11/28/2021 01:10:53 - INFO - __main__ -   Batch number = 97
Evaluating:  98%|█████████▊| 97/99 [00:19<00:00,  3.10it/s]11/28/2021 01:10:54 - INFO - __main__ -   Batch number = 98
Evaluating:  99%|█████████▉| 98/99 [00:19<00:00,  2.57it/s]11/28/2021 01:10:54 - INFO - __main__ -   Batch number = 99
Evaluating: 100%|██████████| 99/99 [00:19<00:00,  3.03it/s]Evaluating: 100%|██████████| 99/99 [00:19<00:00,  5.01it/s]11/28/2021 01:10:56 - INFO - __main__ -   Language wo, split test does not exist

/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:10:56 - INFO - __main__ -   ***** Evaluation result  in es *****
11/28/2021 01:10:56 - INFO - __main__ -     f1 = 0.8805728525607989
11/28/2021 01:10:56 - INFO - __main__ -     loss = 0.37986416467512496
11/28/2021 01:10:56 - INFO - __main__ -     precision = 0.8868371559072812
11/28/2021 01:10:56 - INFO - __main__ -     recall = 0.8743964261201488
14.05user 5.11system 0:18.18elapsed 105%CPU (0avgtext+0avgdata 3938892maxresident)k
0inputs+32outputs (0major+1324638minor)pagefaults 0swaps
33.19user 11.55system 0:43.98elapsed 101%CPU (0avgtext+0avgdata 3945592maxresident)k
0inputs+952outputs (0major+1528459minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:10:59 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='wo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:10:59 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:10:59 - INFO - __main__ -   Seed = 2
11/28/2021 01:10:59 - INFO - root -   save model
11/28/2021 01:10:59 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='wo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:10:59 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:11:02 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:11:08 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:11:08 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:11:08 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
11/28/2021 01:11:08 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:11:08 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:11:08 - INFO - root -   loading lang adpater am/wiki@ukp
11/28/2021 01:11:08 - INFO - __main__ -   Adapter Languages : ['am'], Length : 1
11/28/2021 01:11:08 - INFO - __main__ -   Adapter Names ['am/wiki@ukp'], Length : 1
11/28/2021 01:11:08 - INFO - __main__ -   Language = am
11/28/2021 01:11:08 - INFO - __main__ -   Adapter Name = am/wiki@ukp
Found matching adapter at: adapters/ukp/bert-base-multilingual-cased_am_wiki_pfeiffer.json
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/am/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_am_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/adapter_config.json
Adding adapter 'am' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/head_config.json
11/28/2021 01:11:15 - INFO - __main__ -   Language wo, split test does not exist
13.94user 5.37system 0:19.09elapsed 101%CPU (0avgtext+0avgdata 3932164maxresident)k
0inputs+56outputs (0major+1472006minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:11:18 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='wo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:11:18 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:11:18 - INFO - __main__ -   Seed = 3
11/28/2021 01:11:18 - INFO - root -   save model
11/28/2021 01:11:18 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='wo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:11:18 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:11:21 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:11:27 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:11:27 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:11:27 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
11/28/2021 01:11:27 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:11:27 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:11:27 - INFO - root -   loading lang adpater am/wiki@ukp
11/28/2021 01:11:27 - INFO - __main__ -   Adapter Languages : ['am'], Length : 1
11/28/2021 01:11:27 - INFO - __main__ -   Adapter Names ['am/wiki@ukp'], Length : 1
11/28/2021 01:11:27 - INFO - __main__ -   Language = am
11/28/2021 01:11:27 - INFO - __main__ -   Adapter Name = am/wiki@ukp
Found matching adapter at: adapters/ukp/bert-base-multilingual-cased_am_wiki_pfeiffer.json
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/am/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_am_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/adapter_config.json
Adding adapter 'am' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/head_config.json
11/28/2021 01:11:35 - INFO - __main__ -   Language wo, split test does not exist
14.51user 7.63system 0:19.72elapsed 112%CPU (0avgtext+0avgdata 3939672maxresident)k
0inputs+40outputs (0major+1513129minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:12:53 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bxr', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:12:53 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:12:53 - INFO - __main__ -   Seed = 1
11/28/2021 01:12:53 - INFO - root -   save model
11/28/2021 01:12:53 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bxr', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:12:53 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:12:56 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:13:02 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:13:02 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:13:02 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
11/28/2021 01:13:02 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:13:02 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:13:02 - INFO - root -   loading lang adpater am/wiki@ukp
11/28/2021 01:13:02 - INFO - __main__ -   Adapter Languages : ['am'], Length : 1
11/28/2021 01:13:02 - INFO - __main__ -   Adapter Names ['am/wiki@ukp'], Length : 1
11/28/2021 01:13:02 - INFO - __main__ -   Language = am
11/28/2021 01:13:02 - INFO - __main__ -   Adapter Name = am/wiki@ukp
Found matching adapter at: adapters/ukp/bert-base-multilingual-cased_am_wiki_pfeiffer.json
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/am/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_am_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/adapter_config.json
Adding adapter 'am' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/head_config.json
11/28/2021 01:13:08 - INFO - __main__ -   Language bxr, split test does not exist
13.50user 5.08system 0:18.17elapsed 102%CPU (0avgtext+0avgdata 3933272maxresident)k
0inputs+56outputs (0major+1435999minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:13:11 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bxr', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:13:11 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:13:11 - INFO - __main__ -   Seed = 2
11/28/2021 01:13:11 - INFO - root -   save model
11/28/2021 01:13:11 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bxr', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:13:11 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:13:14 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:13:20 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:13:20 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:13:20 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
11/28/2021 01:13:20 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:13:20 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:13:20 - INFO - root -   loading lang adpater am/wiki@ukp
11/28/2021 01:13:20 - INFO - __main__ -   Adapter Languages : ['am'], Length : 1
11/28/2021 01:13:20 - INFO - __main__ -   Adapter Names ['am/wiki@ukp'], Length : 1
11/28/2021 01:13:20 - INFO - __main__ -   Language = am
11/28/2021 01:13:20 - INFO - __main__ -   Adapter Name = am/wiki@ukp
Found matching adapter at: adapters/ukp/bert-base-multilingual-cased_am_wiki_pfeiffer.json
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/am/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_am_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/adapter_config.json
Adding adapter 'am' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/head_config.json
11/28/2021 01:13:27 - INFO - __main__ -   Language bxr, split test does not exist
14.95user 6.83system 0:18.19elapsed 119%CPU (0avgtext+0avgdata 3936904maxresident)k
0inputs+56outputs (0major+1517377minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:13:29 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bxr', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:13:29 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:13:29 - INFO - __main__ -   Seed = 3
11/28/2021 01:13:29 - INFO - root -   save model
11/28/2021 01:13:29 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bxr', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:13:29 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:13:32 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:13:38 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:13:38 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:13:38 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
11/28/2021 01:13:38 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:13:38 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:13:38 - INFO - root -   loading lang adpater am/wiki@ukp
11/28/2021 01:13:38 - INFO - __main__ -   Adapter Languages : ['am'], Length : 1
11/28/2021 01:13:38 - INFO - __main__ -   Adapter Names ['am/wiki@ukp'], Length : 1
11/28/2021 01:13:38 - INFO - __main__ -   Language = am
11/28/2021 01:13:38 - INFO - __main__ -   Adapter Name = am/wiki@ukp
Found matching adapter at: adapters/ukp/bert-base-multilingual-cased_am_wiki_pfeiffer.json
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/am/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_am_wiki_pfeiffer.zip.
PyTorch version 1.10.0+cu102 available.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/adapter_config.json
Adding adapter 'am' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/head_config.json
11/28/2021 01:13:40 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bxr', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:13:40 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:13:40 - INFO - __main__ -   Seed = 1
11/28/2021 01:13:40 - INFO - root -   save model
11/28/2021 01:13:40 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bxr', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:13:40 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:13:43 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
11/28/2021 01:13:46 - INFO - __main__ -   Language bxr, split test does not exist
14.39user 5.25system 0:19.17elapsed 102%CPU (0avgtext+0avgdata 3939732maxresident)k
0inputs+32outputs (0major+1557889minor)pagefaults 0swaps
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:13:49 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:13:49 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:13:49 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
11/28/2021 01:13:49 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:13:49 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:13:49 - INFO - root -   loading lang adpater am/wiki@ukp
11/28/2021 01:13:49 - INFO - __main__ -   Adapter Languages : ['am'], Length : 1
11/28/2021 01:13:49 - INFO - __main__ -   Adapter Names ['am/wiki@ukp'], Length : 1
11/28/2021 01:13:49 - INFO - __main__ -   Language = am
11/28/2021 01:13:49 - INFO - __main__ -   Adapter Name = am/wiki@ukp
Found matching adapter at: adapters/ukp/bert-base-multilingual-cased_am_wiki_pfeiffer.json
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/am/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_am_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/adapter_config.json
Adding adapter 'am' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/head_config.json
11/28/2021 01:13:56 - INFO - __main__ -   Language bxr, split test does not exist
14.02user 5.36system 0:18.55elapsed 104%CPU (0avgtext+0avgdata 3934536maxresident)k
0inputs+48outputs (0major+1221613minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:13:59 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bxr', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:13:59 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:13:59 - INFO - __main__ -   Seed = 2
11/28/2021 01:13:59 - INFO - root -   save model
11/28/2021 01:13:59 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bxr', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:13:59 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:14:01 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:14:08 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:14:08 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:14:08 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
11/28/2021 01:14:08 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:14:08 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:14:08 - INFO - root -   loading lang adpater am/wiki@ukp
11/28/2021 01:14:08 - INFO - __main__ -   Adapter Languages : ['am'], Length : 1
11/28/2021 01:14:08 - INFO - __main__ -   Adapter Names ['am/wiki@ukp'], Length : 1
11/28/2021 01:14:08 - INFO - __main__ -   Language = am
11/28/2021 01:14:08 - INFO - __main__ -   Adapter Name = am/wiki@ukp
Found matching adapter at: adapters/ukp/bert-base-multilingual-cased_am_wiki_pfeiffer.json
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/am/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_am_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/adapter_config.json
Adding adapter 'am' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/head_config.json
11/28/2021 01:14:17 - INFO - __main__ -   Language bxr, split test does not exist
15.58user 5.95system 0:20.74elapsed 103%CPU (0avgtext+0avgdata 3937636maxresident)k
0inputs+56outputs (0major+1222246minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:14:19 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bxr', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:14:19 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:14:19 - INFO - __main__ -   Seed = 3
11/28/2021 01:14:19 - INFO - root -   save model
11/28/2021 01:14:19 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bxr', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:14:19 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:14:22 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:14:28 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:14:28 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:14:28 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
11/28/2021 01:14:28 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:14:28 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:14:28 - INFO - root -   loading lang adpater am/wiki@ukp
11/28/2021 01:14:28 - INFO - __main__ -   Adapter Languages : ['am'], Length : 1
11/28/2021 01:14:28 - INFO - __main__ -   Adapter Names ['am/wiki@ukp'], Length : 1
11/28/2021 01:14:28 - INFO - __main__ -   Language = am
11/28/2021 01:14:28 - INFO - __main__ -   Adapter Name = am/wiki@ukp
Found matching adapter at: adapters/ukp/bert-base-multilingual-cased_am_wiki_pfeiffer.json
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/am/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_am_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/adapter_config.json
Adding adapter 'am' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/head_config.json
11/28/2021 01:14:35 - INFO - __main__ -   Language bxr, split test does not exist
13.03user 5.37system 0:18.50elapsed 99%CPU (0avgtext+0avgdata 3944916maxresident)k
0inputs+48outputs (0major+1464379minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:20:24 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='is', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:20:24 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:20:24 - INFO - __main__ -   Seed = 1
11/28/2021 01:20:24 - INFO - root -   save model
11/28/2021 01:20:24 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='is', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:20:24 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:20:26 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:20:33 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:20:33 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:20:33 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
11/28/2021 01:20:33 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:20:33 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:20:33 - INFO - root -   loading lang adpater am/wiki@ukp
11/28/2021 01:20:33 - INFO - __main__ -   Adapter Languages : ['am'], Length : 1
11/28/2021 01:20:33 - INFO - __main__ -   Adapter Names ['am/wiki@ukp'], Length : 1
11/28/2021 01:20:33 - INFO - __main__ -   Language = am
11/28/2021 01:20:33 - INFO - __main__ -   Adapter Name = am/wiki@ukp
Found matching adapter at: adapters/ukp/bert-base-multilingual-cased_am_wiki_pfeiffer.json
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/am/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_am_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/adapter_config.json
Adding adapter 'am' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/head_config.json
11/28/2021 01:20:40 - INFO - __main__ -   Language adapter for is not found, using am instead
11/28/2021 01:20:40 - INFO - __main__ -   Set active language adapter to am
11/28/2021 01:20:40 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:20:40 - INFO - __main__ -   Adapter Languages = ['am']
11/28/2021 01:20:40 - INFO - __main__ -   all languages = is
11/28/2021 01:20:40 - INFO - __main__ -   Creating features from dataset file at /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/is/test.bert-base-multilingual-cased in language is
11/28/2021 01:20:40 - INFO - utils_tag -   lang_id=0, lang=is, lang2id=None
11/28/2021 01:20:40 - INFO - utils_tag -   Writing example 0 of 6401
11/28/2021 01:20:40 - INFO - utils_tag -   *** Example ***
11/28/2021 01:20:40 - INFO - utils_tag -   guid: is-1
11/28/2021 01:20:40 - INFO - utils_tag -   tokens: [CLS] Í flest ##um l ##ön ##dum set ##ja menn á b ##æk ##ur anna ##ð tv ##egg ##ja þann fr ##ó ##ð ##lei ##k er þar innan lands he ##fir g ##jö ##rst [UNK] eða þann annan er min ##nis ##am ##lega ##stu ##r þ ##yki ##r þó að annars sta ##ðar hafi held ##ur g ##jö ##rst [UNK] eða l ##ög sí ##n set ##ja menn á b ##æk ##ur hver þ ##jó ##ð á sí ##na tung ##u . [SEP]
11/28/2021 01:20:40 - INFO - utils_tag -   input_ids: 101 238 91762 10465 180 15248 27983 11847 10320 39974 255 170 36850 10546 39873 12332 19767 91600 10320 53849 12127 10443 12332 36777 10174 10163 23822 24318 31361 10261 61644 175 37735 21328 100 25604 53849 45569 10163 13484 12597 11008 37263 19987 10129 284 35696 10129 62760 12071 85120 16527 39322 67447 11991 10546 175 37735 21328 100 25604 180 59424 18419 10115 11847 10320 39974 255 170 36850 10546 27855 284 29366 12332 255 18419 10219 94065 10138 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:20:40 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:20:40 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:20:40 - INFO - utils_tag -   label_ids: -100 2 6 -100 8 -100 -100 16 -100 8 2 8 -100 -100 11 -100 9 -100 -100 6 8 -100 -100 -100 -100 14 3 2 8 4 -100 16 -100 -100 13 5 6 11 14 1 -100 -100 -100 -100 -100 16 -100 -100 2 14 11 8 -100 4 3 -100 16 -100 -100 13 5 8 -100 11 -100 16 -100 8 2 8 -100 -100 6 8 -100 -100 2 11 -100 8 -100 13 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
11/28/2021 01:20:40 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
11/28/2021 01:20:40 - INFO - utils_tag -   *** Example ***
11/28/2021 01:20:40 - INFO - utils_tag -   guid: is-2
11/28/2021 01:20:40 - INFO - utils_tag -   tokens: [CLS] En af því að tung ##urn ##ar eru ó ##lí ##kar hver anna ##rri , þ ##ær þegar er úr ein ##ni og hinn ##i sö ##mu tung ##u hafa gen ##gist eða grein ##st , þá þar ##f ó ##lí ##ka sta ##fi í að hafa en ei ##gi hin ##a sö ##mu alla í ö ##llum , sem ei ##gi rit ##a gr ##ikk ##ir latín ##ust ##ö ##fum gir ##sku ##na og ei ##gi latín ##umen ##n gir ##sku ##m st ##ö ##fum latín ##u , né enn held ##ur heb ##res ##kir menn heb ##res ##kun ##a hvor ##ki gr ##ís ##kum st ##ö ##fum né latín ##u , held ##ur rit ##ar sínum st ##ö ##fum hver þ ##jó ##ð [SEP]
11/28/2021 01:20:40 - INFO - utils_tag -   input_ids: 101 10243 10452 25272 12071 94065 63158 10354 18098 273 30006 15190 27855 39873 24874 117 284 19210 44241 10163 29351 10290 10342 10156 62736 10116 100721 11717 94065 10138 34320 15331 77362 25604 98885 10562 117 35193 23822 10575 273 30006 10371 16527 14403 267 12071 34320 10110 10805 11210 19911 10113 100721 11717 10512 267 276 72666 117 11531 10805 11210 80140 10113 30518 64907 10835 40005 19265 14902 101851 46264 14836 10219 10156 10805 11210 40005 33380 10115 46264 14836 10147 28780 14902 101851 40005 10138 117 12757 17776 11991 10546 79807 11234 46994 39974 79807 11234 49581 10113 13099 10506 30518 20096 36811 28780 14902 101851 12757 40005 10138 117 11991 10546 80140 10354 93472 28780 14902 101851 27855 284 29366 12332 102 0
11/28/2021 01:20:40 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0
11/28/2021 01:20:40 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:20:40 - INFO - utils_tag -   label_ids: -100 5 2 11 14 8 -100 -100 4 1 -100 -100 6 11 -100 13 11 -100 3 14 2 6 -100 5 6 -100 1 -100 8 -100 4 16 -100 5 16 -100 13 3 16 -100 1 -100 -100 8 -100 2 10 4 5 3 -100 6 -100 1 -100 6 2 6 -100 13 14 3 -100 16 -100 12 -100 -100 8 -100 -100 -100 8 -100 -100 5 3 -100 8 -100 -100 1 -100 -100 8 -100 -100 8 -100 13 5 3 3 -100 1 -100 -100 8 8 -100 -100 -100 5 -100 1 -100 -100 8 -100 -100 5 8 -100 13 5 -100 16 -100 11 8 -100 -100 6 8 -100 -100 -100 -100
11/28/2021 01:20:40 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
11/28/2021 01:20:40 - INFO - utils_tag -   *** Example ***
11/28/2021 01:20:40 - INFO - utils_tag -   guid: is-3
11/28/2021 01:20:40 - INFO - utils_tag -   tokens: [CLS] sí ##na tung ##u . [SEP]
11/28/2021 01:20:40 - INFO - utils_tag -   input_ids: 101 18419 10219 94065 10138 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:20:40 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:20:40 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:20:40 - INFO - utils_tag -   label_ids: -100 11 -100 8 -100 13 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
11/28/2021 01:20:40 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
11/28/2021 01:20:40 - INFO - utils_tag -   *** Example ***
11/28/2021 01:20:40 - INFO - utils_tag -   guid: is-4
11/28/2021 01:20:40 - INFO - utils_tag -   tokens: [CLS] H ##vere ##ga tung ##u er ma ##ður skal rit ##a anna ##rra ##r tung ##u st ##ö ##fum , þá ver ##ður sum ##ra sta ##fa vant af því að hver tung ##a he ##fir hl ##jó ##ð þau er ei ##gi finn ##ast í anna ##rri . [SEP]
11/28/2021 01:20:40 - INFO - utils_tag -   input_ids: 101 145 29282 10483 94065 10138 10163 10824 17225 17183 80140 10113 39873 21084 10129 94065 10138 28780 14902 101851 117 35193 16719 17225 28439 10288 16527 13369 23266 10452 25272 12071 27855 94065 10113 10261 61644 53264 29366 12332 76500 10163 10805 11210 75495 15171 267 39873 24874 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:20:40 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:20:40 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:20:40 - INFO - utils_tag -   label_ids: -100 6 -100 -100 8 -100 14 8 -100 4 16 -100 11 -100 -100 8 -100 8 -100 -100 13 3 16 -100 6 -100 8 -100 1 2 11 14 6 8 -100 4 -100 8 -100 -100 6 14 3 -100 16 -100 2 11 -100 13 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
11/28/2021 01:20:40 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
11/28/2021 01:20:40 - INFO - utils_tag -   *** Example ***
11/28/2021 01:20:40 - INFO - utils_tag -   guid: is-5
11/28/2021 01:20:40 - INFO - utils_tag -   tokens: [CLS] Sv ##o gang ##a og sum ##ir sta ##fir af því að ei ##gi finns ##t það hl ##jó ##ð í tung ##unn ##i sem sta ##fir ##nir hafa , þeir er af gang ##a . [SEP]
11/28/2021 01:20:40 - INFO - utils_tag -   input_ids: 101 53068 10133 16330 10113 10156 28439 10835 16527 61644 10452 25272 12071 10805 11210 10370 10123 23053 53264 29366 12332 267 94065 104939 10116 11531 16527 61644 23907 34320 117 56055 10163 10452 16330 10113 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:20:40 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:20:40 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:20:40 - INFO - utils_tag -   label_ids: -100 3 -100 16 -100 3 6 -100 8 -100 2 11 14 3 -100 16 -100 6 8 -100 -100 2 8 -100 -100 14 8 -100 -100 4 13 11 14 2 16 -100 13 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
11/28/2021 01:20:40 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
11/28/2021 01:20:49 - INFO - __main__ -   Saving features into cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_is_bert-base-multilingual-cased_128, len(features)=6401
11/28/2021 01:20:51 - INFO - __main__ -   ***** Running evaluation  in is *****
11/28/2021 01:20:51 - INFO - __main__ -     Num examples = 6401
11/28/2021 01:20:51 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/201 [00:00<?, ?it/s]11/28/2021 01:20:51 - INFO - __main__ -   Batch number = 1
Evaluating:   0%|          | 1/201 [00:00<01:04,  3.09it/s]11/28/2021 01:20:51 - INFO - __main__ -   Batch number = 2
Evaluating:   1%|          | 2/201 [00:00<00:58,  3.41it/s]11/28/2021 01:20:51 - INFO - __main__ -   Batch number = 3
Evaluating:   1%|▏         | 3/201 [00:00<00:56,  3.50it/s]11/28/2021 01:20:52 - INFO - __main__ -   Batch number = 4
Evaluating:   2%|▏         | 4/201 [00:01<00:55,  3.53it/s]11/28/2021 01:20:52 - INFO - __main__ -   Batch number = 5
Evaluating:   2%|▏         | 5/201 [00:01<00:55,  3.50it/s]11/28/2021 01:20:52 - INFO - __main__ -   Batch number = 6
Evaluating:   3%|▎         | 6/201 [00:01<00:56,  3.46it/s]11/28/2021 01:20:52 - INFO - __main__ -   Batch number = 7
Evaluating:   3%|▎         | 7/201 [00:02<00:56,  3.45it/s]11/28/2021 01:20:53 - INFO - __main__ -   Batch number = 8
Evaluating:   4%|▍         | 8/201 [00:02<00:55,  3.49it/s]11/28/2021 01:20:53 - INFO - __main__ -   Batch number = 9
Evaluating:   4%|▍         | 9/201 [00:02<00:55,  3.45it/s]11/28/2021 01:20:53 - INFO - __main__ -   Batch number = 10
Evaluating:   5%|▍         | 10/201 [00:02<00:55,  3.42it/s]11/28/2021 01:20:54 - INFO - __main__ -   Batch number = 11
Evaluating:   5%|▌         | 11/201 [00:03<00:56,  3.35it/s]11/28/2021 01:20:54 - INFO - __main__ -   Batch number = 12
Evaluating:   6%|▌         | 12/201 [00:03<00:57,  3.28it/s]11/28/2021 01:20:54 - INFO - __main__ -   Batch number = 13
Evaluating:   6%|▋         | 13/201 [00:03<00:57,  3.28it/s]11/28/2021 01:20:55 - INFO - __main__ -   Batch number = 14
Evaluating:   7%|▋         | 14/201 [00:04<00:56,  3.29it/s]11/28/2021 01:20:55 - INFO - __main__ -   Batch number = 15
Evaluating:   7%|▋         | 15/201 [00:04<00:51,  3.59it/s]11/28/2021 01:20:55 - INFO - __main__ -   Batch number = 16
Evaluating:   8%|▊         | 16/201 [00:04<00:44,  4.12it/s]11/28/2021 01:20:55 - INFO - __main__ -   Batch number = 17
Evaluating:   8%|▊         | 17/201 [00:04<00:39,  4.69it/s]11/28/2021 01:20:55 - INFO - __main__ -   Batch number = 18
Evaluating:   9%|▉         | 18/201 [00:04<00:35,  5.17it/s]11/28/2021 01:20:56 - INFO - __main__ -   Batch number = 19
Evaluating:   9%|▉         | 19/201 [00:04<00:32,  5.57it/s]11/28/2021 01:20:56 - INFO - __main__ -   Batch number = 20
Evaluating:  10%|▉         | 20/201 [00:05<00:30,  5.86it/s]11/28/2021 01:20:56 - INFO - __main__ -   Batch number = 21
Evaluating:  10%|█         | 21/201 [00:05<00:29,  6.12it/s]11/28/2021 01:20:56 - INFO - __main__ -   Batch number = 22
Evaluating:  11%|█         | 22/201 [00:05<00:28,  6.32it/s]11/28/2021 01:20:56 - INFO - __main__ -   Batch number = 23
Evaluating:  11%|█▏        | 23/201 [00:05<00:27,  6.46it/s]11/28/2021 01:20:56 - INFO - __main__ -   Batch number = 24
Evaluating:  12%|█▏        | 24/201 [00:05<00:26,  6.56it/s]11/28/2021 01:20:56 - INFO - __main__ -   Batch number = 25
Evaluating:  12%|█▏        | 25/201 [00:05<00:26,  6.63it/s]11/28/2021 01:20:57 - INFO - __main__ -   Batch number = 26
Evaluating:  13%|█▎        | 26/201 [00:05<00:26,  6.73it/s]11/28/2021 01:20:57 - INFO - __main__ -   Batch number = 27
Evaluating:  13%|█▎        | 27/201 [00:06<00:25,  6.79it/s]11/28/2021 01:20:57 - INFO - __main__ -   Batch number = 28
Evaluating:  14%|█▍        | 28/201 [00:06<00:25,  6.80it/s]11/28/2021 01:20:57 - INFO - __main__ -   Batch number = 29
Evaluating:  14%|█▍        | 29/201 [00:06<00:25,  6.86it/s]11/28/2021 01:20:57 - INFO - __main__ -   Batch number = 30
Evaluating:  15%|█▍        | 30/201 [00:06<00:24,  6.94it/s]11/28/2021 01:20:57 - INFO - __main__ -   Batch number = 31
Evaluating:  15%|█▌        | 31/201 [00:06<00:24,  6.98it/s]11/28/2021 01:20:57 - INFO - __main__ -   Batch number = 32
Evaluating:  16%|█▌        | 32/201 [00:06<00:23,  7.05it/s]11/28/2021 01:20:58 - INFO - __main__ -   Batch number = 33
Evaluating:  16%|█▋        | 33/201 [00:06<00:23,  7.00it/s]11/28/2021 01:20:58 - INFO - __main__ -   Batch number = 34
Evaluating:  17%|█▋        | 34/201 [00:07<00:24,  6.92it/s]11/28/2021 01:20:58 - INFO - __main__ -   Batch number = 35
Evaluating:  17%|█▋        | 35/201 [00:07<00:24,  6.89it/s]11/28/2021 01:20:58 - INFO - __main__ -   Batch number = 36
Evaluating:  18%|█▊        | 36/201 [00:07<00:24,  6.83it/s]11/28/2021 01:20:58 - INFO - __main__ -   Batch number = 37
Evaluating:  18%|█▊        | 37/201 [00:07<00:24,  6.73it/s]11/28/2021 01:20:58 - INFO - __main__ -   Batch number = 38
Evaluating:  19%|█▉        | 38/201 [00:07<00:24,  6.76it/s]11/28/2021 01:20:58 - INFO - __main__ -   Batch number = 39
Evaluating:  19%|█▉        | 39/201 [00:07<00:23,  6.77it/s]11/28/2021 01:20:59 - INFO - __main__ -   Batch number = 40
Evaluating:  20%|█▉        | 40/201 [00:08<00:24,  6.69it/s]11/28/2021 01:20:59 - INFO - __main__ -   Batch number = 41
Evaluating:  20%|██        | 41/201 [00:08<00:26,  6.00it/s]11/28/2021 01:20:59 - INFO - __main__ -   Batch number = 42
Evaluating:  21%|██        | 42/201 [00:08<00:26,  6.07it/s]11/28/2021 01:20:59 - INFO - __main__ -   Batch number = 43
Evaluating:  21%|██▏       | 43/201 [00:08<00:25,  6.10it/s]11/28/2021 01:20:59 - INFO - __main__ -   Batch number = 44
Evaluating:  22%|██▏       | 44/201 [00:08<00:25,  6.15it/s]11/28/2021 01:20:59 - INFO - __main__ -   Batch number = 45
Evaluating:  22%|██▏       | 45/201 [00:08<00:25,  6.12it/s]11/28/2021 01:21:00 - INFO - __main__ -   Batch number = 46
Evaluating:  23%|██▎       | 46/201 [00:09<00:25,  6.04it/s]11/28/2021 01:21:00 - INFO - __main__ -   Batch number = 47
Evaluating:  23%|██▎       | 47/201 [00:09<00:24,  6.21it/s]11/28/2021 01:21:00 - INFO - __main__ -   Batch number = 48
Evaluating:  24%|██▍       | 48/201 [00:09<00:24,  6.28it/s]11/28/2021 01:21:00 - INFO - __main__ -   Batch number = 49
Evaluating:  24%|██▍       | 49/201 [00:09<00:23,  6.36it/s]11/28/2021 01:21:00 - INFO - __main__ -   Batch number = 50
Evaluating:  25%|██▍       | 50/201 [00:09<00:23,  6.40it/s]11/28/2021 01:21:00 - INFO - __main__ -   Batch number = 51
Evaluating:  25%|██▌       | 51/201 [00:09<00:23,  6.45it/s]11/28/2021 01:21:01 - INFO - __main__ -   Batch number = 52
Evaluating:  26%|██▌       | 52/201 [00:09<00:23,  6.41it/s]11/28/2021 01:21:01 - INFO - __main__ -   Batch number = 53
Evaluating:  26%|██▋       | 53/201 [00:10<00:22,  6.51it/s]11/28/2021 01:21:01 - INFO - __main__ -   Batch number = 54
Evaluating:  27%|██▋       | 54/201 [00:10<00:22,  6.44it/s]11/28/2021 01:21:01 - INFO - __main__ -   Batch number = 55
Evaluating:  27%|██▋       | 55/201 [00:10<00:22,  6.49it/s]11/28/2021 01:21:01 - INFO - __main__ -   Batch number = 56
Evaluating:  28%|██▊       | 56/201 [00:10<00:22,  6.45it/s]11/28/2021 01:21:01 - INFO - __main__ -   Batch number = 57
Evaluating:  28%|██▊       | 57/201 [00:10<00:22,  6.45it/s]11/28/2021 01:21:01 - INFO - __main__ -   Batch number = 58
Evaluating:  29%|██▉       | 58/201 [00:10<00:22,  6.33it/s]11/28/2021 01:21:02 - INFO - __main__ -   Batch number = 59
Evaluating:  29%|██▉       | 59/201 [00:11<00:22,  6.28it/s]11/28/2021 01:21:02 - INFO - __main__ -   Batch number = 60
Evaluating:  30%|██▉       | 60/201 [00:11<00:22,  6.33it/s]11/28/2021 01:21:02 - INFO - __main__ -   Batch number = 61
Evaluating:  30%|███       | 61/201 [00:11<00:22,  6.33it/s]11/28/2021 01:21:02 - INFO - __main__ -   Batch number = 62
Evaluating:  31%|███       | 62/201 [00:11<00:21,  6.38it/s]11/28/2021 01:21:02 - INFO - __main__ -   Batch number = 63
Evaluating:  31%|███▏      | 63/201 [00:11<00:21,  6.31it/s]11/28/2021 01:21:02 - INFO - __main__ -   Batch number = 64
Evaluating:  32%|███▏      | 64/201 [00:11<00:21,  6.36it/s]11/28/2021 01:21:03 - INFO - __main__ -   Batch number = 65
Evaluating:  32%|███▏      | 65/201 [00:12<00:21,  6.29it/s]11/28/2021 01:21:03 - INFO - __main__ -   Batch number = 66
Evaluating:  33%|███▎      | 66/201 [00:12<00:21,  6.28it/s]11/28/2021 01:21:03 - INFO - __main__ -   Batch number = 67
Evaluating:  33%|███▎      | 67/201 [00:12<00:21,  6.27it/s]11/28/2021 01:21:03 - INFO - __main__ -   Batch number = 68
Evaluating:  34%|███▍      | 68/201 [00:12<00:20,  6.34it/s]11/28/2021 01:21:03 - INFO - __main__ -   Batch number = 69
Evaluating:  34%|███▍      | 69/201 [00:12<00:20,  6.38it/s]11/28/2021 01:21:03 - INFO - __main__ -   Batch number = 70
Evaluating:  35%|███▍      | 70/201 [00:12<00:20,  6.46it/s]11/28/2021 01:21:03 - INFO - __main__ -   Batch number = 71
Evaluating:  35%|███▌      | 71/201 [00:12<00:19,  6.53it/s]11/28/2021 01:21:04 - INFO - __main__ -   Batch number = 72
Evaluating:  36%|███▌      | 72/201 [00:13<00:19,  6.51it/s]11/28/2021 01:21:04 - INFO - __main__ -   Batch number = 73
Evaluating:  36%|███▋      | 73/201 [00:13<00:22,  5.59it/s]11/28/2021 01:21:04 - INFO - __main__ -   Batch number = 74
Evaluating:  37%|███▋      | 74/201 [00:13<00:21,  5.92it/s]11/28/2021 01:21:04 - INFO - __main__ -   Batch number = 75
Evaluating:  37%|███▋      | 75/201 [00:13<00:20,  6.08it/s]11/28/2021 01:21:04 - INFO - __main__ -   Batch number = 76
Evaluating:  38%|███▊      | 76/201 [00:13<00:19,  6.26it/s]11/28/2021 01:21:04 - INFO - __main__ -   Batch number = 77
Evaluating:  38%|███▊      | 77/201 [00:13<00:19,  6.38it/s]11/28/2021 01:21:05 - INFO - __main__ -   Batch number = 78
Evaluating:  39%|███▉      | 78/201 [00:14<00:19,  6.39it/s]11/28/2021 01:21:05 - INFO - __main__ -   Batch number = 79
Evaluating:  39%|███▉      | 79/201 [00:14<00:18,  6.50it/s]11/28/2021 01:21:05 - INFO - __main__ -   Batch number = 80
Evaluating:  40%|███▉      | 80/201 [00:14<00:18,  6.57it/s]11/28/2021 01:21:05 - INFO - __main__ -   Batch number = 81
Evaluating:  40%|████      | 81/201 [00:14<00:17,  6.70it/s]11/28/2021 01:21:05 - INFO - __main__ -   Batch number = 82
Evaluating:  41%|████      | 82/201 [00:14<00:17,  6.82it/s]11/28/2021 01:21:05 - INFO - __main__ -   Batch number = 83
Evaluating:  41%|████▏     | 83/201 [00:14<00:16,  6.95it/s]11/28/2021 01:21:06 - INFO - __main__ -   Batch number = 84
Evaluating:  42%|████▏     | 84/201 [00:14<00:16,  7.03it/s]11/28/2021 01:21:06 - INFO - __main__ -   Batch number = 85
Evaluating:  42%|████▏     | 85/201 [00:15<00:16,  6.97it/s]11/28/2021 01:21:06 - INFO - __main__ -   Batch number = 86
Evaluating:  43%|████▎     | 86/201 [00:15<00:16,  6.99it/s]11/28/2021 01:21:06 - INFO - __main__ -   Batch number = 87
Evaluating:  43%|████▎     | 87/201 [00:15<00:16,  6.92it/s]11/28/2021 01:21:06 - INFO - __main__ -   Batch number = 88
Evaluating:  44%|████▍     | 88/201 [00:15<00:16,  6.96it/s]11/28/2021 01:21:06 - INFO - __main__ -   Batch number = 89
Evaluating:  44%|████▍     | 89/201 [00:15<00:16,  6.98it/s]11/28/2021 01:21:06 - INFO - __main__ -   Batch number = 90
Evaluating:  45%|████▍     | 90/201 [00:15<00:16,  6.92it/s]11/28/2021 01:21:07 - INFO - __main__ -   Batch number = 91
Evaluating:  45%|████▌     | 91/201 [00:15<00:16,  6.83it/s]11/28/2021 01:21:07 - INFO - __main__ -   Batch number = 92
Evaluating:  46%|████▌     | 92/201 [00:16<00:15,  6.82it/s]11/28/2021 01:21:07 - INFO - __main__ -   Batch number = 93
Evaluating:  46%|████▋     | 93/201 [00:16<00:15,  6.86it/s]11/28/2021 01:21:07 - INFO - __main__ -   Batch number = 94
Evaluating:  47%|████▋     | 94/201 [00:16<00:15,  6.87it/s]11/28/2021 01:21:07 - INFO - __main__ -   Batch number = 95
Evaluating:  47%|████▋     | 95/201 [00:16<00:15,  6.89it/s]11/28/2021 01:21:07 - INFO - __main__ -   Batch number = 96
Evaluating:  48%|████▊     | 96/201 [00:16<00:15,  6.89it/s]11/28/2021 01:21:07 - INFO - __main__ -   Batch number = 97
Evaluating:  48%|████▊     | 97/201 [00:16<00:15,  6.92it/s]11/28/2021 01:21:08 - INFO - __main__ -   Batch number = 98
Evaluating:  49%|████▉     | 98/201 [00:16<00:15,  6.83it/s]11/28/2021 01:21:08 - INFO - __main__ -   Batch number = 99
Evaluating:  49%|████▉     | 99/201 [00:17<00:15,  6.74it/s]11/28/2021 01:21:08 - INFO - __main__ -   Batch number = 100
Evaluating:  50%|████▉     | 100/201 [00:17<00:15,  6.73it/s]11/28/2021 01:21:08 - INFO - __main__ -   Batch number = 101
Evaluating:  50%|█████     | 101/201 [00:17<00:14,  6.75it/s]11/28/2021 01:21:08 - INFO - __main__ -   Batch number = 102
Evaluating:  51%|█████     | 102/201 [00:17<00:14,  6.69it/s]11/28/2021 01:21:08 - INFO - __main__ -   Batch number = 103
Evaluating:  51%|█████     | 103/201 [00:17<00:14,  6.70it/s]11/28/2021 01:21:08 - INFO - __main__ -   Batch number = 104
Evaluating:  52%|█████▏    | 104/201 [00:17<00:14,  6.73it/s]11/28/2021 01:21:09 - INFO - __main__ -   Batch number = 105
Evaluating:  52%|█████▏    | 105/201 [00:18<00:14,  6.77it/s]11/28/2021 01:21:09 - INFO - __main__ -   Batch number = 106
Evaluating:  53%|█████▎    | 106/201 [00:18<00:14,  6.78it/s]11/28/2021 01:21:09 - INFO - __main__ -   Batch number = 107
Evaluating:  53%|█████▎    | 107/201 [00:18<00:15,  6.20it/s]11/28/2021 01:21:09 - INFO - __main__ -   Batch number = 108
Evaluating:  54%|█████▎    | 108/201 [00:18<00:14,  6.37it/s]11/28/2021 01:21:09 - INFO - __main__ -   Batch number = 109
Evaluating:  54%|█████▍    | 109/201 [00:18<00:14,  6.51it/s]11/28/2021 01:21:09 - INFO - __main__ -   Batch number = 110
Evaluating:  55%|█████▍    | 110/201 [00:18<00:13,  6.56it/s]11/28/2021 01:21:10 - INFO - __main__ -   Batch number = 111
Evaluating:  55%|█████▌    | 111/201 [00:18<00:13,  6.64it/s]11/28/2021 01:21:10 - INFO - __main__ -   Batch number = 112
Evaluating:  56%|█████▌    | 112/201 [00:19<00:13,  6.63it/s]11/28/2021 01:21:10 - INFO - __main__ -   Batch number = 113
Evaluating:  56%|█████▌    | 113/201 [00:19<00:13,  6.62it/s]11/28/2021 01:21:10 - INFO - __main__ -   Batch number = 114
Evaluating:  57%|█████▋    | 114/201 [00:19<00:13,  6.59it/s]11/28/2021 01:21:10 - INFO - __main__ -   Batch number = 115
Evaluating:  57%|█████▋    | 115/201 [00:19<00:13,  6.59it/s]11/28/2021 01:21:10 - INFO - __main__ -   Batch number = 116
Evaluating:  58%|█████▊    | 116/201 [00:19<00:12,  6.54it/s]11/28/2021 01:21:10 - INFO - __main__ -   Batch number = 117
Evaluating:  58%|█████▊    | 117/201 [00:19<00:12,  6.57it/s]11/28/2021 01:21:11 - INFO - __main__ -   Batch number = 118
Evaluating:  59%|█████▊    | 118/201 [00:20<00:12,  6.57it/s]11/28/2021 01:21:11 - INFO - __main__ -   Batch number = 119
Evaluating:  59%|█████▉    | 119/201 [00:20<00:12,  6.55it/s]11/28/2021 01:21:11 - INFO - __main__ -   Batch number = 120
Evaluating:  60%|█████▉    | 120/201 [00:20<00:12,  6.59it/s]11/28/2021 01:21:11 - INFO - __main__ -   Batch number = 121
Evaluating:  60%|██████    | 121/201 [00:20<00:12,  6.52it/s]11/28/2021 01:21:11 - INFO - __main__ -   Batch number = 122
Evaluating:  61%|██████    | 122/201 [00:20<00:12,  6.41it/s]11/28/2021 01:21:11 - INFO - __main__ -   Batch number = 123
Evaluating:  61%|██████    | 123/201 [00:20<00:12,  6.39it/s]11/28/2021 01:21:11 - INFO - __main__ -   Batch number = 124
Evaluating:  62%|██████▏   | 124/201 [00:20<00:12,  6.36it/s]11/28/2021 01:21:12 - INFO - __main__ -   Batch number = 125
Evaluating:  62%|██████▏   | 125/201 [00:21<00:11,  6.40it/s]11/28/2021 01:21:12 - INFO - __main__ -   Batch number = 126
Evaluating:  63%|██████▎   | 126/201 [00:21<00:11,  6.47it/s]11/28/2021 01:21:12 - INFO - __main__ -   Batch number = 127
Evaluating:  63%|██████▎   | 127/201 [00:21<00:11,  6.47it/s]11/28/2021 01:21:12 - INFO - __main__ -   Batch number = 128
Evaluating:  64%|██████▎   | 128/201 [00:21<00:11,  6.47it/s]11/28/2021 01:21:12 - INFO - __main__ -   Batch number = 129
Evaluating:  64%|██████▍   | 129/201 [00:21<00:11,  6.46it/s]11/28/2021 01:21:12 - INFO - __main__ -   Batch number = 130
Evaluating:  65%|██████▍   | 130/201 [00:21<00:11,  6.45it/s]11/28/2021 01:21:13 - INFO - __main__ -   Batch number = 131
Evaluating:  65%|██████▌   | 131/201 [00:22<00:10,  6.45it/s]11/28/2021 01:21:13 - INFO - __main__ -   Batch number = 132
Evaluating:  66%|██████▌   | 132/201 [00:22<00:13,  4.98it/s]11/28/2021 01:21:13 - INFO - __main__ -   Batch number = 133
Evaluating:  66%|██████▌   | 133/201 [00:22<00:12,  5.26it/s]11/28/2021 01:21:13 - INFO - __main__ -   Batch number = 134
Evaluating:  67%|██████▋   | 134/201 [00:22<00:12,  5.48it/s]11/28/2021 01:21:13 - INFO - __main__ -   Batch number = 135
Evaluating:  67%|██████▋   | 135/201 [00:22<00:11,  5.75it/s]11/28/2021 01:21:14 - INFO - __main__ -   Batch number = 136
Evaluating:  68%|██████▊   | 136/201 [00:22<00:10,  5.98it/s]11/28/2021 01:21:14 - INFO - __main__ -   Batch number = 137
Evaluating:  68%|██████▊   | 137/201 [00:23<00:10,  6.04it/s]11/28/2021 01:21:14 - INFO - __main__ -   Batch number = 138
Evaluating:  69%|██████▊   | 138/201 [00:23<00:12,  4.88it/s]11/28/2021 01:21:14 - INFO - __main__ -   Batch number = 139
Evaluating:  69%|██████▉   | 139/201 [00:23<00:12,  5.16it/s]11/28/2021 01:21:14 - INFO - __main__ -   Batch number = 140
Evaluating:  70%|██████▉   | 140/201 [00:23<00:11,  5.39it/s]11/28/2021 01:21:14 - INFO - __main__ -   Batch number = 141
Evaluating:  70%|███████   | 141/201 [00:23<00:10,  5.52it/s]11/28/2021 01:21:15 - INFO - __main__ -   Batch number = 142
Evaluating:  71%|███████   | 142/201 [00:24<00:10,  5.64it/s]11/28/2021 01:21:15 - INFO - __main__ -   Batch number = 143
Evaluating:  71%|███████   | 143/201 [00:24<00:10,  5.76it/s]11/28/2021 01:21:15 - INFO - __main__ -   Batch number = 144
Evaluating:  72%|███████▏  | 144/201 [00:24<00:09,  5.91it/s]11/28/2021 01:21:15 - INFO - __main__ -   Batch number = 145
Evaluating:  72%|███████▏  | 145/201 [00:24<00:09,  6.05it/s]11/28/2021 01:21:15 - INFO - __main__ -   Batch number = 146
Evaluating:  73%|███████▎  | 146/201 [00:24<00:08,  6.22it/s]11/28/2021 01:21:15 - INFO - __main__ -   Batch number = 147
Evaluating:  73%|███████▎  | 147/201 [00:24<00:08,  6.31it/s]11/28/2021 01:21:16 - INFO - __main__ -   Batch number = 148
Evaluating:  74%|███████▎  | 148/201 [00:25<00:08,  6.40it/s]11/28/2021 01:21:16 - INFO - __main__ -   Batch number = 149
Evaluating:  74%|███████▍  | 149/201 [00:25<00:08,  6.39it/s]11/28/2021 01:21:16 - INFO - __main__ -   Batch number = 150
Evaluating:  75%|███████▍  | 150/201 [00:25<00:08,  6.36it/s]11/28/2021 01:21:16 - INFO - __main__ -   Batch number = 151
Evaluating:  75%|███████▌  | 151/201 [00:25<00:07,  6.36it/s]11/28/2021 01:21:16 - INFO - __main__ -   Batch number = 152
Evaluating:  76%|███████▌  | 152/201 [00:25<00:07,  6.36it/s]11/28/2021 01:21:16 - INFO - __main__ -   Batch number = 153
Evaluating:  76%|███████▌  | 153/201 [00:25<00:07,  6.42it/s]11/28/2021 01:21:17 - INFO - __main__ -   Batch number = 154
Evaluating:  77%|███████▋  | 154/201 [00:25<00:07,  6.36it/s]11/28/2021 01:21:17 - INFO - __main__ -   Batch number = 155
Evaluating:  77%|███████▋  | 155/201 [00:26<00:07,  6.35it/s]11/28/2021 01:21:17 - INFO - __main__ -   Batch number = 156
Evaluating:  78%|███████▊  | 156/201 [00:26<00:07,  6.42it/s]11/28/2021 01:21:17 - INFO - __main__ -   Batch number = 157
Evaluating:  78%|███████▊  | 157/201 [00:26<00:06,  6.47it/s]11/28/2021 01:21:17 - INFO - __main__ -   Batch number = 158
Evaluating:  79%|███████▊  | 158/201 [00:26<00:07,  5.43it/s]11/28/2021 01:21:17 - INFO - __main__ -   Batch number = 159
Evaluating:  79%|███████▉  | 159/201 [00:27<00:09,  4.42it/s]11/28/2021 01:21:18 - INFO - __main__ -   Batch number = 160
Evaluating:  80%|███████▉  | 160/201 [00:27<00:11,  3.63it/s]11/28/2021 01:21:18 - INFO - __main__ -   Batch number = 161
Evaluating:  80%|████████  | 161/201 [00:27<00:13,  3.01it/s]11/28/2021 01:21:19 - INFO - __main__ -   Batch number = 162
Evaluating:  81%|████████  | 162/201 [00:28<00:13,  2.94it/s]11/28/2021 01:21:19 - INFO - __main__ -   Batch number = 163
Evaluating:  81%|████████  | 163/201 [00:28<00:12,  2.96it/s]11/28/2021 01:21:19 - INFO - __main__ -   Batch number = 164
Evaluating:  82%|████████▏ | 164/201 [00:28<00:12,  2.96it/s]11/28/2021 01:21:20 - INFO - __main__ -   Batch number = 165
Evaluating:  82%|████████▏ | 165/201 [00:29<00:12,  2.97it/s]11/28/2021 01:21:20 - INFO - __main__ -   Batch number = 166
Evaluating:  83%|████████▎ | 166/201 [00:29<00:11,  2.97it/s]11/28/2021 01:21:20 - INFO - __main__ -   Batch number = 167
Evaluating:  83%|████████▎ | 167/201 [00:29<00:11,  2.99it/s]11/28/2021 01:21:21 - INFO - __main__ -   Batch number = 168
Evaluating:  84%|████████▎ | 168/201 [00:30<00:11,  2.99it/s]11/28/2021 01:21:21 - INFO - __main__ -   Batch number = 169
Evaluating:  84%|████████▍ | 169/201 [00:30<00:10,  2.98it/s]11/28/2021 01:21:21 - INFO - __main__ -   Batch number = 170
Evaluating:  85%|████████▍ | 170/201 [00:30<00:10,  3.00it/s]11/28/2021 01:21:22 - INFO - __main__ -   Batch number = 171
Evaluating:  85%|████████▌ | 171/201 [00:31<00:09,  3.00it/s]11/28/2021 01:21:22 - INFO - __main__ -   Batch number = 172
Evaluating:  86%|████████▌ | 172/201 [00:31<00:09,  3.00it/s]11/28/2021 01:21:22 - INFO - __main__ -   Batch number = 173
Evaluating:  86%|████████▌ | 173/201 [00:31<00:09,  3.00it/s]11/28/2021 01:21:23 - INFO - __main__ -   Batch number = 174
Evaluating:  87%|████████▋ | 174/201 [00:32<00:09,  3.00it/s]11/28/2021 01:21:23 - INFO - __main__ -   Batch number = 175
Evaluating:  87%|████████▋ | 175/201 [00:32<00:08,  3.00it/s]11/28/2021 01:21:23 - INFO - __main__ -   Batch number = 176
Evaluating:  88%|████████▊ | 176/201 [00:32<00:08,  3.00it/s]11/28/2021 01:21:24 - INFO - __main__ -   Batch number = 177
Evaluating:  88%|████████▊ | 177/201 [00:33<00:07,  3.01it/s]11/28/2021 01:21:24 - INFO - __main__ -   Batch number = 178
Evaluating:  89%|████████▊ | 178/201 [00:33<00:08,  2.86it/s]11/28/2021 01:21:24 - INFO - __main__ -   Batch number = 179
Evaluating:  89%|████████▉ | 179/201 [00:33<00:07,  2.94it/s]11/28/2021 01:21:25 - INFO - __main__ -   Batch number = 180
Evaluating:  90%|████████▉ | 180/201 [00:34<00:06,  3.01it/s]11/28/2021 01:21:25 - INFO - __main__ -   Batch number = 181
Evaluating:  90%|█████████ | 181/201 [00:34<00:06,  3.03it/s]11/28/2021 01:21:25 - INFO - __main__ -   Batch number = 182
Evaluating:  91%|█████████ | 182/201 [00:34<00:06,  3.03it/s]11/28/2021 01:21:26 - INFO - __main__ -   Batch number = 183
Evaluating:  91%|█████████ | 183/201 [00:35<00:05,  3.02it/s]11/28/2021 01:21:26 - INFO - __main__ -   Batch number = 184
Evaluating:  92%|█████████▏| 184/201 [00:35<00:05,  3.02it/s]11/28/2021 01:21:26 - INFO - __main__ -   Batch number = 185
Evaluating:  92%|█████████▏| 185/201 [00:35<00:05,  3.00it/s]11/28/2021 01:21:27 - INFO - __main__ -   Batch number = 186
Evaluating:  93%|█████████▎| 186/201 [00:36<00:05,  2.99it/s]11/28/2021 01:21:27 - INFO - __main__ -   Batch number = 187
Evaluating:  93%|█████████▎| 187/201 [00:36<00:04,  2.99it/s]11/28/2021 01:21:27 - INFO - __main__ -   Batch number = 188
Evaluating:  94%|█████████▎| 188/201 [00:36<00:04,  3.00it/s]11/28/2021 01:21:28 - INFO - __main__ -   Batch number = 189
Evaluating:  94%|█████████▍| 189/201 [00:37<00:03,  3.05it/s]11/28/2021 01:21:28 - INFO - __main__ -   Batch number = 190
Evaluating:  95%|█████████▍| 190/201 [00:37<00:03,  3.06it/s]11/28/2021 01:21:28 - INFO - __main__ -   Batch number = 191
Evaluating:  95%|█████████▌| 191/201 [00:37<00:03,  3.07it/s]11/28/2021 01:21:29 - INFO - __main__ -   Batch number = 192
Evaluating:  96%|█████████▌| 192/201 [00:38<00:02,  3.08it/s]11/28/2021 01:21:29 - INFO - __main__ -   Batch number = 193
Evaluating:  96%|█████████▌| 193/201 [00:38<00:02,  2.90it/s]11/28/2021 01:21:29 - INFO - __main__ -   Batch number = 194
Evaluating:  97%|█████████▋| 194/201 [00:38<00:02,  2.94it/s]11/28/2021 01:21:30 - INFO - __main__ -   Batch number = 195
Evaluating:  97%|█████████▋| 195/201 [00:39<00:02,  2.96it/s]11/28/2021 01:21:30 - INFO - __main__ -   Batch number = 196
Evaluating:  98%|█████████▊| 196/201 [00:39<00:01,  2.97it/s]11/28/2021 01:21:30 - INFO - __main__ -   Batch number = 197
Evaluating:  98%|█████████▊| 197/201 [00:39<00:01,  2.99it/s]11/28/2021 01:21:31 - INFO - __main__ -   Batch number = 198
Evaluating:  99%|█████████▊| 198/201 [00:40<00:00,  3.00it/s]11/28/2021 01:21:31 - INFO - __main__ -   Batch number = 199
Evaluating:  99%|█████████▉| 199/201 [00:40<00:00,  3.02it/s]11/28/2021 01:21:31 - INFO - __main__ -   Batch number = 200
Evaluating: 100%|█████████▉| 200/201 [00:40<00:00,  3.05it/s]11/28/2021 01:21:32 - INFO - __main__ -   Batch number = 201
Evaluating: 100%|██████████| 201/201 [00:40<00:00,  4.90it/s]
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:21:36 - INFO - __main__ -   ***** Evaluation result  in is *****
11/28/2021 01:21:36 - INFO - __main__ -     f1 = 0.747874410298782
11/28/2021 01:21:36 - INFO - __main__ -     loss = 0.8833516974057725
11/28/2021 01:21:36 - INFO - __main__ -     precision = 0.7533914631117555
11/28/2021 01:21:36 - INFO - __main__ -     recall = 0.7424375723499256
57.51user 16.51system 1:15.15elapsed 98%CPU (0avgtext+0avgdata 3942524maxresident)k
4792inputs+23136outputs (0major+1557552minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:21:39 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='is', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:21:39 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:21:39 - INFO - __main__ -   Seed = 2
11/28/2021 01:21:39 - INFO - root -   save model
11/28/2021 01:21:39 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='is', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:21:39 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:21:42 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:21:48 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:21:48 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:21:48 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
11/28/2021 01:21:48 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:21:48 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:21:48 - INFO - root -   loading lang adpater am/wiki@ukp
11/28/2021 01:21:48 - INFO - __main__ -   Adapter Languages : ['am'], Length : 1
11/28/2021 01:21:48 - INFO - __main__ -   Adapter Names ['am/wiki@ukp'], Length : 1
11/28/2021 01:21:48 - INFO - __main__ -   Language = am
11/28/2021 01:21:48 - INFO - __main__ -   Adapter Name = am/wiki@ukp
Found matching adapter at: adapters/ukp/bert-base-multilingual-cased_am_wiki_pfeiffer.json
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/am/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_am_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/adapter_config.json
Adding adapter 'am' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/head_config.json
11/28/2021 01:21:56 - INFO - __main__ -   Language adapter for is not found, using am instead
11/28/2021 01:21:56 - INFO - __main__ -   Set active language adapter to am
11/28/2021 01:21:56 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:21:56 - INFO - __main__ -   Adapter Languages = ['am']
11/28/2021 01:21:56 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_is_bert-base-multilingual-cased_128
11/28/2021 01:21:57 - INFO - __main__ -   ***** Running evaluation  in is *****
11/28/2021 01:21:57 - INFO - __main__ -     Num examples = 6401
11/28/2021 01:21:57 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/201 [00:00<?, ?it/s]11/28/2021 01:21:57 - INFO - __main__ -   Batch number = 1
Evaluating:   0%|          | 1/201 [00:00<01:17,  2.59it/s]11/28/2021 01:21:57 - INFO - __main__ -   Batch number = 2
Evaluating:   1%|          | 2/201 [00:00<01:04,  3.06it/s]11/28/2021 01:21:58 - INFO - __main__ -   Batch number = 3
Evaluating:   1%|▏         | 3/201 [00:00<01:00,  3.30it/s]11/28/2021 01:21:58 - INFO - __main__ -   Batch number = 4
Evaluating:   2%|▏         | 4/201 [00:01<00:58,  3.40it/s]11/28/2021 01:21:58 - INFO - __main__ -   Batch number = 5
Evaluating:   2%|▏         | 5/201 [00:01<00:56,  3.45it/s]11/28/2021 01:21:58 - INFO - __main__ -   Batch number = 6
Evaluating:   3%|▎         | 6/201 [00:01<00:56,  3.46it/s]11/28/2021 01:21:59 - INFO - __main__ -   Batch number = 7
Evaluating:   3%|▎         | 7/201 [00:02<00:56,  3.46it/s]11/28/2021 01:21:59 - INFO - __main__ -   Batch number = 8
Evaluating:   4%|▍         | 8/201 [00:02<00:55,  3.51it/s]11/28/2021 01:21:59 - INFO - __main__ -   Batch number = 9
Evaluating:   4%|▍         | 9/201 [00:02<00:54,  3.52it/s]11/28/2021 01:22:00 - INFO - __main__ -   Batch number = 10
Evaluating:   5%|▍         | 10/201 [00:02<00:54,  3.53it/s]11/28/2021 01:22:00 - INFO - __main__ -   Batch number = 11
Evaluating:   5%|▌         | 11/201 [00:03<00:53,  3.56it/s]11/28/2021 01:22:00 - INFO - __main__ -   Batch number = 12
Evaluating:   6%|▌         | 12/201 [00:03<00:53,  3.55it/s]11/28/2021 01:22:00 - INFO - __main__ -   Batch number = 13
Evaluating:   6%|▋         | 13/201 [00:03<00:52,  3.55it/s]11/28/2021 01:22:01 - INFO - __main__ -   Batch number = 14
Evaluating:   7%|▋         | 14/201 [00:04<00:52,  3.57it/s]11/28/2021 01:22:01 - INFO - __main__ -   Batch number = 15
Evaluating:   7%|▋         | 15/201 [00:04<00:52,  3.56it/s]11/28/2021 01:22:01 - INFO - __main__ -   Batch number = 16
Evaluating:   8%|▊         | 16/201 [00:04<00:52,  3.55it/s]11/28/2021 01:22:02 - INFO - __main__ -   Batch number = 17
Evaluating:   8%|▊         | 17/201 [00:04<00:51,  3.55it/s]11/28/2021 01:22:02 - INFO - __main__ -   Batch number = 18
Evaluating:   9%|▉         | 18/201 [00:05<00:51,  3.55it/s]11/28/2021 01:22:02 - INFO - __main__ -   Batch number = 19
Evaluating:   9%|▉         | 19/201 [00:05<00:51,  3.55it/s]11/28/2021 01:22:02 - INFO - __main__ -   Batch number = 20
Evaluating:  10%|▉         | 20/201 [00:05<00:51,  3.54it/s]11/28/2021 01:22:03 - INFO - __main__ -   Batch number = 21
Evaluating:  10%|█         | 21/201 [00:06<00:51,  3.52it/s]11/28/2021 01:22:03 - INFO - __main__ -   Batch number = 22
Evaluating:  11%|█         | 22/201 [00:06<00:59,  3.02it/s]11/28/2021 01:22:03 - INFO - __main__ -   Batch number = 23
Evaluating:  11%|█▏        | 23/201 [00:06<00:56,  3.18it/s]11/28/2021 01:22:04 - INFO - __main__ -   Batch number = 24
Evaluating:  12%|█▏        | 24/201 [00:07<00:53,  3.28it/s]11/28/2021 01:22:04 - INFO - __main__ -   Batch number = 25
Evaluating:  12%|█▏        | 25/201 [00:07<00:52,  3.36it/s]11/28/2021 01:22:04 - INFO - __main__ -   Batch number = 26
Evaluating:  13%|█▎        | 26/201 [00:07<00:50,  3.45it/s]11/28/2021 01:22:05 - INFO - __main__ -   Batch number = 27
Evaluating:  13%|█▎        | 27/201 [00:07<00:49,  3.49it/s]11/28/2021 01:22:05 - INFO - __main__ -   Batch number = 28
Evaluating:  14%|█▍        | 28/201 [00:08<00:49,  3.51it/s]11/28/2021 01:22:05 - INFO - __main__ -   Batch number = 29
Evaluating:  14%|█▍        | 29/201 [00:08<00:49,  3.51it/s]11/28/2021 01:22:05 - INFO - __main__ -   Batch number = 30
Evaluating:  15%|█▍        | 30/201 [00:08<00:48,  3.51it/s]11/28/2021 01:22:06 - INFO - __main__ -   Batch number = 31
Evaluating:  15%|█▌        | 31/201 [00:08<00:48,  3.50it/s]11/28/2021 01:22:06 - INFO - __main__ -   Batch number = 32
Evaluating:  16%|█▌        | 32/201 [00:09<00:48,  3.47it/s]11/28/2021 01:22:06 - INFO - __main__ -   Batch number = 33
Evaluating:  16%|█▋        | 33/201 [00:09<00:48,  3.46it/s]11/28/2021 01:22:07 - INFO - __main__ -   Batch number = 34
Evaluating:  17%|█▋        | 34/201 [00:09<00:48,  3.42it/s]11/28/2021 01:22:07 - INFO - __main__ -   Batch number = 35
Evaluating:  17%|█▋        | 35/201 [00:10<00:43,  3.82it/s]11/28/2021 01:22:07 - INFO - __main__ -   Batch number = 36
Evaluating:  18%|█▊        | 36/201 [00:10<00:37,  4.43it/s]11/28/2021 01:22:07 - INFO - __main__ -   Batch number = 37
Evaluating:  18%|█▊        | 37/201 [00:10<00:32,  4.98it/s]11/28/2021 01:22:07 - INFO - __main__ -   Batch number = 38
Evaluating:  19%|█▉        | 38/201 [00:10<00:30,  5.42it/s]11/28/2021 01:22:07 - INFO - __main__ -   Batch number = 39
Evaluating:  19%|█▉        | 39/201 [00:10<00:27,  5.80it/s]11/28/2021 01:22:08 - INFO - __main__ -   Batch number = 40
Evaluating:  20%|█▉        | 40/201 [00:10<00:26,  6.05it/s]11/28/2021 01:22:08 - INFO - __main__ -   Batch number = 41
Evaluating:  20%|██        | 41/201 [00:10<00:25,  6.29it/s]11/28/2021 01:22:08 - INFO - __main__ -   Batch number = 42
Evaluating:  21%|██        | 42/201 [00:11<00:24,  6.48it/s]11/28/2021 01:22:08 - INFO - __main__ -   Batch number = 43
Evaluating:  21%|██▏       | 43/201 [00:11<00:28,  5.45it/s]11/28/2021 01:22:08 - INFO - __main__ -   Batch number = 44
Evaluating:  22%|██▏       | 44/201 [00:11<00:26,  5.90it/s]11/28/2021 01:22:08 - INFO - __main__ -   Batch number = 45
Evaluating:  22%|██▏       | 45/201 [00:11<00:24,  6.28it/s]11/28/2021 01:22:09 - INFO - __main__ -   Batch number = 46
Evaluating:  23%|██▎       | 46/201 [00:11<00:23,  6.57it/s]11/28/2021 01:22:09 - INFO - __main__ -   Batch number = 47
Evaluating:  23%|██▎       | 47/201 [00:11<00:22,  6.77it/s]11/28/2021 01:22:09 - INFO - __main__ -   Batch number = 48
Evaluating:  24%|██▍       | 48/201 [00:12<00:22,  6.95it/s]11/28/2021 01:22:09 - INFO - __main__ -   Batch number = 49
Evaluating:  24%|██▍       | 49/201 [00:12<00:21,  7.07it/s]11/28/2021 01:22:09 - INFO - __main__ -   Batch number = 50
Evaluating:  25%|██▍       | 50/201 [00:12<00:21,  7.14it/s]11/28/2021 01:22:09 - INFO - __main__ -   Batch number = 51
Evaluating:  25%|██▌       | 51/201 [00:12<00:20,  7.20it/s]11/28/2021 01:22:09 - INFO - __main__ -   Batch number = 52
Evaluating:  26%|██▌       | 52/201 [00:12<00:20,  7.23it/s]11/28/2021 01:22:10 - INFO - __main__ -   Batch number = 53
Evaluating:  26%|██▋       | 53/201 [00:12<00:20,  7.25it/s]11/28/2021 01:22:10 - INFO - __main__ -   Batch number = 54
Evaluating:  27%|██▋       | 54/201 [00:12<00:20,  7.28it/s]11/28/2021 01:22:10 - INFO - __main__ -   Batch number = 55
Evaluating:  27%|██▋       | 55/201 [00:12<00:19,  7.30it/s]11/28/2021 01:22:10 - INFO - __main__ -   Batch number = 56
Evaluating:  28%|██▊       | 56/201 [00:13<00:19,  7.29it/s]11/28/2021 01:22:10 - INFO - __main__ -   Batch number = 57
Evaluating:  28%|██▊       | 57/201 [00:13<00:19,  7.30it/s]11/28/2021 01:22:10 - INFO - __main__ -   Batch number = 58
Evaluating:  29%|██▉       | 58/201 [00:13<00:19,  7.29it/s]11/28/2021 01:22:10 - INFO - __main__ -   Batch number = 59
Evaluating:  29%|██▉       | 59/201 [00:13<00:19,  7.28it/s]11/28/2021 01:22:10 - INFO - __main__ -   Batch number = 60
Evaluating:  30%|██▉       | 60/201 [00:13<00:19,  7.30it/s]11/28/2021 01:22:11 - INFO - __main__ -   Batch number = 61
Evaluating:  30%|███       | 61/201 [00:13<00:19,  7.31it/s]11/28/2021 01:22:11 - INFO - __main__ -   Batch number = 62
Evaluating:  31%|███       | 62/201 [00:13<00:19,  7.30it/s]11/28/2021 01:22:11 - INFO - __main__ -   Batch number = 63
Evaluating:  31%|███▏      | 63/201 [00:14<00:18,  7.29it/s]11/28/2021 01:22:11 - INFO - __main__ -   Batch number = 64
Evaluating:  32%|███▏      | 64/201 [00:14<00:18,  7.29it/s]11/28/2021 01:22:11 - INFO - __main__ -   Batch number = 65
Evaluating:  32%|███▏      | 65/201 [00:14<00:18,  7.27it/s]11/28/2021 01:22:11 - INFO - __main__ -   Batch number = 66
Evaluating:  33%|███▎      | 66/201 [00:14<00:18,  7.27it/s]11/28/2021 01:22:11 - INFO - __main__ -   Batch number = 67
Evaluating:  33%|███▎      | 67/201 [00:14<00:18,  7.26it/s]11/28/2021 01:22:12 - INFO - __main__ -   Batch number = 68
Evaluating:  34%|███▍      | 68/201 [00:14<00:18,  7.26it/s]11/28/2021 01:22:12 - INFO - __main__ -   Batch number = 69
Evaluating:  34%|███▍      | 69/201 [00:14<00:18,  7.27it/s]11/28/2021 01:22:12 - INFO - __main__ -   Batch number = 70
Evaluating:  35%|███▍      | 70/201 [00:15<00:18,  7.25it/s]11/28/2021 01:22:12 - INFO - __main__ -   Batch number = 71
Evaluating:  35%|███▌      | 71/201 [00:15<00:17,  7.22it/s]11/28/2021 01:22:12 - INFO - __main__ -   Batch number = 72
Evaluating:  36%|███▌      | 72/201 [00:15<00:18,  7.10it/s]11/28/2021 01:22:12 - INFO - __main__ -   Batch number = 73
Evaluating:  36%|███▋      | 73/201 [00:15<00:17,  7.15it/s]11/28/2021 01:22:12 - INFO - __main__ -   Batch number = 74
Evaluating:  37%|███▋      | 74/201 [00:15<00:17,  7.17it/s]11/28/2021 01:22:13 - INFO - __main__ -   Batch number = 75
Evaluating:  37%|███▋      | 75/201 [00:15<00:17,  7.19it/s]11/28/2021 01:22:13 - INFO - __main__ -   Batch number = 76
Evaluating:  38%|███▊      | 76/201 [00:15<00:17,  7.20it/s]11/28/2021 01:22:13 - INFO - __main__ -   Batch number = 77
Evaluating:  38%|███▊      | 77/201 [00:16<00:17,  7.05it/s]11/28/2021 01:22:13 - INFO - __main__ -   Batch number = 78
Evaluating:  39%|███▉      | 78/201 [00:16<00:17,  6.95it/s]11/28/2021 01:22:13 - INFO - __main__ -   Batch number = 79
Evaluating:  39%|███▉      | 79/201 [00:16<00:17,  6.97it/s]11/28/2021 01:22:13 - INFO - __main__ -   Batch number = 80
Evaluating:  40%|███▉      | 80/201 [00:16<00:18,  6.67it/s]11/28/2021 01:22:13 - INFO - __main__ -   Batch number = 81
Evaluating:  40%|████      | 81/201 [00:16<00:17,  6.74it/s]11/28/2021 01:22:14 - INFO - __main__ -   Batch number = 82
Evaluating:  41%|████      | 82/201 [00:16<00:17,  6.76it/s]11/28/2021 01:22:14 - INFO - __main__ -   Batch number = 83
Evaluating:  41%|████▏     | 83/201 [00:16<00:17,  6.80it/s]11/28/2021 01:22:14 - INFO - __main__ -   Batch number = 84
Evaluating:  42%|████▏     | 84/201 [00:17<00:17,  6.77it/s]11/28/2021 01:22:14 - INFO - __main__ -   Batch number = 85
Evaluating:  42%|████▏     | 85/201 [00:17<00:17,  6.79it/s]11/28/2021 01:22:14 - INFO - __main__ -   Batch number = 86
Evaluating:  43%|████▎     | 86/201 [00:17<00:16,  6.82it/s]11/28/2021 01:22:14 - INFO - __main__ -   Batch number = 87
Evaluating:  43%|████▎     | 87/201 [00:17<00:16,  6.86it/s]11/28/2021 01:22:14 - INFO - __main__ -   Batch number = 88
Evaluating:  44%|████▍     | 88/201 [00:17<00:16,  6.69it/s]11/28/2021 01:22:15 - INFO - __main__ -   Batch number = 89
Evaluating:  44%|████▍     | 89/201 [00:17<00:16,  6.77it/s]11/28/2021 01:22:15 - INFO - __main__ -   Batch number = 90
Evaluating:  45%|████▍     | 90/201 [00:17<00:16,  6.87it/s]11/28/2021 01:22:15 - INFO - __main__ -   Batch number = 91
Evaluating:  45%|████▌     | 91/201 [00:18<00:15,  6.92it/s]11/28/2021 01:22:15 - INFO - __main__ -   Batch number = 92
Evaluating:  46%|████▌     | 92/201 [00:18<00:15,  6.96it/s]11/28/2021 01:22:15 - INFO - __main__ -   Batch number = 93
Evaluating:  46%|████▋     | 93/201 [00:18<00:15,  7.00it/s]11/28/2021 01:22:15 - INFO - __main__ -   Batch number = 94
Evaluating:  47%|████▋     | 94/201 [00:18<00:15,  7.04it/s]11/28/2021 01:22:15 - INFO - __main__ -   Batch number = 95
Evaluating:  47%|████▋     | 95/201 [00:18<00:14,  7.08it/s]11/28/2021 01:22:16 - INFO - __main__ -   Batch number = 96
Evaluating:  48%|████▊     | 96/201 [00:18<00:14,  7.12it/s]11/28/2021 01:22:16 - INFO - __main__ -   Batch number = 97
Evaluating:  48%|████▊     | 97/201 [00:18<00:14,  7.13it/s]11/28/2021 01:22:16 - INFO - __main__ -   Batch number = 98
Evaluating:  49%|████▉     | 98/201 [00:19<00:14,  7.14it/s]11/28/2021 01:22:16 - INFO - __main__ -   Batch number = 99
Evaluating:  49%|████▉     | 99/201 [00:19<00:14,  7.10it/s]11/28/2021 01:22:16 - INFO - __main__ -   Batch number = 100
Evaluating:  50%|████▉     | 100/201 [00:19<00:14,  7.09it/s]11/28/2021 01:22:16 - INFO - __main__ -   Batch number = 101
Evaluating:  50%|█████     | 101/201 [00:19<00:14,  7.11it/s]11/28/2021 01:22:16 - INFO - __main__ -   Batch number = 102
Evaluating:  51%|█████     | 102/201 [00:19<00:13,  7.10it/s]11/28/2021 01:22:17 - INFO - __main__ -   Batch number = 103
Evaluating:  51%|█████     | 103/201 [00:19<00:13,  7.11it/s]11/28/2021 01:22:17 - INFO - __main__ -   Batch number = 104
Evaluating:  52%|█████▏    | 104/201 [00:19<00:13,  7.10it/s]11/28/2021 01:22:17 - INFO - __main__ -   Batch number = 105
Evaluating:  52%|█████▏    | 105/201 [00:20<00:13,  7.10it/s]11/28/2021 01:22:17 - INFO - __main__ -   Batch number = 106
Evaluating:  53%|█████▎    | 106/201 [00:20<00:13,  7.10it/s]11/28/2021 01:22:17 - INFO - __main__ -   Batch number = 107
Evaluating:  53%|█████▎    | 107/201 [00:20<00:13,  7.04it/s]11/28/2021 01:22:17 - INFO - __main__ -   Batch number = 108
Evaluating:  54%|█████▎    | 108/201 [00:20<00:13,  6.98it/s]11/28/2021 01:22:17 - INFO - __main__ -   Batch number = 109
Evaluating:  54%|█████▍    | 109/201 [00:20<00:13,  6.96it/s]11/28/2021 01:22:18 - INFO - __main__ -   Batch number = 110
Evaluating:  55%|█████▍    | 110/201 [00:20<00:13,  6.93it/s]11/28/2021 01:22:18 - INFO - __main__ -   Batch number = 111
Evaluating:  55%|█████▌    | 111/201 [00:20<00:12,  6.92it/s]11/28/2021 01:22:18 - INFO - __main__ -   Batch number = 112
Evaluating:  56%|█████▌    | 112/201 [00:21<00:12,  6.96it/s]11/28/2021 01:22:18 - INFO - __main__ -   Batch number = 113
Evaluating:  56%|█████▌    | 113/201 [00:21<00:12,  7.00it/s]11/28/2021 01:22:18 - INFO - __main__ -   Batch number = 114
Evaluating:  57%|█████▋    | 114/201 [00:21<00:12,  6.98it/s]11/28/2021 01:22:18 - INFO - __main__ -   Batch number = 115
Evaluating:  57%|█████▋    | 115/201 [00:21<00:16,  5.32it/s]11/28/2021 01:22:19 - INFO - __main__ -   Batch number = 116
Evaluating:  58%|█████▊    | 116/201 [00:21<00:15,  5.64it/s]11/28/2021 01:22:19 - INFO - __main__ -   Batch number = 117
Evaluating:  58%|█████▊    | 117/201 [00:21<00:14,  5.89it/s]11/28/2021 01:22:19 - INFO - __main__ -   Batch number = 118
Evaluating:  59%|█████▊    | 118/201 [00:22<00:13,  6.03it/s]11/28/2021 01:22:19 - INFO - __main__ -   Batch number = 119
Evaluating:  59%|█████▉    | 119/201 [00:22<00:13,  6.21it/s]11/28/2021 01:22:19 - INFO - __main__ -   Batch number = 120
Evaluating:  60%|█████▉    | 120/201 [00:22<00:12,  6.30it/s]11/28/2021 01:22:19 - INFO - __main__ -   Batch number = 121
Evaluating:  60%|██████    | 121/201 [00:22<00:12,  6.37it/s]11/28/2021 01:22:20 - INFO - __main__ -   Batch number = 122
Evaluating:  61%|██████    | 122/201 [00:22<00:12,  6.38it/s]11/28/2021 01:22:20 - INFO - __main__ -   Batch number = 123
Evaluating:  61%|██████    | 123/201 [00:22<00:12,  6.37it/s]11/28/2021 01:22:20 - INFO - __main__ -   Batch number = 124
Evaluating:  62%|██████▏   | 124/201 [00:23<00:12,  6.36it/s]11/28/2021 01:22:20 - INFO - __main__ -   Batch number = 125
Evaluating:  62%|██████▏   | 125/201 [00:23<00:11,  6.43it/s]11/28/2021 01:22:20 - INFO - __main__ -   Batch number = 126
Evaluating:  63%|██████▎   | 126/201 [00:23<00:11,  6.48it/s]11/28/2021 01:22:20 - INFO - __main__ -   Batch number = 127
Evaluating:  63%|██████▎   | 127/201 [00:23<00:11,  6.51it/s]11/28/2021 01:22:20 - INFO - __main__ -   Batch number = 128
Evaluating:  64%|██████▎   | 128/201 [00:23<00:11,  6.60it/s]11/28/2021 01:22:21 - INFO - __main__ -   Batch number = 129
Evaluating:  64%|██████▍   | 129/201 [00:23<00:11,  6.55it/s]11/28/2021 01:22:21 - INFO - __main__ -   Batch number = 130
Evaluating:  65%|██████▍   | 130/201 [00:23<00:10,  6.57it/s]11/28/2021 01:22:21 - INFO - __main__ -   Batch number = 131
Evaluating:  65%|██████▌   | 131/201 [00:24<00:10,  6.67it/s]11/28/2021 01:22:21 - INFO - __main__ -   Batch number = 132
Evaluating:  66%|██████▌   | 132/201 [00:24<00:10,  6.52it/s]11/28/2021 01:22:21 - INFO - __main__ -   Batch number = 133
Evaluating:  66%|██████▌   | 133/201 [00:24<00:10,  6.46it/s]11/28/2021 01:22:21 - INFO - __main__ -   Batch number = 134
Evaluating:  67%|██████▋   | 134/201 [00:24<00:10,  6.44it/s]11/28/2021 01:22:22 - INFO - __main__ -   Batch number = 135
Evaluating:  67%|██████▋   | 135/201 [00:24<00:10,  6.36it/s]11/28/2021 01:22:22 - INFO - __main__ -   Batch number = 136
Evaluating:  68%|██████▊   | 136/201 [00:24<00:10,  6.31it/s]11/28/2021 01:22:22 - INFO - __main__ -   Batch number = 137
Evaluating:  68%|██████▊   | 137/201 [00:25<00:10,  6.17it/s]11/28/2021 01:22:22 - INFO - __main__ -   Batch number = 138
Evaluating:  69%|██████▊   | 138/201 [00:25<00:10,  6.22it/s]11/28/2021 01:22:22 - INFO - __main__ -   Batch number = 139
Evaluating:  69%|██████▉   | 139/201 [00:25<00:09,  6.27it/s]11/28/2021 01:22:22 - INFO - __main__ -   Batch number = 140
Evaluating:  70%|██████▉   | 140/201 [00:25<00:09,  6.28it/s]11/28/2021 01:22:22 - INFO - __main__ -   Batch number = 141
Evaluating:  70%|███████   | 141/201 [00:25<00:09,  6.29it/s]11/28/2021 01:22:23 - INFO - __main__ -   Batch number = 142
Evaluating:  71%|███████   | 142/201 [00:25<00:09,  6.34it/s]11/28/2021 01:22:23 - INFO - __main__ -   Batch number = 143
Evaluating:  71%|███████   | 143/201 [00:25<00:09,  6.33it/s]11/28/2021 01:22:23 - INFO - __main__ -   Batch number = 144
Evaluating:  72%|███████▏  | 144/201 [00:26<00:08,  6.46it/s]11/28/2021 01:22:23 - INFO - __main__ -   Batch number = 145
Evaluating:  72%|███████▏  | 145/201 [00:26<00:08,  6.49it/s]11/28/2021 01:22:23 - INFO - __main__ -   Batch number = 146
Evaluating:  73%|███████▎  | 146/201 [00:26<00:10,  5.12it/s]11/28/2021 01:22:24 - INFO - __main__ -   Batch number = 147
Evaluating:  73%|███████▎  | 147/201 [00:26<00:10,  5.33it/s]11/28/2021 01:22:24 - INFO - __main__ -   Batch number = 148
Evaluating:  74%|███████▎  | 148/201 [00:26<00:09,  5.57it/s]11/28/2021 01:22:24 - INFO - __main__ -   Batch number = 149
Evaluating:  74%|███████▍  | 149/201 [00:27<00:08,  5.80it/s]11/28/2021 01:22:24 - INFO - __main__ -   Batch number = 150
Evaluating:  75%|███████▍  | 150/201 [00:27<00:08,  5.79it/s]11/28/2021 01:22:24 - INFO - __main__ -   Batch number = 151
Evaluating:  75%|███████▌  | 151/201 [00:27<00:08,  5.98it/s]11/28/2021 01:22:24 - INFO - __main__ -   Batch number = 152
Evaluating:  76%|███████▌  | 152/201 [00:27<00:08,  6.07it/s]11/28/2021 01:22:25 - INFO - __main__ -   Batch number = 153
Evaluating:  76%|███████▌  | 153/201 [00:27<00:07,  6.07it/s]11/28/2021 01:22:25 - INFO - __main__ -   Batch number = 154
Evaluating:  77%|███████▋  | 154/201 [00:27<00:07,  6.06it/s]11/28/2021 01:22:25 - INFO - __main__ -   Batch number = 155
Evaluating:  77%|███████▋  | 155/201 [00:28<00:07,  6.10it/s]11/28/2021 01:22:25 - INFO - __main__ -   Batch number = 156
Evaluating:  78%|███████▊  | 156/201 [00:28<00:07,  6.16it/s]11/28/2021 01:22:25 - INFO - __main__ -   Batch number = 157
Evaluating:  78%|███████▊  | 157/201 [00:28<00:06,  6.34it/s]11/28/2021 01:22:25 - INFO - __main__ -   Batch number = 158
Evaluating:  79%|███████▊  | 158/201 [00:28<00:06,  6.40it/s]11/28/2021 01:22:25 - INFO - __main__ -   Batch number = 159
Evaluating:  79%|███████▉  | 159/201 [00:28<00:06,  6.35it/s]11/28/2021 01:22:26 - INFO - __main__ -   Batch number = 160
Evaluating:  80%|███████▉  | 160/201 [00:28<00:06,  6.46it/s]11/28/2021 01:22:26 - INFO - __main__ -   Batch number = 161
Evaluating:  80%|████████  | 161/201 [00:28<00:06,  6.36it/s]11/28/2021 01:22:26 - INFO - __main__ -   Batch number = 162
Evaluating:  81%|████████  | 162/201 [00:29<00:06,  6.36it/s]11/28/2021 01:22:26 - INFO - __main__ -   Batch number = 163
Evaluating:  81%|████████  | 163/201 [00:29<00:05,  6.37it/s]11/28/2021 01:22:26 - INFO - __main__ -   Batch number = 164
Evaluating:  82%|████████▏ | 164/201 [00:29<00:05,  6.38it/s]11/28/2021 01:22:26 - INFO - __main__ -   Batch number = 165
Evaluating:  82%|████████▏ | 165/201 [00:29<00:05,  6.45it/s]11/28/2021 01:22:27 - INFO - __main__ -   Batch number = 166
Evaluating:  83%|████████▎ | 166/201 [00:29<00:05,  6.43it/s]11/28/2021 01:22:27 - INFO - __main__ -   Batch number = 167
Evaluating:  83%|████████▎ | 167/201 [00:29<00:05,  6.55it/s]11/28/2021 01:22:27 - INFO - __main__ -   Batch number = 168
Evaluating:  84%|████████▎ | 168/201 [00:30<00:05,  6.41it/s]11/28/2021 01:22:27 - INFO - __main__ -   Batch number = 169
Evaluating:  84%|████████▍ | 169/201 [00:30<00:04,  6.41it/s]11/28/2021 01:22:27 - INFO - __main__ -   Batch number = 170
Evaluating:  85%|████████▍ | 170/201 [00:30<00:04,  6.30it/s]11/28/2021 01:22:27 - INFO - __main__ -   Batch number = 171
Evaluating:  85%|████████▌ | 171/201 [00:30<00:04,  6.46it/s]11/28/2021 01:22:27 - INFO - __main__ -   Batch number = 172
Evaluating:  86%|████████▌ | 172/201 [00:30<00:04,  6.46it/s]11/28/2021 01:22:28 - INFO - __main__ -   Batch number = 173
Evaluating:  86%|████████▌ | 173/201 [00:30<00:04,  6.42it/s]11/28/2021 01:22:28 - INFO - __main__ -   Batch number = 174
Evaluating:  87%|████████▋ | 174/201 [00:31<00:04,  6.13it/s]11/28/2021 01:22:28 - INFO - __main__ -   Batch number = 175
Evaluating:  87%|████████▋ | 175/201 [00:31<00:04,  6.12it/s]11/28/2021 01:22:28 - INFO - __main__ -   Batch number = 176
Evaluating:  88%|████████▊ | 176/201 [00:31<00:04,  6.04it/s]11/28/2021 01:22:28 - INFO - __main__ -   Batch number = 177
Evaluating:  88%|████████▊ | 177/201 [00:31<00:04,  5.55it/s]11/28/2021 01:22:29 - INFO - __main__ -   Batch number = 178
Evaluating:  89%|████████▊ | 178/201 [00:31<00:04,  5.71it/s]11/28/2021 01:22:29 - INFO - __main__ -   Batch number = 179
Evaluating:  89%|████████▉ | 179/201 [00:31<00:03,  6.00it/s]11/28/2021 01:22:29 - INFO - __main__ -   Batch number = 180
Evaluating:  90%|████████▉ | 180/201 [00:32<00:03,  6.12it/s]11/28/2021 01:22:29 - INFO - __main__ -   Batch number = 181
Evaluating:  90%|█████████ | 181/201 [00:32<00:03,  6.32it/s]11/28/2021 01:22:29 - INFO - __main__ -   Batch number = 182
Evaluating:  91%|█████████ | 182/201 [00:32<00:03,  6.16it/s]11/28/2021 01:22:29 - INFO - __main__ -   Batch number = 183
Evaluating:  91%|█████████ | 183/201 [00:32<00:02,  6.23it/s]11/28/2021 01:22:29 - INFO - __main__ -   Batch number = 184
Evaluating:  92%|█████████▏| 184/201 [00:32<00:02,  6.08it/s]11/28/2021 01:22:30 - INFO - __main__ -   Batch number = 185
Evaluating:  92%|█████████▏| 185/201 [00:32<00:02,  6.08it/s]11/28/2021 01:22:30 - INFO - __main__ -   Batch number = 186
Evaluating:  93%|█████████▎| 186/201 [00:33<00:02,  6.13it/s]11/28/2021 01:22:30 - INFO - __main__ -   Batch number = 187
Evaluating:  93%|█████████▎| 187/201 [00:33<00:02,  6.21it/s]11/28/2021 01:22:30 - INFO - __main__ -   Batch number = 188
Evaluating:  94%|█████████▎| 188/201 [00:33<00:02,  6.10it/s]11/28/2021 01:22:30 - INFO - __main__ -   Batch number = 189
Evaluating:  94%|█████████▍| 189/201 [00:33<00:01,  6.31it/s]11/28/2021 01:22:30 - INFO - __main__ -   Batch number = 190
Evaluating:  95%|█████████▍| 190/201 [00:33<00:01,  6.33it/s]11/28/2021 01:22:31 - INFO - __main__ -   Batch number = 191
Evaluating:  95%|█████████▌| 191/201 [00:33<00:01,  5.89it/s]11/28/2021 01:22:31 - INFO - __main__ -   Batch number = 192
Evaluating:  96%|█████████▌| 192/201 [00:34<00:03,  2.57it/s]11/28/2021 01:22:32 - INFO - __main__ -   Batch number = 193
Evaluating:  96%|█████████▌| 193/201 [00:35<00:02,  2.74it/s]11/28/2021 01:22:32 - INFO - __main__ -   Batch number = 194
Evaluating:  97%|█████████▋| 194/201 [00:35<00:02,  2.84it/s]11/28/2021 01:22:32 - INFO - __main__ -   Batch number = 195
Evaluating:  97%|█████████▋| 195/201 [00:35<00:02,  2.91it/s]11/28/2021 01:22:33 - INFO - __main__ -   Batch number = 196
Evaluating:  98%|█████████▊| 196/201 [00:36<00:01,  2.92it/s]11/28/2021 01:22:33 - INFO - __main__ -   Batch number = 197
Evaluating:  98%|█████████▊| 197/201 [00:36<00:01,  3.02it/s]11/28/2021 01:22:33 - INFO - __main__ -   Batch number = 198
Evaluating:  99%|█████████▊| 198/201 [00:36<00:01,  2.57it/s]11/28/2021 01:22:34 - INFO - __main__ -   Batch number = 199
Evaluating:  99%|█████████▉| 199/201 [00:37<00:00,  2.71it/s]11/28/2021 01:22:34 - INFO - __main__ -   Batch number = 200
Evaluating: 100%|█████████▉| 200/201 [00:37<00:00,  2.80it/s]11/28/2021 01:22:34 - INFO - __main__ -   Batch number = 201
Evaluating: 100%|██████████| 201/201 [00:37<00:00,  5.35it/s]
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:22:39 - INFO - __main__ -   ***** Evaluation result  in is *****
11/28/2021 01:22:39 - INFO - __main__ -     f1 = 0.7290210607122594
11/28/2021 01:22:39 - INFO - __main__ -     loss = 0.9863566989032784
11/28/2021 01:22:39 - INFO - __main__ -     precision = 0.735749654731027
11/28/2021 01:22:39 - INFO - __main__ -     recall = 0.722414420373739
47.37user 15.57system 1:03.48elapsed 99%CPU (0avgtext+0avgdata 3956104maxresident)k
8inputs+1720outputs (0major+1501101minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:22:42 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='is', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:22:42 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:22:42 - INFO - __main__ -   Seed = 3
11/28/2021 01:22:42 - INFO - root -   save model
11/28/2021 01:22:42 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='is', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:22:42 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:22:45 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:22:51 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:22:51 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:22:51 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
11/28/2021 01:22:51 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:22:51 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:22:51 - INFO - root -   loading lang adpater am/wiki@ukp
11/28/2021 01:22:51 - INFO - __main__ -   Adapter Languages : ['am'], Length : 1
11/28/2021 01:22:51 - INFO - __main__ -   Adapter Names ['am/wiki@ukp'], Length : 1
11/28/2021 01:22:51 - INFO - __main__ -   Language = am
11/28/2021 01:22:51 - INFO - __main__ -   Adapter Name = am/wiki@ukp
Found matching adapter at: adapters/ukp/bert-base-multilingual-cased_am_wiki_pfeiffer.json
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/am/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_am_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/adapter_config.json
Adding adapter 'am' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/head_config.json
11/28/2021 01:22:58 - INFO - __main__ -   Language adapter for is not found, using am instead
11/28/2021 01:22:58 - INFO - __main__ -   Set active language adapter to am
11/28/2021 01:22:58 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:22:58 - INFO - __main__ -   Adapter Languages = ['am']
11/28/2021 01:22:58 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_is_bert-base-multilingual-cased_128
11/28/2021 01:22:59 - INFO - __main__ -   ***** Running evaluation  in is *****
11/28/2021 01:22:59 - INFO - __main__ -     Num examples = 6401
11/28/2021 01:22:59 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/201 [00:00<?, ?it/s]11/28/2021 01:22:59 - INFO - __main__ -   Batch number = 1
Evaluating:   0%|          | 1/201 [00:00<00:33,  6.04it/s]11/28/2021 01:22:59 - INFO - __main__ -   Batch number = 2
Evaluating:   1%|          | 2/201 [00:00<00:30,  6.52it/s]11/28/2021 01:22:59 - INFO - __main__ -   Batch number = 3
Evaluating:   1%|▏         | 3/201 [00:00<00:29,  6.74it/s]11/28/2021 01:22:59 - INFO - __main__ -   Batch number = 4
Evaluating:   2%|▏         | 4/201 [00:00<00:28,  6.92it/s]11/28/2021 01:22:59 - INFO - __main__ -   Batch number = 5
Evaluating:   2%|▏         | 5/201 [00:00<00:27,  7.03it/s]11/28/2021 01:22:59 - INFO - __main__ -   Batch number = 6
Evaluating:   3%|▎         | 6/201 [00:00<00:27,  7.10it/s]11/28/2021 01:23:00 - INFO - __main__ -   Batch number = 7
Evaluating:   3%|▎         | 7/201 [00:01<00:27,  7.13it/s]11/28/2021 01:23:00 - INFO - __main__ -   Batch number = 8
Evaluating:   4%|▍         | 8/201 [00:01<00:33,  5.76it/s]11/28/2021 01:23:00 - INFO - __main__ -   Batch number = 9
Evaluating:   4%|▍         | 9/201 [00:01<00:39,  4.83it/s]11/28/2021 01:23:00 - INFO - __main__ -   Batch number = 10
Evaluating:   5%|▍         | 10/201 [00:01<00:43,  4.35it/s]11/28/2021 01:23:00 - INFO - __main__ -   Batch number = 11
Evaluating:   5%|▌         | 11/201 [00:02<00:46,  4.07it/s]11/28/2021 01:23:01 - INFO - __main__ -   Batch number = 12
Evaluating:   6%|▌         | 12/201 [00:02<00:48,  3.90it/s]11/28/2021 01:23:01 - INFO - __main__ -   Batch number = 13
Evaluating:   6%|▋         | 13/201 [00:02<00:49,  3.82it/s]11/28/2021 01:23:01 - INFO - __main__ -   Batch number = 14
Evaluating:   7%|▋         | 14/201 [00:02<00:50,  3.73it/s]11/28/2021 01:23:02 - INFO - __main__ -   Batch number = 15
Evaluating:   7%|▋         | 15/201 [00:03<00:50,  3.68it/s]11/28/2021 01:23:02 - INFO - __main__ -   Batch number = 16
Evaluating:   8%|▊         | 16/201 [00:03<00:50,  3.65it/s]11/28/2021 01:23:02 - INFO - __main__ -   Batch number = 17
Evaluating:   8%|▊         | 17/201 [00:03<00:50,  3.63it/s]11/28/2021 01:23:02 - INFO - __main__ -   Batch number = 18
Evaluating:   9%|▉         | 18/201 [00:04<00:50,  3.60it/s]11/28/2021 01:23:03 - INFO - __main__ -   Batch number = 19
Evaluating:   9%|▉         | 19/201 [00:04<00:50,  3.58it/s]11/28/2021 01:23:03 - INFO - __main__ -   Batch number = 20
Evaluating:  10%|▉         | 20/201 [00:04<00:50,  3.59it/s]11/28/2021 01:23:03 - INFO - __main__ -   Batch number = 21
Evaluating:  10%|█         | 21/201 [00:04<00:50,  3.58it/s]11/28/2021 01:23:04 - INFO - __main__ -   Batch number = 22
Evaluating:  11%|█         | 22/201 [00:05<00:50,  3.57it/s]11/28/2021 01:23:04 - INFO - __main__ -   Batch number = 23
Evaluating:  11%|█▏        | 23/201 [00:05<00:49,  3.56it/s]11/28/2021 01:23:04 - INFO - __main__ -   Batch number = 24
Evaluating:  12%|█▏        | 24/201 [00:05<00:49,  3.54it/s]11/28/2021 01:23:04 - INFO - __main__ -   Batch number = 25
Evaluating:  12%|█▏        | 25/201 [00:06<00:49,  3.55it/s]11/28/2021 01:23:05 - INFO - __main__ -   Batch number = 26
Evaluating:  13%|█▎        | 26/201 [00:06<00:49,  3.53it/s]11/28/2021 01:23:05 - INFO - __main__ -   Batch number = 27
Evaluating:  13%|█▎        | 27/201 [00:06<00:49,  3.50it/s]11/28/2021 01:23:05 - INFO - __main__ -   Batch number = 28
Evaluating:  14%|█▍        | 28/201 [00:06<00:49,  3.48it/s]11/28/2021 01:23:06 - INFO - __main__ -   Batch number = 29
Evaluating:  14%|█▍        | 29/201 [00:07<00:49,  3.48it/s]11/28/2021 01:23:06 - INFO - __main__ -   Batch number = 30
Evaluating:  15%|█▍        | 30/201 [00:07<00:49,  3.47it/s]11/28/2021 01:23:06 - INFO - __main__ -   Batch number = 31
Evaluating:  15%|█▌        | 31/201 [00:07<00:49,  3.46it/s]11/28/2021 01:23:06 - INFO - __main__ -   Batch number = 32
Evaluating:  16%|█▌        | 32/201 [00:08<00:48,  3.47it/s]11/28/2021 01:23:07 - INFO - __main__ -   Batch number = 33
Evaluating:  16%|█▋        | 33/201 [00:08<00:48,  3.47it/s]11/28/2021 01:23:07 - INFO - __main__ -   Batch number = 34
Evaluating:  17%|█▋        | 34/201 [00:08<00:47,  3.52it/s]11/28/2021 01:23:07 - INFO - __main__ -   Batch number = 35
Evaluating:  17%|█▋        | 35/201 [00:08<00:46,  3.53it/s]11/28/2021 01:23:08 - INFO - __main__ -   Batch number = 36
Evaluating:  18%|█▊        | 36/201 [00:09<00:45,  3.62it/s]11/28/2021 01:23:08 - INFO - __main__ -   Batch number = 37
Evaluating:  18%|█▊        | 37/201 [00:09<00:45,  3.61it/s]11/28/2021 01:23:08 - INFO - __main__ -   Batch number = 38
Evaluating:  19%|█▉        | 38/201 [00:09<00:45,  3.58it/s]11/28/2021 01:23:08 - INFO - __main__ -   Batch number = 39
Evaluating:  19%|█▉        | 39/201 [00:09<00:45,  3.58it/s]11/28/2021 01:23:09 - INFO - __main__ -   Batch number = 40
Evaluating:  20%|█▉        | 40/201 [00:10<00:44,  3.58it/s]11/28/2021 01:23:09 - INFO - __main__ -   Batch number = 41
Evaluating:  20%|██        | 41/201 [00:10<00:44,  3.57it/s]11/28/2021 01:23:09 - INFO - __main__ -   Batch number = 42
Evaluating:  21%|██        | 42/201 [00:10<00:44,  3.56it/s]11/28/2021 01:23:10 - INFO - __main__ -   Batch number = 43
Evaluating:  21%|██▏       | 43/201 [00:11<00:44,  3.54it/s]11/28/2021 01:23:10 - INFO - __main__ -   Batch number = 44
Evaluating:  22%|██▏       | 44/201 [00:11<00:44,  3.49it/s]11/28/2021 01:23:10 - INFO - __main__ -   Batch number = 45
Evaluating:  22%|██▏       | 45/201 [00:11<00:45,  3.44it/s]11/28/2021 01:23:10 - INFO - __main__ -   Batch number = 46
Evaluating:  23%|██▎       | 46/201 [00:12<00:45,  3.42it/s]11/28/2021 01:23:11 - INFO - __main__ -   Batch number = 47
Evaluating:  23%|██▎       | 47/201 [00:12<00:45,  3.36it/s]11/28/2021 01:23:11 - INFO - __main__ -   Batch number = 48
Evaluating:  24%|██▍       | 48/201 [00:12<00:46,  3.31it/s]11/28/2021 01:23:11 - INFO - __main__ -   Batch number = 49
Evaluating:  24%|██▍       | 49/201 [00:12<00:46,  3.29it/s]11/28/2021 01:23:12 - INFO - __main__ -   Batch number = 50
Evaluating:  25%|██▍       | 50/201 [00:13<00:46,  3.25it/s]11/28/2021 01:23:12 - INFO - __main__ -   Batch number = 51
Evaluating:  25%|██▌       | 51/201 [00:13<00:46,  3.25it/s]11/28/2021 01:23:12 - INFO - __main__ -   Batch number = 52
Evaluating:  26%|██▌       | 52/201 [00:13<00:46,  3.21it/s]11/28/2021 01:23:13 - INFO - __main__ -   Batch number = 53
Evaluating:  26%|██▋       | 53/201 [00:14<00:46,  3.20it/s]11/28/2021 01:23:13 - INFO - __main__ -   Batch number = 54
Evaluating:  27%|██▋       | 54/201 [00:14<00:46,  3.19it/s]11/28/2021 01:23:13 - INFO - __main__ -   Batch number = 55
Evaluating:  27%|██▋       | 55/201 [00:14<00:46,  3.17it/s]11/28/2021 01:23:14 - INFO - __main__ -   Batch number = 56
Evaluating:  28%|██▊       | 56/201 [00:15<00:45,  3.17it/s]11/28/2021 01:23:14 - INFO - __main__ -   Batch number = 57
Evaluating:  28%|██▊       | 57/201 [00:15<00:45,  3.17it/s]11/28/2021 01:23:14 - INFO - __main__ -   Batch number = 58
Evaluating:  29%|██▉       | 58/201 [00:15<00:45,  3.15it/s]11/28/2021 01:23:14 - INFO - __main__ -   Batch number = 59
Evaluating:  29%|██▉       | 59/201 [00:16<00:44,  3.16it/s]11/28/2021 01:23:15 - INFO - __main__ -   Batch number = 60
Evaluating:  30%|██▉       | 60/201 [00:16<00:44,  3.19it/s]11/28/2021 01:23:15 - INFO - __main__ -   Batch number = 61
Evaluating:  30%|███       | 61/201 [00:16<00:37,  3.72it/s]11/28/2021 01:23:15 - INFO - __main__ -   Batch number = 62
Evaluating:  31%|███       | 62/201 [00:16<00:39,  3.53it/s]11/28/2021 01:23:16 - INFO - __main__ -   Batch number = 63
Evaluating:  31%|███▏      | 63/201 [00:17<00:39,  3.52it/s]11/28/2021 01:23:16 - INFO - __main__ -   Batch number = 64
Evaluating:  32%|███▏      | 64/201 [00:17<00:39,  3.46it/s]11/28/2021 01:23:16 - INFO - __main__ -   Batch number = 65
Evaluating:  32%|███▏      | 65/201 [00:17<00:39,  3.41it/s]11/28/2021 01:23:16 - INFO - __main__ -   Batch number = 66
PyTorch version 1.10.0+cu102 available.
Evaluating:  33%|███▎      | 66/201 [00:18<00:40,  3.36it/s]11/28/2021 01:23:17 - INFO - __main__ -   Batch number = 67
Evaluating:  33%|███▎      | 67/201 [00:18<00:36,  3.64it/s]11/28/2021 01:23:17 - INFO - __main__ -   Batch number = 68
11/28/2021 01:23:17 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:23:17 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:23:17 - INFO - __main__ -   Seed = 1
11/28/2021 01:23:17 - INFO - root -   save model
11/28/2021 01:23:17 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:23:17 - INFO - __main__ -   Loading pretrained model and tokenizer
Evaluating:  34%|███▍      | 68/201 [00:18<00:31,  4.19it/s]11/28/2021 01:23:17 - INFO - __main__ -   Batch number = 69
Evaluating:  34%|███▍      | 69/201 [00:18<00:27,  4.74it/s]11/28/2021 01:23:17 - INFO - __main__ -   Batch number = 70
Evaluating:  35%|███▍      | 70/201 [00:18<00:25,  5.21it/s]11/28/2021 01:23:17 - INFO - __main__ -   Batch number = 71
Evaluating:  35%|███▌      | 71/201 [00:18<00:23,  5.62it/s]11/28/2021 01:23:18 - INFO - __main__ -   Batch number = 72
Evaluating:  36%|███▌      | 72/201 [00:19<00:21,  5.95it/s]11/28/2021 01:23:18 - INFO - __main__ -   Batch number = 73
Evaluating:  36%|███▋      | 73/201 [00:19<00:20,  6.21it/s]11/28/2021 01:23:18 - INFO - __main__ -   Batch number = 74
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  37%|███▋      | 74/201 [00:19<00:19,  6.46it/s]11/28/2021 01:23:18 - INFO - __main__ -   Batch number = 75
Evaluating:  37%|███▋      | 75/201 [00:19<00:18,  6.65it/s]11/28/2021 01:23:18 - INFO - __main__ -   Batch number = 76
Evaluating:  38%|███▊      | 76/201 [00:19<00:18,  6.78it/s]11/28/2021 01:23:18 - INFO - __main__ -   Batch number = 77
Evaluating:  38%|███▊      | 77/201 [00:19<00:18,  6.89it/s]11/28/2021 01:23:18 - INFO - __main__ -   Batch number = 78
Evaluating:  39%|███▉      | 78/201 [00:19<00:17,  6.96it/s]11/28/2021 01:23:19 - INFO - __main__ -   Batch number = 79
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  39%|███▉      | 79/201 [00:20<00:17,  7.01it/s]11/28/2021 01:23:19 - INFO - __main__ -   Batch number = 80
Evaluating:  40%|███▉      | 80/201 [00:20<00:17,  7.05it/s]11/28/2021 01:23:19 - INFO - __main__ -   Batch number = 81
Evaluating:  40%|████      | 81/201 [00:20<00:16,  7.09it/s]11/28/2021 01:23:19 - INFO - __main__ -   Batch number = 82
Evaluating:  41%|████      | 82/201 [00:20<00:16,  7.10it/s]11/28/2021 01:23:19 - INFO - __main__ -   Batch number = 83
Evaluating:  41%|████▏     | 83/201 [00:20<00:16,  7.12it/s]11/28/2021 01:23:19 - INFO - __main__ -   Batch number = 84
Evaluating:  42%|████▏     | 84/201 [00:20<00:16,  7.15it/s]11/28/2021 01:23:19 - INFO - __main__ -   Batch number = 85
loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
Evaluating:  42%|████▏     | 85/201 [00:20<00:16,  7.04it/s]11/28/2021 01:23:20 - INFO - __main__ -   Batch number = 86
11/28/2021 01:23:20 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
Evaluating:  43%|████▎     | 86/201 [00:21<00:16,  7.03it/s]11/28/2021 01:23:20 - INFO - __main__ -   Batch number = 87
Evaluating:  43%|████▎     | 87/201 [00:21<00:16,  6.98it/s]11/28/2021 01:23:20 - INFO - __main__ -   Batch number = 88
Evaluating:  44%|████▍     | 88/201 [00:21<00:16,  6.92it/s]11/28/2021 01:23:20 - INFO - __main__ -   Batch number = 89
Evaluating:  44%|████▍     | 89/201 [00:21<00:16,  6.86it/s]11/28/2021 01:23:20 - INFO - __main__ -   Batch number = 90
Evaluating:  45%|████▍     | 90/201 [00:21<00:21,  5.23it/s]11/28/2021 01:23:20 - INFO - __main__ -   Batch number = 91
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  45%|████▌     | 91/201 [00:21<00:19,  5.60it/s]11/28/2021 01:23:21 - INFO - __main__ -   Batch number = 92
Evaluating:  46%|████▌     | 92/201 [00:22<00:18,  5.91it/s]11/28/2021 01:23:21 - INFO - __main__ -   Batch number = 93
Evaluating:  46%|████▋     | 93/201 [00:22<00:17,  6.13it/s]11/28/2021 01:23:21 - INFO - __main__ -   Batch number = 94
Evaluating:  47%|████▋     | 94/201 [00:22<00:17,  6.29it/s]11/28/2021 01:23:21 - INFO - __main__ -   Batch number = 95
Evaluating:  47%|████▋     | 95/201 [00:22<00:16,  6.41it/s]11/28/2021 01:23:21 - INFO - __main__ -   Batch number = 96
Evaluating:  48%|████▊     | 96/201 [00:22<00:16,  6.50it/s]11/28/2021 01:23:21 - INFO - __main__ -   Batch number = 97
Evaluating:  48%|████▊     | 97/201 [00:22<00:15,  6.61it/s]11/28/2021 01:23:21 - INFO - __main__ -   Batch number = 98
Evaluating:  49%|████▉     | 98/201 [00:22<00:15,  6.64it/s]11/28/2021 01:23:22 - INFO - __main__ -   Batch number = 99
Evaluating:  49%|████▉     | 99/201 [00:23<00:15,  6.64it/s]11/28/2021 01:23:22 - INFO - __main__ -   Batch number = 100
Evaluating:  50%|████▉     | 100/201 [00:23<00:15,  6.62it/s]11/28/2021 01:23:22 - INFO - __main__ -   Batch number = 101
Evaluating:  50%|█████     | 101/201 [00:23<00:15,  6.61it/s]11/28/2021 01:23:22 - INFO - __main__ -   Batch number = 102
Evaluating:  51%|█████     | 102/201 [00:23<00:15,  6.58it/s]11/28/2021 01:23:22 - INFO - __main__ -   Batch number = 103
Evaluating:  51%|█████     | 103/201 [00:23<00:14,  6.59it/s]11/28/2021 01:23:22 - INFO - __main__ -   Batch number = 104
Evaluating:  52%|█████▏    | 104/201 [00:23<00:14,  6.62it/s]11/28/2021 01:23:23 - INFO - __main__ -   Batch number = 105
Evaluating:  52%|█████▏    | 105/201 [00:24<00:14,  6.62it/s]11/28/2021 01:23:23 - INFO - __main__ -   Batch number = 106
Evaluating:  53%|█████▎    | 106/201 [00:24<00:14,  6.62it/s]11/28/2021 01:23:23 - INFO - __main__ -   Batch number = 107
Evaluating:  53%|█████▎    | 107/201 [00:24<00:14,  6.63it/s]11/28/2021 01:23:23 - INFO - __main__ -   Batch number = 108
Evaluating:  54%|█████▎    | 108/201 [00:24<00:13,  6.66it/s]11/28/2021 01:23:23 - INFO - __main__ -   Batch number = 109
Evaluating:  54%|█████▍    | 109/201 [00:24<00:14,  6.50it/s]11/28/2021 01:23:23 - INFO - __main__ -   Batch number = 110
Evaluating:  55%|█████▍    | 110/201 [00:24<00:13,  6.53it/s]11/28/2021 01:23:23 - INFO - __main__ -   Batch number = 111
Evaluating:  55%|█████▌    | 111/201 [00:24<00:13,  6.54it/s]11/28/2021 01:23:24 - INFO - __main__ -   Batch number = 112
Evaluating:  56%|█████▌    | 112/201 [00:25<00:13,  6.52it/s]11/28/2021 01:23:24 - INFO - __main__ -   Batch number = 113
Evaluating:  56%|█████▌    | 113/201 [00:25<00:13,  6.45it/s]11/28/2021 01:23:24 - INFO - __main__ -   Batch number = 114
Evaluating:  57%|█████▋    | 114/201 [00:25<00:13,  6.49it/s]11/28/2021 01:23:24 - INFO - __main__ -   Batch number = 115
Evaluating:  57%|█████▋    | 115/201 [00:25<00:13,  6.54it/s]11/28/2021 01:23:24 - INFO - __main__ -   Batch number = 116
Evaluating:  58%|█████▊    | 116/201 [00:25<00:12,  6.57it/s]11/28/2021 01:23:24 - INFO - __main__ -   Batch number = 117
Evaluating:  58%|█████▊    | 117/201 [00:25<00:12,  6.55it/s]11/28/2021 01:23:25 - INFO - __main__ -   Batch number = 118
Evaluating:  59%|█████▊    | 118/201 [00:26<00:12,  6.54it/s]11/28/2021 01:23:25 - INFO - __main__ -   Batch number = 119
Evaluating:  59%|█████▉    | 119/201 [00:26<00:12,  6.49it/s]11/28/2021 01:23:25 - INFO - __main__ -   Batch number = 120
Evaluating:  60%|█████▉    | 120/201 [00:26<00:12,  6.46it/s]11/28/2021 01:23:25 - INFO - __main__ -   Batch number = 121
Evaluating:  60%|██████    | 121/201 [00:26<00:12,  6.45it/s]11/28/2021 01:23:25 - INFO - __main__ -   Batch number = 122
Evaluating:  61%|██████    | 122/201 [00:26<00:15,  5.01it/s]11/28/2021 01:23:25 - INFO - __main__ -   Batch number = 123
Evaluating:  61%|██████    | 123/201 [00:26<00:14,  5.22it/s]11/28/2021 01:23:26 - INFO - __main__ -   Batch number = 124
Evaluating:  62%|██████▏   | 124/201 [00:27<00:14,  5.40it/s]11/28/2021 01:23:26 - INFO - __main__ -   Batch number = 125
Evaluating:  62%|██████▏   | 125/201 [00:27<00:13,  5.63it/s]11/28/2021 01:23:26 - INFO - __main__ -   Batch number = 126
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:23:26 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:23:26 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:23:26 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
11/28/2021 01:23:26 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:23:26 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_adapter.bin
Evaluating:  63%|██████▎   | 126/201 [00:27<00:12,  5.79it/s]11/28/2021 01:23:26 - INFO - __main__ -   Batch number = 127
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:23:26 - INFO - root -   loading lang adpater am/wiki@ukp
11/28/2021 01:23:26 - INFO - __main__ -   Adapter Languages : ['am'], Length : 1
11/28/2021 01:23:26 - INFO - __main__ -   Adapter Names ['am/wiki@ukp'], Length : 1
11/28/2021 01:23:26 - INFO - __main__ -   Language = am
11/28/2021 01:23:26 - INFO - __main__ -   Adapter Name = am/wiki@ukp
Found matching adapter at: adapters/ukp/bert-base-multilingual-cased_am_wiki_pfeiffer.json
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/am/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_am_wiki_pfeiffer.zip.
Evaluating:  63%|██████▎   | 127/201 [00:27<00:12,  5.97it/s]11/28/2021 01:23:26 - INFO - __main__ -   Batch number = 128
Evaluating:  64%|██████▎   | 128/201 [00:27<00:11,  6.15it/s]11/28/2021 01:23:26 - INFO - __main__ -   Batch number = 129
Evaluating:  64%|██████▍   | 129/201 [00:27<00:11,  6.32it/s]11/28/2021 01:23:27 - INFO - __main__ -   Batch number = 130
Evaluating:  65%|██████▍   | 130/201 [00:28<00:11,  6.40it/s]11/28/2021 01:23:27 - INFO - __main__ -   Batch number = 131
Evaluating:  65%|██████▌   | 131/201 [00:28<00:10,  6.44it/s]11/28/2021 01:23:27 - INFO - __main__ -   Batch number = 132
Evaluating:  66%|██████▌   | 132/201 [00:28<00:10,  6.44it/s]11/28/2021 01:23:27 - INFO - __main__ -   Batch number = 133
Evaluating:  66%|██████▌   | 133/201 [00:28<00:10,  6.50it/s]11/28/2021 01:23:27 - INFO - __main__ -   Batch number = 134
Evaluating:  67%|██████▋   | 134/201 [00:28<00:10,  6.53it/s]11/28/2021 01:23:27 - INFO - __main__ -   Batch number = 135
Evaluating:  67%|██████▋   | 135/201 [00:28<00:10,  6.54it/s]11/28/2021 01:23:27 - INFO - __main__ -   Batch number = 136
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/adapter_config.json
Adding adapter 'am' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/pytorch_adapter.bin
Evaluating:  68%|██████▊   | 136/201 [00:28<00:09,  6.58it/s]11/28/2021 01:23:28 - INFO - __main__ -   Batch number = 137
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/head_config.json
Evaluating:  68%|██████▊   | 137/201 [00:29<00:09,  6.60it/s]11/28/2021 01:23:28 - INFO - __main__ -   Batch number = 138
Evaluating:  69%|██████▊   | 138/201 [00:29<00:09,  6.59it/s]11/28/2021 01:23:28 - INFO - __main__ -   Batch number = 139
Evaluating:  69%|██████▉   | 139/201 [00:29<00:09,  6.49it/s]11/28/2021 01:23:28 - INFO - __main__ -   Batch number = 140
Evaluating:  70%|██████▉   | 140/201 [00:29<00:09,  6.44it/s]11/28/2021 01:23:28 - INFO - __main__ -   Batch number = 141
Evaluating:  70%|███████   | 141/201 [00:29<00:09,  6.26it/s]11/28/2021 01:23:28 - INFO - __main__ -   Batch number = 142
Evaluating:  71%|███████   | 142/201 [00:29<00:09,  6.17it/s]11/28/2021 01:23:29 - INFO - __main__ -   Batch number = 143
Evaluating:  71%|███████   | 143/201 [00:30<00:09,  6.10it/s]11/28/2021 01:23:29 - INFO - __main__ -   Batch number = 144
Evaluating:  72%|███████▏  | 144/201 [00:30<00:09,  6.04it/s]11/28/2021 01:23:29 - INFO - __main__ -   Batch number = 145
Evaluating:  72%|███████▏  | 145/201 [00:30<00:09,  6.04it/s]11/28/2021 01:23:29 - INFO - __main__ -   Batch number = 146
Evaluating:  73%|███████▎  | 146/201 [00:30<00:09,  6.09it/s]11/28/2021 01:23:29 - INFO - __main__ -   Batch number = 147
Evaluating:  73%|███████▎  | 147/201 [00:30<00:08,  6.15it/s]11/28/2021 01:23:29 - INFO - __main__ -   Batch number = 148
Evaluating:  74%|███████▎  | 148/201 [00:30<00:08,  6.18it/s]11/28/2021 01:23:30 - INFO - __main__ -   Batch number = 149
Evaluating:  74%|███████▍  | 149/201 [00:31<00:08,  6.25it/s]11/28/2021 01:23:30 - INFO - __main__ -   Batch number = 150
Evaluating:  75%|███████▍  | 150/201 [00:31<00:08,  6.28it/s]11/28/2021 01:23:30 - INFO - __main__ -   Batch number = 151
Evaluating:  75%|███████▌  | 151/201 [00:31<00:08,  6.09it/s]11/28/2021 01:23:30 - INFO - __main__ -   Batch number = 152
Evaluating:  76%|███████▌  | 152/201 [00:31<00:08,  6.04it/s]11/28/2021 01:23:30 - INFO - __main__ -   Batch number = 153
Evaluating:  76%|███████▌  | 153/201 [00:31<00:09,  5.33it/s]11/28/2021 01:23:30 - INFO - __main__ -   Batch number = 154
Evaluating:  77%|███████▋  | 154/201 [00:31<00:08,  5.61it/s]11/28/2021 01:23:31 - INFO - __main__ -   Batch number = 155
Evaluating:  77%|███████▋  | 155/201 [00:32<00:07,  5.80it/s]11/28/2021 01:23:31 - INFO - __main__ -   Batch number = 156
Evaluating:  78%|███████▊  | 156/201 [00:32<00:07,  5.97it/s]11/28/2021 01:23:31 - INFO - __main__ -   Batch number = 157
Evaluating:  78%|███████▊  | 157/201 [00:32<00:07,  6.10it/s]11/28/2021 01:23:31 - INFO - __main__ -   Batch number = 158
Evaluating:  79%|███████▊  | 158/201 [00:32<00:06,  6.15it/s]11/28/2021 01:23:31 - INFO - __main__ -   Batch number = 159
Evaluating:  79%|███████▉  | 159/201 [00:32<00:06,  6.17it/s]11/28/2021 01:23:31 - INFO - __main__ -   Batch number = 160
Evaluating:  80%|███████▉  | 160/201 [00:32<00:06,  6.15it/s]11/28/2021 01:23:32 - INFO - __main__ -   Batch number = 161
Evaluating:  80%|████████  | 161/201 [00:33<00:06,  5.86it/s]11/28/2021 01:23:32 - INFO - __main__ -   Batch number = 162
Evaluating:  81%|████████  | 162/201 [00:33<00:06,  5.82it/s]11/28/2021 01:23:32 - INFO - __main__ -   Batch number = 163
Evaluating:  81%|████████  | 163/201 [00:33<00:06,  5.43it/s]11/28/2021 01:23:32 - INFO - __main__ -   Batch number = 164
Evaluating:  82%|████████▏ | 164/201 [00:33<00:06,  5.32it/s]11/28/2021 01:23:32 - INFO - __main__ -   Batch number = 165
Evaluating:  82%|████████▏ | 165/201 [00:34<00:08,  4.02it/s]11/28/2021 01:23:33 - INFO - __main__ -   Batch number = 166
Evaluating:  83%|████████▎ | 166/201 [00:34<00:07,  4.45it/s]11/28/2021 01:23:33 - INFO - __main__ -   Batch number = 167
Evaluating:  83%|████████▎ | 167/201 [00:34<00:07,  4.83it/s]11/28/2021 01:23:33 - INFO - __main__ -   Batch number = 168
Evaluating:  84%|████████▎ | 168/201 [00:34<00:06,  5.18it/s]11/28/2021 01:23:33 - INFO - __main__ -   Batch number = 169
Evaluating:  84%|████████▍ | 169/201 [00:34<00:05,  5.46it/s]11/28/2021 01:23:33 - INFO - __main__ -   Batch number = 170
Evaluating:  85%|████████▍ | 170/201 [00:34<00:05,  5.60it/s]11/28/2021 01:23:34 - INFO - __main__ -   Batch number = 171
Evaluating:  85%|████████▌ | 171/201 [00:35<00:05,  5.74it/s]11/28/2021 01:23:34 - INFO - __main__ -   Batch number = 172
Evaluating:  86%|████████▌ | 172/201 [00:35<00:04,  5.86it/s]11/28/2021 01:23:34 - INFO - __main__ -   Batch number = 173
Evaluating:  86%|████████▌ | 173/201 [00:35<00:04,  5.62it/s]11/28/2021 01:23:34 - INFO - __main__ -   Batch number = 174
Evaluating:  87%|████████▋ | 174/201 [00:35<00:04,  5.75it/s]11/28/2021 01:23:34 - INFO - __main__ -   Batch number = 175
Evaluating:  87%|████████▋ | 175/201 [00:35<00:04,  5.87it/s]11/28/2021 01:23:34 - INFO - __main__ -   Batch number = 176
Evaluating:  88%|████████▊ | 176/201 [00:35<00:04,  5.93it/s]11/28/2021 01:23:35 - INFO - __main__ -   Batch number = 177
Evaluating:  88%|████████▊ | 177/201 [00:36<00:04,  5.95it/s]11/28/2021 01:23:35 - INFO - __main__ -   Batch number = 178
Evaluating:  89%|████████▊ | 178/201 [00:36<00:03,  6.01it/s]11/28/2021 01:23:35 - INFO - __main__ -   Batch number = 179
11/28/2021 01:23:35 - INFO - __main__ -   Language adapter for fi not found, using am instead
11/28/2021 01:23:35 - INFO - __main__ -   Set active language adapter to am
11/28/2021 01:23:35 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:23:35 - INFO - __main__ -   Adapter Languages = ['am']
11/28/2021 01:23:35 - INFO - __main__ -   all languages = fi
11/28/2021 01:23:35 - INFO - __main__ -   Creating features from dataset file at /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/fi/test.bert-base-multilingual-cased in language fi
11/28/2021 01:23:35 - INFO - utils_tag -   lang_id=0, lang=fi, lang2id=None
Evaluating:  89%|████████▉ | 179/201 [00:36<00:03,  6.01it/s]11/28/2021 01:23:35 - INFO - __main__ -   Batch number = 180
Evaluating:  90%|████████▉ | 180/201 [00:36<00:03,  5.90it/s]11/28/2021 01:23:35 - INFO - __main__ -   Batch number = 181
11/28/2021 01:23:35 - INFO - utils_tag -   Writing example 0 of 6550
11/28/2021 01:23:35 - INFO - utils_tag -   *** Example ***
11/28/2021 01:23:35 - INFO - utils_tag -   guid: fi-1
11/28/2021 01:23:35 - INFO - utils_tag -   tokens: [CLS] kun sai ##s t ##än gr ##oba ##n kun ##toon [SEP]
11/28/2021 01:23:35 - INFO - utils_tag -   input_ids: 101 13158 13410 10107 188 12801 30518 45717 10115 13158 60360 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:23:35 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:23:35 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:23:35 - INFO - utils_tag -   label_ids: -100 14 16 -100 6 -100 8 -100 -100 3 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
11/28/2021 01:23:35 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
11/28/2021 01:23:35 - INFO - utils_tag -   *** Example ***
11/28/2021 01:23:35 - INFO - utils_tag -   guid: fi-2
11/28/2021 01:23:35 - INFO - utils_tag -   tokens: [CLS] Ole ##t maa ##mme arma ##hin Suomen ##maa [SEP]
11/28/2021 01:23:35 - INFO - utils_tag -   input_ids: 101 31482 10123 49123 21209 28758 14383 15759 20528 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:23:35 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:23:35 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:23:35 - INFO - utils_tag -   label_ids: -100 4 -100 8 -100 1 -100 12 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
11/28/2021 01:23:35 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
11/28/2021 01:23:35 - INFO - utils_tag -   *** Example ***
11/28/2021 01:23:35 - INFO - utils_tag -   guid: fi-3
11/28/2021 01:23:35 - INFO - utils_tag -   tokens: [CLS] ei mu ##lle mi ##kk ##ään p ##ään ##sä ##rky ole , jos jo ##ku mua ih ##hai ##lee . [SEP]
11/28/2021 01:23:35 - INFO - utils_tag -   input_ids: 101 10805 12361 11270 12132 20024 14928 184 14928 14934 77576 17409 117 21705 12541 10853 56944 17736 37821 30188 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:23:35 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:23:35 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:23:35 - INFO - utils_tag -   label_ids: -100 4 11 -100 6 -100 -100 8 -100 -100 -100 16 13 14 11 -100 11 16 -100 -100 13 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
11/28/2021 01:23:35 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
11/28/2021 01:23:35 - INFO - utils_tag -   *** Example ***
11/28/2021 01:23:35 - INFO - utils_tag -   guid: fi-4
11/28/2021 01:23:35 - INFO - utils_tag -   tokens: [CLS] ei se o ##o , sillä ta ##vala , k ##ä ##yny ##p pit ##kk ##ää ai ##kkaa ##v var ##mma ##am mon ##neen kuu ##kka ##utt ##een ##kaa . [SEP]
11/28/2021 01:23:35 - INFO - utils_tag -   input_ids: 101 10805 10126 183 10133 117 27289 11057 19006 117 179 11013 83545 10410 55277 20024 18534 11346 63305 10477 10299 21936 11008 34372 37250 107591 20878 66117 13129 62266 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:23:35 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:23:35 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:23:35 - INFO - utils_tag -   label_ids: -100 4 11 4 -100 13 6 8 -100 13 16 -100 -100 -100 1 -100 -100 8 -100 -100 10 -100 -100 6 -100 8 -100 -100 -100 -100 13 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
11/28/2021 01:23:35 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
11/28/2021 01:23:35 - INFO - utils_tag -   *** Example ***
11/28/2021 01:23:35 - INFO - utils_tag -   guid: fi-5
11/28/2021 01:23:35 - INFO - utils_tag -   tokens: [CLS] Ole vi ##im ##äs ##es p ##ää ##s ens ##mä ##ine . [SEP]
11/28/2021 01:23:35 - INFO - utils_tag -   input_ids: 101 31482 13956 11759 31291 10171 184 18534 10107 55683 44193 11088 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:23:35 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:23:35 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:23:35 - INFO - utils_tag -   label_ids: -100 4 1 -100 -100 -100 8 -100 -100 1 -100 -100 13 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
11/28/2021 01:23:35 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Evaluating:  90%|█████████ | 181/201 [00:36<00:03,  5.96it/s]11/28/2021 01:23:35 - INFO - __main__ -   Batch number = 182
Evaluating:  91%|█████████ | 182/201 [00:36<00:03,  5.90it/s]11/28/2021 01:23:36 - INFO - __main__ -   Batch number = 183
Evaluating:  91%|█████████ | 183/201 [00:37<00:03,  5.89it/s]11/28/2021 01:23:36 - INFO - __main__ -   Batch number = 184
Evaluating:  92%|█████████▏| 184/201 [00:37<00:02,  5.84it/s]11/28/2021 01:23:36 - INFO - __main__ -   Batch number = 185
Evaluating:  92%|█████████▏| 185/201 [00:37<00:02,  5.88it/s]11/28/2021 01:23:36 - INFO - __main__ -   Batch number = 186
Evaluating:  93%|█████████▎| 186/201 [00:37<00:02,  5.90it/s]11/28/2021 01:23:36 - INFO - __main__ -   Batch number = 187
Evaluating:  93%|█████████▎| 187/201 [00:37<00:02,  5.94it/s]11/28/2021 01:23:36 - INFO - __main__ -   Batch number = 188
Evaluating:  94%|█████████▎| 188/201 [00:37<00:02,  5.94it/s]11/28/2021 01:23:37 - INFO - __main__ -   Batch number = 189
Evaluating:  94%|█████████▍| 189/201 [00:38<00:02,  5.95it/s]11/28/2021 01:23:37 - INFO - __main__ -   Batch number = 190
Evaluating:  95%|█████████▍| 190/201 [00:38<00:01,  6.01it/s]11/28/2021 01:23:37 - INFO - __main__ -   Batch number = 191
Evaluating:  95%|█████████▌| 191/201 [00:38<00:01,  6.05it/s]11/28/2021 01:23:37 - INFO - __main__ -   Batch number = 192
Evaluating:  96%|█████████▌| 192/201 [00:38<00:01,  6.14it/s]11/28/2021 01:23:37 - INFO - __main__ -   Batch number = 193
Evaluating:  96%|█████████▌| 193/201 [00:38<00:01,  5.94it/s]11/28/2021 01:23:37 - INFO - __main__ -   Batch number = 194
Evaluating:  97%|█████████▋| 194/201 [00:38<00:01,  5.92it/s]11/28/2021 01:23:38 - INFO - __main__ -   Batch number = 195
Evaluating:  97%|█████████▋| 195/201 [00:39<00:00,  6.00it/s]11/28/2021 01:23:38 - INFO - __main__ -   Batch number = 196
Evaluating:  98%|█████████▊| 196/201 [00:39<00:00,  5.99it/s]11/28/2021 01:23:38 - INFO - __main__ -   Batch number = 197
Evaluating:  98%|█████████▊| 197/201 [00:39<00:00,  6.12it/s]11/28/2021 01:23:38 - INFO - __main__ -   Batch number = 198
Evaluating:  99%|█████████▊| 198/201 [00:39<00:00,  6.11it/s]11/28/2021 01:23:38 - INFO - __main__ -   Batch number = 199
Evaluating:  99%|█████████▉| 199/201 [00:39<00:00,  6.10it/s]11/28/2021 01:23:38 - INFO - __main__ -   Batch number = 200
Evaluating: 100%|█████████▉| 200/201 [00:39<00:00,  6.12it/s]11/28/2021 01:23:39 - INFO - __main__ -   Batch number = 201
Evaluating: 100%|██████████| 201/201 [00:39<00:00,  5.03it/s]11/28/2021 01:23:40 - INFO - __main__ -   Saving features into cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_fi_bert-base-multilingual-cased_128, len(features)=6550
11/28/2021 01:23:42 - INFO - __main__ -   ***** Running evaluation  in fi *****
11/28/2021 01:23:42 - INFO - __main__ -     Num examples = 6550
11/28/2021 01:23:42 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/205 [00:00<?, ?it/s]11/28/2021 01:23:42 - INFO - __main__ -   Batch number = 1
Evaluating:   0%|          | 1/205 [00:00<01:02,  3.26it/s]11/28/2021 01:23:42 - INFO - __main__ -   Batch number = 2
Evaluating:   1%|          | 2/205 [00:00<01:00,  3.35it/s]11/28/2021 01:23:42 - INFO - __main__ -   Batch number = 3
Evaluating:   1%|▏         | 3/205 [00:00<01:00,  3.31it/s]11/28/2021 01:23:43 - INFO - __main__ -   Batch number = 4

/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:23:43 - INFO - __main__ -   ***** Evaluation result  in is *****
11/28/2021 01:23:43 - INFO - __main__ -     f1 = 0.7394750619408236
11/28/2021 01:23:43 - INFO - __main__ -     loss = 0.9067785489321941
11/28/2021 01:23:43 - INFO - __main__ -     precision = 0.7456088441778519
11/28/2021 01:23:43 - INFO - __main__ -     recall = 0.7334413758888705
Evaluating:   2%|▏         | 4/205 [00:01<01:01,  3.28it/s]11/28/2021 01:23:43 - INFO - __main__ -   Batch number = 5
Evaluating:   2%|▏         | 5/205 [00:01<01:01,  3.26it/s]11/28/2021 01:23:43 - INFO - __main__ -   Batch number = 6
Evaluating:   3%|▎         | 6/205 [00:01<01:00,  3.27it/s]11/28/2021 01:23:43 - INFO - __main__ -   Batch number = 7
49.10user 16.37system 1:03.41elapsed 103%CPU (0avgtext+0avgdata 3944652maxresident)k
0inputs+1720outputs (0major+1775160minor)pagefaults 0swaps
Evaluating:   3%|▎         | 7/205 [00:02<01:00,  3.25it/s]11/28/2021 01:23:44 - INFO - __main__ -   Batch number = 8
Evaluating:   4%|▍         | 8/205 [00:02<01:00,  3.26it/s]11/28/2021 01:23:44 - INFO - __main__ -   Batch number = 9
Evaluating:   4%|▍         | 9/205 [00:02<01:00,  3.25it/s]11/28/2021 01:23:44 - INFO - __main__ -   Batch number = 10
Evaluating:   5%|▍         | 10/205 [00:03<01:00,  3.24it/s]11/28/2021 01:23:45 - INFO - __main__ -   Batch number = 11
Evaluating:   5%|▌         | 11/205 [00:03<00:59,  3.24it/s]11/28/2021 01:23:45 - INFO - __main__ -   Batch number = 12
Evaluating:   6%|▌         | 12/205 [00:03<00:58,  3.32it/s]11/28/2021 01:23:45 - INFO - __main__ -   Batch number = 13
Evaluating:   6%|▋         | 13/205 [00:03<00:58,  3.28it/s]11/28/2021 01:23:46 - INFO - __main__ -   Batch number = 14
Evaluating:   7%|▋         | 14/205 [00:04<00:58,  3.24it/s]11/28/2021 01:23:46 - INFO - __main__ -   Batch number = 15
Evaluating:   7%|▋         | 15/205 [00:04<00:59,  3.21it/s]11/28/2021 01:23:46 - INFO - __main__ -   Batch number = 16
Evaluating:   8%|▊         | 16/205 [00:04<00:59,  3.18it/s]11/28/2021 01:23:47 - INFO - __main__ -   Batch number = 17
Evaluating:   8%|▊         | 17/205 [00:05<00:59,  3.15it/s]11/28/2021 01:23:47 - INFO - __main__ -   Batch number = 18
Evaluating:   9%|▉         | 18/205 [00:05<00:59,  3.15it/s]11/28/2021 01:23:47 - INFO - __main__ -   Batch number = 19
Evaluating:   9%|▉         | 19/205 [00:05<00:58,  3.18it/s]11/28/2021 01:23:48 - INFO - __main__ -   Batch number = 20
Evaluating:  10%|▉         | 20/205 [00:06<00:58,  3.18it/s]11/28/2021 01:23:48 - INFO - __main__ -   Batch number = 21
Evaluating:  10%|█         | 21/205 [00:06<00:57,  3.18it/s]11/28/2021 01:23:48 - INFO - __main__ -   Batch number = 22
Evaluating:  11%|█         | 22/205 [00:06<00:57,  3.20it/s]11/28/2021 01:23:48 - INFO - __main__ -   Batch number = 23
Evaluating:  11%|█         | 23/205 [00:07<00:57,  3.19it/s]11/28/2021 01:23:49 - INFO - __main__ -   Batch number = 24
Evaluating:  12%|█▏        | 24/205 [00:07<00:56,  3.20it/s]11/28/2021 01:23:49 - INFO - __main__ -   Batch number = 25
Evaluating:  12%|█▏        | 25/205 [00:07<00:55,  3.22it/s]11/28/2021 01:23:49 - INFO - __main__ -   Batch number = 26
Evaluating:  13%|█▎        | 26/205 [00:08<00:55,  3.22it/s]11/28/2021 01:23:50 - INFO - __main__ -   Batch number = 27
Evaluating:  13%|█▎        | 27/205 [00:08<00:55,  3.22it/s]11/28/2021 01:23:50 - INFO - __main__ -   Batch number = 28
Evaluating:  14%|█▎        | 28/205 [00:08<00:54,  3.23it/s]11/28/2021 01:23:50 - INFO - __main__ -   Batch number = 29
Evaluating:  14%|█▍        | 29/205 [00:09<00:55,  3.18it/s]11/28/2021 01:23:51 - INFO - __main__ -   Batch number = 30
Evaluating:  15%|█▍        | 30/205 [00:09<00:54,  3.18it/s]11/28/2021 01:23:51 - INFO - __main__ -   Batch number = 31
Evaluating:  15%|█▌        | 31/205 [00:09<00:54,  3.19it/s]11/28/2021 01:23:51 - INFO - __main__ -   Batch number = 32
Evaluating:  16%|█▌        | 32/205 [00:09<00:54,  3.19it/s]11/28/2021 01:23:52 - INFO - __main__ -   Batch number = 33
Evaluating:  16%|█▌        | 33/205 [00:10<00:53,  3.19it/s]11/28/2021 01:23:52 - INFO - __main__ -   Batch number = 34
Evaluating:  17%|█▋        | 34/205 [00:10<00:53,  3.20it/s]11/28/2021 01:23:52 - INFO - __main__ -   Batch number = 35
Evaluating:  17%|█▋        | 35/205 [00:10<00:53,  3.19it/s]11/28/2021 01:23:53 - INFO - __main__ -   Batch number = 36
Evaluating:  18%|█▊        | 36/205 [00:11<00:53,  3.18it/s]11/28/2021 01:23:53 - INFO - __main__ -   Batch number = 37
Evaluating:  18%|█▊        | 37/205 [00:11<00:52,  3.22it/s]11/28/2021 01:23:53 - INFO - __main__ -   Batch number = 38
Evaluating:  19%|█▊        | 38/205 [00:11<00:52,  3.20it/s]11/28/2021 01:23:53 - INFO - __main__ -   Batch number = 39
Evaluating:  19%|█▉        | 39/205 [00:12<00:51,  3.20it/s]11/28/2021 01:23:54 - INFO - __main__ -   Batch number = 40
Evaluating:  20%|█▉        | 40/205 [00:12<00:51,  3.20it/s]11/28/2021 01:23:54 - INFO - __main__ -   Batch number = 41
Evaluating:  20%|██        | 41/205 [00:12<00:51,  3.18it/s]11/28/2021 01:23:54 - INFO - __main__ -   Batch number = 42
Evaluating:  20%|██        | 42/205 [00:13<00:51,  3.16it/s]11/28/2021 01:23:55 - INFO - __main__ -   Batch number = 43
Evaluating:  21%|██        | 43/205 [00:13<00:51,  3.15it/s]11/28/2021 01:23:55 - INFO - __main__ -   Batch number = 44
Evaluating:  21%|██▏       | 44/205 [00:13<00:50,  3.17it/s]11/28/2021 01:23:55 - INFO - __main__ -   Batch number = 45
Evaluating:  22%|██▏       | 45/205 [00:14<00:51,  3.11it/s]11/28/2021 01:23:56 - INFO - __main__ -   Batch number = 46
Evaluating:  22%|██▏       | 46/205 [00:14<00:50,  3.15it/s]11/28/2021 01:23:56 - INFO - __main__ -   Batch number = 47
Evaluating:  23%|██▎       | 47/205 [00:14<00:50,  3.15it/s]11/28/2021 01:23:56 - INFO - __main__ -   Batch number = 48
Evaluating:  23%|██▎       | 48/205 [00:14<00:49,  3.16it/s]11/28/2021 01:23:57 - INFO - __main__ -   Batch number = 49
Evaluating:  24%|██▍       | 49/205 [00:15<00:44,  3.47it/s]11/28/2021 01:23:57 - INFO - __main__ -   Batch number = 50
Evaluating:  24%|██▍       | 50/205 [00:15<00:38,  3.98it/s]11/28/2021 01:23:57 - INFO - __main__ -   Batch number = 51
Evaluating:  25%|██▍       | 51/205 [00:15<00:34,  4.52it/s]11/28/2021 01:23:57 - INFO - __main__ -   Batch number = 52
Evaluating:  25%|██▌       | 52/205 [00:15<00:30,  4.95it/s]11/28/2021 01:23:57 - INFO - __main__ -   Batch number = 53
Evaluating:  26%|██▌       | 53/205 [00:15<00:28,  5.39it/s]11/28/2021 01:23:57 - INFO - __main__ -   Batch number = 54
Evaluating:  26%|██▋       | 54/205 [00:15<00:26,  5.76it/s]11/28/2021 01:23:58 - INFO - __main__ -   Batch number = 55
Evaluating:  27%|██▋       | 55/205 [00:16<00:24,  6.02it/s]11/28/2021 01:23:58 - INFO - __main__ -   Batch number = 56
Evaluating:  27%|██▋       | 56/205 [00:16<00:24,  6.12it/s]11/28/2021 01:23:58 - INFO - __main__ -   Batch number = 57
Evaluating:  28%|██▊       | 57/205 [00:16<00:23,  6.27it/s]11/28/2021 01:23:58 - INFO - __main__ -   Batch number = 58
Evaluating:  28%|██▊       | 58/205 [00:16<00:23,  6.39it/s]11/28/2021 01:23:58 - INFO - __main__ -   Batch number = 59
Evaluating:  29%|██▉       | 59/205 [00:16<00:22,  6.48it/s]11/28/2021 01:23:58 - INFO - __main__ -   Batch number = 60
Evaluating:  29%|██▉       | 60/205 [00:16<00:22,  6.57it/s]11/28/2021 01:23:59 - INFO - __main__ -   Batch number = 61
Evaluating:  30%|██▉       | 61/205 [00:17<00:21,  6.58it/s]11/28/2021 01:23:59 - INFO - __main__ -   Batch number = 62
Evaluating:  30%|███       | 62/205 [00:17<00:22,  6.50it/s]11/28/2021 01:23:59 - INFO - __main__ -   Batch number = 63
Evaluating:  31%|███       | 63/205 [00:17<00:21,  6.54it/s]11/28/2021 01:23:59 - INFO - __main__ -   Batch number = 64
Evaluating:  31%|███       | 64/205 [00:17<00:21,  6.49it/s]11/28/2021 01:23:59 - INFO - __main__ -   Batch number = 65
Evaluating:  32%|███▏      | 65/205 [00:17<00:21,  6.42it/s]11/28/2021 01:23:59 - INFO - __main__ -   Batch number = 66
Evaluating:  32%|███▏      | 66/205 [00:17<00:21,  6.36it/s]11/28/2021 01:23:59 - INFO - __main__ -   Batch number = 67
Evaluating:  33%|███▎      | 67/205 [00:17<00:21,  6.45it/s]11/28/2021 01:24:00 - INFO - __main__ -   Batch number = 68
Evaluating:  33%|███▎      | 68/205 [00:18<00:20,  6.54it/s]11/28/2021 01:24:00 - INFO - __main__ -   Batch number = 69
Evaluating:  34%|███▎      | 69/205 [00:18<00:21,  6.44it/s]11/28/2021 01:24:00 - INFO - __main__ -   Batch number = 70
Evaluating:  34%|███▍      | 70/205 [00:18<00:21,  6.40it/s]11/28/2021 01:24:00 - INFO - __main__ -   Batch number = 71
Evaluating:  35%|███▍      | 71/205 [00:18<00:20,  6.38it/s]11/28/2021 01:24:00 - INFO - __main__ -   Batch number = 72
Evaluating:  35%|███▌      | 72/205 [00:18<00:28,  4.61it/s]11/28/2021 01:24:01 - INFO - __main__ -   Batch number = 73
Evaluating:  36%|███▌      | 73/205 [00:19<00:25,  5.08it/s]11/28/2021 01:24:01 - INFO - __main__ -   Batch number = 74
Evaluating:  36%|███▌      | 74/205 [00:19<00:23,  5.51it/s]11/28/2021 01:24:01 - INFO - __main__ -   Batch number = 75
Evaluating:  37%|███▋      | 75/205 [00:19<00:22,  5.88it/s]11/28/2021 01:24:01 - INFO - __main__ -   Batch number = 76
Evaluating:  37%|███▋      | 76/205 [00:19<00:20,  6.18it/s]11/28/2021 01:24:01 - INFO - __main__ -   Batch number = 77
Evaluating:  38%|███▊      | 77/205 [00:19<00:19,  6.43it/s]11/28/2021 01:24:01 - INFO - __main__ -   Batch number = 78
Evaluating:  38%|███▊      | 78/205 [00:19<00:19,  6.58it/s]11/28/2021 01:24:01 - INFO - __main__ -   Batch number = 79
Evaluating:  39%|███▊      | 79/205 [00:19<00:18,  6.68it/s]11/28/2021 01:24:02 - INFO - __main__ -   Batch number = 80
Evaluating:  39%|███▉      | 80/205 [00:20<00:18,  6.78it/s]11/28/2021 01:24:02 - INFO - __main__ -   Batch number = 81
Evaluating:  40%|███▉      | 81/205 [00:20<00:18,  6.83it/s]11/28/2021 01:24:02 - INFO - __main__ -   Batch number = 82
Evaluating:  40%|████      | 82/205 [00:20<00:17,  6.92it/s]11/28/2021 01:24:02 - INFO - __main__ -   Batch number = 83
Evaluating:  40%|████      | 83/205 [00:20<00:17,  6.97it/s]11/28/2021 01:24:02 - INFO - __main__ -   Batch number = 84
Evaluating:  41%|████      | 84/205 [00:20<00:17,  6.92it/s]11/28/2021 01:24:02 - INFO - __main__ -   Batch number = 85
Evaluating:  41%|████▏     | 85/205 [00:20<00:17,  6.88it/s]11/28/2021 01:24:02 - INFO - __main__ -   Batch number = 86
Evaluating:  42%|████▏     | 86/205 [00:20<00:17,  6.82it/s]11/28/2021 01:24:03 - INFO - __main__ -   Batch number = 87
Evaluating:  42%|████▏     | 87/205 [00:21<00:17,  6.76it/s]11/28/2021 01:24:03 - INFO - __main__ -   Batch number = 88
Evaluating:  43%|████▎     | 88/205 [00:21<00:17,  6.74it/s]11/28/2021 01:24:03 - INFO - __main__ -   Batch number = 89
Evaluating:  43%|████▎     | 89/205 [00:21<00:17,  6.72it/s]11/28/2021 01:24:03 - INFO - __main__ -   Batch number = 90
Evaluating:  44%|████▍     | 90/205 [00:21<00:17,  6.68it/s]11/28/2021 01:24:03 - INFO - __main__ -   Batch number = 91
Evaluating:  44%|████▍     | 91/205 [00:21<00:17,  6.62it/s]11/28/2021 01:24:03 - INFO - __main__ -   Batch number = 92
Evaluating:  45%|████▍     | 92/205 [00:21<00:17,  6.59it/s]11/28/2021 01:24:04 - INFO - __main__ -   Batch number = 93
Evaluating:  45%|████▌     | 93/205 [00:22<00:17,  6.58it/s]11/28/2021 01:24:04 - INFO - __main__ -   Batch number = 94
Evaluating:  46%|████▌     | 94/205 [00:22<00:16,  6.61it/s]11/28/2021 01:24:04 - INFO - __main__ -   Batch number = 95
Evaluating:  46%|████▋     | 95/205 [00:22<00:16,  6.72it/s]11/28/2021 01:24:04 - INFO - __main__ -   Batch number = 96
Evaluating:  47%|████▋     | 96/205 [00:22<00:16,  6.77it/s]11/28/2021 01:24:04 - INFO - __main__ -   Batch number = 97
Evaluating:  47%|████▋     | 97/205 [00:22<00:16,  6.74it/s]11/28/2021 01:24:04 - INFO - __main__ -   Batch number = 98
Evaluating:  48%|████▊     | 98/205 [00:22<00:15,  6.75it/s]11/28/2021 01:24:04 - INFO - __main__ -   Batch number = 99
Evaluating:  48%|████▊     | 99/205 [00:22<00:15,  6.67it/s]11/28/2021 01:24:05 - INFO - __main__ -   Batch number = 100
Evaluating:  49%|████▉     | 100/205 [00:23<00:15,  6.76it/s]11/28/2021 01:24:05 - INFO - __main__ -   Batch number = 101
Evaluating:  49%|████▉     | 101/205 [00:23<00:15,  6.75it/s]11/28/2021 01:24:05 - INFO - __main__ -   Batch number = 102
Evaluating:  50%|████▉     | 102/205 [00:23<00:15,  6.71it/s]11/28/2021 01:24:05 - INFO - __main__ -   Batch number = 103
Evaluating:  50%|█████     | 103/205 [00:23<00:15,  6.70it/s]11/28/2021 01:24:05 - INFO - __main__ -   Batch number = 104
Evaluating:  51%|█████     | 104/205 [00:23<00:15,  6.66it/s]11/28/2021 01:24:05 - INFO - __main__ -   Batch number = 105
Evaluating:  51%|█████     | 105/205 [00:23<00:15,  6.66it/s]11/28/2021 01:24:05 - INFO - __main__ -   Batch number = 106
Evaluating:  52%|█████▏    | 106/205 [00:23<00:14,  6.64it/s]11/28/2021 01:24:06 - INFO - __main__ -   Batch number = 107
Evaluating:  52%|█████▏    | 107/205 [00:24<00:14,  6.68it/s]11/28/2021 01:24:06 - INFO - __main__ -   Batch number = 108
Evaluating:  53%|█████▎    | 108/205 [00:24<00:14,  6.64it/s]11/28/2021 01:24:06 - INFO - __main__ -   Batch number = 109
Evaluating:  53%|█████▎    | 109/205 [00:24<00:14,  6.56it/s]11/28/2021 01:24:06 - INFO - __main__ -   Batch number = 110
Evaluating:  54%|█████▎    | 110/205 [00:24<00:14,  6.61it/s]11/28/2021 01:24:06 - INFO - __main__ -   Batch number = 111
Evaluating:  54%|█████▍    | 111/205 [00:24<00:14,  6.68it/s]11/28/2021 01:24:06 - INFO - __main__ -   Batch number = 112
Evaluating:  55%|█████▍    | 112/205 [00:24<00:15,  5.87it/s]11/28/2021 01:24:07 - INFO - __main__ -   Batch number = 113
Evaluating:  55%|█████▌    | 113/205 [00:25<00:19,  4.81it/s]11/28/2021 01:24:07 - INFO - __main__ -   Batch number = 114
Evaluating:  56%|█████▌    | 114/205 [00:25<00:21,  4.24it/s]11/28/2021 01:24:07 - INFO - __main__ -   Batch number = 115
Evaluating:  56%|█████▌    | 115/205 [00:25<00:23,  3.85it/s]11/28/2021 01:24:07 - INFO - __main__ -   Batch number = 116
Evaluating:  57%|█████▋    | 116/205 [00:26<00:24,  3.60it/s]11/28/2021 01:24:08 - INFO - __main__ -   Batch number = 117
Evaluating:  57%|█████▋    | 117/205 [00:26<00:25,  3.44it/s]11/28/2021 01:24:08 - INFO - __main__ -   Batch number = 118
Evaluating:  58%|█████▊    | 118/205 [00:26<00:26,  3.31it/s]11/28/2021 01:24:08 - INFO - __main__ -   Batch number = 119
Evaluating:  58%|█████▊    | 119/205 [00:27<00:26,  3.25it/s]11/28/2021 01:24:09 - INFO - __main__ -   Batch number = 120
Evaluating:  59%|█████▊    | 120/205 [00:27<00:26,  3.21it/s]11/28/2021 01:24:09 - INFO - __main__ -   Batch number = 121
Evaluating:  59%|█████▉    | 121/205 [00:27<00:26,  3.16it/s]11/28/2021 01:24:09 - INFO - __main__ -   Batch number = 122
Evaluating:  60%|█████▉    | 122/205 [00:28<00:26,  3.16it/s]11/28/2021 01:24:10 - INFO - __main__ -   Batch number = 123
Evaluating:  60%|██████    | 123/205 [00:28<00:26,  3.14it/s]11/28/2021 01:24:10 - INFO - __main__ -   Batch number = 124
Evaluating:  60%|██████    | 124/205 [00:28<00:25,  3.13it/s]11/28/2021 01:24:11 - INFO - __main__ -   Batch number = 125
Evaluating:  61%|██████    | 125/205 [00:29<00:29,  2.68it/s]11/28/2021 01:24:11 - INFO - __main__ -   Batch number = 126
Evaluating:  61%|██████▏   | 126/205 [00:29<00:27,  2.82it/s]11/28/2021 01:24:11 - INFO - __main__ -   Batch number = 127
Evaluating:  62%|██████▏   | 127/205 [00:29<00:26,  2.89it/s]11/28/2021 01:24:12 - INFO - __main__ -   Batch number = 128
Evaluating:  62%|██████▏   | 128/205 [00:30<00:26,  2.96it/s]11/28/2021 01:24:12 - INFO - __main__ -   Batch number = 129
Evaluating:  63%|██████▎   | 129/205 [00:30<00:25,  3.02it/s]11/28/2021 01:24:12 - INFO - __main__ -   Batch number = 130
Evaluating:  63%|██████▎   | 130/205 [00:30<00:24,  3.03it/s]11/28/2021 01:24:12 - INFO - __main__ -   Batch number = 131
Evaluating:  64%|██████▍   | 131/205 [00:31<00:24,  3.08it/s]11/28/2021 01:24:13 - INFO - __main__ -   Batch number = 132
Evaluating:  64%|██████▍   | 132/205 [00:31<00:23,  3.09it/s]11/28/2021 01:24:13 - INFO - __main__ -   Batch number = 133
Evaluating:  65%|██████▍   | 133/205 [00:31<00:23,  3.08it/s]11/28/2021 01:24:13 - INFO - __main__ -   Batch number = 134
Evaluating:  65%|██████▌   | 134/205 [00:32<00:22,  3.11it/s]11/28/2021 01:24:14 - INFO - __main__ -   Batch number = 135
Evaluating:  66%|██████▌   | 135/205 [00:32<00:22,  3.12it/s]11/28/2021 01:24:14 - INFO - __main__ -   Batch number = 136
Evaluating:  66%|██████▋   | 136/205 [00:32<00:22,  3.11it/s]11/28/2021 01:24:14 - INFO - __main__ -   Batch number = 137
Evaluating:  67%|██████▋   | 137/205 [00:33<00:21,  3.10it/s]11/28/2021 01:24:15 - INFO - __main__ -   Batch number = 138
Evaluating:  67%|██████▋   | 138/205 [00:33<00:21,  3.11it/s]11/28/2021 01:24:15 - INFO - __main__ -   Batch number = 139
Evaluating:  68%|██████▊   | 139/205 [00:33<00:21,  3.10it/s]11/28/2021 01:24:15 - INFO - __main__ -   Batch number = 140
Evaluating:  68%|██████▊   | 140/205 [00:34<00:21,  3.08it/s]11/28/2021 01:24:16 - INFO - __main__ -   Batch number = 141
Evaluating:  69%|██████▉   | 141/205 [00:34<00:20,  3.11it/s]11/28/2021 01:24:16 - INFO - __main__ -   Batch number = 142
Evaluating:  69%|██████▉   | 142/205 [00:34<00:20,  3.07it/s]11/28/2021 01:24:16 - INFO - __main__ -   Batch number = 143
Evaluating:  70%|██████▉   | 143/205 [00:35<00:20,  3.05it/s]11/28/2021 01:24:17 - INFO - __main__ -   Batch number = 144
Evaluating:  70%|███████   | 144/205 [00:35<00:19,  3.06it/s]11/28/2021 01:24:17 - INFO - __main__ -   Batch number = 145
Evaluating:  71%|███████   | 145/205 [00:35<00:19,  3.07it/s]11/28/2021 01:24:17 - INFO - __main__ -   Batch number = 146
Evaluating:  71%|███████   | 146/205 [00:36<00:19,  3.08it/s]11/28/2021 01:24:18 - INFO - __main__ -   Batch number = 147
Evaluating:  72%|███████▏  | 147/205 [00:36<00:18,  3.06it/s]11/28/2021 01:24:18 - INFO - __main__ -   Batch number = 148
Evaluating:  72%|███████▏  | 148/205 [00:36<00:18,  3.08it/s]11/28/2021 01:24:18 - INFO - __main__ -   Batch number = 149
Evaluating:  73%|███████▎  | 149/205 [00:36<00:18,  3.08it/s]11/28/2021 01:24:19 - INFO - __main__ -   Batch number = 150
Evaluating:  73%|███████▎  | 150/205 [00:37<00:16,  3.40it/s]11/28/2021 01:24:19 - INFO - __main__ -   Batch number = 151
Evaluating:  74%|███████▎  | 151/205 [00:37<00:16,  3.33it/s]11/28/2021 01:24:19 - INFO - __main__ -   Batch number = 152
Evaluating:  74%|███████▍  | 152/205 [00:37<00:16,  3.23it/s]11/28/2021 01:24:19 - INFO - __main__ -   Batch number = 153
Evaluating:  75%|███████▍  | 153/205 [00:38<00:16,  3.19it/s]11/28/2021 01:24:20 - INFO - __main__ -   Batch number = 154
Evaluating:  75%|███████▌  | 154/205 [00:38<00:16,  3.14it/s]11/28/2021 01:24:20 - INFO - __main__ -   Batch number = 155
Evaluating:  76%|███████▌  | 155/205 [00:38<00:14,  3.42it/s]11/28/2021 01:24:20 - INFO - __main__ -   Batch number = 156
Evaluating:  76%|███████▌  | 156/205 [00:39<00:17,  2.76it/s]11/28/2021 01:24:21 - INFO - __main__ -   Batch number = 157
Evaluating:  77%|███████▋  | 157/205 [00:39<00:18,  2.58it/s]11/28/2021 01:24:21 - INFO - __main__ -   Batch number = 158
Evaluating:  77%|███████▋  | 158/205 [00:40<00:19,  2.42it/s]11/28/2021 01:24:22 - INFO - __main__ -   Batch number = 159
Evaluating:  78%|███████▊  | 159/205 [00:40<00:19,  2.31it/s]11/28/2021 01:24:22 - INFO - __main__ -   Batch number = 160
Evaluating:  78%|███████▊  | 160/205 [00:41<00:20,  2.22it/s]11/28/2021 01:24:23 - INFO - __main__ -   Batch number = 161
Evaluating:  79%|███████▊  | 161/205 [00:41<00:20,  2.17it/s]11/28/2021 01:24:23 - INFO - __main__ -   Batch number = 162
Evaluating:  79%|███████▉  | 162/205 [00:42<00:20,  2.13it/s]11/28/2021 01:24:24 - INFO - __main__ -   Batch number = 163
Evaluating:  80%|███████▉  | 163/205 [00:42<00:19,  2.13it/s]11/28/2021 01:24:24 - INFO - __main__ -   Batch number = 164
Evaluating:  80%|████████  | 164/205 [00:43<00:19,  2.09it/s]11/28/2021 01:24:25 - INFO - __main__ -   Batch number = 165
Evaluating:  80%|████████  | 165/205 [00:43<00:19,  2.09it/s]11/28/2021 01:24:25 - INFO - __main__ -   Batch number = 166
Evaluating:  81%|████████  | 166/205 [00:44<00:19,  2.02it/s]11/28/2021 01:24:26 - INFO - __main__ -   Batch number = 167
Evaluating:  81%|████████▏ | 167/205 [00:44<00:18,  2.03it/s]11/28/2021 01:24:26 - INFO - __main__ -   Batch number = 168
Evaluating:  82%|████████▏ | 168/205 [00:45<00:18,  2.03it/s]11/28/2021 01:24:27 - INFO - __main__ -   Batch number = 169
Evaluating:  82%|████████▏ | 169/205 [00:45<00:17,  2.05it/s]11/28/2021 01:24:27 - INFO - __main__ -   Batch number = 170
Evaluating:  83%|████████▎ | 170/205 [00:46<00:17,  2.04it/s]11/28/2021 01:24:28 - INFO - __main__ -   Batch number = 171
Evaluating:  83%|████████▎ | 171/205 [00:46<00:16,  2.05it/s]11/28/2021 01:24:28 - INFO - __main__ -   Batch number = 172
Evaluating:  84%|████████▍ | 172/205 [00:47<00:16,  2.05it/s]11/28/2021 01:24:29 - INFO - __main__ -   Batch number = 173
Evaluating:  84%|████████▍ | 173/205 [00:47<00:15,  2.06it/s]11/28/2021 01:24:29 - INFO - __main__ -   Batch number = 174
Evaluating:  85%|████████▍ | 174/205 [00:48<00:15,  2.04it/s]11/28/2021 01:24:30 - INFO - __main__ -   Batch number = 175
Evaluating:  85%|████████▌ | 175/205 [00:48<00:14,  2.05it/s]11/28/2021 01:24:30 - INFO - __main__ -   Batch number = 176
Evaluating:  86%|████████▌ | 176/205 [00:48<00:14,  2.04it/s]11/28/2021 01:24:31 - INFO - __main__ -   Batch number = 177
Evaluating:  86%|████████▋ | 177/205 [00:49<00:13,  2.06it/s]11/28/2021 01:24:31 - INFO - __main__ -   Batch number = 178
Evaluating:  87%|████████▋ | 178/205 [00:49<00:13,  2.05it/s]11/28/2021 01:24:32 - INFO - __main__ -   Batch number = 179
Evaluating:  87%|████████▋ | 179/205 [00:50<00:12,  2.06it/s]11/28/2021 01:24:32 - INFO - __main__ -   Batch number = 180
Evaluating:  88%|████████▊ | 180/205 [00:50<00:12,  2.06it/s]11/28/2021 01:24:33 - INFO - __main__ -   Batch number = 181
Evaluating:  88%|████████▊ | 181/205 [00:51<00:11,  2.07it/s]11/28/2021 01:24:33 - INFO - __main__ -   Batch number = 182
Evaluating:  89%|████████▉ | 182/205 [00:51<00:11,  2.06it/s]11/28/2021 01:24:34 - INFO - __main__ -   Batch number = 183
Evaluating:  89%|████████▉ | 183/205 [00:52<00:10,  2.07it/s]11/28/2021 01:24:34 - INFO - __main__ -   Batch number = 184
Evaluating:  90%|████████▉ | 184/205 [00:52<00:10,  2.07it/s]11/28/2021 01:24:34 - INFO - __main__ -   Batch number = 185
Evaluating:  90%|█████████ | 185/205 [00:53<00:09,  2.06it/s]11/28/2021 01:24:35 - INFO - __main__ -   Batch number = 186
Evaluating:  91%|█████████ | 186/205 [00:53<00:09,  2.06it/s]11/28/2021 01:24:35 - INFO - __main__ -   Batch number = 187
Evaluating:  91%|█████████ | 187/205 [00:54<00:08,  2.08it/s]11/28/2021 01:24:36 - INFO - __main__ -   Batch number = 188
Evaluating:  92%|█████████▏| 188/205 [00:54<00:08,  2.07it/s]11/28/2021 01:24:36 - INFO - __main__ -   Batch number = 189
Evaluating:  92%|█████████▏| 189/205 [00:55<00:07,  2.08it/s]11/28/2021 01:24:37 - INFO - __main__ -   Batch number = 190
Evaluating:  93%|█████████▎| 190/205 [00:55<00:07,  2.07it/s]11/28/2021 01:24:37 - INFO - __main__ -   Batch number = 191
Evaluating:  93%|█████████▎| 191/205 [00:56<00:06,  2.07it/s]11/28/2021 01:24:38 - INFO - __main__ -   Batch number = 192
Evaluating:  94%|█████████▎| 192/205 [00:56<00:06,  2.06it/s]11/28/2021 01:24:38 - INFO - __main__ -   Batch number = 193
Evaluating:  94%|█████████▍| 193/205 [00:57<00:05,  2.06it/s]11/28/2021 01:24:39 - INFO - __main__ -   Batch number = 194
Evaluating:  95%|█████████▍| 194/205 [00:57<00:05,  2.05it/s]11/28/2021 01:24:39 - INFO - __main__ -   Batch number = 195
Evaluating:  95%|█████████▌| 195/205 [00:58<00:04,  2.07it/s]11/28/2021 01:24:40 - INFO - __main__ -   Batch number = 196
Evaluating:  96%|█████████▌| 196/205 [00:58<00:04,  2.06it/s]11/28/2021 01:24:40 - INFO - __main__ -   Batch number = 197
Evaluating:  96%|█████████▌| 197/205 [00:59<00:03,  2.08it/s]11/28/2021 01:24:41 - INFO - __main__ -   Batch number = 198
Evaluating:  97%|█████████▋| 198/205 [00:59<00:03,  2.07it/s]11/28/2021 01:24:41 - INFO - __main__ -   Batch number = 199
Evaluating:  97%|█████████▋| 199/205 [01:00<00:02,  2.08it/s]11/28/2021 01:24:42 - INFO - __main__ -   Batch number = 200
Evaluating:  98%|█████████▊| 200/205 [01:00<00:02,  2.07it/s]11/28/2021 01:24:42 - INFO - __main__ -   Batch number = 201
Evaluating:  98%|█████████▊| 201/205 [01:01<00:01,  2.08it/s]11/28/2021 01:24:43 - INFO - __main__ -   Batch number = 202
Evaluating:  99%|█████████▊| 202/205 [01:01<00:01,  2.07it/s]11/28/2021 01:24:43 - INFO - __main__ -   Batch number = 203
Evaluating:  99%|█████████▉| 203/205 [01:02<00:00,  2.08it/s]11/28/2021 01:24:44 - INFO - __main__ -   Batch number = 204
Evaluating: 100%|█████████▉| 204/205 [01:02<00:00,  2.07it/s]11/28/2021 01:24:44 - INFO - __main__ -   Batch number = 205
Evaluating: 100%|██████████| 205/205 [01:02<00:00,  2.24it/s]Evaluating: 100%|██████████| 205/205 [01:02<00:00,  3.26it/s]
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:24:47 - INFO - __main__ -   ***** Evaluation result  in fi *****
11/28/2021 01:24:47 - INFO - __main__ -     f1 = 0.7538488872409292
11/28/2021 01:24:47 - INFO - __main__ -     loss = 0.9076627782205257
11/28/2021 01:24:47 - INFO - __main__ -     precision = 0.759049826548232
11/28/2021 01:24:47 - INFO - __main__ -     recall = 0.7487187356145466
70.02user 23.07system 1:32.65elapsed 100%CPU (0avgtext+0avgdata 3939660maxresident)k
2496inputs+23088outputs (0major+1672104minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:24:50 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:24:50 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:24:50 - INFO - __main__ -   Seed = 2
11/28/2021 01:24:50 - INFO - root -   save model
11/28/2021 01:24:50 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:24:50 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:24:53 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:24:56 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='pt', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:24:56 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:24:56 - INFO - __main__ -   Seed = 1
11/28/2021 01:24:56 - INFO - root -   save model
11/28/2021 01:24:56 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='pt', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:24:56 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:24:59 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:24:59 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:24:59 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:24:59 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
11/28/2021 01:24:59 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:24:59 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:24:59 - INFO - root -   loading lang adpater am/wiki@ukp
11/28/2021 01:24:59 - INFO - __main__ -   Adapter Languages : ['am'], Length : 1
11/28/2021 01:24:59 - INFO - __main__ -   Adapter Names ['am/wiki@ukp'], Length : 1
11/28/2021 01:24:59 - INFO - __main__ -   Language = am
11/28/2021 01:24:59 - INFO - __main__ -   Adapter Name = am/wiki@ukp
Found matching adapter at: adapters/ukp/bert-base-multilingual-cased_am_wiki_pfeiffer.json
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/am/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_am_wiki_pfeiffer.zip.
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/adapter_config.json
Adding adapter 'am' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/head_config.json
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:25:05 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:25:05 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:25:05 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
11/28/2021 01:25:05 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:25:05 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:25:06 - INFO - root -   loading lang adpater am/wiki@ukp
11/28/2021 01:25:06 - INFO - __main__ -   Adapter Languages : ['am'], Length : 1
11/28/2021 01:25:06 - INFO - __main__ -   Adapter Names ['am/wiki@ukp'], Length : 1
11/28/2021 01:25:06 - INFO - __main__ -   Language = am
11/28/2021 01:25:06 - INFO - __main__ -   Adapter Name = am/wiki@ukp
Found matching adapter at: adapters/ukp/bert-base-multilingual-cased_am_wiki_pfeiffer.json
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/am/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_am_wiki_pfeiffer.zip.
11/28/2021 01:25:07 - INFO - __main__ -   Language adapter for fi not found, using am instead
11/28/2021 01:25:07 - INFO - __main__ -   Set active language adapter to am
11/28/2021 01:25:07 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:25:07 - INFO - __main__ -   Adapter Languages = ['am']
11/28/2021 01:25:07 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_fi_bert-base-multilingual-cased_128
11/28/2021 01:25:08 - INFO - __main__ -   ***** Running evaluation  in fi *****
11/28/2021 01:25:08 - INFO - __main__ -     Num examples = 6550
11/28/2021 01:25:08 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/205 [00:00<?, ?it/s]Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/adapter_config.json
Adding adapter 'am' of type 'text_lang'.
11/28/2021 01:25:08 - INFO - __main__ -   Batch number = 1
Loading module weights from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/head_config.json
Evaluating:   0%|          | 1/205 [00:00<01:03,  3.22it/s]11/28/2021 01:25:08 - INFO - __main__ -   Batch number = 2
Evaluating:   1%|          | 2/205 [00:00<01:03,  3.19it/s]11/28/2021 01:25:08 - INFO - __main__ -   Batch number = 3
Evaluating:   1%|▏         | 3/205 [00:00<01:02,  3.23it/s]11/28/2021 01:25:09 - INFO - __main__ -   Batch number = 4
Evaluating:   2%|▏         | 4/205 [00:01<01:02,  3.22it/s]11/28/2021 01:25:09 - INFO - __main__ -   Batch number = 5
Evaluating:   2%|▏         | 5/205 [00:01<01:02,  3.21it/s]11/28/2021 01:25:09 - INFO - __main__ -   Batch number = 6
Evaluating:   3%|▎         | 6/205 [00:01<01:02,  3.21it/s]11/28/2021 01:25:10 - INFO - __main__ -   Batch number = 7
Evaluating:   3%|▎         | 7/205 [00:02<01:01,  3.21it/s]11/28/2021 01:25:10 - INFO - __main__ -   Batch number = 8
Evaluating:   4%|▍         | 8/205 [00:02<01:01,  3.20it/s]11/28/2021 01:25:10 - INFO - __main__ -   Batch number = 9
Evaluating:   4%|▍         | 9/205 [00:02<01:00,  3.22it/s]11/28/2021 01:25:11 - INFO - __main__ -   Batch number = 10
Evaluating:   5%|▍         | 10/205 [00:03<01:00,  3.21it/s]11/28/2021 01:25:11 - INFO - __main__ -   Batch number = 11
Evaluating:   5%|▌         | 11/205 [00:03<01:00,  3.22it/s]11/28/2021 01:25:11 - INFO - __main__ -   Batch number = 12
Evaluating:   6%|▌         | 12/205 [00:03<01:00,  3.22it/s]11/28/2021 01:25:11 - INFO - __main__ -   Batch number = 13
Evaluating:   6%|▋         | 13/205 [00:04<00:59,  3.20it/s]11/28/2021 01:25:12 - INFO - __main__ -   Batch number = 14
Evaluating:   7%|▋         | 14/205 [00:04<00:59,  3.19it/s]11/28/2021 01:25:12 - INFO - __main__ -   Batch number = 15
Evaluating:   7%|▋         | 15/205 [00:04<00:59,  3.18it/s]11/28/2021 01:25:12 - INFO - __main__ -   Batch number = 16
Evaluating:   8%|▊         | 16/205 [00:04<00:59,  3.17it/s]11/28/2021 01:25:13 - INFO - __main__ -   Batch number = 17
Evaluating:   8%|▊         | 17/205 [00:05<00:59,  3.14it/s]11/28/2021 01:25:13 - INFO - __main__ -   Batch number = 18
Evaluating:   9%|▉         | 18/205 [00:05<00:59,  3.12it/s]11/28/2021 01:25:13 - INFO - __main__ -   Batch number = 19
Evaluating:   9%|▉         | 19/205 [00:05<00:58,  3.16it/s]11/28/2021 01:25:14 - INFO - __main__ -   Batch number = 20
Evaluating:  10%|▉         | 20/205 [00:06<01:04,  2.86it/s]11/28/2021 01:25:14 - INFO - __main__ -   Batch number = 21
Evaluating:  10%|█         | 21/205 [00:06<01:02,  2.96it/s]11/28/2021 01:25:14 - INFO - __main__ -   Batch number = 22
Evaluating:  11%|█         | 22/205 [00:07<01:00,  3.03it/s]11/28/2021 01:25:15 - INFO - __main__ -   Batch number = 23
Evaluating:  11%|█         | 23/205 [00:07<00:59,  3.07it/s]11/28/2021 01:25:15 - INFO - __main__ -   Batch number = 24
Evaluating:  12%|█▏        | 24/205 [00:07<00:58,  3.10it/s]11/28/2021 01:25:15 - INFO - __main__ -   Batch number = 25
Evaluating:  12%|█▏        | 25/205 [00:07<00:57,  3.13it/s]11/28/2021 01:25:16 - INFO - __main__ -   Batch number = 26
Evaluating:  13%|█▎        | 26/205 [00:08<00:56,  3.14it/s]11/28/2021 01:25:16 - INFO - __main__ -   Batch number = 27
Evaluating:  13%|█▎        | 27/205 [00:08<00:56,  3.15it/s]11/28/2021 01:25:16 - INFO - __main__ -   Batch number = 28
11/28/2021 01:25:17 - INFO - __main__ -   Language adapter for pt not found, using am instead
11/28/2021 01:25:17 - INFO - __main__ -   Set active language adapter to am
11/28/2021 01:25:17 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:25:17 - INFO - __main__ -   Adapter Languages = ['am']
11/28/2021 01:25:17 - INFO - __main__ -   all languages = pt
11/28/2021 01:25:17 - INFO - __main__ -   Creating features from dataset file at /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/pt/test.bert-base-multilingual-cased in language pt
11/28/2021 01:25:17 - INFO - utils_tag -   lang_id=0, lang=pt, lang2id=None
Evaluating:  14%|█▎        | 28/205 [00:08<00:55,  3.19it/s]11/28/2021 01:25:17 - INFO - __main__ -   Batch number = 29
11/28/2021 01:25:17 - INFO - utils_tag -   Writing example 0 of 2682
11/28/2021 01:25:17 - INFO - utils_tag -   *** Example ***
11/28/2021 01:25:17 - INFO - utils_tag -   guid: pt-1
11/28/2021 01:25:17 - INFO - utils_tag -   tokens: [CLS] É por isso que , explica , não tem pena de Hillary Clinton . [SEP]
11/28/2021 01:25:17 - INFO - utils_tag -   input_ids: 101 234 10183 24344 10121 117 44364 117 11420 12900 39465 10104 72734 24139 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:25:17 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:25:17 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:25:17 - INFO - utils_tag -   label_ids: -100 4 2 11 14 13 16 13 3 16 8 2 12 12 13 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
11/28/2021 01:25:17 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
11/28/2021 01:25:17 - INFO - utils_tag -   *** Example ***
11/28/2021 01:25:17 - INFO - utils_tag -   guid: pt-2
11/28/2021 01:25:17 - INFO - utils_tag -   tokens: [CLS] « Eles [ Hillary e Bill Clinton ] podem ter alguma espécie de acordo e quem som ##os nós para dizer se é bom ou mau ? [SEP]
11/28/2021 01:25:17 - INFO - utils_tag -   input_ids: 101 208 49848 164 72734 173 13160 24139 166 21584 12718 79985 19388 10104 19979 173 29826 10181 10310 59588 10220 97306 10126 263 32965 10431 43024 136 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:25:17 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:25:17 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:25:17 - INFO - utils_tag -   label_ids: -100 13 11 13 12 5 12 12 13 16 16 6 8 2 8 5 11 4 -100 11 14 16 14 4 1 5 1 13 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
11/28/2021 01:25:17 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
11/28/2021 01:25:17 - INFO - utils_tag -   *** Example ***
11/28/2021 01:25:17 - INFO - utils_tag -   guid: pt-3
11/28/2021 01:25:17 - INFO - utils_tag -   tokens: [CLS] A ##cho que eles têm mais mat ##uri ##dade em relação ao sexo , como em relação às drogas , do que podem mostrar » . [SEP]
11/28/2021 01:25:17 - INFO - utils_tag -   input_ids: 101 138 16575 10121 19695 33387 10614 17255 13091 16114 10266 30438 10610 37688 117 10225 10266 30438 19377 76699 117 10149 10121 21584 48535 220 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:25:17 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:25:17 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:25:17 - INFO - utils_tag -   label_ids: -100 16 -100 14 11 16 6 8 -100 -100 2 8 6 8 13 3 2 8 6 8 13 14 11 16 16 13 13 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
11/28/2021 01:25:17 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
11/28/2021 01:25:17 - INFO - utils_tag -   *** Example ***
11/28/2021 01:25:17 - INFO - utils_tag -   guid: pt-4
11/28/2021 01:25:17 - INFO - utils_tag -   tokens: [CLS] Tal ##vez o Presidente , quando liga a televisão de man ##hã e des ##co ##bre que apesar de Hillary continuar ao seu lado o país ins ##iste em discu ##tir se ele deve ou não abandonar o cargo porque teve uma relação extra ##con ##ju ##gal com Monica Lew ##ins ##ky , pense , como uma das personagens de « Happiness » : [SEP]
11/28/2021 01:25:17 - INFO - utils_tag -   input_ids: 101 24471 27468 183 17822 117 11971 18561 169 36933 10104 10817 72765 173 10139 10812 13724 10121 45651 10104 72734 36055 10610 10617 15776 183 12115 15498 15188 10266 110076 18330 10126 12637 19847 10431 11420 38700 183 15856 16348 19395 10437 30438 19868 23486 10761 17026 10212 29242 84730 14411 11445 117 77184 117 10225 10437 10242 49203 10104 208 86458 220 131 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:25:17 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:25:17 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:25:17 - INFO - utils_tag -   label_ids: -100 3 -100 6 12 13 14 16 6 8 2 8 -100 5 16 -100 -100 14 3 2 12 16 2 6 8 6 8 16 -100 14 16 -100 14 11 16 5 3 16 6 8 14 16 6 8 1 -100 -100 -100 2 12 12 -100 -100 13 16 13 2 9 2 8 2 13 12 13 13 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
11/28/2021 01:25:17 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
11/28/2021 01:25:17 - INFO - utils_tag -   *** Example ***
11/28/2021 01:25:17 - INFO - utils_tag -   guid: pt-5
11/28/2021 01:25:17 - INFO - utils_tag -   tokens: [CLS] « Vivo num Estado de Iron ##ia » . [SEP]
11/28/2021 01:25:17 - INFO - utils_tag -   input_ids: 101 208 86394 22436 14359 10104 19247 10280 220 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:25:17 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:25:17 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:25:17 - INFO - utils_tag -   label_ids: -100 13 16 2 8 2 8 -100 13 13 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
11/28/2021 01:25:17 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Evaluating:  14%|█▍        | 29/205 [00:09<00:55,  3.20it/s]11/28/2021 01:25:17 - INFO - __main__ -   Batch number = 30
Evaluating:  15%|█▍        | 30/205 [00:09<00:54,  3.19it/s]11/28/2021 01:25:17 - INFO - __main__ -   Batch number = 31
Evaluating:  15%|█▌        | 31/205 [00:09<00:54,  3.20it/s]11/28/2021 01:25:18 - INFO - __main__ -   Batch number = 32
Evaluating:  16%|█▌        | 32/205 [00:10<00:54,  3.20it/s]11/28/2021 01:25:18 - INFO - __main__ -   Batch number = 33
Evaluating:  16%|█▌        | 33/205 [00:10<00:53,  3.20it/s]11/28/2021 01:25:18 - INFO - __main__ -   Batch number = 34
Evaluating:  17%|█▋        | 34/205 [00:10<00:53,  3.19it/s]11/28/2021 01:25:19 - INFO - __main__ -   Batch number = 35
Evaluating:  17%|█▋        | 35/205 [00:11<00:52,  3.21it/s]11/28/2021 01:25:19 - INFO - __main__ -   Batch number = 36
Evaluating:  18%|█▊        | 36/205 [00:11<00:52,  3.19it/s]11/28/2021 01:25:19 - INFO - __main__ -   Batch number = 37
Evaluating:  18%|█▊        | 37/205 [00:11<00:52,  3.19it/s]11/28/2021 01:25:19 - INFO - __main__ -   Batch number = 38
Evaluating:  19%|█▊        | 38/205 [00:12<00:52,  3.20it/s]11/28/2021 01:25:20 - INFO - __main__ -   Batch number = 39
Evaluating:  19%|█▉        | 39/205 [00:12<00:51,  3.21it/s]11/28/2021 01:25:20 - INFO - __main__ -   Batch number = 40
11/28/2021 01:25:20 - INFO - __main__ -   Saving features into cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_pt_bert-base-multilingual-cased_128, len(features)=2682
Evaluating:  20%|█▉        | 40/205 [00:12<00:51,  3.21it/s]11/28/2021 01:25:20 - INFO - __main__ -   Batch number = 41
Evaluating:  20%|██        | 41/205 [00:12<00:50,  3.23it/s]11/28/2021 01:25:21 - INFO - __main__ -   Batch number = 42
11/28/2021 01:25:21 - INFO - __main__ -   ***** Running evaluation  in pt *****
11/28/2021 01:25:21 - INFO - __main__ -     Num examples = 2682
11/28/2021 01:25:21 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/84 [00:00<?, ?it/s]11/28/2021 01:25:21 - INFO - __main__ -   Batch number = 1
Evaluating:  20%|██        | 42/205 [00:13<00:55,  2.95it/s]11/28/2021 01:25:21 - INFO - __main__ -   Batch number = 43
Evaluating:   1%|          | 1/84 [00:00<00:39,  2.10it/s]11/28/2021 01:25:21 - INFO - __main__ -   Batch number = 2
Evaluating:  21%|██        | 43/205 [00:13<01:01,  2.64it/s]11/28/2021 01:25:22 - INFO - __main__ -   Batch number = 44
Evaluating:   2%|▏         | 2/84 [00:00<00:38,  2.13it/s]11/28/2021 01:25:22 - INFO - __main__ -   Batch number = 3
Evaluating:  21%|██▏       | 44/205 [00:14<01:05,  2.47it/s]11/28/2021 01:25:22 - INFO - __main__ -   Batch number = 45
Evaluating:   4%|▎         | 3/84 [00:01<00:37,  2.13it/s]11/28/2021 01:25:22 - INFO - __main__ -   Batch number = 4
Evaluating:  22%|██▏       | 45/205 [00:14<01:07,  2.36it/s]11/28/2021 01:25:23 - INFO - __main__ -   Batch number = 46
Evaluating:   5%|▍         | 4/84 [00:01<00:37,  2.15it/s]11/28/2021 01:25:23 - INFO - __main__ -   Batch number = 5
Evaluating:  22%|██▏       | 46/205 [00:15<01:09,  2.29it/s]11/28/2021 01:25:23 - INFO - __main__ -   Batch number = 47
Evaluating:   6%|▌         | 5/84 [00:02<00:36,  2.15it/s]11/28/2021 01:25:23 - INFO - __main__ -   Batch number = 6
Evaluating:  23%|██▎       | 47/205 [00:15<01:10,  2.23it/s]11/28/2021 01:25:23 - INFO - __main__ -   Batch number = 48
Evaluating:   7%|▋         | 6/84 [00:02<00:36,  2.15it/s]11/28/2021 01:25:24 - INFO - __main__ -   Batch number = 7
Evaluating:  23%|██▎       | 48/205 [00:16<01:11,  2.21it/s]Evaluating:   8%|▊         | 7/84 [00:03<00:33,  2.33it/s]11/28/2021 01:25:24 - INFO - __main__ -   Batch number = 8
11/28/2021 01:25:24 - INFO - __main__ -   Batch number = 49
Evaluating:  10%|▉         | 8/84 [00:03<00:34,  2.21it/s]11/28/2021 01:25:24 - INFO - __main__ -   Batch number = 9
Evaluating:  24%|██▍       | 49/205 [00:16<01:15,  2.07it/s]11/28/2021 01:25:24 - INFO - __main__ -   Batch number = 50
Evaluating:  11%|█         | 9/84 [00:04<00:34,  2.20it/s]11/28/2021 01:25:25 - INFO - __main__ -   Batch number = 10
Evaluating:  24%|██▍       | 50/205 [00:17<01:14,  2.09it/s]11/28/2021 01:25:25 - INFO - __main__ -   Batch number = 51
Evaluating:  12%|█▏        | 10/84 [00:04<00:33,  2.19it/s]11/28/2021 01:25:25 - INFO - __main__ -   Batch number = 11
Evaluating:  25%|██▍       | 51/205 [00:17<01:13,  2.10it/s]11/28/2021 01:25:25 - INFO - __main__ -   Batch number = 52
Evaluating:  13%|█▎        | 11/84 [00:05<00:33,  2.18it/s]11/28/2021 01:25:26 - INFO - __main__ -   Batch number = 12
Evaluating:  25%|██▌       | 52/205 [00:18<01:12,  2.10it/s]11/28/2021 01:25:26 - INFO - __main__ -   Batch number = 53
Evaluating:  14%|█▍        | 12/84 [00:05<00:33,  2.17it/s]11/28/2021 01:25:26 - INFO - __main__ -   Batch number = 13
Evaluating:  26%|██▌       | 53/205 [00:18<01:12,  2.11it/s]11/28/2021 01:25:26 - INFO - __main__ -   Batch number = 54
Evaluating:  15%|█▌        | 13/84 [00:05<00:32,  2.16it/s]11/28/2021 01:25:27 - INFO - __main__ -   Batch number = 14
Evaluating:  26%|██▋       | 54/205 [00:19<01:11,  2.11it/s]11/28/2021 01:25:27 - INFO - __main__ -   Batch number = 55
Evaluating:  17%|█▋        | 14/84 [00:06<00:32,  2.16it/s]11/28/2021 01:25:27 - INFO - __main__ -   Batch number = 15
Evaluating:  27%|██▋       | 55/205 [00:19<01:10,  2.11it/s]11/28/2021 01:25:27 - INFO - __main__ -   Batch number = 56
Evaluating:  18%|█▊        | 15/84 [00:06<00:31,  2.21it/s]11/28/2021 01:25:28 - INFO - __main__ -   Batch number = 16
Evaluating:  27%|██▋       | 56/205 [00:19<01:07,  2.20it/s]11/28/2021 01:25:28 - INFO - __main__ -   Batch number = 57
Evaluating:  19%|█▉        | 16/84 [00:07<00:28,  2.39it/s]11/28/2021 01:25:28 - INFO - __main__ -   Batch number = 17
Evaluating:  28%|██▊       | 57/205 [00:20<01:01,  2.40it/s]11/28/2021 01:25:28 - INFO - __main__ -   Batch number = 58
Evaluating:  20%|██        | 17/84 [00:07<00:25,  2.59it/s]11/28/2021 01:25:28 - INFO - __main__ -   Batch number = 18
Evaluating:  28%|██▊       | 58/205 [00:20<00:56,  2.59it/s]11/28/2021 01:25:28 - INFO - __main__ -   Batch number = 59
Evaluating:  21%|██▏       | 18/84 [00:07<00:24,  2.74it/s]11/28/2021 01:25:29 - INFO - __main__ -   Batch number = 19
Evaluating:  29%|██▉       | 59/205 [00:20<00:53,  2.73it/s]11/28/2021 01:25:29 - INFO - __main__ -   Batch number = 60
Evaluating:  23%|██▎       | 19/84 [00:08<00:22,  2.86it/s]11/28/2021 01:25:29 - INFO - __main__ -   Batch number = 20
Evaluating:  29%|██▉       | 60/205 [00:21<00:50,  2.85it/s]11/28/2021 01:25:29 - INFO - __main__ -   Batch number = 61
Evaluating:  24%|██▍       | 20/84 [00:08<00:21,  2.94it/s]11/28/2021 01:25:29 - INFO - __main__ -   Batch number = 21
Evaluating:  30%|██▉       | 61/205 [00:21<00:48,  2.97it/s]11/28/2021 01:25:29 - INFO - __main__ -   Batch number = 62
Evaluating:  25%|██▌       | 21/84 [00:08<00:20,  3.02it/s]11/28/2021 01:25:30 - INFO - __main__ -   Batch number = 22
Evaluating:  30%|███       | 62/205 [00:21<00:47,  3.03it/s]11/28/2021 01:25:30 - INFO - __main__ -   Batch number = 63
Evaluating:  26%|██▌       | 22/84 [00:09<00:20,  3.09it/s]11/28/2021 01:25:30 - INFO - __main__ -   Batch number = 23
Evaluating:  31%|███       | 63/205 [00:22<00:46,  3.07it/s]11/28/2021 01:25:30 - INFO - __main__ -   Batch number = 64
Evaluating:  27%|██▋       | 23/84 [00:09<00:19,  3.14it/s]11/28/2021 01:25:30 - INFO - __main__ -   Batch number = 24
Evaluating:  31%|███       | 64/205 [00:22<00:45,  3.10it/s]11/28/2021 01:25:30 - INFO - __main__ -   Batch number = 65
Evaluating:  29%|██▊       | 24/84 [00:09<00:18,  3.17it/s]11/28/2021 01:25:31 - INFO - __main__ -   Batch number = 25
Evaluating:  32%|███▏      | 65/205 [00:22<00:44,  3.12it/s]11/28/2021 01:25:31 - INFO - __main__ -   Batch number = 66
Evaluating:  30%|██▉       | 25/84 [00:10<00:18,  3.18it/s]11/28/2021 01:25:31 - INFO - __main__ -   Batch number = 26
Evaluating:  32%|███▏      | 66/205 [00:23<00:44,  3.13it/s]11/28/2021 01:25:31 - INFO - __main__ -   Batch number = 67
Evaluating:  31%|███       | 26/84 [00:10<00:18,  3.19it/s]11/28/2021 01:25:31 - INFO - __main__ -   Batch number = 27
Evaluating:  33%|███▎      | 67/205 [00:23<00:43,  3.15it/s]11/28/2021 01:25:31 - INFO - __main__ -   Batch number = 68
Evaluating:  32%|███▏      | 27/84 [00:10<00:17,  3.20it/s]11/28/2021 01:25:31 - INFO - __main__ -   Batch number = 28
Evaluating:  33%|███▎      | 68/205 [00:23<00:43,  3.16it/s]11/28/2021 01:25:31 - INFO - __main__ -   Batch number = 69
Evaluating:  33%|███▎      | 28/84 [00:10<00:17,  3.22it/s]11/28/2021 01:25:32 - INFO - __main__ -   Batch number = 29
Evaluating:  34%|███▎      | 69/205 [00:24<00:42,  3.17it/s]11/28/2021 01:25:32 - INFO - __main__ -   Batch number = 70
Evaluating:  35%|███▍      | 29/84 [00:11<00:16,  3.24it/s]11/28/2021 01:25:32 - INFO - __main__ -   Batch number = 30
Evaluating:  34%|███▍      | 70/205 [00:24<00:43,  3.12it/s]11/28/2021 01:25:32 - INFO - __main__ -   Batch number = 71
Evaluating:  36%|███▌      | 30/84 [00:11<00:19,  2.79it/s]11/28/2021 01:25:33 - INFO - __main__ -   Batch number = 31
Evaluating:  35%|███▍      | 71/205 [00:24<00:48,  2.77it/s]11/28/2021 01:25:33 - INFO - __main__ -   Batch number = 72
Evaluating:  37%|███▋      | 31/84 [00:12<00:20,  2.54it/s]11/28/2021 01:25:33 - INFO - __main__ -   Batch number = 32
Evaluating:  35%|███▌      | 72/205 [00:25<00:52,  2.54it/s]11/28/2021 01:25:33 - INFO - __main__ -   Batch number = 73
Evaluating:  38%|███▊      | 32/84 [00:12<00:21,  2.41it/s]11/28/2021 01:25:33 - INFO - __main__ -   Batch number = 33
Evaluating:  36%|███▌      | 73/205 [00:25<00:55,  2.40it/s]11/28/2021 01:25:34 - INFO - __main__ -   Batch number = 74
Evaluating:  39%|███▉      | 33/84 [00:13<00:21,  2.32it/s]11/28/2021 01:25:34 - INFO - __main__ -   Batch number = 34
Evaluating:  36%|███▌      | 74/205 [00:26<00:55,  2.36it/s]11/28/2021 01:25:34 - INFO - __main__ -   Batch number = 75
Evaluating:  40%|████      | 34/84 [00:13<00:22,  2.26it/s]11/28/2021 01:25:34 - INFO - __main__ -   Batch number = 35
Evaluating:  37%|███▋      | 75/205 [00:26<00:57,  2.28it/s]11/28/2021 01:25:34 - INFO - __main__ -   Batch number = 76
Evaluating:  42%|████▏     | 35/84 [00:14<00:21,  2.24it/s]11/28/2021 01:25:35 - INFO - __main__ -   Batch number = 36
Evaluating:  37%|███▋      | 76/205 [00:27<00:57,  2.23it/s]11/28/2021 01:25:35 - INFO - __main__ -   Batch number = 77
Evaluating:  43%|████▎     | 36/84 [00:14<00:21,  2.22it/s]11/28/2021 01:25:35 - INFO - __main__ -   Batch number = 37
Evaluating:  38%|███▊      | 77/205 [00:27<00:58,  2.19it/s]11/28/2021 01:25:35 - INFO - __main__ -   Batch number = 78
Evaluating:  44%|████▍     | 37/84 [00:14<00:21,  2.20it/s]11/28/2021 01:25:36 - INFO - __main__ -   Batch number = 38
Evaluating:  38%|███▊      | 78/205 [00:28<00:58,  2.17it/s]11/28/2021 01:25:36 - INFO - __main__ -   Batch number = 79
Evaluating:  45%|████▌     | 38/84 [00:15<00:21,  2.18it/s]11/28/2021 01:25:36 - INFO - __main__ -   Batch number = 39
Evaluating:  39%|███▊      | 79/205 [00:28<00:58,  2.16it/s]11/28/2021 01:25:36 - INFO - __main__ -   Batch number = 80
Evaluating:  46%|████▋     | 39/84 [00:15<00:20,  2.17it/s]11/28/2021 01:25:37 - INFO - __main__ -   Batch number = 40
Evaluating:  39%|███▉      | 80/205 [00:29<00:58,  2.15it/s]11/28/2021 01:25:37 - INFO - __main__ -   Batch number = 81
Evaluating:  48%|████▊     | 40/84 [00:16<00:20,  2.16it/s]11/28/2021 01:25:37 - INFO - __main__ -   Batch number = 41
Evaluating:  40%|███▉      | 81/205 [00:29<00:57,  2.14it/s]11/28/2021 01:25:37 - INFO - __main__ -   Batch number = 82
Evaluating:  49%|████▉     | 41/84 [00:16<00:19,  2.16it/s]11/28/2021 01:25:38 - INFO - __main__ -   Batch number = 42
Evaluating:  40%|████      | 82/205 [00:29<00:57,  2.14it/s]11/28/2021 01:25:38 - INFO - __main__ -   Batch number = 83
Evaluating:  50%|█████     | 42/84 [00:17<00:19,  2.16it/s]11/28/2021 01:25:38 - INFO - __main__ -   Batch number = 43
Evaluating:  40%|████      | 83/205 [00:30<00:57,  2.14it/s]11/28/2021 01:25:38 - INFO - __main__ -   Batch number = 84
Evaluating:  51%|█████     | 43/84 [00:17<00:19,  2.15it/s]11/28/2021 01:25:39 - INFO - __main__ -   Batch number = 44
Evaluating:  41%|████      | 84/205 [00:30<00:56,  2.13it/s]11/28/2021 01:25:39 - INFO - __main__ -   Batch number = 85
Evaluating:  52%|█████▏    | 44/84 [00:18<00:18,  2.21it/s]11/28/2021 01:25:39 - INFO - __main__ -   Batch number = 45
Evaluating:  41%|████▏     | 85/205 [00:31<00:55,  2.18it/s]11/28/2021 01:25:39 - INFO - __main__ -   Batch number = 86
Evaluating:  54%|█████▎    | 45/84 [00:18<00:17,  2.19it/s]11/28/2021 01:25:39 - INFO - __main__ -   Batch number = 46
Evaluating:  42%|████▏     | 86/205 [00:31<00:55,  2.16it/s]11/28/2021 01:25:40 - INFO - __main__ -   Batch number = 87
Evaluating:  55%|█████▍    | 46/84 [00:19<00:17,  2.18it/s]11/28/2021 01:25:40 - INFO - __main__ -   Batch number = 47
Evaluating:  42%|████▏     | 87/205 [00:32<00:54,  2.17it/s]11/28/2021 01:25:40 - INFO - __main__ -   Batch number = 88
Evaluating:  56%|█████▌    | 47/84 [00:19<00:16,  2.18it/s]11/28/2021 01:25:40 - INFO - __main__ -   Batch number = 48
Evaluating:  43%|████▎     | 88/205 [00:32<00:54,  2.16it/s]11/28/2021 01:25:41 - INFO - __main__ -   Batch number = 89
Evaluating:  57%|█████▋    | 48/84 [00:20<00:16,  2.17it/s]11/28/2021 01:25:41 - INFO - __main__ -   Batch number = 49
Evaluating:  43%|████▎     | 89/205 [00:33<00:53,  2.15it/s]11/28/2021 01:25:41 - INFO - __main__ -   Batch number = 90
Evaluating:  58%|█████▊    | 49/84 [00:20<00:16,  2.16it/s]11/28/2021 01:25:41 - INFO - __main__ -   Batch number = 50
Evaluating:  44%|████▍     | 90/205 [00:33<00:53,  2.15it/s]11/28/2021 01:25:41 - INFO - __main__ -   Batch number = 91
Evaluating:  60%|█████▉    | 50/84 [00:20<00:15,  2.15it/s]11/28/2021 01:25:42 - INFO - __main__ -   Batch number = 51
Evaluating:  44%|████▍     | 91/205 [00:34<00:52,  2.15it/s]11/28/2021 01:25:42 - INFO - __main__ -   Batch number = 92
Evaluating:  61%|██████    | 51/84 [00:21<00:15,  2.16it/s]11/28/2021 01:25:42 - INFO - __main__ -   Batch number = 52
Evaluating:  45%|████▍     | 92/205 [00:34<00:52,  2.15it/s]11/28/2021 01:25:42 - INFO - __main__ -   Batch number = 93
Evaluating:  62%|██████▏   | 52/84 [00:21<00:14,  2.14it/s]11/28/2021 01:25:43 - INFO - __main__ -   Batch number = 53
Evaluating:  45%|████▌     | 93/205 [00:35<00:52,  2.15it/s]11/28/2021 01:25:43 - INFO - __main__ -   Batch number = 94
Evaluating:  63%|██████▎   | 53/84 [00:22<00:14,  2.14it/s]11/28/2021 01:25:43 - INFO - __main__ -   Batch number = 54
Evaluating:  46%|████▌     | 94/205 [00:35<00:51,  2.16it/s]11/28/2021 01:25:43 - INFO - __main__ -   Batch number = 95
Evaluating:  64%|██████▍   | 54/84 [00:22<00:13,  2.18it/s]11/28/2021 01:25:44 - INFO - __main__ -   Batch number = 55
Evaluating:  46%|████▋     | 95/205 [00:36<00:51,  2.14it/s]11/28/2021 01:25:44 - INFO - __main__ -   Batch number = 96
Evaluating:  65%|██████▌   | 55/84 [00:23<00:13,  2.17it/s]11/28/2021 01:25:44 - INFO - __main__ -   Batch number = 56
Evaluating:  47%|████▋     | 96/205 [00:36<00:51,  2.13it/s]11/28/2021 01:25:44 - INFO - __main__ -   Batch number = 97
Evaluating:  67%|██████▋   | 56/84 [00:23<00:12,  2.16it/s]11/28/2021 01:25:45 - INFO - __main__ -   Batch number = 57
Evaluating:  47%|████▋     | 97/205 [00:36<00:50,  2.13it/s]11/28/2021 01:25:45 - INFO - __main__ -   Batch number = 98
Evaluating:  68%|██████▊   | 57/84 [00:24<00:12,  2.16it/s]11/28/2021 01:25:45 - INFO - __main__ -   Batch number = 58
Evaluating:  48%|████▊     | 98/205 [00:37<00:50,  2.13it/s]11/28/2021 01:25:45 - INFO - __main__ -   Batch number = 99
Evaluating:  69%|██████▉   | 58/84 [00:24<00:12,  2.14it/s]11/28/2021 01:25:46 - INFO - __main__ -   Batch number = 59
Evaluating:  48%|████▊     | 99/205 [00:37<00:49,  2.12it/s]11/28/2021 01:25:46 - INFO - __main__ -   Batch number = 100
Evaluating:  70%|███████   | 59/84 [00:25<00:11,  2.14it/s]11/28/2021 01:25:46 - INFO - __main__ -   Batch number = 60
Evaluating:  49%|████▉     | 100/205 [00:38<00:49,  2.12it/s]11/28/2021 01:25:46 - INFO - __main__ -   Batch number = 101
Evaluating:  71%|███████▏  | 60/84 [00:25<00:11,  2.15it/s]11/28/2021 01:25:46 - INFO - __main__ -   Batch number = 61
Evaluating:  49%|████▉     | 101/205 [00:38<00:48,  2.14it/s]11/28/2021 01:25:47 - INFO - __main__ -   Batch number = 102
Evaluating:  73%|███████▎  | 61/84 [00:26<00:10,  2.13it/s]11/28/2021 01:25:47 - INFO - __main__ -   Batch number = 62
Evaluating:  50%|████▉     | 102/205 [00:39<00:48,  2.13it/s]11/28/2021 01:25:47 - INFO - __main__ -   Batch number = 103
Evaluating:  74%|███████▍  | 62/84 [00:26<00:10,  2.14it/s]11/28/2021 01:25:47 - INFO - __main__ -   Batch number = 63
Evaluating:  50%|█████     | 103/205 [00:39<00:47,  2.13it/s]11/28/2021 01:25:48 - INFO - __main__ -   Batch number = 104
Evaluating:  75%|███████▌  | 63/84 [00:27<00:09,  2.14it/s]11/28/2021 01:25:48 - INFO - __main__ -   Batch number = 64
Evaluating:  51%|█████     | 104/205 [00:40<00:47,  2.13it/s]11/28/2021 01:25:48 - INFO - __main__ -   Batch number = 105
Evaluating:  76%|███████▌  | 64/84 [00:27<00:09,  2.14it/s]11/28/2021 01:25:48 - INFO - __main__ -   Batch number = 65
Evaluating:  51%|█████     | 105/205 [00:40<00:47,  2.12it/s]11/28/2021 01:25:48 - INFO - __main__ -   Batch number = 106
Evaluating:  77%|███████▋  | 65/84 [00:27<00:08,  2.14it/s]11/28/2021 01:25:49 - INFO - __main__ -   Batch number = 66
Evaluating:  52%|█████▏    | 106/205 [00:41<00:46,  2.12it/s]11/28/2021 01:25:49 - INFO - __main__ -   Batch number = 107
Evaluating:  79%|███████▊  | 66/84 [00:28<00:08,  2.15it/s]11/28/2021 01:25:49 - INFO - __main__ -   Batch number = 67
Evaluating:  52%|█████▏    | 107/205 [00:41<00:46,  2.11it/s]11/28/2021 01:25:49 - INFO - __main__ -   Batch number = 108
Evaluating:  80%|███████▉  | 67/84 [00:28<00:07,  2.15it/s]11/28/2021 01:25:50 - INFO - __main__ -   Batch number = 68
Evaluating:  53%|█████▎    | 108/205 [00:42<00:46,  2.10it/s]11/28/2021 01:25:50 - INFO - __main__ -   Batch number = 109
Evaluating:  81%|████████  | 68/84 [00:29<00:07,  2.15it/s]11/28/2021 01:25:50 - INFO - __main__ -   Batch number = 69
Evaluating:  53%|█████▎    | 109/205 [00:42<00:45,  2.11it/s]11/28/2021 01:25:50 - INFO - __main__ -   Batch number = 110
Evaluating:  82%|████████▏ | 69/84 [00:29<00:06,  2.15it/s]11/28/2021 01:25:51 - INFO - __main__ -   Batch number = 70
Evaluating:  54%|█████▎    | 110/205 [00:43<00:44,  2.11it/s]11/28/2021 01:25:51 - INFO - __main__ -   Batch number = 111
Evaluating:  83%|████████▎ | 70/84 [00:30<00:06,  2.14it/s]11/28/2021 01:25:51 - INFO - __main__ -   Batch number = 71
Evaluating:  54%|█████▍    | 111/205 [00:43<00:44,  2.12it/s]11/28/2021 01:25:51 - INFO - __main__ -   Batch number = 112
Evaluating:  85%|████████▍ | 71/84 [00:30<00:06,  2.13it/s]11/28/2021 01:25:52 - INFO - __main__ -   Batch number = 72
Evaluating:  55%|█████▍    | 112/205 [00:44<00:44,  2.10it/s]11/28/2021 01:25:52 - INFO - __main__ -   Batch number = 113
Evaluating:  86%|████████▌ | 72/84 [00:31<00:05,  2.15it/s]11/28/2021 01:25:52 - INFO - __main__ -   Batch number = 73
Evaluating:  55%|█████▌    | 113/205 [00:44<00:43,  2.10it/s]11/28/2021 01:25:52 - INFO - __main__ -   Batch number = 114
Evaluating:  87%|████████▋ | 73/84 [00:31<00:05,  2.15it/s]11/28/2021 01:25:52 - INFO - __main__ -   Batch number = 74
Evaluating:  56%|█████▌    | 114/205 [00:45<00:43,  2.09it/s]11/28/2021 01:25:53 - INFO - __main__ -   Batch number = 115
Evaluating:  88%|████████▊ | 74/84 [00:32<00:04,  2.15it/s]11/28/2021 01:25:53 - INFO - __main__ -   Batch number = 75
Evaluating:  56%|█████▌    | 115/205 [00:45<00:42,  2.09it/s]11/28/2021 01:25:53 - INFO - __main__ -   Batch number = 116
Evaluating:  89%|████████▉ | 75/84 [00:32<00:04,  2.15it/s]11/28/2021 01:25:53 - INFO - __main__ -   Batch number = 76
Evaluating:  57%|█████▋    | 116/205 [00:45<00:42,  2.09it/s]11/28/2021 01:25:54 - INFO - __main__ -   Batch number = 117
Evaluating:  90%|█████████ | 76/84 [00:33<00:03,  2.15it/s]11/28/2021 01:25:54 - INFO - __main__ -   Batch number = 77
Evaluating:  57%|█████▋    | 117/205 [00:46<00:41,  2.11it/s]11/28/2021 01:25:54 - INFO - __main__ -   Batch number = 118
Evaluating:  92%|█████████▏| 77/84 [00:33<00:03,  2.13it/s]11/28/2021 01:25:54 - INFO - __main__ -   Batch number = 78
Evaluating:  58%|█████▊    | 118/205 [00:46<00:41,  2.10it/s]11/28/2021 01:25:55 - INFO - __main__ -   Batch number = 119
Evaluating:  93%|█████████▎| 78/84 [00:34<00:02,  2.14it/s]11/28/2021 01:25:55 - INFO - __main__ -   Batch number = 79
Evaluating:  58%|█████▊    | 119/205 [00:47<00:41,  2.10it/s]11/28/2021 01:25:55 - INFO - __main__ -   Batch number = 120
PyTorch version 1.10.0+cu102 available.
Evaluating:  94%|█████████▍| 79/84 [00:34<00:02,  2.12it/s]11/28/2021 01:25:55 - INFO - __main__ -   Batch number = 80
11/28/2021 01:25:56 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='zh', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:25:56 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:25:56 - INFO - __main__ -   Seed = 1
11/28/2021 01:25:56 - INFO - root -   save model
11/28/2021 01:25:56 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='zh', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:25:56 - INFO - __main__ -   Loading pretrained model and tokenizer
Evaluating:  59%|█████▊    | 120/205 [00:48<00:44,  1.93it/s]11/28/2021 01:25:56 - INFO - __main__ -   Batch number = 121
Evaluating:  95%|█████████▌| 80/84 [00:35<00:02,  1.93it/s]11/28/2021 01:25:56 - INFO - __main__ -   Batch number = 81
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  59%|█████▉    | 121/205 [00:48<00:46,  1.81it/s]11/28/2021 01:25:56 - INFO - __main__ -   Batch number = 122
Evaluating:  96%|█████████▋| 81/84 [00:35<00:01,  1.83it/s]11/28/2021 01:25:57 - INFO - __main__ -   Batch number = 82
Evaluating:  60%|█████▉    | 122/205 [00:49<00:47,  1.74it/s]11/28/2021 01:25:57 - INFO - __main__ -   Batch number = 123
Evaluating:  98%|█████████▊| 82/84 [00:36<00:01,  1.75it/s]11/28/2021 01:25:57 - INFO - __main__ -   Batch number = 83
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  60%|██████    | 123/205 [00:49<00:48,  1.69it/s]11/28/2021 01:25:58 - INFO - __main__ -   Batch number = 124
Evaluating:  99%|█████████▉| 83/84 [00:36<00:00,  1.70it/s]11/28/2021 01:25:58 - INFO - __main__ -   Batch number = 84
loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:25:58 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
Evaluating:  60%|██████    | 124/205 [00:50<00:48,  1.65it/s]11/28/2021 01:25:58 - INFO - __main__ -   Batch number = 125
Evaluating: 100%|██████████| 84/84 [00:37<00:00,  1.77it/s]Evaluating: 100%|██████████| 84/84 [00:37<00:00,  2.24it/s]Evaluating:  61%|██████    | 125/205 [00:51<00:45,  1.77it/s]11/28/2021 01:25:59 - INFO - __main__ -   Batch number = 126
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  61%|██████▏   | 126/205 [00:51<00:39,  2.02it/s]11/28/2021 01:25:59 - INFO - __main__ -   Batch number = 127
Evaluating:  62%|██████▏   | 127/205 [00:51<00:38,  2.04it/s]11/28/2021 01:26:00 - INFO - __main__ -   Batch number = 128

/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:26:00 - INFO - __main__ -   ***** Evaluation result  in pt *****
11/28/2021 01:26:00 - INFO - __main__ -     f1 = 0.8766631835597352
11/28/2021 01:26:00 - INFO - __main__ -     loss = 0.4160561121645428
11/28/2021 01:26:00 - INFO - __main__ -     precision = 0.8836033632326353
11/28/2021 01:26:00 - INFO - __main__ -     recall = 0.8698311762368025
Evaluating:  62%|██████▏   | 128/205 [00:52<00:37,  2.05it/s]11/28/2021 01:26:00 - INFO - __main__ -   Batch number = 129
Evaluating:  63%|██████▎   | 129/205 [00:52<00:37,  2.05it/s]11/28/2021 01:26:01 - INFO - __main__ -   Batch number = 130
Evaluating:  63%|██████▎   | 130/205 [00:53<00:36,  2.08it/s]11/28/2021 01:26:01 - INFO - __main__ -   Batch number = 131
51.12user 16.84system 1:06.93elapsed 101%CPU (0avgtext+0avgdata 3932388maxresident)k
1792inputs+9640outputs (0major+1756058minor)pagefaults 0swaps
Evaluating:  64%|██████▍   | 131/205 [00:53<00:35,  2.08it/s]11/28/2021 01:26:02 - INFO - __main__ -   Batch number = 132
Evaluating:  64%|██████▍   | 132/205 [00:54<00:35,  2.08it/s]11/28/2021 01:26:02 - INFO - __main__ -   Batch number = 133
Evaluating:  65%|██████▍   | 133/205 [00:54<00:34,  2.08it/s]11/28/2021 01:26:02 - INFO - __main__ -   Batch number = 134
PyTorch version 1.10.0+cu102 available.
Evaluating:  65%|██████▌   | 134/205 [00:55<00:34,  2.08it/s]11/28/2021 01:26:03 - INFO - __main__ -   Batch number = 135
11/28/2021 01:26:03 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='pt', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:26:03 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:26:03 - INFO - __main__ -   Seed = 2
11/28/2021 01:26:03 - INFO - root -   save model
11/28/2021 01:26:03 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='pt', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:26:03 - INFO - __main__ -   Loading pretrained model and tokenizer
Evaluating:  66%|██████▌   | 135/205 [00:55<00:33,  2.08it/s]11/28/2021 01:26:03 - INFO - __main__ -   Batch number = 136
Evaluating:  66%|██████▋   | 136/205 [00:56<00:33,  2.08it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
11/28/2021 01:26:04 - INFO - __main__ -   Batch number = 137
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  67%|██████▋   | 137/205 [00:56<00:34,  2.00it/s]11/28/2021 01:26:04 - INFO - __main__ -   Batch number = 138
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  67%|██████▋   | 138/205 [00:57<00:33,  2.03it/s]11/28/2021 01:26:05 - INFO - __main__ -   Batch number = 139
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:26:05 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:26:05 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:26:05 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
11/28/2021 01:26:05 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:26:05 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:26:05 - INFO - root -   loading lang adpater am/wiki@ukp
11/28/2021 01:26:05 - INFO - __main__ -   Adapter Languages : ['am'], Length : 1
11/28/2021 01:26:05 - INFO - __main__ -   Adapter Names ['am/wiki@ukp'], Length : 1
11/28/2021 01:26:05 - INFO - __main__ -   Language = am
11/28/2021 01:26:05 - INFO - __main__ -   Adapter Name = am/wiki@ukp
Found matching adapter at: adapters/ukp/bert-base-multilingual-cased_am_wiki_pfeiffer.json
Evaluating:  68%|██████▊   | 139/205 [00:57<00:32,  2.04it/s]11/28/2021 01:26:05 - INFO - __main__ -   Batch number = 140
loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:26:06 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/am/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_am_wiki_pfeiffer.zip.
Evaluating:  68%|██████▊   | 140/205 [00:58<00:31,  2.05it/s]11/28/2021 01:26:06 - INFO - __main__ -   Batch number = 141
Evaluating:  69%|██████▉   | 141/205 [00:58<00:31,  2.06it/s]11/28/2021 01:26:06 - INFO - __main__ -   Batch number = 142
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  69%|██████▉   | 142/205 [00:59<00:30,  2.07it/s]11/28/2021 01:26:07 - INFO - __main__ -   Batch number = 143
Evaluating:  70%|██████▉   | 143/205 [00:59<00:29,  2.08it/s]11/28/2021 01:26:07 - INFO - __main__ -   Batch number = 144
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/adapter_config.json
Adding adapter 'am' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/head_config.json
Evaluating:  70%|███████   | 144/205 [01:00<00:29,  2.08it/s]11/28/2021 01:26:08 - INFO - __main__ -   Batch number = 145
Evaluating:  71%|███████   | 145/205 [01:00<00:29,  2.04it/s]11/28/2021 01:26:08 - INFO - __main__ -   Batch number = 146
Evaluating:  71%|███████   | 146/205 [01:01<00:28,  2.05it/s]11/28/2021 01:26:09 - INFO - __main__ -   Batch number = 147
Evaluating:  72%|███████▏  | 147/205 [01:01<00:28,  2.06it/s]11/28/2021 01:26:09 - INFO - __main__ -   Batch number = 148
Evaluating:  72%|███████▏  | 148/205 [01:02<00:27,  2.06it/s]11/28/2021 01:26:10 - INFO - __main__ -   Batch number = 149
Evaluating:  73%|███████▎  | 149/205 [01:02<00:27,  2.07it/s]11/28/2021 01:26:10 - INFO - __main__ -   Batch number = 150
Evaluating:  73%|███████▎  | 150/205 [01:02<00:26,  2.07it/s]11/28/2021 01:26:11 - INFO - __main__ -   Batch number = 151
Evaluating:  74%|███████▎  | 151/205 [01:03<00:26,  2.07it/s]11/28/2021 01:26:11 - INFO - __main__ -   Batch number = 152
Evaluating:  74%|███████▍  | 152/205 [01:03<00:25,  2.07it/s]11/28/2021 01:26:12 - INFO - __main__ -   Batch number = 153
Evaluating:  75%|███████▍  | 153/205 [01:04<00:25,  2.07it/s]11/28/2021 01:26:12 - INFO - __main__ -   Batch number = 154
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:26:13 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:26:13 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:26:13 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
11/28/2021 01:26:13 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:26:13 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:26:13 - INFO - root -   loading lang adpater am/wiki@ukp
11/28/2021 01:26:13 - INFO - __main__ -   Adapter Languages : ['am'], Length : 1
11/28/2021 01:26:13 - INFO - __main__ -   Adapter Names ['am/wiki@ukp'], Length : 1
11/28/2021 01:26:13 - INFO - __main__ -   Language = am
11/28/2021 01:26:13 - INFO - __main__ -   Adapter Name = am/wiki@ukp
Found matching adapter at: adapters/ukp/bert-base-multilingual-cased_am_wiki_pfeiffer.json
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/am/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_am_wiki_pfeiffer.zip.
Evaluating:  75%|███████▌  | 154/205 [01:04<00:24,  2.06it/s]11/28/2021 01:26:13 - INFO - __main__ -   Batch number = 155
Evaluating:  76%|███████▌  | 155/205 [01:05<00:24,  2.06it/s]11/28/2021 01:26:13 - INFO - __main__ -   Batch number = 156
Evaluating:  76%|███████▌  | 156/205 [01:05<00:23,  2.07it/s]11/28/2021 01:26:14 - INFO - __main__ -   Batch number = 157
Evaluating:  77%|███████▋  | 157/205 [01:06<00:23,  2.07it/s]11/28/2021 01:26:14 - INFO - __main__ -   Batch number = 158
Evaluating:  77%|███████▋  | 158/205 [01:06<00:23,  2.01it/s]11/28/2021 01:26:15 - INFO - __main__ -   Batch number = 159
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/adapter_config.json
Adding adapter 'am' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/head_config.json
Evaluating:  78%|███████▊  | 159/205 [01:07<00:22,  2.04it/s]11/28/2021 01:26:15 - INFO - __main__ -   Batch number = 160
Evaluating:  78%|███████▊  | 160/205 [01:07<00:21,  2.05it/s]11/28/2021 01:26:16 - INFO - __main__ -   Batch number = 161
11/28/2021 01:26:16 - INFO - __main__ -   Language adapter for zh not found, using am instead
11/28/2021 01:26:16 - INFO - __main__ -   Set active language adapter to am
11/28/2021 01:26:16 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:26:16 - INFO - __main__ -   Adapter Languages = ['am']
11/28/2021 01:26:16 - INFO - __main__ -   all languages = zh
11/28/2021 01:26:16 - INFO - __main__ -   Creating features from dataset file at /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/zh/test.bert-base-multilingual-cased in language zh
11/28/2021 01:26:16 - INFO - utils_tag -   lang_id=0, lang=zh, lang2id=None
11/28/2021 01:26:16 - INFO - utils_tag -   Writing example 0 of 3458
11/28/2021 01:26:16 - INFO - utils_tag -   *** Example ***
11/28/2021 01:26:16 - INFO - utils_tag -   guid: zh-1
11/28/2021 01:26:16 - INFO - utils_tag -   tokens: [CLS] 2012 年 12 月 我 在 韩 国 留 学 的 时 候 ， 有 一 天 接 到 一 个 通 知 。 [SEP]
11/28/2021 01:26:16 - INFO - utils_tag -   input_ids: 101 10185 3642 10186 4460 3976 3031 8363 3014 5625 3365 5718 4356 2339 10064 4461 2072 3198 4161 2555 2072 2102 7735 5817 1882 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:26:16 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:26:16 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:26:16 - INFO - utils_tag -   label_ids: -100 8 -100 8 -100 11 2 12 -100 16 -100 10 8 -100 13 16 9 8 16 16 9 8 8 -100 13 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
11/28/2021 01:26:16 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
11/28/2021 01:26:16 - INFO - utils_tag -   *** Example ***
11/28/2021 01:26:16 - INFO - utils_tag -   guid: zh-2
11/28/2021 01:26:16 - INFO - utils_tag -   tokens: [CLS] 就 是 学 校 带 我 们 留 学 生 去 旅 行 。 [SEP]
11/28/2021 01:26:16 - INFO - utils_tag -   input_ids: 101 3472 4380 3365 4572 3616 3976 2206 5625 3365 5600 2724 4340 7069 1882 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:26:16 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:26:16 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:26:16 - INFO - utils_tag -   label_ids: -100 3 16 8 -100 16 11 -100 8 -100 -100 16 16 -100 13 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
11/28/2021 01:26:16 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
11/28/2021 01:26:16 - INFO - utils_tag -   *** Example ***
11/28/2021 01:26:16 - INFO - utils_tag -   guid: zh-3
11/28/2021 01:26:16 - INFO - utils_tag -   tokens: [CLS] 地 方 是 韩 国 和 朝 鲜 的 过 境 处 。 [SEP]
11/28/2021 01:26:16 - INFO - utils_tag -   input_ids: 101 3035 4335 4380 8363 3014 2833 4469 8685 5718 7693 3139 3182 1882 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:26:16 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:26:16 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:26:16 - INFO - utils_tag -   label_ids: -100 8 -100 4 12 -100 5 12 -100 10 8 -100 -100 13 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
11/28/2021 01:26:16 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
11/28/2021 01:26:16 - INFO - utils_tag -   *** Example ***
11/28/2021 01:26:16 - INFO - utils_tag -   guid: zh-4
11/28/2021 01:26:16 - INFO - utils_tag -   tokens: [CLS] 虽 然 我 和 我 的 朋 友 不 想 去 ， 但 [UNK] 机 不 可 失 时 不 再 来 [UNK] ， 我 给 朋 友 这 样 告 诉 了 以 后 她 勉 强 同 意 跟 我 一 起 去 过 境 处 。 [SEP]
11/28/2021 01:26:16 - INFO - utils_tag -   input_ids: 101 6959 5322 3976 2833 3976 5718 4462 2731 2080 3898 2724 10064 2243 100 4482 2080 2756 3204 4356 2080 2475 4501 100 10064 3976 6342 4462 2731 7700 4577 2810 7327 2146 2204 2775 3239 2617 3731 2773 3906 7554 3976 2072 7533 2724 7693 3139 3182 1882 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:26:16 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:26:16 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:26:16 - INFO - utils_tag -   label_ids: -100 14 -100 11 5 11 10 8 -100 3 4 16 13 3 13 8 -100 -100 -100 8 -100 -100 -100 13 13 11 2 8 -100 3 -100 16 -100 4 2 -100 11 1 -100 16 -100 2 11 3 -100 16 8 -100 -100 13 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
11/28/2021 01:26:16 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
11/28/2021 01:26:16 - INFO - utils_tag -   *** Example ***
11/28/2021 01:26:16 - INFO - utils_tag -   guid: zh-5
11/28/2021 01:26:16 - INFO - utils_tag -   tokens: [CLS] 我 们 做 学 校 的 巴 士 去 那 里 ， 终 于 到 了 向 往 的 地 方 。 [SEP]
11/28/2021 01:26:16 - INFO - utils_tag -   input_ids: 101 3976 2206 2363 3365 4572 5718 3594 3169 2724 7802 7906 10064 6332 2151 2555 2146 2778 3762 5718 3035 4335 1882 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:26:16 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:26:16 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
11/28/2021 01:26:16 - INFO - utils_tag -   label_ids: -100 11 -100 16 8 -100 10 8 -100 16 11 -100 13 3 -100 16 4 16 -100 10 8 -100 13 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
11/28/2021 01:26:16 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Evaluating:  79%|███████▊  | 161/205 [01:08<00:23,  1.87it/s]11/28/2021 01:26:16 - INFO - __main__ -   Batch number = 162
Evaluating:  79%|███████▉  | 162/205 [01:09<00:22,  1.88it/s]11/28/2021 01:26:17 - INFO - __main__ -   Batch number = 163
Evaluating:  80%|███████▉  | 163/205 [01:09<00:24,  1.75it/s]11/28/2021 01:26:17 - INFO - __main__ -   Batch number = 164
Evaluating:  80%|████████  | 164/205 [01:10<00:22,  1.83it/s]11/28/2021 01:26:18 - INFO - __main__ -   Batch number = 165
Evaluating:  80%|████████  | 165/205 [01:10<00:23,  1.72it/s]11/28/2021 01:26:19 - INFO - __main__ -   Batch number = 166
Evaluating:  81%|████████  | 166/205 [01:11<00:21,  1.81it/s]11/28/2021 01:26:19 - INFO - __main__ -   Batch number = 167
11/28/2021 01:26:19 - INFO - __main__ -   Saving features into cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_zh_bert-base-multilingual-cased_128, len(features)=3458
Evaluating:  81%|████████▏ | 167/205 [01:11<00:20,  1.87it/s]11/28/2021 01:26:20 - INFO - __main__ -   Batch number = 168
11/28/2021 01:26:20 - INFO - __main__ -   ***** Running evaluation  in zh *****
11/28/2021 01:26:20 - INFO - __main__ -     Num examples = 3458
11/28/2021 01:26:20 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/109 [00:00<?, ?it/s]11/28/2021 01:26:20 - INFO - __main__ -   Batch number = 1
Evaluating:  82%|████████▏ | 168/205 [01:12<00:19,  1.91it/s]11/28/2021 01:26:20 - INFO - __main__ -   Batch number = 169
Evaluating:   1%|          | 1/109 [00:00<00:53,  2.02it/s]11/28/2021 01:26:20 - INFO - __main__ -   Batch number = 2
Evaluating:  82%|████████▏ | 169/205 [01:12<00:18,  1.94it/s]11/28/2021 01:26:21 - INFO - __main__ -   Batch number = 170
Evaluating:   2%|▏         | 2/109 [00:00<00:51,  2.10it/s]11/28/2021 01:26:21 - INFO - __main__ -   Batch number = 3
Evaluating:  83%|████████▎ | 170/205 [01:13<00:17,  1.97it/s]11/28/2021 01:26:21 - INFO - __main__ -   Batch number = 171
Evaluating:   3%|▎         | 3/109 [00:01<00:50,  2.11it/s]11/28/2021 01:26:21 - INFO - __main__ -   Batch number = 4
Evaluating:  83%|████████▎ | 171/205 [01:13<00:17,  2.00it/s]11/28/2021 01:26:22 - INFO - __main__ -   Batch number = 172
Evaluating:   4%|▎         | 4/109 [00:01<00:49,  2.12it/s]11/28/2021 01:26:22 - INFO - __main__ -   Batch number = 5
Evaluating:  84%|████████▍ | 172/205 [01:14<00:16,  2.02it/s]11/28/2021 01:26:22 - INFO - __main__ -   Batch number = 173
Evaluating:   5%|▍         | 5/109 [00:02<00:48,  2.12it/s]11/28/2021 01:26:22 - INFO - __main__ -   Batch number = 6
Evaluating:  84%|████████▍ | 173/205 [01:14<00:15,  2.04it/s]11/28/2021 01:26:22 - INFO - __main__ -   Batch number = 174
Evaluating:   6%|▌         | 6/109 [00:02<00:48,  2.13it/s]11/28/2021 01:26:23 - INFO - __main__ -   Batch number = 7
Evaluating:  85%|████████▍ | 174/205 [01:15<00:15,  2.06it/s]11/28/2021 01:26:23 - INFO - __main__ -   Batch number = 175
Evaluating:   6%|▋         | 7/109 [00:03<00:48,  2.12it/s]11/28/2021 01:26:23 - INFO - __main__ -   Batch number = 8
Evaluating:  85%|████████▌ | 175/205 [01:15<00:13,  2.15it/s]11/28/2021 01:26:23 - INFO - __main__ -   Batch number = 176
Evaluating:  86%|████████▌ | 176/205 [01:15<00:12,  2.34it/s]11/28/2021 01:26:24 - INFO - __main__ -   Batch number = 177
Evaluating:   7%|▋         | 8/109 [00:03<00:48,  2.10it/s]11/28/2021 01:26:24 - INFO - __main__ -   Batch number = 9
Evaluating:  86%|████████▋ | 177/205 [01:16<00:11,  2.55it/s]11/28/2021 01:26:24 - INFO - __main__ -   Batch number = 178
Evaluating:   8%|▊         | 9/109 [00:04<00:47,  2.10it/s]11/28/2021 01:26:24 - INFO - __main__ -   Batch number = 10
Evaluating:  87%|████████▋ | 178/205 [01:16<00:10,  2.69it/s]11/28/2021 01:26:24 - INFO - __main__ -   Batch number = 179
Evaluating:  87%|████████▋ | 179/205 [01:16<00:09,  2.86it/s]11/28/2021 01:26:25 - INFO - __main__ -   Batch number = 180
Evaluating:   9%|▉         | 10/109 [00:04<00:46,  2.11it/s]11/28/2021 01:26:25 - INFO - __main__ -   Batch number = 11
Evaluating:  88%|████████▊ | 180/205 [01:17<00:08,  3.03it/s]11/28/2021 01:26:25 - INFO - __main__ -   Batch number = 181
Evaluating:  10%|█         | 11/109 [00:05<00:46,  2.12it/s]11/28/2021 01:26:25 - INFO - __main__ -   Batch number = 12
Evaluating:  88%|████████▊ | 181/205 [01:17<00:07,  3.14it/s]11/28/2021 01:26:25 - INFO - __main__ -   Batch number = 182
Evaluating:  89%|████████▉ | 182/205 [01:17<00:07,  3.21it/s]11/28/2021 01:26:26 - INFO - __main__ -   Batch number = 183
Evaluating:  11%|█         | 12/109 [00:05<00:45,  2.13it/s]11/28/2021 01:26:26 - INFO - __main__ -   Batch number = 13
Evaluating:  89%|████████▉ | 183/205 [01:18<00:06,  3.23it/s]11/28/2021 01:26:26 - INFO - __main__ -   Batch number = 184
Evaluating:  12%|█▏        | 13/109 [00:06<00:44,  2.13it/s]11/28/2021 01:26:26 - INFO - __main__ -   Batch number = 14
Evaluating:  90%|████████▉ | 184/205 [01:18<00:06,  3.25it/s]11/28/2021 01:26:26 - INFO - __main__ -   Batch number = 185
11/28/2021 01:26:26 - INFO - __main__ -   Language adapter for pt not found, using am instead
11/28/2021 01:26:26 - INFO - __main__ -   Set active language adapter to am
11/28/2021 01:26:26 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:26:26 - INFO - __main__ -   Adapter Languages = ['am']
11/28/2021 01:26:26 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_pt_bert-base-multilingual-cased_128
Evaluating:  90%|█████████ | 185/205 [01:18<00:06,  3.25it/s]11/28/2021 01:26:26 - INFO - __main__ -   Batch number = 186
Evaluating:  13%|█▎        | 14/109 [00:06<00:44,  2.13it/s]11/28/2021 01:26:27 - INFO - __main__ -   Batch number = 15
Evaluating:  91%|█████████ | 186/205 [01:18<00:05,  3.23it/s]11/28/2021 01:26:27 - INFO - __main__ -   Batch number = 187
11/28/2021 01:26:27 - INFO - __main__ -   ***** Running evaluation  in pt *****
11/28/2021 01:26:27 - INFO - __main__ -     Num examples = 2682
11/28/2021 01:26:27 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/84 [00:00<?, ?it/s]11/28/2021 01:26:27 - INFO - __main__ -   Batch number = 1
Evaluating:  14%|█▍        | 15/109 [00:07<00:44,  2.12it/s]11/28/2021 01:26:27 - INFO - __main__ -   Batch number = 16
Evaluating:  91%|█████████ | 187/205 [01:19<00:05,  3.19it/s]11/28/2021 01:26:27 - INFO - __main__ -   Batch number = 188
Evaluating:   1%|          | 1/84 [00:00<00:38,  2.13it/s]11/28/2021 01:26:27 - INFO - __main__ -   Batch number = 2
Evaluating:  15%|█▍        | 16/109 [00:07<00:43,  2.12it/s]11/28/2021 01:26:28 - INFO - __main__ -   Batch number = 17
Evaluating:  92%|█████████▏| 188/205 [01:19<00:06,  2.74it/s]11/28/2021 01:26:28 - INFO - __main__ -   Batch number = 189
Evaluating:   2%|▏         | 2/84 [00:00<00:37,  2.19it/s]11/28/2021 01:26:28 - INFO - __main__ -   Batch number = 3
Evaluating:  16%|█▌        | 17/109 [00:08<00:43,  2.13it/s]11/28/2021 01:26:28 - INFO - __main__ -   Batch number = 18
Evaluating:  92%|█████████▏| 189/205 [01:20<00:06,  2.50it/s]11/28/2021 01:26:28 - INFO - __main__ -   Batch number = 190
Evaluating:   4%|▎         | 3/84 [00:01<00:36,  2.20it/s]11/28/2021 01:26:28 - INFO - __main__ -   Batch number = 4
Evaluating:  17%|█▋        | 18/109 [00:08<00:42,  2.13it/s]11/28/2021 01:26:28 - INFO - __main__ -   Batch number = 19
Evaluating:  93%|█████████▎| 190/205 [01:20<00:06,  2.35it/s]11/28/2021 01:26:29 - INFO - __main__ -   Batch number = 191
Evaluating:   5%|▍         | 4/84 [00:01<00:36,  2.22it/s]11/28/2021 01:26:29 - INFO - __main__ -   Batch number = 5
Evaluating:  17%|█▋        | 19/109 [00:08<00:42,  2.13it/s]Evaluating:  93%|█████████▎| 191/205 [01:21<00:06,  2.26it/s]Evaluating:   6%|▌         | 5/84 [00:02<00:32,  2.43it/s]11/28/2021 01:26:29 - INFO - __main__ -   Batch number = 20
11/28/2021 01:26:29 - INFO - __main__ -   Batch number = 192
11/28/2021 01:26:29 - INFO - __main__ -   Batch number = 6
Evaluating:  18%|█▊        | 20/109 [00:09<00:47,  1.87it/s]11/28/2021 01:26:30 - INFO - __main__ -   Batch number = 21
Evaluating:   7%|▋         | 6/84 [00:02<00:35,  2.22it/s]11/28/2021 01:26:30 - INFO - __main__ -   Batch number = 7
Evaluating:  94%|█████████▎| 192/205 [01:21<00:06,  1.97it/s]11/28/2021 01:26:30 - INFO - __main__ -   Batch number = 193
Evaluating:   8%|▊         | 7/84 [00:03<00:32,  2.37it/s]11/28/2021 01:26:30 - INFO - __main__ -   Batch number = 8
Evaluating:  94%|█████████▍| 193/205 [01:22<00:05,  2.03it/s]11/28/2021 01:26:30 - INFO - __main__ -   Batch number = 194
Evaluating:  19%|█▉        | 21/109 [00:10<00:46,  1.90it/s]11/28/2021 01:26:30 - INFO - __main__ -   Batch number = 22
Evaluating:  10%|▉         | 8/84 [00:03<00:31,  2.42it/s]11/28/2021 01:26:30 - INFO - __main__ -   Batch number = 9
Evaluating:  95%|█████████▍| 194/205 [01:22<00:05,  2.07it/s]11/28/2021 01:26:31 - INFO - __main__ -   Batch number = 195
Evaluating:  20%|██        | 22/109 [00:10<00:47,  1.82it/s]11/28/2021 01:26:31 - INFO - __main__ -   Batch number = 23
Evaluating:  11%|█         | 9/84 [00:03<00:30,  2.46it/s]11/28/2021 01:26:31 - INFO - __main__ -   Batch number = 10
Evaluating:  95%|█████████▌| 195/205 [01:23<00:04,  2.08it/s]11/28/2021 01:26:31 - INFO - __main__ -   Batch number = 196
Evaluating:  12%|█▏        | 10/84 [00:04<00:30,  2.44it/s]11/28/2021 01:26:31 - INFO - __main__ -   Batch number = 11
Evaluating:  21%|██        | 23/109 [00:11<00:49,  1.75it/s]11/28/2021 01:26:31 - INFO - __main__ -   Batch number = 24
Evaluating:  96%|█████████▌| 196/205 [01:23<00:04,  2.06it/s]11/28/2021 01:26:32 - INFO - __main__ -   Batch number = 197
Evaluating:  13%|█▎        | 11/84 [00:04<00:30,  2.41it/s]11/28/2021 01:26:32 - INFO - __main__ -   Batch number = 12
Evaluating:  22%|██▏       | 24/109 [00:12<00:49,  1.70it/s]11/28/2021 01:26:32 - INFO - __main__ -   Batch number = 25
Evaluating:  96%|█████████▌| 197/205 [01:24<00:03,  2.05it/s]11/28/2021 01:26:32 - INFO - __main__ -   Batch number = 198
Evaluating:  14%|█▍        | 12/84 [00:05<00:30,  2.38it/s]11/28/2021 01:26:32 - INFO - __main__ -   Batch number = 13
Evaluating:  15%|█▌        | 13/84 [00:05<00:30,  2.31it/s]11/28/2021 01:26:33 - INFO - __main__ -   Batch number = 14
Evaluating:  97%|█████████▋| 198/205 [01:24<00:03,  2.04it/s]11/28/2021 01:26:33 - INFO - __main__ -   Batch number = 199
Evaluating:  23%|██▎       | 25/109 [00:12<00:50,  1.68it/s]11/28/2021 01:26:33 - INFO - __main__ -   Batch number = 26
Evaluating:  17%|█▋        | 14/84 [00:06<00:30,  2.29it/s]11/28/2021 01:26:33 - INFO - __main__ -   Batch number = 15
Evaluating:  97%|█████████▋| 199/205 [01:25<00:02,  2.03it/s]11/28/2021 01:26:33 - INFO - __main__ -   Batch number = 200
Evaluating:  24%|██▍       | 26/109 [00:13<00:50,  1.66it/s]11/28/2021 01:26:33 - INFO - __main__ -   Batch number = 27
Evaluating:  18%|█▊        | 15/84 [00:06<00:29,  2.31it/s]11/28/2021 01:26:33 - INFO - __main__ -   Batch number = 16
Evaluating:  98%|█████████▊| 200/205 [01:25<00:02,  2.02it/s]11/28/2021 01:26:34 - INFO - __main__ -   Batch number = 201
Evaluating:  19%|█▉        | 16/84 [00:06<00:29,  2.31it/s]11/28/2021 01:26:34 - INFO - __main__ -   Batch number = 17
Evaluating:  25%|██▍       | 27/109 [00:13<00:49,  1.64it/s]11/28/2021 01:26:34 - INFO - __main__ -   Batch number = 28
Evaluating:  98%|█████████▊| 201/205 [01:26<00:01,  2.02it/s]Evaluating:  20%|██        | 17/84 [00:07<00:25,  2.60it/s]11/28/2021 01:26:34 - INFO - __main__ -   Batch number = 18
11/28/2021 01:26:34 - INFO - __main__ -   Batch number = 202
Evaluating:  26%|██▌       | 28/109 [00:14<00:49,  1.65it/s]11/28/2021 01:26:34 - INFO - __main__ -   Batch number = 29
Evaluating:  21%|██▏       | 18/84 [00:07<00:26,  2.44it/s]11/28/2021 01:26:35 - INFO - __main__ -   Batch number = 19
Evaluating:  99%|█████████▊| 202/205 [01:26<00:01,  1.93it/s]11/28/2021 01:26:35 - INFO - __main__ -   Batch number = 203
Evaluating:  23%|██▎       | 19/84 [00:08<00:26,  2.47it/s]11/28/2021 01:26:35 - INFO - __main__ -   Batch number = 20
Evaluating:  27%|██▋       | 29/109 [00:15<00:48,  1.64it/s]11/28/2021 01:26:35 - INFO - __main__ -   Batch number = 30
Evaluating:  99%|█████████▉| 203/205 [01:27<00:01,  1.98it/s]11/28/2021 01:26:35 - INFO - __main__ -   Batch number = 204
Evaluating:  24%|██▍       | 20/84 [00:08<00:26,  2.45it/s]11/28/2021 01:26:35 - INFO - __main__ -   Batch number = 21
Evaluating: 100%|█████████▉| 204/205 [01:27<00:00,  1.99it/s]11/28/2021 01:26:36 - INFO - __main__ -   Batch number = 205
Evaluating:  28%|██▊       | 30/109 [00:15<00:48,  1.63it/s]11/28/2021 01:26:36 - INFO - __main__ -   Batch number = 31
Evaluating:  25%|██▌       | 21/84 [00:08<00:26,  2.42it/s]11/28/2021 01:26:36 - INFO - __main__ -   Batch number = 22
Evaluating: 100%|██████████| 205/205 [01:28<00:00,  2.16it/s]Evaluating: 100%|██████████| 205/205 [01:28<00:00,  2.32it/s]Evaluating:  26%|██▌       | 22/84 [00:09<00:24,  2.56it/s]11/28/2021 01:26:36 - INFO - __main__ -   Batch number = 23
Evaluating:  28%|██▊       | 31/109 [00:16<00:48,  1.62it/s]11/28/2021 01:26:36 - INFO - __main__ -   Batch number = 32
Evaluating:  27%|██▋       | 23/84 [00:09<00:22,  2.76it/s]11/28/2021 01:26:36 - INFO - __main__ -   Batch number = 24
Evaluating:  29%|██▊       | 24/84 [00:09<00:20,  2.92it/s]11/28/2021 01:26:37 - INFO - __main__ -   Batch number = 25
Evaluating:  29%|██▉       | 32/109 [00:16<00:47,  1.61it/s]11/28/2021 01:26:37 - INFO - __main__ -   Batch number = 33
Evaluating:  30%|██▉       | 25/84 [00:10<00:19,  3.00it/s]11/28/2021 01:26:37 - INFO - __main__ -   Batch number = 26
Evaluating:  31%|███       | 26/84 [00:10<00:18,  3.07it/s]11/28/2021 01:26:37 - INFO - __main__ -   Batch number = 27
Evaluating:  30%|███       | 33/109 [00:17<00:46,  1.62it/s]11/28/2021 01:26:38 - INFO - __main__ -   Batch number = 34
Evaluating:  32%|███▏      | 27/84 [00:10<00:18,  3.15it/s]11/28/2021 01:26:38 - INFO - __main__ -   Batch number = 28
Evaluating:  33%|███▎      | 28/84 [00:10<00:17,  3.21it/s]11/28/2021 01:26:38 - INFO - __main__ -   Batch number = 29

/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:26:38 - INFO - __main__ -   ***** Evaluation result  in fi *****
11/28/2021 01:26:38 - INFO - __main__ -     f1 = 0.7373963874798961
11/28/2021 01:26:38 - INFO - __main__ -     loss = 0.9928989725868876
11/28/2021 01:26:38 - INFO - __main__ -     precision = 0.7432120702016896
11/28/2021 01:26:38 - INFO - __main__ -     recall = 0.7316710142703698
Evaluating:  31%|███       | 34/109 [00:18<00:46,  1.62it/s]11/28/2021 01:26:38 - INFO - __main__ -   Batch number = 35
Evaluating:  35%|███▍      | 29/84 [00:11<00:16,  3.25it/s]11/28/2021 01:26:38 - INFO - __main__ -   Batch number = 30
Evaluating:  36%|███▌      | 30/84 [00:11<00:16,  3.30it/s]11/28/2021 01:26:39 - INFO - __main__ -   Batch number = 31
Evaluating:  32%|███▏      | 35/109 [00:18<00:45,  1.61it/s]11/28/2021 01:26:39 - INFO - __main__ -   Batch number = 36
Evaluating:  37%|███▋      | 31/84 [00:11<00:15,  3.33it/s]11/28/2021 01:26:39 - INFO - __main__ -   Batch number = 32
82.04user 29.70system 1:51.02elapsed 100%CPU (0avgtext+0avgdata 3939136maxresident)k
0inputs+920outputs (0major+1764739minor)pagefaults 0swaps
Evaluating:  38%|███▊      | 32/84 [00:12<00:15,  3.36it/s]11/28/2021 01:26:39 - INFO - __main__ -   Batch number = 33
Evaluating:  33%|███▎      | 36/109 [00:19<00:45,  1.61it/s]11/28/2021 01:26:39 - INFO - __main__ -   Batch number = 37
Evaluating:  39%|███▉      | 33/84 [00:12<00:15,  3.28it/s]11/28/2021 01:26:39 - INFO - __main__ -   Batch number = 34
Evaluating:  40%|████      | 34/84 [00:12<00:15,  3.31it/s]11/28/2021 01:26:40 - INFO - __main__ -   Batch number = 35
Evaluating:  34%|███▍      | 37/109 [00:20<00:44,  1.61it/s]11/28/2021 01:26:40 - INFO - __main__ -   Batch number = 38
Evaluating:  42%|████▏     | 35/84 [00:13<00:14,  3.32it/s]11/28/2021 01:26:40 - INFO - __main__ -   Batch number = 36
Evaluating:  43%|████▎     | 36/84 [00:13<00:14,  3.33it/s]11/28/2021 01:26:40 - INFO - __main__ -   Batch number = 37
PyTorch version 1.10.0+cu102 available.
Evaluating:  44%|████▍     | 37/84 [00:13<00:14,  3.34it/s]11/28/2021 01:26:41 - INFO - __main__ -   Batch number = 38
Evaluating:  35%|███▍      | 38/109 [00:20<00:43,  1.63it/s]11/28/2021 01:26:41 - INFO - __main__ -   Batch number = 39
11/28/2021 01:26:41 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:26:41 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:26:41 - INFO - __main__ -   Seed = 3
11/28/2021 01:26:41 - INFO - root -   save model
11/28/2021 01:26:41 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:26:41 - INFO - __main__ -   Loading pretrained model and tokenizer
Evaluating:  45%|████▌     | 38/84 [00:13<00:13,  3.34it/s]11/28/2021 01:26:41 - INFO - __main__ -   Batch number = 39
Evaluating:  36%|███▌      | 39/109 [00:21<00:40,  1.73it/s]11/28/2021 01:26:41 - INFO - __main__ -   Batch number = 40
Evaluating:  46%|████▋     | 39/84 [00:14<00:13,  3.33it/s]11/28/2021 01:26:41 - INFO - __main__ -   Batch number = 40
Evaluating:  48%|████▊     | 40/84 [00:14<00:13,  3.31it/s]11/28/2021 01:26:42 - INFO - __main__ -   Batch number = 41
Evaluating:  37%|███▋      | 40/109 [00:21<00:37,  1.85it/s]11/28/2021 01:26:42 - INFO - __main__ -   Batch number = 41
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  49%|████▉     | 41/84 [00:14<00:13,  3.24it/s]11/28/2021 01:26:42 - INFO - __main__ -   Batch number = 42
Evaluating:  38%|███▊      | 41/109 [00:22<00:35,  1.94it/s]11/28/2021 01:26:42 - INFO - __main__ -   Batch number = 42
Evaluating:  50%|█████     | 42/84 [00:15<00:12,  3.26it/s]11/28/2021 01:26:42 - INFO - __main__ -   Batch number = 43
Evaluating:  51%|█████     | 43/84 [00:15<00:12,  3.31it/s]11/28/2021 01:26:42 - INFO - __main__ -   Batch number = 44
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  39%|███▊      | 42/109 [00:22<00:33,  1.99it/s]11/28/2021 01:26:43 - INFO - __main__ -   Batch number = 43
Evaluating:  52%|█████▏    | 44/84 [00:15<00:12,  3.31it/s]11/28/2021 01:26:43 - INFO - __main__ -   Batch number = 45
Evaluating:  39%|███▉      | 43/109 [00:22<00:32,  2.04it/s]11/28/2021 01:26:43 - INFO - __main__ -   Batch number = 44
Evaluating:  54%|█████▎    | 45/84 [00:16<00:11,  3.34it/s]11/28/2021 01:26:43 - INFO - __main__ -   Batch number = 46
loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
Evaluating:  55%|█████▍    | 46/84 [00:16<00:11,  3.36it/s]11/28/2021 01:26:43 - INFO - __main__ -   Batch number = 47
Evaluating:  40%|████      | 44/109 [00:23<00:31,  2.08it/s]11/28/2021 01:26:43 - INFO - __main__ -   Batch number = 45
11/28/2021 01:26:44 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
Evaluating:  56%|█████▌    | 47/84 [00:16<00:11,  3.35it/s]11/28/2021 01:26:44 - INFO - __main__ -   Batch number = 48
Evaluating:  41%|████▏     | 45/109 [00:23<00:30,  2.11it/s]11/28/2021 01:26:44 - INFO - __main__ -   Batch number = 46
Evaluating:  57%|█████▋    | 48/84 [00:16<00:10,  3.35it/s]11/28/2021 01:26:44 - INFO - __main__ -   Batch number = 49
Evaluating:  58%|█████▊    | 49/84 [00:17<00:10,  3.28it/s]Evaluating:  42%|████▏     | 46/109 [00:24<00:28,  2.20it/s]11/28/2021 01:26:44 - INFO - __main__ -   Batch number = 50
11/28/2021 01:26:44 - INFO - __main__ -   Batch number = 47
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  60%|█████▉    | 50/84 [00:17<00:10,  3.15it/s]11/28/2021 01:26:45 - INFO - __main__ -   Batch number = 51
Evaluating:  43%|████▎     | 47/109 [00:24<00:28,  2.16it/s]11/28/2021 01:26:45 - INFO - __main__ -   Batch number = 48
Evaluating:  61%|██████    | 51/84 [00:17<00:10,  3.29it/s]11/28/2021 01:26:45 - INFO - __main__ -   Batch number = 52
Evaluating:  62%|██████▏   | 52/84 [00:18<00:09,  3.41it/s]11/28/2021 01:26:45 - INFO - __main__ -   Batch number = 53
Evaluating:  44%|████▍     | 48/109 [00:25<00:28,  2.17it/s]11/28/2021 01:26:45 - INFO - __main__ -   Batch number = 49
Evaluating:  63%|██████▎   | 53/84 [00:18<00:08,  3.50it/s]11/28/2021 01:26:45 - INFO - __main__ -   Batch number = 54
Evaluating:  64%|██████▍   | 54/84 [00:18<00:08,  3.59it/s]11/28/2021 01:26:46 - INFO - __main__ -   Batch number = 55
Evaluating:  45%|████▍     | 49/109 [00:25<00:27,  2.15it/s]11/28/2021 01:26:46 - INFO - __main__ -   Batch number = 50
Evaluating:  65%|██████▌   | 55/84 [00:19<00:08,  3.52it/s]11/28/2021 01:26:46 - INFO - __main__ -   Batch number = 56
Evaluating:  46%|████▌     | 50/109 [00:26<00:27,  2.15it/s]11/28/2021 01:26:46 - INFO - __main__ -   Batch number = 51
Evaluating:  67%|██████▋   | 56/84 [00:19<00:08,  3.40it/s]11/28/2021 01:26:46 - INFO - __main__ -   Batch number = 57
Evaluating:  68%|██████▊   | 57/84 [00:19<00:08,  3.36it/s]11/28/2021 01:26:47 - INFO - __main__ -   Batch number = 58
Evaluating:  47%|████▋     | 51/109 [00:26<00:26,  2.15it/s]11/28/2021 01:26:47 - INFO - __main__ -   Batch number = 52
Evaluating:  69%|██████▉   | 58/84 [00:19<00:07,  3.37it/s]11/28/2021 01:26:47 - INFO - __main__ -   Batch number = 59
Evaluating:  48%|████▊     | 52/109 [00:27<00:26,  2.14it/s]11/28/2021 01:26:47 - INFO - __main__ -   Batch number = 53
Evaluating:  70%|███████   | 59/84 [00:20<00:07,  3.37it/s]11/28/2021 01:26:47 - INFO - __main__ -   Batch number = 60
Evaluating:  71%|███████▏  | 60/84 [00:20<00:07,  3.40it/s]11/28/2021 01:26:48 - INFO - __main__ -   Batch number = 61
Evaluating:  49%|████▊     | 53/109 [00:27<00:26,  2.14it/s]11/28/2021 01:26:48 - INFO - __main__ -   Batch number = 54
Evaluating:  73%|███████▎  | 61/84 [00:20<00:07,  3.08it/s]11/28/2021 01:26:48 - INFO - __main__ -   Batch number = 62
Evaluating:  50%|████▉     | 54/109 [00:28<00:25,  2.14it/s]11/28/2021 01:26:48 - INFO - __main__ -   Batch number = 55
Evaluating:  74%|███████▍  | 62/84 [00:21<00:08,  2.74it/s]11/28/2021 01:26:48 - INFO - __main__ -   Batch number = 63
Evaluating:  50%|█████     | 55/109 [00:28<00:25,  2.14it/s]11/28/2021 01:26:49 - INFO - __main__ -   Batch number = 56
Evaluating:  75%|███████▌  | 63/84 [00:21<00:08,  2.54it/s]11/28/2021 01:26:49 - INFO - __main__ -   Batch number = 64
Evaluating:  51%|█████▏    | 56/109 [00:29<00:24,  2.13it/s]11/28/2021 01:26:49 - INFO - __main__ -   Batch number = 57
Evaluating:  76%|███████▌  | 64/84 [00:22<00:08,  2.43it/s]11/28/2021 01:26:49 - INFO - __main__ -   Batch number = 65
Evaluating:  52%|█████▏    | 57/109 [00:29<00:24,  2.13it/s]11/28/2021 01:26:49 - INFO - __main__ -   Batch number = 58
Evaluating:  77%|███████▋  | 65/84 [00:22<00:08,  2.35it/s]11/28/2021 01:26:50 - INFO - __main__ -   Batch number = 66
Evaluating:  53%|█████▎    | 58/109 [00:29<00:23,  2.13it/s]11/28/2021 01:26:50 - INFO - __main__ -   Batch number = 59
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:26:50 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:26:50 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:26:50 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
11/28/2021 01:26:50 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:26:50 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:26:50 - INFO - root -   loading lang adpater am/wiki@ukp
11/28/2021 01:26:50 - INFO - __main__ -   Adapter Languages : ['am'], Length : 1
11/28/2021 01:26:50 - INFO - __main__ -   Adapter Names ['am/wiki@ukp'], Length : 1
11/28/2021 01:26:50 - INFO - __main__ -   Language = am
11/28/2021 01:26:50 - INFO - __main__ -   Adapter Name = am/wiki@ukp
Evaluating:  79%|███████▊  | 66/84 [00:23<00:07,  2.28it/s]Found matching adapter at: adapters/ukp/bert-base-multilingual-cased_am_wiki_pfeiffer.json
11/28/2021 01:26:50 - INFO - __main__ -   Batch number = 67
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/am/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_am_wiki_pfeiffer.zip.
Evaluating:  54%|█████▍    | 59/109 [00:30<00:23,  2.13it/s]11/28/2021 01:26:50 - INFO - __main__ -   Batch number = 60
Evaluating:  80%|███████▉  | 67/84 [00:23<00:07,  2.23it/s]11/28/2021 01:26:51 - INFO - __main__ -   Batch number = 68
Evaluating:  55%|█████▌    | 60/109 [00:30<00:22,  2.13it/s]11/28/2021 01:26:51 - INFO - __main__ -   Batch number = 61
Evaluating:  81%|████████  | 68/84 [00:24<00:07,  2.20it/s]11/28/2021 01:26:51 - INFO - __main__ -   Batch number = 69
Evaluating:  56%|█████▌    | 61/109 [00:31<00:22,  2.14it/s]11/28/2021 01:26:51 - INFO - __main__ -   Batch number = 62
Evaluating:  82%|████████▏ | 69/84 [00:24<00:06,  2.18it/s]11/28/2021 01:26:52 - INFO - __main__ -   Batch number = 70
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/adapter_config.json
Adding adapter 'am' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/head_config.json
Evaluating:  57%|█████▋    | 62/109 [00:31<00:22,  2.12it/s]11/28/2021 01:26:52 - INFO - __main__ -   Batch number = 63
Evaluating:  83%|████████▎ | 70/84 [00:25<00:06,  2.20it/s]11/28/2021 01:26:52 - INFO - __main__ -   Batch number = 71
Evaluating:  58%|█████▊    | 63/109 [00:32<00:21,  2.11it/s]11/28/2021 01:26:52 - INFO - __main__ -   Batch number = 64
Evaluating:  85%|████████▍ | 71/84 [00:25<00:05,  2.19it/s]11/28/2021 01:26:53 - INFO - __main__ -   Batch number = 72
Evaluating:  59%|█████▊    | 64/109 [00:32<00:21,  2.09it/s]11/28/2021 01:26:53 - INFO - __main__ -   Batch number = 65
Evaluating:  86%|████████▌ | 72/84 [00:26<00:05,  2.16it/s]11/28/2021 01:26:53 - INFO - __main__ -   Batch number = 73
Evaluating:  60%|█████▉    | 65/109 [00:33<00:21,  2.08it/s]11/28/2021 01:26:53 - INFO - __main__ -   Batch number = 66
Evaluating:  87%|████████▋ | 73/84 [00:26<00:05,  2.15it/s]11/28/2021 01:26:53 - INFO - __main__ -   Batch number = 74
Evaluating:  61%|██████    | 66/109 [00:33<00:20,  2.09it/s]11/28/2021 01:26:54 - INFO - __main__ -   Batch number = 67
Evaluating:  88%|████████▊ | 74/84 [00:26<00:04,  2.16it/s]11/28/2021 01:26:54 - INFO - __main__ -   Batch number = 75
Evaluating:  61%|██████▏   | 67/109 [00:34<00:20,  2.10it/s]Evaluating:  89%|████████▉ | 75/84 [00:27<00:04,  2.22it/s]11/28/2021 01:26:54 - INFO - __main__ -   Batch number = 76
11/28/2021 01:26:54 - INFO - __main__ -   Batch number = 68
Evaluating:  62%|██████▏   | 68/109 [00:34<00:20,  1.96it/s]11/28/2021 01:26:55 - INFO - __main__ -   Batch number = 69
Evaluating:  90%|█████████ | 76/84 [00:27<00:03,  2.17it/s]11/28/2021 01:26:55 - INFO - __main__ -   Batch number = 77
Evaluating:  63%|██████▎   | 69/109 [00:35<00:19,  2.01it/s]11/28/2021 01:26:55 - INFO - __main__ -   Batch number = 70
Evaluating:  92%|█████████▏| 77/84 [00:28<00:03,  2.17it/s]11/28/2021 01:26:55 - INFO - __main__ -   Batch number = 78
Evaluating:  64%|██████▍   | 70/109 [00:35<00:18,  2.06it/s]11/28/2021 01:26:56 - INFO - __main__ -   Batch number = 71
Evaluating:  93%|█████████▎| 78/84 [00:28<00:02,  2.17it/s]11/28/2021 01:26:56 - INFO - __main__ -   Batch number = 79
Evaluating:  94%|█████████▍| 79/84 [00:29<00:02,  2.19it/s]11/28/2021 01:26:56 - INFO - __main__ -   Batch number = 80
Evaluating:  65%|██████▌   | 71/109 [00:36<00:18,  2.08it/s]11/28/2021 01:26:56 - INFO - __main__ -   Batch number = 72
Evaluating:  95%|█████████▌| 80/84 [00:29<00:01,  2.19it/s]11/28/2021 01:26:57 - INFO - __main__ -   Batch number = 81
Evaluating:  66%|██████▌   | 72/109 [00:36<00:17,  2.10it/s]11/28/2021 01:26:57 - INFO - __main__ -   Batch number = 73
Evaluating:  96%|█████████▋| 81/84 [00:30<00:01,  2.18it/s]11/28/2021 01:26:57 - INFO - __main__ -   Batch number = 82
Evaluating:  67%|██████▋   | 73/109 [00:37<00:17,  2.11it/s]11/28/2021 01:26:57 - INFO - __main__ -   Batch number = 74
Evaluating:  98%|█████████▊| 82/84 [00:30<00:00,  2.17it/s]11/28/2021 01:26:58 - INFO - __main__ -   Batch number = 83
Evaluating:  68%|██████▊   | 74/109 [00:37<00:16,  2.12it/s]11/28/2021 01:26:58 - INFO - __main__ -   Batch number = 75
Evaluating:  99%|█████████▉| 83/84 [00:31<00:00,  2.16it/s]11/28/2021 01:26:58 - INFO - __main__ -   Batch number = 84
Evaluating:  69%|██████▉   | 75/109 [00:38<00:16,  2.12it/s]11/28/2021 01:26:58 - INFO - __main__ -   Batch number = 76
Evaluating: 100%|██████████| 84/84 [00:31<00:00,  2.26it/s]Evaluating: 100%|██████████| 84/84 [00:31<00:00,  2.67it/s]Evaluating:  70%|██████▉   | 76/109 [00:38<00:15,  2.14it/s]11/28/2021 01:26:59 - INFO - __main__ -   Batch number = 77
Evaluating:  71%|███████   | 77/109 [00:39<00:14,  2.15it/s]11/28/2021 01:26:59 - INFO - __main__ -   Batch number = 78
Evaluating:  72%|███████▏  | 78/109 [00:39<00:13,  2.33it/s]11/28/2021 01:26:59 - INFO - __main__ -   Batch number = 79
Evaluating:  72%|███████▏  | 79/109 [00:39<00:13,  2.24it/s]11/28/2021 01:27:00 - INFO - __main__ -   Batch number = 80

/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:27:00 - INFO - __main__ -   ***** Evaluation result  in pt *****
11/28/2021 01:27:00 - INFO - __main__ -     f1 = 0.8685652556474165
11/28/2021 01:27:00 - INFO - __main__ -     loss = 0.454947350635415
11/28/2021 01:27:00 - INFO - __main__ -     precision = 0.8758521329678824
11/28/2021 01:27:00 - INFO - __main__ -     recall = 0.8613986279829275
Evaluating:  73%|███████▎  | 80/109 [00:40<00:13,  2.22it/s]11/28/2021 01:27:00 - INFO - __main__ -   Batch number = 81
Evaluating:  74%|███████▍  | 81/109 [00:40<00:12,  2.21it/s]11/28/2021 01:27:01 - INFO - __main__ -   Batch number = 82
45.32user 16.42system 0:59.71elapsed 103%CPU (0avgtext+0avgdata 3937312maxresident)k
0inputs+656outputs (0major+1668854minor)pagefaults 0swaps
Evaluating:  75%|███████▌  | 82/109 [00:41<00:12,  2.20it/s]11/28/2021 01:27:01 - INFO - __main__ -   Batch number = 83
Evaluating:  76%|███████▌  | 83/109 [00:41<00:11,  2.19it/s]11/28/2021 01:27:02 - INFO - __main__ -   Batch number = 84
Evaluating:  77%|███████▋  | 84/109 [00:42<00:11,  2.19it/s]11/28/2021 01:27:02 - INFO - __main__ -   Batch number = 85
11/28/2021 01:27:02 - INFO - __main__ -   Language adapter for fi not found, using am instead
11/28/2021 01:27:02 - INFO - __main__ -   Set active language adapter to am
11/28/2021 01:27:02 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:27:02 - INFO - __main__ -   Adapter Languages = ['am']
11/28/2021 01:27:02 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_fi_bert-base-multilingual-cased_128
PyTorch version 1.10.0+cu102 available.
Evaluating:  78%|███████▊  | 85/109 [00:42<00:11,  2.18it/s]11/28/2021 01:27:03 - INFO - __main__ -   Batch number = 86
11/28/2021 01:27:03 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='pt', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:27:03 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:27:03 - INFO - __main__ -   Seed = 3
11/28/2021 01:27:03 - INFO - root -   save model
11/28/2021 01:27:03 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='pt', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:27:03 - INFO - __main__ -   Loading pretrained model and tokenizer
Evaluating:  79%|███████▉  | 86/109 [00:43<00:10,  2.17it/s]11/28/2021 01:27:03 - INFO - __main__ -   Batch number = 87
Evaluating:  80%|███████▉  | 87/109 [00:43<00:10,  2.16it/s]11/28/2021 01:27:04 - INFO - __main__ -   Batch number = 88
11/28/2021 01:27:04 - INFO - __main__ -   ***** Running evaluation  in fi *****
11/28/2021 01:27:04 - INFO - __main__ -     Num examples = 6550
11/28/2021 01:27:04 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/205 [00:00<?, ?it/s]11/28/2021 01:27:04 - INFO - __main__ -   Batch number = 1
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:   0%|          | 1/205 [00:00<01:24,  2.42it/s]11/28/2021 01:27:04 - INFO - __main__ -   Batch number = 2
Evaluating:  81%|████████  | 88/109 [00:43<00:09,  2.16it/s]11/28/2021 01:27:04 - INFO - __main__ -   Batch number = 89
Evaluating:   1%|          | 2/205 [00:00<01:11,  2.85it/s]11/28/2021 01:27:04 - INFO - __main__ -   Batch number = 3
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  82%|████████▏ | 89/109 [00:44<00:09,  2.16it/s]11/28/2021 01:27:04 - INFO - __main__ -   Batch number = 90
Evaluating:   1%|▏         | 3/205 [00:01<01:07,  2.98it/s]11/28/2021 01:27:05 - INFO - __main__ -   Batch number = 4
Evaluating:   2%|▏         | 4/205 [00:01<01:06,  3.04it/s]11/28/2021 01:27:05 - INFO - __main__ -   Batch number = 5
Evaluating:  83%|████████▎ | 90/109 [00:44<00:08,  2.16it/s]11/28/2021 01:27:05 - INFO - __main__ -   Batch number = 91
Evaluating:   2%|▏         | 5/205 [00:01<01:04,  3.10it/s]11/28/2021 01:27:05 - INFO - __main__ -   Batch number = 6
loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
Evaluating:  83%|████████▎ | 91/109 [00:45<00:08,  2.14it/s]11/28/2021 01:27:05 - INFO - __main__ -   Batch number = 92
11/28/2021 01:27:05 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
Evaluating:   3%|▎         | 6/205 [00:01<01:03,  3.15it/s]11/28/2021 01:27:06 - INFO - __main__ -   Batch number = 7
Evaluating:   3%|▎         | 7/205 [00:02<01:02,  3.19it/s]11/28/2021 01:27:06 - INFO - __main__ -   Batch number = 8
Evaluating:  84%|████████▍ | 92/109 [00:45<00:07,  2.14it/s]11/28/2021 01:27:06 - INFO - __main__ -   Batch number = 93
Evaluating:   4%|▍         | 8/205 [00:02<01:01,  3.20it/s]11/28/2021 01:27:06 - INFO - __main__ -   Batch number = 9
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  85%|████████▌ | 93/109 [00:46<00:07,  2.15it/s]11/28/2021 01:27:06 - INFO - __main__ -   Batch number = 94
Evaluating:   4%|▍         | 9/205 [00:02<01:01,  3.20it/s]11/28/2021 01:27:06 - INFO - __main__ -   Batch number = 10
Evaluating:   5%|▍         | 10/205 [00:03<01:00,  3.22it/s]11/28/2021 01:27:07 - INFO - __main__ -   Batch number = 11
Evaluating:  86%|████████▌ | 94/109 [00:46<00:07,  2.12it/s]11/28/2021 01:27:07 - INFO - __main__ -   Batch number = 95
Evaluating:   5%|▌         | 11/205 [00:03<01:00,  3.23it/s]11/28/2021 01:27:07 - INFO - __main__ -   Batch number = 12
Evaluating:  87%|████████▋ | 95/109 [00:47<00:06,  2.13it/s]11/28/2021 01:27:07 - INFO - __main__ -   Batch number = 96
Evaluating:   6%|▌         | 12/205 [00:03<00:59,  3.24it/s]11/28/2021 01:27:07 - INFO - __main__ -   Batch number = 13
Evaluating:   6%|▋         | 13/205 [00:04<00:58,  3.27it/s]11/28/2021 01:27:08 - INFO - __main__ -   Batch number = 14
Evaluating:  88%|████████▊ | 96/109 [00:47<00:06,  2.12it/s]11/28/2021 01:27:08 - INFO - __main__ -   Batch number = 97
Evaluating:   7%|▋         | 14/205 [00:04<00:58,  3.27it/s]11/28/2021 01:27:08 - INFO - __main__ -   Batch number = 15
Evaluating:   7%|▋         | 15/205 [00:04<00:51,  3.67it/s]11/28/2021 01:27:08 - INFO - __main__ -   Batch number = 16
Evaluating:  89%|████████▉ | 97/109 [00:48<00:05,  2.13it/s]11/28/2021 01:27:08 - INFO - __main__ -   Batch number = 98
Evaluating:   8%|▊         | 16/205 [00:04<00:44,  4.26it/s]11/28/2021 01:27:08 - INFO - __main__ -   Batch number = 17
Evaluating:   8%|▊         | 17/205 [00:04<00:39,  4.79it/s]11/28/2021 01:27:08 - INFO - __main__ -   Batch number = 18
Evaluating:   9%|▉         | 18/205 [00:05<00:35,  5.26it/s]11/28/2021 01:27:09 - INFO - __main__ -   Batch number = 19
Evaluating:  90%|████████▉ | 98/109 [00:48<00:05,  2.14it/s]11/28/2021 01:27:09 - INFO - __main__ -   Batch number = 99
Evaluating:   9%|▉         | 19/205 [00:05<00:32,  5.65it/s]11/28/2021 01:27:09 - INFO - __main__ -   Batch number = 20
Evaluating:  10%|▉         | 20/205 [00:05<00:31,  5.97it/s]11/28/2021 01:27:09 - INFO - __main__ -   Batch number = 21
Evaluating:  10%|█         | 21/205 [00:05<00:29,  6.20it/s]11/28/2021 01:27:09 - INFO - __main__ -   Batch number = 22
Evaluating:  91%|█████████ | 99/109 [00:49<00:04,  2.15it/s]Evaluating:  11%|█         | 22/205 [00:05<00:28,  6.38it/s]11/28/2021 01:27:09 - INFO - __main__ -   Batch number = 23
11/28/2021 01:27:09 - INFO - __main__ -   Batch number = 100
Evaluating:  11%|█         | 23/205 [00:05<00:28,  6.45it/s]11/28/2021 01:27:09 - INFO - __main__ -   Batch number = 24
Evaluating:  12%|█▏        | 24/205 [00:05<00:27,  6.55it/s]11/28/2021 01:27:09 - INFO - __main__ -   Batch number = 25
Evaluating:  12%|█▏        | 25/205 [00:06<00:27,  6.64it/s]11/28/2021 01:27:10 - INFO - __main__ -   Batch number = 26
Evaluating:  92%|█████████▏| 100/109 [00:49<00:04,  1.94it/s]11/28/2021 01:27:10 - INFO - __main__ -   Batch number = 101
Evaluating:  13%|█▎        | 26/205 [00:06<00:26,  6.68it/s]11/28/2021 01:27:10 - INFO - __main__ -   Batch number = 27
Evaluating:  13%|█▎        | 27/205 [00:06<00:26,  6.70it/s]11/28/2021 01:27:10 - INFO - __main__ -   Batch number = 28
Evaluating:  14%|█▎        | 28/205 [00:06<00:26,  6.72it/s]11/28/2021 01:27:10 - INFO - __main__ -   Batch number = 29
Evaluating:  14%|█▍        | 29/205 [00:06<00:26,  6.76it/s]11/28/2021 01:27:10 - INFO - __main__ -   Batch number = 30
Evaluating:  15%|█▍        | 30/205 [00:06<00:25,  6.77it/s]11/28/2021 01:27:10 - INFO - __main__ -   Batch number = 31
Evaluating:  93%|█████████▎| 101/109 [00:50<00:04,  1.83it/s]11/28/2021 01:27:10 - INFO - __main__ -   Batch number = 102
Evaluating:  15%|█▌        | 31/205 [00:06<00:25,  6.79it/s]11/28/2021 01:27:11 - INFO - __main__ -   Batch number = 32
Evaluating:  16%|█▌        | 32/205 [00:07<00:25,  6.80it/s]11/28/2021 01:27:11 - INFO - __main__ -   Batch number = 33
Evaluating:  16%|█▌        | 33/205 [00:07<00:25,  6.80it/s]11/28/2021 01:27:11 - INFO - __main__ -   Batch number = 34
Evaluating:  17%|█▋        | 34/205 [00:07<00:25,  6.80it/s]11/28/2021 01:27:11 - INFO - __main__ -   Batch number = 35
Evaluating:  94%|█████████▎| 102/109 [00:51<00:03,  1.75it/s]11/28/2021 01:27:11 - INFO - __main__ -   Batch number = 103
Evaluating:  17%|█▋        | 35/205 [00:07<00:25,  6.70it/s]11/28/2021 01:27:11 - INFO - __main__ -   Batch number = 36
Evaluating:  18%|█▊        | 36/205 [00:07<00:25,  6.68it/s]11/28/2021 01:27:11 - INFO - __main__ -   Batch number = 37
Evaluating:  18%|█▊        | 37/205 [00:07<00:25,  6.72it/s]11/28/2021 01:27:11 - INFO - __main__ -   Batch number = 38
Evaluating:  19%|█▊        | 38/205 [00:08<00:24,  6.76it/s]11/28/2021 01:27:12 - INFO - __main__ -   Batch number = 39
Evaluating:  94%|█████████▍| 103/109 [00:51<00:03,  1.71it/s]11/28/2021 01:27:12 - INFO - __main__ -   Batch number = 104
Evaluating:  19%|█▉        | 39/205 [00:08<00:24,  6.77it/s]11/28/2021 01:27:12 - INFO - __main__ -   Batch number = 40
Evaluating:  20%|█▉        | 40/205 [00:08<00:24,  6.76it/s]11/28/2021 01:27:12 - INFO - __main__ -   Batch number = 41
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:27:12 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:27:12 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:27:12 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
11/28/2021 01:27:12 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:27:12 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:27:12 - INFO - root -   loading lang adpater am/wiki@ukp
11/28/2021 01:27:12 - INFO - __main__ -   Adapter Languages : ['am'], Length : 1
11/28/2021 01:27:12 - INFO - __main__ -   Adapter Names ['am/wiki@ukp'], Length : 1
11/28/2021 01:27:12 - INFO - __main__ -   Language = am
11/28/2021 01:27:12 - INFO - __main__ -   Adapter Name = am/wiki@ukp
Found matching adapter at: adapters/ukp/bert-base-multilingual-cased_am_wiki_pfeiffer.json
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/am/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_am_wiki_pfeiffer.zip.
Evaluating:  20%|██        | 41/205 [00:08<00:24,  6.77it/s]11/28/2021 01:27:12 - INFO - __main__ -   Batch number = 42
Evaluating:  20%|██        | 42/205 [00:08<00:24,  6.76it/s]11/28/2021 01:27:12 - INFO - __main__ -   Batch number = 43
Evaluating:  95%|█████████▌| 104/109 [00:52<00:02,  1.68it/s]11/28/2021 01:27:12 - INFO - __main__ -   Batch number = 105
Evaluating:  21%|██        | 43/205 [00:08<00:23,  6.78it/s]11/28/2021 01:27:12 - INFO - __main__ -   Batch number = 44
Evaluating:  21%|██▏       | 44/205 [00:08<00:23,  6.78it/s]11/28/2021 01:27:12 - INFO - __main__ -   Batch number = 45
Evaluating:  22%|██▏       | 45/205 [00:09<00:23,  6.78it/s]11/28/2021 01:27:13 - INFO - __main__ -   Batch number = 46
Evaluating:  22%|██▏       | 46/205 [00:09<00:23,  6.79it/s]11/28/2021 01:27:13 - INFO - __main__ -   Batch number = 47
Evaluating:  96%|█████████▋| 105/109 [00:52<00:02,  1.66it/s]11/28/2021 01:27:13 - INFO - __main__ -   Batch number = 106
Evaluating:  23%|██▎       | 47/205 [00:09<00:23,  6.79it/s]11/28/2021 01:27:13 - INFO - __main__ -   Batch number = 48
Evaluating:  23%|██▎       | 48/205 [00:09<00:23,  6.77it/s]11/28/2021 01:27:13 - INFO - __main__ -   Batch number = 49
Evaluating:  24%|██▍       | 49/205 [00:09<00:23,  6.76it/s]11/28/2021 01:27:13 - INFO - __main__ -   Batch number = 50
Evaluating:  24%|██▍       | 50/205 [00:09<00:22,  6.78it/s]11/28/2021 01:27:13 - INFO - __main__ -   Batch number = 51
Evaluating:  25%|██▍       | 51/205 [00:09<00:22,  6.79it/s]11/28/2021 01:27:13 - INFO - __main__ -   Batch number = 52
Evaluating:  97%|█████████▋| 106/109 [00:53<00:01,  1.65it/s]11/28/2021 01:27:13 - INFO - __main__ -   Batch number = 107
Evaluating:  25%|██▌       | 52/205 [00:10<00:22,  6.82it/s]11/28/2021 01:27:14 - INFO - __main__ -   Batch number = 53
Evaluating:  26%|██▌       | 53/205 [00:10<00:22,  6.85it/s]11/28/2021 01:27:14 - INFO - __main__ -   Batch number = 54
Evaluating:  26%|██▋       | 54/205 [00:10<00:22,  6.86it/s]11/28/2021 01:27:14 - INFO - __main__ -   Batch number = 55
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/adapter_config.json
Adding adapter 'am' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/head_config.json
Evaluating:  27%|██▋       | 55/205 [00:10<00:21,  6.85it/s]11/28/2021 01:27:14 - INFO - __main__ -   Batch number = 56
Evaluating:  98%|█████████▊| 107/109 [00:54<00:01,  1.63it/s]11/28/2021 01:27:14 - INFO - __main__ -   Batch number = 108
Evaluating:  27%|██▋       | 56/205 [00:10<00:21,  6.82it/s]11/28/2021 01:27:14 - INFO - __main__ -   Batch number = 57
Evaluating:  28%|██▊       | 57/205 [00:10<00:22,  6.62it/s]11/28/2021 01:27:14 - INFO - __main__ -   Batch number = 58
Evaluating:  28%|██▊       | 58/205 [00:10<00:22,  6.67it/s]11/28/2021 01:27:15 - INFO - __main__ -   Batch number = 59
Evaluating:  29%|██▉       | 59/205 [00:11<00:21,  6.72it/s]11/28/2021 01:27:15 - INFO - __main__ -   Batch number = 60
Evaluating:  99%|█████████▉| 108/109 [00:54<00:00,  1.62it/s]11/28/2021 01:27:15 - INFO - __main__ -   Batch number = 109
Evaluating:  29%|██▉       | 60/205 [00:11<00:21,  6.69it/s]11/28/2021 01:27:15 - INFO - __main__ -   Batch number = 61
Evaluating: 100%|██████████| 109/109 [00:54<00:00,  2.10it/s]Evaluating: 100%|██████████| 109/109 [00:54<00:00,  1.99it/s]Evaluating:  30%|██▉       | 61/205 [00:11<00:21,  6.77it/s]11/28/2021 01:27:15 - INFO - __main__ -   Batch number = 62
Evaluating:  30%|███       | 62/205 [00:11<00:21,  6.71it/s]11/28/2021 01:27:15 - INFO - __main__ -   Batch number = 63
Evaluating:  31%|███       | 63/205 [00:11<00:21,  6.71it/s]11/28/2021 01:27:15 - INFO - __main__ -   Batch number = 64
Evaluating:  31%|███       | 64/205 [00:11<00:21,  6.67it/s]11/28/2021 01:27:15 - INFO - __main__ -   Batch number = 65
Evaluating:  32%|███▏      | 65/205 [00:12<00:20,  6.70it/s]11/28/2021 01:27:16 - INFO - __main__ -   Batch number = 66
Evaluating:  32%|███▏      | 66/205 [00:12<00:20,  6.65it/s]11/28/2021 01:27:16 - INFO - __main__ -   Batch number = 67
Evaluating:  33%|███▎      | 67/205 [00:12<00:21,  6.57it/s]11/28/2021 01:27:16 - INFO - __main__ -   Batch number = 68
Evaluating:  33%|███▎      | 68/205 [00:12<00:21,  6.50it/s]11/28/2021 01:27:16 - INFO - __main__ -   Batch number = 69
Evaluating:  34%|███▎      | 69/205 [00:12<00:21,  6.46it/s]11/28/2021 01:27:16 - INFO - __main__ -   Batch number = 70
Evaluating:  34%|███▍      | 70/205 [00:12<00:20,  6.45it/s]11/28/2021 01:27:16 - INFO - __main__ -   Batch number = 71

/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:27:16 - INFO - __main__ -   ***** Evaluation result  in zh *****
11/28/2021 01:27:16 - INFO - __main__ -     f1 = 0.6206086262410648
11/28/2021 01:27:16 - INFO - __main__ -     loss = 1.4001762221712586
11/28/2021 01:27:16 - INFO - __main__ -     precision = 0.6269457280057987
11/28/2021 01:27:16 - INFO - __main__ -     recall = 0.6143983520386419
Evaluating:  35%|███▍      | 71/205 [00:12<00:20,  6.52it/s]11/28/2021 01:27:16 - INFO - __main__ -   Batch number = 72
Evaluating:  35%|███▌      | 72/205 [00:13<00:20,  6.41it/s]11/28/2021 01:27:17 - INFO - __main__ -   Batch number = 73
Evaluating:  36%|███▌      | 73/205 [00:13<00:21,  6.25it/s]11/28/2021 01:27:17 - INFO - __main__ -   Batch number = 74
Evaluating:  36%|███▌      | 74/205 [00:13<00:21,  6.20it/s]11/28/2021 01:27:17 - INFO - __main__ -   Batch number = 75
Evaluating:  37%|███▋      | 75/205 [00:13<00:20,  6.32it/s]11/28/2021 01:27:17 - INFO - __main__ -   Batch number = 76
Evaluating:  37%|███▋      | 76/205 [00:13<00:20,  6.45it/s]11/28/2021 01:27:17 - INFO - __main__ -   Batch number = 77
Evaluating:  38%|███▊      | 77/205 [00:13<00:19,  6.66it/s]11/28/2021 01:27:17 - INFO - __main__ -   Batch number = 78
Evaluating:  38%|███▊      | 78/205 [00:14<00:18,  6.79it/s]11/28/2021 01:27:18 - INFO - __main__ -   Batch number = 79
62.19user 20.49system 1:23.98elapsed 98%CPU (0avgtext+0avgdata 3937712maxresident)k
1864inputs+12352outputs (0major+1713760minor)pagefaults 0swaps
Evaluating:  39%|███▊      | 79/205 [00:14<00:18,  6.92it/s]11/28/2021 01:27:18 - INFO - __main__ -   Batch number = 80
Evaluating:  39%|███▉      | 80/205 [00:14<00:18,  6.90it/s]11/28/2021 01:27:18 - INFO - __main__ -   Batch number = 81
Evaluating:  40%|███▉      | 81/205 [00:14<00:17,  6.97it/s]11/28/2021 01:27:18 - INFO - __main__ -   Batch number = 82
Evaluating:  40%|████      | 82/205 [00:14<00:17,  6.97it/s]11/28/2021 01:27:18 - INFO - __main__ -   Batch number = 83
Evaluating:  40%|████      | 83/205 [00:14<00:17,  6.93it/s]11/28/2021 01:27:18 - INFO - __main__ -   Batch number = 84
Evaluating:  41%|████      | 84/205 [00:14<00:17,  6.78it/s]11/28/2021 01:27:18 - INFO - __main__ -   Batch number = 85
Evaluating:  41%|████▏     | 85/205 [00:15<00:17,  6.77it/s]11/28/2021 01:27:19 - INFO - __main__ -   Batch number = 86
Evaluating:  42%|████▏     | 86/205 [00:15<00:17,  6.80it/s]11/28/2021 01:27:19 - INFO - __main__ -   Batch number = 87
Evaluating:  42%|████▏     | 87/205 [00:15<00:18,  6.50it/s]11/28/2021 01:27:19 - INFO - __main__ -   Batch number = 88
Evaluating:  43%|████▎     | 88/205 [00:15<00:17,  6.65it/s]11/28/2021 01:27:19 - INFO - __main__ -   Batch number = 89
Evaluating:  43%|████▎     | 89/205 [00:15<00:17,  6.63it/s]11/28/2021 01:27:19 - INFO - __main__ -   Batch number = 90
Evaluating:  44%|████▍     | 90/205 [00:15<00:18,  6.17it/s]11/28/2021 01:27:19 - INFO - __main__ -   Batch number = 91
PyTorch version 1.10.0+cu102 available.
Evaluating:  44%|████▍     | 91/205 [00:15<00:17,  6.38it/s]11/28/2021 01:27:20 - INFO - __main__ -   Batch number = 92
Evaluating:  45%|████▍     | 92/205 [00:16<00:17,  6.52it/s]11/28/2021 01:27:20 - INFO - __main__ -   Batch number = 93
Evaluating:  45%|████▌     | 93/205 [00:16<00:17,  6.57it/s]11/28/2021 01:27:20 - INFO - __main__ -   Batch number = 94
Evaluating:  46%|████▌     | 94/205 [00:16<00:16,  6.56it/s]11/28/2021 01:27:20 - INFO - __main__ -   Batch number = 95
11/28/2021 01:27:20 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='zh', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:27:20 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:27:20 - INFO - __main__ -   Seed = 2
11/28/2021 01:27:20 - INFO - root -   save model
11/28/2021 01:27:20 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='zh', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:27:20 - INFO - __main__ -   Loading pretrained model and tokenizer
Evaluating:  46%|████▋     | 95/205 [00:16<00:16,  6.57it/s]11/28/2021 01:27:20 - INFO - __main__ -   Batch number = 96
Evaluating:  47%|████▋     | 96/205 [00:16<00:16,  6.65it/s]11/28/2021 01:27:20 - INFO - __main__ -   Batch number = 97
Evaluating:  47%|████▋     | 97/205 [00:16<00:16,  6.55it/s]11/28/2021 01:27:20 - INFO - __main__ -   Batch number = 98
Evaluating:  48%|████▊     | 98/205 [00:17<00:16,  6.50it/s]11/28/2021 01:27:21 - INFO - __main__ -   Batch number = 99
Evaluating:  48%|████▊     | 99/205 [00:17<00:16,  6.48it/s]11/28/2021 01:27:21 - INFO - __main__ -   Batch number = 100
Evaluating:  49%|████▉     | 100/205 [00:17<00:16,  6.45it/s]11/28/2021 01:27:21 - INFO - __main__ -   Batch number = 101
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  49%|████▉     | 101/205 [00:17<00:16,  6.40it/s]11/28/2021 01:27:21 - INFO - __main__ -   Batch number = 102
Evaluating:  50%|████▉     | 102/205 [00:17<00:16,  6.30it/s]11/28/2021 01:27:21 - INFO - __main__ -   Batch number = 103
Evaluating:  50%|█████     | 103/205 [00:17<00:16,  6.31it/s]11/28/2021 01:27:21 - INFO - __main__ -   Batch number = 104
Evaluating:  51%|█████     | 104/205 [00:17<00:15,  6.42it/s]11/28/2021 01:27:22 - INFO - __main__ -   Batch number = 105
Evaluating:  51%|█████     | 105/205 [00:18<00:15,  6.38it/s]11/28/2021 01:27:22 - INFO - __main__ -   Batch number = 106
Evaluating:  52%|█████▏    | 106/205 [00:18<00:15,  6.38it/s]11/28/2021 01:27:22 - INFO - __main__ -   Batch number = 107
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  52%|█████▏    | 107/205 [00:18<00:15,  6.43it/s]11/28/2021 01:27:22 - INFO - __main__ -   Batch number = 108
Evaluating:  53%|█████▎    | 108/205 [00:18<00:14,  6.62it/s]11/28/2021 01:27:22 - INFO - __main__ -   Batch number = 109
Evaluating:  53%|█████▎    | 109/205 [00:18<00:14,  6.60it/s]11/28/2021 01:27:22 - INFO - __main__ -   Batch number = 110
Evaluating:  54%|█████▎    | 110/205 [00:18<00:14,  6.42it/s]11/28/2021 01:27:22 - INFO - __main__ -   Batch number = 111
Evaluating:  54%|█████▍    | 111/205 [00:19<00:14,  6.44it/s]11/28/2021 01:27:23 - INFO - __main__ -   Batch number = 112
loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
Evaluating:  55%|█████▍    | 112/205 [00:19<00:14,  6.48it/s]11/28/2021 01:27:23 - INFO - __main__ -   Batch number = 113
Evaluating:  55%|█████▌    | 113/205 [00:19<00:14,  6.54it/s]11/28/2021 01:27:23 - INFO - __main__ -   Batch number = 114
11/28/2021 01:27:23 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
Evaluating:  56%|█████▌    | 114/205 [00:19<00:13,  6.61it/s]11/28/2021 01:27:23 - INFO - __main__ -   Batch number = 115
Evaluating:  56%|█████▌    | 115/205 [00:19<00:13,  6.54it/s]11/28/2021 01:27:23 - INFO - __main__ -   Batch number = 116
Evaluating:  57%|█████▋    | 116/205 [00:19<00:13,  6.56it/s]11/28/2021 01:27:23 - INFO - __main__ -   Batch number = 117
Evaluating:  57%|█████▋    | 117/205 [00:19<00:14,  6.23it/s]11/28/2021 01:27:24 - INFO - __main__ -   Batch number = 118
11/28/2021 01:27:24 - INFO - __main__ -   Language adapter for pt not found, using am instead
11/28/2021 01:27:24 - INFO - __main__ -   Set active language adapter to am
11/28/2021 01:27:24 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:27:24 - INFO - __main__ -   Adapter Languages = ['am']
11/28/2021 01:27:24 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_pt_bert-base-multilingual-cased_128
Evaluating:  58%|█████▊    | 118/205 [00:20<00:13,  6.28it/s]11/28/2021 01:27:24 - INFO - __main__ -   Batch number = 119
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  58%|█████▊    | 119/205 [00:20<00:13,  6.34it/s]11/28/2021 01:27:24 - INFO - __main__ -   Batch number = 120
Evaluating:  59%|█████▊    | 120/205 [00:20<00:13,  6.43it/s]11/28/2021 01:27:24 - INFO - __main__ -   Batch number = 121
Evaluating:  59%|█████▉    | 121/205 [00:20<00:13,  6.46it/s]11/28/2021 01:27:24 - INFO - __main__ -   Batch number = 122
11/28/2021 01:27:24 - INFO - __main__ -   ***** Running evaluation  in pt *****
11/28/2021 01:27:24 - INFO - __main__ -     Num examples = 2682
11/28/2021 01:27:24 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/84 [00:00<?, ?it/s]11/28/2021 01:27:24 - INFO - __main__ -   Batch number = 1
Evaluating:  60%|█████▉    | 122/205 [00:20<00:17,  4.87it/s]11/28/2021 01:27:24 - INFO - __main__ -   Batch number = 123
Evaluating:   1%|          | 1/84 [00:00<00:27,  3.02it/s]11/28/2021 01:27:25 - INFO - __main__ -   Batch number = 2
Evaluating:  60%|██████    | 123/205 [00:21<00:19,  4.12it/s]11/28/2021 01:27:25 - INFO - __main__ -   Batch number = 124
Evaluating:   2%|▏         | 2/84 [00:00<00:26,  3.12it/s]11/28/2021 01:27:25 - INFO - __main__ -   Batch number = 3
Evaluating:  60%|██████    | 124/205 [00:21<00:21,  3.72it/s]11/28/2021 01:27:25 - INFO - __main__ -   Batch number = 125
Evaluating:   4%|▎         | 3/84 [00:00<00:25,  3.16it/s]11/28/2021 01:27:25 - INFO - __main__ -   Batch number = 4
Evaluating:  61%|██████    | 125/205 [00:21<00:23,  3.47it/s]11/28/2021 01:27:25 - INFO - __main__ -   Batch number = 126
Evaluating:   5%|▍         | 4/84 [00:01<00:25,  3.18it/s]11/28/2021 01:27:25 - INFO - __main__ -   Batch number = 5
Evaluating:  61%|██████▏   | 126/205 [00:22<00:23,  3.34it/s]11/28/2021 01:27:26 - INFO - __main__ -   Batch number = 127
Evaluating:   6%|▌         | 5/84 [00:01<00:24,  3.19it/s]11/28/2021 01:27:26 - INFO - __main__ -   Batch number = 6
Evaluating:  62%|██████▏   | 127/205 [00:22<00:23,  3.26it/s]11/28/2021 01:27:26 - INFO - __main__ -   Batch number = 128
Evaluating:   7%|▋         | 6/84 [00:01<00:24,  3.21it/s]11/28/2021 01:27:26 - INFO - __main__ -   Batch number = 7
Evaluating:  62%|██████▏   | 128/205 [00:22<00:23,  3.24it/s]11/28/2021 01:27:26 - INFO - __main__ -   Batch number = 129
Evaluating:   8%|▊         | 7/84 [00:02<00:24,  3.21it/s]11/28/2021 01:27:26 - INFO - __main__ -   Batch number = 8
Evaluating:  63%|██████▎   | 129/205 [00:23<00:23,  3.20it/s]Evaluating:  10%|▉         | 8/84 [00:02<00:23,  3.20it/s]11/28/2021 01:27:27 - INFO - __main__ -   Batch number = 130
11/28/2021 01:27:27 - INFO - __main__ -   Batch number = 9
Evaluating:  11%|█         | 9/84 [00:02<00:23,  3.17it/s]11/28/2021 01:27:27 - INFO - __main__ -   Batch number = 10
Evaluating:  63%|██████▎   | 130/205 [00:23<00:23,  3.16it/s]11/28/2021 01:27:27 - INFO - __main__ -   Batch number = 131
Evaluating:  12%|█▏        | 10/84 [00:03<00:23,  3.19it/s]11/28/2021 01:27:27 - INFO - __main__ -   Batch number = 11
Evaluating:  64%|██████▍   | 131/205 [00:23<00:23,  3.13it/s]11/28/2021 01:27:27 - INFO - __main__ -   Batch number = 132
Evaluating:  13%|█▎        | 11/84 [00:03<00:22,  3.20it/s]11/28/2021 01:27:28 - INFO - __main__ -   Batch number = 12
Evaluating:  64%|██████▍   | 132/205 [00:24<00:23,  3.12it/s]11/28/2021 01:27:28 - INFO - __main__ -   Batch number = 133
Evaluating:  14%|█▍        | 12/84 [00:03<00:22,  3.22it/s]11/28/2021 01:27:28 - INFO - __main__ -   Batch number = 13
Evaluating:  65%|██████▍   | 133/205 [00:24<00:23,  3.11it/s]11/28/2021 01:27:28 - INFO - __main__ -   Batch number = 134
Evaluating:  15%|█▌        | 13/84 [00:04<00:22,  3.19it/s]11/28/2021 01:27:28 - INFO - __main__ -   Batch number = 14
Evaluating:  65%|██████▌   | 134/205 [00:24<00:22,  3.12it/s]11/28/2021 01:27:28 - INFO - __main__ -   Batch number = 135
Evaluating:  17%|█▋        | 14/84 [00:04<00:21,  3.19it/s]11/28/2021 01:27:29 - INFO - __main__ -   Batch number = 15
Evaluating:  66%|██████▌   | 135/205 [00:25<00:22,  3.12it/s]11/28/2021 01:27:29 - INFO - __main__ -   Batch number = 136
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:27:29 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:27:29 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:27:29 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
11/28/2021 01:27:29 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:27:29 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_model_head.bin
Evaluating:  18%|█▊        | 15/84 [00:04<00:21,  3.19it/s]11/28/2021 01:27:29 - INFO - root -   loading lang adpater am/wiki@ukp
11/28/2021 01:27:29 - INFO - __main__ -   Batch number = 16
11/28/2021 01:27:29 - INFO - __main__ -   Adapter Languages : ['am'], Length : 1
11/28/2021 01:27:29 - INFO - __main__ -   Adapter Names ['am/wiki@ukp'], Length : 1
11/28/2021 01:27:29 - INFO - __main__ -   Language = am
11/28/2021 01:27:29 - INFO - __main__ -   Adapter Name = am/wiki@ukp
Found matching adapter at: adapters/ukp/bert-base-multilingual-cased_am_wiki_pfeiffer.json
Evaluating:  66%|██████▋   | 136/205 [00:25<00:22,  3.12it/s]11/28/2021 01:27:29 - INFO - __main__ -   Batch number = 137
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/am/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_am_wiki_pfeiffer.zip.
Evaluating:  19%|█▉        | 16/84 [00:05<00:21,  3.21it/s]11/28/2021 01:27:29 - INFO - __main__ -   Batch number = 17
Evaluating:  67%|██████▋   | 137/205 [00:25<00:21,  3.12it/s]11/28/2021 01:27:29 - INFO - __main__ -   Batch number = 138
Evaluating:  20%|██        | 17/84 [00:05<00:20,  3.23it/s]11/28/2021 01:27:30 - INFO - __main__ -   Batch number = 18
Evaluating:  67%|██████▋   | 138/205 [00:26<00:21,  3.14it/s]11/28/2021 01:27:30 - INFO - __main__ -   Batch number = 139
Evaluating:  21%|██▏       | 18/84 [00:05<00:20,  3.24it/s]11/28/2021 01:27:30 - INFO - __main__ -   Batch number = 19
Evaluating:  68%|██████▊   | 139/205 [00:26<00:21,  3.12it/s]11/28/2021 01:27:30 - INFO - __main__ -   Batch number = 140
Evaluating:  23%|██▎       | 19/84 [00:05<00:20,  3.12it/s]11/28/2021 01:27:30 - INFO - __main__ -   Batch number = 20
Evaluating:  68%|██████▊   | 140/205 [00:26<00:24,  2.68it/s]11/28/2021 01:27:30 - INFO - __main__ -   Batch number = 141
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/adapter_config.json
Adding adapter 'am' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/head_config.json
Evaluating:  24%|██▍       | 20/84 [00:06<00:23,  2.78it/s]11/28/2021 01:27:31 - INFO - __main__ -   Batch number = 21
Evaluating:  69%|██████▉   | 141/205 [00:27<00:25,  2.48it/s]11/28/2021 01:27:31 - INFO - __main__ -   Batch number = 142
Evaluating:  25%|██▌       | 21/84 [00:06<00:24,  2.56it/s]11/28/2021 01:27:31 - INFO - __main__ -   Batch number = 22
Evaluating:  69%|██████▉   | 142/205 [00:27<00:26,  2.35it/s]11/28/2021 01:27:31 - INFO - __main__ -   Batch number = 143
Evaluating:  26%|██▌       | 22/84 [00:07<00:25,  2.43it/s]11/28/2021 01:27:32 - INFO - __main__ -   Batch number = 23
Evaluating:  70%|██████▉   | 143/205 [00:28<00:27,  2.27it/s]11/28/2021 01:27:32 - INFO - __main__ -   Batch number = 144
Evaluating:  27%|██▋       | 23/84 [00:07<00:26,  2.34it/s]11/28/2021 01:27:32 - INFO - __main__ -   Batch number = 24
Evaluating:  70%|███████   | 144/205 [00:28<00:28,  2.15it/s]11/28/2021 01:27:32 - INFO - __main__ -   Batch number = 145
Evaluating:  29%|██▊       | 24/84 [00:08<00:27,  2.15it/s]11/28/2021 01:27:33 - INFO - __main__ -   Batch number = 25
Evaluating:  71%|███████   | 145/205 [00:29<00:30,  1.95it/s]11/28/2021 01:27:33 - INFO - __main__ -   Batch number = 146
Evaluating:  30%|██▉       | 25/84 [00:08<00:30,  1.96it/s]11/28/2021 01:27:33 - INFO - __main__ -   Batch number = 26
Evaluating:  71%|███████   | 146/205 [00:30<00:32,  1.82it/s]11/28/2021 01:27:34 - INFO - __main__ -   Batch number = 147
Evaluating:  31%|███       | 26/84 [00:09<00:31,  1.84it/s]11/28/2021 01:27:34 - INFO - __main__ -   Batch number = 27
Evaluating:  72%|███████▏  | 147/205 [00:30<00:33,  1.76it/s]11/28/2021 01:27:34 - INFO - __main__ -   Batch number = 148
Evaluating:  32%|███▏      | 27/84 [00:10<00:31,  1.78it/s]11/28/2021 01:27:34 - INFO - __main__ -   Batch number = 28
Evaluating:  72%|███████▏  | 148/205 [00:31<00:33,  1.69it/s]11/28/2021 01:27:35 - INFO - __main__ -   Batch number = 149
Evaluating:  33%|███▎      | 28/84 [00:10<00:32,  1.74it/s]11/28/2021 01:27:35 - INFO - __main__ -   Batch number = 29
Evaluating:  73%|███████▎  | 149/205 [00:32<00:34,  1.64it/s]11/28/2021 01:27:36 - INFO - __main__ -   Batch number = 150
Evaluating:  35%|███▍      | 29/84 [00:11<00:32,  1.70it/s]11/28/2021 01:27:36 - INFO - __main__ -   Batch number = 30
Evaluating:  73%|███████▎  | 150/205 [00:32<00:33,  1.62it/s]11/28/2021 01:27:36 - INFO - __main__ -   Batch number = 151
Evaluating:  36%|███▌      | 30/84 [00:12<00:32,  1.67it/s]11/28/2021 01:27:36 - INFO - __main__ -   Batch number = 31
Evaluating:  74%|███████▎  | 151/205 [00:33<00:33,  1.60it/s]11/28/2021 01:27:37 - INFO - __main__ -   Batch number = 152
Evaluating:  37%|███▋      | 31/84 [00:12<00:32,  1.65it/s]11/28/2021 01:27:37 - INFO - __main__ -   Batch number = 32
Evaluating:  74%|███████▍  | 152/205 [00:33<00:33,  1.60it/s]11/28/2021 01:27:37 - INFO - __main__ -   Batch number = 153
Evaluating:  38%|███▊      | 32/84 [00:13<00:31,  1.64it/s]11/28/2021 01:27:38 - INFO - __main__ -   Batch number = 33
Evaluating:  75%|███████▍  | 153/205 [00:34<00:32,  1.60it/s]11/28/2021 01:27:38 - INFO - __main__ -   Batch number = 154
Evaluating:  39%|███▉      | 33/84 [00:13<00:31,  1.62it/s]11/28/2021 01:27:38 - INFO - __main__ -   Batch number = 34
Evaluating:  75%|███████▌  | 154/205 [00:35<00:31,  1.60it/s]11/28/2021 01:27:39 - INFO - __main__ -   Batch number = 155
Evaluating:  40%|████      | 34/84 [00:14<00:30,  1.62it/s]11/28/2021 01:27:39 - INFO - __main__ -   Batch number = 35
Evaluating:  76%|███████▌  | 155/205 [00:35<00:31,  1.60it/s]11/28/2021 01:27:39 - INFO - __main__ -   Batch number = 156
Evaluating:  42%|████▏     | 35/84 [00:15<00:30,  1.62it/s]11/28/2021 01:27:39 - INFO - __main__ -   Batch number = 36
11/28/2021 01:27:40 - INFO - __main__ -   Language adapter for zh not found, using am instead
11/28/2021 01:27:40 - INFO - __main__ -   Set active language adapter to am
11/28/2021 01:27:40 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:27:40 - INFO - __main__ -   Adapter Languages = ['am']
11/28/2021 01:27:40 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_zh_bert-base-multilingual-cased_128
Evaluating:  76%|███████▌  | 156/205 [00:36<00:30,  1.59it/s]11/28/2021 01:27:40 - INFO - __main__ -   Batch number = 157
Evaluating:  43%|████▎     | 36/84 [00:15<00:29,  1.62it/s]11/28/2021 01:27:40 - INFO - __main__ -   Batch number = 37
11/28/2021 01:27:41 - INFO - __main__ -   ***** Running evaluation  in zh *****
11/28/2021 01:27:41 - INFO - __main__ -     Num examples = 3458
11/28/2021 01:27:41 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/109 [00:00<?, ?it/s]11/28/2021 01:27:41 - INFO - __main__ -   Batch number = 1
Evaluating:  77%|███████▋  | 157/205 [00:37<00:30,  1.59it/s]11/28/2021 01:27:41 - INFO - __main__ -   Batch number = 158
Evaluating:  44%|████▍     | 37/84 [00:16<00:29,  1.61it/s]11/28/2021 01:27:41 - INFO - __main__ -   Batch number = 38
Evaluating:   1%|          | 1/109 [00:00<01:06,  1.63it/s]11/28/2021 01:27:41 - INFO - __main__ -   Batch number = 2
Evaluating:  77%|███████▋  | 158/205 [00:37<00:29,  1.59it/s]11/28/2021 01:27:41 - INFO - __main__ -   Batch number = 159
Evaluating:  45%|████▌     | 38/84 [00:17<00:28,  1.61it/s]11/28/2021 01:27:41 - INFO - __main__ -   Batch number = 39
Evaluating:   2%|▏         | 2/109 [00:01<01:02,  1.70it/s]11/28/2021 01:27:42 - INFO - __main__ -   Batch number = 3
Evaluating:  78%|███████▊  | 159/205 [00:38<00:28,  1.59it/s]11/28/2021 01:27:42 - INFO - __main__ -   Batch number = 160
Evaluating:  46%|████▋     | 39/84 [00:17<00:28,  1.61it/s]11/28/2021 01:27:42 - INFO - __main__ -   Batch number = 40
Evaluating:   3%|▎         | 3/109 [00:01<01:03,  1.68it/s]11/28/2021 01:27:42 - INFO - __main__ -   Batch number = 4
Evaluating:  48%|████▊     | 40/84 [00:18<00:27,  1.60it/s]11/28/2021 01:27:43 - INFO - __main__ -   Batch number = 41
Evaluating:  78%|███████▊  | 160/205 [00:38<00:28,  1.58it/s]11/28/2021 01:27:43 - INFO - __main__ -   Batch number = 161
Evaluating:   4%|▎         | 4/109 [00:02<01:02,  1.67it/s]11/28/2021 01:27:43 - INFO - __main__ -   Batch number = 5
Evaluating:  49%|████▉     | 41/84 [00:18<00:26,  1.59it/s]11/28/2021 01:27:43 - INFO - __main__ -   Batch number = 42
Evaluating:  79%|███████▊  | 161/205 [00:39<00:27,  1.58it/s]11/28/2021 01:27:43 - INFO - __main__ -   Batch number = 162
Evaluating:   5%|▍         | 5/109 [00:03<01:02,  1.65it/s]11/28/2021 01:27:44 - INFO - __main__ -   Batch number = 6
Evaluating:  50%|█████     | 42/84 [00:19<00:26,  1.59it/s]11/28/2021 01:27:44 - INFO - __main__ -   Batch number = 43
Evaluating:  79%|███████▉  | 162/205 [00:40<00:27,  1.57it/s]11/28/2021 01:27:44 - INFO - __main__ -   Batch number = 163
Evaluating:   6%|▌         | 6/109 [00:03<01:02,  1.64it/s]11/28/2021 01:27:44 - INFO - __main__ -   Batch number = 7
Evaluating:  51%|█████     | 43/84 [00:20<00:25,  1.59it/s]11/28/2021 01:27:44 - INFO - __main__ -   Batch number = 44
Evaluating:  80%|███████▉  | 163/205 [00:40<00:26,  1.56it/s]11/28/2021 01:27:44 - INFO - __main__ -   Batch number = 164
Evaluating:   6%|▋         | 7/109 [00:04<01:02,  1.64it/s]11/28/2021 01:27:45 - INFO - __main__ -   Batch number = 8
Evaluating:  52%|█████▏    | 44/84 [00:20<00:25,  1.60it/s]11/28/2021 01:27:45 - INFO - __main__ -   Batch number = 45
Evaluating:  80%|████████  | 164/205 [00:41<00:26,  1.57it/s]11/28/2021 01:27:45 - INFO - __main__ -   Batch number = 165
Evaluating:   7%|▋         | 8/109 [00:04<01:01,  1.64it/s]11/28/2021 01:27:45 - INFO - __main__ -   Batch number = 9
Evaluating:  54%|█████▎    | 45/84 [00:21<00:24,  1.60it/s]11/28/2021 01:27:46 - INFO - __main__ -   Batch number = 46
Evaluating:  80%|████████  | 165/205 [00:42<00:25,  1.56it/s]11/28/2021 01:27:46 - INFO - __main__ -   Batch number = 166
Evaluating:   8%|▊         | 9/109 [00:05<01:01,  1.64it/s]11/28/2021 01:27:46 - INFO - __main__ -   Batch number = 10
Evaluating:  55%|█████▍    | 46/84 [00:22<00:23,  1.61it/s]11/28/2021 01:27:46 - INFO - __main__ -   Batch number = 47
Evaluating:  81%|████████  | 166/205 [00:42<00:24,  1.57it/s]11/28/2021 01:27:46 - INFO - __main__ -   Batch number = 167
Evaluating:   9%|▉         | 10/109 [00:06<01:00,  1.64it/s]11/28/2021 01:27:47 - INFO - __main__ -   Batch number = 11
Evaluating:  56%|█████▌    | 47/84 [00:22<00:23,  1.60it/s]11/28/2021 01:27:47 - INFO - __main__ -   Batch number = 48
Evaluating:  81%|████████▏ | 167/205 [00:43<00:24,  1.56it/s]11/28/2021 01:27:47 - INFO - __main__ -   Batch number = 168
Evaluating:  10%|█         | 11/109 [00:06<00:59,  1.65it/s]11/28/2021 01:27:47 - INFO - __main__ -   Batch number = 12
Evaluating:  57%|█████▋    | 48/84 [00:23<00:22,  1.60it/s]11/28/2021 01:27:48 - INFO - __main__ -   Batch number = 49
Evaluating:  82%|████████▏ | 168/205 [00:44<00:23,  1.56it/s]11/28/2021 01:27:48 - INFO - __main__ -   Batch number = 169
Evaluating:  11%|█         | 12/109 [00:07<00:58,  1.67it/s]11/28/2021 01:27:48 - INFO - __main__ -   Batch number = 13
Evaluating:  58%|█████▊    | 49/84 [00:23<00:21,  1.59it/s]11/28/2021 01:27:48 - INFO - __main__ -   Batch number = 50
Evaluating:  12%|█▏        | 13/109 [00:07<00:53,  1.79it/s]11/28/2021 01:27:48 - INFO - __main__ -   Batch number = 14
Evaluating:  82%|████████▏ | 169/205 [00:44<00:23,  1.56it/s]11/28/2021 01:27:48 - INFO - __main__ -   Batch number = 170
Evaluating:  13%|█▎        | 14/109 [00:08<00:50,  1.89it/s]11/28/2021 01:27:49 - INFO - __main__ -   Batch number = 15
Evaluating:  60%|█████▉    | 50/84 [00:24<00:21,  1.59it/s]11/28/2021 01:27:49 - INFO - __main__ -   Batch number = 51
Evaluating:  83%|████████▎ | 170/205 [00:45<00:22,  1.57it/s]11/28/2021 01:27:49 - INFO - __main__ -   Batch number = 171
Evaluating:  14%|█▍        | 15/109 [00:08<00:47,  2.00it/s]11/28/2021 01:27:49 - INFO - __main__ -   Batch number = 16
Evaluating:  61%|██████    | 51/84 [00:25<00:20,  1.61it/s]11/28/2021 01:27:49 - INFO - __main__ -   Batch number = 52
Evaluating:  83%|████████▎ | 171/205 [00:46<00:21,  1.58it/s]11/28/2021 01:27:50 - INFO - __main__ -   Batch number = 172
Evaluating:  15%|█▍        | 16/109 [00:09<00:44,  2.07it/s]11/28/2021 01:27:50 - INFO - __main__ -   Batch number = 17
Evaluating:  16%|█▌        | 17/109 [00:09<00:43,  2.13it/s]11/28/2021 01:27:50 - INFO - __main__ -   Batch number = 18
Evaluating:  62%|██████▏   | 52/84 [00:25<00:19,  1.61it/s]11/28/2021 01:27:50 - INFO - __main__ -   Batch number = 53
Evaluating:  84%|████████▍ | 172/205 [00:46<00:20,  1.58it/s]11/28/2021 01:27:50 - INFO - __main__ -   Batch number = 173
Evaluating:  17%|█▋        | 18/109 [00:09<00:41,  2.21it/s]11/28/2021 01:27:50 - INFO - __main__ -   Batch number = 19
Evaluating:  63%|██████▎   | 53/84 [00:26<00:19,  1.60it/s]11/28/2021 01:27:51 - INFO - __main__ -   Batch number = 54
Evaluating:  17%|█▋        | 19/109 [00:10<00:39,  2.30it/s]11/28/2021 01:27:51 - INFO - __main__ -   Batch number = 20
Evaluating:  84%|████████▍ | 173/205 [00:47<00:20,  1.56it/s]11/28/2021 01:27:51 - INFO - __main__ -   Batch number = 174
Evaluating:  18%|█▊        | 20/109 [00:10<00:37,  2.38it/s]11/28/2021 01:27:51 - INFO - __main__ -   Batch number = 21
Evaluating:  64%|██████▍   | 54/84 [00:27<00:18,  1.61it/s]11/28/2021 01:27:51 - INFO - __main__ -   Batch number = 55
Evaluating:  85%|████████▍ | 174/205 [00:47<00:19,  1.56it/s]11/28/2021 01:27:51 - INFO - __main__ -   Batch number = 175
Evaluating:  19%|█▉        | 21/109 [00:11<00:37,  2.38it/s]11/28/2021 01:27:52 - INFO - __main__ -   Batch number = 22
Evaluating:  65%|██████▌   | 55/84 [00:27<00:18,  1.61it/s]11/28/2021 01:27:52 - INFO - __main__ -   Batch number = 56
Evaluating:  20%|██        | 22/109 [00:11<00:34,  2.49it/s]11/28/2021 01:27:52 - INFO - __main__ -   Batch number = 23
Evaluating:  85%|████████▌ | 175/205 [00:48<00:19,  1.56it/s]11/28/2021 01:27:52 - INFO - __main__ -   Batch number = 176
Evaluating:  21%|██        | 23/109 [00:11<00:35,  2.40it/s]11/28/2021 01:27:52 - INFO - __main__ -   Batch number = 24
Evaluating:  67%|██████▋   | 56/84 [00:28<00:17,  1.58it/s]11/28/2021 01:27:53 - INFO - __main__ -   Batch number = 57
Evaluating:  86%|████████▌ | 176/205 [00:49<00:18,  1.58it/s]11/28/2021 01:27:53 - INFO - __main__ -   Batch number = 177
Evaluating:  22%|██▏       | 24/109 [00:12<00:33,  2.51it/s]11/28/2021 01:27:53 - INFO - __main__ -   Batch number = 25
Evaluating:  68%|██████▊   | 57/84 [00:28<00:17,  1.58it/s]11/28/2021 01:27:53 - INFO - __main__ -   Batch number = 58
Evaluating:  23%|██▎       | 25/109 [00:12<00:35,  2.39it/s]11/28/2021 01:27:53 - INFO - __main__ -   Batch number = 26
Evaluating:  86%|████████▋ | 177/205 [00:49<00:17,  1.58it/s]11/28/2021 01:27:53 - INFO - __main__ -   Batch number = 178
Evaluating:  24%|██▍       | 26/109 [00:13<00:32,  2.52it/s]11/28/2021 01:27:54 - INFO - __main__ -   Batch number = 27
Evaluating:  69%|██████▉   | 58/84 [00:29<00:16,  1.59it/s]11/28/2021 01:27:54 - INFO - __main__ -   Batch number = 59
Evaluating:  87%|████████▋ | 178/205 [00:50<00:17,  1.59it/s]11/28/2021 01:27:54 - INFO - __main__ -   Batch number = 179
Evaluating:  25%|██▍       | 27/109 [00:13<00:33,  2.45it/s]11/28/2021 01:27:54 - INFO - __main__ -   Batch number = 28
Evaluating:  70%|███████   | 59/84 [00:30<00:15,  1.59it/s]11/28/2021 01:27:54 - INFO - __main__ -   Batch number = 60
Evaluating:  26%|██▌       | 28/109 [00:13<00:33,  2.40it/s]11/28/2021 01:27:54 - INFO - __main__ -   Batch number = 29
Evaluating:  87%|████████▋ | 179/205 [00:51<00:16,  1.58it/s]11/28/2021 01:27:55 - INFO - __main__ -   Batch number = 180
Evaluating:  27%|██▋       | 29/109 [00:14<00:34,  2.35it/s]11/28/2021 01:27:55 - INFO - __main__ -   Batch number = 30
Evaluating:  71%|███████▏  | 60/84 [00:30<00:15,  1.59it/s]11/28/2021 01:27:55 - INFO - __main__ -   Batch number = 61
Evaluating:  88%|████████▊ | 180/205 [00:51<00:15,  1.58it/s]11/28/2021 01:27:55 - INFO - __main__ -   Batch number = 181
Evaluating:  28%|██▊       | 30/109 [00:14<00:34,  2.31it/s]11/28/2021 01:27:55 - INFO - __main__ -   Batch number = 31
Evaluating:  73%|███████▎  | 61/84 [00:31<00:14,  1.60it/s]11/28/2021 01:27:56 - INFO - __main__ -   Batch number = 62
Evaluating:  28%|██▊       | 31/109 [00:15<00:33,  2.30it/s]11/28/2021 01:27:56 - INFO - __main__ -   Batch number = 32
Evaluating:  88%|████████▊ | 181/205 [00:52<00:15,  1.58it/s]11/28/2021 01:27:56 - INFO - __main__ -   Batch number = 182
Evaluating:  29%|██▉       | 32/109 [00:15<00:32,  2.38it/s]11/28/2021 01:27:56 - INFO - __main__ -   Batch number = 33
Evaluating:  74%|███████▍  | 62/84 [00:32<00:13,  1.60it/s]11/28/2021 01:27:56 - INFO - __main__ -   Batch number = 63
Evaluating:  30%|███       | 33/109 [00:15<00:29,  2.60it/s]11/28/2021 01:27:56 - INFO - __main__ -   Batch number = 34
Evaluating:  89%|████████▉ | 182/205 [00:52<00:14,  1.59it/s]11/28/2021 01:27:57 - INFO - __main__ -   Batch number = 183
Evaluating:  31%|███       | 34/109 [00:16<00:28,  2.59it/s]11/28/2021 01:27:57 - INFO - __main__ -   Batch number = 35
Evaluating:  75%|███████▌  | 63/84 [00:32<00:13,  1.61it/s]11/28/2021 01:27:57 - INFO - __main__ -   Batch number = 64
Evaluating:  89%|████████▉ | 183/205 [00:53<00:13,  1.58it/s]11/28/2021 01:27:57 - INFO - __main__ -   Batch number = 184
Evaluating:  32%|███▏      | 35/109 [00:16<00:29,  2.48it/s]11/28/2021 01:27:57 - INFO - __main__ -   Batch number = 36
Evaluating:  76%|███████▌  | 64/84 [00:33<00:12,  1.61it/s]11/28/2021 01:27:58 - INFO - __main__ -   Batch number = 65
Evaluating:  90%|████████▉ | 184/205 [00:54<00:13,  1.58it/s]11/28/2021 01:27:58 - INFO - __main__ -   Batch number = 185
Evaluating:  33%|███▎      | 36/109 [00:17<00:30,  2.39it/s]11/28/2021 01:27:58 - INFO - __main__ -   Batch number = 37
Evaluating:  77%|███████▋  | 65/84 [00:33<00:11,  1.61it/s]11/28/2021 01:27:58 - INFO - __main__ -   Batch number = 66
Evaluating:  34%|███▍      | 37/109 [00:17<00:30,  2.34it/s]11/28/2021 01:27:58 - INFO - __main__ -   Batch number = 38
Evaluating:  90%|█████████ | 185/205 [00:54<00:12,  1.57it/s]11/28/2021 01:27:58 - INFO - __main__ -   Batch number = 186
Evaluating:  35%|███▍      | 38/109 [00:18<00:31,  2.27it/s]11/28/2021 01:27:59 - INFO - __main__ -   Batch number = 39
Evaluating:  79%|███████▊  | 66/84 [00:34<00:11,  1.61it/s]11/28/2021 01:27:59 - INFO - __main__ -   Batch number = 67
Evaluating:  91%|█████████ | 186/205 [00:55<00:12,  1.58it/s]11/28/2021 01:27:59 - INFO - __main__ -   Batch number = 187
Evaluating:  36%|███▌      | 39/109 [00:18<00:30,  2.26it/s]11/28/2021 01:27:59 - INFO - __main__ -   Batch number = 40
Evaluating:  80%|███████▉  | 67/84 [00:35<00:10,  1.61it/s]11/28/2021 01:27:59 - INFO - __main__ -   Batch number = 68
Evaluating:  91%|█████████ | 187/205 [00:56<00:11,  1.57it/s]11/28/2021 01:28:00 - INFO - __main__ -   Batch number = 188
Evaluating:  37%|███▋      | 40/109 [00:19<00:33,  2.08it/s]11/28/2021 01:28:00 - INFO - __main__ -   Batch number = 41
Evaluating:  81%|████████  | 68/84 [00:35<00:09,  1.62it/s]11/28/2021 01:28:00 - INFO - __main__ -   Batch number = 69
Evaluating:  38%|███▊      | 41/109 [00:19<00:31,  2.14it/s]11/28/2021 01:28:00 - INFO - __main__ -   Batch number = 42
Evaluating:  92%|█████████▏| 188/205 [00:56<00:10,  1.58it/s]11/28/2021 01:28:00 - INFO - __main__ -   Batch number = 189
Evaluating:  39%|███▊      | 42/109 [00:20<00:30,  2.16it/s]11/28/2021 01:28:01 - INFO - __main__ -   Batch number = 43
Evaluating:  82%|████████▏ | 69/84 [00:36<00:09,  1.61it/s]11/28/2021 01:28:01 - INFO - __main__ -   Batch number = 70
Evaluating:  92%|█████████▏| 189/205 [00:57<00:10,  1.57it/s]11/28/2021 01:28:01 - INFO - __main__ -   Batch number = 190
Evaluating:  39%|███▉      | 43/109 [00:20<00:30,  2.19it/s]11/28/2021 01:28:01 - INFO - __main__ -   Batch number = 44
Evaluating:  83%|████████▎ | 70/84 [00:36<00:08,  1.62it/s]11/28/2021 01:28:01 - INFO - __main__ -   Batch number = 71
Evaluating:  40%|████      | 44/109 [00:20<00:29,  2.20it/s]11/28/2021 01:28:01 - INFO - __main__ -   Batch number = 45
Evaluating:  93%|█████████▎| 190/205 [00:58<00:09,  1.58it/s]11/28/2021 01:28:02 - INFO - __main__ -   Batch number = 191
Evaluating:  85%|████████▍ | 71/84 [00:37<00:08,  1.62it/s]11/28/2021 01:28:02 - INFO - __main__ -   Batch number = 72
Evaluating:  41%|████▏     | 45/109 [00:21<00:28,  2.21it/s]11/28/2021 01:28:02 - INFO - __main__ -   Batch number = 46
Evaluating:  93%|█████████▎| 191/205 [00:58<00:08,  1.57it/s]11/28/2021 01:28:02 - INFO - __main__ -   Batch number = 192
Evaluating:  42%|████▏     | 46/109 [00:21<00:28,  2.19it/s]11/28/2021 01:28:02 - INFO - __main__ -   Batch number = 47
Evaluating:  86%|████████▌ | 72/84 [00:38<00:07,  1.62it/s]11/28/2021 01:28:02 - INFO - __main__ -   Batch number = 73
Evaluating:  94%|█████████▎| 192/205 [00:59<00:08,  1.58it/s]11/28/2021 01:28:03 - INFO - __main__ -   Batch number = 193
Evaluating:  43%|████▎     | 47/109 [00:22<00:28,  2.19it/s]11/28/2021 01:28:03 - INFO - __main__ -   Batch number = 48
Evaluating:  87%|████████▋ | 73/84 [00:38<00:06,  1.61it/s]11/28/2021 01:28:03 - INFO - __main__ -   Batch number = 74
Evaluating:  44%|████▍     | 48/109 [00:22<00:27,  2.20it/s]11/28/2021 01:28:03 - INFO - __main__ -   Batch number = 49
Evaluating:  94%|█████████▍| 193/205 [00:59<00:07,  1.58it/s]11/28/2021 01:28:03 - INFO - __main__ -   Batch number = 194
Evaluating:  88%|████████▊ | 74/84 [00:39<00:06,  1.62it/s]11/28/2021 01:28:04 - INFO - __main__ -   Batch number = 75
Evaluating:  45%|████▍     | 49/109 [00:23<00:27,  2.21it/s]11/28/2021 01:28:04 - INFO - __main__ -   Batch number = 50
Evaluating:  95%|█████████▍| 194/205 [01:00<00:06,  1.59it/s]11/28/2021 01:28:04 - INFO - __main__ -   Batch number = 195
Evaluating:  46%|████▌     | 50/109 [00:23<00:26,  2.23it/s]11/28/2021 01:28:04 - INFO - __main__ -   Batch number = 51
Evaluating:  89%|████████▉ | 75/84 [00:40<00:05,  1.62it/s]11/28/2021 01:28:04 - INFO - __main__ -   Batch number = 76
Evaluating:  95%|█████████▌| 195/205 [01:01<00:06,  1.59it/s]11/28/2021 01:28:05 - INFO - __main__ -   Batch number = 196
Evaluating:  47%|████▋     | 51/109 [00:24<00:27,  2.09it/s]11/28/2021 01:28:05 - INFO - __main__ -   Batch number = 52
Evaluating:  90%|█████████ | 76/84 [00:40<00:04,  1.62it/s]11/28/2021 01:28:05 - INFO - __main__ -   Batch number = 77
Evaluating:  48%|████▊     | 52/109 [00:24<00:26,  2.13it/s]11/28/2021 01:28:05 - INFO - __main__ -   Batch number = 53
Evaluating:  96%|█████████▌| 196/205 [01:01<00:05,  1.59it/s]11/28/2021 01:28:05 - INFO - __main__ -   Batch number = 197
Evaluating:  92%|█████████▏| 77/84 [00:41<00:04,  1.62it/s]11/28/2021 01:28:06 - INFO - __main__ -   Batch number = 78
Evaluating:  49%|████▊     | 53/109 [00:25<00:26,  2.13it/s]11/28/2021 01:28:06 - INFO - __main__ -   Batch number = 54
Evaluating:  96%|█████████▌| 197/205 [01:02<00:05,  1.57it/s]11/28/2021 01:28:06 - INFO - __main__ -   Batch number = 198
Evaluating:  50%|████▉     | 54/109 [00:25<00:25,  2.15it/s]11/28/2021 01:28:06 - INFO - __main__ -   Batch number = 55
Evaluating:  93%|█████████▎| 78/84 [00:41<00:03,  1.63it/s]11/28/2021 01:28:06 - INFO - __main__ -   Batch number = 79
Evaluating:  50%|█████     | 55/109 [00:26<00:24,  2.16it/s]11/28/2021 01:28:07 - INFO - __main__ -   Batch number = 56
Evaluating:  97%|█████████▋| 198/205 [01:03<00:04,  1.57it/s]11/28/2021 01:28:07 - INFO - __main__ -   Batch number = 199
Evaluating:  94%|█████████▍| 79/84 [00:42<00:03,  1.62it/s]11/28/2021 01:28:07 - INFO - __main__ -   Batch number = 80
Evaluating:  51%|█████▏    | 56/109 [00:26<00:24,  2.17it/s]11/28/2021 01:28:07 - INFO - __main__ -   Batch number = 57
Evaluating:  97%|█████████▋| 199/205 [01:03<00:03,  1.57it/s]11/28/2021 01:28:07 - INFO - __main__ -   Batch number = 200
Evaluating:  95%|█████████▌| 80/84 [00:43<00:02,  1.62it/s]11/28/2021 01:28:07 - INFO - __main__ -   Batch number = 81
Evaluating:  52%|█████▏    | 57/109 [00:26<00:23,  2.17it/s]11/28/2021 01:28:07 - INFO - __main__ -   Batch number = 58
Evaluating:  98%|█████████▊| 200/205 [01:04<00:03,  1.57it/s]11/28/2021 01:28:08 - INFO - __main__ -   Batch number = 201
Evaluating:  53%|█████▎    | 58/109 [00:27<00:23,  2.14it/s]11/28/2021 01:28:08 - INFO - __main__ -   Batch number = 59
Evaluating:  96%|█████████▋| 81/84 [00:43<00:01,  1.61it/s]11/28/2021 01:28:08 - INFO - __main__ -   Batch number = 82
Evaluating:  54%|█████▍    | 59/109 [00:27<00:23,  2.16it/s]11/28/2021 01:28:08 - INFO - __main__ -   Batch number = 60
Evaluating:  98%|█████████▊| 201/205 [01:05<00:02,  1.56it/s]11/28/2021 01:28:09 - INFO - __main__ -   Batch number = 202
Evaluating:  98%|█████████▊| 82/84 [00:44<00:01,  1.61it/s]11/28/2021 01:28:09 - INFO - __main__ -   Batch number = 83
Evaluating:  55%|█████▌    | 60/109 [00:28<00:22,  2.19it/s]11/28/2021 01:28:09 - INFO - __main__ -   Batch number = 61
Evaluating:  56%|█████▌    | 61/109 [00:28<00:20,  2.36it/s]Evaluating:  99%|█████████▊| 202/205 [01:05<00:01,  1.57it/s]Evaluating:  99%|█████████▉| 83/84 [00:45<00:00,  1.63it/s]11/28/2021 01:28:09 - INFO - __main__ -   Batch number = 62
11/28/2021 01:28:09 - INFO - __main__ -   Batch number = 203
11/28/2021 01:28:09 - INFO - __main__ -   Batch number = 84
Evaluating:  57%|█████▋    | 62/109 [00:29<00:21,  2.22it/s]11/28/2021 01:28:10 - INFO - __main__ -   Batch number = 63
Evaluating: 100%|██████████| 84/84 [00:45<00:00,  1.74it/s]Evaluating: 100%|██████████| 84/84 [00:45<00:00,  1.85it/s]Evaluating:  99%|█████████▉| 203/205 [01:06<00:01,  1.61it/s]11/28/2021 01:28:10 - INFO - __main__ -   Batch number = 204
Evaluating: 100%|█████████▉| 204/205 [01:06<00:00,  1.88it/s]11/28/2021 01:28:10 - INFO - __main__ -   Batch number = 205
Evaluating:  58%|█████▊    | 63/109 [00:29<00:20,  2.26it/s]11/28/2021 01:28:10 - INFO - __main__ -   Batch number = 64
Evaluating: 100%|██████████| 205/205 [01:06<00:00,  2.19it/s]Evaluating: 100%|██████████| 205/205 [01:06<00:00,  3.07it/s]Evaluating:  59%|█████▊    | 64/109 [00:30<00:20,  2.25it/s]11/28/2021 01:28:11 - INFO - __main__ -   Batch number = 65
Evaluating:  60%|█████▉    | 65/109 [00:30<00:19,  2.24it/s]11/28/2021 01:28:11 - INFO - __main__ -   Batch number = 66

/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:28:11 - INFO - __main__ -   ***** Evaluation result  in pt *****
11/28/2021 01:28:11 - INFO - __main__ -     f1 = 0.8828090404702571
11/28/2021 01:28:11 - INFO - __main__ -     loss = 0.39961111953570727
11/28/2021 01:28:11 - INFO - __main__ -     precision = 0.8872076788830715
11/28/2021 01:28:11 - INFO - __main__ -     recall = 0.8784538024226296
Evaluating:  61%|██████    | 66/109 [00:30<00:19,  2.24it/s]11/28/2021 01:28:12 - INFO - __main__ -   Batch number = 67
Evaluating:  61%|██████▏   | 67/109 [00:31<00:18,  2.24it/s]11/28/2021 01:28:12 - INFO - __main__ -   Batch number = 68
51.42user 18.31system 1:11.40elapsed 97%CPU (0avgtext+0avgdata 3940696maxresident)k
0inputs+680outputs (0major+1535838minor)pagefaults 0swaps
Evaluating:  62%|██████▏   | 68/109 [00:31<00:18,  2.22it/s]11/28/2021 01:28:12 - INFO - __main__ -   Batch number = 69

/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:28:13 - INFO - __main__ -   ***** Evaluation result  in fi *****
11/28/2021 01:28:13 - INFO - __main__ -     f1 = 0.7510387596899226
11/28/2021 01:28:13 - INFO - __main__ -     loss = 0.910625213966137
11/28/2021 01:28:13 - INFO - __main__ -     precision = 0.7589221369262102
11/28/2021 01:28:13 - INFO - __main__ -     recall = 0.7433174773668866
Evaluating:  63%|██████▎   | 69/109 [00:32<00:18,  2.19it/s]11/28/2021 01:28:13 - INFO - __main__ -   Batch number = 70
Evaluating:  64%|██████▍   | 70/109 [00:32<00:17,  2.20it/s]11/28/2021 01:28:13 - INFO - __main__ -   Batch number = 71
Evaluating:  65%|██████▌   | 71/109 [00:33<00:17,  2.22it/s]11/28/2021 01:28:14 - INFO - __main__ -   Batch number = 72
69.83user 24.41system 1:35.17elapsed 99%CPU (0avgtext+0avgdata 3937100maxresident)k
0inputs+872outputs (0major+1571742minor)pagefaults 0swaps
Evaluating:  66%|██████▌   | 72/109 [00:33<00:16,  2.21it/s]11/28/2021 01:28:14 - INFO - __main__ -   Batch number = 73
Evaluating:  67%|██████▋   | 73/109 [00:34<00:16,  2.20it/s]11/28/2021 01:28:15 - INFO - __main__ -   Batch number = 74
Evaluating:  68%|██████▊   | 74/109 [00:34<00:15,  2.20it/s]11/28/2021 01:28:15 - INFO - __main__ -   Batch number = 75
Evaluating:  69%|██████▉   | 75/109 [00:35<00:15,  2.19it/s]11/28/2021 01:28:16 - INFO - __main__ -   Batch number = 76
Evaluating:  70%|██████▉   | 76/109 [00:35<00:15,  2.18it/s]11/28/2021 01:28:16 - INFO - __main__ -   Batch number = 77
Evaluating:  71%|███████   | 77/109 [00:36<00:14,  2.16it/s]11/28/2021 01:28:17 - INFO - __main__ -   Batch number = 78
Evaluating:  72%|███████▏  | 78/109 [00:36<00:14,  2.18it/s]11/28/2021 01:28:17 - INFO - __main__ -   Batch number = 79
Evaluating:  72%|███████▏  | 79/109 [00:36<00:13,  2.21it/s]11/28/2021 01:28:17 - INFO - __main__ -   Batch number = 80
Evaluating:  73%|███████▎  | 80/109 [00:37<00:13,  2.23it/s]11/28/2021 01:28:18 - INFO - __main__ -   Batch number = 81
Evaluating:  74%|███████▍  | 81/109 [00:37<00:12,  2.23it/s]11/28/2021 01:28:18 - INFO - __main__ -   Batch number = 82
Evaluating:  75%|███████▌  | 82/109 [00:38<00:12,  2.22it/s]11/28/2021 01:28:19 - INFO - __main__ -   Batch number = 83
Evaluating:  76%|███████▌  | 83/109 [00:38<00:11,  2.22it/s]11/28/2021 01:28:19 - INFO - __main__ -   Batch number = 84
Evaluating:  77%|███████▋  | 84/109 [00:39<00:12,  1.93it/s]11/28/2021 01:28:20 - INFO - __main__ -   Batch number = 85
Evaluating:  78%|███████▊  | 85/109 [00:39<00:13,  1.84it/s]11/28/2021 01:28:20 - INFO - __main__ -   Batch number = 86
Evaluating:  79%|███████▉  | 86/109 [00:40<00:13,  1.77it/s]11/28/2021 01:28:21 - INFO - __main__ -   Batch number = 87
Evaluating:  80%|███████▉  | 87/109 [00:41<00:12,  1.73it/s]11/28/2021 01:28:22 - INFO - __main__ -   Batch number = 88
Evaluating:  81%|████████  | 88/109 [00:41<00:12,  1.71it/s]11/28/2021 01:28:22 - INFO - __main__ -   Batch number = 89
Evaluating:  82%|████████▏ | 89/109 [00:42<00:11,  1.69it/s]11/28/2021 01:28:23 - INFO - __main__ -   Batch number = 90
Evaluating:  83%|████████▎ | 90/109 [00:42<00:10,  1.76it/s]11/28/2021 01:28:23 - INFO - __main__ -   Batch number = 91
Evaluating:  83%|████████▎ | 91/109 [00:43<00:09,  1.87it/s]11/28/2021 01:28:24 - INFO - __main__ -   Batch number = 92
Evaluating:  84%|████████▍ | 92/109 [00:43<00:08,  1.92it/s]11/28/2021 01:28:24 - INFO - __main__ -   Batch number = 93
Evaluating:  85%|████████▌ | 93/109 [00:44<00:08,  1.98it/s]11/28/2021 01:28:25 - INFO - __main__ -   Batch number = 94
Evaluating:  86%|████████▌ | 94/109 [00:44<00:07,  2.04it/s]11/28/2021 01:28:25 - INFO - __main__ -   Batch number = 95
Evaluating:  87%|████████▋ | 95/109 [00:45<00:06,  2.11it/s]11/28/2021 01:28:26 - INFO - __main__ -   Batch number = 96
Evaluating:  88%|████████▊ | 96/109 [00:45<00:05,  2.22it/s]11/28/2021 01:28:26 - INFO - __main__ -   Batch number = 97
Evaluating:  89%|████████▉ | 97/109 [00:45<00:04,  2.42it/s]11/28/2021 01:28:26 - INFO - __main__ -   Batch number = 98
Evaluating:  90%|████████▉ | 98/109 [00:46<00:04,  2.61it/s]11/28/2021 01:28:27 - INFO - __main__ -   Batch number = 99
Evaluating:  91%|█████████ | 99/109 [00:46<00:03,  2.76it/s]11/28/2021 01:28:27 - INFO - __main__ -   Batch number = 100
Evaluating:  92%|█████████▏| 100/109 [00:46<00:03,  2.85it/s]11/28/2021 01:28:27 - INFO - __main__ -   Batch number = 101
Evaluating:  93%|█████████▎| 101/109 [00:47<00:02,  2.93it/s]11/28/2021 01:28:28 - INFO - __main__ -   Batch number = 102
Evaluating:  94%|█████████▎| 102/109 [00:47<00:02,  2.99it/s]11/28/2021 01:28:28 - INFO - __main__ -   Batch number = 103
Evaluating:  94%|█████████▍| 103/109 [00:47<00:01,  3.02it/s]11/28/2021 01:28:28 - INFO - __main__ -   Batch number = 104
Evaluating:  95%|█████████▌| 104/109 [00:48<00:01,  3.05it/s]11/28/2021 01:28:29 - INFO - __main__ -   Batch number = 105
Evaluating:  96%|█████████▋| 105/109 [00:48<00:01,  3.07it/s]11/28/2021 01:28:29 - INFO - __main__ -   Batch number = 106
Evaluating:  97%|█████████▋| 106/109 [00:48<00:00,  3.09it/s]11/28/2021 01:28:29 - INFO - __main__ -   Batch number = 107
Evaluating:  98%|█████████▊| 107/109 [00:49<00:00,  3.10it/s]11/28/2021 01:28:30 - INFO - __main__ -   Batch number = 108
Evaluating:  99%|█████████▉| 108/109 [00:49<00:00,  3.11it/s]11/28/2021 01:28:30 - INFO - __main__ -   Batch number = 109
Evaluating: 100%|██████████| 109/109 [00:49<00:00,  2.20it/s]
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:28:32 - INFO - __main__ -   ***** Evaluation result  in zh *****
11/28/2021 01:28:32 - INFO - __main__ -     f1 = 0.6285467065652377
11/28/2021 01:28:32 - INFO - __main__ -     loss = 1.3126799775919784
11/28/2021 01:28:32 - INFO - __main__ -     precision = 0.6381872757888571
11/28/2021 01:28:32 - INFO - __main__ -     recall = 0.619193067197045
55.11user 17.81system 1:15.31elapsed 96%CPU (0avgtext+0avgdata 3942068maxresident)k
0inputs+744outputs (0major+1284096minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:28:35 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='zh', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:28:35 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:28:35 - INFO - __main__ -   Seed = 3
11/28/2021 01:28:35 - INFO - root -   save model
11/28/2021 01:28:35 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='zh', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:28:35 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:28:38 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:28:44 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:28:44 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:28:44 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
11/28/2021 01:28:44 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:28:44 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:28:45 - INFO - root -   loading lang adpater am/wiki@ukp
11/28/2021 01:28:45 - INFO - __main__ -   Adapter Languages : ['am'], Length : 1
11/28/2021 01:28:45 - INFO - __main__ -   Adapter Names ['am/wiki@ukp'], Length : 1
11/28/2021 01:28:45 - INFO - __main__ -   Language = am
11/28/2021 01:28:45 - INFO - __main__ -   Adapter Name = am/wiki@ukp
Found matching adapter at: adapters/ukp/bert-base-multilingual-cased_am_wiki_pfeiffer.json
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/am/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_am_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/adapter_config.json
Adding adapter 'am' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/head_config.json
11/28/2021 01:28:54 - INFO - __main__ -   Language adapter for zh not found, using am instead
11/28/2021 01:28:54 - INFO - __main__ -   Set active language adapter to am
11/28/2021 01:28:54 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:28:54 - INFO - __main__ -   Adapter Languages = ['am']
11/28/2021 01:28:54 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_zh_bert-base-multilingual-cased_128
11/28/2021 01:28:55 - INFO - __main__ -   ***** Running evaluation  in zh *****
11/28/2021 01:28:55 - INFO - __main__ -     Num examples = 3458
11/28/2021 01:28:55 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/109 [00:00<?, ?it/s]11/28/2021 01:28:55 - INFO - __main__ -   Batch number = 1
Evaluating:   1%|          | 1/109 [00:00<00:34,  3.17it/s]11/28/2021 01:28:55 - INFO - __main__ -   Batch number = 2
Evaluating:   2%|▏         | 2/109 [00:00<00:32,  3.25it/s]11/28/2021 01:28:55 - INFO - __main__ -   Batch number = 3
Evaluating:   3%|▎         | 3/109 [00:01<00:40,  2.65it/s]11/28/2021 01:28:56 - INFO - __main__ -   Batch number = 4
Evaluating:   4%|▎         | 4/109 [00:01<00:43,  2.43it/s]11/28/2021 01:28:56 - INFO - __main__ -   Batch number = 5
Evaluating:   5%|▍         | 5/109 [00:02<00:44,  2.32it/s]11/28/2021 01:28:57 - INFO - __main__ -   Batch number = 6
Evaluating:   6%|▌         | 6/109 [00:02<00:45,  2.26it/s]11/28/2021 01:28:57 - INFO - __main__ -   Batch number = 7
Evaluating:   6%|▋         | 7/109 [00:02<00:45,  2.22it/s]11/28/2021 01:28:58 - INFO - __main__ -   Batch number = 8
Evaluating:   7%|▋         | 8/109 [00:03<00:46,  2.18it/s]11/28/2021 01:28:58 - INFO - __main__ -   Batch number = 9
Evaluating:   8%|▊         | 9/109 [00:03<00:46,  2.17it/s]11/28/2021 01:28:59 - INFO - __main__ -   Batch number = 10
Evaluating:   9%|▉         | 10/109 [00:04<00:45,  2.17it/s]11/28/2021 01:28:59 - INFO - __main__ -   Batch number = 11
Evaluating:  10%|█         | 11/109 [00:04<00:44,  2.19it/s]11/28/2021 01:28:59 - INFO - __main__ -   Batch number = 12
Evaluating:  11%|█         | 12/109 [00:05<00:44,  2.18it/s]11/28/2021 01:29:00 - INFO - __main__ -   Batch number = 13
Evaluating:  12%|█▏        | 13/109 [00:05<00:44,  2.17it/s]11/28/2021 01:29:00 - INFO - __main__ -   Batch number = 14
Evaluating:  13%|█▎        | 14/109 [00:06<00:43,  2.16it/s]11/28/2021 01:29:01 - INFO - __main__ -   Batch number = 15
Evaluating:  14%|█▍        | 15/109 [00:06<00:43,  2.16it/s]11/28/2021 01:29:01 - INFO - __main__ -   Batch number = 16
Evaluating:  15%|█▍        | 16/109 [00:07<00:43,  2.14it/s]11/28/2021 01:29:02 - INFO - __main__ -   Batch number = 17
Evaluating:  16%|█▌        | 17/109 [00:07<00:43,  2.13it/s]11/28/2021 01:29:02 - INFO - __main__ -   Batch number = 18
Evaluating:  17%|█▋        | 18/109 [00:08<00:42,  2.14it/s]11/28/2021 01:29:03 - INFO - __main__ -   Batch number = 19
Evaluating:  17%|█▋        | 19/109 [00:08<00:42,  2.14it/s]11/28/2021 01:29:03 - INFO - __main__ -   Batch number = 20
Evaluating:  18%|█▊        | 20/109 [00:09<00:41,  2.13it/s]11/28/2021 01:29:04 - INFO - __main__ -   Batch number = 21
Evaluating:  19%|█▉        | 21/109 [00:09<00:41,  2.12it/s]11/28/2021 01:29:04 - INFO - __main__ -   Batch number = 22
Evaluating:  20%|██        | 22/109 [00:09<00:41,  2.10it/s]11/28/2021 01:29:05 - INFO - __main__ -   Batch number = 23
Evaluating:  21%|██        | 23/109 [00:10<00:40,  2.12it/s]11/28/2021 01:29:05 - INFO - __main__ -   Batch number = 24
Evaluating:  22%|██▏       | 24/109 [00:10<00:40,  2.12it/s]11/28/2021 01:29:06 - INFO - __main__ -   Batch number = 25
Evaluating:  23%|██▎       | 25/109 [00:11<00:39,  2.13it/s]11/28/2021 01:29:06 - INFO - __main__ -   Batch number = 26
Evaluating:  24%|██▍       | 26/109 [00:11<00:38,  2.14it/s]11/28/2021 01:29:06 - INFO - __main__ -   Batch number = 27
Evaluating:  25%|██▍       | 27/109 [00:12<00:38,  2.13it/s]11/28/2021 01:29:07 - INFO - __main__ -   Batch number = 28
Evaluating:  26%|██▌       | 28/109 [00:12<00:38,  2.13it/s]11/28/2021 01:29:07 - INFO - __main__ -   Batch number = 29
Evaluating:  27%|██▋       | 29/109 [00:13<00:37,  2.12it/s]11/28/2021 01:29:08 - INFO - __main__ -   Batch number = 30
Evaluating:  28%|██▊       | 30/109 [00:13<00:37,  2.13it/s]11/28/2021 01:29:08 - INFO - __main__ -   Batch number = 31
Evaluating:  28%|██▊       | 31/109 [00:14<00:36,  2.13it/s]11/28/2021 01:29:09 - INFO - __main__ -   Batch number = 32
Evaluating:  29%|██▉       | 32/109 [00:14<00:36,  2.13it/s]11/28/2021 01:29:09 - INFO - __main__ -   Batch number = 33
Evaluating:  30%|███       | 33/109 [00:15<00:35,  2.12it/s]11/28/2021 01:29:10 - INFO - __main__ -   Batch number = 34
Evaluating:  31%|███       | 34/109 [00:15<00:34,  2.15it/s]11/28/2021 01:29:10 - INFO - __main__ -   Batch number = 35
Evaluating:  32%|███▏      | 35/109 [00:15<00:30,  2.40it/s]11/28/2021 01:29:11 - INFO - __main__ -   Batch number = 36
Evaluating:  33%|███▎      | 36/109 [00:16<00:28,  2.60it/s]11/28/2021 01:29:11 - INFO - __main__ -   Batch number = 37
Evaluating:  34%|███▍      | 37/109 [00:16<00:26,  2.77it/s]11/28/2021 01:29:11 - INFO - __main__ -   Batch number = 38
Evaluating:  35%|███▍      | 38/109 [00:16<00:25,  2.80it/s]11/28/2021 01:29:12 - INFO - __main__ -   Batch number = 39
Evaluating:  36%|███▌      | 39/109 [00:17<00:27,  2.56it/s]11/28/2021 01:29:12 - INFO - __main__ -   Batch number = 40
Evaluating:  37%|███▋      | 40/109 [00:17<00:28,  2.40it/s]11/28/2021 01:29:12 - INFO - __main__ -   Batch number = 41
Evaluating:  38%|███▊      | 41/109 [00:18<00:29,  2.31it/s]11/28/2021 01:29:13 - INFO - __main__ -   Batch number = 42
Evaluating:  39%|███▊      | 42/109 [00:18<00:29,  2.27it/s]11/28/2021 01:29:13 - INFO - __main__ -   Batch number = 43
Evaluating:  39%|███▉      | 43/109 [00:19<00:29,  2.22it/s]11/28/2021 01:29:14 - INFO - __main__ -   Batch number = 44
Evaluating:  40%|████      | 44/109 [00:19<00:28,  2.24it/s]11/28/2021 01:29:14 - INFO - __main__ -   Batch number = 45
Evaluating:  41%|████▏     | 45/109 [00:20<00:31,  2.01it/s]11/28/2021 01:29:15 - INFO - __main__ -   Batch number = 46
Evaluating:  42%|████▏     | 46/109 [00:20<00:30,  2.05it/s]11/28/2021 01:29:15 - INFO - __main__ -   Batch number = 47
Evaluating:  43%|████▎     | 47/109 [00:21<00:29,  2.08it/s]11/28/2021 01:29:16 - INFO - __main__ -   Batch number = 48
Evaluating:  44%|████▍     | 48/109 [00:21<00:29,  2.10it/s]11/28/2021 01:29:16 - INFO - __main__ -   Batch number = 49
Evaluating:  45%|████▍     | 49/109 [00:22<00:28,  2.10it/s]11/28/2021 01:29:17 - INFO - __main__ -   Batch number = 50
Evaluating:  46%|████▌     | 50/109 [00:22<00:27,  2.11it/s]11/28/2021 01:29:17 - INFO - __main__ -   Batch number = 51
Evaluating:  47%|████▋     | 51/109 [00:23<00:27,  2.12it/s]11/28/2021 01:29:18 - INFO - __main__ -   Batch number = 52
Evaluating:  48%|████▊     | 52/109 [00:23<00:26,  2.13it/s]11/28/2021 01:29:18 - INFO - __main__ -   Batch number = 53
Evaluating:  49%|████▊     | 53/109 [00:23<00:26,  2.14it/s]11/28/2021 01:29:19 - INFO - __main__ -   Batch number = 54
Evaluating:  50%|████▉     | 54/109 [00:24<00:25,  2.14it/s]11/28/2021 01:29:19 - INFO - __main__ -   Batch number = 55
Evaluating:  50%|█████     | 55/109 [00:24<00:25,  2.15it/s]11/28/2021 01:29:20 - INFO - __main__ -   Batch number = 56
Evaluating:  51%|█████▏    | 56/109 [00:25<00:24,  2.14it/s]11/28/2021 01:29:20 - INFO - __main__ -   Batch number = 57
Evaluating:  52%|█████▏    | 57/109 [00:25<00:24,  2.15it/s]11/28/2021 01:29:21 - INFO - __main__ -   Batch number = 58
Evaluating:  53%|█████▎    | 58/109 [00:26<00:23,  2.15it/s]11/28/2021 01:29:21 - INFO - __main__ -   Batch number = 59
Evaluating:  54%|█████▍    | 59/109 [00:26<00:23,  2.14it/s]11/28/2021 01:29:21 - INFO - __main__ -   Batch number = 60
Evaluating:  55%|█████▌    | 60/109 [00:27<00:22,  2.14it/s]11/28/2021 01:29:22 - INFO - __main__ -   Batch number = 61
Evaluating:  56%|█████▌    | 61/109 [00:27<00:22,  2.12it/s]11/28/2021 01:29:22 - INFO - __main__ -   Batch number = 62
Evaluating:  57%|█████▋    | 62/109 [00:28<00:22,  2.10it/s]11/28/2021 01:29:23 - INFO - __main__ -   Batch number = 63
Evaluating:  58%|█████▊    | 63/109 [00:28<00:21,  2.10it/s]11/28/2021 01:29:23 - INFO - __main__ -   Batch number = 64
Evaluating:  59%|█████▊    | 64/109 [00:29<00:21,  2.10it/s]11/28/2021 01:29:24 - INFO - __main__ -   Batch number = 65
Evaluating:  60%|█████▉    | 65/109 [00:29<00:20,  2.10it/s]11/28/2021 01:29:24 - INFO - __main__ -   Batch number = 66
Evaluating:  61%|██████    | 66/109 [00:30<00:21,  2.02it/s]11/28/2021 01:29:25 - INFO - __main__ -   Batch number = 67
Evaluating:  61%|██████▏   | 67/109 [00:30<00:20,  2.04it/s]11/28/2021 01:29:25 - INFO - __main__ -   Batch number = 68
Evaluating:  62%|██████▏   | 68/109 [00:31<00:19,  2.07it/s]11/28/2021 01:29:26 - INFO - __main__ -   Batch number = 69
Evaluating:  63%|██████▎   | 69/109 [00:31<00:19,  2.09it/s]11/28/2021 01:29:26 - INFO - __main__ -   Batch number = 70
Evaluating:  64%|██████▍   | 70/109 [00:32<00:18,  2.10it/s]11/28/2021 01:29:27 - INFO - __main__ -   Batch number = 71
Evaluating:  65%|██████▌   | 71/109 [00:32<00:18,  2.10it/s]11/28/2021 01:29:27 - INFO - __main__ -   Batch number = 72
Evaluating:  66%|██████▌   | 72/109 [00:33<00:17,  2.12it/s]11/28/2021 01:29:28 - INFO - __main__ -   Batch number = 73
Evaluating:  67%|██████▋   | 73/109 [00:33<00:16,  2.13it/s]11/28/2021 01:29:28 - INFO - __main__ -   Batch number = 74
Evaluating:  68%|██████▊   | 74/109 [00:33<00:16,  2.13it/s]11/28/2021 01:29:29 - INFO - __main__ -   Batch number = 75
Evaluating:  69%|██████▉   | 75/109 [00:34<00:15,  2.14it/s]11/28/2021 01:29:29 - INFO - __main__ -   Batch number = 76
Evaluating:  70%|██████▉   | 76/109 [00:34<00:14,  2.30it/s]11/28/2021 01:29:29 - INFO - __main__ -   Batch number = 77
Evaluating:  71%|███████   | 77/109 [00:35<00:14,  2.20it/s]11/28/2021 01:29:30 - INFO - __main__ -   Batch number = 78
Evaluating:  72%|███████▏  | 78/109 [00:35<00:14,  2.15it/s]11/28/2021 01:29:30 - INFO - __main__ -   Batch number = 79
Evaluating:  72%|███████▏  | 79/109 [00:36<00:13,  2.15it/s]11/28/2021 01:29:31 - INFO - __main__ -   Batch number = 80
Evaluating:  73%|███████▎  | 80/109 [00:36<00:13,  2.14it/s]11/28/2021 01:29:31 - INFO - __main__ -   Batch number = 81
Evaluating:  74%|███████▍  | 81/109 [00:37<00:13,  2.14it/s]11/28/2021 01:29:32 - INFO - __main__ -   Batch number = 82
Evaluating:  75%|███████▌  | 82/109 [00:37<00:12,  2.11it/s]11/28/2021 01:29:32 - INFO - __main__ -   Batch number = 83
Evaluating:  76%|███████▌  | 83/109 [00:38<00:12,  2.13it/s]11/28/2021 01:29:33 - INFO - __main__ -   Batch number = 84
Evaluating:  77%|███████▋  | 84/109 [00:38<00:11,  2.14it/s]11/28/2021 01:29:33 - INFO - __main__ -   Batch number = 85
Evaluating:  78%|███████▊  | 85/109 [00:39<00:11,  2.15it/s]11/28/2021 01:29:34 - INFO - __main__ -   Batch number = 86
Evaluating:  79%|███████▉  | 86/109 [00:39<00:10,  2.15it/s]11/28/2021 01:29:34 - INFO - __main__ -   Batch number = 87
Evaluating:  80%|███████▉  | 87/109 [00:39<00:10,  2.14it/s]11/28/2021 01:29:35 - INFO - __main__ -   Batch number = 88
Evaluating:  81%|████████  | 88/109 [00:40<00:09,  2.14it/s]11/28/2021 01:29:35 - INFO - __main__ -   Batch number = 89
Evaluating:  82%|████████▏ | 89/109 [00:40<00:09,  2.11it/s]11/28/2021 01:29:36 - INFO - __main__ -   Batch number = 90
Evaluating:  83%|████████▎ | 90/109 [00:41<00:08,  2.12it/s]11/28/2021 01:29:36 - INFO - __main__ -   Batch number = 91
Evaluating:  83%|████████▎ | 91/109 [00:41<00:08,  2.12it/s]11/28/2021 01:29:37 - INFO - __main__ -   Batch number = 92
Evaluating:  84%|████████▍ | 92/109 [00:42<00:08,  2.11it/s]11/28/2021 01:29:37 - INFO - __main__ -   Batch number = 93
Evaluating:  85%|████████▌ | 93/109 [00:42<00:07,  2.11it/s]11/28/2021 01:29:37 - INFO - __main__ -   Batch number = 94
Evaluating:  86%|████████▌ | 94/109 [00:43<00:07,  1.92it/s]11/28/2021 01:29:38 - INFO - __main__ -   Batch number = 95
Evaluating:  87%|████████▋ | 95/109 [00:44<00:07,  1.81it/s]11/28/2021 01:29:39 - INFO - __main__ -   Batch number = 96
Evaluating:  88%|████████▊ | 96/109 [00:44<00:07,  1.73it/s]11/28/2021 01:29:39 - INFO - __main__ -   Batch number = 97
Evaluating:  89%|████████▉ | 97/109 [00:45<00:07,  1.59it/s]11/28/2021 01:29:40 - INFO - __main__ -   Batch number = 98
Evaluating:  90%|████████▉ | 98/109 [00:46<00:06,  1.61it/s]11/28/2021 01:29:41 - INFO - __main__ -   Batch number = 99
Evaluating:  91%|█████████ | 99/109 [00:46<00:06,  1.62it/s]11/28/2021 01:29:41 - INFO - __main__ -   Batch number = 100
Evaluating:  92%|█████████▏| 100/109 [00:47<00:05,  1.62it/s]11/28/2021 01:29:42 - INFO - __main__ -   Batch number = 101
Evaluating:  93%|█████████▎| 101/109 [00:47<00:04,  1.61it/s]11/28/2021 01:29:43 - INFO - __main__ -   Batch number = 102
Evaluating:  94%|█████████▎| 102/109 [00:48<00:04,  1.59it/s]11/28/2021 01:29:43 - INFO - __main__ -   Batch number = 103
Evaluating:  94%|█████████▍| 103/109 [00:49<00:03,  1.60it/s]11/28/2021 01:29:44 - INFO - __main__ -   Batch number = 104
Evaluating:  95%|█████████▌| 104/109 [00:49<00:03,  1.61it/s]11/28/2021 01:29:44 - INFO - __main__ -   Batch number = 105
Evaluating:  96%|█████████▋| 105/109 [00:50<00:02,  1.58it/s]11/28/2021 01:29:45 - INFO - __main__ -   Batch number = 106
Evaluating:  97%|█████████▋| 106/109 [00:51<00:01,  1.59it/s]11/28/2021 01:29:46 - INFO - __main__ -   Batch number = 107
Evaluating:  98%|█████████▊| 107/109 [00:51<00:01,  1.60it/s]11/28/2021 01:29:46 - INFO - __main__ -   Batch number = 108
Evaluating:  99%|█████████▉| 108/109 [00:52<00:00,  1.60it/s]11/28/2021 01:29:47 - INFO - __main__ -   Batch number = 109
Evaluating: 100%|██████████| 109/109 [00:52<00:00,  2.02it/s]Evaluating: 100%|██████████| 109/109 [00:52<00:00,  2.08it/s]
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:29:49 - INFO - __main__ -   ***** Evaluation result  in zh *****
11/28/2021 01:29:49 - INFO - __main__ -     f1 = 0.6246939489331933
11/28/2021 01:29:49 - INFO - __main__ -     loss = 1.4302742169537674
11/28/2021 01:29:49 - INFO - __main__ -     precision = 0.6310496656942507
11/28/2021 01:29:49 - INFO - __main__ -     recall = 0.6184649808211393
55.71user 20.43system 1:16.91elapsed 99%CPU (0avgtext+0avgdata 3938616maxresident)k
0inputs+752outputs (0major+1497401minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 16:17:21 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='zh', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 16:17:21 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 16:17:21 - INFO - __main__ -   Seed = 1
11/28/2021 16:17:21 - INFO - root -   save model
11/28/2021 16:17:21 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='zh', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 16:17:21 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 16:17:24 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 16:17:29 - INFO - __main__ -   Using lang2id = None
11/28/2021 16:17:29 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 16:17:29 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
11/28/2021 16:17:29 - INFO - root -   Trying to decide if add adapter
11/28/2021 16:17:29 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 16:17:29 - INFO - root -   loading lang adpater am/wiki@ukp
11/28/2021 16:17:29 - INFO - __main__ -   Adapter Languages : ['am'], Length : 1
11/28/2021 16:17:29 - INFO - __main__ -   Adapter Names ['am/wiki@ukp'], Length : 1
11/28/2021 16:17:29 - INFO - __main__ -   Language = am
11/28/2021 16:17:29 - INFO - __main__ -   Adapter Name = am/wiki@ukp
Found matching adapter at: adapters/ukp/bert-base-multilingual-cased_am_wiki_pfeiffer.json
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/am/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_am_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/adapter_config.json
Adding adapter 'am' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/head_config.json
11/28/2021 16:17:38 - INFO - __main__ -   Language adapter for zh not found, using am instead
11/28/2021 16:17:38 - INFO - __main__ -   Set active language adapter to am
11/28/2021 16:17:38 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 16:17:38 - INFO - __main__ -   Adapter Languages = ['am']
11/28/2021 16:17:38 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_zh_bert-base-multilingual-cased_128
11/28/2021 16:17:39 - INFO - __main__ -   ***** Running evaluation  in zh *****
11/28/2021 16:17:39 - INFO - __main__ -     Num examples = 3458
11/28/2021 16:17:39 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/109 [00:00<?, ?it/s]11/28/2021 16:17:39 - INFO - __main__ -   Batch number = 1
Evaluating:   1%|          | 1/109 [00:00<00:33,  3.19it/s]11/28/2021 16:17:39 - INFO - __main__ -   Batch number = 2
Evaluating:   2%|▏         | 2/109 [00:00<00:33,  3.24it/s]11/28/2021 16:17:39 - INFO - __main__ -   Batch number = 3
Evaluating:   3%|▎         | 3/109 [00:00<00:32,  3.26it/s]11/28/2021 16:17:40 - INFO - __main__ -   Batch number = 4
Evaluating:   4%|▎         | 4/109 [00:01<00:32,  3.27it/s]11/28/2021 16:17:40 - INFO - __main__ -   Batch number = 5
Evaluating:   5%|▍         | 5/109 [00:01<00:31,  3.31it/s]11/28/2021 16:17:40 - INFO - __main__ -   Batch number = 6
Evaluating:   6%|▌         | 6/109 [00:01<00:31,  3.28it/s]11/28/2021 16:17:41 - INFO - __main__ -   Batch number = 7
Evaluating:   6%|▋         | 7/109 [00:02<00:30,  3.37it/s]11/28/2021 16:17:41 - INFO - __main__ -   Batch number = 8
Evaluating:   7%|▋         | 8/109 [00:02<00:30,  3.29it/s]11/28/2021 16:17:41 - INFO - __main__ -   Batch number = 9
Evaluating:   8%|▊         | 9/109 [00:02<00:30,  3.33it/s]11/28/2021 16:17:41 - INFO - __main__ -   Batch number = 10
Evaluating:   9%|▉         | 10/109 [00:03<00:29,  3.33it/s]11/28/2021 16:17:42 - INFO - __main__ -   Batch number = 11
Evaluating:  10%|█         | 11/109 [00:03<00:33,  2.90it/s]11/28/2021 16:17:42 - INFO - __main__ -   Batch number = 12
Evaluating:  11%|█         | 12/109 [00:03<00:36,  2.63it/s]11/28/2021 16:17:43 - INFO - __main__ -   Batch number = 13
Evaluating:  12%|█▏        | 13/109 [00:04<00:33,  2.90it/s]11/28/2021 16:17:43 - INFO - __main__ -   Batch number = 14
Evaluating:  13%|█▎        | 14/109 [00:04<00:36,  2.58it/s]11/28/2021 16:17:43 - INFO - __main__ -   Batch number = 15
Evaluating:  14%|█▍        | 15/109 [00:05<00:38,  2.42it/s]11/28/2021 16:17:44 - INFO - __main__ -   Batch number = 16
Evaluating:  15%|█▍        | 16/109 [00:05<00:39,  2.34it/s]11/28/2021 16:17:44 - INFO - __main__ -   Batch number = 17
Evaluating:  16%|█▌        | 17/109 [00:06<00:40,  2.29it/s]11/28/2021 16:17:45 - INFO - __main__ -   Batch number = 18
Evaluating:  17%|█▋        | 18/109 [00:06<00:40,  2.25it/s]11/28/2021 16:17:45 - INFO - __main__ -   Batch number = 19
Evaluating:  17%|█▋        | 19/109 [00:06<00:40,  2.23it/s]11/28/2021 16:17:46 - INFO - __main__ -   Batch number = 20
Evaluating:  18%|█▊        | 20/109 [00:07<00:40,  2.21it/s]11/28/2021 16:17:46 - INFO - __main__ -   Batch number = 21
Evaluating:  19%|█▉        | 21/109 [00:07<00:40,  2.20it/s]11/28/2021 16:17:47 - INFO - __main__ -   Batch number = 22
Evaluating:  20%|██        | 22/109 [00:08<00:39,  2.19it/s]11/28/2021 16:17:47 - INFO - __main__ -   Batch number = 23
Evaluating:  21%|██        | 23/109 [00:08<00:39,  2.18it/s]11/28/2021 16:17:48 - INFO - __main__ -   Batch number = 24
Evaluating:  22%|██▏       | 24/109 [00:09<00:39,  2.18it/s]11/28/2021 16:17:48 - INFO - __main__ -   Batch number = 25
Evaluating:  23%|██▎       | 25/109 [00:09<00:38,  2.18it/s]11/28/2021 16:17:49 - INFO - __main__ -   Batch number = 26
Evaluating:  24%|██▍       | 26/109 [00:10<00:38,  2.15it/s]11/28/2021 16:17:49 - INFO - __main__ -   Batch number = 27
Evaluating:  25%|██▍       | 27/109 [00:10<00:38,  2.16it/s]11/28/2021 16:17:49 - INFO - __main__ -   Batch number = 28
Evaluating:  26%|██▌       | 28/109 [00:11<00:37,  2.17it/s]11/28/2021 16:17:50 - INFO - __main__ -   Batch number = 29
Evaluating:  27%|██▋       | 29/109 [00:11<00:36,  2.18it/s]11/28/2021 16:17:50 - INFO - __main__ -   Batch number = 30
Evaluating:  28%|██▊       | 30/109 [00:12<00:36,  2.17it/s]11/28/2021 16:17:51 - INFO - __main__ -   Batch number = 31
Evaluating:  28%|██▊       | 31/109 [00:12<00:36,  2.15it/s]11/28/2021 16:17:51 - INFO - __main__ -   Batch number = 32
Evaluating:  29%|██▉       | 32/109 [00:13<00:35,  2.15it/s]11/28/2021 16:17:52 - INFO - __main__ -   Batch number = 33
Evaluating:  30%|███       | 33/109 [00:13<00:35,  2.16it/s]11/28/2021 16:17:52 - INFO - __main__ -   Batch number = 34
Evaluating:  31%|███       | 34/109 [00:13<00:34,  2.16it/s]11/28/2021 16:17:53 - INFO - __main__ -   Batch number = 35
Evaluating:  32%|███▏      | 35/109 [00:14<00:34,  2.17it/s]11/28/2021 16:17:53 - INFO - __main__ -   Batch number = 36
Evaluating:  33%|███▎      | 36/109 [00:14<00:33,  2.17it/s]11/28/2021 16:17:54 - INFO - __main__ -   Batch number = 37
Evaluating:  34%|███▍      | 37/109 [00:15<00:33,  2.17it/s]11/28/2021 16:17:54 - INFO - __main__ -   Batch number = 38
Evaluating:  35%|███▍      | 38/109 [00:15<00:33,  2.14it/s]11/28/2021 16:17:55 - INFO - __main__ -   Batch number = 39
Evaluating:  36%|███▌      | 39/109 [00:16<00:32,  2.14it/s]11/28/2021 16:17:55 - INFO - __main__ -   Batch number = 40
Evaluating:  37%|███▋      | 40/109 [00:16<00:32,  2.15it/s]11/28/2021 16:17:55 - INFO - __main__ -   Batch number = 41
Evaluating:  38%|███▊      | 41/109 [00:17<00:31,  2.15it/s]11/28/2021 16:17:56 - INFO - __main__ -   Batch number = 42
Evaluating:  39%|███▊      | 42/109 [00:17<00:31,  2.16it/s]11/28/2021 16:17:56 - INFO - __main__ -   Batch number = 43
Evaluating:  39%|███▉      | 43/109 [00:18<00:30,  2.17it/s]11/28/2021 16:17:57 - INFO - __main__ -   Batch number = 44
Evaluating:  40%|████      | 44/109 [00:18<00:29,  2.18it/s]11/28/2021 16:17:57 - INFO - __main__ -   Batch number = 45
Evaluating:  41%|████▏     | 45/109 [00:19<00:29,  2.18it/s]11/28/2021 16:17:58 - INFO - __main__ -   Batch number = 46
Evaluating:  42%|████▏     | 46/109 [00:19<00:30,  2.08it/s]11/28/2021 16:17:58 - INFO - __main__ -   Batch number = 47
Evaluating:  43%|████▎     | 47/109 [00:20<00:29,  2.11it/s]11/28/2021 16:17:59 - INFO - __main__ -   Batch number = 48
Evaluating:  44%|████▍     | 48/109 [00:20<00:28,  2.12it/s]11/28/2021 16:17:59 - INFO - __main__ -   Batch number = 49
Evaluating:  45%|████▍     | 49/109 [00:20<00:28,  2.14it/s]11/28/2021 16:18:00 - INFO - __main__ -   Batch number = 50
Evaluating:  46%|████▌     | 50/109 [00:21<00:27,  2.15it/s]11/28/2021 16:18:00 - INFO - __main__ -   Batch number = 51
Evaluating:  47%|████▋     | 51/109 [00:21<00:27,  2.14it/s]11/28/2021 16:18:01 - INFO - __main__ -   Batch number = 52
Evaluating:  48%|████▊     | 52/109 [00:22<00:26,  2.12it/s]11/28/2021 16:18:01 - INFO - __main__ -   Batch number = 53
Evaluating:  49%|████▊     | 53/109 [00:22<00:26,  2.15it/s]11/28/2021 16:18:02 - INFO - __main__ -   Batch number = 54
Evaluating:  50%|████▉     | 54/109 [00:23<00:25,  2.17it/s]11/28/2021 16:18:02 - INFO - __main__ -   Batch number = 55
Evaluating:  50%|█████     | 55/109 [00:23<00:24,  2.17it/s]11/28/2021 16:18:02 - INFO - __main__ -   Batch number = 56
Evaluating:  51%|█████▏    | 56/109 [00:24<00:24,  2.17it/s]11/28/2021 16:18:03 - INFO - __main__ -   Batch number = 57
Evaluating:  52%|█████▏    | 57/109 [00:24<00:25,  2.05it/s]11/28/2021 16:18:03 - INFO - __main__ -   Batch number = 58
Evaluating:  53%|█████▎    | 58/109 [00:25<00:24,  2.08it/s]11/28/2021 16:18:04 - INFO - __main__ -   Batch number = 59
Evaluating:  54%|█████▍    | 59/109 [00:25<00:23,  2.10it/s]11/28/2021 16:18:04 - INFO - __main__ -   Batch number = 60
Evaluating:  55%|█████▌    | 60/109 [00:26<00:23,  2.10it/s]11/28/2021 16:18:05 - INFO - __main__ -   Batch number = 61
Evaluating:  56%|█████▌    | 61/109 [00:26<00:22,  2.11it/s]11/28/2021 16:18:05 - INFO - __main__ -   Batch number = 62
Evaluating:  57%|█████▋    | 62/109 [00:27<00:22,  2.13it/s]11/28/2021 16:18:06 - INFO - __main__ -   Batch number = 63
Evaluating:  58%|█████▊    | 63/109 [00:27<00:21,  2.14it/s]11/28/2021 16:18:06 - INFO - __main__ -   Batch number = 64
Evaluating:  59%|█████▊    | 64/109 [00:27<00:20,  2.15it/s]11/28/2021 16:18:07 - INFO - __main__ -   Batch number = 65
Evaluating:  60%|█████▉    | 65/109 [00:28<00:20,  2.15it/s]11/28/2021 16:18:07 - INFO - __main__ -   Batch number = 66
Evaluating:  61%|██████    | 66/109 [00:28<00:19,  2.16it/s]11/28/2021 16:18:08 - INFO - __main__ -   Batch number = 67
Evaluating:  61%|██████▏   | 67/109 [00:29<00:19,  2.15it/s]11/28/2021 16:18:08 - INFO - __main__ -   Batch number = 68
Evaluating:  62%|██████▏   | 68/109 [00:29<00:19,  2.15it/s]11/28/2021 16:18:09 - INFO - __main__ -   Batch number = 69
Evaluating:  63%|██████▎   | 69/109 [00:30<00:18,  2.17it/s]11/28/2021 16:18:09 - INFO - __main__ -   Batch number = 70
Evaluating:  64%|██████▍   | 70/109 [00:30<00:17,  2.17it/s]11/28/2021 16:18:09 - INFO - __main__ -   Batch number = 71
Evaluating:  65%|██████▌   | 71/109 [00:31<00:17,  2.17it/s]11/28/2021 16:18:10 - INFO - __main__ -   Batch number = 72
Evaluating:  66%|██████▌   | 72/109 [00:31<00:17,  2.16it/s]11/28/2021 16:18:10 - INFO - __main__ -   Batch number = 73
Evaluating:  67%|██████▋   | 73/109 [00:32<00:16,  2.15it/s]11/28/2021 16:18:11 - INFO - __main__ -   Batch number = 74
Evaluating:  68%|██████▊   | 74/109 [00:32<00:16,  2.15it/s]11/28/2021 16:18:11 - INFO - __main__ -   Batch number = 75
Evaluating:  69%|██████▉   | 75/109 [00:33<00:15,  2.14it/s]11/28/2021 16:18:12 - INFO - __main__ -   Batch number = 76
Evaluating:  70%|██████▉   | 76/109 [00:33<00:15,  2.14it/s]11/28/2021 16:18:12 - INFO - __main__ -   Batch number = 77
Evaluating:  71%|███████   | 77/109 [00:33<00:14,  2.16it/s]11/28/2021 16:18:13 - INFO - __main__ -   Batch number = 78
Evaluating:  72%|███████▏  | 78/109 [00:34<00:16,  1.89it/s]11/28/2021 16:18:13 - INFO - __main__ -   Batch number = 79
Evaluating:  72%|███████▏  | 79/109 [00:35<00:15,  1.99it/s]11/28/2021 16:18:14 - INFO - __main__ -   Batch number = 80
Evaluating:  73%|███████▎  | 80/109 [00:35<00:14,  2.04it/s]11/28/2021 16:18:14 - INFO - __main__ -   Batch number = 81
Evaluating:  74%|███████▍  | 81/109 [00:36<00:13,  2.07it/s]11/28/2021 16:18:15 - INFO - __main__ -   Batch number = 82
Evaluating:  75%|███████▌  | 82/109 [00:36<00:12,  2.11it/s]11/28/2021 16:18:15 - INFO - __main__ -   Batch number = 83
Evaluating:  76%|███████▌  | 83/109 [00:36<00:12,  2.13it/s]11/28/2021 16:18:16 - INFO - __main__ -   Batch number = 84
Evaluating:  77%|███████▋  | 84/109 [00:37<00:11,  2.14it/s]11/28/2021 16:18:16 - INFO - __main__ -   Batch number = 85
Evaluating:  78%|███████▊  | 85/109 [00:37<00:11,  2.14it/s]11/28/2021 16:18:17 - INFO - __main__ -   Batch number = 86
Evaluating:  79%|███████▉  | 86/109 [00:38<00:10,  2.16it/s]11/28/2021 16:18:17 - INFO - __main__ -   Batch number = 87
Evaluating:  80%|███████▉  | 87/109 [00:38<00:10,  2.16it/s]11/28/2021 16:18:18 - INFO - __main__ -   Batch number = 88
Evaluating:  81%|████████  | 88/109 [00:39<00:09,  2.17it/s]11/28/2021 16:18:18 - INFO - __main__ -   Batch number = 89
Evaluating:  82%|████████▏ | 89/109 [00:39<00:09,  2.16it/s]11/28/2021 16:18:18 - INFO - __main__ -   Batch number = 90
Evaluating:  83%|████████▎ | 90/109 [00:40<00:08,  2.16it/s]11/28/2021 16:18:19 - INFO - __main__ -   Batch number = 91
Evaluating:  83%|████████▎ | 91/109 [00:40<00:08,  2.16it/s]11/28/2021 16:18:19 - INFO - __main__ -   Batch number = 92
Evaluating:  84%|████████▍ | 92/109 [00:41<00:07,  2.14it/s]11/28/2021 16:18:20 - INFO - __main__ -   Batch number = 93
Evaluating:  85%|████████▌ | 93/109 [00:41<00:07,  2.15it/s]11/28/2021 16:18:20 - INFO - __main__ -   Batch number = 94
Evaluating:  86%|████████▌ | 94/109 [00:42<00:07,  2.14it/s]11/28/2021 16:18:21 - INFO - __main__ -   Batch number = 95
Evaluating:  87%|████████▋ | 95/109 [00:42<00:06,  2.15it/s]11/28/2021 16:18:21 - INFO - __main__ -   Batch number = 96
Evaluating:  88%|████████▊ | 96/109 [00:42<00:06,  2.16it/s]11/28/2021 16:18:22 - INFO - __main__ -   Batch number = 97
Evaluating:  89%|████████▉ | 97/109 [00:43<00:05,  2.16it/s]11/28/2021 16:18:22 - INFO - __main__ -   Batch number = 98
Evaluating:  90%|████████▉ | 98/109 [00:43<00:05,  2.17it/s]11/28/2021 16:18:23 - INFO - __main__ -   Batch number = 99
Evaluating:  91%|█████████ | 99/109 [00:44<00:04,  2.35it/s]11/28/2021 16:18:23 - INFO - __main__ -   Batch number = 100
Evaluating:  92%|█████████▏| 100/109 [00:44<00:03,  2.28it/s]11/28/2021 16:18:23 - INFO - __main__ -   Batch number = 101
Evaluating:  93%|█████████▎| 101/109 [00:45<00:03,  2.23it/s]11/28/2021 16:18:24 - INFO - __main__ -   Batch number = 102
Evaluating:  94%|█████████▎| 102/109 [00:45<00:03,  2.21it/s]11/28/2021 16:18:24 - INFO - __main__ -   Batch number = 103
Evaluating:  94%|█████████▍| 103/109 [00:46<00:02,  2.20it/s]11/28/2021 16:18:25 - INFO - __main__ -   Batch number = 104
Evaluating:  95%|█████████▌| 104/109 [00:46<00:02,  2.19it/s]11/28/2021 16:18:25 - INFO - __main__ -   Batch number = 105
Evaluating:  96%|█████████▋| 105/109 [00:47<00:01,  2.20it/s]11/28/2021 16:18:26 - INFO - __main__ -   Batch number = 106
Evaluating:  97%|█████████▋| 106/109 [00:47<00:01,  2.19it/s]11/28/2021 16:18:26 - INFO - __main__ -   Batch number = 107
Evaluating:  98%|█████████▊| 107/109 [00:47<00:00,  2.20it/s]11/28/2021 16:18:27 - INFO - __main__ -   Batch number = 108
Evaluating:  99%|█████████▉| 108/109 [00:48<00:00,  2.19it/s]11/28/2021 16:18:27 - INFO - __main__ -   Batch number = 109
Evaluating: 100%|██████████| 109/109 [00:48<00:00,  2.86it/s]Evaluating: 100%|██████████| 109/109 [00:48<00:00,  2.25it/s]
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 16:18:29 - INFO - __main__ -   ***** Evaluation result  in zh *****
11/28/2021 16:18:29 - INFO - __main__ -     f1 = 0.6206086262410648
11/28/2021 16:18:29 - INFO - __main__ -     loss = 1.4001762221712586
11/28/2021 16:18:29 - INFO - __main__ -     precision = 0.6269457280057987
11/28/2021 16:18:29 - INFO - __main__ -     recall = 0.6143983520386419
52.07user 18.56system 1:10.34elapsed 100%CPU (0avgtext+0avgdata 3932320maxresident)k
8inputs+776outputs (0major+1781612minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 16:18:32 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='zh', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 16:18:32 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 16:18:32 - INFO - __main__ -   Seed = 2
11/28/2021 16:18:32 - INFO - root -   save model
11/28/2021 16:18:32 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='zh', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 16:18:32 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 16:18:34 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 16:18:40 - INFO - __main__ -   Using lang2id = None
11/28/2021 16:18:40 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 16:18:40 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
11/28/2021 16:18:40 - INFO - root -   Trying to decide if add adapter
11/28/2021 16:18:40 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 16:18:40 - INFO - root -   loading lang adpater am/wiki@ukp
11/28/2021 16:18:40 - INFO - __main__ -   Adapter Languages : ['am'], Length : 1
11/28/2021 16:18:40 - INFO - __main__ -   Adapter Names ['am/wiki@ukp'], Length : 1
11/28/2021 16:18:40 - INFO - __main__ -   Language = am
11/28/2021 16:18:40 - INFO - __main__ -   Adapter Name = am/wiki@ukp
Found matching adapter at: adapters/ukp/bert-base-multilingual-cased_am_wiki_pfeiffer.json
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/am/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_am_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/adapter_config.json
Adding adapter 'am' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/head_config.json
11/28/2021 16:18:48 - INFO - __main__ -   Language adapter for zh not found, using am instead
11/28/2021 16:18:48 - INFO - __main__ -   Set active language adapter to am
11/28/2021 16:18:48 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 16:18:48 - INFO - __main__ -   Adapter Languages = ['am']
11/28/2021 16:18:48 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_zh_bert-base-multilingual-cased_128
11/28/2021 16:18:49 - INFO - __main__ -   ***** Running evaluation  in zh *****
11/28/2021 16:18:49 - INFO - __main__ -     Num examples = 3458
11/28/2021 16:18:49 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/109 [00:00<?, ?it/s]11/28/2021 16:18:49 - INFO - __main__ -   Batch number = 1
Evaluating:   1%|          | 1/109 [00:00<00:16,  6.39it/s]11/28/2021 16:18:49 - INFO - __main__ -   Batch number = 2
Evaluating:   2%|▏         | 2/109 [00:00<00:16,  6.62it/s]11/28/2021 16:18:49 - INFO - __main__ -   Batch number = 3
Evaluating:   3%|▎         | 3/109 [00:00<00:15,  6.65it/s]11/28/2021 16:18:49 - INFO - __main__ -   Batch number = 4
Evaluating:   4%|▎         | 4/109 [00:00<00:15,  6.75it/s]11/28/2021 16:18:50 - INFO - __main__ -   Batch number = 5
Evaluating:   5%|▍         | 5/109 [00:00<00:15,  6.76it/s]11/28/2021 16:18:50 - INFO - __main__ -   Batch number = 6
Evaluating:   6%|▌         | 6/109 [00:00<00:15,  6.72it/s]11/28/2021 16:18:50 - INFO - __main__ -   Batch number = 7
Evaluating:   6%|▋         | 7/109 [00:01<00:15,  6.62it/s]11/28/2021 16:18:50 - INFO - __main__ -   Batch number = 8
Evaluating:   7%|▋         | 8/109 [00:01<00:15,  6.58it/s]11/28/2021 16:18:50 - INFO - __main__ -   Batch number = 9
Evaluating:   8%|▊         | 9/109 [00:01<00:15,  6.54it/s]11/28/2021 16:18:50 - INFO - __main__ -   Batch number = 10
Evaluating:   9%|▉         | 10/109 [00:01<00:14,  6.64it/s]11/28/2021 16:18:50 - INFO - __main__ -   Batch number = 11
Evaluating:  10%|█         | 11/109 [00:01<00:14,  6.70it/s]11/28/2021 16:18:51 - INFO - __main__ -   Batch number = 12
Evaluating:  11%|█         | 12/109 [00:01<00:15,  6.37it/s]11/28/2021 16:18:51 - INFO - __main__ -   Batch number = 13
Evaluating:  12%|█▏        | 13/109 [00:01<00:14,  6.67it/s]11/28/2021 16:18:51 - INFO - __main__ -   Batch number = 14
Evaluating:  13%|█▎        | 14/109 [00:02<00:13,  6.86it/s]11/28/2021 16:18:51 - INFO - __main__ -   Batch number = 15
Evaluating:  14%|█▍        | 15/109 [00:02<00:13,  7.01it/s]11/28/2021 16:18:51 - INFO - __main__ -   Batch number = 16
Evaluating:  15%|█▍        | 16/109 [00:02<00:13,  7.12it/s]11/28/2021 16:18:51 - INFO - __main__ -   Batch number = 17
Evaluating:  16%|█▌        | 17/109 [00:02<00:16,  5.53it/s]11/28/2021 16:18:52 - INFO - __main__ -   Batch number = 18
Evaluating:  17%|█▋        | 18/109 [00:02<00:19,  4.66it/s]11/28/2021 16:18:52 - INFO - __main__ -   Batch number = 19
Evaluating:  17%|█▋        | 19/109 [00:03<00:21,  4.15it/s]11/28/2021 16:18:52 - INFO - __main__ -   Batch number = 20
Evaluating:  18%|█▊        | 20/109 [00:03<00:23,  3.82it/s]11/28/2021 16:18:52 - INFO - __main__ -   Batch number = 21
Evaluating:  19%|█▉        | 21/109 [00:03<00:24,  3.61it/s]11/28/2021 16:18:53 - INFO - __main__ -   Batch number = 22
Evaluating:  20%|██        | 22/109 [00:04<00:24,  3.48it/s]11/28/2021 16:18:53 - INFO - __main__ -   Batch number = 23
Evaluating:  21%|██        | 23/109 [00:04<00:25,  3.40it/s]11/28/2021 16:18:53 - INFO - __main__ -   Batch number = 24
Evaluating:  22%|██▏       | 24/109 [00:04<00:25,  3.37it/s]11/28/2021 16:18:54 - INFO - __main__ -   Batch number = 25
Evaluating:  23%|██▎       | 25/109 [00:05<00:25,  3.34it/s]11/28/2021 16:18:54 - INFO - __main__ -   Batch number = 26
Evaluating:  24%|██▍       | 26/109 [00:05<00:25,  3.31it/s]11/28/2021 16:18:54 - INFO - __main__ -   Batch number = 27
Evaluating:  25%|██▍       | 27/109 [00:05<00:24,  3.30it/s]11/28/2021 16:18:55 - INFO - __main__ -   Batch number = 28
Evaluating:  26%|██▌       | 28/109 [00:06<00:24,  3.29it/s]11/28/2021 16:18:55 - INFO - __main__ -   Batch number = 29
Evaluating:  27%|██▋       | 29/109 [00:06<00:24,  3.28it/s]11/28/2021 16:18:55 - INFO - __main__ -   Batch number = 30
Evaluating:  28%|██▊       | 30/109 [00:06<00:24,  3.25it/s]11/28/2021 16:18:56 - INFO - __main__ -   Batch number = 31
Evaluating:  28%|██▊       | 31/109 [00:07<00:26,  2.94it/s]11/28/2021 16:18:56 - INFO - __main__ -   Batch number = 32
Evaluating:  29%|██▉       | 32/109 [00:07<00:25,  3.04it/s]11/28/2021 16:18:56 - INFO - __main__ -   Batch number = 33
Evaluating:  30%|███       | 33/109 [00:07<00:24,  3.11it/s]11/28/2021 16:18:57 - INFO - __main__ -   Batch number = 34
Evaluating:  31%|███       | 34/109 [00:07<00:23,  3.14it/s]11/28/2021 16:18:57 - INFO - __main__ -   Batch number = 35
Evaluating:  32%|███▏      | 35/109 [00:08<00:23,  3.17it/s]11/28/2021 16:18:57 - INFO - __main__ -   Batch number = 36
Evaluating:  33%|███▎      | 36/109 [00:08<00:22,  3.20it/s]11/28/2021 16:18:58 - INFO - __main__ -   Batch number = 37
Evaluating:  34%|███▍      | 37/109 [00:08<00:22,  3.21it/s]11/28/2021 16:18:58 - INFO - __main__ -   Batch number = 38
Evaluating:  35%|███▍      | 38/109 [00:09<00:21,  3.24it/s]11/28/2021 16:18:58 - INFO - __main__ -   Batch number = 39
Evaluating:  36%|███▌      | 39/109 [00:09<00:21,  3.24it/s]11/28/2021 16:18:58 - INFO - __main__ -   Batch number = 40
Evaluating:  37%|███▋      | 40/109 [00:09<00:21,  3.23it/s]11/28/2021 16:18:59 - INFO - __main__ -   Batch number = 41
Evaluating:  38%|███▊      | 41/109 [00:10<00:20,  3.24it/s]11/28/2021 16:18:59 - INFO - __main__ -   Batch number = 42
Evaluating:  39%|███▊      | 42/109 [00:10<00:20,  3.23it/s]11/28/2021 16:18:59 - INFO - __main__ -   Batch number = 43
Evaluating:  39%|███▉      | 43/109 [00:10<00:20,  3.23it/s]11/28/2021 16:19:00 - INFO - __main__ -   Batch number = 44
Evaluating:  40%|████      | 44/109 [00:11<00:20,  3.23it/s]11/28/2021 16:19:00 - INFO - __main__ -   Batch number = 45
Evaluating:  41%|████▏     | 45/109 [00:11<00:19,  3.24it/s]11/28/2021 16:19:00 - INFO - __main__ -   Batch number = 46
Evaluating:  42%|████▏     | 46/109 [00:11<00:19,  3.19it/s]11/28/2021 16:19:01 - INFO - __main__ -   Batch number = 47
Evaluating:  43%|████▎     | 47/109 [00:12<00:19,  3.12it/s]11/28/2021 16:19:01 - INFO - __main__ -   Batch number = 48
Evaluating:  44%|████▍     | 48/109 [00:12<00:19,  3.18it/s]11/28/2021 16:19:01 - INFO - __main__ -   Batch number = 49
Evaluating:  45%|████▍     | 49/109 [00:12<00:18,  3.16it/s]11/28/2021 16:19:02 - INFO - __main__ -   Batch number = 50
Evaluating:  46%|████▌     | 50/109 [00:12<00:18,  3.17it/s]11/28/2021 16:19:02 - INFO - __main__ -   Batch number = 51
Evaluating:  47%|████▋     | 51/109 [00:13<00:18,  3.19it/s]11/28/2021 16:19:02 - INFO - __main__ -   Batch number = 52
Evaluating:  48%|████▊     | 52/109 [00:13<00:17,  3.19it/s]11/28/2021 16:19:03 - INFO - __main__ -   Batch number = 53
Evaluating:  49%|████▊     | 53/109 [00:13<00:17,  3.19it/s]11/28/2021 16:19:03 - INFO - __main__ -   Batch number = 54
Evaluating:  50%|████▉     | 54/109 [00:14<00:17,  3.23it/s]11/28/2021 16:19:03 - INFO - __main__ -   Batch number = 55
Evaluating:  50%|█████     | 55/109 [00:14<00:16,  3.26it/s]11/28/2021 16:19:03 - INFO - __main__ -   Batch number = 56
Evaluating:  51%|█████▏    | 56/109 [00:14<00:16,  3.23it/s]11/28/2021 16:19:04 - INFO - __main__ -   Batch number = 57
Evaluating:  52%|█████▏    | 57/109 [00:15<00:16,  3.21it/s]11/28/2021 16:19:04 - INFO - __main__ -   Batch number = 58
Evaluating:  53%|█████▎    | 58/109 [00:15<00:15,  3.20it/s]11/28/2021 16:19:04 - INFO - __main__ -   Batch number = 59
Evaluating:  54%|█████▍    | 59/109 [00:15<00:15,  3.19it/s]11/28/2021 16:19:05 - INFO - __main__ -   Batch number = 60
Evaluating:  55%|█████▌    | 60/109 [00:16<00:15,  3.19it/s]11/28/2021 16:19:05 - INFO - __main__ -   Batch number = 61
Evaluating:  56%|█████▌    | 61/109 [00:16<00:15,  3.19it/s]11/28/2021 16:19:05 - INFO - __main__ -   Batch number = 62
Evaluating:  57%|█████▋    | 62/109 [00:16<00:14,  3.24it/s]11/28/2021 16:19:06 - INFO - __main__ -   Batch number = 63
Evaluating:  58%|█████▊    | 63/109 [00:16<00:14,  3.21it/s]11/28/2021 16:19:06 - INFO - __main__ -   Batch number = 64
Evaluating:  59%|█████▊    | 64/109 [00:17<00:13,  3.26it/s]11/28/2021 16:19:06 - INFO - __main__ -   Batch number = 65
Evaluating:  60%|█████▉    | 65/109 [00:17<00:13,  3.22it/s]11/28/2021 16:19:07 - INFO - __main__ -   Batch number = 66
Evaluating:  61%|██████    | 66/109 [00:17<00:13,  3.21it/s]11/28/2021 16:19:07 - INFO - __main__ -   Batch number = 67
Evaluating:  61%|██████▏   | 67/109 [00:18<00:13,  3.21it/s]11/28/2021 16:19:07 - INFO - __main__ -   Batch number = 68
Evaluating:  62%|██████▏   | 68/109 [00:18<00:12,  3.22it/s]11/28/2021 16:19:07 - INFO - __main__ -   Batch number = 69
Evaluating:  63%|██████▎   | 69/109 [00:18<00:12,  3.23it/s]11/28/2021 16:19:08 - INFO - __main__ -   Batch number = 70
Evaluating:  64%|██████▍   | 70/109 [00:19<00:12,  3.23it/s]11/28/2021 16:19:08 - INFO - __main__ -   Batch number = 71
Evaluating:  65%|██████▌   | 71/109 [00:19<00:11,  3.23it/s]11/28/2021 16:19:08 - INFO - __main__ -   Batch number = 72
Evaluating:  66%|██████▌   | 72/109 [00:19<00:11,  3.20it/s]11/28/2021 16:19:09 - INFO - __main__ -   Batch number = 73
Evaluating:  67%|██████▋   | 73/109 [00:20<00:11,  3.19it/s]11/28/2021 16:19:09 - INFO - __main__ -   Batch number = 74
Evaluating:  68%|██████▊   | 74/109 [00:20<00:10,  3.20it/s]11/28/2021 16:19:09 - INFO - __main__ -   Batch number = 75
Evaluating:  69%|██████▉   | 75/109 [00:20<00:10,  3.25it/s]11/28/2021 16:19:10 - INFO - __main__ -   Batch number = 76
Evaluating:  70%|██████▉   | 76/109 [00:21<00:11,  2.81it/s]11/28/2021 16:19:10 - INFO - __main__ -   Batch number = 77
Evaluating:  71%|███████   | 77/109 [00:21<00:12,  2.57it/s]11/28/2021 16:19:11 - INFO - __main__ -   Batch number = 78
Evaluating:  72%|███████▏  | 78/109 [00:22<00:13,  2.36it/s]11/28/2021 16:19:11 - INFO - __main__ -   Batch number = 79
Evaluating:  72%|███████▏  | 79/109 [00:22<00:13,  2.29it/s]11/28/2021 16:19:12 - INFO - __main__ -   Batch number = 80
Evaluating:  73%|███████▎  | 80/109 [00:23<00:12,  2.25it/s]11/28/2021 16:19:12 - INFO - __main__ -   Batch number = 81
Evaluating:  74%|███████▍  | 81/109 [00:23<00:12,  2.23it/s]11/28/2021 16:19:12 - INFO - __main__ -   Batch number = 82
Evaluating:  75%|███████▌  | 82/109 [00:24<00:12,  2.18it/s]11/28/2021 16:19:13 - INFO - __main__ -   Batch number = 83
Evaluating:  76%|███████▌  | 83/109 [00:24<00:12,  2.16it/s]11/28/2021 16:19:13 - INFO - __main__ -   Batch number = 84
Evaluating:  77%|███████▋  | 84/109 [00:24<00:11,  2.14it/s]11/28/2021 16:19:14 - INFO - __main__ -   Batch number = 85
Evaluating:  78%|███████▊  | 85/109 [00:25<00:11,  2.12it/s]11/28/2021 16:19:14 - INFO - __main__ -   Batch number = 86
Evaluating:  79%|███████▉  | 86/109 [00:25<00:10,  2.11it/s]11/28/2021 16:19:15 - INFO - __main__ -   Batch number = 87
Evaluating:  80%|███████▉  | 87/109 [00:26<00:10,  2.12it/s]11/28/2021 16:19:15 - INFO - __main__ -   Batch number = 88
Evaluating:  81%|████████  | 88/109 [00:26<00:09,  2.12it/s]11/28/2021 16:19:16 - INFO - __main__ -   Batch number = 89
Evaluating:  82%|████████▏ | 89/109 [00:27<00:09,  2.15it/s]11/28/2021 16:19:16 - INFO - __main__ -   Batch number = 90
Evaluating:  83%|████████▎ | 90/109 [00:27<00:08,  2.16it/s]11/28/2021 16:19:17 - INFO - __main__ -   Batch number = 91
Evaluating:  83%|████████▎ | 91/109 [00:28<00:08,  2.15it/s]11/28/2021 16:19:17 - INFO - __main__ -   Batch number = 92
Evaluating:  84%|████████▍ | 92/109 [00:28<00:07,  2.15it/s]11/28/2021 16:19:18 - INFO - __main__ -   Batch number = 93
Evaluating:  85%|████████▌ | 93/109 [00:29<00:07,  2.15it/s]11/28/2021 16:19:18 - INFO - __main__ -   Batch number = 94
Evaluating:  86%|████████▌ | 94/109 [00:29<00:06,  2.15it/s]11/28/2021 16:19:19 - INFO - __main__ -   Batch number = 95
Evaluating:  87%|████████▋ | 95/109 [00:30<00:06,  2.15it/s]11/28/2021 16:19:19 - INFO - __main__ -   Batch number = 96
Evaluating:  88%|████████▊ | 96/109 [00:30<00:06,  2.15it/s]11/28/2021 16:19:20 - INFO - __main__ -   Batch number = 97
Evaluating:  89%|████████▉ | 97/109 [00:31<00:05,  2.15it/s]11/28/2021 16:19:20 - INFO - __main__ -   Batch number = 98
Evaluating:  90%|████████▉ | 98/109 [00:31<00:05,  2.15it/s]11/28/2021 16:19:20 - INFO - __main__ -   Batch number = 99
Evaluating:  91%|█████████ | 99/109 [00:31<00:04,  2.20it/s]11/28/2021 16:19:21 - INFO - __main__ -   Batch number = 100
Evaluating:  92%|█████████▏| 100/109 [00:32<00:04,  2.19it/s]11/28/2021 16:19:21 - INFO - __main__ -   Batch number = 101
Evaluating:  93%|█████████▎| 101/109 [00:32<00:03,  2.17it/s]11/28/2021 16:19:22 - INFO - __main__ -   Batch number = 102
Evaluating:  94%|█████████▎| 102/109 [00:33<00:03,  2.14it/s]11/28/2021 16:19:22 - INFO - __main__ -   Batch number = 103
Evaluating:  94%|█████████▍| 103/109 [00:33<00:02,  2.14it/s]11/28/2021 16:19:23 - INFO - __main__ -   Batch number = 104
Evaluating:  95%|█████████▌| 104/109 [00:34<00:02,  2.14it/s]11/28/2021 16:19:23 - INFO - __main__ -   Batch number = 105
Evaluating:  96%|█████████▋| 105/109 [00:34<00:01,  2.14it/s]11/28/2021 16:19:24 - INFO - __main__ -   Batch number = 106
Evaluating:  97%|█████████▋| 106/109 [00:35<00:01,  2.15it/s]11/28/2021 16:19:24 - INFO - __main__ -   Batch number = 107
Evaluating:  98%|█████████▊| 107/109 [00:35<00:00,  2.14it/s]11/28/2021 16:19:25 - INFO - __main__ -   Batch number = 108
Evaluating:  99%|█████████▉| 108/109 [00:36<00:00,  2.15it/s]11/28/2021 16:19:25 - INFO - __main__ -   Batch number = 109
Evaluating: 100%|██████████| 109/109 [00:36<00:00,  2.80it/s]Evaluating: 100%|██████████| 109/109 [00:36<00:00,  3.01it/s]
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 16:19:27 - INFO - __main__ -   ***** Evaluation result  in zh *****
11/28/2021 16:19:27 - INFO - __main__ -     f1 = 0.6285467065652377
11/28/2021 16:19:27 - INFO - __main__ -     loss = 1.3126799775919784
11/28/2021 16:19:27 - INFO - __main__ -     precision = 0.6381872757888571
11/28/2021 16:19:27 - INFO - __main__ -     recall = 0.619193067197045
44.40user 14.70system 0:57.95elapsed 102%CPU (0avgtext+0avgdata 3937992maxresident)k
0inputs+760outputs (0major+1619626minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 16:19:29 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='zh', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 16:19:29 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 16:19:29 - INFO - __main__ -   Seed = 3
11/28/2021 16:19:29 - INFO - root -   save model
11/28/2021 16:19:29 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='zh', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_am//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 16:19:29 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 16:19:32 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 16:19:38 - INFO - __main__ -   Using lang2id = None
11/28/2021 16:19:38 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 16:19:38 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
11/28/2021 16:19:38 - INFO - root -   Trying to decide if add adapter
11/28/2021 16:19:38 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 16:19:38 - INFO - root -   loading lang adpater am/wiki@ukp
11/28/2021 16:19:38 - INFO - __main__ -   Adapter Languages : ['am'], Length : 1
11/28/2021 16:19:38 - INFO - __main__ -   Adapter Names ['am/wiki@ukp'], Length : 1
11/28/2021 16:19:38 - INFO - __main__ -   Language = am
11/28/2021 16:19:38 - INFO - __main__ -   Adapter Name = am/wiki@ukp
Found matching adapter at: adapters/ukp/bert-base-multilingual-cased_am_wiki_pfeiffer.json
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/am/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_am_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/adapter_config.json
Adding adapter 'am' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/8d40e9aeb008a8b24a99bd46c92e538c8c8a31c9a43aeb5a1dc1ca985bb739ba-e113dbeaa603232ffe5d897a971e7f491f72ba6850a9ad2c0242a8fdc941eb52-extracted/head_config.json
11/28/2021 16:19:49 - INFO - __main__ -   Language adapter for zh not found, using am instead
11/28/2021 16:19:49 - INFO - __main__ -   Set active language adapter to am
11/28/2021 16:19:49 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 16:19:49 - INFO - __main__ -   Adapter Languages = ['am']
11/28/2021 16:19:49 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_zh_bert-base-multilingual-cased_128
11/28/2021 16:19:49 - INFO - __main__ -   ***** Running evaluation  in zh *****
11/28/2021 16:19:49 - INFO - __main__ -     Num examples = 3458
11/28/2021 16:19:49 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/109 [00:00<?, ?it/s]11/28/2021 16:19:49 - INFO - __main__ -   Batch number = 1
Evaluating:   1%|          | 1/109 [00:00<00:48,  2.21it/s]11/28/2021 16:19:50 - INFO - __main__ -   Batch number = 2
Evaluating:   2%|▏         | 2/109 [00:00<00:48,  2.22it/s]11/28/2021 16:19:50 - INFO - __main__ -   Batch number = 3
Evaluating:   3%|▎         | 3/109 [00:01<00:48,  2.21it/s]11/28/2021 16:19:51 - INFO - __main__ -   Batch number = 4
Evaluating:   4%|▎         | 4/109 [00:01<00:48,  2.19it/s]11/28/2021 16:19:51 - INFO - __main__ -   Batch number = 5
Evaluating:   5%|▍         | 5/109 [00:02<00:47,  2.20it/s]11/28/2021 16:19:52 - INFO - __main__ -   Batch number = 6
Evaluating:   6%|▌         | 6/109 [00:02<00:46,  2.20it/s]11/28/2021 16:19:52 - INFO - __main__ -   Batch number = 7
Evaluating:   6%|▋         | 7/109 [00:03<00:46,  2.20it/s]11/28/2021 16:19:53 - INFO - __main__ -   Batch number = 8
Evaluating:   7%|▋         | 8/109 [00:03<00:45,  2.21it/s]11/28/2021 16:19:53 - INFO - __main__ -   Batch number = 9
Evaluating:   8%|▊         | 9/109 [00:04<00:45,  2.22it/s]11/28/2021 16:19:54 - INFO - __main__ -   Batch number = 10
Evaluating:   9%|▉         | 10/109 [00:04<00:44,  2.21it/s]11/28/2021 16:19:54 - INFO - __main__ -   Batch number = 11
Evaluating:  10%|█         | 11/109 [00:04<00:44,  2.19it/s]11/28/2021 16:19:54 - INFO - __main__ -   Batch number = 12
Evaluating:  11%|█         | 12/109 [00:05<00:43,  2.21it/s]11/28/2021 16:19:55 - INFO - __main__ -   Batch number = 13
Evaluating:  12%|█▏        | 13/109 [00:05<00:43,  2.21it/s]11/28/2021 16:19:55 - INFO - __main__ -   Batch number = 14
Evaluating:  13%|█▎        | 14/109 [00:06<00:42,  2.24it/s]11/28/2021 16:19:56 - INFO - __main__ -   Batch number = 15
Evaluating:  14%|█▍        | 15/109 [00:06<00:43,  2.17it/s]11/28/2021 16:19:56 - INFO - __main__ -   Batch number = 16
Evaluating:  15%|█▍        | 16/109 [00:07<00:41,  2.25it/s]11/28/2021 16:19:57 - INFO - __main__ -   Batch number = 17
Evaluating:  16%|█▌        | 17/109 [00:07<00:40,  2.28it/s]11/28/2021 16:19:57 - INFO - __main__ -   Batch number = 18
Evaluating:  17%|█▋        | 18/109 [00:08<00:39,  2.31it/s]11/28/2021 16:19:58 - INFO - __main__ -   Batch number = 19
Evaluating:  17%|█▋        | 19/109 [00:08<00:38,  2.32it/s]11/28/2021 16:19:58 - INFO - __main__ -   Batch number = 20
Evaluating:  18%|█▊        | 20/109 [00:08<00:38,  2.31it/s]11/28/2021 16:19:58 - INFO - __main__ -   Batch number = 21
Evaluating:  19%|█▉        | 21/109 [00:09<00:39,  2.24it/s]11/28/2021 16:19:59 - INFO - __main__ -   Batch number = 22
Evaluating:  20%|██        | 22/109 [00:09<00:32,  2.68it/s]11/28/2021 16:19:59 - INFO - __main__ -   Batch number = 23
Evaluating:  21%|██        | 23/109 [00:10<00:34,  2.50it/s]11/28/2021 16:20:00 - INFO - __main__ -   Batch number = 24
Evaluating:  22%|██▏       | 24/109 [00:10<00:35,  2.42it/s]11/28/2021 16:20:00 - INFO - __main__ -   Batch number = 25
Evaluating:  23%|██▎       | 25/109 [00:10<00:35,  2.35it/s]11/28/2021 16:20:00 - INFO - __main__ -   Batch number = 26
Evaluating:  24%|██▍       | 26/109 [00:11<00:36,  2.28it/s]11/28/2021 16:20:01 - INFO - __main__ -   Batch number = 27
Evaluating:  25%|██▍       | 27/109 [00:11<00:35,  2.33it/s]11/28/2021 16:20:01 - INFO - __main__ -   Batch number = 28
Evaluating:  26%|██▌       | 28/109 [00:12<00:35,  2.29it/s]11/28/2021 16:20:02 - INFO - __main__ -   Batch number = 29
Evaluating:  27%|██▋       | 29/109 [00:12<00:35,  2.27it/s]11/28/2021 16:20:02 - INFO - __main__ -   Batch number = 30
Evaluating:  28%|██▊       | 30/109 [00:13<00:35,  2.25it/s]11/28/2021 16:20:03 - INFO - __main__ -   Batch number = 31
Evaluating:  28%|██▊       | 31/109 [00:13<00:34,  2.25it/s]11/28/2021 16:20:03 - INFO - __main__ -   Batch number = 32
Evaluating:  29%|██▉       | 32/109 [00:14<00:34,  2.23it/s]11/28/2021 16:20:04 - INFO - __main__ -   Batch number = 33
Evaluating:  30%|███       | 33/109 [00:14<00:34,  2.22it/s]11/28/2021 16:20:04 - INFO - __main__ -   Batch number = 34
Evaluating:  31%|███       | 34/109 [00:15<00:33,  2.22it/s]11/28/2021 16:20:04 - INFO - __main__ -   Batch number = 35
Evaluating:  32%|███▏      | 35/109 [00:15<00:33,  2.23it/s]11/28/2021 16:20:05 - INFO - __main__ -   Batch number = 36
Evaluating:  33%|███▎      | 36/109 [00:15<00:31,  2.35it/s]11/28/2021 16:20:05 - INFO - __main__ -   Batch number = 37
Evaluating:  34%|███▍      | 37/109 [00:16<00:31,  2.30it/s]11/28/2021 16:20:06 - INFO - __main__ -   Batch number = 38
Evaluating:  35%|███▍      | 38/109 [00:16<00:31,  2.28it/s]11/28/2021 16:20:06 - INFO - __main__ -   Batch number = 39
Evaluating:  36%|███▌      | 39/109 [00:17<00:29,  2.34it/s]11/28/2021 16:20:07 - INFO - __main__ -   Batch number = 40
Evaluating:  37%|███▋      | 40/109 [00:17<00:29,  2.36it/s]11/28/2021 16:20:07 - INFO - __main__ -   Batch number = 41
Evaluating:  38%|███▊      | 41/109 [00:17<00:28,  2.38it/s]11/28/2021 16:20:07 - INFO - __main__ -   Batch number = 42
Evaluating:  39%|███▊      | 42/109 [00:18<00:28,  2.38it/s]11/28/2021 16:20:08 - INFO - __main__ -   Batch number = 43
Evaluating:  39%|███▉      | 43/109 [00:18<00:28,  2.32it/s]11/28/2021 16:20:08 - INFO - __main__ -   Batch number = 44
Evaluating:  40%|████      | 44/109 [00:19<00:28,  2.31it/s]11/28/2021 16:20:09 - INFO - __main__ -   Batch number = 45
Evaluating:  41%|████▏     | 45/109 [00:19<00:27,  2.33it/s]11/28/2021 16:20:09 - INFO - __main__ -   Batch number = 46
Evaluating:  42%|████▏     | 46/109 [00:20<00:25,  2.47it/s]11/28/2021 16:20:09 - INFO - __main__ -   Batch number = 47
Evaluating:  43%|████▎     | 47/109 [00:20<00:23,  2.69it/s]11/28/2021 16:20:10 - INFO - __main__ -   Batch number = 48
Evaluating:  44%|████▍     | 48/109 [00:20<00:21,  2.88it/s]11/28/2021 16:20:10 - INFO - __main__ -   Batch number = 49
Evaluating:  45%|████▍     | 49/109 [00:20<00:19,  3.03it/s]11/28/2021 16:20:10 - INFO - __main__ -   Batch number = 50
Evaluating:  46%|████▌     | 50/109 [00:21<00:19,  3.10it/s]11/28/2021 16:20:11 - INFO - __main__ -   Batch number = 51
Evaluating:  47%|████▋     | 51/109 [00:21<00:18,  3.11it/s]11/28/2021 16:20:11 - INFO - __main__ -   Batch number = 52
Evaluating:  48%|████▊     | 52/109 [00:21<00:17,  3.22it/s]11/28/2021 16:20:11 - INFO - __main__ -   Batch number = 53
Evaluating:  49%|████▊     | 53/109 [00:22<00:17,  3.29it/s]11/28/2021 16:20:12 - INFO - __main__ -   Batch number = 54
Evaluating:  50%|████▉     | 54/109 [00:22<00:16,  3.33it/s]11/28/2021 16:20:12 - INFO - __main__ -   Batch number = 55
Evaluating:  50%|█████     | 55/109 [00:22<00:16,  3.36it/s]11/28/2021 16:20:12 - INFO - __main__ -   Batch number = 56
Evaluating:  51%|█████▏    | 56/109 [00:22<00:15,  3.39it/s]11/28/2021 16:20:12 - INFO - __main__ -   Batch number = 57
Evaluating:  52%|█████▏    | 57/109 [00:23<00:15,  3.41it/s]11/28/2021 16:20:13 - INFO - __main__ -   Batch number = 58
Evaluating:  53%|█████▎    | 58/109 [00:23<00:15,  3.37it/s]11/28/2021 16:20:13 - INFO - __main__ -   Batch number = 59
Evaluating:  54%|█████▍    | 59/109 [00:23<00:15,  3.30it/s]11/28/2021 16:20:13 - INFO - __main__ -   Batch number = 60
Evaluating:  55%|█████▌    | 60/109 [00:24<00:14,  3.35it/s]11/28/2021 16:20:14 - INFO - __main__ -   Batch number = 61
Evaluating:  56%|█████▌    | 61/109 [00:24<00:14,  3.39it/s]11/28/2021 16:20:14 - INFO - __main__ -   Batch number = 62
Evaluating:  57%|█████▋    | 62/109 [00:24<00:13,  3.40it/s]11/28/2021 16:20:14 - INFO - __main__ -   Batch number = 63
Evaluating:  58%|█████▊    | 63/109 [00:25<00:13,  3.41it/s]11/28/2021 16:20:15 - INFO - __main__ -   Batch number = 64
Evaluating:  59%|█████▊    | 64/109 [00:25<00:13,  3.44it/s]11/28/2021 16:20:15 - INFO - __main__ -   Batch number = 65
Evaluating:  60%|█████▉    | 65/109 [00:25<00:12,  3.44it/s]11/28/2021 16:20:15 - INFO - __main__ -   Batch number = 66
Evaluating:  61%|██████    | 66/109 [00:25<00:12,  3.32it/s]11/28/2021 16:20:15 - INFO - __main__ -   Batch number = 67
Evaluating:  61%|██████▏   | 67/109 [00:26<00:12,  3.29it/s]11/28/2021 16:20:16 - INFO - __main__ -   Batch number = 68
Evaluating:  62%|██████▏   | 68/109 [00:26<00:12,  3.36it/s]11/28/2021 16:20:16 - INFO - __main__ -   Batch number = 69
Evaluating:  63%|██████▎   | 69/109 [00:26<00:11,  3.44it/s]11/28/2021 16:20:16 - INFO - __main__ -   Batch number = 70
Evaluating:  64%|██████▍   | 70/109 [00:27<00:11,  3.50it/s]11/28/2021 16:20:17 - INFO - __main__ -   Batch number = 71
Evaluating:  65%|██████▌   | 71/109 [00:27<00:10,  3.51it/s]11/28/2021 16:20:17 - INFO - __main__ -   Batch number = 72
Evaluating:  66%|██████▌   | 72/109 [00:27<00:10,  3.49it/s]11/28/2021 16:20:17 - INFO - __main__ -   Batch number = 73
Evaluating:  67%|██████▋   | 73/109 [00:27<00:10,  3.48it/s]11/28/2021 16:20:17 - INFO - __main__ -   Batch number = 74
Evaluating:  68%|██████▊   | 74/109 [00:28<00:10,  3.48it/s]11/28/2021 16:20:18 - INFO - __main__ -   Batch number = 75
Evaluating:  69%|██████▉   | 75/109 [00:28<00:10,  3.39it/s]11/28/2021 16:20:18 - INFO - __main__ -   Batch number = 76
Evaluating:  70%|██████▉   | 76/109 [00:28<00:09,  3.37it/s]11/28/2021 16:20:18 - INFO - __main__ -   Batch number = 77
Evaluating:  71%|███████   | 77/109 [00:29<00:09,  3.40it/s]11/28/2021 16:20:19 - INFO - __main__ -   Batch number = 78
Evaluating:  72%|███████▏  | 78/109 [00:29<00:09,  3.42it/s]11/28/2021 16:20:19 - INFO - __main__ -   Batch number = 79
Evaluating:  72%|███████▏  | 79/109 [00:29<00:08,  3.44it/s]11/28/2021 16:20:19 - INFO - __main__ -   Batch number = 80
Evaluating:  73%|███████▎  | 80/109 [00:30<00:08,  3.45it/s]11/28/2021 16:20:19 - INFO - __main__ -   Batch number = 81
Evaluating:  74%|███████▍  | 81/109 [00:30<00:08,  3.45it/s]11/28/2021 16:20:20 - INFO - __main__ -   Batch number = 82
Evaluating:  75%|███████▌  | 82/109 [00:30<00:07,  3.42it/s]11/28/2021 16:20:20 - INFO - __main__ -   Batch number = 83
Evaluating:  76%|███████▌  | 83/109 [00:30<00:07,  3.33it/s]11/28/2021 16:20:20 - INFO - __main__ -   Batch number = 84
Evaluating:  77%|███████▋  | 84/109 [00:31<00:07,  3.34it/s]11/28/2021 16:20:21 - INFO - __main__ -   Batch number = 85
Evaluating:  78%|███████▊  | 85/109 [00:31<00:07,  3.31it/s]11/28/2021 16:20:21 - INFO - __main__ -   Batch number = 86
Evaluating:  79%|███████▉  | 86/109 [00:31<00:06,  3.36it/s]11/28/2021 16:20:21 - INFO - __main__ -   Batch number = 87
Evaluating:  80%|███████▉  | 87/109 [00:32<00:06,  3.38it/s]11/28/2021 16:20:22 - INFO - __main__ -   Batch number = 88
Evaluating:  81%|████████  | 88/109 [00:32<00:06,  3.41it/s]11/28/2021 16:20:22 - INFO - __main__ -   Batch number = 89
Evaluating:  82%|████████▏ | 89/109 [00:32<00:05,  3.43it/s]11/28/2021 16:20:22 - INFO - __main__ -   Batch number = 90
Evaluating:  83%|████████▎ | 90/109 [00:32<00:05,  3.44it/s]11/28/2021 16:20:22 - INFO - __main__ -   Batch number = 91
Evaluating:  83%|████████▎ | 91/109 [00:33<00:05,  3.37it/s]11/28/2021 16:20:23 - INFO - __main__ -   Batch number = 92
Evaluating:  84%|████████▍ | 92/109 [00:33<00:05,  3.36it/s]11/28/2021 16:20:23 - INFO - __main__ -   Batch number = 93
Evaluating:  85%|████████▌ | 93/109 [00:33<00:04,  3.40it/s]11/28/2021 16:20:23 - INFO - __main__ -   Batch number = 94
Evaluating:  86%|████████▌ | 94/109 [00:34<00:04,  3.42it/s]11/28/2021 16:20:24 - INFO - __main__ -   Batch number = 95
Evaluating:  87%|████████▋ | 95/109 [00:34<00:04,  3.45it/s]11/28/2021 16:20:24 - INFO - __main__ -   Batch number = 96
Evaluating:  88%|████████▊ | 96/109 [00:34<00:03,  3.44it/s]11/28/2021 16:20:24 - INFO - __main__ -   Batch number = 97
Evaluating:  89%|████████▉ | 97/109 [00:35<00:03,  3.46it/s]11/28/2021 16:20:24 - INFO - __main__ -   Batch number = 98
Evaluating:  90%|████████▉ | 98/109 [00:35<00:03,  3.42it/s]11/28/2021 16:20:25 - INFO - __main__ -   Batch number = 99
Evaluating:  91%|█████████ | 99/109 [00:35<00:03,  3.32it/s]11/28/2021 16:20:25 - INFO - __main__ -   Batch number = 100
Evaluating:  92%|█████████▏| 100/109 [00:35<00:02,  3.35it/s]11/28/2021 16:20:25 - INFO - __main__ -   Batch number = 101
Evaluating:  93%|█████████▎| 101/109 [00:36<00:02,  3.38it/s]11/28/2021 16:20:26 - INFO - __main__ -   Batch number = 102
Evaluating:  94%|█████████▎| 102/109 [00:36<00:02,  3.15it/s]11/28/2021 16:20:26 - INFO - __main__ -   Batch number = 103
Evaluating:  94%|█████████▍| 103/109 [00:36<00:01,  3.23it/s]11/28/2021 16:20:26 - INFO - __main__ -   Batch number = 104
Evaluating:  95%|█████████▌| 104/109 [00:37<00:01,  3.27it/s]11/28/2021 16:20:27 - INFO - __main__ -   Batch number = 105
Evaluating:  96%|█████████▋| 105/109 [00:37<00:01,  3.31it/s]11/28/2021 16:20:27 - INFO - __main__ -   Batch number = 106
Evaluating:  97%|█████████▋| 106/109 [00:37<00:00,  3.33it/s]11/28/2021 16:20:27 - INFO - __main__ -   Batch number = 107
Evaluating:  98%|█████████▊| 107/109 [00:38<00:00,  3.35it/s]11/28/2021 16:20:28 - INFO - __main__ -   Batch number = 108
Evaluating:  99%|█████████▉| 108/109 [00:38<00:00,  3.36it/s]11/28/2021 16:20:28 - INFO - __main__ -   Batch number = 109
Evaluating: 100%|██████████| 109/109 [00:38<00:00,  2.83it/s]
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 16:20:29 - INFO - __main__ -   ***** Evaluation result  in zh *****
11/28/2021 16:20:29 - INFO - __main__ -     f1 = 0.6246939489331933
11/28/2021 16:20:29 - INFO - __main__ -     loss = 1.4302742169537674
11/28/2021 16:20:29 - INFO - __main__ -     precision = 0.6310496656942507
11/28/2021 16:20:29 - INFO - __main__ -     recall = 0.6184649808211393
48.36user 17.55system 1:02.79elapsed 104%CPU (0avgtext+0avgdata 3934064maxresident)k
0inputs+752outputs (0major+1665500minor)pagefaults 0swaps
