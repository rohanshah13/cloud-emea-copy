PyTorch version 1.9.0+cu102 available.
11/21/2021 11:12:34 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ar', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/21/2021 11:12:34 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/21/2021 11:12:34 - INFO - __main__ -   Seed = 1
11/21/2021 11:12:34 - INFO - root -   save model
11/21/2021 11:12:34 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ar', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/21/2021 11:12:34 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/21/2021 11:12:37 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/21/2021 11:12:42 - INFO - __main__ -   Using lang2id = None
11/21/2021 11:12:42 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/21/2021 11:12:42 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
11/21/2021 11:12:42 - INFO - root -   Trying to decide if add adapter
11/21/2021 11:12:42 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_model_head.bin
11/21/2021 11:12:42 - INFO - root -   loading lang adpater bh/wiki@ukp
11/21/2021 11:12:42 - INFO - __main__ -   Adapter Languages : ['bh'], Length : 1
11/21/2021 11:12:42 - INFO - __main__ -   Adapter Names ['bh/wiki@ukp'], Length : 1
11/21/2021 11:12:42 - INFO - __main__ -   Language = bh
11/21/2021 11:12:42 - INFO - __main__ -   Adapter Name = bh/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/bh/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_bh_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/adapter_config.json
Adding adapter 'bh' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/head_config.json
11/21/2021 11:12:48 - INFO - __main__ -   Language adapter for ar not found, using bh instead
11/21/2021 11:12:48 - INFO - __main__ -   Set active language adapter to bh
11/21/2021 11:12:48 - INFO - __main__ -   Args Adapter Weight = None
11/21/2021 11:12:48 - INFO - __main__ -   Adapter Languages = ['bh']
11/21/2021 11:12:48 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_ar_bert-base-multilingual-cased_128
11/21/2021 11:12:48 - INFO - __main__ -   ***** Running evaluation  in ar *****
11/21/2021 11:12:48 - INFO - __main__ -     Num examples = 1784
11/21/2021 11:12:48 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/56 [00:00<?, ?it/s]11/21/2021 11:12:48 - INFO - __main__ -   Batch number = 1
Evaluating:   2%|▏         | 1/56 [00:00<00:07,  7.18it/s]11/21/2021 11:12:48 - INFO - __main__ -   Batch number = 2
Evaluating:   4%|▎         | 2/56 [00:00<00:07,  7.34it/s]11/21/2021 11:12:48 - INFO - __main__ -   Batch number = 3
Evaluating:   5%|▌         | 3/56 [00:00<00:07,  7.35it/s]11/21/2021 11:12:49 - INFO - __main__ -   Batch number = 4
Evaluating:   7%|▋         | 4/56 [00:00<00:07,  7.37it/s]11/21/2021 11:12:49 - INFO - __main__ -   Batch number = 5
Evaluating:   9%|▉         | 5/56 [00:00<00:06,  7.38it/s]11/21/2021 11:12:49 - INFO - __main__ -   Batch number = 6
Evaluating:  11%|█         | 6/56 [00:00<00:06,  7.36it/s]11/21/2021 11:12:49 - INFO - __main__ -   Batch number = 7
Evaluating:  12%|█▎        | 7/56 [00:00<00:06,  7.35it/s]11/21/2021 11:12:49 - INFO - __main__ -   Batch number = 8
Evaluating:  14%|█▍        | 8/56 [00:01<00:06,  7.34it/s]11/21/2021 11:12:49 - INFO - __main__ -   Batch number = 9
Evaluating:  16%|█▌        | 9/56 [00:01<00:06,  7.31it/s]11/21/2021 11:12:49 - INFO - __main__ -   Batch number = 10
Evaluating:  18%|█▊        | 10/56 [00:01<00:06,  7.31it/s]11/21/2021 11:12:49 - INFO - __main__ -   Batch number = 11
Evaluating:  20%|█▉        | 11/56 [00:01<00:06,  7.28it/s]11/21/2021 11:12:50 - INFO - __main__ -   Batch number = 12
Evaluating:  21%|██▏       | 12/56 [00:01<00:06,  7.29it/s]11/21/2021 11:12:50 - INFO - __main__ -   Batch number = 13
Evaluating:  23%|██▎       | 13/56 [00:01<00:05,  7.27it/s]11/21/2021 11:12:50 - INFO - __main__ -   Batch number = 14
Evaluating:  25%|██▌       | 14/56 [00:01<00:05,  7.29it/s]11/21/2021 11:12:50 - INFO - __main__ -   Batch number = 15
Evaluating:  27%|██▋       | 15/56 [00:02<00:05,  7.29it/s]11/21/2021 11:12:50 - INFO - __main__ -   Batch number = 16
Evaluating:  29%|██▊       | 16/56 [00:02<00:05,  7.29it/s]11/21/2021 11:12:50 - INFO - __main__ -   Batch number = 17
Evaluating:  30%|███       | 17/56 [00:02<00:05,  7.28it/s]11/21/2021 11:12:50 - INFO - __main__ -   Batch number = 18
Evaluating:  32%|███▏      | 18/56 [00:02<00:05,  7.26it/s]11/21/2021 11:12:51 - INFO - __main__ -   Batch number = 19
Evaluating:  34%|███▍      | 19/56 [00:02<00:05,  7.27it/s]11/21/2021 11:12:51 - INFO - __main__ -   Batch number = 20
Evaluating:  36%|███▌      | 20/56 [00:02<00:04,  7.28it/s]11/21/2021 11:12:51 - INFO - __main__ -   Batch number = 21
Evaluating:  38%|███▊      | 21/56 [00:02<00:04,  7.23it/s]11/21/2021 11:12:51 - INFO - __main__ -   Batch number = 22
Evaluating:  39%|███▉      | 22/56 [00:03<00:04,  7.21it/s]11/21/2021 11:12:51 - INFO - __main__ -   Batch number = 23
Evaluating:  41%|████      | 23/56 [00:03<00:04,  7.23it/s]11/21/2021 11:12:51 - INFO - __main__ -   Batch number = 24
Evaluating:  43%|████▎     | 24/56 [00:03<00:04,  7.23it/s]11/21/2021 11:12:51 - INFO - __main__ -   Batch number = 25
Evaluating:  45%|████▍     | 25/56 [00:03<00:04,  7.24it/s]11/21/2021 11:12:52 - INFO - __main__ -   Batch number = 26
Evaluating:  46%|████▋     | 26/56 [00:03<00:04,  7.23it/s]11/21/2021 11:12:52 - INFO - __main__ -   Batch number = 27
Evaluating:  48%|████▊     | 27/56 [00:03<00:04,  7.21it/s]11/21/2021 11:12:52 - INFO - __main__ -   Batch number = 28
Evaluating:  50%|█████     | 28/56 [00:03<00:03,  7.22it/s]11/21/2021 11:12:52 - INFO - __main__ -   Batch number = 29
Evaluating:  52%|█████▏    | 29/56 [00:03<00:03,  7.23it/s]11/21/2021 11:12:52 - INFO - __main__ -   Batch number = 30
Evaluating:  54%|█████▎    | 30/56 [00:04<00:03,  7.20it/s]11/21/2021 11:12:52 - INFO - __main__ -   Batch number = 31
Evaluating:  55%|█████▌    | 31/56 [00:04<00:03,  7.21it/s]11/21/2021 11:12:52 - INFO - __main__ -   Batch number = 32
Evaluating:  57%|█████▋    | 32/56 [00:04<00:03,  7.21it/s]11/21/2021 11:12:53 - INFO - __main__ -   Batch number = 33
Evaluating:  59%|█████▉    | 33/56 [00:04<00:03,  7.21it/s]11/21/2021 11:12:53 - INFO - __main__ -   Batch number = 34
Evaluating:  61%|██████    | 34/56 [00:04<00:03,  7.21it/s]11/21/2021 11:12:53 - INFO - __main__ -   Batch number = 35
Evaluating:  62%|██████▎   | 35/56 [00:04<00:02,  7.22it/s]11/21/2021 11:12:53 - INFO - __main__ -   Batch number = 36
Evaluating:  64%|██████▍   | 36/56 [00:04<00:02,  7.20it/s]11/21/2021 11:12:53 - INFO - __main__ -   Batch number = 37
Evaluating:  66%|██████▌   | 37/56 [00:05<00:03,  5.50it/s]11/21/2021 11:12:53 - INFO - __main__ -   Batch number = 38
Evaluating:  68%|██████▊   | 38/56 [00:05<00:03,  5.92it/s]11/21/2021 11:12:54 - INFO - __main__ -   Batch number = 39
Evaluating:  70%|██████▉   | 39/56 [00:05<00:02,  6.23it/s]11/21/2021 11:12:54 - INFO - __main__ -   Batch number = 40
Evaluating:  71%|███████▏  | 40/56 [00:05<00:02,  6.49it/s]11/21/2021 11:12:54 - INFO - __main__ -   Batch number = 41
Evaluating:  73%|███████▎  | 41/56 [00:05<00:02,  6.69it/s]11/21/2021 11:12:54 - INFO - __main__ -   Batch number = 42
Evaluating:  75%|███████▌  | 42/56 [00:05<00:02,  6.82it/s]11/21/2021 11:12:54 - INFO - __main__ -   Batch number = 43
Evaluating:  77%|███████▋  | 43/56 [00:06<00:01,  6.90it/s]11/21/2021 11:12:54 - INFO - __main__ -   Batch number = 44
Evaluating:  79%|███████▊  | 44/56 [00:06<00:01,  6.98it/s]11/21/2021 11:12:54 - INFO - __main__ -   Batch number = 45
Evaluating:  80%|████████  | 45/56 [00:06<00:01,  7.02it/s]11/21/2021 11:12:54 - INFO - __main__ -   Batch number = 46
Evaluating:  82%|████████▏ | 46/56 [00:06<00:01,  7.06it/s]11/21/2021 11:12:55 - INFO - __main__ -   Batch number = 47
Evaluating:  84%|████████▍ | 47/56 [00:06<00:01,  7.09it/s]11/21/2021 11:12:55 - INFO - __main__ -   Batch number = 48
Evaluating:  86%|████████▌ | 48/56 [00:06<00:01,  7.11it/s]11/21/2021 11:12:55 - INFO - __main__ -   Batch number = 49
Evaluating:  88%|████████▊ | 49/56 [00:06<00:00,  7.12it/s]11/21/2021 11:12:55 - INFO - __main__ -   Batch number = 50
Evaluating:  89%|████████▉ | 50/56 [00:07<00:00,  7.13it/s]11/21/2021 11:12:55 - INFO - __main__ -   Batch number = 51
Evaluating:  91%|█████████ | 51/56 [00:07<00:00,  7.12it/s]11/21/2021 11:12:55 - INFO - __main__ -   Batch number = 52
Evaluating:  93%|█████████▎| 52/56 [00:07<00:00,  7.12it/s]11/21/2021 11:12:55 - INFO - __main__ -   Batch number = 53
Evaluating:  95%|█████████▍| 53/56 [00:07<00:00,  7.13it/s]11/21/2021 11:12:56 - INFO - __main__ -   Batch number = 54
Evaluating:  96%|█████████▋| 54/56 [00:07<00:00,  7.11it/s]11/21/2021 11:12:56 - INFO - __main__ -   Batch number = 55
Evaluating:  98%|█████████▊| 55/56 [00:07<00:00,  7.11it/s]11/21/2021 11:12:56 - INFO - __main__ -   Batch number = 56
Evaluating: 100%|██████████| 56/56 [00:07<00:00,  7.61it/s]Evaluating: 100%|██████████| 56/56 [00:07<00:00,  7.11it/s]
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/21/2021 11:12:57 - INFO - __main__ -   ***** Evaluation result  in ar *****
11/21/2021 11:12:57 - INFO - __main__ -     f1 = 0.5752189392167563
11/21/2021 11:12:57 - INFO - __main__ -     loss = 1.3790158065302032
11/21/2021 11:12:57 - INFO - __main__ -     precision = 0.5675786558629708
11/21/2021 11:12:57 - INFO - __main__ -     recall = 0.583067723836367
24.07user 10.76system 0:24.71elapsed 140%CPU (0avgtext+0avgdata 3871416maxresident)k
0inputs+520outputs (0major+1327493minor)pagefaults 0swaps
PyTorch version 1.9.0+cu102 available.
11/21/2021 11:12:59 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ar', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/21/2021 11:12:59 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/21/2021 11:12:59 - INFO - __main__ -   Seed = 2
11/21/2021 11:12:59 - INFO - root -   save model
11/21/2021 11:12:59 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ar', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/21/2021 11:12:59 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/21/2021 11:13:02 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/21/2021 11:13:07 - INFO - __main__ -   Using lang2id = None
11/21/2021 11:13:07 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/21/2021 11:13:07 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
11/21/2021 11:13:07 - INFO - root -   Trying to decide if add adapter
11/21/2021 11:13:07 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_model_head.bin
11/21/2021 11:13:07 - INFO - root -   loading lang adpater bh/wiki@ukp
11/21/2021 11:13:07 - INFO - __main__ -   Adapter Languages : ['bh'], Length : 1
11/21/2021 11:13:07 - INFO - __main__ -   Adapter Names ['bh/wiki@ukp'], Length : 1
11/21/2021 11:13:07 - INFO - __main__ -   Language = bh
11/21/2021 11:13:07 - INFO - __main__ -   Adapter Name = bh/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/bh/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_bh_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/adapter_config.json
Adding adapter 'bh' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/head_config.json
11/21/2021 11:13:12 - INFO - __main__ -   Language adapter for ar not found, using bh instead
11/21/2021 11:13:12 - INFO - __main__ -   Set active language adapter to bh
11/21/2021 11:13:12 - INFO - __main__ -   Args Adapter Weight = None
11/21/2021 11:13:12 - INFO - __main__ -   Adapter Languages = ['bh']
11/21/2021 11:13:12 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_ar_bert-base-multilingual-cased_128
11/21/2021 11:13:13 - INFO - __main__ -   ***** Running evaluation  in ar *****
11/21/2021 11:13:13 - INFO - __main__ -     Num examples = 1784
11/21/2021 11:13:13 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/56 [00:00<?, ?it/s]11/21/2021 11:13:13 - INFO - __main__ -   Batch number = 1
Evaluating:   2%|▏         | 1/56 [00:00<00:08,  6.48it/s]11/21/2021 11:13:13 - INFO - __main__ -   Batch number = 2
Evaluating:   4%|▎         | 2/56 [00:00<00:07,  6.82it/s]11/21/2021 11:13:13 - INFO - __main__ -   Batch number = 3
Evaluating:   5%|▌         | 3/56 [00:00<00:07,  7.05it/s]11/21/2021 11:13:13 - INFO - __main__ -   Batch number = 4
Evaluating:   7%|▋         | 4/56 [00:00<00:07,  7.16it/s]11/21/2021 11:13:13 - INFO - __main__ -   Batch number = 5
Evaluating:   9%|▉         | 5/56 [00:00<00:07,  7.24it/s]11/21/2021 11:13:13 - INFO - __main__ -   Batch number = 6
Evaluating:  11%|█         | 6/56 [00:00<00:06,  7.28it/s]11/21/2021 11:13:13 - INFO - __main__ -   Batch number = 7
Evaluating:  12%|█▎        | 7/56 [00:00<00:06,  7.30it/s]11/21/2021 11:13:13 - INFO - __main__ -   Batch number = 8
Evaluating:  14%|█▍        | 8/56 [00:01<00:06,  7.31it/s]11/21/2021 11:13:14 - INFO - __main__ -   Batch number = 9
Evaluating:  16%|█▌        | 9/56 [00:01<00:06,  7.31it/s]11/21/2021 11:13:14 - INFO - __main__ -   Batch number = 10
Evaluating:  18%|█▊        | 10/56 [00:01<00:06,  7.28it/s]11/21/2021 11:13:14 - INFO - __main__ -   Batch number = 11
Evaluating:  20%|█▉        | 11/56 [00:01<00:06,  7.28it/s]11/21/2021 11:13:14 - INFO - __main__ -   Batch number = 12
Evaluating:  21%|██▏       | 12/56 [00:01<00:06,  7.28it/s]11/21/2021 11:13:14 - INFO - __main__ -   Batch number = 13
Evaluating:  23%|██▎       | 13/56 [00:01<00:05,  7.28it/s]11/21/2021 11:13:14 - INFO - __main__ -   Batch number = 14
Evaluating:  25%|██▌       | 14/56 [00:01<00:05,  7.25it/s]11/21/2021 11:13:14 - INFO - __main__ -   Batch number = 15
Evaluating:  27%|██▋       | 15/56 [00:02<00:05,  7.24it/s]11/21/2021 11:13:15 - INFO - __main__ -   Batch number = 16
Evaluating:  29%|██▊       | 16/56 [00:02<00:05,  7.24it/s]11/21/2021 11:13:15 - INFO - __main__ -   Batch number = 17
Evaluating:  30%|███       | 17/56 [00:02<00:05,  7.24it/s]11/21/2021 11:13:15 - INFO - __main__ -   Batch number = 18
Evaluating:  32%|███▏      | 18/56 [00:02<00:05,  7.24it/s]11/21/2021 11:13:15 - INFO - __main__ -   Batch number = 19
Evaluating:  34%|███▍      | 19/56 [00:02<00:05,  7.25it/s]11/21/2021 11:13:15 - INFO - __main__ -   Batch number = 20
Evaluating:  36%|███▌      | 20/56 [00:02<00:04,  7.25it/s]11/21/2021 11:13:15 - INFO - __main__ -   Batch number = 21
Evaluating:  38%|███▊      | 21/56 [00:02<00:04,  7.23it/s]11/21/2021 11:13:15 - INFO - __main__ -   Batch number = 22
Evaluating:  39%|███▉      | 22/56 [00:03<00:04,  7.22it/s]11/21/2021 11:13:16 - INFO - __main__ -   Batch number = 23
Evaluating:  41%|████      | 23/56 [00:03<00:04,  7.21it/s]11/21/2021 11:13:16 - INFO - __main__ -   Batch number = 24
Evaluating:  43%|████▎     | 24/56 [00:03<00:04,  7.21it/s]11/21/2021 11:13:16 - INFO - __main__ -   Batch number = 25
Evaluating:  45%|████▍     | 25/56 [00:03<00:04,  7.11it/s]11/21/2021 11:13:16 - INFO - __main__ -   Batch number = 26
Evaluating:  46%|████▋     | 26/56 [00:03<00:04,  7.12it/s]11/21/2021 11:13:16 - INFO - __main__ -   Batch number = 27
Evaluating:  48%|████▊     | 27/56 [00:03<00:04,  7.14it/s]11/21/2021 11:13:16 - INFO - __main__ -   Batch number = 28
Evaluating:  50%|█████     | 28/56 [00:03<00:03,  7.17it/s]11/21/2021 11:13:16 - INFO - __main__ -   Batch number = 29
Evaluating:  52%|█████▏    | 29/56 [00:04<00:03,  7.18it/s]11/21/2021 11:13:17 - INFO - __main__ -   Batch number = 30
Evaluating:  54%|█████▎    | 30/56 [00:04<00:03,  7.15it/s]11/21/2021 11:13:17 - INFO - __main__ -   Batch number = 31
Evaluating:  55%|█████▌    | 31/56 [00:04<00:03,  7.12it/s]11/21/2021 11:13:17 - INFO - __main__ -   Batch number = 32
Evaluating:  57%|█████▋    | 32/56 [00:04<00:03,  7.13it/s]11/21/2021 11:13:17 - INFO - __main__ -   Batch number = 33
Evaluating:  59%|█████▉    | 33/56 [00:04<00:03,  7.13it/s]11/21/2021 11:13:17 - INFO - __main__ -   Batch number = 34
Evaluating:  61%|██████    | 34/56 [00:04<00:03,  7.15it/s]11/21/2021 11:13:17 - INFO - __main__ -   Batch number = 35
Evaluating:  62%|██████▎   | 35/56 [00:04<00:02,  7.17it/s]11/21/2021 11:13:17 - INFO - __main__ -   Batch number = 36
Evaluating:  64%|██████▍   | 36/56 [00:05<00:02,  7.14it/s]11/21/2021 11:13:18 - INFO - __main__ -   Batch number = 37
Evaluating:  66%|██████▌   | 37/56 [00:05<00:02,  7.14it/s]11/21/2021 11:13:18 - INFO - __main__ -   Batch number = 38
Evaluating:  68%|██████▊   | 38/56 [00:05<00:02,  7.15it/s]11/21/2021 11:13:18 - INFO - __main__ -   Batch number = 39
Evaluating:  70%|██████▉   | 39/56 [00:05<00:02,  7.15it/s]11/21/2021 11:13:18 - INFO - __main__ -   Batch number = 40
Evaluating:  71%|███████▏  | 40/56 [00:05<00:02,  7.13it/s]11/21/2021 11:13:18 - INFO - __main__ -   Batch number = 41
Evaluating:  73%|███████▎  | 41/56 [00:05<00:02,  6.91it/s]11/21/2021 11:13:18 - INFO - __main__ -   Batch number = 42
Evaluating:  75%|███████▌  | 42/56 [00:05<00:02,  6.97it/s]11/21/2021 11:13:18 - INFO - __main__ -   Batch number = 43
Evaluating:  77%|███████▋  | 43/56 [00:06<00:01,  7.02it/s]11/21/2021 11:13:19 - INFO - __main__ -   Batch number = 44
Evaluating:  79%|███████▊  | 44/56 [00:06<00:01,  7.05it/s]11/21/2021 11:13:19 - INFO - __main__ -   Batch number = 45
Evaluating:  80%|████████  | 45/56 [00:06<00:01,  7.06it/s]11/21/2021 11:13:19 - INFO - __main__ -   Batch number = 46
Evaluating:  82%|████████▏ | 46/56 [00:06<00:01,  7.08it/s]11/21/2021 11:13:19 - INFO - __main__ -   Batch number = 47
Evaluating:  84%|████████▍ | 47/56 [00:06<00:01,  6.98it/s]11/21/2021 11:13:19 - INFO - __main__ -   Batch number = 48
Evaluating:  86%|████████▌ | 48/56 [00:06<00:01,  6.98it/s]11/21/2021 11:13:19 - INFO - __main__ -   Batch number = 49
Evaluating:  88%|████████▊ | 49/56 [00:06<00:00,  7.00it/s]11/21/2021 11:13:19 - INFO - __main__ -   Batch number = 50
Evaluating:  89%|████████▉ | 50/56 [00:06<00:00,  7.04it/s]11/21/2021 11:13:20 - INFO - __main__ -   Batch number = 51
Evaluating:  91%|█████████ | 51/56 [00:07<00:00,  7.06it/s]11/21/2021 11:13:20 - INFO - __main__ -   Batch number = 52
Evaluating:  93%|█████████▎| 52/56 [00:07<00:00,  7.07it/s]11/21/2021 11:13:20 - INFO - __main__ -   Batch number = 53
Evaluating:  95%|█████████▍| 53/56 [00:07<00:00,  7.10it/s]11/21/2021 11:13:20 - INFO - __main__ -   Batch number = 54
Evaluating:  96%|█████████▋| 54/56 [00:07<00:00,  7.10it/s]11/21/2021 11:13:20 - INFO - __main__ -   Batch number = 55
Evaluating:  98%|█████████▊| 55/56 [00:07<00:00,  7.07it/s]11/21/2021 11:13:20 - INFO - __main__ -   Batch number = 56
Evaluating: 100%|██████████| 56/56 [00:07<00:00,  7.57it/s]Evaluating: 100%|██████████| 56/56 [00:07<00:00,  7.17it/s]
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/21/2021 11:13:21 - INFO - __main__ -   ***** Evaluation result  in ar *****
11/21/2021 11:13:21 - INFO - __main__ -     f1 = 0.584051556797888
11/21/2021 11:13:21 - INFO - __main__ -     loss = 1.3992302141019277
11/21/2021 11:13:21 - INFO - __main__ -     precision = 0.5740378011243672
11/21/2021 11:13:21 - INFO - __main__ -     recall = 0.594420883491821
22.53user 10.01system 0:24.33elapsed 133%CPU (0avgtext+0avgdata 3869712maxresident)k
0inputs+512outputs (0major+1362288minor)pagefaults 0swaps
PyTorch version 1.9.0+cu102 available.
11/21/2021 11:13:23 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ar', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/21/2021 11:13:23 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/21/2021 11:13:23 - INFO - __main__ -   Seed = 3
11/21/2021 11:13:23 - INFO - root -   save model
11/21/2021 11:13:23 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ar', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/21/2021 11:13:23 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/21/2021 11:13:26 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/21/2021 11:13:32 - INFO - __main__ -   Using lang2id = None
11/21/2021 11:13:32 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/21/2021 11:13:32 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
11/21/2021 11:13:32 - INFO - root -   Trying to decide if add adapter
11/21/2021 11:13:32 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_model_head.bin
11/21/2021 11:13:32 - INFO - root -   loading lang adpater bh/wiki@ukp
11/21/2021 11:13:32 - INFO - __main__ -   Adapter Languages : ['bh'], Length : 1
11/21/2021 11:13:32 - INFO - __main__ -   Adapter Names ['bh/wiki@ukp'], Length : 1
11/21/2021 11:13:32 - INFO - __main__ -   Language = bh
11/21/2021 11:13:32 - INFO - __main__ -   Adapter Name = bh/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/bh/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_bh_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/adapter_config.json
Adding adapter 'bh' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/head_config.json
11/21/2021 11:13:38 - INFO - __main__ -   Language adapter for ar not found, using bh instead
11/21/2021 11:13:38 - INFO - __main__ -   Set active language adapter to bh
11/21/2021 11:13:38 - INFO - __main__ -   Args Adapter Weight = None
11/21/2021 11:13:38 - INFO - __main__ -   Adapter Languages = ['bh']
11/21/2021 11:13:38 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_ar_bert-base-multilingual-cased_128
11/21/2021 11:13:38 - INFO - __main__ -   ***** Running evaluation  in ar *****
11/21/2021 11:13:38 - INFO - __main__ -     Num examples = 1784
11/21/2021 11:13:38 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/56 [00:00<?, ?it/s]11/21/2021 11:13:38 - INFO - __main__ -   Batch number = 1
Evaluating:   2%|▏         | 1/56 [00:00<00:07,  7.14it/s]11/21/2021 11:13:38 - INFO - __main__ -   Batch number = 2
Evaluating:   4%|▎         | 2/56 [00:00<00:07,  7.28it/s]11/21/2021 11:13:38 - INFO - __main__ -   Batch number = 3
Evaluating:   5%|▌         | 3/56 [00:00<00:08,  6.22it/s]11/21/2021 11:13:38 - INFO - __main__ -   Batch number = 4
Evaluating:   7%|▋         | 4/56 [00:00<00:07,  6.62it/s]11/21/2021 11:13:38 - INFO - __main__ -   Batch number = 5
Evaluating:   9%|▉         | 5/56 [00:00<00:07,  6.87it/s]11/21/2021 11:13:38 - INFO - __main__ -   Batch number = 6
Evaluating:  11%|█         | 6/56 [00:00<00:07,  7.01it/s]11/21/2021 11:13:39 - INFO - __main__ -   Batch number = 7
Evaluating:  12%|█▎        | 7/56 [00:01<00:06,  7.09it/s]11/21/2021 11:13:39 - INFO - __main__ -   Batch number = 8
Evaluating:  14%|█▍        | 8/56 [00:01<00:06,  7.17it/s]11/21/2021 11:13:39 - INFO - __main__ -   Batch number = 9
Evaluating:  16%|█▌        | 9/56 [00:01<00:06,  7.21it/s]11/21/2021 11:13:39 - INFO - __main__ -   Batch number = 10
Evaluating:  18%|█▊        | 10/56 [00:01<00:06,  7.26it/s]11/21/2021 11:13:39 - INFO - __main__ -   Batch number = 11
Evaluating:  20%|█▉        | 11/56 [00:01<00:06,  7.27it/s]11/21/2021 11:13:39 - INFO - __main__ -   Batch number = 12
Evaluating:  21%|██▏       | 12/56 [00:01<00:06,  7.28it/s]11/21/2021 11:13:39 - INFO - __main__ -   Batch number = 13
Evaluating:  23%|██▎       | 13/56 [00:01<00:05,  7.29it/s]11/21/2021 11:13:40 - INFO - __main__ -   Batch number = 14
Evaluating:  25%|██▌       | 14/56 [00:01<00:05,  7.28it/s]11/21/2021 11:13:40 - INFO - __main__ -   Batch number = 15
Evaluating:  27%|██▋       | 15/56 [00:02<00:05,  7.27it/s]11/21/2021 11:13:40 - INFO - __main__ -   Batch number = 16
Evaluating:  29%|██▊       | 16/56 [00:02<00:05,  7.28it/s]11/21/2021 11:13:40 - INFO - __main__ -   Batch number = 17
Evaluating:  30%|███       | 17/56 [00:02<00:05,  7.28it/s]11/21/2021 11:13:40 - INFO - __main__ -   Batch number = 18
Evaluating:  32%|███▏      | 18/56 [00:02<00:05,  7.26it/s]11/21/2021 11:13:40 - INFO - __main__ -   Batch number = 19
Evaluating:  34%|███▍      | 19/56 [00:02<00:05,  7.27it/s]11/21/2021 11:13:40 - INFO - __main__ -   Batch number = 20
Evaluating:  36%|███▌      | 20/56 [00:02<00:04,  7.24it/s]11/21/2021 11:13:41 - INFO - __main__ -   Batch number = 21
Evaluating:  38%|███▊      | 21/56 [00:02<00:04,  7.23it/s]11/21/2021 11:13:41 - INFO - __main__ -   Batch number = 22
Evaluating:  39%|███▉      | 22/56 [00:03<00:04,  7.22it/s]11/21/2021 11:13:41 - INFO - __main__ -   Batch number = 23
Evaluating:  41%|████      | 23/56 [00:03<00:04,  7.22it/s]11/21/2021 11:13:41 - INFO - __main__ -   Batch number = 24
Evaluating:  43%|████▎     | 24/56 [00:03<00:04,  7.21it/s]11/21/2021 11:13:41 - INFO - __main__ -   Batch number = 25
Evaluating:  45%|████▍     | 25/56 [00:03<00:04,  7.21it/s]11/21/2021 11:13:41 - INFO - __main__ -   Batch number = 26
Evaluating:  46%|████▋     | 26/56 [00:03<00:04,  7.22it/s]11/21/2021 11:13:41 - INFO - __main__ -   Batch number = 27
Evaluating:  48%|████▊     | 27/56 [00:03<00:04,  7.22it/s]11/21/2021 11:13:42 - INFO - __main__ -   Batch number = 28
Evaluating:  50%|█████     | 28/56 [00:03<00:03,  7.22it/s]11/21/2021 11:13:42 - INFO - __main__ -   Batch number = 29
Evaluating:  52%|█████▏    | 29/56 [00:04<00:03,  7.22it/s]11/21/2021 11:13:42 - INFO - __main__ -   Batch number = 30
Evaluating:  54%|█████▎    | 30/56 [00:04<00:03,  7.20it/s]11/21/2021 11:13:42 - INFO - __main__ -   Batch number = 31
Evaluating:  55%|█████▌    | 31/56 [00:04<00:03,  7.11it/s]11/21/2021 11:13:42 - INFO - __main__ -   Batch number = 32
Evaluating:  57%|█████▋    | 32/56 [00:04<00:03,  7.12it/s]11/21/2021 11:13:42 - INFO - __main__ -   Batch number = 33
Evaluating:  59%|█████▉    | 33/56 [00:04<00:03,  7.12it/s]11/21/2021 11:13:42 - INFO - __main__ -   Batch number = 34
Evaluating:  61%|██████    | 34/56 [00:04<00:03,  7.15it/s]11/21/2021 11:13:43 - INFO - __main__ -   Batch number = 35
Evaluating:  62%|██████▎   | 35/56 [00:04<00:02,  7.16it/s]11/21/2021 11:13:43 - INFO - __main__ -   Batch number = 36
Evaluating:  64%|██████▍   | 36/56 [00:05<00:02,  7.15it/s]11/21/2021 11:13:43 - INFO - __main__ -   Batch number = 37
Evaluating:  66%|██████▌   | 37/56 [00:05<00:02,  7.15it/s]11/21/2021 11:13:43 - INFO - __main__ -   Batch number = 38
Evaluating:  68%|██████▊   | 38/56 [00:05<00:02,  7.14it/s]11/21/2021 11:13:43 - INFO - __main__ -   Batch number = 39
Evaluating:  70%|██████▉   | 39/56 [00:05<00:02,  6.62it/s]11/21/2021 11:13:43 - INFO - __main__ -   Batch number = 40
Evaluating:  71%|███████▏  | 40/56 [00:05<00:02,  6.78it/s]11/21/2021 11:13:43 - INFO - __main__ -   Batch number = 41
Evaluating:  73%|███████▎  | 41/56 [00:05<00:02,  6.88it/s]11/21/2021 11:13:44 - INFO - __main__ -   Batch number = 42
Evaluating:  75%|███████▌  | 42/56 [00:05<00:02,  6.94it/s]11/21/2021 11:13:44 - INFO - __main__ -   Batch number = 43
Evaluating:  77%|███████▋  | 43/56 [00:06<00:01,  7.00it/s]11/21/2021 11:13:44 - INFO - __main__ -   Batch number = 44
Evaluating:  79%|███████▊  | 44/56 [00:06<00:01,  7.04it/s]11/21/2021 11:13:44 - INFO - __main__ -   Batch number = 45
Evaluating:  80%|████████  | 45/56 [00:06<00:01,  7.06it/s]11/21/2021 11:13:44 - INFO - __main__ -   Batch number = 46
Evaluating:  82%|████████▏ | 46/56 [00:06<00:01,  7.09it/s]11/21/2021 11:13:44 - INFO - __main__ -   Batch number = 47
Evaluating:  84%|████████▍ | 47/56 [00:06<00:01,  7.11it/s]11/21/2021 11:13:44 - INFO - __main__ -   Batch number = 48
Evaluating:  86%|████████▌ | 48/56 [00:06<00:01,  7.11it/s]11/21/2021 11:13:45 - INFO - __main__ -   Batch number = 49
Evaluating:  88%|████████▊ | 49/56 [00:06<00:00,  7.09it/s]11/21/2021 11:13:45 - INFO - __main__ -   Batch number = 50
Evaluating:  89%|████████▉ | 50/56 [00:07<00:00,  7.09it/s]11/21/2021 11:13:45 - INFO - __main__ -   Batch number = 51
Evaluating:  91%|█████████ | 51/56 [00:07<00:00,  7.08it/s]11/21/2021 11:13:45 - INFO - __main__ -   Batch number = 52
Evaluating:  93%|█████████▎| 52/56 [00:07<00:00,  7.09it/s]11/21/2021 11:13:45 - INFO - __main__ -   Batch number = 53
Evaluating:  95%|█████████▍| 53/56 [00:07<00:00,  7.09it/s]11/21/2021 11:13:45 - INFO - __main__ -   Batch number = 54
Evaluating:  96%|█████████▋| 54/56 [00:07<00:00,  7.05it/s]11/21/2021 11:13:45 - INFO - __main__ -   Batch number = 55
Evaluating:  98%|█████████▊| 55/56 [00:07<00:00,  7.05it/s]11/21/2021 11:13:45 - INFO - __main__ -   Batch number = 56
Evaluating: 100%|██████████| 56/56 [00:07<00:00,  7.56it/s]Evaluating: 100%|██████████| 56/56 [00:07<00:00,  7.14it/s]
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/21/2021 11:13:47 - INFO - __main__ -   ***** Evaluation result  in ar *****
11/21/2021 11:13:47 - INFO - __main__ -     f1 = 0.5765629087104369
11/21/2021 11:13:47 - INFO - __main__ -     loss = 1.2875152719872338
11/21/2021 11:13:47 - INFO - __main__ -     precision = 0.5725640959035769
11/21/2021 11:13:47 - INFO - __main__ -     recall = 0.5806179701288097
22.73user 11.36system 0:25.50elapsed 133%CPU (0avgtext+0avgdata 3870748maxresident)k
0inputs+520outputs (0major+1317563minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:04:05 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='hi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:04:05 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:04:05 - INFO - __main__ -   Seed = 1
11/28/2021 01:04:05 - INFO - root -   save model
11/28/2021 01:04:05 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='hi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:04:05 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:04:08 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:04:14 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:04:14 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:04:14 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
11/28/2021 01:04:14 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:04:14 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:04:14 - INFO - root -   loading lang adpater bh/wiki@ukp
11/28/2021 01:04:14 - INFO - __main__ -   Adapter Languages : ['bh'], Length : 1
11/28/2021 01:04:14 - INFO - __main__ -   Adapter Names ['bh/wiki@ukp'], Length : 1
11/28/2021 01:04:14 - INFO - __main__ -   Language = bh
11/28/2021 01:04:14 - INFO - __main__ -   Adapter Name = bh/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/bh/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_bh_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/adapter_config.json
Adding adapter 'bh' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/head_config.json
11/28/2021 01:04:23 - INFO - __main__ -   Language adapter for hi not found, using bh instead
11/28/2021 01:04:23 - INFO - __main__ -   Set active language adapter to bh
11/28/2021 01:04:23 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:04:23 - INFO - __main__ -   Adapter Languages = ['bh']
11/28/2021 01:04:23 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_hi_bert-base-multilingual-cased_128
11/28/2021 01:04:24 - INFO - __main__ -   ***** Running evaluation  in hi *****
11/28/2021 01:04:24 - INFO - __main__ -     Num examples = 2685
11/28/2021 01:04:24 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/84 [00:00<?, ?it/s]11/28/2021 01:04:24 - INFO - __main__ -   Batch number = 1
Evaluating:   1%|          | 1/84 [00:00<00:12,  6.75it/s]11/28/2021 01:04:24 - INFO - __main__ -   Batch number = 2
Evaluating:   2%|▏         | 2/84 [00:00<00:12,  6.64it/s]11/28/2021 01:04:24 - INFO - __main__ -   Batch number = 3
Evaluating:   4%|▎         | 3/84 [00:00<00:12,  6.56it/s]11/28/2021 01:04:24 - INFO - __main__ -   Batch number = 4
Evaluating:   5%|▍         | 4/84 [00:00<00:12,  6.53it/s]11/28/2021 01:04:24 - INFO - __main__ -   Batch number = 5
Evaluating:   6%|▌         | 5/84 [00:00<00:12,  6.28it/s]11/28/2021 01:04:24 - INFO - __main__ -   Batch number = 6
Evaluating:   7%|▋         | 6/84 [00:00<00:12,  6.35it/s]11/28/2021 01:04:24 - INFO - __main__ -   Batch number = 7
Evaluating:   8%|▊         | 7/84 [00:01<00:12,  6.39it/s]11/28/2021 01:04:25 - INFO - __main__ -   Batch number = 8
Evaluating:  10%|▉         | 8/84 [00:01<00:11,  6.42it/s]11/28/2021 01:04:25 - INFO - __main__ -   Batch number = 9
Evaluating:  11%|█         | 9/84 [00:01<00:11,  6.41it/s]11/28/2021 01:04:25 - INFO - __main__ -   Batch number = 10
Evaluating:  12%|█▏        | 10/84 [00:01<00:11,  6.42it/s]11/28/2021 01:04:25 - INFO - __main__ -   Batch number = 11
Evaluating:  13%|█▎        | 11/84 [00:01<00:11,  6.42it/s]11/28/2021 01:04:25 - INFO - __main__ -   Batch number = 12
Evaluating:  14%|█▍        | 12/84 [00:01<00:11,  6.42it/s]11/28/2021 01:04:25 - INFO - __main__ -   Batch number = 13
Evaluating:  15%|█▌        | 13/84 [00:02<00:11,  6.43it/s]11/28/2021 01:04:26 - INFO - __main__ -   Batch number = 14
Evaluating:  17%|█▋        | 14/84 [00:02<00:10,  6.44it/s]11/28/2021 01:04:26 - INFO - __main__ -   Batch number = 15
Evaluating:  18%|█▊        | 15/84 [00:02<00:10,  6.44it/s]11/28/2021 01:04:26 - INFO - __main__ -   Batch number = 16
Evaluating:  19%|█▉        | 16/84 [00:02<00:10,  6.45it/s]11/28/2021 01:04:26 - INFO - __main__ -   Batch number = 17
Evaluating:  20%|██        | 17/84 [00:02<00:10,  6.45it/s]11/28/2021 01:04:26 - INFO - __main__ -   Batch number = 18
Evaluating:  21%|██▏       | 18/84 [00:02<00:10,  6.45it/s]11/28/2021 01:04:26 - INFO - __main__ -   Batch number = 19
Evaluating:  23%|██▎       | 19/84 [00:02<00:10,  6.41it/s]11/28/2021 01:04:27 - INFO - __main__ -   Batch number = 20
Evaluating:  24%|██▍       | 20/84 [00:03<00:14,  4.37it/s]11/28/2021 01:04:27 - INFO - __main__ -   Batch number = 21
Evaluating:  25%|██▌       | 21/84 [00:03<00:12,  4.91it/s]11/28/2021 01:04:27 - INFO - __main__ -   Batch number = 22
Evaluating:  26%|██▌       | 22/84 [00:03<00:11,  5.38it/s]11/28/2021 01:04:27 - INFO - __main__ -   Batch number = 23
Evaluating:  27%|██▋       | 23/84 [00:03<00:10,  5.77it/s]11/28/2021 01:04:27 - INFO - __main__ -   Batch number = 24
Evaluating:  29%|██▊       | 24/84 [00:03<00:09,  6.08it/s]11/28/2021 01:04:27 - INFO - __main__ -   Batch number = 25
Evaluating:  30%|██▉       | 25/84 [00:04<00:09,  6.27it/s]11/28/2021 01:04:28 - INFO - __main__ -   Batch number = 26
Evaluating:  31%|███       | 26/84 [00:04<00:09,  6.44it/s]11/28/2021 01:04:28 - INFO - __main__ -   Batch number = 27
Evaluating:  32%|███▏      | 27/84 [00:04<00:08,  6.59it/s]11/28/2021 01:04:28 - INFO - __main__ -   Batch number = 28
Evaluating:  33%|███▎      | 28/84 [00:04<00:08,  6.68it/s]11/28/2021 01:04:28 - INFO - __main__ -   Batch number = 29
Evaluating:  35%|███▍      | 29/84 [00:04<00:08,  6.74it/s]11/28/2021 01:04:28 - INFO - __main__ -   Batch number = 30
Evaluating:  36%|███▌      | 30/84 [00:04<00:07,  6.76it/s]11/28/2021 01:04:28 - INFO - __main__ -   Batch number = 31
Evaluating:  37%|███▋      | 31/84 [00:04<00:07,  6.68it/s]11/28/2021 01:04:28 - INFO - __main__ -   Batch number = 32
Evaluating:  38%|███▊      | 32/84 [00:05<00:07,  6.65it/s]11/28/2021 01:04:29 - INFO - __main__ -   Batch number = 33
Evaluating:  39%|███▉      | 33/84 [00:05<00:07,  6.63it/s]11/28/2021 01:04:29 - INFO - __main__ -   Batch number = 34
Evaluating:  40%|████      | 34/84 [00:05<00:07,  6.59it/s]11/28/2021 01:04:29 - INFO - __main__ -   Batch number = 35
Evaluating:  42%|████▏     | 35/84 [00:05<00:07,  6.60it/s]11/28/2021 01:04:29 - INFO - __main__ -   Batch number = 36
Evaluating:  43%|████▎     | 36/84 [00:05<00:07,  6.56it/s]11/28/2021 01:04:29 - INFO - __main__ -   Batch number = 37
Evaluating:  44%|████▍     | 37/84 [00:05<00:07,  6.59it/s]11/28/2021 01:04:29 - INFO - __main__ -   Batch number = 38
Evaluating:  45%|████▌     | 38/84 [00:06<00:06,  6.59it/s]11/28/2021 01:04:30 - INFO - __main__ -   Batch number = 39
Evaluating:  46%|████▋     | 39/84 [00:06<00:06,  6.57it/s]11/28/2021 01:04:30 - INFO - __main__ -   Batch number = 40
Evaluating:  48%|████▊     | 40/84 [00:06<00:06,  6.62it/s]11/28/2021 01:04:30 - INFO - __main__ -   Batch number = 41
Evaluating:  49%|████▉     | 41/84 [00:06<00:06,  6.63it/s]11/28/2021 01:04:30 - INFO - __main__ -   Batch number = 42
Evaluating:  50%|█████     | 42/84 [00:06<00:06,  6.63it/s]11/28/2021 01:04:30 - INFO - __main__ -   Batch number = 43
Evaluating:  51%|█████     | 43/84 [00:06<00:06,  6.61it/s]11/28/2021 01:04:30 - INFO - __main__ -   Batch number = 44
Evaluating:  52%|█████▏    | 44/84 [00:06<00:06,  6.58it/s]11/28/2021 01:04:30 - INFO - __main__ -   Batch number = 45
Evaluating:  54%|█████▎    | 45/84 [00:07<00:05,  6.57it/s]11/28/2021 01:04:31 - INFO - __main__ -   Batch number = 46
Evaluating:  55%|█████▍    | 46/84 [00:07<00:05,  6.52it/s]11/28/2021 01:04:31 - INFO - __main__ -   Batch number = 47
Evaluating:  56%|█████▌    | 47/84 [00:07<00:05,  6.44it/s]11/28/2021 01:04:31 - INFO - __main__ -   Batch number = 48
Evaluating:  57%|█████▋    | 48/84 [00:07<00:05,  6.30it/s]11/28/2021 01:04:31 - INFO - __main__ -   Batch number = 49
Evaluating:  58%|█████▊    | 49/84 [00:07<00:05,  6.26it/s]11/28/2021 01:04:31 - INFO - __main__ -   Batch number = 50
Evaluating:  60%|█████▉    | 50/84 [00:07<00:05,  6.29it/s]11/28/2021 01:04:32 - INFO - __main__ -   Batch number = 51
Evaluating:  61%|██████    | 51/84 [00:08<00:05,  5.52it/s]11/28/2021 01:04:32 - INFO - __main__ -   Batch number = 52
Evaluating:  62%|██████▏   | 52/84 [00:08<00:05,  5.85it/s]11/28/2021 01:04:32 - INFO - __main__ -   Batch number = 53
Evaluating:  63%|██████▎   | 53/84 [00:08<00:05,  6.05it/s]11/28/2021 01:04:32 - INFO - __main__ -   Batch number = 54
Evaluating:  64%|██████▍   | 54/84 [00:08<00:04,  6.25it/s]11/28/2021 01:04:32 - INFO - __main__ -   Batch number = 55
Evaluating:  65%|██████▌   | 55/84 [00:08<00:04,  6.36it/s]11/28/2021 01:04:32 - INFO - __main__ -   Batch number = 56
Evaluating:  67%|██████▋   | 56/84 [00:08<00:04,  6.46it/s]11/28/2021 01:04:32 - INFO - __main__ -   Batch number = 57
Evaluating:  68%|██████▊   | 57/84 [00:09<00:04,  6.51it/s]11/28/2021 01:04:33 - INFO - __main__ -   Batch number = 58
Evaluating:  69%|██████▉   | 58/84 [00:09<00:03,  6.54it/s]11/28/2021 01:04:33 - INFO - __main__ -   Batch number = 59
Evaluating:  70%|███████   | 59/84 [00:09<00:03,  6.53it/s]11/28/2021 01:04:33 - INFO - __main__ -   Batch number = 60
Evaluating:  71%|███████▏  | 60/84 [00:09<00:03,  6.46it/s]11/28/2021 01:04:33 - INFO - __main__ -   Batch number = 61
Evaluating:  73%|███████▎  | 61/84 [00:09<00:03,  6.38it/s]11/28/2021 01:04:33 - INFO - __main__ -   Batch number = 62
Evaluating:  74%|███████▍  | 62/84 [00:09<00:03,  6.22it/s]11/28/2021 01:04:33 - INFO - __main__ -   Batch number = 63
Evaluating:  75%|███████▌  | 63/84 [00:09<00:03,  6.18it/s]11/28/2021 01:04:34 - INFO - __main__ -   Batch number = 64
Evaluating:  76%|███████▌  | 64/84 [00:10<00:03,  6.29it/s]11/28/2021 01:04:34 - INFO - __main__ -   Batch number = 65
Evaluating:  77%|███████▋  | 65/84 [00:10<00:03,  6.26it/s]11/28/2021 01:04:34 - INFO - __main__ -   Batch number = 66
Evaluating:  79%|███████▊  | 66/84 [00:10<00:02,  6.31it/s]11/28/2021 01:04:34 - INFO - __main__ -   Batch number = 67
Evaluating:  80%|███████▉  | 67/84 [00:10<00:02,  6.34it/s]11/28/2021 01:04:34 - INFO - __main__ -   Batch number = 68
Evaluating:  81%|████████  | 68/84 [00:10<00:02,  6.34it/s]11/28/2021 01:04:34 - INFO - __main__ -   Batch number = 69
Evaluating:  82%|████████▏ | 69/84 [00:10<00:02,  6.38it/s]11/28/2021 01:04:34 - INFO - __main__ -   Batch number = 70
Evaluating:  83%|████████▎ | 70/84 [00:11<00:02,  6.39it/s]11/28/2021 01:04:35 - INFO - __main__ -   Batch number = 71
Evaluating:  85%|████████▍ | 71/84 [00:11<00:02,  6.45it/s]11/28/2021 01:04:35 - INFO - __main__ -   Batch number = 72
Evaluating:  86%|████████▌ | 72/84 [00:11<00:01,  6.51it/s]11/28/2021 01:04:35 - INFO - __main__ -   Batch number = 73
Evaluating:  87%|████████▋ | 73/84 [00:11<00:01,  6.50it/s]11/28/2021 01:04:35 - INFO - __main__ -   Batch number = 74
Evaluating:  88%|████████▊ | 74/84 [00:11<00:01,  6.52it/s]11/28/2021 01:04:35 - INFO - __main__ -   Batch number = 75
Evaluating:  89%|████████▉ | 75/84 [00:11<00:01,  6.56it/s]11/28/2021 01:04:35 - INFO - __main__ -   Batch number = 76
Evaluating:  90%|█████████ | 76/84 [00:11<00:01,  6.52it/s]11/28/2021 01:04:36 - INFO - __main__ -   Batch number = 77
Evaluating:  92%|█████████▏| 77/84 [00:12<00:01,  6.46it/s]11/28/2021 01:04:36 - INFO - __main__ -   Batch number = 78
Evaluating:  93%|█████████▎| 78/84 [00:12<00:00,  6.41it/s]11/28/2021 01:04:36 - INFO - __main__ -   Batch number = 79
Evaluating:  94%|█████████▍| 79/84 [00:12<00:00,  6.36it/s]11/28/2021 01:04:36 - INFO - __main__ -   Batch number = 80
Evaluating:  95%|█████████▌| 80/84 [00:12<00:00,  6.34it/s]11/28/2021 01:04:36 - INFO - __main__ -   Batch number = 81
Evaluating:  96%|█████████▋| 81/84 [00:12<00:00,  6.33it/s]11/28/2021 01:04:36 - INFO - __main__ -   Batch number = 82
Evaluating:  98%|█████████▊| 82/84 [00:12<00:00,  6.31it/s]11/28/2021 01:04:37 - INFO - __main__ -   Batch number = 83
Evaluating:  99%|█████████▉| 83/84 [00:13<00:00,  4.64it/s]11/28/2021 01:04:37 - INFO - __main__ -   Batch number = 84
Evaluating: 100%|██████████| 84/84 [00:13<00:00,  5.09it/s]Evaluating: 100%|██████████| 84/84 [00:13<00:00,  6.25it/s]
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:04:38 - INFO - __main__ -   ***** Evaluation result  in hi *****
11/28/2021 01:04:38 - INFO - __main__ -     f1 = 0.6062471914409461
11/28/2021 01:04:38 - INFO - __main__ -     loss = 1.260144530307679
11/28/2021 01:04:38 - INFO - __main__ -     precision = 0.6052807422822123
11/28/2021 01:04:38 - INFO - __main__ -     recall = 0.6072167317857964
26.08user 9.32system 0:35.72elapsed 99%CPU (0avgtext+0avgdata 3998392maxresident)k
96inputs+704outputs (0major+1763842minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:04:41 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='hi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:04:41 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:04:41 - INFO - __main__ -   Seed = 2
11/28/2021 01:04:41 - INFO - root -   save model
11/28/2021 01:04:41 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='hi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:04:41 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:04:44 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:04:50 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:04:50 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:04:50 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
11/28/2021 01:04:50 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:04:50 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:04:50 - INFO - root -   loading lang adpater bh/wiki@ukp
11/28/2021 01:04:50 - INFO - __main__ -   Adapter Languages : ['bh'], Length : 1
11/28/2021 01:04:50 - INFO - __main__ -   Adapter Names ['bh/wiki@ukp'], Length : 1
11/28/2021 01:04:50 - INFO - __main__ -   Language = bh
11/28/2021 01:04:50 - INFO - __main__ -   Adapter Name = bh/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/bh/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_bh_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/adapter_config.json
Adding adapter 'bh' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/head_config.json
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:04:55 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='cdo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:04:55 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:04:55 - INFO - __main__ -   Seed = 1
11/28/2021 01:04:55 - INFO - root -   save model
11/28/2021 01:04:55 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='cdo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:04:55 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:04:58 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
11/28/2021 01:04:58 - INFO - __main__ -   Language adapter for hi not found, using bh instead
11/28/2021 01:04:58 - INFO - __main__ -   Set active language adapter to bh
11/28/2021 01:04:58 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:04:58 - INFO - __main__ -   Adapter Languages = ['bh']
11/28/2021 01:04:58 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_hi_bert-base-multilingual-cased_128
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
11/28/2021 01:04:59 - INFO - __main__ -   ***** Running evaluation  in hi *****
11/28/2021 01:04:59 - INFO - __main__ -     Num examples = 2685
11/28/2021 01:04:59 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/84 [00:00<?, ?it/s]11/28/2021 01:04:59 - INFO - __main__ -   Batch number = 1
Evaluating:   1%|          | 1/84 [00:00<00:25,  3.28it/s]11/28/2021 01:04:59 - INFO - __main__ -   Batch number = 2
Evaluating:   2%|▏         | 2/84 [00:00<00:24,  3.33it/s]11/28/2021 01:04:59 - INFO - __main__ -   Batch number = 3
Evaluating:   4%|▎         | 3/84 [00:00<00:24,  3.33it/s]11/28/2021 01:05:00 - INFO - __main__ -   Batch number = 4
Evaluating:   5%|▍         | 4/84 [00:01<00:24,  3.31it/s]11/28/2021 01:05:00 - INFO - __main__ -   Batch number = 5
Evaluating:   6%|▌         | 5/84 [00:01<00:24,  3.28it/s]11/28/2021 01:05:00 - INFO - __main__ -   Batch number = 6
Evaluating:   7%|▋         | 6/84 [00:01<00:23,  3.25it/s]11/28/2021 01:05:01 - INFO - __main__ -   Batch number = 7
Evaluating:   8%|▊         | 7/84 [00:02<00:23,  3.22it/s]11/28/2021 01:05:01 - INFO - __main__ -   Batch number = 8
Evaluating:  10%|▉         | 8/84 [00:02<00:23,  3.18it/s]11/28/2021 01:05:01 - INFO - __main__ -   Batch number = 9
Evaluating:  11%|█         | 9/84 [00:02<00:23,  3.18it/s]11/28/2021 01:05:01 - INFO - __main__ -   Batch number = 10
Evaluating:  12%|█▏        | 10/84 [00:03<00:23,  3.16it/s]11/28/2021 01:05:02 - INFO - __main__ -   Batch number = 11
Evaluating:  13%|█▎        | 11/84 [00:03<00:23,  3.14it/s]11/28/2021 01:05:02 - INFO - __main__ -   Batch number = 12
Evaluating:  14%|█▍        | 12/84 [00:03<00:23,  3.13it/s]11/28/2021 01:05:02 - INFO - __main__ -   Batch number = 13
Evaluating:  15%|█▌        | 13/84 [00:04<00:22,  3.11it/s]11/28/2021 01:05:03 - INFO - __main__ -   Batch number = 14
Evaluating:  17%|█▋        | 14/84 [00:04<00:22,  3.12it/s]11/28/2021 01:05:03 - INFO - __main__ -   Batch number = 15
Evaluating:  18%|█▊        | 15/84 [00:04<00:22,  3.12it/s]11/28/2021 01:05:03 - INFO - __main__ -   Batch number = 16
Evaluating:  19%|█▉        | 16/84 [00:05<00:21,  3.12it/s]11/28/2021 01:05:04 - INFO - __main__ -   Batch number = 17
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:05:04 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:05:04 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:05:04 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
11/28/2021 01:05:04 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:05:04 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:05:04 - INFO - root -   loading lang adpater bh/wiki@ukp
11/28/2021 01:05:04 - INFO - __main__ -   Adapter Languages : ['bh'], Length : 1
11/28/2021 01:05:04 - INFO - __main__ -   Adapter Names ['bh/wiki@ukp'], Length : 1
11/28/2021 01:05:04 - INFO - __main__ -   Language = bh
11/28/2021 01:05:04 - INFO - __main__ -   Adapter Name = bh/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/bh/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_bh_wiki_pfeiffer.zip.
Evaluating:  20%|██        | 17/84 [00:05<00:21,  3.11it/s]11/28/2021 01:05:04 - INFO - __main__ -   Batch number = 18
Evaluating:  21%|██▏       | 18/84 [00:05<00:21,  3.10it/s]11/28/2021 01:05:04 - INFO - __main__ -   Batch number = 19
Evaluating:  23%|██▎       | 19/84 [00:06<00:20,  3.10it/s]11/28/2021 01:05:05 - INFO - __main__ -   Batch number = 20
Evaluating:  24%|██▍       | 20/84 [00:06<00:20,  3.11it/s]11/28/2021 01:05:05 - INFO - __main__ -   Batch number = 21
Evaluating:  25%|██▌       | 21/84 [00:06<00:20,  3.11it/s]11/28/2021 01:05:05 - INFO - __main__ -   Batch number = 22
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/adapter_config.json
Adding adapter 'bh' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/head_config.json
Evaluating:  26%|██▌       | 22/84 [00:06<00:20,  3.09it/s]11/28/2021 01:05:06 - INFO - __main__ -   Batch number = 23
Evaluating:  27%|██▋       | 23/84 [00:07<00:19,  3.07it/s]11/28/2021 01:05:06 - INFO - __main__ -   Batch number = 24
Evaluating:  29%|██▊       | 24/84 [00:07<00:19,  3.09it/s]11/28/2021 01:05:06 - INFO - __main__ -   Batch number = 25
Evaluating:  30%|██▉       | 25/84 [00:07<00:18,  3.17it/s]11/28/2021 01:05:07 - INFO - __main__ -   Batch number = 26
Evaluating:  31%|███       | 26/84 [00:08<00:18,  3.15it/s]11/28/2021 01:05:07 - INFO - __main__ -   Batch number = 27
Evaluating:  32%|███▏      | 27/84 [00:08<00:18,  3.14it/s]11/28/2021 01:05:07 - INFO - __main__ -   Batch number = 28
Evaluating:  33%|███▎      | 28/84 [00:08<00:17,  3.12it/s]11/28/2021 01:05:08 - INFO - __main__ -   Batch number = 29
Evaluating:  35%|███▍      | 29/84 [00:09<00:17,  3.11it/s]11/28/2021 01:05:08 - INFO - __main__ -   Batch number = 30
Evaluating:  36%|███▌      | 30/84 [00:09<00:17,  3.09it/s]11/28/2021 01:05:08 - INFO - __main__ -   Batch number = 31
Evaluating:  37%|███▋      | 31/84 [00:09<00:17,  3.08it/s]11/28/2021 01:05:09 - INFO - __main__ -   Batch number = 32
Evaluating:  38%|███▊      | 32/84 [00:10<00:16,  3.08it/s]11/28/2021 01:05:09 - INFO - __main__ -   Batch number = 33
Evaluating:  39%|███▉      | 33/84 [00:10<00:16,  3.09it/s]11/28/2021 01:05:09 - INFO - __main__ -   Batch number = 34
Evaluating:  40%|████      | 34/84 [00:10<00:15,  3.13it/s]11/28/2021 01:05:10 - INFO - __main__ -   Batch number = 35
Evaluating:  42%|████▏     | 35/84 [00:11<00:15,  3.10it/s]11/28/2021 01:05:10 - INFO - __main__ -   Batch number = 36
Evaluating:  43%|████▎     | 36/84 [00:11<00:15,  3.08it/s]11/28/2021 01:05:10 - INFO - __main__ -   Batch number = 37
Evaluating:  44%|████▍     | 37/84 [00:11<00:15,  3.07it/s]11/28/2021 01:05:11 - INFO - __main__ -   Batch number = 38
Evaluating:  45%|████▌     | 38/84 [00:12<00:15,  3.05it/s]11/28/2021 01:05:11 - INFO - __main__ -   Batch number = 39
Evaluating:  46%|████▋     | 39/84 [00:12<00:14,  3.05it/s]11/28/2021 01:05:11 - INFO - __main__ -   Batch number = 40
Evaluating:  48%|████▊     | 40/84 [00:12<00:14,  3.05it/s]11/28/2021 01:05:12 - INFO - __main__ -   Batch number = 41
Evaluating:  49%|████▉     | 41/84 [00:13<00:14,  2.99it/s]11/28/2021 01:05:12 - INFO - __main__ -   Batch number = 42
Evaluating:  50%|█████     | 42/84 [00:13<00:13,  3.07it/s]11/28/2021 01:05:12 - INFO - __main__ -   Batch number = 43
Evaluating:  51%|█████     | 43/84 [00:13<00:13,  3.11it/s]11/28/2021 01:05:12 - INFO - __main__ -   Batch number = 44
Evaluating:  52%|█████▏    | 44/84 [00:14<00:12,  3.15it/s]11/28/2021 01:05:13 - INFO - __main__ -   Batch number = 45
Evaluating:  54%|█████▎    | 45/84 [00:14<00:12,  3.16it/s]11/28/2021 01:05:13 - INFO - __main__ -   Batch number = 46
Evaluating:  55%|█████▍    | 46/84 [00:14<00:12,  3.15it/s]11/28/2021 01:05:13 - INFO - __main__ -   Batch number = 47
Evaluating:  56%|█████▌    | 47/84 [00:15<00:11,  3.14it/s]11/28/2021 01:05:14 - INFO - __main__ -   Batch number = 48
11/28/2021 01:05:14 - INFO - __main__ -   Language cdo, split test does not exist
Evaluating:  57%|█████▋    | 48/84 [00:15<00:11,  3.13it/s]11/28/2021 01:05:14 - INFO - __main__ -   Batch number = 49
Evaluating:  58%|█████▊    | 49/84 [00:15<00:11,  3.13it/s]11/28/2021 01:05:14 - INFO - __main__ -   Batch number = 50
15.73user 6.27system 0:21.35elapsed 103%CPU (0avgtext+0avgdata 3988636maxresident)k
0inputs+48outputs (0major+1578705minor)pagefaults 0swaps
Evaluating:  60%|█████▉    | 50/84 [00:16<00:10,  3.10it/s]11/28/2021 01:05:15 - INFO - __main__ -   Batch number = 51
Evaluating:  61%|██████    | 51/84 [00:16<00:10,  3.12it/s]11/28/2021 01:05:15 - INFO - __main__ -   Batch number = 52
Evaluating:  62%|██████▏   | 52/84 [00:16<00:10,  3.12it/s]11/28/2021 01:05:15 - INFO - __main__ -   Batch number = 53
Evaluating:  63%|██████▎   | 53/84 [00:16<00:09,  3.12it/s]11/28/2021 01:05:16 - INFO - __main__ -   Batch number = 54
Evaluating:  64%|██████▍   | 54/84 [00:17<00:09,  3.15it/s]11/28/2021 01:05:16 - INFO - __main__ -   Batch number = 55
PyTorch version 1.10.0+cu102 available.
Evaluating:  65%|██████▌   | 55/84 [00:17<00:09,  3.18it/s]11/28/2021 01:05:16 - INFO - __main__ -   Batch number = 56
11/28/2021 01:05:16 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='cdo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:05:16 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:05:16 - INFO - __main__ -   Seed = 2
11/28/2021 01:05:16 - INFO - root -   save model
11/28/2021 01:05:16 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='cdo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:05:16 - INFO - __main__ -   Loading pretrained model and tokenizer
Evaluating:  67%|██████▋   | 56/84 [00:17<00:08,  3.39it/s]11/28/2021 01:05:17 - INFO - __main__ -   Batch number = 57
Evaluating:  68%|██████▊   | 57/84 [00:18<00:08,  3.31it/s]11/28/2021 01:05:17 - INFO - __main__ -   Batch number = 58
Evaluating:  69%|██████▉   | 58/84 [00:18<00:07,  3.26it/s]11/28/2021 01:05:17 - INFO - __main__ -   Batch number = 59
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  70%|███████   | 59/84 [00:18<00:07,  3.21it/s]11/28/2021 01:05:17 - INFO - __main__ -   Batch number = 60
Evaluating:  71%|███████▏  | 60/84 [00:19<00:07,  3.18it/s]11/28/2021 01:05:18 - INFO - __main__ -   Batch number = 61
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  73%|███████▎  | 61/84 [00:19<00:07,  3.14it/s]11/28/2021 01:05:18 - INFO - __main__ -   Batch number = 62
Evaluating:  74%|███████▍  | 62/84 [00:19<00:06,  3.15it/s]11/28/2021 01:05:18 - INFO - __main__ -   Batch number = 63
Evaluating:  75%|███████▌  | 63/84 [00:20<00:06,  3.17it/s]11/28/2021 01:05:19 - INFO - __main__ -   Batch number = 64
loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
Evaluating:  76%|███████▌  | 64/84 [00:20<00:06,  3.11it/s]11/28/2021 01:05:19 - INFO - __main__ -   Batch number = 65
11/28/2021 01:05:19 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
Evaluating:  77%|███████▋  | 65/84 [00:20<00:06,  3.15it/s]11/28/2021 01:05:19 - INFO - __main__ -   Batch number = 66
Evaluating:  79%|███████▊  | 66/84 [00:21<00:05,  3.14it/s]11/28/2021 01:05:20 - INFO - __main__ -   Batch number = 67
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  80%|███████▉  | 67/84 [00:21<00:05,  3.16it/s]11/28/2021 01:05:20 - INFO - __main__ -   Batch number = 68
Evaluating:  81%|████████  | 68/84 [00:21<00:05,  3.15it/s]11/28/2021 01:05:20 - INFO - __main__ -   Batch number = 69
Evaluating:  82%|████████▏ | 69/84 [00:21<00:04,  3.13it/s]11/28/2021 01:05:21 - INFO - __main__ -   Batch number = 70
Evaluating:  83%|████████▎ | 70/84 [00:22<00:04,  3.11it/s]11/28/2021 01:05:21 - INFO - __main__ -   Batch number = 71
Evaluating:  85%|████████▍ | 71/84 [00:22<00:04,  3.14it/s]11/28/2021 01:05:21 - INFO - __main__ -   Batch number = 72
Evaluating:  86%|████████▌ | 72/84 [00:22<00:03,  3.13it/s]11/28/2021 01:05:22 - INFO - __main__ -   Batch number = 73
Evaluating:  87%|████████▋ | 73/84 [00:23<00:03,  3.16it/s]11/28/2021 01:05:22 - INFO - __main__ -   Batch number = 74
Evaluating:  88%|████████▊ | 74/84 [00:23<00:03,  3.14it/s]11/28/2021 01:05:22 - INFO - __main__ -   Batch number = 75
Evaluating:  89%|████████▉ | 75/84 [00:23<00:02,  3.17it/s]11/28/2021 01:05:23 - INFO - __main__ -   Batch number = 76
Evaluating:  90%|█████████ | 76/84 [00:24<00:02,  3.15it/s]11/28/2021 01:05:23 - INFO - __main__ -   Batch number = 77
Evaluating:  92%|█████████▏| 77/84 [00:24<00:02,  3.18it/s]11/28/2021 01:05:23 - INFO - __main__ -   Batch number = 78
Evaluating:  93%|█████████▎| 78/84 [00:24<00:01,  3.21it/s]11/28/2021 01:05:24 - INFO - __main__ -   Batch number = 79
Evaluating:  94%|█████████▍| 79/84 [00:25<00:01,  3.27it/s]11/28/2021 01:05:24 - INFO - __main__ -   Batch number = 80
Evaluating:  95%|█████████▌| 80/84 [00:25<00:01,  2.80it/s]11/28/2021 01:05:24 - INFO - __main__ -   Batch number = 81
Evaluating:  96%|█████████▋| 81/84 [00:25<00:01,  2.91it/s]11/28/2021 01:05:25 - INFO - __main__ -   Batch number = 82
Evaluating:  98%|█████████▊| 82/84 [00:26<00:00,  3.02it/s]11/28/2021 01:05:25 - INFO - __main__ -   Batch number = 83
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:05:25 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:05:25 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:05:25 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
11/28/2021 01:05:25 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:05:25 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:05:25 - INFO - root -   loading lang adpater bh/wiki@ukp
11/28/2021 01:05:25 - INFO - __main__ -   Adapter Languages : ['bh'], Length : 1
11/28/2021 01:05:25 - INFO - __main__ -   Adapter Names ['bh/wiki@ukp'], Length : 1
11/28/2021 01:05:25 - INFO - __main__ -   Language = bh
11/28/2021 01:05:25 - INFO - __main__ -   Adapter Name = bh/wiki@ukp
Evaluating:  99%|█████████▉| 83/84 [00:26<00:00,  3.07it/s]11/28/2021 01:05:25 - INFO - __main__ -   Batch number = 84
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/bh/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_bh_wiki_pfeiffer.zip.
Evaluating: 100%|██████████| 84/84 [00:26<00:00,  3.16it/s]Evaluating: 100%|██████████| 84/84 [00:26<00:00,  3.13it/s]
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:05:27 - INFO - __main__ -   ***** Evaluation result  in hi *****
11/28/2021 01:05:27 - INFO - __main__ -     f1 = 0.5607254005455874
11/28/2021 01:05:27 - INFO - __main__ -     loss = 1.4815663638569059
11/28/2021 01:05:27 - INFO - __main__ -     precision = 0.5624325574225374
11/28/2021 01:05:27 - INFO - __main__ -     recall = 0.559028575806328
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/adapter_config.json
Adding adapter 'bh' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/head_config.json
37.17user 14.17system 0:48.71elapsed 105%CPU (0avgtext+0avgdata 3992836maxresident)k
0inputs+696outputs (0major+1649965minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:05:30 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='hi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:05:30 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:05:30 - INFO - __main__ -   Seed = 3
11/28/2021 01:05:30 - INFO - root -   save model
11/28/2021 01:05:30 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='hi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:05:30 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:05:33 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
11/28/2021 01:05:35 - INFO - __main__ -   Language cdo, split test does not exist
14.88user 5.33system 0:20.85elapsed 96%CPU (0avgtext+0avgdata 3987764maxresident)k
0inputs+24outputs (0major+1380496minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:05:37 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='cdo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:05:37 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:05:37 - INFO - __main__ -   Seed = 3
11/28/2021 01:05:37 - INFO - root -   save model
11/28/2021 01:05:37 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='cdo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:05:37 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:05:39 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:05:39 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:05:39 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
11/28/2021 01:05:39 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:05:39 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:05:39 - INFO - root -   loading lang adpater bh/wiki@ukp
11/28/2021 01:05:39 - INFO - __main__ -   Adapter Languages : ['bh'], Length : 1
11/28/2021 01:05:39 - INFO - __main__ -   Adapter Names ['bh/wiki@ukp'], Length : 1
11/28/2021 01:05:39 - INFO - __main__ -   Language = bh
11/28/2021 01:05:39 - INFO - __main__ -   Adapter Name = bh/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/bh/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_bh_wiki_pfeiffer.zip.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:05:40 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/adapter_config.json
Adding adapter 'bh' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/head_config.json
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:05:46 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:05:46 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:05:46 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
11/28/2021 01:05:46 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:05:46 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:05:46 - INFO - root -   loading lang adpater bh/wiki@ukp
11/28/2021 01:05:46 - INFO - __main__ -   Adapter Languages : ['bh'], Length : 1
11/28/2021 01:05:46 - INFO - __main__ -   Adapter Names ['bh/wiki@ukp'], Length : 1
11/28/2021 01:05:46 - INFO - __main__ -   Language = bh
11/28/2021 01:05:46 - INFO - __main__ -   Adapter Name = bh/wiki@ukp
11/28/2021 01:05:46 - INFO - __main__ -   Language adapter for hi not found, using bh instead
11/28/2021 01:05:46 - INFO - __main__ -   Set active language adapter to bh
11/28/2021 01:05:46 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:05:46 - INFO - __main__ -   Adapter Languages = ['bh']
11/28/2021 01:05:46 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_hi_bert-base-multilingual-cased_128
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/bh/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_bh_wiki_pfeiffer.zip.
11/28/2021 01:05:47 - INFO - __main__ -   ***** Running evaluation  in hi *****
11/28/2021 01:05:47 - INFO - __main__ -     Num examples = 2685
11/28/2021 01:05:47 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/84 [00:00<?, ?it/s]11/28/2021 01:05:47 - INFO - __main__ -   Batch number = 1
Evaluating:   1%|          | 1/84 [00:00<00:13,  6.25it/s]11/28/2021 01:05:47 - INFO - __main__ -   Batch number = 2
Evaluating:   2%|▏         | 2/84 [00:00<00:12,  6.65it/s]11/28/2021 01:05:47 - INFO - __main__ -   Batch number = 3
Evaluating:   4%|▎         | 3/84 [00:00<00:12,  6.69it/s]11/28/2021 01:05:47 - INFO - __main__ -   Batch number = 4
Evaluating:   5%|▍         | 4/84 [00:00<00:11,  6.68it/s]11/28/2021 01:05:47 - INFO - __main__ -   Batch number = 5
Evaluating:   6%|▌         | 5/84 [00:00<00:11,  6.66it/s]11/28/2021 01:05:47 - INFO - __main__ -   Batch number = 6
Evaluating:   7%|▋         | 6/84 [00:00<00:11,  6.61it/s]11/28/2021 01:05:48 - INFO - __main__ -   Batch number = 7
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/adapter_config.json
Adding adapter 'bh' of type 'text_lang'.
Evaluating:   8%|▊         | 7/84 [00:01<00:11,  6.56it/s]11/28/2021 01:05:48 - INFO - __main__ -   Batch number = 8
Loading module weights from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/head_config.json
Evaluating:  10%|▉         | 8/84 [00:01<00:11,  6.53it/s]11/28/2021 01:05:48 - INFO - __main__ -   Batch number = 9
Evaluating:  11%|█         | 9/84 [00:01<00:11,  6.43it/s]11/28/2021 01:05:48 - INFO - __main__ -   Batch number = 10
Evaluating:  12%|█▏        | 10/84 [00:01<00:11,  6.37it/s]11/28/2021 01:05:48 - INFO - __main__ -   Batch number = 11
Evaluating:  13%|█▎        | 11/84 [00:01<00:11,  6.31it/s]11/28/2021 01:05:48 - INFO - __main__ -   Batch number = 12
Evaluating:  14%|█▍        | 12/84 [00:01<00:11,  6.27it/s]11/28/2021 01:05:49 - INFO - __main__ -   Batch number = 13
Evaluating:  15%|█▌        | 13/84 [00:02<00:11,  6.28it/s]11/28/2021 01:05:49 - INFO - __main__ -   Batch number = 14
Evaluating:  17%|█▋        | 14/84 [00:02<00:11,  6.25it/s]11/28/2021 01:05:49 - INFO - __main__ -   Batch number = 15
Evaluating:  18%|█▊        | 15/84 [00:02<00:11,  6.25it/s]11/28/2021 01:05:49 - INFO - __main__ -   Batch number = 16
Evaluating:  19%|█▉        | 16/84 [00:02<00:11,  6.17it/s]11/28/2021 01:05:49 - INFO - __main__ -   Batch number = 17
Evaluating:  20%|██        | 17/84 [00:02<00:10,  6.12it/s]11/28/2021 01:05:49 - INFO - __main__ -   Batch number = 18
Evaluating:  21%|██▏       | 18/84 [00:02<00:10,  6.17it/s]11/28/2021 01:05:50 - INFO - __main__ -   Batch number = 19
PyTorch version 1.10.0+cu102 available.
Evaluating:  23%|██▎       | 19/84 [00:02<00:10,  6.13it/s]11/28/2021 01:05:50 - INFO - __main__ -   Batch number = 20
Evaluating:  24%|██▍       | 20/84 [00:03<00:10,  6.13it/s]11/28/2021 01:05:50 - INFO - __main__ -   Batch number = 21
Evaluating:  25%|██▌       | 21/84 [00:03<00:10,  6.19it/s]11/28/2021 01:05:50 - INFO - __main__ -   Batch number = 22
11/28/2021 01:05:50 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ru', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:05:50 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:05:50 - INFO - __main__ -   Seed = 1
11/28/2021 01:05:50 - INFO - root -   save model
11/28/2021 01:05:50 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ru', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:05:50 - INFO - __main__ -   Loading pretrained model and tokenizer
Evaluating:  26%|██▌       | 22/84 [00:03<00:09,  6.20it/s]11/28/2021 01:05:50 - INFO - __main__ -   Batch number = 23
Evaluating:  27%|██▋       | 23/84 [00:03<00:09,  6.22it/s]11/28/2021 01:05:50 - INFO - __main__ -   Batch number = 24
Evaluating:  29%|██▊       | 24/84 [00:03<00:09,  6.22it/s]11/28/2021 01:05:51 - INFO - __main__ -   Batch number = 25
Evaluating:  30%|██▉       | 25/84 [00:03<00:09,  6.25it/s]11/28/2021 01:05:51 - INFO - __main__ -   Batch number = 26
Evaluating:  31%|███       | 26/84 [00:04<00:09,  6.31it/s]11/28/2021 01:05:51 - INFO - __main__ -   Batch number = 27
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  32%|███▏      | 27/84 [00:04<00:09,  6.33it/s]11/28/2021 01:05:51 - INFO - __main__ -   Batch number = 28
Evaluating:  33%|███▎      | 28/84 [00:04<00:08,  6.35it/s]11/28/2021 01:05:51 - INFO - __main__ -   Batch number = 29
Evaluating:  35%|███▍      | 29/84 [00:04<00:08,  6.30it/s]11/28/2021 01:05:51 - INFO - __main__ -   Batch number = 30
Evaluating:  36%|███▌      | 30/84 [00:04<00:08,  6.27it/s]11/28/2021 01:05:51 - INFO - __main__ -   Batch number = 31
Evaluating:  37%|███▋      | 31/84 [00:04<00:08,  6.25it/s]11/28/2021 01:05:52 - INFO - __main__ -   Batch number = 32
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  38%|███▊      | 32/84 [00:05<00:08,  6.22it/s]11/28/2021 01:05:52 - INFO - __main__ -   Batch number = 33
Evaluating:  39%|███▉      | 33/84 [00:05<00:08,  6.21it/s]11/28/2021 01:05:52 - INFO - __main__ -   Batch number = 34
Evaluating:  40%|████      | 34/84 [00:05<00:08,  6.20it/s]11/28/2021 01:05:52 - INFO - __main__ -   Batch number = 35
Evaluating:  42%|████▏     | 35/84 [00:05<00:07,  6.16it/s]11/28/2021 01:05:52 - INFO - __main__ -   Batch number = 36
Evaluating:  43%|████▎     | 36/84 [00:05<00:07,  6.18it/s]11/28/2021 01:05:52 - INFO - __main__ -   Batch number = 37
loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
Evaluating:  44%|████▍     | 37/84 [00:05<00:07,  6.14it/s]11/28/2021 01:05:53 - INFO - __main__ -   Batch number = 38
Evaluating:  45%|████▌     | 38/84 [00:06<00:07,  6.12it/s]11/28/2021 01:05:53 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
11/28/2021 01:05:53 - INFO - __main__ -   Batch number = 39
Evaluating:  46%|████▋     | 39/84 [00:06<00:07,  6.13it/s]11/28/2021 01:05:53 - INFO - __main__ -   Batch number = 40
Evaluating:  48%|████▊     | 40/84 [00:06<00:07,  6.13it/s]11/28/2021 01:05:53 - INFO - __main__ -   Batch number = 41
Evaluating:  49%|████▉     | 41/84 [00:06<00:06,  6.23it/s]11/28/2021 01:05:53 - INFO - __main__ -   Batch number = 42
Evaluating:  50%|█████     | 42/84 [00:06<00:06,  6.31it/s]11/28/2021 01:05:53 - INFO - __main__ -   Batch number = 43
Evaluating:  51%|█████     | 43/84 [00:06<00:06,  6.30it/s]11/28/2021 01:05:54 - INFO - __main__ -   Batch number = 44
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  52%|█████▏    | 44/84 [00:07<00:06,  6.27it/s]11/28/2021 01:05:54 - INFO - __main__ -   Batch number = 45
Evaluating:  54%|█████▎    | 45/84 [00:07<00:06,  6.23it/s]11/28/2021 01:05:54 - INFO - __main__ -   Batch number = 46
Evaluating:  55%|█████▍    | 46/84 [00:07<00:06,  6.24it/s]11/28/2021 01:05:54 - INFO - __main__ -   Batch number = 47
Evaluating:  56%|█████▌    | 47/84 [00:07<00:05,  6.24it/s]11/28/2021 01:05:54 - INFO - __main__ -   Batch number = 48
Evaluating:  57%|█████▋    | 48/84 [00:07<00:05,  6.22it/s]11/28/2021 01:05:54 - INFO - __main__ -   Batch number = 49
Evaluating:  58%|█████▊    | 49/84 [00:07<00:05,  6.13it/s]11/28/2021 01:05:55 - INFO - __main__ -   Batch number = 50
Evaluating:  60%|█████▉    | 50/84 [00:07<00:05,  6.11it/s]11/28/2021 01:05:55 - INFO - __main__ -   Batch number = 51
PyTorch version 1.10.0+cu102 available.
Evaluating:  61%|██████    | 51/84 [00:08<00:05,  6.12it/s]11/28/2021 01:05:55 - INFO - __main__ -   Batch number = 52
11/28/2021 01:05:55 - INFO - __main__ -   Language cdo, split test does not exist
Evaluating:  62%|██████▏   | 52/84 [00:08<00:05,  6.03it/s]11/28/2021 01:05:55 - INFO - __main__ -   Batch number = 53
11/28/2021 01:05:55 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='gn', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:05:55 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:05:55 - INFO - __main__ -   Seed = 1
11/28/2021 01:05:55 - INFO - root -   save model
11/28/2021 01:05:55 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='gn', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:05:55 - INFO - __main__ -   Loading pretrained model and tokenizer
Evaluating:  63%|██████▎   | 53/84 [00:08<00:05,  6.06it/s]11/28/2021 01:05:55 - INFO - __main__ -   Batch number = 54
Evaluating:  64%|██████▍   | 54/84 [00:08<00:04,  6.17it/s]11/28/2021 01:05:55 - INFO - __main__ -   Batch number = 55
Evaluating:  65%|██████▌   | 55/84 [00:08<00:04,  6.24it/s]11/28/2021 01:05:55 - INFO - __main__ -   Batch number = 56
Evaluating:  67%|██████▋   | 56/84 [00:08<00:04,  6.29it/s]11/28/2021 01:05:56 - INFO - __main__ -   Batch number = 57
Evaluating:  68%|██████▊   | 57/84 [00:09<00:04,  6.29it/s]11/28/2021 01:05:56 - INFO - __main__ -   Batch number = 58
14.79user 6.11system 0:20.30elapsed 102%CPU (0avgtext+0avgdata 3999996maxresident)k
0inputs+56outputs (0major+1432538minor)pagefaults 0swaps
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  69%|██████▉   | 58/84 [00:09<00:04,  6.29it/s]11/28/2021 01:05:56 - INFO - __main__ -   Batch number = 59
Evaluating:  70%|███████   | 59/84 [00:09<00:03,  6.29it/s]11/28/2021 01:05:56 - INFO - __main__ -   Batch number = 60
Evaluating:  71%|███████▏  | 60/84 [00:09<00:03,  6.34it/s]11/28/2021 01:05:56 - INFO - __main__ -   Batch number = 61
Evaluating:  73%|███████▎  | 61/84 [00:09<00:03,  6.40it/s]11/28/2021 01:05:56 - INFO - __main__ -   Batch number = 62
Evaluating:  74%|███████▍  | 62/84 [00:09<00:03,  6.45it/s]11/28/2021 01:05:57 - INFO - __main__ -   Batch number = 63
Evaluating:  75%|███████▌  | 63/84 [00:10<00:03,  6.48it/s]11/28/2021 01:05:57 - INFO - __main__ -   Batch number = 64
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  76%|███████▌  | 64/84 [00:10<00:03,  6.52it/s]11/28/2021 01:05:57 - INFO - __main__ -   Batch number = 65
Evaluating:  77%|███████▋  | 65/84 [00:10<00:02,  6.53it/s]11/28/2021 01:05:57 - INFO - __main__ -   Batch number = 66
Evaluating:  79%|███████▊  | 66/84 [00:10<00:02,  6.53it/s]11/28/2021 01:05:57 - INFO - __main__ -   Batch number = 67
Evaluating:  80%|███████▉  | 67/84 [00:10<00:02,  6.51it/s]11/28/2021 01:05:57 - INFO - __main__ -   Batch number = 68
Evaluating:  81%|████████  | 68/84 [00:10<00:02,  6.48it/s]11/28/2021 01:05:58 - INFO - __main__ -   Batch number = 69
loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
Evaluating:  82%|████████▏ | 69/84 [00:10<00:02,  6.43it/s]11/28/2021 01:05:58 - INFO - __main__ -   Batch number = 70
Evaluating:  83%|████████▎ | 70/84 [00:11<00:02,  6.39it/s]11/28/2021 01:05:58 - INFO - __main__ -   Batch number = 71
11/28/2021 01:05:58 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
Evaluating:  85%|████████▍ | 71/84 [00:11<00:02,  6.38it/s]11/28/2021 01:05:58 - INFO - __main__ -   Batch number = 72
Evaluating:  86%|████████▌ | 72/84 [00:11<00:01,  6.39it/s]11/28/2021 01:05:58 - INFO - __main__ -   Batch number = 73
Evaluating:  87%|████████▋ | 73/84 [00:11<00:01,  6.40it/s]11/28/2021 01:05:58 - INFO - __main__ -   Batch number = 74
Evaluating:  88%|████████▊ | 74/84 [00:11<00:01,  6.42it/s]11/28/2021 01:05:58 - INFO - __main__ -   Batch number = 75
Evaluating:  89%|████████▉ | 75/84 [00:11<00:01,  6.47it/s]11/28/2021 01:05:59 - INFO - __main__ -   Batch number = 76
Evaluating:  90%|█████████ | 76/84 [00:12<00:01,  6.51it/s]11/28/2021 01:05:59 - INFO - __main__ -   Batch number = 77
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:05:59 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:05:59 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:05:59 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
11/28/2021 01:05:59 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:05:59 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Evaluating:  92%|█████████▏| 77/84 [00:12<00:01,  6.55it/s]11/28/2021 01:05:59 - INFO - __main__ -   Batch number = 78
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:05:59 - INFO - root -   loading lang adpater bh/wiki@ukp
11/28/2021 01:05:59 - INFO - __main__ -   Adapter Languages : ['bh'], Length : 1
11/28/2021 01:05:59 - INFO - __main__ -   Adapter Names ['bh/wiki@ukp'], Length : 1
11/28/2021 01:05:59 - INFO - __main__ -   Language = bh
11/28/2021 01:05:59 - INFO - __main__ -   Adapter Name = bh/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/bh/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_bh_wiki_pfeiffer.zip.
Evaluating:  93%|█████████▎| 78/84 [00:12<00:00,  6.57it/s]11/28/2021 01:05:59 - INFO - __main__ -   Batch number = 79
Evaluating:  94%|█████████▍| 79/84 [00:12<00:00,  6.62it/s]11/28/2021 01:05:59 - INFO - __main__ -   Batch number = 80
Evaluating:  95%|█████████▌| 80/84 [00:12<00:00,  6.63it/s]11/28/2021 01:05:59 - INFO - __main__ -   Batch number = 81
Evaluating:  96%|█████████▋| 81/84 [00:12<00:00,  6.65it/s]11/28/2021 01:06:00 - INFO - __main__ -   Batch number = 82
Evaluating:  98%|█████████▊| 82/84 [00:12<00:00,  6.67it/s]11/28/2021 01:06:00 - INFO - __main__ -   Batch number = 83
Evaluating:  99%|█████████▉| 83/84 [00:13<00:00,  6.70it/s]11/28/2021 01:06:00 - INFO - __main__ -   Batch number = 84
Evaluating: 100%|██████████| 84/84 [00:13<00:00,  6.82it/s]Evaluating: 100%|██████████| 84/84 [00:13<00:00,  6.35it/s]Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/adapter_config.json
Adding adapter 'bh' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/head_config.json

/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:06:01 - INFO - __main__ -   ***** Evaluation result  in hi *****
11/28/2021 01:06:01 - INFO - __main__ -     f1 = 0.5659486345777931
11/28/2021 01:06:01 - INFO - __main__ -     loss = 1.4649267480486916
11/28/2021 01:06:01 - INFO - __main__ -     precision = 0.5719594726753511
11/28/2021 01:06:01 - INFO - __main__ -     recall = 0.5600628208074772
26.51user 10.33system 0:34.05elapsed 108%CPU (0avgtext+0avgdata 3987420maxresident)k
0inputs+664outputs (0major+1581401minor)pagefaults 0swaps
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:06:04 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:06:04 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:06:04 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
11/28/2021 01:06:04 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:06:04 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:06:04 - INFO - root -   loading lang adpater bh/wiki@ukp
11/28/2021 01:06:04 - INFO - __main__ -   Adapter Languages : ['bh'], Length : 1
11/28/2021 01:06:04 - INFO - __main__ -   Adapter Names ['bh/wiki@ukp'], Length : 1
11/28/2021 01:06:04 - INFO - __main__ -   Language = bh
11/28/2021 01:06:04 - INFO - __main__ -   Adapter Name = bh/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/bh/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_bh_wiki_pfeiffer.zip.
11/28/2021 01:06:05 - INFO - __main__ -   Language adapter for ru not found, using bh instead
11/28/2021 01:06:05 - INFO - __main__ -   Set active language adapter to bh
11/28/2021 01:06:05 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:06:05 - INFO - __main__ -   Adapter Languages = ['bh']
11/28/2021 01:06:05 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_ru_bert-base-multilingual-cased_128
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/adapter_config.json
Adding adapter 'bh' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/head_config.json
11/28/2021 01:06:06 - INFO - __main__ -   ***** Running evaluation  in ru *****
11/28/2021 01:06:06 - INFO - __main__ -     Num examples = 8995
11/28/2021 01:06:06 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/282 [00:00<?, ?it/s]11/28/2021 01:06:06 - INFO - __main__ -   Batch number = 1
Evaluating:   0%|          | 1/282 [00:00<00:44,  6.31it/s]11/28/2021 01:06:06 - INFO - __main__ -   Batch number = 2
Evaluating:   1%|          | 2/282 [00:00<00:41,  6.71it/s]11/28/2021 01:06:06 - INFO - __main__ -   Batch number = 3
Evaluating:   1%|          | 3/282 [00:00<00:40,  6.91it/s]11/28/2021 01:06:06 - INFO - __main__ -   Batch number = 4
Evaluating:   1%|▏         | 4/282 [00:00<00:39,  6.99it/s]11/28/2021 01:06:07 - INFO - __main__ -   Batch number = 5
Evaluating:   2%|▏         | 5/282 [00:00<00:39,  7.05it/s]11/28/2021 01:06:07 - INFO - __main__ -   Batch number = 6
Evaluating:   2%|▏         | 6/282 [00:00<00:39,  7.06it/s]11/28/2021 01:06:07 - INFO - __main__ -   Batch number = 7
Evaluating:   2%|▏         | 7/282 [00:01<00:38,  7.06it/s]11/28/2021 01:06:07 - INFO - __main__ -   Batch number = 8
Evaluating:   3%|▎         | 8/282 [00:01<00:38,  7.08it/s]11/28/2021 01:06:07 - INFO - __main__ -   Batch number = 9
Evaluating:   3%|▎         | 9/282 [00:01<00:38,  7.05it/s]11/28/2021 01:06:07 - INFO - __main__ -   Batch number = 10
Evaluating:   4%|▎         | 10/282 [00:01<00:38,  7.05it/s]11/28/2021 01:06:07 - INFO - __main__ -   Batch number = 11
Evaluating:   4%|▍         | 11/282 [00:01<00:38,  7.05it/s]11/28/2021 01:06:08 - INFO - __main__ -   Batch number = 12
Evaluating:   4%|▍         | 12/282 [00:01<00:38,  7.05it/s]11/28/2021 01:06:08 - INFO - __main__ -   Batch number = 13
Evaluating:   5%|▍         | 13/282 [00:01<00:38,  7.02it/s]11/28/2021 01:06:08 - INFO - __main__ -   Batch number = 14
Evaluating:   5%|▍         | 14/282 [00:02<00:38,  7.00it/s]11/28/2021 01:06:08 - INFO - __main__ -   Batch number = 15
Evaluating:   5%|▌         | 15/282 [00:02<00:37,  7.03it/s]11/28/2021 01:06:08 - INFO - __main__ -   Batch number = 16
Evaluating:   6%|▌         | 16/282 [00:02<00:37,  7.03it/s]11/28/2021 01:06:08 - INFO - __main__ -   Batch number = 17
Evaluating:   6%|▌         | 17/282 [00:02<00:37,  6.98it/s]11/28/2021 01:06:08 - INFO - __main__ -   Batch number = 18
Evaluating:   6%|▋         | 18/282 [00:02<00:37,  6.98it/s]11/28/2021 01:06:09 - INFO - __main__ -   Batch number = 19
Evaluating:   7%|▋         | 19/282 [00:02<00:37,  6.98it/s]11/28/2021 01:06:09 - INFO - __main__ -   Batch number = 20
Evaluating:   7%|▋         | 20/282 [00:02<00:37,  6.98it/s]11/28/2021 01:06:09 - INFO - __main__ -   Batch number = 21
Evaluating:   7%|▋         | 21/282 [00:03<00:37,  6.96it/s]11/28/2021 01:06:09 - INFO - __main__ -   Batch number = 22
Evaluating:   8%|▊         | 22/282 [00:03<00:49,  5.28it/s]11/28/2021 01:06:09 - INFO - __main__ -   Batch number = 23
Evaluating:   8%|▊         | 23/282 [00:03<00:46,  5.57it/s]11/28/2021 01:06:09 - INFO - __main__ -   Batch number = 24
Evaluating:   9%|▊         | 24/282 [00:03<00:45,  5.69it/s]11/28/2021 01:06:10 - INFO - __main__ -   Batch number = 25
Evaluating:   9%|▉         | 25/282 [00:03<00:44,  5.80it/s]11/28/2021 01:06:10 - INFO - __main__ -   Batch number = 26
Evaluating:   9%|▉         | 26/282 [00:03<00:43,  5.90it/s]11/28/2021 01:06:10 - INFO - __main__ -   Batch number = 27
Evaluating:  10%|▉         | 27/282 [00:04<00:42,  5.98it/s]11/28/2021 01:06:10 - INFO - __main__ -   Batch number = 28
Evaluating:  10%|▉         | 28/282 [00:04<00:42,  6.02it/s]11/28/2021 01:06:10 - INFO - __main__ -   Batch number = 29
Evaluating:  10%|█         | 29/282 [00:04<00:41,  6.07it/s]11/28/2021 01:06:10 - INFO - __main__ -   Batch number = 30
Evaluating:  11%|█         | 30/282 [00:04<00:41,  6.13it/s]11/28/2021 01:06:11 - INFO - __main__ -   Batch number = 31
11/28/2021 01:06:11 - INFO - __main__ -   Language gn, split test does not exist
Evaluating:  11%|█         | 31/282 [00:04<00:41,  6.11it/s]11/28/2021 01:06:11 - INFO - __main__ -   Batch number = 32
Evaluating:  11%|█▏        | 32/282 [00:04<00:41,  6.04it/s]11/28/2021 01:06:11 - INFO - __main__ -   Batch number = 33
Evaluating:  12%|█▏        | 33/282 [00:05<00:41,  6.03it/s]11/28/2021 01:06:11 - INFO - __main__ -   Batch number = 34
Evaluating:  12%|█▏        | 34/282 [00:05<00:40,  6.08it/s]11/28/2021 01:06:11 - INFO - __main__ -   Batch number = 35
Evaluating:  12%|█▏        | 35/282 [00:05<00:40,  6.11it/s]11/28/2021 01:06:11 - INFO - __main__ -   Batch number = 36
13.00user 6.10system 0:18.05elapsed 105%CPU (0avgtext+0avgdata 3988588maxresident)k
0inputs+48outputs (0major+1625219minor)pagefaults 0swaps
Evaluating:  13%|█▎        | 36/282 [00:05<00:39,  6.16it/s]11/28/2021 01:06:12 - INFO - __main__ -   Batch number = 37
Evaluating:  13%|█▎        | 37/282 [00:05<00:39,  6.14it/s]11/28/2021 01:06:12 - INFO - __main__ -   Batch number = 38
Evaluating:  13%|█▎        | 38/282 [00:05<00:40,  6.08it/s]11/28/2021 01:06:12 - INFO - __main__ -   Batch number = 39
Evaluating:  14%|█▍        | 39/282 [00:06<00:39,  6.12it/s]11/28/2021 01:06:12 - INFO - __main__ -   Batch number = 40
Evaluating:  14%|█▍        | 40/282 [00:06<00:39,  6.15it/s]11/28/2021 01:06:12 - INFO - __main__ -   Batch number = 41
Evaluating:  15%|█▍        | 41/282 [00:06<00:39,  6.12it/s]11/28/2021 01:06:12 - INFO - __main__ -   Batch number = 42
Evaluating:  15%|█▍        | 42/282 [00:06<00:39,  6.12it/s]11/28/2021 01:06:13 - INFO - __main__ -   Batch number = 43
Evaluating:  15%|█▌        | 43/282 [00:06<00:39,  6.13it/s]11/28/2021 01:06:13 - INFO - __main__ -   Batch number = 44
Evaluating:  16%|█▌        | 44/282 [00:06<00:38,  6.20it/s]11/28/2021 01:06:13 - INFO - __main__ -   Batch number = 45
PyTorch version 1.10.0+cu102 available.
Evaluating:  16%|█▌        | 45/282 [00:07<00:38,  6.23it/s]11/28/2021 01:06:13 - INFO - __main__ -   Batch number = 46
Evaluating:  16%|█▋        | 46/282 [00:07<00:37,  6.27it/s]11/28/2021 01:06:13 - INFO - __main__ -   Batch number = 47
11/28/2021 01:06:13 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='gn', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:06:13 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:06:13 - INFO - __main__ -   Seed = 2
11/28/2021 01:06:13 - INFO - root -   save model
11/28/2021 01:06:13 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='gn', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:06:13 - INFO - __main__ -   Loading pretrained model and tokenizer
Evaluating:  17%|█▋        | 47/282 [00:07<00:37,  6.29it/s]11/28/2021 01:06:13 - INFO - __main__ -   Batch number = 48
Evaluating:  17%|█▋        | 48/282 [00:07<00:36,  6.35it/s]11/28/2021 01:06:13 - INFO - __main__ -   Batch number = 49
Evaluating:  17%|█▋        | 49/282 [00:07<00:36,  6.38it/s]11/28/2021 01:06:14 - INFO - __main__ -   Batch number = 50
Evaluating:  18%|█▊        | 50/282 [00:07<00:36,  6.43it/s]11/28/2021 01:06:14 - INFO - __main__ -   Batch number = 51
Evaluating:  18%|█▊        | 51/282 [00:07<00:35,  6.44it/s]11/28/2021 01:06:14 - INFO - __main__ -   Batch number = 52
Evaluating:  18%|█▊        | 52/282 [00:08<00:35,  6.46it/s]11/28/2021 01:06:14 - INFO - __main__ -   Batch number = 53
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  19%|█▉        | 53/282 [00:08<00:35,  6.45it/s]11/28/2021 01:06:14 - INFO - __main__ -   Batch number = 54
Evaluating:  19%|█▉        | 54/282 [00:08<00:35,  6.34it/s]11/28/2021 01:06:14 - INFO - __main__ -   Batch number = 55
Evaluating:  20%|█▉        | 55/282 [00:08<00:36,  6.30it/s]11/28/2021 01:06:15 - INFO - __main__ -   Batch number = 56
Evaluating:  20%|█▉        | 56/282 [00:08<00:36,  6.22it/s]11/28/2021 01:06:15 - INFO - __main__ -   Batch number = 57
Evaluating:  20%|██        | 57/282 [00:08<00:36,  6.23it/s]11/28/2021 01:06:15 - INFO - __main__ -   Batch number = 58
Evaluating:  21%|██        | 58/282 [00:09<00:35,  6.22it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
11/28/2021 01:06:15 - INFO - __main__ -   Batch number = 59
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  21%|██        | 59/282 [00:09<00:35,  6.28it/s]11/28/2021 01:06:15 - INFO - __main__ -   Batch number = 60
Evaluating:  21%|██▏       | 60/282 [00:09<00:35,  6.27it/s]11/28/2021 01:06:15 - INFO - __main__ -   Batch number = 61
Evaluating:  22%|██▏       | 61/282 [00:09<00:35,  6.17it/s]11/28/2021 01:06:16 - INFO - __main__ -   Batch number = 62
Evaluating:  22%|██▏       | 62/282 [00:09<00:35,  6.18it/s]11/28/2021 01:06:16 - INFO - __main__ -   Batch number = 63
Evaluating:  22%|██▏       | 63/282 [00:09<00:34,  6.34it/s]11/28/2021 01:06:16 - INFO - __main__ -   Batch number = 64
loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
Evaluating:  23%|██▎       | 64/282 [00:10<00:33,  6.44it/s]11/28/2021 01:06:16 - INFO - __main__ -   Batch number = 65
11/28/2021 01:06:16 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
Evaluating:  23%|██▎       | 65/282 [00:10<00:33,  6.49it/s]11/28/2021 01:06:16 - INFO - __main__ -   Batch number = 66
Evaluating:  23%|██▎       | 66/282 [00:10<00:33,  6.47it/s]11/28/2021 01:06:16 - INFO - __main__ -   Batch number = 67
Evaluating:  24%|██▍       | 67/282 [00:10<00:32,  6.59it/s]11/28/2021 01:06:16 - INFO - __main__ -   Batch number = 68
Evaluating:  24%|██▍       | 68/282 [00:10<00:32,  6.65it/s]11/28/2021 01:06:17 - INFO - __main__ -   Batch number = 69
Evaluating:  24%|██▍       | 69/282 [00:10<00:32,  6.61it/s]11/28/2021 01:06:17 - INFO - __main__ -   Batch number = 70
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  25%|██▍       | 70/282 [00:10<00:32,  6.57it/s]11/28/2021 01:06:17 - INFO - __main__ -   Batch number = 71
Evaluating:  25%|██▌       | 71/282 [00:11<00:32,  6.59it/s]11/28/2021 01:06:17 - INFO - __main__ -   Batch number = 72
Evaluating:  26%|██▌       | 72/282 [00:11<00:31,  6.57it/s]11/28/2021 01:06:17 - INFO - __main__ -   Batch number = 73
Evaluating:  26%|██▌       | 73/282 [00:11<00:31,  6.57it/s]11/28/2021 01:06:17 - INFO - __main__ -   Batch number = 74
Evaluating:  26%|██▌       | 74/282 [00:11<00:31,  6.54it/s]11/28/2021 01:06:18 - INFO - __main__ -   Batch number = 75
Evaluating:  27%|██▋       | 75/282 [00:11<00:31,  6.57it/s]11/28/2021 01:06:18 - INFO - __main__ -   Batch number = 76
Evaluating:  27%|██▋       | 76/282 [00:11<00:31,  6.60it/s]11/28/2021 01:06:18 - INFO - __main__ -   Batch number = 77
Evaluating:  27%|██▋       | 77/282 [00:12<00:31,  6.59it/s]11/28/2021 01:06:18 - INFO - __main__ -   Batch number = 78
Evaluating:  28%|██▊       | 78/282 [00:12<00:31,  6.54it/s]11/28/2021 01:06:18 - INFO - __main__ -   Batch number = 79
Evaluating:  28%|██▊       | 79/282 [00:12<00:30,  6.56it/s]11/28/2021 01:06:18 - INFO - __main__ -   Batch number = 80
Evaluating:  28%|██▊       | 80/282 [00:12<00:30,  6.53it/s]11/28/2021 01:06:18 - INFO - __main__ -   Batch number = 81
Evaluating:  29%|██▊       | 81/282 [00:12<00:30,  6.55it/s]11/28/2021 01:06:19 - INFO - __main__ -   Batch number = 82
Evaluating:  29%|██▉       | 82/282 [00:12<00:30,  6.50it/s]11/28/2021 01:06:19 - INFO - __main__ -   Batch number = 83
Evaluating:  29%|██▉       | 83/282 [00:12<00:30,  6.46it/s]11/28/2021 01:06:19 - INFO - __main__ -   Batch number = 84
Evaluating:  30%|██▉       | 84/282 [00:13<00:30,  6.41it/s]11/28/2021 01:06:19 - INFO - __main__ -   Batch number = 85
Evaluating:  30%|███       | 85/282 [00:13<00:30,  6.37it/s]11/28/2021 01:06:19 - INFO - __main__ -   Batch number = 86
Evaluating:  30%|███       | 86/282 [00:13<00:30,  6.42it/s]11/28/2021 01:06:19 - INFO - __main__ -   Batch number = 87
Evaluating:  31%|███       | 87/282 [00:13<00:30,  6.45it/s]11/28/2021 01:06:20 - INFO - __main__ -   Batch number = 88
Evaluating:  31%|███       | 88/282 [00:13<00:30,  6.39it/s]11/28/2021 01:06:20 - INFO - __main__ -   Batch number = 89
Evaluating:  32%|███▏      | 89/282 [00:13<00:30,  6.39it/s]11/28/2021 01:06:20 - INFO - __main__ -   Batch number = 90
Evaluating:  32%|███▏      | 90/282 [00:14<00:30,  6.40it/s]11/28/2021 01:06:20 - INFO - __main__ -   Batch number = 91
Evaluating:  32%|███▏      | 91/282 [00:14<00:30,  6.37it/s]11/28/2021 01:06:20 - INFO - __main__ -   Batch number = 92
Evaluating:  33%|███▎      | 92/282 [00:14<00:29,  6.37it/s]11/28/2021 01:06:20 - INFO - __main__ -   Batch number = 93
Evaluating:  33%|███▎      | 93/282 [00:14<00:29,  6.38it/s]11/28/2021 01:06:20 - INFO - __main__ -   Batch number = 94
Evaluating:  33%|███▎      | 94/282 [00:14<00:29,  6.40it/s]11/28/2021 01:06:21 - INFO - __main__ -   Batch number = 95
Evaluating:  34%|███▎      | 95/282 [00:14<00:29,  6.32it/s]11/28/2021 01:06:21 - INFO - __main__ -   Batch number = 96
Evaluating:  34%|███▍      | 96/282 [00:14<00:29,  6.22it/s]11/28/2021 01:06:21 - INFO - __main__ -   Batch number = 97
Evaluating:  34%|███▍      | 97/282 [00:15<00:29,  6.18it/s]11/28/2021 01:06:21 - INFO - __main__ -   Batch number = 98
Evaluating:  35%|███▍      | 98/282 [00:15<00:29,  6.23it/s]11/28/2021 01:06:21 - INFO - __main__ -   Batch number = 99
Evaluating:  35%|███▌      | 99/282 [00:15<00:29,  6.29it/s]11/28/2021 01:06:21 - INFO - __main__ -   Batch number = 100
Evaluating:  35%|███▌      | 100/282 [00:15<00:28,  6.33it/s]11/28/2021 01:06:22 - INFO - __main__ -   Batch number = 101
Evaluating:  36%|███▌      | 101/282 [00:15<00:34,  5.20it/s]11/28/2021 01:06:22 - INFO - __main__ -   Batch number = 102
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:06:22 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:06:22 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:06:22 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
11/28/2021 01:06:22 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:06:22 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:06:22 - INFO - root -   loading lang adpater bh/wiki@ukp
11/28/2021 01:06:22 - INFO - __main__ -   Adapter Languages : ['bh'], Length : 1
11/28/2021 01:06:22 - INFO - __main__ -   Adapter Names ['bh/wiki@ukp'], Length : 1
11/28/2021 01:06:22 - INFO - __main__ -   Language = bh
11/28/2021 01:06:22 - INFO - __main__ -   Adapter Name = bh/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/bh/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_bh_wiki_pfeiffer.zip.
Evaluating:  36%|███▌      | 102/282 [00:16<00:41,  4.30it/s]11/28/2021 01:06:22 - INFO - __main__ -   Batch number = 103
Evaluating:  37%|███▋      | 103/282 [00:16<00:46,  3.81it/s]11/28/2021 01:06:23 - INFO - __main__ -   Batch number = 104
Evaluating:  37%|███▋      | 104/282 [00:16<00:50,  3.53it/s]11/28/2021 01:06:23 - INFO - __main__ -   Batch number = 105
Evaluating:  37%|███▋      | 105/282 [00:17<00:53,  3.33it/s]11/28/2021 01:06:23 - INFO - __main__ -   Batch number = 106
Evaluating:  38%|███▊      | 106/282 [00:17<00:54,  3.21it/s]11/28/2021 01:06:24 - INFO - __main__ -   Batch number = 107
Evaluating:  38%|███▊      | 107/282 [00:17<00:55,  3.14it/s]11/28/2021 01:06:24 - INFO - __main__ -   Batch number = 108
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/adapter_config.json
Adding adapter 'bh' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/head_config.json
Evaluating:  38%|███▊      | 108/282 [00:18<00:56,  3.08it/s]11/28/2021 01:06:24 - INFO - __main__ -   Batch number = 109
Evaluating:  39%|███▊      | 109/282 [00:18<00:56,  3.04it/s]11/28/2021 01:06:25 - INFO - __main__ -   Batch number = 110
Evaluating:  39%|███▉      | 110/282 [00:18<00:57,  3.01it/s]11/28/2021 01:06:25 - INFO - __main__ -   Batch number = 111
Evaluating:  39%|███▉      | 111/282 [00:19<00:57,  2.98it/s]11/28/2021 01:06:25 - INFO - __main__ -   Batch number = 112
Evaluating:  40%|███▉      | 112/282 [00:19<00:57,  2.98it/s]11/28/2021 01:06:26 - INFO - __main__ -   Batch number = 113
Evaluating:  40%|████      | 113/282 [00:19<00:56,  2.98it/s]11/28/2021 01:06:26 - INFO - __main__ -   Batch number = 114
Evaluating:  40%|████      | 114/282 [00:20<00:56,  2.96it/s]11/28/2021 01:06:26 - INFO - __main__ -   Batch number = 115
Evaluating:  41%|████      | 115/282 [00:20<00:56,  2.96it/s]11/28/2021 01:06:27 - INFO - __main__ -   Batch number = 116
Evaluating:  41%|████      | 116/282 [00:20<00:55,  2.97it/s]11/28/2021 01:06:27 - INFO - __main__ -   Batch number = 117
Evaluating:  41%|████▏     | 117/282 [00:21<00:56,  2.95it/s]11/28/2021 01:06:27 - INFO - __main__ -   Batch number = 118
Evaluating:  42%|████▏     | 118/282 [00:21<00:55,  2.96it/s]11/28/2021 01:06:28 - INFO - __main__ -   Batch number = 119
Evaluating:  42%|████▏     | 119/282 [00:21<00:55,  2.93it/s]11/28/2021 01:06:28 - INFO - __main__ -   Batch number = 120
Evaluating:  43%|████▎     | 120/282 [00:22<00:55,  2.93it/s]11/28/2021 01:06:28 - INFO - __main__ -   Batch number = 121
Evaluating:  43%|████▎     | 121/282 [00:22<00:54,  2.95it/s]11/28/2021 01:06:29 - INFO - __main__ -   Batch number = 122
Evaluating:  43%|████▎     | 122/282 [00:22<00:53,  2.97it/s]11/28/2021 01:06:29 - INFO - __main__ -   Batch number = 123
Evaluating:  44%|████▎     | 123/282 [00:23<00:52,  3.04it/s]11/28/2021 01:06:29 - INFO - __main__ -   Batch number = 124
11/28/2021 01:06:29 - INFO - __main__ -   Language gn, split test does not exist
Evaluating:  44%|████▍     | 124/282 [00:23<00:52,  3.01it/s]11/28/2021 01:06:30 - INFO - __main__ -   Batch number = 125
Evaluating:  44%|████▍     | 125/282 [00:23<00:52,  2.97it/s]11/28/2021 01:06:30 - INFO - __main__ -   Batch number = 126
15.22user 6.87system 0:18.66elapsed 118%CPU (0avgtext+0avgdata 3988352maxresident)k
0inputs+40outputs (0major+1349779minor)pagefaults 0swaps
Evaluating:  45%|████▍     | 126/282 [00:24<00:52,  2.95it/s]11/28/2021 01:06:30 - INFO - __main__ -   Batch number = 127
Evaluating:  45%|████▌     | 127/282 [00:24<00:52,  2.95it/s]11/28/2021 01:06:31 - INFO - __main__ -   Batch number = 128
Evaluating:  45%|████▌     | 128/282 [00:24<00:51,  2.97it/s]11/28/2021 01:06:31 - INFO - __main__ -   Batch number = 129
Evaluating:  46%|████▌     | 129/282 [00:25<00:51,  2.95it/s]11/28/2021 01:06:31 - INFO - __main__ -   Batch number = 130
PyTorch version 1.10.0+cu102 available.
Evaluating:  46%|████▌     | 130/282 [00:25<00:51,  2.98it/s]11/28/2021 01:06:32 - INFO - __main__ -   Batch number = 131
11/28/2021 01:06:32 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='gn', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:06:32 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:06:32 - INFO - __main__ -   Seed = 3
11/28/2021 01:06:32 - INFO - root -   save model
11/28/2021 01:06:32 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='gn', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:06:32 - INFO - __main__ -   Loading pretrained model and tokenizer
Evaluating:  46%|████▋     | 131/282 [00:25<00:50,  2.98it/s]11/28/2021 01:06:32 - INFO - __main__ -   Batch number = 132
Evaluating:  47%|████▋     | 132/282 [00:26<00:50,  2.97it/s]11/28/2021 01:06:32 - INFO - __main__ -   Batch number = 133
Evaluating:  47%|████▋     | 133/282 [00:26<00:50,  2.96it/s]11/28/2021 01:06:33 - INFO - __main__ -   Batch number = 134
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  48%|████▊     | 134/282 [00:27<00:49,  2.97it/s]11/28/2021 01:06:33 - INFO - __main__ -   Batch number = 135
Evaluating:  48%|████▊     | 135/282 [00:27<00:49,  2.94it/s]11/28/2021 01:06:33 - INFO - __main__ -   Batch number = 136
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  48%|████▊     | 136/282 [00:27<00:49,  2.94it/s]11/28/2021 01:06:34 - INFO - __main__ -   Batch number = 137
Evaluating:  49%|████▊     | 137/282 [00:28<00:49,  2.95it/s]11/28/2021 01:06:34 - INFO - __main__ -   Batch number = 138
Evaluating:  49%|████▉     | 138/282 [00:28<00:50,  2.85it/s]11/28/2021 01:06:34 - INFO - __main__ -   Batch number = 139
loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:06:35 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
Evaluating:  49%|████▉     | 139/282 [00:28<00:49,  2.89it/s]11/28/2021 01:06:35 - INFO - __main__ -   Batch number = 140
Evaluating:  50%|████▉     | 140/282 [00:29<00:48,  2.93it/s]11/28/2021 01:06:35 - INFO - __main__ -   Batch number = 141
Evaluating:  50%|█████     | 141/282 [00:29<00:48,  2.94it/s]11/28/2021 01:06:35 - INFO - __main__ -   Batch number = 142
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  50%|█████     | 142/282 [00:29<00:47,  2.94it/s]11/28/2021 01:06:36 - INFO - __main__ -   Batch number = 143
Evaluating:  51%|█████     | 143/282 [00:30<00:47,  2.95it/s]11/28/2021 01:06:36 - INFO - __main__ -   Batch number = 144
Evaluating:  51%|█████     | 144/282 [00:30<00:47,  2.93it/s]11/28/2021 01:06:36 - INFO - __main__ -   Batch number = 145
Evaluating:  51%|█████▏    | 145/282 [00:30<00:46,  2.95it/s]11/28/2021 01:06:37 - INFO - __main__ -   Batch number = 146
Evaluating:  52%|█████▏    | 146/282 [00:31<00:46,  2.95it/s]11/28/2021 01:06:37 - INFO - __main__ -   Batch number = 147
Evaluating:  52%|█████▏    | 147/282 [00:31<00:45,  2.95it/s]11/28/2021 01:06:37 - INFO - __main__ -   Batch number = 148
Evaluating:  52%|█████▏    | 148/282 [00:31<00:45,  2.96it/s]11/28/2021 01:06:38 - INFO - __main__ -   Batch number = 149
Evaluating:  53%|█████▎    | 149/282 [00:32<00:44,  2.99it/s]11/28/2021 01:06:38 - INFO - __main__ -   Batch number = 150
Evaluating:  53%|█████▎    | 150/282 [00:32<00:43,  3.00it/s]11/28/2021 01:06:38 - INFO - __main__ -   Batch number = 151
Evaluating:  54%|█████▎    | 151/282 [00:32<00:43,  3.02it/s]11/28/2021 01:06:39 - INFO - __main__ -   Batch number = 152
Evaluating:  54%|█████▍    | 152/282 [00:33<00:43,  3.01it/s]11/28/2021 01:06:39 - INFO - __main__ -   Batch number = 153
Evaluating:  54%|█████▍    | 153/282 [00:33<00:43,  2.97it/s]11/28/2021 01:06:39 - INFO - __main__ -   Batch number = 154
Evaluating:  55%|█████▍    | 154/282 [00:33<00:43,  2.96it/s]11/28/2021 01:06:40 - INFO - __main__ -   Batch number = 155
Evaluating:  55%|█████▍    | 155/282 [00:34<00:42,  2.97it/s]11/28/2021 01:06:40 - INFO - __main__ -   Batch number = 156
Evaluating:  55%|█████▌    | 156/282 [00:34<00:42,  2.94it/s]11/28/2021 01:06:40 - INFO - __main__ -   Batch number = 157
Evaluating:  56%|█████▌    | 157/282 [00:34<00:42,  2.96it/s]11/28/2021 01:06:41 - INFO - __main__ -   Batch number = 158
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:06:41 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:06:41 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:06:41 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
11/28/2021 01:06:41 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:06:41 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:06:41 - INFO - root -   loading lang adpater bh/wiki@ukp
11/28/2021 01:06:41 - INFO - __main__ -   Adapter Languages : ['bh'], Length : 1
11/28/2021 01:06:41 - INFO - __main__ -   Adapter Names ['bh/wiki@ukp'], Length : 1
11/28/2021 01:06:41 - INFO - __main__ -   Language = bh
11/28/2021 01:06:41 - INFO - __main__ -   Adapter Name = bh/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/bh/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_bh_wiki_pfeiffer.zip.
Evaluating:  56%|█████▌    | 158/282 [00:35<00:42,  2.95it/s]11/28/2021 01:06:41 - INFO - __main__ -   Batch number = 159
Evaluating:  56%|█████▋    | 159/282 [00:35<00:41,  2.94it/s]11/28/2021 01:06:41 - INFO - __main__ -   Batch number = 160
Evaluating:  57%|█████▋    | 160/282 [00:35<00:41,  2.92it/s]11/28/2021 01:06:42 - INFO - __main__ -   Batch number = 161
Evaluating:  57%|█████▋    | 161/282 [00:36<00:41,  2.93it/s]11/28/2021 01:06:42 - INFO - __main__ -   Batch number = 162
Evaluating:  57%|█████▋    | 162/282 [00:36<00:41,  2.92it/s]11/28/2021 01:06:43 - INFO - __main__ -   Batch number = 163
Evaluating:  58%|█████▊    | 163/282 [00:36<00:40,  2.93it/s]11/28/2021 01:06:43 - INFO - __main__ -   Batch number = 164
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/adapter_config.json
Adding adapter 'bh' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/head_config.json
Evaluating:  58%|█████▊    | 164/282 [00:37<00:39,  2.96it/s]11/28/2021 01:06:43 - INFO - __main__ -   Batch number = 165
Evaluating:  59%|█████▊    | 165/282 [00:37<00:39,  2.99it/s]11/28/2021 01:06:43 - INFO - __main__ -   Batch number = 166
Evaluating:  59%|█████▉    | 166/282 [00:37<00:39,  2.97it/s]11/28/2021 01:06:44 - INFO - __main__ -   Batch number = 167
Evaluating:  59%|█████▉    | 167/282 [00:38<00:38,  2.96it/s]11/28/2021 01:06:44 - INFO - __main__ -   Batch number = 168
Evaluating:  60%|█████▉    | 168/282 [00:38<00:38,  2.94it/s]11/28/2021 01:06:45 - INFO - __main__ -   Batch number = 169
Evaluating:  60%|█████▉    | 169/282 [00:38<00:38,  2.95it/s]11/28/2021 01:06:45 - INFO - __main__ -   Batch number = 170
Evaluating:  60%|██████    | 170/282 [00:39<00:38,  2.94it/s]11/28/2021 01:06:45 - INFO - __main__ -   Batch number = 171
Evaluating:  61%|██████    | 171/282 [00:39<00:37,  2.93it/s]11/28/2021 01:06:46 - INFO - __main__ -   Batch number = 172
Evaluating:  61%|██████    | 172/282 [00:39<00:37,  2.92it/s]11/28/2021 01:06:46 - INFO - __main__ -   Batch number = 173
Evaluating:  61%|██████▏   | 173/282 [00:40<00:37,  2.94it/s]11/28/2021 01:06:46 - INFO - __main__ -   Batch number = 174
Evaluating:  62%|██████▏   | 174/282 [00:40<00:36,  2.92it/s]11/28/2021 01:06:47 - INFO - __main__ -   Batch number = 175
Evaluating:  62%|██████▏   | 175/282 [00:40<00:36,  2.93it/s]11/28/2021 01:06:47 - INFO - __main__ -   Batch number = 176
Evaluating:  62%|██████▏   | 176/282 [00:41<00:36,  2.91it/s]11/28/2021 01:06:47 - INFO - __main__ -   Batch number = 177
Evaluating:  63%|██████▎   | 177/282 [00:41<00:35,  2.93it/s]11/28/2021 01:06:48 - INFO - __main__ -   Batch number = 178
11/28/2021 01:06:48 - INFO - __main__ -   Language gn, split test does not exist
Evaluating:  63%|██████▎   | 178/282 [00:42<00:37,  2.74it/s]11/28/2021 01:06:48 - INFO - __main__ -   Batch number = 179
Evaluating:  63%|██████▎   | 179/282 [00:42<00:36,  2.80it/s]11/28/2021 01:06:48 - INFO - __main__ -   Batch number = 180
13.95user 5.58system 0:18.36elapsed 106%CPU (0avgtext+0avgdata 3989664maxresident)k
0inputs+32outputs (0major+1509870minor)pagefaults 0swaps
Evaluating:  64%|██████▍   | 180/282 [00:42<00:36,  2.83it/s]11/28/2021 01:06:49 - INFO - __main__ -   Batch number = 181
Evaluating:  64%|██████▍   | 181/282 [00:42<00:30,  3.34it/s]11/28/2021 01:06:49 - INFO - __main__ -   Batch number = 182
Evaluating:  65%|██████▍   | 182/282 [00:43<00:26,  3.81it/s]11/28/2021 01:06:49 - INFO - __main__ -   Batch number = 183
Evaluating:  65%|██████▍   | 183/282 [00:43<00:23,  4.29it/s]11/28/2021 01:06:49 - INFO - __main__ -   Batch number = 184
Evaluating:  65%|██████▌   | 184/282 [00:43<00:21,  4.61it/s]11/28/2021 01:06:49 - INFO - __main__ -   Batch number = 185
Evaluating:  66%|██████▌   | 185/282 [00:43<00:19,  5.02it/s]11/28/2021 01:06:50 - INFO - __main__ -   Batch number = 186
Evaluating:  66%|██████▌   | 186/282 [00:43<00:18,  5.24it/s]11/28/2021 01:06:50 - INFO - __main__ -   Batch number = 187
Evaluating:  66%|██████▋   | 187/282 [00:43<00:17,  5.46it/s]11/28/2021 01:06:50 - INFO - __main__ -   Batch number = 188
Evaluating:  67%|██████▋   | 188/282 [00:44<00:16,  5.57it/s]11/28/2021 01:06:50 - INFO - __main__ -   Batch number = 189
Evaluating:  67%|██████▋   | 189/282 [00:44<00:16,  5.79it/s]11/28/2021 01:06:50 - INFO - __main__ -   Batch number = 190
Evaluating:  67%|██████▋   | 190/282 [00:44<00:15,  5.89it/s]11/28/2021 01:06:50 - INFO - __main__ -   Batch number = 191
Evaluating:  68%|██████▊   | 191/282 [00:44<00:14,  6.07it/s]11/28/2021 01:06:51 - INFO - __main__ -   Batch number = 192
Evaluating:  68%|██████▊   | 192/282 [00:44<00:14,  6.09it/s]11/28/2021 01:06:51 - INFO - __main__ -   Batch number = 193
Evaluating:  68%|██████▊   | 193/282 [00:44<00:14,  6.19it/s]11/28/2021 01:06:51 - INFO - __main__ -   Batch number = 194
Evaluating:  69%|██████▉   | 194/282 [00:45<00:14,  6.03it/s]11/28/2021 01:06:51 - INFO - __main__ -   Batch number = 195
Evaluating:  69%|██████▉   | 195/282 [00:45<00:14,  5.94it/s]11/28/2021 01:06:51 - INFO - __main__ -   Batch number = 196
Evaluating:  70%|██████▉   | 196/282 [00:45<00:14,  5.86it/s]11/28/2021 01:06:51 - INFO - __main__ -   Batch number = 197
Evaluating:  70%|██████▉   | 197/282 [00:45<00:14,  5.95it/s]11/28/2021 01:06:52 - INFO - __main__ -   Batch number = 198
Evaluating:  70%|███████   | 198/282 [00:45<00:14,  5.90it/s]11/28/2021 01:06:52 - INFO - __main__ -   Batch number = 199
Evaluating:  71%|███████   | 199/282 [00:45<00:13,  5.97it/s]11/28/2021 01:06:52 - INFO - __main__ -   Batch number = 200
Evaluating:  71%|███████   | 200/282 [00:46<00:13,  5.94it/s]11/28/2021 01:06:52 - INFO - __main__ -   Batch number = 201
Evaluating:  71%|███████▏  | 201/282 [00:46<00:13,  6.03it/s]11/28/2021 01:06:52 - INFO - __main__ -   Batch number = 202
Evaluating:  72%|███████▏  | 202/282 [00:46<00:13,  5.96it/s]11/28/2021 01:06:52 - INFO - __main__ -   Batch number = 203
Evaluating:  72%|███████▏  | 203/282 [00:46<00:13,  6.02it/s]11/28/2021 01:06:53 - INFO - __main__ -   Batch number = 204
Evaluating:  72%|███████▏  | 204/282 [00:46<00:13,  6.00it/s]11/28/2021 01:06:53 - INFO - __main__ -   Batch number = 205
Evaluating:  73%|███████▎  | 205/282 [00:46<00:12,  6.09it/s]11/28/2021 01:06:53 - INFO - __main__ -   Batch number = 206
Evaluating:  73%|███████▎  | 206/282 [00:47<00:12,  5.92it/s]11/28/2021 01:06:53 - INFO - __main__ -   Batch number = 207
Evaluating:  73%|███████▎  | 207/282 [00:47<00:12,  5.96it/s]11/28/2021 01:06:53 - INFO - __main__ -   Batch number = 208
Evaluating:  74%|███████▍  | 208/282 [00:47<00:12,  5.78it/s]11/28/2021 01:06:53 - INFO - __main__ -   Batch number = 209
Evaluating:  74%|███████▍  | 209/282 [00:47<00:12,  5.82it/s]11/28/2021 01:06:54 - INFO - __main__ -   Batch number = 210
Evaluating:  74%|███████▍  | 210/282 [00:47<00:12,  5.71it/s]11/28/2021 01:06:54 - INFO - __main__ -   Batch number = 211
Evaluating:  75%|███████▍  | 211/282 [00:47<00:12,  5.80it/s]11/28/2021 01:06:54 - INFO - __main__ -   Batch number = 212
Evaluating:  75%|███████▌  | 212/282 [00:48<00:12,  5.81it/s]11/28/2021 01:06:54 - INFO - __main__ -   Batch number = 213
Evaluating:  76%|███████▌  | 213/282 [00:48<00:12,  5.68it/s]11/28/2021 01:06:54 - INFO - __main__ -   Batch number = 214
Evaluating:  76%|███████▌  | 214/282 [00:48<00:12,  5.62it/s]11/28/2021 01:06:54 - INFO - __main__ -   Batch number = 215
Evaluating:  76%|███████▌  | 215/282 [00:48<00:11,  5.71it/s]11/28/2021 01:06:55 - INFO - __main__ -   Batch number = 216
Evaluating:  77%|███████▋  | 216/282 [00:48<00:11,  5.67it/s]11/28/2021 01:06:55 - INFO - __main__ -   Batch number = 217
Evaluating:  77%|███████▋  | 217/282 [00:48<00:11,  5.75it/s]11/28/2021 01:06:55 - INFO - __main__ -   Batch number = 218
Evaluating:  77%|███████▋  | 218/282 [00:49<00:11,  5.75it/s]11/28/2021 01:06:55 - INFO - __main__ -   Batch number = 219
Evaluating:  78%|███████▊  | 219/282 [00:49<00:10,  5.86it/s]11/28/2021 01:06:55 - INFO - __main__ -   Batch number = 220
Evaluating:  78%|███████▊  | 220/282 [00:49<00:10,  5.87it/s]11/28/2021 01:06:55 - INFO - __main__ -   Batch number = 221
Evaluating:  78%|███████▊  | 221/282 [00:49<00:10,  5.91it/s]11/28/2021 01:06:56 - INFO - __main__ -   Batch number = 222
Evaluating:  79%|███████▊  | 222/282 [00:49<00:10,  5.84it/s]11/28/2021 01:06:56 - INFO - __main__ -   Batch number = 223
Evaluating:  79%|███████▉  | 223/282 [00:50<00:09,  5.92it/s]11/28/2021 01:06:56 - INFO - __main__ -   Batch number = 224
Evaluating:  79%|███████▉  | 224/282 [00:50<00:09,  5.87it/s]11/28/2021 01:06:56 - INFO - __main__ -   Batch number = 225
Evaluating:  80%|███████▉  | 225/282 [00:50<00:09,  5.98it/s]11/28/2021 01:06:56 - INFO - __main__ -   Batch number = 226
Evaluating:  80%|████████  | 226/282 [00:50<00:09,  5.93it/s]11/28/2021 01:06:56 - INFO - __main__ -   Batch number = 227
Evaluating:  80%|████████  | 227/282 [00:50<00:09,  5.98it/s]11/28/2021 01:06:57 - INFO - __main__ -   Batch number = 228
Evaluating:  81%|████████  | 228/282 [00:50<00:09,  5.96it/s]11/28/2021 01:06:57 - INFO - __main__ -   Batch number = 229
Evaluating:  81%|████████  | 229/282 [00:51<00:08,  6.04it/s]11/28/2021 01:06:57 - INFO - __main__ -   Batch number = 230
Evaluating:  82%|████████▏ | 230/282 [00:51<00:08,  5.90it/s]11/28/2021 01:06:57 - INFO - __main__ -   Batch number = 231
Evaluating:  82%|████████▏ | 231/282 [00:51<00:08,  5.92it/s]11/28/2021 01:06:57 - INFO - __main__ -   Batch number = 232
Evaluating:  82%|████████▏ | 232/282 [00:51<00:08,  5.73it/s]11/28/2021 01:06:58 - INFO - __main__ -   Batch number = 233
Evaluating:  83%|████████▎ | 233/282 [00:51<00:08,  5.78it/s]11/28/2021 01:06:58 - INFO - __main__ -   Batch number = 234
Evaluating:  83%|████████▎ | 234/282 [00:51<00:08,  5.76it/s]11/28/2021 01:06:58 - INFO - __main__ -   Batch number = 235
Evaluating:  83%|████████▎ | 235/282 [00:52<00:07,  5.88it/s]11/28/2021 01:06:58 - INFO - __main__ -   Batch number = 236
Evaluating:  84%|████████▎ | 236/282 [00:52<00:07,  5.88it/s]11/28/2021 01:06:58 - INFO - __main__ -   Batch number = 237
Evaluating:  84%|████████▍ | 237/282 [00:52<00:07,  5.68it/s]11/28/2021 01:06:58 - INFO - __main__ -   Batch number = 238
Evaluating:  84%|████████▍ | 238/282 [00:52<00:07,  5.69it/s]11/28/2021 01:06:59 - INFO - __main__ -   Batch number = 239
Evaluating:  85%|████████▍ | 239/282 [00:52<00:07,  5.75it/s]11/28/2021 01:06:59 - INFO - __main__ -   Batch number = 240
Evaluating:  85%|████████▌ | 240/282 [00:52<00:07,  5.67it/s]11/28/2021 01:06:59 - INFO - __main__ -   Batch number = 241
Evaluating:  85%|████████▌ | 241/282 [00:53<00:07,  5.75it/s]11/28/2021 01:06:59 - INFO - __main__ -   Batch number = 242
Evaluating:  86%|████████▌ | 242/282 [00:53<00:08,  5.00it/s]11/28/2021 01:06:59 - INFO - __main__ -   Batch number = 243
Evaluating:  86%|████████▌ | 243/282 [00:53<00:07,  5.33it/s]11/28/2021 01:07:00 - INFO - __main__ -   Batch number = 244
Evaluating:  87%|████████▋ | 244/282 [00:53<00:06,  5.49it/s]11/28/2021 01:07:00 - INFO - __main__ -   Batch number = 245
Evaluating:  87%|████████▋ | 245/282 [00:53<00:06,  5.72it/s]11/28/2021 01:07:00 - INFO - __main__ -   Batch number = 246
Evaluating:  87%|████████▋ | 246/282 [00:54<00:06,  5.74it/s]11/28/2021 01:07:00 - INFO - __main__ -   Batch number = 247
Evaluating:  88%|████████▊ | 247/282 [00:54<00:06,  5.81it/s]11/28/2021 01:07:00 - INFO - __main__ -   Batch number = 248
Evaluating:  88%|████████▊ | 248/282 [00:54<00:05,  5.75it/s]11/28/2021 01:07:00 - INFO - __main__ -   Batch number = 249
Evaluating:  88%|████████▊ | 249/282 [00:54<00:05,  5.88it/s]11/28/2021 01:07:01 - INFO - __main__ -   Batch number = 250
Evaluating:  89%|████████▊ | 250/282 [00:54<00:05,  5.88it/s]11/28/2021 01:07:01 - INFO - __main__ -   Batch number = 251
Evaluating:  89%|████████▉ | 251/282 [00:54<00:05,  6.01it/s]11/28/2021 01:07:01 - INFO - __main__ -   Batch number = 252
Evaluating:  89%|████████▉ | 252/282 [00:55<00:05,  5.95it/s]11/28/2021 01:07:01 - INFO - __main__ -   Batch number = 253
Evaluating:  90%|████████▉ | 253/282 [00:55<00:04,  5.99it/s]11/28/2021 01:07:01 - INFO - __main__ -   Batch number = 254
Evaluating:  90%|█████████ | 254/282 [00:55<00:04,  5.89it/s]11/28/2021 01:07:01 - INFO - __main__ -   Batch number = 255
Evaluating:  90%|█████████ | 255/282 [00:55<00:04,  5.97it/s]11/28/2021 01:07:02 - INFO - __main__ -   Batch number = 256
Evaluating:  91%|█████████ | 256/282 [00:55<00:04,  5.93it/s]11/28/2021 01:07:02 - INFO - __main__ -   Batch number = 257
Evaluating:  91%|█████████ | 257/282 [00:55<00:04,  6.04it/s]11/28/2021 01:07:02 - INFO - __main__ -   Batch number = 258
Evaluating:  91%|█████████▏| 258/282 [00:56<00:04,  5.91it/s]11/28/2021 01:07:02 - INFO - __main__ -   Batch number = 259
Evaluating:  92%|█████████▏| 259/282 [00:56<00:03,  6.00it/s]11/28/2021 01:07:02 - INFO - __main__ -   Batch number = 260
Evaluating:  92%|█████████▏| 260/282 [00:56<00:03,  5.92it/s]11/28/2021 01:07:02 - INFO - __main__ -   Batch number = 261
Evaluating:  93%|█████████▎| 261/282 [00:56<00:03,  6.03it/s]11/28/2021 01:07:03 - INFO - __main__ -   Batch number = 262
Evaluating:  93%|█████████▎| 262/282 [00:56<00:03,  5.94it/s]11/28/2021 01:07:03 - INFO - __main__ -   Batch number = 263
Evaluating:  93%|█████████▎| 263/282 [00:56<00:03,  5.99it/s]11/28/2021 01:07:03 - INFO - __main__ -   Batch number = 264
Evaluating:  94%|█████████▎| 264/282 [00:57<00:03,  5.87it/s]11/28/2021 01:07:03 - INFO - __main__ -   Batch number = 265
Evaluating:  94%|█████████▍| 265/282 [00:57<00:02,  5.90it/s]11/28/2021 01:07:03 - INFO - __main__ -   Batch number = 266
Evaluating:  94%|█████████▍| 266/282 [00:57<00:02,  5.82it/s]11/28/2021 01:07:03 - INFO - __main__ -   Batch number = 267
Evaluating:  95%|█████████▍| 267/282 [00:57<00:02,  5.95it/s]11/28/2021 01:07:04 - INFO - __main__ -   Batch number = 268
Evaluating:  95%|█████████▌| 268/282 [00:57<00:02,  5.77it/s]11/28/2021 01:07:04 - INFO - __main__ -   Batch number = 269
Evaluating:  95%|█████████▌| 269/282 [00:57<00:02,  5.80it/s]11/28/2021 01:07:04 - INFO - __main__ -   Batch number = 270
Evaluating:  96%|█████████▌| 270/282 [00:58<00:02,  5.54it/s]11/28/2021 01:07:04 - INFO - __main__ -   Batch number = 271
Evaluating:  96%|█████████▌| 271/282 [00:58<00:02,  4.67it/s]11/28/2021 01:07:04 - INFO - __main__ -   Batch number = 272
Evaluating:  96%|█████████▋| 272/282 [00:58<00:02,  4.82it/s]11/28/2021 01:07:05 - INFO - __main__ -   Batch number = 273
Evaluating:  97%|█████████▋| 273/282 [00:58<00:01,  5.06it/s]11/28/2021 01:07:05 - INFO - __main__ -   Batch number = 274
Evaluating:  97%|█████████▋| 274/282 [00:58<00:01,  5.20it/s]11/28/2021 01:07:05 - INFO - __main__ -   Batch number = 275
Evaluating:  98%|█████████▊| 275/282 [00:59<00:01,  5.27it/s]11/28/2021 01:07:05 - INFO - __main__ -   Batch number = 276
Evaluating:  98%|█████████▊| 276/282 [00:59<00:01,  5.19it/s]11/28/2021 01:07:05 - INFO - __main__ -   Batch number = 277
Evaluating:  98%|█████████▊| 277/282 [00:59<00:00,  5.31it/s]11/28/2021 01:07:05 - INFO - __main__ -   Batch number = 278
Evaluating:  99%|█████████▊| 278/282 [00:59<00:00,  5.36it/s]11/28/2021 01:07:06 - INFO - __main__ -   Batch number = 279
Evaluating:  99%|█████████▉| 279/282 [00:59<00:00,  5.52it/s]11/28/2021 01:07:06 - INFO - __main__ -   Batch number = 280
Evaluating:  99%|█████████▉| 280/282 [01:00<00:00,  5.51it/s]11/28/2021 01:07:06 - INFO - __main__ -   Batch number = 281
Evaluating: 100%|█████████▉| 281/282 [01:00<00:00,  5.62it/s]11/28/2021 01:07:06 - INFO - __main__ -   Batch number = 282
Evaluating: 100%|██████████| 282/282 [01:00<00:00,  4.68it/s]
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:07:11 - INFO - __main__ -   ***** Evaluation result  in ru *****
11/28/2021 01:07:11 - INFO - __main__ -     f1 = 0.8430669763787125
11/28/2021 01:07:11 - INFO - __main__ -     loss = 0.5659780527683015
11/28/2021 01:07:11 - INFO - __main__ -     precision = 0.847054668347207
11/28/2021 01:07:11 - INFO - __main__ -     recall = 0.8391166543179386
63.78user 21.21system 1:23.37elapsed 101%CPU (0avgtext+0avgdata 3985492maxresident)k
0inputs+1768outputs (0major+1918132minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:07:13 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ru', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:07:13 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:07:13 - INFO - __main__ -   Seed = 2
11/28/2021 01:07:13 - INFO - root -   save model
11/28/2021 01:07:13 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ru', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:07:13 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:07:16 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:07:22 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:07:22 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:07:22 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
11/28/2021 01:07:22 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:07:22 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:07:22 - INFO - root -   loading lang adpater bh/wiki@ukp
11/28/2021 01:07:22 - INFO - __main__ -   Adapter Languages : ['bh'], Length : 1
11/28/2021 01:07:22 - INFO - __main__ -   Adapter Names ['bh/wiki@ukp'], Length : 1
11/28/2021 01:07:22 - INFO - __main__ -   Language = bh
11/28/2021 01:07:22 - INFO - __main__ -   Adapter Name = bh/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/bh/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_bh_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/adapter_config.json
Adding adapter 'bh' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/head_config.json
11/28/2021 01:07:29 - INFO - __main__ -   Language adapter for ru not found, using bh instead
11/28/2021 01:07:29 - INFO - __main__ -   Set active language adapter to bh
11/28/2021 01:07:29 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:07:29 - INFO - __main__ -   Adapter Languages = ['bh']
11/28/2021 01:07:29 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_ru_bert-base-multilingual-cased_128
11/28/2021 01:07:31 - INFO - __main__ -   ***** Running evaluation  in ru *****
11/28/2021 01:07:31 - INFO - __main__ -     Num examples = 8995
11/28/2021 01:07:31 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/282 [00:00<?, ?it/s]11/28/2021 01:07:31 - INFO - __main__ -   Batch number = 1
Evaluating:   0%|          | 1/282 [00:00<00:41,  6.73it/s]11/28/2021 01:07:31 - INFO - __main__ -   Batch number = 2
Evaluating:   1%|          | 2/282 [00:00<00:40,  6.96it/s]11/28/2021 01:07:31 - INFO - __main__ -   Batch number = 3
Evaluating:   1%|          | 3/282 [00:00<00:39,  7.02it/s]11/28/2021 01:07:31 - INFO - __main__ -   Batch number = 4
Evaluating:   1%|▏         | 4/282 [00:00<00:39,  7.08it/s]11/28/2021 01:07:31 - INFO - __main__ -   Batch number = 5
Evaluating:   2%|▏         | 5/282 [00:00<00:39,  7.09it/s]11/28/2021 01:07:32 - INFO - __main__ -   Batch number = 6
Evaluating:   2%|▏         | 6/282 [00:00<00:38,  7.10it/s]11/28/2021 01:07:32 - INFO - __main__ -   Batch number = 7
Evaluating:   2%|▏         | 7/282 [00:00<00:38,  7.11it/s]11/28/2021 01:07:32 - INFO - __main__ -   Batch number = 8
Evaluating:   3%|▎         | 8/282 [00:01<00:38,  7.10it/s]11/28/2021 01:07:32 - INFO - __main__ -   Batch number = 9
Evaluating:   3%|▎         | 9/282 [00:01<00:38,  7.10it/s]11/28/2021 01:07:32 - INFO - __main__ -   Batch number = 10
Evaluating:   4%|▎         | 10/282 [00:01<00:38,  7.08it/s]11/28/2021 01:07:32 - INFO - __main__ -   Batch number = 11
Evaluating:   4%|▍         | 11/282 [00:01<00:38,  7.08it/s]11/28/2021 01:07:32 - INFO - __main__ -   Batch number = 12
Evaluating:   4%|▍         | 12/282 [00:01<00:38,  7.05it/s]11/28/2021 01:07:33 - INFO - __main__ -   Batch number = 13
Evaluating:   5%|▍         | 13/282 [00:01<00:38,  6.98it/s]11/28/2021 01:07:33 - INFO - __main__ -   Batch number = 14
Evaluating:   5%|▍         | 14/282 [00:01<00:39,  6.85it/s]11/28/2021 01:07:33 - INFO - __main__ -   Batch number = 15
Evaluating:   5%|▌         | 15/282 [00:02<00:38,  6.88it/s]11/28/2021 01:07:33 - INFO - __main__ -   Batch number = 16
Evaluating:   6%|▌         | 16/282 [00:02<00:38,  6.90it/s]11/28/2021 01:07:33 - INFO - __main__ -   Batch number = 17
Evaluating:   6%|▌         | 17/282 [00:02<00:38,  6.87it/s]11/28/2021 01:07:33 - INFO - __main__ -   Batch number = 18
Evaluating:   6%|▋         | 18/282 [00:02<00:38,  6.92it/s]11/28/2021 01:07:33 - INFO - __main__ -   Batch number = 19
Evaluating:   7%|▋         | 19/282 [00:02<00:38,  6.89it/s]11/28/2021 01:07:34 - INFO - __main__ -   Batch number = 20
Evaluating:   7%|▋         | 20/282 [00:02<00:38,  6.80it/s]11/28/2021 01:07:34 - INFO - __main__ -   Batch number = 21
Evaluating:   7%|▋         | 21/282 [00:03<00:37,  6.87it/s]11/28/2021 01:07:34 - INFO - __main__ -   Batch number = 22
Evaluating:   8%|▊         | 22/282 [00:03<00:38,  6.82it/s]11/28/2021 01:07:34 - INFO - __main__ -   Batch number = 23
Evaluating:   8%|▊         | 23/282 [00:03<00:37,  6.82it/s]11/28/2021 01:07:34 - INFO - __main__ -   Batch number = 24
Evaluating:   9%|▊         | 24/282 [00:03<00:45,  5.62it/s]11/28/2021 01:07:34 - INFO - __main__ -   Batch number = 25
Evaluating:   9%|▉         | 25/282 [00:03<00:44,  5.76it/s]11/28/2021 01:07:35 - INFO - __main__ -   Batch number = 26
Evaluating:   9%|▉         | 26/282 [00:03<00:43,  5.87it/s]11/28/2021 01:07:35 - INFO - __main__ -   Batch number = 27
Evaluating:  10%|▉         | 27/282 [00:04<00:43,  5.89it/s]11/28/2021 01:07:35 - INFO - __main__ -   Batch number = 28
Evaluating:  10%|▉         | 28/282 [00:04<00:42,  5.95it/s]11/28/2021 01:07:35 - INFO - __main__ -   Batch number = 29
Evaluating:  10%|█         | 29/282 [00:04<00:41,  6.06it/s]11/28/2021 01:07:35 - INFO - __main__ -   Batch number = 30
Evaluating:  11%|█         | 30/282 [00:04<00:41,  6.13it/s]11/28/2021 01:07:35 - INFO - __main__ -   Batch number = 31
Evaluating:  11%|█         | 31/282 [00:04<00:40,  6.21it/s]11/28/2021 01:07:36 - INFO - __main__ -   Batch number = 32
Evaluating:  11%|█▏        | 32/282 [00:04<00:39,  6.27it/s]11/28/2021 01:07:36 - INFO - __main__ -   Batch number = 33
Evaluating:  12%|█▏        | 33/282 [00:05<00:39,  6.26it/s]11/28/2021 01:07:36 - INFO - __main__ -   Batch number = 34
Evaluating:  12%|█▏        | 34/282 [00:05<00:38,  6.37it/s]11/28/2021 01:07:36 - INFO - __main__ -   Batch number = 35
Evaluating:  12%|█▏        | 35/282 [00:05<00:38,  6.46it/s]11/28/2021 01:07:36 - INFO - __main__ -   Batch number = 36
Evaluating:  13%|█▎        | 36/282 [00:05<00:37,  6.53it/s]11/28/2021 01:07:36 - INFO - __main__ -   Batch number = 37
Evaluating:  13%|█▎        | 37/282 [00:05<00:37,  6.60it/s]11/28/2021 01:07:36 - INFO - __main__ -   Batch number = 38
Evaluating:  13%|█▎        | 38/282 [00:05<00:36,  6.65it/s]11/28/2021 01:07:37 - INFO - __main__ -   Batch number = 39
Evaluating:  14%|█▍        | 39/282 [00:05<00:36,  6.64it/s]11/28/2021 01:07:37 - INFO - __main__ -   Batch number = 40
Evaluating:  14%|█▍        | 40/282 [00:06<00:36,  6.66it/s]11/28/2021 01:07:37 - INFO - __main__ -   Batch number = 41
Evaluating:  15%|█▍        | 41/282 [00:06<00:36,  6.67it/s]11/28/2021 01:07:37 - INFO - __main__ -   Batch number = 42
Evaluating:  15%|█▍        | 42/282 [00:06<00:36,  6.64it/s]11/28/2021 01:07:37 - INFO - __main__ -   Batch number = 43
Evaluating:  15%|█▌        | 43/282 [00:06<00:36,  6.64it/s]11/28/2021 01:07:37 - INFO - __main__ -   Batch number = 44
Evaluating:  16%|█▌        | 44/282 [00:06<00:35,  6.68it/s]11/28/2021 01:07:38 - INFO - __main__ -   Batch number = 45
Evaluating:  16%|█▌        | 45/282 [00:06<00:35,  6.68it/s]11/28/2021 01:07:38 - INFO - __main__ -   Batch number = 46
Evaluating:  16%|█▋        | 46/282 [00:06<00:35,  6.65it/s]11/28/2021 01:07:38 - INFO - __main__ -   Batch number = 47
Evaluating:  17%|█▋        | 47/282 [00:07<00:35,  6.65it/s]11/28/2021 01:07:38 - INFO - __main__ -   Batch number = 48
Evaluating:  17%|█▋        | 48/282 [00:07<00:35,  6.56it/s]11/28/2021 01:07:38 - INFO - __main__ -   Batch number = 49
Evaluating:  17%|█▋        | 49/282 [00:07<00:35,  6.49it/s]11/28/2021 01:07:38 - INFO - __main__ -   Batch number = 50
Evaluating:  18%|█▊        | 50/282 [00:07<00:35,  6.55it/s]11/28/2021 01:07:38 - INFO - __main__ -   Batch number = 51
Evaluating:  18%|█▊        | 51/282 [00:07<00:35,  6.59it/s]11/28/2021 01:07:39 - INFO - __main__ -   Batch number = 52
Evaluating:  18%|█▊        | 52/282 [00:07<00:35,  6.51it/s]11/28/2021 01:07:39 - INFO - __main__ -   Batch number = 53
Evaluating:  19%|█▉        | 53/282 [00:08<00:35,  6.47it/s]11/28/2021 01:07:39 - INFO - __main__ -   Batch number = 54
Evaluating:  19%|█▉        | 54/282 [00:08<00:35,  6.44it/s]11/28/2021 01:07:39 - INFO - __main__ -   Batch number = 55
Evaluating:  20%|█▉        | 55/282 [00:08<00:35,  6.44it/s]11/28/2021 01:07:39 - INFO - __main__ -   Batch number = 56
Evaluating:  20%|█▉        | 56/282 [00:08<00:40,  5.54it/s]11/28/2021 01:07:39 - INFO - __main__ -   Batch number = 57
Evaluating:  20%|██        | 57/282 [00:08<00:38,  5.78it/s]11/28/2021 01:07:40 - INFO - __main__ -   Batch number = 58
Evaluating:  21%|██        | 58/282 [00:08<00:37,  5.96it/s]11/28/2021 01:07:40 - INFO - __main__ -   Batch number = 59
Evaluating:  21%|██        | 59/282 [00:09<00:36,  6.15it/s]11/28/2021 01:07:40 - INFO - __main__ -   Batch number = 60
Evaluating:  21%|██▏       | 60/282 [00:09<00:35,  6.31it/s]11/28/2021 01:07:40 - INFO - __main__ -   Batch number = 61
Evaluating:  22%|██▏       | 61/282 [00:09<00:34,  6.31it/s]11/28/2021 01:07:40 - INFO - __main__ -   Batch number = 62
Evaluating:  22%|██▏       | 62/282 [00:09<00:34,  6.31it/s]11/28/2021 01:07:40 - INFO - __main__ -   Batch number = 63
Evaluating:  22%|██▏       | 63/282 [00:09<00:34,  6.35it/s]11/28/2021 01:07:41 - INFO - __main__ -   Batch number = 64
Evaluating:  23%|██▎       | 64/282 [00:09<00:34,  6.36it/s]11/28/2021 01:07:41 - INFO - __main__ -   Batch number = 65
Evaluating:  23%|██▎       | 65/282 [00:09<00:34,  6.36it/s]11/28/2021 01:07:41 - INFO - __main__ -   Batch number = 66
Evaluating:  23%|██▎       | 66/282 [00:10<00:33,  6.36it/s]11/28/2021 01:07:41 - INFO - __main__ -   Batch number = 67
Evaluating:  24%|██▍       | 67/282 [00:10<00:33,  6.38it/s]11/28/2021 01:07:41 - INFO - __main__ -   Batch number = 68
Evaluating:  24%|██▍       | 68/282 [00:10<00:33,  6.40it/s]11/28/2021 01:07:41 - INFO - __main__ -   Batch number = 69
Evaluating:  24%|██▍       | 69/282 [00:10<00:33,  6.41it/s]11/28/2021 01:07:41 - INFO - __main__ -   Batch number = 70
Evaluating:  25%|██▍       | 70/282 [00:10<00:32,  6.45it/s]11/28/2021 01:07:42 - INFO - __main__ -   Batch number = 71
Evaluating:  25%|██▌       | 71/282 [00:10<00:32,  6.45it/s]11/28/2021 01:07:42 - INFO - __main__ -   Batch number = 72
Evaluating:  26%|██▌       | 72/282 [00:11<00:32,  6.50it/s]11/28/2021 01:07:42 - INFO - __main__ -   Batch number = 73
Evaluating:  26%|██▌       | 73/282 [00:11<00:32,  6.42it/s]11/28/2021 01:07:42 - INFO - __main__ -   Batch number = 74
Evaluating:  26%|██▌       | 74/282 [00:11<00:32,  6.40it/s]11/28/2021 01:07:42 - INFO - __main__ -   Batch number = 75
Evaluating:  27%|██▋       | 75/282 [00:11<00:32,  6.36it/s]11/28/2021 01:07:42 - INFO - __main__ -   Batch number = 76
Evaluating:  27%|██▋       | 76/282 [00:11<00:32,  6.43it/s]11/28/2021 01:07:43 - INFO - __main__ -   Batch number = 77
Evaluating:  27%|██▋       | 77/282 [00:11<00:32,  6.39it/s]11/28/2021 01:07:43 - INFO - __main__ -   Batch number = 78
Evaluating:  28%|██▊       | 78/282 [00:12<00:32,  6.33it/s]11/28/2021 01:07:43 - INFO - __main__ -   Batch number = 79
Evaluating:  28%|██▊       | 79/282 [00:12<00:32,  6.28it/s]11/28/2021 01:07:43 - INFO - __main__ -   Batch number = 80
Evaluating:  28%|██▊       | 80/282 [00:12<00:32,  6.27it/s]11/28/2021 01:07:43 - INFO - __main__ -   Batch number = 81
Evaluating:  29%|██▊       | 81/282 [00:12<00:32,  6.19it/s]11/28/2021 01:07:43 - INFO - __main__ -   Batch number = 82
Evaluating:  29%|██▉       | 82/282 [00:12<00:32,  6.14it/s]11/28/2021 01:07:44 - INFO - __main__ -   Batch number = 83
Evaluating:  29%|██▉       | 83/282 [00:12<00:32,  6.12it/s]11/28/2021 01:07:44 - INFO - __main__ -   Batch number = 84
Evaluating:  30%|██▉       | 84/282 [00:13<00:32,  6.09it/s]11/28/2021 01:07:44 - INFO - __main__ -   Batch number = 85
Evaluating:  30%|███       | 85/282 [00:13<00:32,  6.10it/s]11/28/2021 01:07:44 - INFO - __main__ -   Batch number = 86
Evaluating:  30%|███       | 86/282 [00:13<00:31,  6.14it/s]11/28/2021 01:07:44 - INFO - __main__ -   Batch number = 87
Evaluating:  31%|███       | 87/282 [00:13<00:32,  6.03it/s]11/28/2021 01:07:44 - INFO - __main__ -   Batch number = 88
Evaluating:  31%|███       | 88/282 [00:13<00:34,  5.69it/s]11/28/2021 01:07:45 - INFO - __main__ -   Batch number = 89
Evaluating:  32%|███▏      | 89/282 [00:13<00:32,  5.85it/s]11/28/2021 01:07:45 - INFO - __main__ -   Batch number = 90
Evaluating:  32%|███▏      | 90/282 [00:14<00:32,  5.93it/s]11/28/2021 01:07:45 - INFO - __main__ -   Batch number = 91
Evaluating:  32%|███▏      | 91/282 [00:14<00:31,  5.97it/s]11/28/2021 01:07:45 - INFO - __main__ -   Batch number = 92
Evaluating:  33%|███▎      | 92/282 [00:14<00:31,  5.95it/s]11/28/2021 01:07:45 - INFO - __main__ -   Batch number = 93
Evaluating:  33%|███▎      | 93/282 [00:14<00:35,  5.30it/s]11/28/2021 01:07:45 - INFO - __main__ -   Batch number = 94
Evaluating:  33%|███▎      | 94/282 [00:14<00:43,  4.34it/s]11/28/2021 01:07:46 - INFO - __main__ -   Batch number = 95
Evaluating:  34%|███▎      | 95/282 [00:15<00:48,  3.83it/s]11/28/2021 01:07:46 - INFO - __main__ -   Batch number = 96
Evaluating:  34%|███▍      | 96/282 [00:15<00:52,  3.51it/s]11/28/2021 01:07:46 - INFO - __main__ -   Batch number = 97
Evaluating:  34%|███▍      | 97/282 [00:15<00:56,  3.30it/s]11/28/2021 01:07:47 - INFO - __main__ -   Batch number = 98
Evaluating:  35%|███▍      | 98/282 [00:16<00:57,  3.18it/s]11/28/2021 01:07:47 - INFO - __main__ -   Batch number = 99
Evaluating:  35%|███▌      | 99/282 [00:16<00:59,  3.10it/s]11/28/2021 01:07:47 - INFO - __main__ -   Batch number = 100
Evaluating:  35%|███▌      | 100/282 [00:16<00:59,  3.06it/s]11/28/2021 01:07:48 - INFO - __main__ -   Batch number = 101
Evaluating:  36%|███▌      | 101/282 [00:17<00:59,  3.04it/s]11/28/2021 01:07:48 - INFO - __main__ -   Batch number = 102
Evaluating:  36%|███▌      | 102/282 [00:17<00:59,  3.01it/s]11/28/2021 01:07:48 - INFO - __main__ -   Batch number = 103
Evaluating:  37%|███▋      | 103/282 [00:17<00:59,  2.99it/s]11/28/2021 01:07:49 - INFO - __main__ -   Batch number = 104
Evaluating:  37%|███▋      | 104/282 [00:18<00:59,  2.98it/s]11/28/2021 01:07:49 - INFO - __main__ -   Batch number = 105
Evaluating:  37%|███▋      | 105/282 [00:18<00:57,  3.07it/s]11/28/2021 01:07:50 - INFO - __main__ -   Batch number = 106
Evaluating:  38%|███▊      | 106/282 [00:18<01:00,  2.92it/s]11/28/2021 01:07:50 - INFO - __main__ -   Batch number = 107
Evaluating:  38%|███▊      | 107/282 [00:19<00:58,  3.01it/s]11/28/2021 01:07:50 - INFO - __main__ -   Batch number = 108
Evaluating:  38%|███▊      | 108/282 [00:19<00:56,  3.06it/s]11/28/2021 01:07:50 - INFO - __main__ -   Batch number = 109
Evaluating:  39%|███▊      | 109/282 [00:19<00:56,  3.07it/s]11/28/2021 01:07:51 - INFO - __main__ -   Batch number = 110
Evaluating:  39%|███▉      | 110/282 [00:20<00:56,  3.06it/s]11/28/2021 01:07:51 - INFO - __main__ -   Batch number = 111
Evaluating:  39%|███▉      | 111/282 [00:20<00:56,  3.03it/s]11/28/2021 01:07:51 - INFO - __main__ -   Batch number = 112
Evaluating:  40%|███▉      | 112/282 [00:20<00:56,  3.01it/s]11/28/2021 01:07:52 - INFO - __main__ -   Batch number = 113
Evaluating:  40%|████      | 113/282 [00:21<00:56,  2.99it/s]11/28/2021 01:07:52 - INFO - __main__ -   Batch number = 114
Evaluating:  40%|████      | 114/282 [00:21<00:56,  2.96it/s]11/28/2021 01:07:52 - INFO - __main__ -   Batch number = 115
Evaluating:  41%|████      | 115/282 [00:21<00:56,  2.95it/s]11/28/2021 01:07:53 - INFO - __main__ -   Batch number = 116
Evaluating:  41%|████      | 116/282 [00:22<00:56,  2.94it/s]11/28/2021 01:07:53 - INFO - __main__ -   Batch number = 117
Evaluating:  41%|████▏     | 117/282 [00:22<00:56,  2.93it/s]11/28/2021 01:07:54 - INFO - __main__ -   Batch number = 118
Evaluating:  42%|████▏     | 118/282 [00:22<00:56,  2.93it/s]11/28/2021 01:07:54 - INFO - __main__ -   Batch number = 119
Evaluating:  42%|████▏     | 119/282 [00:23<00:55,  2.93it/s]11/28/2021 01:07:54 - INFO - __main__ -   Batch number = 120
Evaluating:  43%|████▎     | 120/282 [00:23<00:55,  2.91it/s]11/28/2021 01:07:55 - INFO - __main__ -   Batch number = 121
Evaluating:  43%|████▎     | 121/282 [00:24<00:55,  2.91it/s]11/28/2021 01:07:55 - INFO - __main__ -   Batch number = 122
Evaluating:  43%|████▎     | 122/282 [00:24<00:54,  2.92it/s]11/28/2021 01:07:55 - INFO - __main__ -   Batch number = 123
Evaluating:  44%|████▎     | 123/282 [00:24<00:54,  2.92it/s]11/28/2021 01:07:56 - INFO - __main__ -   Batch number = 124
Evaluating:  44%|████▍     | 124/282 [00:25<00:54,  2.91it/s]11/28/2021 01:07:56 - INFO - __main__ -   Batch number = 125
Evaluating:  44%|████▍     | 125/282 [00:25<00:54,  2.90it/s]11/28/2021 01:07:56 - INFO - __main__ -   Batch number = 126
Evaluating:  45%|████▍     | 126/282 [00:25<00:53,  2.90it/s]11/28/2021 01:07:57 - INFO - __main__ -   Batch number = 127
Evaluating:  45%|████▌     | 127/282 [00:26<00:53,  2.90it/s]11/28/2021 01:07:57 - INFO - __main__ -   Batch number = 128
Evaluating:  45%|████▌     | 128/282 [00:26<00:52,  2.91it/s]11/28/2021 01:07:57 - INFO - __main__ -   Batch number = 129
Evaluating:  46%|████▌     | 129/282 [00:26<00:52,  2.91it/s]11/28/2021 01:07:58 - INFO - __main__ -   Batch number = 130
Evaluating:  46%|████▌     | 130/282 [00:27<00:52,  2.91it/s]11/28/2021 01:07:58 - INFO - __main__ -   Batch number = 131
Evaluating:  46%|████▋     | 131/282 [00:27<00:52,  2.90it/s]11/28/2021 01:07:58 - INFO - __main__ -   Batch number = 132
Evaluating:  47%|████▋     | 132/282 [00:27<00:56,  2.65it/s]11/28/2021 01:07:59 - INFO - __main__ -   Batch number = 133
Evaluating:  47%|████▋     | 133/282 [00:28<00:54,  2.71it/s]11/28/2021 01:07:59 - INFO - __main__ -   Batch number = 134
Evaluating:  48%|████▊     | 134/282 [00:28<00:53,  2.74it/s]11/28/2021 01:07:59 - INFO - __main__ -   Batch number = 135
Evaluating:  48%|████▊     | 135/282 [00:28<00:52,  2.78it/s]11/28/2021 01:08:00 - INFO - __main__ -   Batch number = 136
Evaluating:  48%|████▊     | 136/282 [00:29<00:52,  2.81it/s]11/28/2021 01:08:00 - INFO - __main__ -   Batch number = 137
Evaluating:  49%|████▊     | 137/282 [00:29<00:51,  2.81it/s]11/28/2021 01:08:01 - INFO - __main__ -   Batch number = 138
Evaluating:  49%|████▉     | 138/282 [00:30<00:50,  2.83it/s]11/28/2021 01:08:01 - INFO - __main__ -   Batch number = 139
Evaluating:  49%|████▉     | 139/282 [00:30<00:50,  2.85it/s]11/28/2021 01:08:01 - INFO - __main__ -   Batch number = 140
Evaluating:  50%|████▉     | 140/282 [00:30<00:49,  2.88it/s]11/28/2021 01:08:02 - INFO - __main__ -   Batch number = 141
Evaluating:  50%|█████     | 141/282 [00:31<00:48,  2.89it/s]11/28/2021 01:08:02 - INFO - __main__ -   Batch number = 142
Evaluating:  50%|█████     | 142/282 [00:31<00:48,  2.90it/s]11/28/2021 01:08:02 - INFO - __main__ -   Batch number = 143
Evaluating:  51%|█████     | 143/282 [00:31<00:48,  2.88it/s]11/28/2021 01:08:03 - INFO - __main__ -   Batch number = 144
Evaluating:  51%|█████     | 144/282 [00:32<00:47,  2.88it/s]11/28/2021 01:08:03 - INFO - __main__ -   Batch number = 145
Evaluating:  51%|█████▏    | 145/282 [00:32<00:47,  2.89it/s]11/28/2021 01:08:03 - INFO - __main__ -   Batch number = 146
Evaluating:  52%|█████▏    | 146/282 [00:32<00:48,  2.81it/s]11/28/2021 01:08:04 - INFO - __main__ -   Batch number = 147
Evaluating:  52%|█████▏    | 147/282 [00:33<00:47,  2.82it/s]11/28/2021 01:08:04 - INFO - __main__ -   Batch number = 148
Evaluating:  52%|█████▏    | 148/282 [00:33<00:47,  2.84it/s]11/28/2021 01:08:04 - INFO - __main__ -   Batch number = 149
Evaluating:  53%|█████▎    | 149/282 [00:33<00:46,  2.86it/s]11/28/2021 01:08:05 - INFO - __main__ -   Batch number = 150
Evaluating:  53%|█████▎    | 150/282 [00:34<00:45,  2.87it/s]11/28/2021 01:08:05 - INFO - __main__ -   Batch number = 151
Evaluating:  54%|█████▎    | 151/282 [00:34<00:45,  2.88it/s]11/28/2021 01:08:05 - INFO - __main__ -   Batch number = 152
Evaluating:  54%|█████▍    | 152/282 [00:34<00:44,  2.89it/s]11/28/2021 01:08:06 - INFO - __main__ -   Batch number = 153
Evaluating:  54%|█████▍    | 153/282 [00:35<00:44,  2.90it/s]11/28/2021 01:08:06 - INFO - __main__ -   Batch number = 154
Evaluating:  55%|█████▍    | 154/282 [00:35<00:44,  2.90it/s]11/28/2021 01:08:06 - INFO - __main__ -   Batch number = 155
Evaluating:  55%|█████▍    | 155/282 [00:35<00:43,  2.91it/s]11/28/2021 01:08:07 - INFO - __main__ -   Batch number = 156
Evaluating:  55%|█████▌    | 156/282 [00:36<00:43,  2.93it/s]11/28/2021 01:08:07 - INFO - __main__ -   Batch number = 157
Evaluating:  56%|█████▌    | 157/282 [00:36<00:43,  2.90it/s]11/28/2021 01:08:07 - INFO - __main__ -   Batch number = 158
Evaluating:  56%|█████▌    | 158/282 [00:36<00:42,  2.91it/s]11/28/2021 01:08:08 - INFO - __main__ -   Batch number = 159
Evaluating:  56%|█████▋    | 159/282 [00:37<00:42,  2.90it/s]11/28/2021 01:08:08 - INFO - __main__ -   Batch number = 160
Evaluating:  57%|█████▋    | 160/282 [00:37<00:41,  2.91it/s]11/28/2021 01:08:09 - INFO - __main__ -   Batch number = 161
PyTorch version 1.10.0+cu102 available.
Evaluating:  57%|█████▋    | 161/282 [00:37<00:41,  2.92it/s]11/28/2021 01:08:09 - INFO - __main__ -   Batch number = 162
11/28/2021 01:08:09 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:08:09 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:08:09 - INFO - __main__ -   Seed = 1
11/28/2021 01:08:09 - INFO - root -   save model
11/28/2021 01:08:09 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:08:09 - INFO - __main__ -   Loading pretrained model and tokenizer
Evaluating:  57%|█████▋    | 162/282 [00:38<00:41,  2.91it/s]11/28/2021 01:08:09 - INFO - __main__ -   Batch number = 163
Evaluating:  58%|█████▊    | 163/282 [00:38<00:40,  2.91it/s]11/28/2021 01:08:10 - INFO - __main__ -   Batch number = 164
Evaluating:  58%|█████▊    | 164/282 [00:39<00:40,  2.92it/s]11/28/2021 01:08:10 - INFO - __main__ -   Batch number = 165
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  59%|█████▊    | 165/282 [00:39<00:40,  2.91it/s]11/28/2021 01:08:10 - INFO - __main__ -   Batch number = 166
Evaluating:  59%|█████▉    | 166/282 [00:39<00:39,  2.92it/s]11/28/2021 01:08:11 - INFO - __main__ -   Batch number = 167
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  59%|█████▉    | 167/282 [00:40<00:39,  2.92it/s]11/28/2021 01:08:11 - INFO - __main__ -   Batch number = 168
Evaluating:  60%|█████▉    | 168/282 [00:40<00:38,  2.93it/s]11/28/2021 01:08:11 - INFO - __main__ -   Batch number = 169
loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
Evaluating:  60%|█████▉    | 169/282 [00:40<00:38,  2.92it/s]11/28/2021 01:08:12 - INFO - __main__ -   Batch number = 170
11/28/2021 01:08:12 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
Evaluating:  60%|██████    | 170/282 [00:41<00:38,  2.94it/s]11/28/2021 01:08:12 - INFO - __main__ -   Batch number = 171
Evaluating:  61%|██████    | 171/282 [00:41<00:32,  3.45it/s]11/28/2021 01:08:12 - INFO - __main__ -   Batch number = 172
Evaluating:  61%|██████    | 172/282 [00:41<00:27,  3.94it/s]11/28/2021 01:08:12 - INFO - __main__ -   Batch number = 173
Evaluating:  61%|██████▏   | 173/282 [00:41<00:24,  4.39it/s]11/28/2021 01:08:12 - INFO - __main__ -   Batch number = 174
Evaluating:  62%|██████▏   | 174/282 [00:41<00:22,  4.79it/s]11/28/2021 01:08:13 - INFO - __main__ -   Batch number = 175
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  62%|██████▏   | 175/282 [00:41<00:20,  5.14it/s]11/28/2021 01:08:13 - INFO - __main__ -   Batch number = 176
Evaluating:  62%|██████▏   | 176/282 [00:42<00:19,  5.40it/s]11/28/2021 01:08:13 - INFO - __main__ -   Batch number = 177
Evaluating:  63%|██████▎   | 177/282 [00:42<00:18,  5.61it/s]11/28/2021 01:08:13 - INFO - __main__ -   Batch number = 178
Evaluating:  63%|██████▎   | 178/282 [00:42<00:18,  5.76it/s]11/28/2021 01:08:13 - INFO - __main__ -   Batch number = 179
Evaluating:  63%|██████▎   | 179/282 [00:42<00:17,  5.87it/s]11/28/2021 01:08:13 - INFO - __main__ -   Batch number = 180
Evaluating:  64%|██████▍   | 180/282 [00:42<00:17,  5.95it/s]11/28/2021 01:08:14 - INFO - __main__ -   Batch number = 181
Evaluating:  64%|██████▍   | 181/282 [00:42<00:16,  5.99it/s]11/28/2021 01:08:14 - INFO - __main__ -   Batch number = 182
Evaluating:  65%|██████▍   | 182/282 [00:43<00:16,  5.98it/s]11/28/2021 01:08:14 - INFO - __main__ -   Batch number = 183
Evaluating:  65%|██████▍   | 183/282 [00:43<00:16,  5.99it/s]11/28/2021 01:08:14 - INFO - __main__ -   Batch number = 184
Evaluating:  65%|██████▌   | 184/282 [00:43<00:17,  5.74it/s]11/28/2021 01:08:14 - INFO - __main__ -   Batch number = 185
Evaluating:  66%|██████▌   | 185/282 [00:43<00:16,  5.82it/s]11/28/2021 01:08:14 - INFO - __main__ -   Batch number = 186
Evaluating:  66%|██████▌   | 186/282 [00:43<00:16,  5.88it/s]11/28/2021 01:08:15 - INFO - __main__ -   Batch number = 187
Evaluating:  66%|██████▋   | 187/282 [00:43<00:16,  5.88it/s]11/28/2021 01:08:15 - INFO - __main__ -   Batch number = 188
Evaluating:  67%|██████▋   | 188/282 [00:44<00:15,  5.92it/s]11/28/2021 01:08:15 - INFO - __main__ -   Batch number = 189
Evaluating:  67%|██████▋   | 189/282 [00:44<00:15,  5.86it/s]11/28/2021 01:08:15 - INFO - __main__ -   Batch number = 190
Evaluating:  67%|██████▋   | 190/282 [00:44<00:15,  5.78it/s]11/28/2021 01:08:15 - INFO - __main__ -   Batch number = 191
Evaluating:  68%|██████▊   | 191/282 [00:44<00:15,  5.79it/s]11/28/2021 01:08:15 - INFO - __main__ -   Batch number = 192
Evaluating:  68%|██████▊   | 192/282 [00:44<00:15,  5.83it/s]11/28/2021 01:08:16 - INFO - __main__ -   Batch number = 193
Evaluating:  68%|██████▊   | 193/282 [00:44<00:15,  5.84it/s]11/28/2021 01:08:16 - INFO - __main__ -   Batch number = 194
Evaluating:  69%|██████▉   | 194/282 [00:45<00:15,  5.83it/s]11/28/2021 01:08:16 - INFO - __main__ -   Batch number = 195
Evaluating:  69%|██████▉   | 195/282 [00:45<00:15,  5.69it/s]11/28/2021 01:08:16 - INFO - __main__ -   Batch number = 196
Evaluating:  70%|██████▉   | 196/282 [00:45<00:15,  5.62it/s]11/28/2021 01:08:16 - INFO - __main__ -   Batch number = 197
Evaluating:  70%|██████▉   | 197/282 [00:45<00:15,  5.62it/s]11/28/2021 01:08:17 - INFO - __main__ -   Batch number = 198
Evaluating:  70%|███████   | 198/282 [00:45<00:14,  5.70it/s]11/28/2021 01:08:17 - INFO - __main__ -   Batch number = 199
Evaluating:  71%|███████   | 199/282 [00:45<00:14,  5.74it/s]11/28/2021 01:08:17 - INFO - __main__ -   Batch number = 200
Evaluating:  71%|███████   | 200/282 [00:46<00:14,  5.84it/s]11/28/2021 01:08:17 - INFO - __main__ -   Batch number = 201
Evaluating:  71%|███████▏  | 201/282 [00:46<00:13,  5.86it/s]11/28/2021 01:08:17 - INFO - __main__ -   Batch number = 202
Evaluating:  72%|███████▏  | 202/282 [00:46<00:13,  5.84it/s]11/28/2021 01:08:17 - INFO - __main__ -   Batch number = 203
Evaluating:  72%|███████▏  | 203/282 [00:46<00:13,  5.84it/s]11/28/2021 01:08:18 - INFO - __main__ -   Batch number = 204
Evaluating:  72%|███████▏  | 204/282 [00:46<00:13,  5.87it/s]11/28/2021 01:08:18 - INFO - __main__ -   Batch number = 205
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:08:18 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:08:18 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:08:18 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
11/28/2021 01:08:18 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:08:18 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_adapter.bin
Evaluating:  73%|███████▎  | 205/282 [00:47<00:13,  5.82it/s]11/28/2021 01:08:18 - INFO - __main__ -   Batch number = 206
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:08:18 - INFO - root -   loading lang adpater bh/wiki@ukp
11/28/2021 01:08:18 - INFO - __main__ -   Adapter Languages : ['bh'], Length : 1
11/28/2021 01:08:18 - INFO - __main__ -   Adapter Names ['bh/wiki@ukp'], Length : 1
11/28/2021 01:08:18 - INFO - __main__ -   Language = bh
11/28/2021 01:08:18 - INFO - __main__ -   Adapter Name = bh/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/bh/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_bh_wiki_pfeiffer.zip.
Evaluating:  73%|███████▎  | 206/282 [00:47<00:13,  5.83it/s]11/28/2021 01:08:18 - INFO - __main__ -   Batch number = 207
Evaluating:  73%|███████▎  | 207/282 [00:47<00:12,  5.82it/s]11/28/2021 01:08:18 - INFO - __main__ -   Batch number = 208
Evaluating:  74%|███████▍  | 208/282 [00:47<00:13,  5.68it/s]11/28/2021 01:08:18 - INFO - __main__ -   Batch number = 209
Evaluating:  74%|███████▍  | 209/282 [00:47<00:12,  5.63it/s]11/28/2021 01:08:19 - INFO - __main__ -   Batch number = 210
Evaluating:  74%|███████▍  | 210/282 [00:47<00:12,  5.63it/s]11/28/2021 01:08:19 - INFO - __main__ -   Batch number = 211
Evaluating:  75%|███████▍  | 211/282 [00:48<00:12,  5.65it/s]11/28/2021 01:08:19 - INFO - __main__ -   Batch number = 212
Evaluating:  75%|███████▌  | 212/282 [00:48<00:12,  5.72it/s]11/28/2021 01:08:19 - INFO - __main__ -   Batch number = 213
Evaluating:  76%|███████▌  | 213/282 [00:48<00:12,  5.74it/s]11/28/2021 01:08:19 - INFO - __main__ -   Batch number = 214
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/adapter_config.json
Adding adapter 'bh' of type 'text_lang'.
Evaluating:  76%|███████▌  | 214/282 [00:48<00:11,  5.75it/s]11/28/2021 01:08:19 - INFO - __main__ -   Batch number = 215
Loading module weights from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/head_config.json
Evaluating:  76%|███████▌  | 215/282 [00:48<00:11,  5.68it/s]11/28/2021 01:08:20 - INFO - __main__ -   Batch number = 216
Evaluating:  77%|███████▋  | 216/282 [00:48<00:11,  5.57it/s]11/28/2021 01:08:20 - INFO - __main__ -   Batch number = 217
Evaluating:  77%|███████▋  | 217/282 [00:49<00:11,  5.53it/s]11/28/2021 01:08:20 - INFO - __main__ -   Batch number = 218
Evaluating:  77%|███████▋  | 218/282 [00:49<00:11,  5.55it/s]11/28/2021 01:08:20 - INFO - __main__ -   Batch number = 219
Evaluating:  78%|███████▊  | 219/282 [00:49<00:11,  5.56it/s]11/28/2021 01:08:20 - INFO - __main__ -   Batch number = 220
Evaluating:  78%|███████▊  | 220/282 [00:49<00:11,  5.61it/s]11/28/2021 01:08:21 - INFO - __main__ -   Batch number = 221
Evaluating:  78%|███████▊  | 221/282 [00:49<00:10,  5.69it/s]11/28/2021 01:08:21 - INFO - __main__ -   Batch number = 222
Evaluating:  79%|███████▊  | 222/282 [00:50<00:10,  5.71it/s]11/28/2021 01:08:21 - INFO - __main__ -   Batch number = 223
Evaluating:  79%|███████▉  | 223/282 [00:50<00:10,  5.64it/s]11/28/2021 01:08:21 - INFO - __main__ -   Batch number = 224
Evaluating:  79%|███████▉  | 224/282 [00:50<00:10,  5.60it/s]11/28/2021 01:08:21 - INFO - __main__ -   Batch number = 225
Evaluating:  80%|███████▉  | 225/282 [00:50<00:10,  5.63it/s]11/28/2021 01:08:21 - INFO - __main__ -   Batch number = 226
Evaluating:  80%|████████  | 226/282 [00:50<00:09,  5.67it/s]11/28/2021 01:08:22 - INFO - __main__ -   Batch number = 227
Evaluating:  80%|████████  | 227/282 [00:50<00:09,  5.70it/s]11/28/2021 01:08:22 - INFO - __main__ -   Batch number = 228
Evaluating:  81%|████████  | 228/282 [00:51<00:09,  5.74it/s]11/28/2021 01:08:22 - INFO - __main__ -   Batch number = 229
Evaluating:  81%|████████  | 229/282 [00:51<00:09,  5.70it/s]11/28/2021 01:08:22 - INFO - __main__ -   Batch number = 230
Evaluating:  82%|████████▏ | 230/282 [00:51<00:09,  5.57it/s]11/28/2021 01:08:22 - INFO - __main__ -   Batch number = 231
Evaluating:  82%|████████▏ | 231/282 [00:51<00:09,  5.52it/s]11/28/2021 01:08:22 - INFO - __main__ -   Batch number = 232
Evaluating:  82%|████████▏ | 232/282 [00:51<00:08,  5.57it/s]11/28/2021 01:08:23 - INFO - __main__ -   Batch number = 233
Evaluating:  83%|████████▎ | 233/282 [00:51<00:08,  5.58it/s]11/28/2021 01:08:23 - INFO - __main__ -   Batch number = 234
Evaluating:  83%|████████▎ | 234/282 [00:52<00:08,  5.62it/s]11/28/2021 01:08:23 - INFO - __main__ -   Batch number = 235
Evaluating:  83%|████████▎ | 235/282 [00:52<00:08,  5.64it/s]11/28/2021 01:08:23 - INFO - __main__ -   Batch number = 236
Evaluating:  84%|████████▎ | 236/282 [00:52<00:08,  5.69it/s]11/28/2021 01:08:24 - INFO - __main__ -   Batch number = 237
Evaluating:  84%|████████▍ | 237/282 [00:52<00:09,  4.55it/s]11/28/2021 01:08:24 - INFO - __main__ -   Batch number = 238
Evaluating:  84%|████████▍ | 238/282 [00:53<00:09,  4.81it/s]11/28/2021 01:08:24 - INFO - __main__ -   Batch number = 239
Evaluating:  85%|████████▍ | 239/282 [00:53<00:08,  5.01it/s]11/28/2021 01:08:24 - INFO - __main__ -   Batch number = 240
Evaluating:  85%|████████▌ | 240/282 [00:53<00:08,  5.18it/s]11/28/2021 01:08:24 - INFO - __main__ -   Batch number = 241
Evaluating:  85%|████████▌ | 241/282 [00:53<00:07,  5.31it/s]11/28/2021 01:08:24 - INFO - __main__ -   Batch number = 242
Evaluating:  86%|████████▌ | 242/282 [00:53<00:07,  5.30it/s]11/28/2021 01:08:25 - INFO - __main__ -   Batch number = 243
Evaluating:  86%|████████▌ | 243/282 [00:53<00:07,  5.22it/s]11/28/2021 01:08:25 - INFO - __main__ -   Batch number = 244
Evaluating:  87%|████████▋ | 244/282 [00:54<00:07,  5.21it/s]11/28/2021 01:08:25 - INFO - __main__ -   Batch number = 245
Evaluating:  87%|████████▋ | 245/282 [00:54<00:07,  5.26it/s]11/28/2021 01:08:25 - INFO - __main__ -   Batch number = 246
Evaluating:  87%|████████▋ | 246/282 [00:54<00:06,  5.34it/s]11/28/2021 01:08:25 - INFO - __main__ -   Batch number = 247
Evaluating:  88%|████████▊ | 247/282 [00:54<00:06,  5.41it/s]11/28/2021 01:08:26 - INFO - __main__ -   Batch number = 248
Evaluating:  88%|████████▊ | 248/282 [00:54<00:06,  5.46it/s]11/28/2021 01:08:26 - INFO - __main__ -   Batch number = 249
Evaluating:  88%|████████▊ | 249/282 [00:55<00:06,  5.48it/s]11/28/2021 01:08:26 - INFO - __main__ -   Batch number = 250
Evaluating:  89%|████████▊ | 250/282 [00:55<00:05,  5.49it/s]11/28/2021 01:08:26 - INFO - __main__ -   Batch number = 251
Evaluating:  89%|████████▉ | 251/282 [00:55<00:05,  5.53it/s]11/28/2021 01:08:26 - INFO - __main__ -   Batch number = 252
Evaluating:  89%|████████▉ | 252/282 [00:55<00:05,  5.52it/s]11/28/2021 01:08:26 - INFO - __main__ -   Batch number = 253
Evaluating:  90%|████████▉ | 253/282 [00:55<00:05,  5.36it/s]11/28/2021 01:08:27 - INFO - __main__ -   Batch number = 254
11/28/2021 01:08:27 - INFO - __main__ -   Language mi, split test does not exist
Evaluating:  90%|█████████ | 254/282 [00:55<00:05,  5.27it/s]11/28/2021 01:08:27 - INFO - __main__ -   Batch number = 255
Evaluating:  90%|█████████ | 255/282 [00:56<00:05,  5.06it/s]11/28/2021 01:08:27 - INFO - __main__ -   Batch number = 256
Evaluating:  91%|█████████ | 256/282 [00:56<00:05,  5.12it/s]11/28/2021 01:08:27 - INFO - __main__ -   Batch number = 257
Evaluating:  91%|█████████ | 257/282 [00:56<00:04,  5.17it/s]11/28/2021 01:08:27 - INFO - __main__ -   Batch number = 258
Evaluating:  91%|█████████▏| 258/282 [00:56<00:04,  5.33it/s]11/28/2021 01:08:28 - INFO - __main__ -   Batch number = 259
15.18user 5.68system 0:20.45elapsed 102%CPU (0avgtext+0avgdata 3990724maxresident)k
0inputs+48outputs (0major+1506679minor)pagefaults 0swaps
Evaluating:  92%|█████████▏| 259/282 [00:56<00:04,  5.43it/s]11/28/2021 01:08:28 - INFO - __main__ -   Batch number = 260
Evaluating:  92%|█████████▏| 260/282 [00:57<00:04,  5.47it/s]11/28/2021 01:08:28 - INFO - __main__ -   Batch number = 261
Evaluating:  93%|█████████▎| 261/282 [00:57<00:03,  5.42it/s]11/28/2021 01:08:28 - INFO - __main__ -   Batch number = 262
Evaluating:  93%|█████████▎| 262/282 [00:57<00:03,  5.41it/s]11/28/2021 01:08:28 - INFO - __main__ -   Batch number = 263
Evaluating:  93%|█████████▎| 263/282 [00:57<00:03,  5.47it/s]11/28/2021 01:08:29 - INFO - __main__ -   Batch number = 264
Evaluating:  94%|█████████▎| 264/282 [00:57<00:03,  5.37it/s]11/28/2021 01:08:29 - INFO - __main__ -   Batch number = 265
Evaluating:  94%|█████████▍| 265/282 [00:58<00:03,  5.28it/s]11/28/2021 01:08:29 - INFO - __main__ -   Batch number = 266
Evaluating:  94%|█████████▍| 266/282 [00:58<00:03,  5.24it/s]11/28/2021 01:08:29 - INFO - __main__ -   Batch number = 267
PyTorch version 1.10.0+cu102 available.
Evaluating:  95%|█████████▍| 267/282 [00:58<00:02,  5.17it/s]11/28/2021 01:08:29 - INFO - __main__ -   Batch number = 268
11/28/2021 01:08:29 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:08:29 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:08:29 - INFO - __main__ -   Seed = 2
11/28/2021 01:08:29 - INFO - root -   save model
11/28/2021 01:08:29 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:08:29 - INFO - __main__ -   Loading pretrained model and tokenizer
Evaluating:  95%|█████████▌| 268/282 [00:58<00:02,  5.13it/s]11/28/2021 01:08:29 - INFO - __main__ -   Batch number = 269
Evaluating:  95%|█████████▌| 269/282 [00:58<00:02,  5.15it/s]11/28/2021 01:08:30 - INFO - __main__ -   Batch number = 270
Evaluating:  96%|█████████▌| 270/282 [00:58<00:02,  5.28it/s]11/28/2021 01:08:30 - INFO - __main__ -   Batch number = 271
Evaluating:  96%|█████████▌| 271/282 [00:59<00:02,  5.32it/s]11/28/2021 01:08:30 - INFO - __main__ -   Batch number = 272
Evaluating:  96%|█████████▋| 272/282 [00:59<00:01,  5.29it/s]11/28/2021 01:08:30 - INFO - __main__ -   Batch number = 273
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  97%|█████████▋| 273/282 [00:59<00:01,  5.35it/s]11/28/2021 01:08:30 - INFO - __main__ -   Batch number = 274
Evaluating:  97%|█████████▋| 274/282 [00:59<00:01,  5.43it/s]11/28/2021 01:08:31 - INFO - __main__ -   Batch number = 275
Evaluating:  98%|█████████▊| 275/282 [00:59<00:01,  5.26it/s]11/28/2021 01:08:31 - INFO - __main__ -   Batch number = 276
Evaluating:  98%|█████████▊| 276/282 [01:00<00:01,  5.36it/s]11/28/2021 01:08:31 - INFO - __main__ -   Batch number = 277
Evaluating:  98%|█████████▊| 277/282 [01:00<00:00,  5.39it/s]11/28/2021 01:08:31 - INFO - __main__ -   Batch number = 278
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  99%|█████████▊| 278/282 [01:00<00:00,  5.39it/s]11/28/2021 01:08:31 - INFO - __main__ -   Batch number = 279
Evaluating:  99%|█████████▉| 279/282 [01:00<00:00,  5.31it/s]11/28/2021 01:08:32 - INFO - __main__ -   Batch number = 280
Evaluating:  99%|█████████▉| 280/282 [01:00<00:00,  5.27it/s]11/28/2021 01:08:32 - INFO - __main__ -   Batch number = 281
Evaluating: 100%|█████████▉| 281/282 [01:01<00:00,  5.28it/s]11/28/2021 01:08:32 - INFO - __main__ -   Batch number = 282
Evaluating: 100%|██████████| 282/282 [01:01<00:00,  4.61it/s]loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:08:32 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052

/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:08:36 - INFO - __main__ -   ***** Evaluation result  in ru *****
11/28/2021 01:08:36 - INFO - __main__ -     f1 = 0.8476251910161196
11/28/2021 01:08:36 - INFO - __main__ -     loss = 0.5511785685381991
11/28/2021 01:08:36 - INFO - __main__ -     precision = 0.8524210278007118
11/28/2021 01:08:36 - INFO - __main__ -     recall = 0.842883016382646
64.04user 21.17system 1:25.67elapsed 99%CPU (0avgtext+0avgdata 3986860maxresident)k
0inputs+1752outputs (0major+1813931minor)pagefaults 0swaps
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:08:38 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:08:38 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:08:38 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
11/28/2021 01:08:38 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:08:38 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:08:38 - INFO - root -   loading lang adpater bh/wiki@ukp
11/28/2021 01:08:38 - INFO - __main__ -   Adapter Languages : ['bh'], Length : 1
11/28/2021 01:08:38 - INFO - __main__ -   Adapter Names ['bh/wiki@ukp'], Length : 1
11/28/2021 01:08:38 - INFO - __main__ -   Language = bh
11/28/2021 01:08:38 - INFO - __main__ -   Adapter Name = bh/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/bh/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_bh_wiki_pfeiffer.zip.
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:08:39 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ru', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:08:39 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:08:39 - INFO - __main__ -   Seed = 3
11/28/2021 01:08:39 - INFO - root -   save model
11/28/2021 01:08:39 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ru', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:08:39 - INFO - __main__ -   Loading pretrained model and tokenizer
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/adapter_config.json
Adding adapter 'bh' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/head_config.json
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:08:42 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:08:48 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:08:48 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:08:48 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
11/28/2021 01:08:48 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:08:48 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:08:48 - INFO - root -   loading lang adpater bh/wiki@ukp
11/28/2021 01:08:48 - INFO - __main__ -   Adapter Languages : ['bh'], Length : 1
11/28/2021 01:08:48 - INFO - __main__ -   Adapter Names ['bh/wiki@ukp'], Length : 1
11/28/2021 01:08:48 - INFO - __main__ -   Language = bh
11/28/2021 01:08:48 - INFO - __main__ -   Adapter Name = bh/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/bh/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_bh_wiki_pfeiffer.zip.
11/28/2021 01:08:49 - INFO - __main__ -   Language mi, split test does not exist
16.41user 6.83system 0:21.86elapsed 106%CPU (0avgtext+0avgdata 3986308maxresident)k
16inputs+40outputs (0major+1385484minor)pagefaults 0swaps
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/adapter_config.json
Adding adapter 'bh' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/head_config.json
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:08:51 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:08:51 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:08:51 - INFO - __main__ -   Seed = 3
11/28/2021 01:08:51 - INFO - root -   save model
11/28/2021 01:08:51 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:08:51 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:08:54 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
11/28/2021 01:08:59 - INFO - __main__ -   Language adapter for ru not found, using bh instead
11/28/2021 01:08:59 - INFO - __main__ -   Set active language adapter to bh
11/28/2021 01:08:59 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:08:59 - INFO - __main__ -   Adapter Languages = ['bh']
11/28/2021 01:08:59 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_ru_bert-base-multilingual-cased_128
11/28/2021 01:09:00 - INFO - __main__ -   ***** Running evaluation  in ru *****
11/28/2021 01:09:00 - INFO - __main__ -     Num examples = 8995
11/28/2021 01:09:00 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/282 [00:00<?, ?it/s]11/28/2021 01:09:00 - INFO - __main__ -   Batch number = 1
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:09:00 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:09:00 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:09:00 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
11/28/2021 01:09:00 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:09:00 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:09:00 - INFO - root -   loading lang adpater bh/wiki@ukp
11/28/2021 01:09:00 - INFO - __main__ -   Adapter Languages : ['bh'], Length : 1
11/28/2021 01:09:00 - INFO - __main__ -   Adapter Names ['bh/wiki@ukp'], Length : 1
11/28/2021 01:09:00 - INFO - __main__ -   Language = bh
11/28/2021 01:09:00 - INFO - __main__ -   Adapter Name = bh/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Evaluating:   0%|          | 1/282 [00:00<01:31,  3.07it/s]11/28/2021 01:09:00 - INFO - __main__ -   Batch number = 2
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/bh/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_bh_wiki_pfeiffer.zip.
Evaluating:   1%|          | 2/282 [00:00<01:31,  3.05it/s]11/28/2021 01:09:01 - INFO - __main__ -   Batch number = 3
Evaluating:   1%|          | 3/282 [00:00<01:31,  3.05it/s]11/28/2021 01:09:01 - INFO - __main__ -   Batch number = 4
Evaluating:   1%|▏         | 4/282 [00:01<01:30,  3.07it/s]11/28/2021 01:09:01 - INFO - __main__ -   Batch number = 5
Evaluating:   2%|▏         | 5/282 [00:01<01:31,  3.03it/s]11/28/2021 01:09:02 - INFO - __main__ -   Batch number = 6
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/adapter_config.json
Adding adapter 'bh' of type 'text_lang'.
Evaluating:   2%|▏         | 6/282 [00:01<01:30,  3.03it/s]11/28/2021 01:09:02 - INFO - __main__ -   Batch number = 7
Loading module weights from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/head_config.json
Evaluating:   2%|▏         | 7/282 [00:02<01:28,  3.10it/s]11/28/2021 01:09:02 - INFO - __main__ -   Batch number = 8
Evaluating:   3%|▎         | 8/282 [00:02<01:28,  3.08it/s]11/28/2021 01:09:03 - INFO - __main__ -   Batch number = 9
Evaluating:   3%|▎         | 9/282 [00:02<01:28,  3.07it/s]11/28/2021 01:09:03 - INFO - __main__ -   Batch number = 10
Evaluating:   4%|▎         | 10/282 [00:03<01:28,  3.06it/s]11/28/2021 01:09:03 - INFO - __main__ -   Batch number = 11
Evaluating:   4%|▍         | 11/282 [00:03<01:28,  3.06it/s]11/28/2021 01:09:04 - INFO - __main__ -   Batch number = 12
Evaluating:   4%|▍         | 12/282 [00:03<01:28,  3.07it/s]11/28/2021 01:09:04 - INFO - __main__ -   Batch number = 13
Evaluating:   5%|▍         | 13/282 [00:04<01:27,  3.06it/s]11/28/2021 01:09:04 - INFO - __main__ -   Batch number = 14
Evaluating:   5%|▍         | 14/282 [00:04<01:27,  3.08it/s]11/28/2021 01:09:05 - INFO - __main__ -   Batch number = 15
Evaluating:   5%|▌         | 15/282 [00:04<01:26,  3.09it/s]11/28/2021 01:09:05 - INFO - __main__ -   Batch number = 16
Evaluating:   6%|▌         | 16/282 [00:05<01:25,  3.10it/s]11/28/2021 01:09:05 - INFO - __main__ -   Batch number = 17
Evaluating:   6%|▌         | 17/282 [00:05<01:23,  3.17it/s]11/28/2021 01:09:06 - INFO - __main__ -   Batch number = 18
Evaluating:   6%|▋         | 18/282 [00:05<01:20,  3.29it/s]11/28/2021 01:09:06 - INFO - __main__ -   Batch number = 19
Evaluating:   7%|▋         | 19/282 [00:05<01:08,  3.83it/s]11/28/2021 01:09:06 - INFO - __main__ -   Batch number = 20
Evaluating:   7%|▋         | 20/282 [00:06<01:00,  4.34it/s]11/28/2021 01:09:06 - INFO - __main__ -   Batch number = 21
Evaluating:   7%|▋         | 21/282 [00:06<00:54,  4.78it/s]11/28/2021 01:09:06 - INFO - __main__ -   Batch number = 22
Evaluating:   8%|▊         | 22/282 [00:06<00:50,  5.13it/s]11/28/2021 01:09:06 - INFO - __main__ -   Batch number = 23
Evaluating:   8%|▊         | 23/282 [00:06<00:48,  5.39it/s]11/28/2021 01:09:07 - INFO - __main__ -   Batch number = 24
Evaluating:   9%|▊         | 24/282 [00:06<00:45,  5.61it/s]11/28/2021 01:09:07 - INFO - __main__ -   Batch number = 25
Evaluating:   9%|▉         | 25/282 [00:06<00:44,  5.78it/s]11/28/2021 01:09:07 - INFO - __main__ -   Batch number = 26
Evaluating:   9%|▉         | 26/282 [00:07<00:43,  5.87it/s]11/28/2021 01:09:07 - INFO - __main__ -   Batch number = 27
Evaluating:  10%|▉         | 27/282 [00:07<00:42,  5.97it/s]11/28/2021 01:09:07 - INFO - __main__ -   Batch number = 28
Evaluating:  10%|▉         | 28/282 [00:07<00:42,  5.99it/s]11/28/2021 01:09:07 - INFO - __main__ -   Batch number = 29
Evaluating:  10%|█         | 29/282 [00:07<00:41,  6.07it/s]11/28/2021 01:09:08 - INFO - __main__ -   Batch number = 30
Evaluating:  11%|█         | 30/282 [00:07<00:41,  6.15it/s]11/28/2021 01:09:08 - INFO - __main__ -   Batch number = 31
Evaluating:  11%|█         | 31/282 [00:07<00:40,  6.18it/s]11/28/2021 01:09:08 - INFO - __main__ -   Batch number = 32
Evaluating:  11%|█▏        | 32/282 [00:08<00:40,  6.24it/s]11/28/2021 01:09:08 - INFO - __main__ -   Batch number = 33
Evaluating:  12%|█▏        | 33/282 [00:08<00:39,  6.24it/s]11/28/2021 01:09:08 - INFO - __main__ -   Batch number = 34
Evaluating:  12%|█▏        | 34/282 [00:08<00:39,  6.28it/s]11/28/2021 01:09:08 - INFO - __main__ -   Batch number = 35
Evaluating:  12%|█▏        | 35/282 [00:08<00:39,  6.30it/s]11/28/2021 01:09:09 - INFO - __main__ -   Batch number = 36
Evaluating:  13%|█▎        | 36/282 [00:08<00:38,  6.34it/s]11/28/2021 01:09:09 - INFO - __main__ -   Batch number = 37
Evaluating:  13%|█▎        | 37/282 [00:08<00:38,  6.33it/s]11/28/2021 01:09:09 - INFO - __main__ -   Batch number = 38
Evaluating:  13%|█▎        | 38/282 [00:08<00:38,  6.35it/s]11/28/2021 01:09:09 - INFO - __main__ -   Batch number = 39
Evaluating:  14%|█▍        | 39/282 [00:09<00:38,  6.29it/s]11/28/2021 01:09:09 - INFO - __main__ -   Batch number = 40
Evaluating:  14%|█▍        | 40/282 [00:09<00:39,  6.19it/s]11/28/2021 01:09:09 - INFO - __main__ -   Batch number = 41
Evaluating:  15%|█▍        | 41/282 [00:09<00:39,  6.11it/s]11/28/2021 01:09:10 - INFO - __main__ -   Batch number = 42
11/28/2021 01:09:10 - INFO - __main__ -   Language mi, split test does not exist
Evaluating:  15%|█▍        | 42/282 [00:09<00:39,  6.14it/s]11/28/2021 01:09:10 - INFO - __main__ -   Batch number = 43
Evaluating:  15%|█▌        | 43/282 [00:09<00:38,  6.21it/s]11/28/2021 01:09:10 - INFO - __main__ -   Batch number = 44
Evaluating:  16%|█▌        | 44/282 [00:09<00:37,  6.28it/s]11/28/2021 01:09:10 - INFO - __main__ -   Batch number = 45
Evaluating:  16%|█▌        | 45/282 [00:10<00:37,  6.31it/s]11/28/2021 01:09:10 - INFO - __main__ -   Batch number = 46
Evaluating:  16%|█▋        | 46/282 [00:10<00:37,  6.34it/s]11/28/2021 01:09:10 - INFO - __main__ -   Batch number = 47
Evaluating:  17%|█▋        | 47/282 [00:10<00:36,  6.36it/s]11/28/2021 01:09:10 - INFO - __main__ -   Batch number = 48
15.63user 6.50system 0:20.98elapsed 105%CPU (0avgtext+0avgdata 3988040maxresident)k
0inputs+40outputs (0major+1362641minor)pagefaults 0swaps
Evaluating:  17%|█▋        | 48/282 [00:10<00:36,  6.36it/s]11/28/2021 01:09:11 - INFO - __main__ -   Batch number = 49
Evaluating:  17%|█▋        | 49/282 [00:10<00:36,  6.34it/s]11/28/2021 01:09:11 - INFO - __main__ -   Batch number = 50
Evaluating:  18%|█▊        | 50/282 [00:10<00:36,  6.35it/s]11/28/2021 01:09:11 - INFO - __main__ -   Batch number = 51
Evaluating:  18%|█▊        | 51/282 [00:11<00:36,  6.38it/s]11/28/2021 01:09:11 - INFO - __main__ -   Batch number = 52
Evaluating:  18%|█▊        | 52/282 [00:11<00:36,  6.37it/s]11/28/2021 01:09:11 - INFO - __main__ -   Batch number = 53
Evaluating:  19%|█▉        | 53/282 [00:11<00:35,  6.39it/s]11/28/2021 01:09:11 - INFO - __main__ -   Batch number = 54
Evaluating:  19%|█▉        | 54/282 [00:11<00:35,  6.40it/s]11/28/2021 01:09:12 - INFO - __main__ -   Batch number = 55
Evaluating:  20%|█▉        | 55/282 [00:11<00:35,  6.39it/s]11/28/2021 01:09:12 - INFO - __main__ -   Batch number = 56
Evaluating:  20%|█▉        | 56/282 [00:11<00:35,  6.41it/s]11/28/2021 01:09:12 - INFO - __main__ -   Batch number = 57
Evaluating:  20%|██        | 57/282 [00:11<00:35,  6.42it/s]11/28/2021 01:09:12 - INFO - __main__ -   Batch number = 58
Evaluating:  21%|██        | 58/282 [00:12<00:35,  6.40it/s]11/28/2021 01:09:12 - INFO - __main__ -   Batch number = 59
Evaluating:  21%|██        | 59/282 [00:12<00:34,  6.40it/s]11/28/2021 01:09:12 - INFO - __main__ -   Batch number = 60
Evaluating:  21%|██▏       | 60/282 [00:12<00:34,  6.42it/s]11/28/2021 01:09:13 - INFO - __main__ -   Batch number = 61
Evaluating:  22%|██▏       | 61/282 [00:12<00:34,  6.43it/s]11/28/2021 01:09:13 - INFO - __main__ -   Batch number = 62
Evaluating:  22%|██▏       | 62/282 [00:12<00:34,  6.34it/s]11/28/2021 01:09:13 - INFO - __main__ -   Batch number = 63
Evaluating:  22%|██▏       | 63/282 [00:12<00:34,  6.27it/s]11/28/2021 01:09:13 - INFO - __main__ -   Batch number = 64
Evaluating:  23%|██▎       | 64/282 [00:13<00:34,  6.30it/s]11/28/2021 01:09:13 - INFO - __main__ -   Batch number = 65
Evaluating:  23%|██▎       | 65/282 [00:13<00:33,  6.44it/s]11/28/2021 01:09:13 - INFO - __main__ -   Batch number = 66
Evaluating:  23%|██▎       | 66/282 [00:13<00:32,  6.55it/s]11/28/2021 01:09:13 - INFO - __main__ -   Batch number = 67
Evaluating:  24%|██▍       | 67/282 [00:13<00:32,  6.62it/s]11/28/2021 01:09:14 - INFO - __main__ -   Batch number = 68
Evaluating:  24%|██▍       | 68/282 [00:13<00:31,  6.69it/s]11/28/2021 01:09:14 - INFO - __main__ -   Batch number = 69
Evaluating:  24%|██▍       | 69/282 [00:13<00:31,  6.74it/s]11/28/2021 01:09:14 - INFO - __main__ -   Batch number = 70
Evaluating:  25%|██▍       | 70/282 [00:13<00:31,  6.74it/s]11/28/2021 01:09:14 - INFO - __main__ -   Batch number = 71
Evaluating:  25%|██▌       | 71/282 [00:14<00:31,  6.76it/s]11/28/2021 01:09:14 - INFO - __main__ -   Batch number = 72
Evaluating:  26%|██▌       | 72/282 [00:14<00:31,  6.77it/s]11/28/2021 01:09:14 - INFO - __main__ -   Batch number = 73
Evaluating:  26%|██▌       | 73/282 [00:14<00:31,  6.73it/s]11/28/2021 01:09:14 - INFO - __main__ -   Batch number = 74
Evaluating:  26%|██▌       | 74/282 [00:14<00:31,  6.63it/s]11/28/2021 01:09:15 - INFO - __main__ -   Batch number = 75
Evaluating:  27%|██▋       | 75/282 [00:14<00:31,  6.60it/s]11/28/2021 01:09:15 - INFO - __main__ -   Batch number = 76
Evaluating:  27%|██▋       | 76/282 [00:14<00:31,  6.58it/s]11/28/2021 01:09:15 - INFO - __main__ -   Batch number = 77
Evaluating:  27%|██▋       | 77/282 [00:15<00:31,  6.46it/s]11/28/2021 01:09:15 - INFO - __main__ -   Batch number = 78
Evaluating:  28%|██▊       | 78/282 [00:15<00:32,  6.30it/s]11/28/2021 01:09:15 - INFO - __main__ -   Batch number = 79
Evaluating:  28%|██▊       | 79/282 [00:15<00:33,  6.09it/s]11/28/2021 01:09:15 - INFO - __main__ -   Batch number = 80
Evaluating:  28%|██▊       | 80/282 [00:15<00:33,  6.11it/s]11/28/2021 01:09:16 - INFO - __main__ -   Batch number = 81
Evaluating:  29%|██▊       | 81/282 [00:15<00:32,  6.18it/s]11/28/2021 01:09:16 - INFO - __main__ -   Batch number = 82
Evaluating:  29%|██▉       | 82/282 [00:15<00:32,  6.23it/s]11/28/2021 01:09:16 - INFO - __main__ -   Batch number = 83
Evaluating:  29%|██▉       | 83/282 [00:16<00:32,  6.18it/s]11/28/2021 01:09:16 - INFO - __main__ -   Batch number = 84
Evaluating:  30%|██▉       | 84/282 [00:16<00:31,  6.24it/s]11/28/2021 01:09:16 - INFO - __main__ -   Batch number = 85
Evaluating:  30%|███       | 85/282 [00:16<00:31,  6.20it/s]11/28/2021 01:09:16 - INFO - __main__ -   Batch number = 86
Evaluating:  30%|███       | 86/282 [00:16<00:31,  6.26it/s]11/28/2021 01:09:17 - INFO - __main__ -   Batch number = 87
Evaluating:  31%|███       | 87/282 [00:16<00:31,  6.15it/s]11/28/2021 01:09:17 - INFO - __main__ -   Batch number = 88
Evaluating:  31%|███       | 88/282 [00:16<00:31,  6.10it/s]11/28/2021 01:09:17 - INFO - __main__ -   Batch number = 89
Evaluating:  32%|███▏      | 89/282 [00:16<00:31,  6.14it/s]11/28/2021 01:09:17 - INFO - __main__ -   Batch number = 90
Evaluating:  32%|███▏      | 90/282 [00:17<00:31,  6.15it/s]11/28/2021 01:09:17 - INFO - __main__ -   Batch number = 91
Evaluating:  32%|███▏      | 91/282 [00:17<00:31,  6.14it/s]11/28/2021 01:09:17 - INFO - __main__ -   Batch number = 92
Evaluating:  33%|███▎      | 92/282 [00:17<00:30,  6.30it/s]11/28/2021 01:09:18 - INFO - __main__ -   Batch number = 93
Evaluating:  33%|███▎      | 93/282 [00:17<00:29,  6.36it/s]11/28/2021 01:09:18 - INFO - __main__ -   Batch number = 94
Evaluating:  33%|███▎      | 94/282 [00:17<00:29,  6.39it/s]11/28/2021 01:09:18 - INFO - __main__ -   Batch number = 95
Evaluating:  34%|███▎      | 95/282 [00:17<00:28,  6.45it/s]11/28/2021 01:09:18 - INFO - __main__ -   Batch number = 96
Evaluating:  34%|███▍      | 96/282 [00:18<00:28,  6.42it/s]11/28/2021 01:09:18 - INFO - __main__ -   Batch number = 97
Evaluating:  34%|███▍      | 97/282 [00:18<00:29,  6.31it/s]11/28/2021 01:09:18 - INFO - __main__ -   Batch number = 98
Evaluating:  35%|███▍      | 98/282 [00:18<00:29,  6.30it/s]11/28/2021 01:09:18 - INFO - __main__ -   Batch number = 99
Evaluating:  35%|███▌      | 99/282 [00:18<00:29,  6.27it/s]11/28/2021 01:09:19 - INFO - __main__ -   Batch number = 100
Evaluating:  35%|███▌      | 100/282 [00:18<00:29,  6.19it/s]11/28/2021 01:09:19 - INFO - __main__ -   Batch number = 101
Evaluating:  36%|███▌      | 101/282 [00:18<00:29,  6.19it/s]11/28/2021 01:09:19 - INFO - __main__ -   Batch number = 102
Evaluating:  36%|███▌      | 102/282 [00:19<00:29,  6.19it/s]11/28/2021 01:09:19 - INFO - __main__ -   Batch number = 103
Evaluating:  37%|███▋      | 103/282 [00:19<00:28,  6.19it/s]11/28/2021 01:09:19 - INFO - __main__ -   Batch number = 104
Evaluating:  37%|███▋      | 104/282 [00:19<00:28,  6.23it/s]11/28/2021 01:09:19 - INFO - __main__ -   Batch number = 105
Evaluating:  37%|███▋      | 105/282 [00:19<00:28,  6.17it/s]11/28/2021 01:09:20 - INFO - __main__ -   Batch number = 106
Evaluating:  38%|███▊      | 106/282 [00:19<00:29,  6.05it/s]11/28/2021 01:09:20 - INFO - __main__ -   Batch number = 107
Evaluating:  38%|███▊      | 107/282 [00:19<00:29,  6.03it/s]11/28/2021 01:09:20 - INFO - __main__ -   Batch number = 108
Evaluating:  38%|███▊      | 108/282 [00:20<00:29,  5.97it/s]11/28/2021 01:09:20 - INFO - __main__ -   Batch number = 109
Evaluating:  39%|███▊      | 109/282 [00:20<00:29,  5.90it/s]11/28/2021 01:09:20 - INFO - __main__ -   Batch number = 110
Evaluating:  39%|███▉      | 110/282 [00:20<00:28,  5.98it/s]11/28/2021 01:09:20 - INFO - __main__ -   Batch number = 111
Evaluating:  39%|███▉      | 111/282 [00:20<00:28,  5.97it/s]11/28/2021 01:09:21 - INFO - __main__ -   Batch number = 112
Evaluating:  40%|███▉      | 112/282 [00:20<00:28,  5.98it/s]11/28/2021 01:09:21 - INFO - __main__ -   Batch number = 113
Evaluating:  40%|████      | 113/282 [00:20<00:27,  6.08it/s]11/28/2021 01:09:21 - INFO - __main__ -   Batch number = 114
Evaluating:  40%|████      | 114/282 [00:21<00:27,  6.11it/s]11/28/2021 01:09:21 - INFO - __main__ -   Batch number = 115
Evaluating:  41%|████      | 115/282 [00:21<00:27,  6.06it/s]11/28/2021 01:09:21 - INFO - __main__ -   Batch number = 116
Evaluating:  41%|████      | 116/282 [00:21<00:26,  6.16it/s]11/28/2021 01:09:21 - INFO - __main__ -   Batch number = 117
Evaluating:  41%|████▏     | 117/282 [00:21<00:26,  6.24it/s]11/28/2021 01:09:22 - INFO - __main__ -   Batch number = 118
Evaluating:  42%|████▏     | 118/282 [00:21<00:26,  6.21it/s]11/28/2021 01:09:22 - INFO - __main__ -   Batch number = 119
Evaluating:  42%|████▏     | 119/282 [00:21<00:25,  6.34it/s]11/28/2021 01:09:22 - INFO - __main__ -   Batch number = 120
Evaluating:  43%|████▎     | 120/282 [00:22<00:32,  4.97it/s]11/28/2021 01:09:22 - INFO - __main__ -   Batch number = 121
Evaluating:  43%|████▎     | 121/282 [00:22<00:38,  4.21it/s]11/28/2021 01:09:23 - INFO - __main__ -   Batch number = 122
Evaluating:  43%|████▎     | 122/282 [00:22<00:42,  3.80it/s]11/28/2021 01:09:23 - INFO - __main__ -   Batch number = 123
Evaluating:  44%|████▎     | 123/282 [00:23<00:44,  3.56it/s]11/28/2021 01:09:23 - INFO - __main__ -   Batch number = 124
Evaluating:  44%|████▍     | 124/282 [00:23<00:47,  3.33it/s]11/28/2021 01:09:24 - INFO - __main__ -   Batch number = 125
Evaluating:  44%|████▍     | 125/282 [00:23<00:48,  3.22it/s]11/28/2021 01:09:24 - INFO - __main__ -   Batch number = 126
Evaluating:  45%|████▍     | 126/282 [00:24<00:49,  3.14it/s]11/28/2021 01:09:24 - INFO - __main__ -   Batch number = 127
Evaluating:  45%|████▌     | 127/282 [00:24<00:50,  3.04it/s]11/28/2021 01:09:25 - INFO - __main__ -   Batch number = 128
Evaluating:  45%|████▌     | 128/282 [00:24<00:51,  3.01it/s]11/28/2021 01:09:25 - INFO - __main__ -   Batch number = 129
Evaluating:  46%|████▌     | 129/282 [00:25<00:51,  2.97it/s]11/28/2021 01:09:25 - INFO - __main__ -   Batch number = 130
Evaluating:  46%|████▌     | 130/282 [00:25<00:51,  2.93it/s]11/28/2021 01:09:26 - INFO - __main__ -   Batch number = 131
Evaluating:  46%|████▋     | 131/282 [00:25<00:51,  2.93it/s]11/28/2021 01:09:26 - INFO - __main__ -   Batch number = 132
Evaluating:  47%|████▋     | 132/282 [00:26<00:51,  2.92it/s]11/28/2021 01:09:26 - INFO - __main__ -   Batch number = 133
Evaluating:  47%|████▋     | 133/282 [00:26<00:51,  2.91it/s]11/28/2021 01:09:27 - INFO - __main__ -   Batch number = 134
Evaluating:  48%|████▊     | 134/282 [00:26<00:50,  2.92it/s]11/28/2021 01:09:27 - INFO - __main__ -   Batch number = 135
Evaluating:  48%|████▊     | 135/282 [00:27<00:50,  2.92it/s]11/28/2021 01:09:27 - INFO - __main__ -   Batch number = 136
Evaluating:  48%|████▊     | 136/282 [00:27<00:50,  2.91it/s]11/28/2021 01:09:28 - INFO - __main__ -   Batch number = 137
Evaluating:  49%|████▊     | 137/282 [00:27<00:49,  2.92it/s]11/28/2021 01:09:28 - INFO - __main__ -   Batch number = 138
Evaluating:  49%|████▉     | 138/282 [00:28<00:49,  2.92it/s]11/28/2021 01:09:28 - INFO - __main__ -   Batch number = 139
Evaluating:  49%|████▉     | 139/282 [00:28<00:48,  2.92it/s]11/28/2021 01:09:29 - INFO - __main__ -   Batch number = 140
Evaluating:  50%|████▉     | 140/282 [00:28<00:48,  2.93it/s]11/28/2021 01:09:29 - INFO - __main__ -   Batch number = 141
Evaluating:  50%|█████     | 141/282 [00:29<00:48,  2.94it/s]11/28/2021 01:09:29 - INFO - __main__ -   Batch number = 142
Evaluating:  50%|█████     | 142/282 [00:29<00:47,  2.93it/s]11/28/2021 01:09:30 - INFO - __main__ -   Batch number = 143
Evaluating:  51%|█████     | 143/282 [00:29<00:47,  2.94it/s]11/28/2021 01:09:30 - INFO - __main__ -   Batch number = 144
Evaluating:  51%|█████     | 144/282 [00:30<00:47,  2.93it/s]11/28/2021 01:09:30 - INFO - __main__ -   Batch number = 145
Evaluating:  51%|█████▏    | 145/282 [00:30<00:47,  2.90it/s]11/28/2021 01:09:31 - INFO - __main__ -   Batch number = 146
Evaluating:  52%|█████▏    | 146/282 [00:31<00:46,  2.92it/s]11/28/2021 01:09:31 - INFO - __main__ -   Batch number = 147
Evaluating:  52%|█████▏    | 147/282 [00:31<00:47,  2.86it/s]11/28/2021 01:09:31 - INFO - __main__ -   Batch number = 148
Evaluating:  52%|█████▏    | 148/282 [00:31<00:52,  2.53it/s]11/28/2021 01:09:32 - INFO - __main__ -   Batch number = 149
Evaluating:  53%|█████▎    | 149/282 [00:32<00:54,  2.46it/s]11/28/2021 01:09:32 - INFO - __main__ -   Batch number = 150
Evaluating:  53%|█████▎    | 150/282 [00:32<00:58,  2.27it/s]11/28/2021 01:09:33 - INFO - __main__ -   Batch number = 151
Evaluating:  54%|█████▎    | 151/282 [00:33<01:00,  2.16it/s]11/28/2021 01:09:33 - INFO - __main__ -   Batch number = 152
Evaluating:  54%|█████▍    | 152/282 [00:33<01:01,  2.10it/s]11/28/2021 01:09:34 - INFO - __main__ -   Batch number = 153
Evaluating:  54%|█████▍    | 153/282 [00:34<01:02,  2.06it/s]11/28/2021 01:09:34 - INFO - __main__ -   Batch number = 154
Evaluating:  55%|█████▍    | 154/282 [00:34<01:02,  2.04it/s]11/28/2021 01:09:35 - INFO - __main__ -   Batch number = 155
Evaluating:  55%|█████▍    | 155/282 [00:35<01:02,  2.02it/s]11/28/2021 01:09:35 - INFO - __main__ -   Batch number = 156
Evaluating:  55%|█████▌    | 156/282 [00:35<01:03,  2.00it/s]11/28/2021 01:09:36 - INFO - __main__ -   Batch number = 157
Evaluating:  56%|█████▌    | 157/282 [00:36<01:02,  1.99it/s]11/28/2021 01:09:36 - INFO - __main__ -   Batch number = 158
Evaluating:  56%|█████▌    | 158/282 [00:36<01:02,  1.99it/s]11/28/2021 01:09:37 - INFO - __main__ -   Batch number = 159
Evaluating:  56%|█████▋    | 159/282 [00:37<01:00,  2.04it/s]11/28/2021 01:09:37 - INFO - __main__ -   Batch number = 160
Evaluating:  57%|█████▋    | 160/282 [00:37<01:00,  2.03it/s]11/28/2021 01:09:38 - INFO - __main__ -   Batch number = 161
Evaluating:  57%|█████▋    | 161/282 [00:38<01:00,  2.00it/s]11/28/2021 01:09:38 - INFO - __main__ -   Batch number = 162
Evaluating:  57%|█████▋    | 162/282 [00:38<01:00,  1.99it/s]11/28/2021 01:09:39 - INFO - __main__ -   Batch number = 163
Evaluating:  58%|█████▊    | 163/282 [00:39<01:00,  1.98it/s]11/28/2021 01:09:39 - INFO - __main__ -   Batch number = 164
Evaluating:  58%|█████▊    | 164/282 [00:39<00:59,  1.98it/s]11/28/2021 01:09:40 - INFO - __main__ -   Batch number = 165
Evaluating:  59%|█████▊    | 165/282 [00:40<00:59,  1.96it/s]11/28/2021 01:09:40 - INFO - __main__ -   Batch number = 166
Evaluating:  59%|█████▉    | 166/282 [00:40<00:58,  1.97it/s]11/28/2021 01:09:41 - INFO - __main__ -   Batch number = 167
Evaluating:  59%|█████▉    | 167/282 [00:41<00:58,  1.95it/s]11/28/2021 01:09:41 - INFO - __main__ -   Batch number = 168
Evaluating:  60%|█████▉    | 168/282 [00:41<00:58,  1.96it/s]11/28/2021 01:09:42 - INFO - __main__ -   Batch number = 169
Evaluating:  60%|█████▉    | 169/282 [00:42<00:57,  1.95it/s]11/28/2021 01:09:43 - INFO - __main__ -   Batch number = 170
Evaluating:  60%|██████    | 170/282 [00:42<00:57,  1.96it/s]11/28/2021 01:09:43 - INFO - __main__ -   Batch number = 171
Evaluating:  61%|██████    | 171/282 [00:43<00:56,  1.96it/s]11/28/2021 01:09:44 - INFO - __main__ -   Batch number = 172
Evaluating:  61%|██████    | 172/282 [00:43<00:55,  1.97it/s]11/28/2021 01:09:44 - INFO - __main__ -   Batch number = 173
Evaluating:  61%|██████▏   | 173/282 [00:44<00:55,  1.97it/s]11/28/2021 01:09:45 - INFO - __main__ -   Batch number = 174
Evaluating:  62%|██████▏   | 174/282 [00:44<00:54,  1.98it/s]11/28/2021 01:09:45 - INFO - __main__ -   Batch number = 175
Evaluating:  62%|██████▏   | 175/282 [00:45<00:54,  1.98it/s]11/28/2021 01:09:46 - INFO - __main__ -   Batch number = 176
Evaluating:  62%|██████▏   | 176/282 [00:45<00:53,  1.98it/s]11/28/2021 01:09:46 - INFO - __main__ -   Batch number = 177
Evaluating:  63%|██████▎   | 177/282 [00:46<00:53,  1.97it/s]11/28/2021 01:09:47 - INFO - __main__ -   Batch number = 178
Evaluating:  63%|██████▎   | 178/282 [00:47<00:52,  1.97it/s]11/28/2021 01:09:47 - INFO - __main__ -   Batch number = 179
Evaluating:  63%|██████▎   | 179/282 [00:47<00:51,  2.01it/s]11/28/2021 01:09:48 - INFO - __main__ -   Batch number = 180
Evaluating:  64%|██████▍   | 180/282 [00:47<00:51,  2.00it/s]11/28/2021 01:09:48 - INFO - __main__ -   Batch number = 181
Evaluating:  64%|██████▍   | 181/282 [00:48<00:56,  1.78it/s]11/28/2021 01:09:49 - INFO - __main__ -   Batch number = 182
Evaluating:  65%|██████▍   | 182/282 [00:49<00:54,  1.83it/s]11/28/2021 01:09:49 - INFO - __main__ -   Batch number = 183
Evaluating:  65%|██████▍   | 183/282 [00:49<00:53,  1.86it/s]11/28/2021 01:09:50 - INFO - __main__ -   Batch number = 184
Evaluating:  65%|██████▌   | 184/282 [00:50<00:51,  1.90it/s]11/28/2021 01:09:50 - INFO - __main__ -   Batch number = 185
Evaluating:  66%|██████▌   | 185/282 [00:50<00:50,  1.91it/s]11/28/2021 01:09:51 - INFO - __main__ -   Batch number = 186
Evaluating:  66%|██████▌   | 186/282 [00:51<00:49,  1.94it/s]11/28/2021 01:09:51 - INFO - __main__ -   Batch number = 187
Evaluating:  66%|██████▋   | 187/282 [00:51<00:49,  1.94it/s]11/28/2021 01:09:52 - INFO - __main__ -   Batch number = 188
Evaluating:  67%|██████▋   | 188/282 [00:52<00:48,  1.95it/s]11/28/2021 01:09:53 - INFO - __main__ -   Batch number = 189
Evaluating:  67%|██████▋   | 189/282 [00:52<00:52,  1.77it/s]11/28/2021 01:09:53 - INFO - __main__ -   Batch number = 190
Evaluating:  67%|██████▋   | 190/282 [00:53<00:50,  1.83it/s]11/28/2021 01:09:54 - INFO - __main__ -   Batch number = 191
Evaluating:  68%|██████▊   | 191/282 [00:53<00:48,  1.86it/s]11/28/2021 01:09:54 - INFO - __main__ -   Batch number = 192
Evaluating:  68%|██████▊   | 192/282 [00:54<00:47,  1.90it/s]11/28/2021 01:09:55 - INFO - __main__ -   Batch number = 193
Evaluating:  68%|██████▊   | 193/282 [00:54<00:46,  1.91it/s]11/28/2021 01:09:55 - INFO - __main__ -   Batch number = 194
Evaluating:  69%|██████▉   | 194/282 [00:55<00:45,  1.93it/s]11/28/2021 01:09:56 - INFO - __main__ -   Batch number = 195
Evaluating:  69%|██████▉   | 195/282 [00:56<00:44,  1.93it/s]11/28/2021 01:09:56 - INFO - __main__ -   Batch number = 196
Evaluating:  70%|██████▉   | 196/282 [00:56<00:44,  1.95it/s]11/28/2021 01:09:57 - INFO - __main__ -   Batch number = 197
Evaluating:  70%|██████▉   | 197/282 [00:57<00:43,  1.96it/s]11/28/2021 01:09:57 - INFO - __main__ -   Batch number = 198
Evaluating:  70%|███████   | 198/282 [00:57<00:42,  1.97it/s]11/28/2021 01:09:58 - INFO - __main__ -   Batch number = 199
Evaluating:  71%|███████   | 199/282 [00:57<00:40,  2.03it/s]11/28/2021 01:09:58 - INFO - __main__ -   Batch number = 200
Evaluating:  71%|███████   | 200/282 [00:58<00:36,  2.25it/s]11/28/2021 01:09:58 - INFO - __main__ -   Batch number = 201
Evaluating:  71%|███████▏  | 201/282 [00:58<00:33,  2.39it/s]11/28/2021 01:09:59 - INFO - __main__ -   Batch number = 202
Evaluating:  72%|███████▏  | 202/282 [00:59<00:31,  2.54it/s]11/28/2021 01:09:59 - INFO - __main__ -   Batch number = 203
Evaluating:  72%|███████▏  | 203/282 [00:59<00:30,  2.63it/s]11/28/2021 01:09:59 - INFO - __main__ -   Batch number = 204
Evaluating:  72%|███████▏  | 204/282 [00:59<00:28,  2.73it/s]11/28/2021 01:10:00 - INFO - __main__ -   Batch number = 205
Evaluating:  73%|███████▎  | 205/282 [01:00<00:27,  2.79it/s]11/28/2021 01:10:00 - INFO - __main__ -   Batch number = 206
Evaluating:  73%|███████▎  | 206/282 [01:00<00:26,  2.84it/s]11/28/2021 01:10:00 - INFO - __main__ -   Batch number = 207
Evaluating:  73%|███████▎  | 207/282 [01:00<00:25,  2.91it/s]11/28/2021 01:10:01 - INFO - __main__ -   Batch number = 208
Evaluating:  74%|███████▍  | 208/282 [01:01<00:24,  2.96it/s]11/28/2021 01:10:01 - INFO - __main__ -   Batch number = 209
Evaluating:  74%|███████▍  | 209/282 [01:01<00:24,  2.94it/s]11/28/2021 01:10:01 - INFO - __main__ -   Batch number = 210
Evaluating:  74%|███████▍  | 210/282 [01:01<00:24,  2.95it/s]11/28/2021 01:10:02 - INFO - __main__ -   Batch number = 211
Evaluating:  75%|███████▍  | 211/282 [01:02<00:24,  2.93it/s]11/28/2021 01:10:02 - INFO - __main__ -   Batch number = 212
Evaluating:  75%|███████▌  | 212/282 [01:02<00:23,  2.95it/s]11/28/2021 01:10:02 - INFO - __main__ -   Batch number = 213
Evaluating:  76%|███████▌  | 213/282 [01:02<00:23,  2.92it/s]11/28/2021 01:10:03 - INFO - __main__ -   Batch number = 214
Evaluating:  76%|███████▌  | 214/282 [01:03<00:23,  2.93it/s]11/28/2021 01:10:03 - INFO - __main__ -   Batch number = 215
Evaluating:  76%|███████▌  | 215/282 [01:03<00:23,  2.90it/s]11/28/2021 01:10:03 - INFO - __main__ -   Batch number = 216
Evaluating:  77%|███████▋  | 216/282 [01:03<00:22,  2.92it/s]11/28/2021 01:10:04 - INFO - __main__ -   Batch number = 217
Evaluating:  77%|███████▋  | 217/282 [01:04<00:22,  2.91it/s]11/28/2021 01:10:04 - INFO - __main__ -   Batch number = 218
Evaluating:  77%|███████▋  | 218/282 [01:04<00:21,  2.94it/s]11/28/2021 01:10:04 - INFO - __main__ -   Batch number = 219
Evaluating:  78%|███████▊  | 219/282 [01:04<00:21,  2.92it/s]11/28/2021 01:10:05 - INFO - __main__ -   Batch number = 220
Evaluating:  78%|███████▊  | 220/282 [01:05<00:21,  2.93it/s]11/28/2021 01:10:05 - INFO - __main__ -   Batch number = 221
Evaluating:  78%|███████▊  | 221/282 [01:05<00:20,  2.91it/s]11/28/2021 01:10:06 - INFO - __main__ -   Batch number = 222
Evaluating:  79%|███████▊  | 222/282 [01:05<00:20,  2.93it/s]11/28/2021 01:10:06 - INFO - __main__ -   Batch number = 223
Evaluating:  79%|███████▉  | 223/282 [01:06<00:20,  2.90it/s]11/28/2021 01:10:06 - INFO - __main__ -   Batch number = 224
Evaluating:  79%|███████▉  | 224/282 [01:06<00:19,  2.98it/s]11/28/2021 01:10:07 - INFO - __main__ -   Batch number = 225
Evaluating:  80%|███████▉  | 225/282 [01:06<00:19,  2.94it/s]11/28/2021 01:10:07 - INFO - __main__ -   Batch number = 226
Evaluating:  80%|████████  | 226/282 [01:07<00:19,  2.94it/s]11/28/2021 01:10:07 - INFO - __main__ -   Batch number = 227
Evaluating:  80%|████████  | 227/282 [01:07<00:18,  2.93it/s]11/28/2021 01:10:08 - INFO - __main__ -   Batch number = 228
Evaluating:  81%|████████  | 228/282 [01:07<00:19,  2.82it/s]11/28/2021 01:10:08 - INFO - __main__ -   Batch number = 229
Evaluating:  81%|████████  | 229/282 [01:08<00:18,  2.88it/s]11/28/2021 01:10:08 - INFO - __main__ -   Batch number = 230
Evaluating:  82%|████████▏ | 230/282 [01:08<00:17,  2.99it/s]11/28/2021 01:10:09 - INFO - __main__ -   Batch number = 231
Evaluating:  82%|████████▏ | 231/282 [01:08<00:16,  3.00it/s]11/28/2021 01:10:09 - INFO - __main__ -   Batch number = 232
Evaluating:  82%|████████▏ | 232/282 [01:09<00:16,  3.04it/s]11/28/2021 01:10:09 - INFO - __main__ -   Batch number = 233
Evaluating:  83%|████████▎ | 233/282 [01:09<00:16,  3.01it/s]11/28/2021 01:10:10 - INFO - __main__ -   Batch number = 234
Evaluating:  83%|████████▎ | 234/282 [01:09<00:15,  3.04it/s]11/28/2021 01:10:10 - INFO - __main__ -   Batch number = 235
Evaluating:  83%|████████▎ | 235/282 [01:10<00:15,  3.11it/s]11/28/2021 01:10:10 - INFO - __main__ -   Batch number = 236
Evaluating:  84%|████████▎ | 236/282 [01:10<00:12,  3.63it/s]11/28/2021 01:10:10 - INFO - __main__ -   Batch number = 237
Evaluating:  84%|████████▍ | 237/282 [01:10<00:11,  4.05it/s]11/28/2021 01:10:11 - INFO - __main__ -   Batch number = 238
Evaluating:  84%|████████▍ | 238/282 [01:10<00:09,  4.52it/s]11/28/2021 01:10:11 - INFO - __main__ -   Batch number = 239
Evaluating:  85%|████████▍ | 239/282 [01:10<00:08,  4.82it/s]11/28/2021 01:10:11 - INFO - __main__ -   Batch number = 240
Evaluating:  85%|████████▌ | 240/282 [01:10<00:08,  5.19it/s]11/28/2021 01:10:11 - INFO - __main__ -   Batch number = 241
Evaluating:  85%|████████▌ | 241/282 [01:11<00:07,  5.32it/s]11/28/2021 01:10:11 - INFO - __main__ -   Batch number = 242
Evaluating:  86%|████████▌ | 242/282 [01:11<00:07,  5.58it/s]11/28/2021 01:10:11 - INFO - __main__ -   Batch number = 243
Evaluating:  86%|████████▌ | 243/282 [01:11<00:06,  5.67it/s]11/28/2021 01:10:12 - INFO - __main__ -   Batch number = 244
Evaluating:  87%|████████▋ | 244/282 [01:11<00:06,  5.85it/s]11/28/2021 01:10:12 - INFO - __main__ -   Batch number = 245
Evaluating:  87%|████████▋ | 245/282 [01:11<00:06,  5.78it/s]11/28/2021 01:10:12 - INFO - __main__ -   Batch number = 246
Evaluating:  87%|████████▋ | 246/282 [01:11<00:06,  5.80it/s]11/28/2021 01:10:12 - INFO - __main__ -   Batch number = 247
Evaluating:  88%|████████▊ | 247/282 [01:12<00:06,  5.67it/s]11/28/2021 01:10:12 - INFO - __main__ -   Batch number = 248
Evaluating:  88%|████████▊ | 248/282 [01:12<00:05,  5.76it/s]11/28/2021 01:10:12 - INFO - __main__ -   Batch number = 249
Evaluating:  88%|████████▊ | 249/282 [01:12<00:05,  5.71it/s]11/28/2021 01:10:13 - INFO - __main__ -   Batch number = 250
Evaluating:  89%|████████▊ | 250/282 [01:12<00:05,  5.82it/s]11/28/2021 01:10:13 - INFO - __main__ -   Batch number = 251
Evaluating:  89%|████████▉ | 251/282 [01:12<00:05,  5.78it/s]11/28/2021 01:10:13 - INFO - __main__ -   Batch number = 252
Evaluating:  89%|████████▉ | 252/282 [01:13<00:05,  5.86it/s]11/28/2021 01:10:13 - INFO - __main__ -   Batch number = 253
Evaluating:  90%|████████▉ | 253/282 [01:13<00:05,  5.80it/s]11/28/2021 01:10:13 - INFO - __main__ -   Batch number = 254
Evaluating:  90%|█████████ | 254/282 [01:13<00:04,  5.92it/s]11/28/2021 01:10:13 - INFO - __main__ -   Batch number = 255
Evaluating:  90%|█████████ | 255/282 [01:13<00:04,  5.86it/s]11/28/2021 01:10:14 - INFO - __main__ -   Batch number = 256
Evaluating:  91%|█████████ | 256/282 [01:13<00:04,  5.87it/s]11/28/2021 01:10:14 - INFO - __main__ -   Batch number = 257
Evaluating:  91%|█████████ | 257/282 [01:13<00:04,  5.77it/s]11/28/2021 01:10:14 - INFO - __main__ -   Batch number = 258
Evaluating:  91%|█████████▏| 258/282 [01:14<00:04,  5.82it/s]11/28/2021 01:10:14 - INFO - __main__ -   Batch number = 259
Evaluating:  92%|█████████▏| 259/282 [01:14<00:04,  5.68it/s]11/28/2021 01:10:14 - INFO - __main__ -   Batch number = 260
Evaluating:  92%|█████████▏| 260/282 [01:14<00:05,  3.97it/s]11/28/2021 01:10:15 - INFO - __main__ -   Batch number = 261
Evaluating:  93%|█████████▎| 261/282 [01:14<00:04,  4.23it/s]11/28/2021 01:10:15 - INFO - __main__ -   Batch number = 262
Evaluating:  93%|█████████▎| 262/282 [01:15<00:04,  4.53it/s]11/28/2021 01:10:15 - INFO - __main__ -   Batch number = 263
Evaluating:  93%|█████████▎| 263/282 [01:15<00:03,  4.76it/s]11/28/2021 01:10:15 - INFO - __main__ -   Batch number = 264
Evaluating:  94%|█████████▎| 264/282 [01:15<00:03,  4.96it/s]11/28/2021 01:10:15 - INFO - __main__ -   Batch number = 265
Evaluating:  94%|█████████▍| 265/282 [01:15<00:03,  4.65it/s]11/28/2021 01:10:16 - INFO - __main__ -   Batch number = 266
Evaluating:  94%|█████████▍| 266/282 [01:15<00:03,  4.91it/s]11/28/2021 01:10:16 - INFO - __main__ -   Batch number = 267
Evaluating:  95%|█████████▍| 267/282 [01:16<00:02,  5.11it/s]11/28/2021 01:10:16 - INFO - __main__ -   Batch number = 268
Evaluating:  95%|█████████▌| 268/282 [01:16<00:02,  5.21it/s]11/28/2021 01:10:16 - INFO - __main__ -   Batch number = 269
Evaluating:  95%|█████████▌| 269/282 [01:16<00:02,  5.36it/s]11/28/2021 01:10:16 - INFO - __main__ -   Batch number = 270
Evaluating:  96%|█████████▌| 270/282 [01:16<00:02,  5.41it/s]11/28/2021 01:10:17 - INFO - __main__ -   Batch number = 271
Evaluating:  96%|█████████▌| 271/282 [01:16<00:02,  4.45it/s]11/28/2021 01:10:17 - INFO - __main__ -   Batch number = 272
Evaluating:  96%|█████████▋| 272/282 [01:17<00:02,  4.66it/s]11/28/2021 01:10:17 - INFO - __main__ -   Batch number = 273
Evaluating:  97%|█████████▋| 273/282 [01:17<00:01,  4.86it/s]11/28/2021 01:10:17 - INFO - __main__ -   Batch number = 274
Evaluating:  97%|█████████▋| 274/282 [01:17<00:01,  5.06it/s]11/28/2021 01:10:18 - INFO - __main__ -   Batch number = 275
Evaluating:  98%|█████████▊| 275/282 [01:17<00:01,  5.18it/s]11/28/2021 01:10:18 - INFO - __main__ -   Batch number = 276
Evaluating:  98%|█████████▊| 276/282 [01:17<00:01,  5.24it/s]11/28/2021 01:10:18 - INFO - __main__ -   Batch number = 277
Evaluating:  98%|█████████▊| 277/282 [01:17<00:00,  5.32it/s]11/28/2021 01:10:18 - INFO - __main__ -   Batch number = 278
Evaluating:  99%|█████████▊| 278/282 [01:18<00:00,  5.35it/s]11/28/2021 01:10:18 - INFO - __main__ -   Batch number = 279
Evaluating:  99%|█████████▉| 279/282 [01:18<00:00,  5.42it/s]11/28/2021 01:10:18 - INFO - __main__ -   Batch number = 280
Evaluating:  99%|█████████▉| 280/282 [01:18<00:00,  5.47it/s]11/28/2021 01:10:19 - INFO - __main__ -   Batch number = 281
Evaluating: 100%|█████████▉| 281/282 [01:18<00:00,  3.87it/s]11/28/2021 01:10:19 - INFO - __main__ -   Batch number = 282
Evaluating: 100%|██████████| 282/282 [01:19<00:00,  3.57it/s]
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:10:23 - INFO - __main__ -   ***** Evaluation result  in ru *****
11/28/2021 01:10:23 - INFO - __main__ -     f1 = 0.8514302908309901
11/28/2021 01:10:23 - INFO - __main__ -     loss = 0.5072712680549486
11/28/2021 01:10:23 - INFO - __main__ -     precision = 0.8585298487162855
11/28/2021 01:10:23 - INFO - __main__ -     recall = 0.8444471886062402
79.46user 27.11system 1:46.56elapsed 100%CPU (0avgtext+0avgdata 3985592maxresident)k
0inputs+1800outputs (0major+1742766minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:12:02 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='eu', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:12:02 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:12:02 - INFO - __main__ -   Seed = 1
11/28/2021 01:12:02 - INFO - root -   save model
11/28/2021 01:12:02 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='eu', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:12:02 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:12:04 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:12:10 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:12:10 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:12:10 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
11/28/2021 01:12:10 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:12:10 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:12:11 - INFO - root -   loading lang adpater bh/wiki@ukp
11/28/2021 01:12:11 - INFO - __main__ -   Adapter Languages : ['bh'], Length : 1
11/28/2021 01:12:11 - INFO - __main__ -   Adapter Names ['bh/wiki@ukp'], Length : 1
11/28/2021 01:12:11 - INFO - __main__ -   Language = bh
11/28/2021 01:12:11 - INFO - __main__ -   Adapter Name = bh/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/bh/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_bh_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/adapter_config.json
Adding adapter 'bh' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/head_config.json
11/28/2021 01:12:18 - INFO - __main__ -   Language adapter for eu not found, using bh instead
11/28/2021 01:12:18 - INFO - __main__ -   Set active language adapter to bh
11/28/2021 01:12:18 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:12:18 - INFO - __main__ -   Adapter Languages = ['bh']
11/28/2021 01:12:18 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_eu_bert-base-multilingual-cased_128
11/28/2021 01:12:18 - INFO - __main__ -   ***** Running evaluation  in eu *****
11/28/2021 01:12:18 - INFO - __main__ -     Num examples = 1799
11/28/2021 01:12:18 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/57 [00:00<?, ?it/s]11/28/2021 01:12:18 - INFO - __main__ -   Batch number = 1
Evaluating:   2%|▏         | 1/57 [00:00<00:08,  7.00it/s]11/28/2021 01:12:18 - INFO - __main__ -   Batch number = 2
Evaluating:   4%|▎         | 2/57 [00:00<00:07,  7.08it/s]11/28/2021 01:12:18 - INFO - __main__ -   Batch number = 3
Evaluating:   5%|▌         | 3/57 [00:00<00:07,  7.09it/s]11/28/2021 01:12:18 - INFO - __main__ -   Batch number = 4
Evaluating:   7%|▋         | 4/57 [00:00<00:07,  7.11it/s]11/28/2021 01:12:19 - INFO - __main__ -   Batch number = 5
Evaluating:   9%|▉         | 5/57 [00:00<00:07,  7.09it/s]11/28/2021 01:12:19 - INFO - __main__ -   Batch number = 6
Evaluating:  11%|█         | 6/57 [00:00<00:08,  6.03it/s]11/28/2021 01:12:19 - INFO - __main__ -   Batch number = 7
Evaluating:  12%|█▏        | 7/57 [00:01<00:07,  6.34it/s]11/28/2021 01:12:19 - INFO - __main__ -   Batch number = 8
Evaluating:  14%|█▍        | 8/57 [00:01<00:07,  6.57it/s]11/28/2021 01:12:19 - INFO - __main__ -   Batch number = 9
Evaluating:  16%|█▌        | 9/57 [00:01<00:07,  6.71it/s]11/28/2021 01:12:19 - INFO - __main__ -   Batch number = 10
Evaluating:  18%|█▊        | 10/57 [00:01<00:06,  6.78it/s]11/28/2021 01:12:19 - INFO - __main__ -   Batch number = 11
Evaluating:  19%|█▉        | 11/57 [00:01<00:06,  6.85it/s]11/28/2021 01:12:20 - INFO - __main__ -   Batch number = 12
Evaluating:  21%|██        | 12/57 [00:01<00:06,  6.92it/s]11/28/2021 01:12:20 - INFO - __main__ -   Batch number = 13
Evaluating:  23%|██▎       | 13/57 [00:01<00:06,  6.96it/s]11/28/2021 01:12:20 - INFO - __main__ -   Batch number = 14
Evaluating:  25%|██▍       | 14/57 [00:02<00:06,  6.98it/s]11/28/2021 01:12:20 - INFO - __main__ -   Batch number = 15
Evaluating:  26%|██▋       | 15/57 [00:02<00:05,  7.01it/s]11/28/2021 01:12:20 - INFO - __main__ -   Batch number = 16
Evaluating:  28%|██▊       | 16/57 [00:02<00:05,  7.03it/s]11/28/2021 01:12:20 - INFO - __main__ -   Batch number = 17
Evaluating:  30%|██▉       | 17/57 [00:02<00:05,  7.00it/s]11/28/2021 01:12:20 - INFO - __main__ -   Batch number = 18
Evaluating:  32%|███▏      | 18/57 [00:02<00:05,  6.98it/s]11/28/2021 01:12:21 - INFO - __main__ -   Batch number = 19
Evaluating:  33%|███▎      | 19/57 [00:02<00:05,  7.00it/s]11/28/2021 01:12:21 - INFO - __main__ -   Batch number = 20
Evaluating:  35%|███▌      | 20/57 [00:02<00:05,  7.02it/s]11/28/2021 01:12:21 - INFO - __main__ -   Batch number = 21
Evaluating:  37%|███▋      | 21/57 [00:03<00:05,  7.01it/s]11/28/2021 01:12:21 - INFO - __main__ -   Batch number = 22
Evaluating:  39%|███▊      | 22/57 [00:03<00:04,  7.03it/s]11/28/2021 01:12:21 - INFO - __main__ -   Batch number = 23
Evaluating:  40%|████      | 23/57 [00:03<00:04,  7.03it/s]11/28/2021 01:12:21 - INFO - __main__ -   Batch number = 24
Evaluating:  42%|████▏     | 24/57 [00:03<00:04,  7.03it/s]11/28/2021 01:12:21 - INFO - __main__ -   Batch number = 25
Evaluating:  44%|████▍     | 25/57 [00:03<00:04,  7.03it/s]11/28/2021 01:12:22 - INFO - __main__ -   Batch number = 26
Evaluating:  46%|████▌     | 26/57 [00:03<00:04,  7.03it/s]11/28/2021 01:12:22 - INFO - __main__ -   Batch number = 27
Evaluating:  47%|████▋     | 27/57 [00:03<00:04,  7.00it/s]11/28/2021 01:12:22 - INFO - __main__ -   Batch number = 28
Evaluating:  49%|████▉     | 28/57 [00:04<00:04,  6.99it/s]11/28/2021 01:12:22 - INFO - __main__ -   Batch number = 29
Evaluating:  51%|█████     | 29/57 [00:04<00:03,  7.01it/s]11/28/2021 01:12:22 - INFO - __main__ -   Batch number = 30
Evaluating:  53%|█████▎    | 30/57 [00:04<00:03,  7.00it/s]11/28/2021 01:12:22 - INFO - __main__ -   Batch number = 31
Evaluating:  54%|█████▍    | 31/57 [00:04<00:03,  7.00it/s]11/28/2021 01:12:22 - INFO - __main__ -   Batch number = 32
Evaluating:  56%|█████▌    | 32/57 [00:04<00:03,  7.00it/s]11/28/2021 01:12:23 - INFO - __main__ -   Batch number = 33
Evaluating:  58%|█████▊    | 33/57 [00:04<00:03,  6.98it/s]11/28/2021 01:12:23 - INFO - __main__ -   Batch number = 34
Evaluating:  60%|█████▉    | 34/57 [00:04<00:03,  6.99it/s]11/28/2021 01:12:23 - INFO - __main__ -   Batch number = 35
Evaluating:  61%|██████▏   | 35/57 [00:05<00:03,  7.00it/s]11/28/2021 01:12:23 - INFO - __main__ -   Batch number = 36
Evaluating:  63%|██████▎   | 36/57 [00:05<00:03,  6.99it/s]11/28/2021 01:12:23 - INFO - __main__ -   Batch number = 37
Evaluating:  65%|██████▍   | 37/57 [00:05<00:02,  6.98it/s]11/28/2021 01:12:23 - INFO - __main__ -   Batch number = 38
Evaluating:  67%|██████▋   | 38/57 [00:05<00:02,  6.96it/s]11/28/2021 01:12:23 - INFO - __main__ -   Batch number = 39
Evaluating:  68%|██████▊   | 39/57 [00:05<00:02,  6.95it/s]11/28/2021 01:12:24 - INFO - __main__ -   Batch number = 40
Evaluating:  70%|███████   | 40/57 [00:05<00:02,  6.97it/s]11/28/2021 01:12:24 - INFO - __main__ -   Batch number = 41
Evaluating:  72%|███████▏  | 41/57 [00:05<00:02,  6.96it/s]11/28/2021 01:12:24 - INFO - __main__ -   Batch number = 42
Evaluating:  74%|███████▎  | 42/57 [00:06<00:02,  6.94it/s]11/28/2021 01:12:24 - INFO - __main__ -   Batch number = 43
Evaluating:  75%|███████▌  | 43/57 [00:06<00:02,  6.72it/s]11/28/2021 01:12:24 - INFO - __main__ -   Batch number = 44
Evaluating:  77%|███████▋  | 44/57 [00:06<00:01,  6.65it/s]11/28/2021 01:12:24 - INFO - __main__ -   Batch number = 45
Evaluating:  79%|███████▉  | 45/57 [00:06<00:01,  6.60it/s]11/28/2021 01:12:24 - INFO - __main__ -   Batch number = 46
Evaluating:  81%|████████  | 46/57 [00:06<00:01,  6.56it/s]11/28/2021 01:12:25 - INFO - __main__ -   Batch number = 47
Evaluating:  82%|████████▏ | 47/57 [00:06<00:01,  6.59it/s]11/28/2021 01:12:25 - INFO - __main__ -   Batch number = 48
Evaluating:  84%|████████▍ | 48/57 [00:06<00:01,  6.54it/s]11/28/2021 01:12:25 - INFO - __main__ -   Batch number = 49
Evaluating:  86%|████████▌ | 49/57 [00:07<00:01,  6.57it/s]11/28/2021 01:12:25 - INFO - __main__ -   Batch number = 50
Evaluating:  88%|████████▊ | 50/57 [00:07<00:01,  6.60it/s]11/28/2021 01:12:25 - INFO - __main__ -   Batch number = 51
Evaluating:  89%|████████▉ | 51/57 [00:07<00:00,  6.64it/s]11/28/2021 01:12:25 - INFO - __main__ -   Batch number = 52
Evaluating:  91%|█████████ | 52/57 [00:07<00:00,  6.68it/s]11/28/2021 01:12:26 - INFO - __main__ -   Batch number = 53
Evaluating:  93%|█████████▎| 53/57 [00:07<00:00,  6.70it/s]11/28/2021 01:12:26 - INFO - __main__ -   Batch number = 54
Evaluating:  95%|█████████▍| 54/57 [00:07<00:00,  6.69it/s]11/28/2021 01:12:26 - INFO - __main__ -   Batch number = 55
Evaluating:  96%|█████████▋| 55/57 [00:08<00:00,  6.64it/s]11/28/2021 01:12:26 - INFO - __main__ -   Batch number = 56
Evaluating:  98%|█████████▊| 56/57 [00:08<00:00,  6.66it/s]11/28/2021 01:12:26 - INFO - __main__ -   Batch number = 57
Evaluating: 100%|██████████| 57/57 [00:08<00:00,  6.93it/s]
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:12:27 - INFO - __main__ -   ***** Evaluation result  in eu *****
11/28/2021 01:12:27 - INFO - __main__ -     f1 = 0.5468345862844158
11/28/2021 01:12:27 - INFO - __main__ -     loss = 1.3598553757918508
11/28/2021 01:12:27 - INFO - __main__ -     precision = 0.5798728596329334
11/28/2021 01:12:27 - INFO - __main__ -     recall = 0.5173580935827654
21.50user 7.51system 0:27.92elapsed 103%CPU (0avgtext+0avgdata 3987404maxresident)k
0inputs+352outputs (0major+1406642minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:12:30 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='eu', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:12:30 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:12:30 - INFO - __main__ -   Seed = 2
11/28/2021 01:12:30 - INFO - root -   save model
11/28/2021 01:12:30 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='eu', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:12:30 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:12:32 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
11/28/2021 01:12:32 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='wo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:12:32 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:12:32 - INFO - __main__ -   Seed = 1
11/28/2021 01:12:32 - INFO - root -   save model
11/28/2021 01:12:32 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='wo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:12:32 - INFO - __main__ -   Loading pretrained model and tokenizer
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:12:35 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:12:38 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:12:38 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:12:38 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
11/28/2021 01:12:38 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:12:38 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:12:38 - INFO - root -   loading lang adpater bh/wiki@ukp
11/28/2021 01:12:38 - INFO - __main__ -   Adapter Languages : ['bh'], Length : 1
11/28/2021 01:12:38 - INFO - __main__ -   Adapter Names ['bh/wiki@ukp'], Length : 1
11/28/2021 01:12:38 - INFO - __main__ -   Language = bh
11/28/2021 01:12:38 - INFO - __main__ -   Adapter Name = bh/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/bh/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_bh_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/adapter_config.json
Adding adapter 'bh' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/head_config.json
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:12:41 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:12:41 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:12:41 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
11/28/2021 01:12:41 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:12:41 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:12:41 - INFO - root -   loading lang adpater bh/wiki@ukp
11/28/2021 01:12:41 - INFO - __main__ -   Adapter Languages : ['bh'], Length : 1
11/28/2021 01:12:41 - INFO - __main__ -   Adapter Names ['bh/wiki@ukp'], Length : 1
11/28/2021 01:12:41 - INFO - __main__ -   Language = bh
11/28/2021 01:12:41 - INFO - __main__ -   Adapter Name = bh/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/bh/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_bh_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/adapter_config.json
Adding adapter 'bh' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/head_config.json
11/28/2021 01:12:45 - INFO - __main__ -   Language adapter for eu not found, using bh instead
11/28/2021 01:12:45 - INFO - __main__ -   Set active language adapter to bh
11/28/2021 01:12:45 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:12:45 - INFO - __main__ -   Adapter Languages = ['bh']
11/28/2021 01:12:45 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_eu_bert-base-multilingual-cased_128
11/28/2021 01:12:46 - INFO - __main__ -   ***** Running evaluation  in eu *****
11/28/2021 01:12:46 - INFO - __main__ -     Num examples = 1799
11/28/2021 01:12:46 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/57 [00:00<?, ?it/s]11/28/2021 01:12:46 - INFO - __main__ -   Batch number = 1
Evaluating:   2%|▏         | 1/57 [00:00<00:09,  6.12it/s]11/28/2021 01:12:46 - INFO - __main__ -   Batch number = 2
Evaluating:   4%|▎         | 2/57 [00:00<00:08,  6.29it/s]11/28/2021 01:12:46 - INFO - __main__ -   Batch number = 3
Evaluating:   5%|▌         | 3/57 [00:00<00:08,  6.50it/s]11/28/2021 01:12:46 - INFO - __main__ -   Batch number = 4
Evaluating:   7%|▋         | 4/57 [00:00<00:07,  6.67it/s]11/28/2021 01:12:46 - INFO - __main__ -   Batch number = 5
Evaluating:   9%|▉         | 5/57 [00:00<00:07,  6.74it/s]11/28/2021 01:12:47 - INFO - __main__ -   Batch number = 6
Evaluating:  11%|█         | 6/57 [00:00<00:07,  6.87it/s]11/28/2021 01:12:47 - INFO - __main__ -   Batch number = 7
Evaluating:  12%|█▏        | 7/57 [00:01<00:07,  6.86it/s]11/28/2021 01:12:47 - INFO - __main__ -   Batch number = 8
Evaluating:  14%|█▍        | 8/57 [00:01<00:07,  6.92it/s]11/28/2021 01:12:47 - INFO - __main__ -   Batch number = 9
Evaluating:  16%|█▌        | 9/57 [00:01<00:06,  6.93it/s]11/28/2021 01:12:47 - INFO - __main__ -   Batch number = 10
Evaluating:  18%|█▊        | 10/57 [00:01<00:06,  6.92it/s]11/28/2021 01:12:47 - INFO - __main__ -   Batch number = 11
Evaluating:  19%|█▉        | 11/57 [00:01<00:06,  6.92it/s]11/28/2021 01:12:47 - INFO - __main__ -   Batch number = 12
Evaluating:  21%|██        | 12/57 [00:01<00:06,  6.90it/s]11/28/2021 01:12:48 - INFO - __main__ -   Batch number = 13
Evaluating:  23%|██▎       | 13/57 [00:01<00:06,  6.90it/s]11/28/2021 01:12:48 - INFO - __main__ -   Batch number = 14
Evaluating:  25%|██▍       | 14/57 [00:02<00:06,  6.84it/s]11/28/2021 01:12:48 - INFO - __main__ -   Batch number = 15
Evaluating:  26%|██▋       | 15/57 [00:02<00:06,  6.86it/s]11/28/2021 01:12:48 - INFO - __main__ -   Batch number = 16
Evaluating:  28%|██▊       | 16/57 [00:02<00:05,  6.87it/s]11/28/2021 01:12:48 - INFO - __main__ -   Batch number = 17
Evaluating:  30%|██▉       | 17/57 [00:02<00:05,  6.92it/s]11/28/2021 01:12:48 - INFO - __main__ -   Batch number = 18
Evaluating:  32%|███▏      | 18/57 [00:02<00:05,  6.96it/s]11/28/2021 01:12:49 - INFO - __main__ -   Batch number = 19
Evaluating:  33%|███▎      | 19/57 [00:02<00:05,  6.99it/s]11/28/2021 01:12:49 - INFO - __main__ -   Batch number = 20
Evaluating:  35%|███▌      | 20/57 [00:02<00:05,  6.96it/s]11/28/2021 01:12:49 - INFO - __main__ -   Batch number = 21
Evaluating:  37%|███▋      | 21/57 [00:03<00:05,  6.91it/s]11/28/2021 01:12:49 - INFO - __main__ -   Batch number = 22
Evaluating:  39%|███▊      | 22/57 [00:03<00:05,  6.83it/s]11/28/2021 01:12:49 - INFO - __main__ -   Batch number = 23
Evaluating:  40%|████      | 23/57 [00:03<00:05,  6.77it/s]11/28/2021 01:12:49 - INFO - __main__ -   Batch number = 24
Evaluating:  42%|████▏     | 24/57 [00:03<00:04,  6.79it/s]11/28/2021 01:12:49 - INFO - __main__ -   Batch number = 25
Evaluating:  44%|████▍     | 25/57 [00:03<00:04,  6.51it/s]11/28/2021 01:12:50 - INFO - __main__ -   Batch number = 26
Evaluating:  46%|████▌     | 26/57 [00:03<00:04,  6.63it/s]11/28/2021 01:12:50 - INFO - __main__ -   Batch number = 27
Evaluating:  47%|████▋     | 27/57 [00:03<00:04,  6.64it/s]11/28/2021 01:12:50 - INFO - __main__ -   Batch number = 28
Evaluating:  49%|████▉     | 28/57 [00:04<00:04,  6.66it/s]11/28/2021 01:12:50 - INFO - __main__ -   Batch number = 29
Evaluating:  51%|█████     | 29/57 [00:04<00:04,  6.63it/s]11/28/2021 01:12:50 - INFO - __main__ -   Batch number = 30
11/28/2021 01:12:50 - INFO - __main__ -   Language wo, split test does not exist
Evaluating:  53%|█████▎    | 30/57 [00:04<00:04,  6.67it/s]11/28/2021 01:12:50 - INFO - __main__ -   Batch number = 31
Evaluating:  54%|█████▍    | 31/57 [00:04<00:03,  6.70it/s]11/28/2021 01:12:50 - INFO - __main__ -   Batch number = 32
Evaluating:  56%|█████▌    | 32/57 [00:04<00:03,  6.76it/s]11/28/2021 01:12:51 - INFO - __main__ -   Batch number = 33
Evaluating:  58%|█████▊    | 33/57 [00:04<00:03,  6.84it/s]11/28/2021 01:12:51 - INFO - __main__ -   Batch number = 34
Evaluating:  60%|█████▉    | 34/57 [00:05<00:03,  6.90it/s]11/28/2021 01:12:51 - INFO - __main__ -   Batch number = 35
Evaluating:  61%|██████▏   | 35/57 [00:05<00:03,  6.91it/s]11/28/2021 01:12:51 - INFO - __main__ -   Batch number = 36
Evaluating:  63%|██████▎   | 36/57 [00:05<00:03,  6.92it/s]11/28/2021 01:12:51 - INFO - __main__ -   Batch number = 37
15.35user 6.05system 0:20.59elapsed 103%CPU (0avgtext+0avgdata 3989548maxresident)k
0inputs+48outputs (0major+1471916minor)pagefaults 0swaps
Evaluating:  65%|██████▍   | 37/57 [00:05<00:02,  6.87it/s]11/28/2021 01:12:51 - INFO - __main__ -   Batch number = 38
Evaluating:  67%|██████▋   | 38/57 [00:05<00:02,  6.78it/s]11/28/2021 01:12:51 - INFO - __main__ -   Batch number = 39
Evaluating:  68%|██████▊   | 39/57 [00:05<00:02,  6.83it/s]11/28/2021 01:12:52 - INFO - __main__ -   Batch number = 40
Evaluating:  70%|███████   | 40/57 [00:05<00:02,  6.88it/s]11/28/2021 01:12:52 - INFO - __main__ -   Batch number = 41
Evaluating:  72%|███████▏  | 41/57 [00:06<00:02,  6.90it/s]11/28/2021 01:12:52 - INFO - __main__ -   Batch number = 42
Evaluating:  74%|███████▎  | 42/57 [00:06<00:02,  6.93it/s]11/28/2021 01:12:52 - INFO - __main__ -   Batch number = 43
Evaluating:  75%|███████▌  | 43/57 [00:06<00:02,  6.95it/s]11/28/2021 01:12:52 - INFO - __main__ -   Batch number = 44
Evaluating:  77%|███████▋  | 44/57 [00:06<00:01,  6.95it/s]11/28/2021 01:12:52 - INFO - __main__ -   Batch number = 45
Evaluating:  79%|███████▉  | 45/57 [00:06<00:01,  6.95it/s]11/28/2021 01:12:52 - INFO - __main__ -   Batch number = 46
Evaluating:  81%|████████  | 46/57 [00:06<00:01,  6.96it/s]11/28/2021 01:12:53 - INFO - __main__ -   Batch number = 47
PyTorch version 1.10.0+cu102 available.
Evaluating:  82%|████████▏ | 47/57 [00:06<00:01,  6.91it/s]11/28/2021 01:12:53 - INFO - __main__ -   Batch number = 48
Evaluating:  84%|████████▍ | 48/57 [00:07<00:01,  6.93it/s]11/28/2021 01:12:53 - INFO - __main__ -   Batch number = 49
Evaluating:  86%|████████▌ | 49/57 [00:07<00:01,  6.94it/s]11/28/2021 01:12:53 - INFO - __main__ -   Batch number = 50
Evaluating:  88%|████████▊ | 50/57 [00:07<00:01,  6.94it/s]11/28/2021 01:12:53 - INFO - __main__ -   Batch number = 51
11/28/2021 01:12:53 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='wo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:12:53 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:12:53 - INFO - __main__ -   Seed = 2
11/28/2021 01:12:53 - INFO - root -   save model
11/28/2021 01:12:53 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='wo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:12:53 - INFO - __main__ -   Loading pretrained model and tokenizer
Evaluating:  89%|████████▉ | 51/57 [00:07<00:00,  6.95it/s]11/28/2021 01:12:53 - INFO - __main__ -   Batch number = 52
Evaluating:  91%|█████████ | 52/57 [00:07<00:00,  6.96it/s]11/28/2021 01:12:53 - INFO - __main__ -   Batch number = 53
Evaluating:  93%|█████████▎| 53/57 [00:07<00:00,  6.94it/s]11/28/2021 01:12:54 - INFO - __main__ -   Batch number = 54
Evaluating:  95%|█████████▍| 54/57 [00:07<00:00,  6.95it/s]11/28/2021 01:12:54 - INFO - __main__ -   Batch number = 55
Evaluating:  96%|█████████▋| 55/57 [00:08<00:00,  6.94it/s]11/28/2021 01:12:54 - INFO - __main__ -   Batch number = 56
Evaluating:  98%|█████████▊| 56/57 [00:08<00:00,  6.92it/s]11/28/2021 01:12:54 - INFO - __main__ -   Batch number = 57
Evaluating: 100%|██████████| 57/57 [00:08<00:00,  6.93it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}


/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:12:55 - INFO - __main__ -   ***** Evaluation result  in eu *****
11/28/2021 01:12:55 - INFO - __main__ -     f1 = 0.5556694396109932
11/28/2021 01:12:55 - INFO - __main__ -     loss = 1.3980139661253543
11/28/2021 01:12:55 - INFO - __main__ -     precision = 0.5801891488302638
11/28/2021 01:12:55 - INFO - __main__ -     recall = 0.5331381786580067
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

22.09user 9.39system 0:27.91elapsed 112%CPU (0avgtext+0avgdata 3997816maxresident)k
0inputs+320outputs (0major+1561451minor)pagefaults 0swaps
loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:12:56 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:12:57 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='eu', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:12:57 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:12:57 - INFO - __main__ -   Seed = 3
11/28/2021 01:12:57 - INFO - root -   save model
11/28/2021 01:12:57 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='eu', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:12:57 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:13:00 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:13:02 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:13:02 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:13:02 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
11/28/2021 01:13:02 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:13:02 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:13:02 - INFO - root -   loading lang adpater bh/wiki@ukp
11/28/2021 01:13:02 - INFO - __main__ -   Adapter Languages : ['bh'], Length : 1
11/28/2021 01:13:02 - INFO - __main__ -   Adapter Names ['bh/wiki@ukp'], Length : 1
11/28/2021 01:13:02 - INFO - __main__ -   Language = bh
11/28/2021 01:13:02 - INFO - __main__ -   Adapter Name = bh/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/bh/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_bh_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/adapter_config.json
Adding adapter 'bh' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/head_config.json
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:13:06 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:13:06 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:13:06 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
11/28/2021 01:13:06 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:13:06 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:13:06 - INFO - root -   loading lang adpater bh/wiki@ukp
11/28/2021 01:13:06 - INFO - __main__ -   Adapter Languages : ['bh'], Length : 1
11/28/2021 01:13:06 - INFO - __main__ -   Adapter Names ['bh/wiki@ukp'], Length : 1
11/28/2021 01:13:06 - INFO - __main__ -   Language = bh
11/28/2021 01:13:06 - INFO - __main__ -   Adapter Name = bh/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/bh/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_bh_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/adapter_config.json
Adding adapter 'bh' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/head_config.json
11/28/2021 01:13:10 - INFO - __main__ -   Language wo, split test does not exist
14.17user 5.49system 0:19.29elapsed 101%CPU (0avgtext+0avgdata 3984448maxresident)k
0inputs+48outputs (0major+1235612minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:13:12 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='wo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:13:12 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:13:12 - INFO - __main__ -   Seed = 3
11/28/2021 01:13:12 - INFO - root -   save model
11/28/2021 01:13:12 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='wo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:13:12 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

11/28/2021 01:13:14 - INFO - __main__ -   Language adapter for eu not found, using bh instead
11/28/2021 01:13:14 - INFO - __main__ -   Set active language adapter to bh
11/28/2021 01:13:14 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:13:14 - INFO - __main__ -   Adapter Languages = ['bh']
11/28/2021 01:13:14 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_eu_bert-base-multilingual-cased_128
11/28/2021 01:13:14 - INFO - __main__ -   ***** Running evaluation  in eu *****
11/28/2021 01:13:14 - INFO - __main__ -     Num examples = 1799
11/28/2021 01:13:14 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/57 [00:00<?, ?it/s]11/28/2021 01:13:14 - INFO - __main__ -   Batch number = 1
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:   2%|▏         | 1/57 [00:00<00:09,  5.76it/s]11/28/2021 01:13:14 - INFO - __main__ -   Batch number = 2
Evaluating:   4%|▎         | 2/57 [00:00<00:08,  6.49it/s]11/28/2021 01:13:14 - INFO - __main__ -   Batch number = 3
Evaluating:   5%|▌         | 3/57 [00:00<00:09,  5.90it/s]11/28/2021 01:13:15 - INFO - __main__ -   Batch number = 4
Evaluating:   7%|▋         | 4/57 [00:00<00:08,  6.33it/s]11/28/2021 01:13:15 - INFO - __main__ -   Batch number = 5
Evaluating:   9%|▉         | 5/57 [00:00<00:07,  6.59it/s]11/28/2021 01:13:15 - INFO - __main__ -   Batch number = 6
loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
Evaluating:  11%|█         | 6/57 [00:00<00:07,  6.75it/s]11/28/2021 01:13:15 - INFO - __main__ -   Batch number = 7
Evaluating:  12%|█▏        | 7/57 [00:01<00:07,  6.86it/s]11/28/2021 01:13:15 - INFO - __main__ -   Batch number = 8
11/28/2021 01:13:15 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
Evaluating:  14%|█▍        | 8/57 [00:01<00:07,  6.93it/s]11/28/2021 01:13:15 - INFO - __main__ -   Batch number = 9
Evaluating:  16%|█▌        | 9/57 [00:01<00:06,  6.97it/s]11/28/2021 01:13:15 - INFO - __main__ -   Batch number = 10
Evaluating:  18%|█▊        | 10/57 [00:01<00:06,  7.00it/s]11/28/2021 01:13:16 - INFO - __main__ -   Batch number = 11
Evaluating:  19%|█▉        | 11/57 [00:01<00:06,  7.00it/s]11/28/2021 01:13:16 - INFO - __main__ -   Batch number = 12
Evaluating:  21%|██        | 12/57 [00:01<00:06,  7.02it/s]11/28/2021 01:13:16 - INFO - __main__ -   Batch number = 13
Evaluating:  23%|██▎       | 13/57 [00:01<00:06,  7.03it/s]11/28/2021 01:13:16 - INFO - __main__ -   Batch number = 14
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  25%|██▍       | 14/57 [00:02<00:06,  7.03it/s]11/28/2021 01:13:16 - INFO - __main__ -   Batch number = 15
Evaluating:  26%|██▋       | 15/57 [00:02<00:05,  7.03it/s]11/28/2021 01:13:16 - INFO - __main__ -   Batch number = 16
Evaluating:  28%|██▊       | 16/57 [00:02<00:05,  7.04it/s]11/28/2021 01:13:16 - INFO - __main__ -   Batch number = 17
Evaluating:  30%|██▉       | 17/57 [00:02<00:05,  7.02it/s]11/28/2021 01:13:17 - INFO - __main__ -   Batch number = 18
Evaluating:  32%|███▏      | 18/57 [00:02<00:05,  7.03it/s]11/28/2021 01:13:17 - INFO - __main__ -   Batch number = 19
Evaluating:  33%|███▎      | 19/57 [00:02<00:05,  7.00it/s]11/28/2021 01:13:17 - INFO - __main__ -   Batch number = 20
Evaluating:  35%|███▌      | 20/57 [00:02<00:05,  7.01it/s]11/28/2021 01:13:17 - INFO - __main__ -   Batch number = 21
Evaluating:  37%|███▋      | 21/57 [00:03<00:05,  7.01it/s]11/28/2021 01:13:17 - INFO - __main__ -   Batch number = 22
Evaluating:  39%|███▊      | 22/57 [00:03<00:04,  7.01it/s]11/28/2021 01:13:17 - INFO - __main__ -   Batch number = 23
Evaluating:  40%|████      | 23/57 [00:03<00:04,  7.02it/s]11/28/2021 01:13:17 - INFO - __main__ -   Batch number = 24
Evaluating:  42%|████▏     | 24/57 [00:03<00:04,  7.02it/s]11/28/2021 01:13:18 - INFO - __main__ -   Batch number = 25
Evaluating:  44%|████▍     | 25/57 [00:03<00:04,  7.01it/s]11/28/2021 01:13:18 - INFO - __main__ -   Batch number = 26
Evaluating:  46%|████▌     | 26/57 [00:03<00:04,  7.01it/s]11/28/2021 01:13:18 - INFO - __main__ -   Batch number = 27
Evaluating:  47%|████▋     | 27/57 [00:03<00:04,  7.00it/s]11/28/2021 01:13:18 - INFO - __main__ -   Batch number = 28
Evaluating:  49%|████▉     | 28/57 [00:04<00:04,  6.97it/s]11/28/2021 01:13:18 - INFO - __main__ -   Batch number = 29
Evaluating:  51%|█████     | 29/57 [00:04<00:04,  6.98it/s]11/28/2021 01:13:18 - INFO - __main__ -   Batch number = 30
Evaluating:  53%|█████▎    | 30/57 [00:04<00:03,  6.94it/s]11/28/2021 01:13:18 - INFO - __main__ -   Batch number = 31
Evaluating:  54%|█████▍    | 31/57 [00:04<00:03,  6.85it/s]11/28/2021 01:13:19 - INFO - __main__ -   Batch number = 32
Evaluating:  56%|█████▌    | 32/57 [00:04<00:03,  6.88it/s]11/28/2021 01:13:19 - INFO - __main__ -   Batch number = 33
Evaluating:  58%|█████▊    | 33/57 [00:04<00:03,  6.91it/s]11/28/2021 01:13:19 - INFO - __main__ -   Batch number = 34
Evaluating:  60%|█████▉    | 34/57 [00:04<00:03,  6.93it/s]11/28/2021 01:13:19 - INFO - __main__ -   Batch number = 35
Evaluating:  61%|██████▏   | 35/57 [00:05<00:03,  6.94it/s]11/28/2021 01:13:19 - INFO - __main__ -   Batch number = 36
Evaluating:  63%|██████▎   | 36/57 [00:05<00:03,  6.96it/s]11/28/2021 01:13:19 - INFO - __main__ -   Batch number = 37
Evaluating:  65%|██████▍   | 37/57 [00:05<00:02,  6.93it/s]11/28/2021 01:13:20 - INFO - __main__ -   Batch number = 38
Evaluating:  67%|██████▋   | 38/57 [00:05<00:03,  5.53it/s]11/28/2021 01:13:20 - INFO - __main__ -   Batch number = 39
Evaluating:  68%|██████▊   | 39/57 [00:05<00:03,  5.88it/s]11/28/2021 01:13:20 - INFO - __main__ -   Batch number = 40
Evaluating:  70%|███████   | 40/57 [00:05<00:02,  6.17it/s]11/28/2021 01:13:20 - INFO - __main__ -   Batch number = 41
Evaluating:  72%|███████▏  | 41/57 [00:06<00:02,  6.40it/s]11/28/2021 01:13:20 - INFO - __main__ -   Batch number = 42
Evaluating:  74%|███████▎  | 42/57 [00:06<00:02,  6.56it/s]11/28/2021 01:13:20 - INFO - __main__ -   Batch number = 43
Evaluating:  75%|███████▌  | 43/57 [00:06<00:02,  6.65it/s]11/28/2021 01:13:20 - INFO - __main__ -   Batch number = 44
Evaluating:  77%|███████▋  | 44/57 [00:06<00:01,  6.73it/s]11/28/2021 01:13:21 - INFO - __main__ -   Batch number = 45
Evaluating:  79%|███████▉  | 45/57 [00:06<00:01,  6.80it/s]11/28/2021 01:13:21 - INFO - __main__ -   Batch number = 46
Evaluating:  81%|████████  | 46/57 [00:06<00:01,  6.82it/s]11/28/2021 01:13:21 - INFO - __main__ -   Batch number = 47
Evaluating:  82%|████████▏ | 47/57 [00:06<00:01,  6.86it/s]11/28/2021 01:13:21 - INFO - __main__ -   Batch number = 48
Evaluating:  84%|████████▍ | 48/57 [00:07<00:01,  6.87it/s]11/28/2021 01:13:21 - INFO - __main__ -   Batch number = 49
Evaluating:  86%|████████▌ | 49/57 [00:07<00:01,  6.87it/s]11/28/2021 01:13:21 - INFO - __main__ -   Batch number = 50
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:13:21 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:13:21 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:13:21 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
11/28/2021 01:13:21 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:13:21 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Evaluating:  88%|████████▊ | 50/57 [00:07<00:01,  6.89it/s]11/28/2021 01:13:21 - INFO - __main__ -   Batch number = 51
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:13:21 - INFO - root -   loading lang adpater bh/wiki@ukp
11/28/2021 01:13:21 - INFO - __main__ -   Adapter Languages : ['bh'], Length : 1
11/28/2021 01:13:21 - INFO - __main__ -   Adapter Names ['bh/wiki@ukp'], Length : 1
11/28/2021 01:13:21 - INFO - __main__ -   Language = bh
11/28/2021 01:13:21 - INFO - __main__ -   Adapter Name = bh/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/bh/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_bh_wiki_pfeiffer.zip.
Evaluating:  89%|████████▉ | 51/57 [00:07<00:00,  6.90it/s]11/28/2021 01:13:22 - INFO - __main__ -   Batch number = 52
Evaluating:  91%|█████████ | 52/57 [00:07<00:00,  6.89it/s]11/28/2021 01:13:22 - INFO - __main__ -   Batch number = 53
Evaluating:  93%|█████████▎| 53/57 [00:07<00:00,  6.82it/s]11/28/2021 01:13:22 - INFO - __main__ -   Batch number = 54
Evaluating:  95%|█████████▍| 54/57 [00:07<00:00,  6.83it/s]11/28/2021 01:13:22 - INFO - __main__ -   Batch number = 55
Evaluating:  96%|█████████▋| 55/57 [00:08<00:00,  6.80it/s]11/28/2021 01:13:22 - INFO - __main__ -   Batch number = 56
Evaluating:  98%|█████████▊| 56/57 [00:08<00:00,  6.82it/s]11/28/2021 01:13:22 - INFO - __main__ -   Batch number = 57
Evaluating: 100%|██████████| 57/57 [00:08<00:00,  6.89it/s]
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:13:23 - INFO - __main__ -   ***** Evaluation result  in eu *****
11/28/2021 01:13:23 - INFO - __main__ -     f1 = 0.5368656861790202
11/28/2021 01:13:23 - INFO - __main__ -     loss = 1.5101409439454998
11/28/2021 01:13:23 - INFO - __main__ -     precision = 0.5740171830252538
11/28/2021 01:13:23 - INFO - __main__ -     recall = 0.5042308923752459
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/adapter_config.json
Adding adapter 'bh' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/head_config.json
22.20user 9.60system 0:28.12elapsed 113%CPU (0avgtext+0avgdata 3982992maxresident)k
0inputs+352outputs (0major+1831868minor)pagefaults 0swaps
11/28/2021 01:13:28 - INFO - __main__ -   Language wo, split test does not exist
13.42user 5.89system 0:18.09elapsed 106%CPU (0avgtext+0avgdata 3983988maxresident)k
0inputs+40outputs (0major+1621045minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:13:42 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='es', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:13:42 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:13:42 - INFO - __main__ -   Seed = 1
11/28/2021 01:13:42 - INFO - root -   save model
11/28/2021 01:13:42 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='es', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:13:42 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:13:45 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:13:51 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:13:51 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:13:51 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
11/28/2021 01:13:51 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:13:51 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:13:51 - INFO - root -   loading lang adpater bh/wiki@ukp
11/28/2021 01:13:51 - INFO - __main__ -   Adapter Languages : ['bh'], Length : 1
11/28/2021 01:13:51 - INFO - __main__ -   Adapter Names ['bh/wiki@ukp'], Length : 1
11/28/2021 01:13:51 - INFO - __main__ -   Language = bh
11/28/2021 01:13:51 - INFO - __main__ -   Adapter Name = bh/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/bh/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_bh_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/adapter_config.json
Adding adapter 'bh' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/head_config.json
11/28/2021 01:14:02 - INFO - __main__ -   Language adapter for es not found, using bh instead
11/28/2021 01:14:02 - INFO - __main__ -   Set active language adapter to bh
11/28/2021 01:14:02 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:14:02 - INFO - __main__ -   Adapter Languages = ['bh']
11/28/2021 01:14:02 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_es_bert-base-multilingual-cased_128
11/28/2021 01:14:03 - INFO - __main__ -   ***** Running evaluation  in es *****
11/28/2021 01:14:03 - INFO - __main__ -     Num examples = 3154
11/28/2021 01:14:03 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/99 [00:00<?, ?it/s]11/28/2021 01:14:03 - INFO - __main__ -   Batch number = 1
Evaluating:   1%|          | 1/99 [00:00<00:28,  3.41it/s]11/28/2021 01:14:03 - INFO - __main__ -   Batch number = 2
Evaluating:   2%|▏         | 2/99 [00:00<00:29,  3.28it/s]11/28/2021 01:14:03 - INFO - __main__ -   Batch number = 3
Evaluating:   3%|▎         | 3/99 [00:00<00:29,  3.22it/s]11/28/2021 01:14:03 - INFO - __main__ -   Batch number = 4
Evaluating:   4%|▍         | 4/99 [00:01<00:29,  3.26it/s]11/28/2021 01:14:04 - INFO - __main__ -   Batch number = 5
Evaluating:   5%|▌         | 5/99 [00:01<00:28,  3.30it/s]11/28/2021 01:14:04 - INFO - __main__ -   Batch number = 6
Evaluating:   6%|▌         | 6/99 [00:01<00:28,  3.30it/s]11/28/2021 01:14:04 - INFO - __main__ -   Batch number = 7
Evaluating:   7%|▋         | 7/99 [00:02<00:30,  3.04it/s]11/28/2021 01:14:05 - INFO - __main__ -   Batch number = 8
Evaluating:   8%|▊         | 8/99 [00:02<00:29,  3.12it/s]11/28/2021 01:14:05 - INFO - __main__ -   Batch number = 9
Evaluating:   9%|▉         | 9/99 [00:02<00:28,  3.18it/s]11/28/2021 01:14:05 - INFO - __main__ -   Batch number = 10
Evaluating:  10%|█         | 10/99 [00:03<00:27,  3.22it/s]11/28/2021 01:14:06 - INFO - __main__ -   Batch number = 11
Evaluating:  11%|█         | 11/99 [00:03<00:27,  3.24it/s]11/28/2021 01:14:06 - INFO - __main__ -   Batch number = 12
Evaluating:  12%|█▏        | 12/99 [00:03<00:27,  3.17it/s]11/28/2021 01:14:06 - INFO - __main__ -   Batch number = 13
Evaluating:  13%|█▎        | 13/99 [00:04<00:26,  3.21it/s]11/28/2021 01:14:07 - INFO - __main__ -   Batch number = 14
Evaluating:  14%|█▍        | 14/99 [00:04<00:25,  3.27it/s]11/28/2021 01:14:07 - INFO - __main__ -   Batch number = 15
Evaluating:  15%|█▌        | 15/99 [00:04<00:25,  3.30it/s]11/28/2021 01:14:07 - INFO - __main__ -   Batch number = 16
Evaluating:  16%|█▌        | 16/99 [00:04<00:25,  3.26it/s]11/28/2021 01:14:08 - INFO - __main__ -   Batch number = 17
Evaluating:  17%|█▋        | 17/99 [00:05<00:24,  3.30it/s]11/28/2021 01:14:08 - INFO - __main__ -   Batch number = 18
Evaluating:  18%|█▊        | 18/99 [00:05<00:24,  3.29it/s]11/28/2021 01:14:08 - INFO - __main__ -   Batch number = 19
Evaluating:  19%|█▉        | 19/99 [00:05<00:24,  3.22it/s]11/28/2021 01:14:08 - INFO - __main__ -   Batch number = 20
Evaluating:  20%|██        | 20/99 [00:06<00:24,  3.22it/s]11/28/2021 01:14:09 - INFO - __main__ -   Batch number = 21
Evaluating:  21%|██        | 21/99 [00:06<00:24,  3.24it/s]11/28/2021 01:14:09 - INFO - __main__ -   Batch number = 22
Evaluating:  22%|██▏       | 22/99 [00:06<00:23,  3.27it/s]11/28/2021 01:14:09 - INFO - __main__ -   Batch number = 23
Evaluating:  23%|██▎       | 23/99 [00:07<00:22,  3.42it/s]11/28/2021 01:14:10 - INFO - __main__ -   Batch number = 24
Evaluating:  24%|██▍       | 24/99 [00:07<00:22,  3.38it/s]11/28/2021 01:14:10 - INFO - __main__ -   Batch number = 25
Evaluating:  25%|██▌       | 25/99 [00:07<00:22,  3.25it/s]11/28/2021 01:14:10 - INFO - __main__ -   Batch number = 26
Evaluating:  26%|██▋       | 26/99 [00:08<00:22,  3.25it/s]11/28/2021 01:14:11 - INFO - __main__ -   Batch number = 27
Evaluating:  27%|██▋       | 27/99 [00:08<00:22,  3.25it/s]11/28/2021 01:14:11 - INFO - __main__ -   Batch number = 28
Evaluating:  28%|██▊       | 28/99 [00:08<00:21,  3.26it/s]11/28/2021 01:14:11 - INFO - __main__ -   Batch number = 29
Evaluating:  29%|██▉       | 29/99 [00:08<00:21,  3.26it/s]11/28/2021 01:14:11 - INFO - __main__ -   Batch number = 30
Evaluating:  30%|███       | 30/99 [00:09<00:21,  3.27it/s]11/28/2021 01:14:12 - INFO - __main__ -   Batch number = 31
Evaluating:  31%|███▏      | 31/99 [00:09<00:20,  3.25it/s]11/28/2021 01:14:12 - INFO - __main__ -   Batch number = 32
Evaluating:  32%|███▏      | 32/99 [00:09<00:21,  3.16it/s]11/28/2021 01:14:12 - INFO - __main__ -   Batch number = 33
Evaluating:  33%|███▎      | 33/99 [00:10<00:20,  3.16it/s]11/28/2021 01:14:13 - INFO - __main__ -   Batch number = 34
Evaluating:  34%|███▍      | 34/99 [00:10<00:20,  3.20it/s]11/28/2021 01:14:13 - INFO - __main__ -   Batch number = 35
Evaluating:  35%|███▌      | 35/99 [00:10<00:19,  3.22it/s]11/28/2021 01:14:13 - INFO - __main__ -   Batch number = 36
Evaluating:  36%|███▋      | 36/99 [00:11<00:19,  3.22it/s]11/28/2021 01:14:14 - INFO - __main__ -   Batch number = 37
Evaluating:  37%|███▋      | 37/99 [00:11<00:19,  3.26it/s]11/28/2021 01:14:14 - INFO - __main__ -   Batch number = 38
Evaluating:  38%|███▊      | 38/99 [00:11<00:18,  3.24it/s]11/28/2021 01:14:14 - INFO - __main__ -   Batch number = 39
Evaluating:  39%|███▉      | 39/99 [00:12<00:18,  3.17it/s]11/28/2021 01:14:15 - INFO - __main__ -   Batch number = 40
Evaluating:  40%|████      | 40/99 [00:12<00:18,  3.15it/s]11/28/2021 01:14:15 - INFO - __main__ -   Batch number = 41
Evaluating:  41%|████▏     | 41/99 [00:12<00:16,  3.60it/s]11/28/2021 01:14:15 - INFO - __main__ -   Batch number = 42
Evaluating:  42%|████▏     | 42/99 [00:12<00:14,  4.06it/s]11/28/2021 01:14:15 - INFO - __main__ -   Batch number = 43
Evaluating:  43%|████▎     | 43/99 [00:12<00:12,  4.49it/s]11/28/2021 01:14:15 - INFO - __main__ -   Batch number = 44
Evaluating:  44%|████▍     | 44/99 [00:13<00:13,  3.94it/s]11/28/2021 01:14:16 - INFO - __main__ -   Batch number = 45
Evaluating:  45%|████▌     | 45/99 [00:13<00:14,  3.69it/s]11/28/2021 01:14:16 - INFO - __main__ -   Batch number = 46
Evaluating:  46%|████▋     | 46/99 [00:13<00:14,  3.56it/s]11/28/2021 01:14:16 - INFO - __main__ -   Batch number = 47
Evaluating:  47%|████▋     | 47/99 [00:14<00:14,  3.47it/s]11/28/2021 01:14:17 - INFO - __main__ -   Batch number = 48
Evaluating:  48%|████▊     | 48/99 [00:14<00:14,  3.43it/s]11/28/2021 01:14:17 - INFO - __main__ -   Batch number = 49
Evaluating:  49%|████▉     | 49/99 [00:14<00:14,  3.39it/s]11/28/2021 01:14:17 - INFO - __main__ -   Batch number = 50
Evaluating:  51%|█████     | 50/99 [00:15<00:14,  3.32it/s]11/28/2021 01:14:18 - INFO - __main__ -   Batch number = 51
Evaluating:  52%|█████▏    | 51/99 [00:15<00:14,  3.20it/s]11/28/2021 01:14:18 - INFO - __main__ -   Batch number = 52
Evaluating:  53%|█████▎    | 52/99 [00:15<00:14,  3.21it/s]11/28/2021 01:14:18 - INFO - __main__ -   Batch number = 53
Evaluating:  54%|█████▎    | 53/99 [00:16<00:14,  3.25it/s]11/28/2021 01:14:19 - INFO - __main__ -   Batch number = 54
Evaluating:  55%|█████▍    | 54/99 [00:16<00:13,  3.28it/s]11/28/2021 01:14:19 - INFO - __main__ -   Batch number = 55
Evaluating:  56%|█████▌    | 55/99 [00:16<00:14,  3.03it/s]11/28/2021 01:14:19 - INFO - __main__ -   Batch number = 56
Evaluating:  57%|█████▋    | 56/99 [00:16<00:13,  3.19it/s]11/28/2021 01:14:20 - INFO - __main__ -   Batch number = 57
Evaluating:  58%|█████▊    | 57/99 [00:17<00:13,  3.11it/s]11/28/2021 01:14:20 - INFO - __main__ -   Batch number = 58
Evaluating:  59%|█████▊    | 58/99 [00:17<00:13,  3.05it/s]11/28/2021 01:14:20 - INFO - __main__ -   Batch number = 59
Evaluating:  60%|█████▉    | 59/99 [00:17<00:13,  3.03it/s]11/28/2021 01:14:21 - INFO - __main__ -   Batch number = 60
Evaluating:  61%|██████    | 60/99 [00:18<00:12,  3.01it/s]11/28/2021 01:14:21 - INFO - __main__ -   Batch number = 61
Evaluating:  62%|██████▏   | 61/99 [00:18<00:12,  3.03it/s]11/28/2021 01:14:21 - INFO - __main__ -   Batch number = 62
Evaluating:  63%|██████▎   | 62/99 [00:18<00:12,  3.02it/s]11/28/2021 01:14:22 - INFO - __main__ -   Batch number = 63
Evaluating:  64%|██████▎   | 63/99 [00:19<00:11,  3.01it/s]11/28/2021 01:14:22 - INFO - __main__ -   Batch number = 64
Evaluating:  65%|██████▍   | 64/99 [00:19<00:11,  3.00it/s]11/28/2021 01:14:22 - INFO - __main__ -   Batch number = 65
Evaluating:  66%|██████▌   | 65/99 [00:19<00:11,  3.00it/s]11/28/2021 01:14:23 - INFO - __main__ -   Batch number = 66
Evaluating:  67%|██████▋   | 66/99 [00:20<00:11,  3.00it/s]11/28/2021 01:14:23 - INFO - __main__ -   Batch number = 67
Evaluating:  68%|██████▊   | 67/99 [00:20<00:10,  2.93it/s]11/28/2021 01:14:23 - INFO - __main__ -   Batch number = 68
Evaluating:  69%|██████▊   | 68/99 [00:21<00:10,  2.95it/s]11/28/2021 01:14:24 - INFO - __main__ -   Batch number = 69
Evaluating:  70%|██████▉   | 69/99 [00:21<00:10,  2.96it/s]11/28/2021 01:14:24 - INFO - __main__ -   Batch number = 70
Evaluating:  71%|███████   | 70/99 [00:21<00:09,  2.98it/s]11/28/2021 01:14:24 - INFO - __main__ -   Batch number = 71
Evaluating:  72%|███████▏  | 71/99 [00:21<00:08,  3.24it/s]11/28/2021 01:14:24 - INFO - __main__ -   Batch number = 72
Evaluating:  73%|███████▎  | 72/99 [00:22<00:08,  3.15it/s]11/28/2021 01:14:25 - INFO - __main__ -   Batch number = 73
Evaluating:  74%|███████▎  | 73/99 [00:22<00:08,  3.09it/s]11/28/2021 01:14:25 - INFO - __main__ -   Batch number = 74
Evaluating:  75%|███████▍  | 74/99 [00:22<00:08,  3.07it/s]11/28/2021 01:14:25 - INFO - __main__ -   Batch number = 75
Evaluating:  76%|███████▌  | 75/99 [00:23<00:07,  3.04it/s]11/28/2021 01:14:26 - INFO - __main__ -   Batch number = 76
Evaluating:  77%|███████▋  | 76/99 [00:23<00:07,  3.02it/s]11/28/2021 01:14:26 - INFO - __main__ -   Batch number = 77
Evaluating:  78%|███████▊  | 77/99 [00:23<00:07,  3.01it/s]11/28/2021 01:14:26 - INFO - __main__ -   Batch number = 78
Evaluating:  79%|███████▉  | 78/99 [00:24<00:06,  3.00it/s]11/28/2021 01:14:27 - INFO - __main__ -   Batch number = 79
Evaluating:  80%|███████▉  | 79/99 [00:24<00:06,  2.99it/s]11/28/2021 01:14:27 - INFO - __main__ -   Batch number = 80
Evaluating:  81%|████████  | 80/99 [00:24<00:06,  2.99it/s]11/28/2021 01:14:28 - INFO - __main__ -   Batch number = 81
Evaluating:  82%|████████▏ | 81/99 [00:25<00:06,  2.99it/s]11/28/2021 01:14:28 - INFO - __main__ -   Batch number = 82
Evaluating:  83%|████████▎ | 82/99 [00:25<00:05,  2.97it/s]11/28/2021 01:14:28 - INFO - __main__ -   Batch number = 83
Evaluating:  84%|████████▍ | 83/99 [00:25<00:05,  2.97it/s]11/28/2021 01:14:29 - INFO - __main__ -   Batch number = 84
Evaluating:  85%|████████▍ | 84/99 [00:26<00:05,  2.98it/s]11/28/2021 01:14:29 - INFO - __main__ -   Batch number = 85
Evaluating:  86%|████████▌ | 85/99 [00:26<00:04,  2.97it/s]11/28/2021 01:14:29 - INFO - __main__ -   Batch number = 86
Evaluating:  87%|████████▋ | 86/99 [00:26<00:03,  3.32it/s]11/28/2021 01:14:29 - INFO - __main__ -   Batch number = 87
Evaluating:  88%|████████▊ | 87/99 [00:27<00:03,  3.22it/s]11/28/2021 01:14:30 - INFO - __main__ -   Batch number = 88
Evaluating:  89%|████████▉ | 88/99 [00:27<00:03,  3.15it/s]11/28/2021 01:14:30 - INFO - __main__ -   Batch number = 89
Evaluating:  90%|████████▉ | 89/99 [00:27<00:03,  3.11it/s]11/28/2021 01:14:30 - INFO - __main__ -   Batch number = 90
Evaluating:  91%|█████████ | 90/99 [00:28<00:02,  3.07it/s]11/28/2021 01:14:31 - INFO - __main__ -   Batch number = 91
Evaluating:  92%|█████████▏| 91/99 [00:28<00:02,  3.06it/s]11/28/2021 01:14:31 - INFO - __main__ -   Batch number = 92
Evaluating:  93%|█████████▎| 92/99 [00:28<00:02,  3.05it/s]11/28/2021 01:14:31 - INFO - __main__ -   Batch number = 93
Evaluating:  94%|█████████▍| 93/99 [00:29<00:01,  3.05it/s]11/28/2021 01:14:32 - INFO - __main__ -   Batch number = 94
Evaluating:  95%|█████████▍| 94/99 [00:29<00:01,  3.05it/s]11/28/2021 01:14:32 - INFO - __main__ -   Batch number = 95
Evaluating:  96%|█████████▌| 95/99 [00:29<00:01,  3.04it/s]11/28/2021 01:14:32 - INFO - __main__ -   Batch number = 96
Evaluating:  97%|█████████▋| 96/99 [00:30<00:00,  3.02it/s]11/28/2021 01:14:33 - INFO - __main__ -   Batch number = 97
Evaluating:  98%|█████████▊| 97/99 [00:30<00:00,  3.00it/s]11/28/2021 01:14:33 - INFO - __main__ -   Batch number = 98
Evaluating:  99%|█████████▉| 98/99 [00:30<00:00,  3.01it/s]11/28/2021 01:14:33 - INFO - __main__ -   Batch number = 99
Evaluating: 100%|██████████| 99/99 [00:31<00:00,  3.33it/s]Evaluating: 100%|██████████| 99/99 [00:31<00:00,  3.19it/s]
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:14:36 - INFO - __main__ -   ***** Evaluation result  in es *****
11/28/2021 01:14:36 - INFO - __main__ -     f1 = 0.86462100456621
11/28/2021 01:14:36 - INFO - __main__ -     loss = 0.4126229797950899
11/28/2021 01:14:36 - INFO - __main__ -     precision = 0.8744381365220497
11/28/2021 01:14:36 - INFO - __main__ -     recall = 0.8550218550941033
41.72user 14.11system 0:56.36elapsed 99%CPU (0avgtext+0avgdata 3990936maxresident)k
0inputs+960outputs (0major+1644013minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:14:39 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='es', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:14:39 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:14:39 - INFO - __main__ -   Seed = 2
11/28/2021 01:14:39 - INFO - root -   save model
11/28/2021 01:14:39 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='es', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:14:39 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:14:41 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:14:43 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bxr', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:14:43 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:14:43 - INFO - __main__ -   Seed = 1
11/28/2021 01:14:43 - INFO - root -   save model
11/28/2021 01:14:43 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bxr', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:14:43 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:14:46 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:14:48 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:14:48 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:14:48 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
11/28/2021 01:14:48 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:14:48 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:14:48 - INFO - root -   loading lang adpater bh/wiki@ukp
11/28/2021 01:14:48 - INFO - __main__ -   Adapter Languages : ['bh'], Length : 1
11/28/2021 01:14:48 - INFO - __main__ -   Adapter Names ['bh/wiki@ukp'], Length : 1
11/28/2021 01:14:48 - INFO - __main__ -   Language = bh
11/28/2021 01:14:48 - INFO - __main__ -   Adapter Name = bh/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/bh/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_bh_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/adapter_config.json
Adding adapter 'bh' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/head_config.json
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:14:52 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:14:52 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:14:52 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
11/28/2021 01:14:52 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:14:52 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:14:52 - INFO - root -   loading lang adpater bh/wiki@ukp
11/28/2021 01:14:52 - INFO - __main__ -   Adapter Languages : ['bh'], Length : 1
11/28/2021 01:14:52 - INFO - __main__ -   Adapter Names ['bh/wiki@ukp'], Length : 1
11/28/2021 01:14:52 - INFO - __main__ -   Language = bh
11/28/2021 01:14:52 - INFO - __main__ -   Adapter Name = bh/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/bh/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_bh_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/adapter_config.json
Adding adapter 'bh' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/head_config.json
11/28/2021 01:14:57 - INFO - __main__ -   Language adapter for es not found, using bh instead
11/28/2021 01:14:57 - INFO - __main__ -   Set active language adapter to bh
11/28/2021 01:14:57 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:14:57 - INFO - __main__ -   Adapter Languages = ['bh']
11/28/2021 01:14:57 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_es_bert-base-multilingual-cased_128
11/28/2021 01:14:58 - INFO - __main__ -   ***** Running evaluation  in es *****
11/28/2021 01:14:58 - INFO - __main__ -     Num examples = 3154
11/28/2021 01:14:58 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/99 [00:00<?, ?it/s]11/28/2021 01:14:58 - INFO - __main__ -   Batch number = 1
Evaluating:   1%|          | 1/99 [00:00<00:33,  2.94it/s]11/28/2021 01:14:58 - INFO - __main__ -   Batch number = 2
Evaluating:   2%|▏         | 2/99 [00:00<00:31,  3.08it/s]11/28/2021 01:14:59 - INFO - __main__ -   Batch number = 3
Evaluating:   3%|▎         | 3/99 [00:00<00:30,  3.10it/s]11/28/2021 01:14:59 - INFO - __main__ -   Batch number = 4
Evaluating:   4%|▍         | 4/99 [00:01<00:30,  3.08it/s]11/28/2021 01:14:59 - INFO - __main__ -   Batch number = 5
Evaluating:   5%|▌         | 5/99 [00:01<00:29,  3.22it/s]11/28/2021 01:15:00 - INFO - __main__ -   Language bxr, split test does not exist
11/28/2021 01:15:00 - INFO - __main__ -   Batch number = 6
Evaluating:   6%|▌         | 6/99 [00:02<00:35,  2.62it/s]11/28/2021 01:15:00 - INFO - __main__ -   Batch number = 7
Evaluating:   7%|▋         | 7/99 [00:02<00:33,  2.72it/s]11/28/2021 01:15:00 - INFO - __main__ -   Batch number = 8
Evaluating:   8%|▊         | 8/99 [00:02<00:32,  2.81it/s]11/28/2021 01:15:01 - INFO - __main__ -   Batch number = 9
14.39user 5.44system 0:19.49elapsed 101%CPU (0avgtext+0avgdata 3985488maxresident)k
0inputs+32outputs (0major+1548876minor)pagefaults 0swaps
Evaluating:   9%|▉         | 9/99 [00:03<00:31,  2.87it/s]11/28/2021 01:15:01 - INFO - __main__ -   Batch number = 10
Evaluating:  10%|█         | 10/99 [00:03<00:30,  2.92it/s]11/28/2021 01:15:01 - INFO - __main__ -   Batch number = 11
Evaluating:  11%|█         | 11/99 [00:03<00:29,  2.96it/s]11/28/2021 01:15:02 - INFO - __main__ -   Batch number = 12
Evaluating:  12%|█▏        | 12/99 [00:04<00:29,  2.98it/s]11/28/2021 01:15:02 - INFO - __main__ -   Batch number = 13
PyTorch version 1.10.0+cu102 available.
Evaluating:  13%|█▎        | 13/99 [00:04<00:28,  3.02it/s]11/28/2021 01:15:02 - INFO - __main__ -   Batch number = 14
Evaluating:  14%|█▍        | 14/99 [00:04<00:28,  3.03it/s]11/28/2021 01:15:03 - INFO - __main__ -   Batch number = 15
11/28/2021 01:15:03 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bxr', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:15:03 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:15:03 - INFO - __main__ -   Seed = 2
11/28/2021 01:15:03 - INFO - root -   save model
11/28/2021 01:15:03 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bxr', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:15:03 - INFO - __main__ -   Loading pretrained model and tokenizer
Evaluating:  15%|█▌        | 15/99 [00:05<00:27,  3.04it/s]11/28/2021 01:15:03 - INFO - __main__ -   Batch number = 16
Evaluating:  16%|█▌        | 16/99 [00:05<00:27,  3.05it/s]11/28/2021 01:15:03 - INFO - __main__ -   Batch number = 17
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  17%|█▋        | 17/99 [00:05<00:26,  3.06it/s]11/28/2021 01:15:04 - INFO - __main__ -   Batch number = 18
Evaluating:  18%|█▊        | 18/99 [00:06<00:26,  3.07it/s]11/28/2021 01:15:04 - INFO - __main__ -   Batch number = 19
Evaluating:  19%|█▉        | 19/99 [00:06<00:26,  3.07it/s]11/28/2021 01:15:04 - INFO - __main__ -   Batch number = 20
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  20%|██        | 20/99 [00:06<00:25,  3.07it/s]11/28/2021 01:15:05 - INFO - __main__ -   Batch number = 21
Evaluating:  21%|██        | 21/99 [00:07<00:28,  2.75it/s]11/28/2021 01:15:05 - INFO - __main__ -   Batch number = 22
loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
Evaluating:  22%|██▏       | 22/99 [00:07<00:27,  2.83it/s]11/28/2021 01:15:05 - INFO - __main__ -   Batch number = 23
11/28/2021 01:15:05 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
Evaluating:  23%|██▎       | 23/99 [00:07<00:26,  2.91it/s]11/28/2021 01:15:06 - INFO - __main__ -   Batch number = 24
Evaluating:  24%|██▍       | 24/99 [00:08<00:25,  2.97it/s]11/28/2021 01:15:06 - INFO - __main__ -   Batch number = 25
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  25%|██▌       | 25/99 [00:08<00:24,  3.01it/s]11/28/2021 01:15:06 - INFO - __main__ -   Batch number = 26
Evaluating:  26%|██▋       | 26/99 [00:08<00:25,  2.82it/s]11/28/2021 01:15:07 - INFO - __main__ -   Batch number = 27
Evaluating:  27%|██▋       | 27/99 [00:09<00:26,  2.72it/s]11/28/2021 01:15:07 - INFO - __main__ -   Batch number = 28
Evaluating:  28%|██▊       | 28/99 [00:09<00:28,  2.47it/s]11/28/2021 01:15:08 - INFO - __main__ -   Batch number = 29
Evaluating:  29%|██▉       | 29/99 [00:10<00:30,  2.33it/s]11/28/2021 01:15:08 - INFO - __main__ -   Batch number = 30
Evaluating:  30%|███       | 30/99 [00:10<00:30,  2.23it/s]11/28/2021 01:15:09 - INFO - __main__ -   Batch number = 31
Evaluating:  31%|███▏      | 31/99 [00:11<00:31,  2.16it/s]11/28/2021 01:15:09 - INFO - __main__ -   Batch number = 32
Evaluating:  32%|███▏      | 32/99 [00:11<00:31,  2.11it/s]11/28/2021 01:15:10 - INFO - __main__ -   Batch number = 33
Evaluating:  33%|███▎      | 33/99 [00:12<00:31,  2.08it/s]11/28/2021 01:15:10 - INFO - __main__ -   Batch number = 34
Evaluating:  34%|███▍      | 34/99 [00:12<00:31,  2.08it/s]11/28/2021 01:15:11 - INFO - __main__ -   Batch number = 35
Evaluating:  35%|███▌      | 35/99 [00:13<00:30,  2.07it/s]11/28/2021 01:15:11 - INFO - __main__ -   Batch number = 36
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:15:12 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:15:12 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:15:12 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
11/28/2021 01:15:12 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:15:12 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:15:12 - INFO - root -   loading lang adpater bh/wiki@ukp
11/28/2021 01:15:12 - INFO - __main__ -   Adapter Languages : ['bh'], Length : 1
11/28/2021 01:15:12 - INFO - __main__ -   Adapter Names ['bh/wiki@ukp'], Length : 1
11/28/2021 01:15:12 - INFO - __main__ -   Language = bh
11/28/2021 01:15:12 - INFO - __main__ -   Adapter Name = bh/wiki@ukp
Evaluating:  36%|███▋      | 36/99 [00:13<00:30,  2.07it/s]11/28/2021 01:15:12 - INFO - __main__ -   Batch number = 37
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/bh/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_bh_wiki_pfeiffer.zip.
Evaluating:  37%|███▋      | 37/99 [00:14<00:30,  2.06it/s]11/28/2021 01:15:12 - INFO - __main__ -   Batch number = 38
Evaluating:  38%|███▊      | 38/99 [00:14<00:29,  2.06it/s]11/28/2021 01:15:13 - INFO - __main__ -   Batch number = 39
Evaluating:  39%|███▉      | 39/99 [00:15<00:29,  2.05it/s]11/28/2021 01:15:13 - INFO - __main__ -   Batch number = 40
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/adapter_config.json
Adding adapter 'bh' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/head_config.json
Evaluating:  40%|████      | 40/99 [00:15<00:28,  2.05it/s]11/28/2021 01:15:14 - INFO - __main__ -   Batch number = 41
Evaluating:  41%|████▏     | 41/99 [00:16<00:28,  2.06it/s]11/28/2021 01:15:14 - INFO - __main__ -   Batch number = 42
Evaluating:  42%|████▏     | 42/99 [00:16<00:27,  2.06it/s]11/28/2021 01:15:15 - INFO - __main__ -   Batch number = 43
Evaluating:  43%|████▎     | 43/99 [00:17<00:29,  1.91it/s]11/28/2021 01:15:15 - INFO - __main__ -   Batch number = 44
Evaluating:  44%|████▍     | 44/99 [00:17<00:28,  1.96it/s]11/28/2021 01:15:16 - INFO - __main__ -   Batch number = 45
Evaluating:  45%|████▌     | 45/99 [00:18<00:27,  1.99it/s]11/28/2021 01:15:16 - INFO - __main__ -   Batch number = 46
Evaluating:  46%|████▋     | 46/99 [00:18<00:26,  2.00it/s]11/28/2021 01:15:17 - INFO - __main__ -   Batch number = 47
Evaluating:  47%|████▋     | 47/99 [00:19<00:25,  2.01it/s]11/28/2021 01:15:17 - INFO - __main__ -   Batch number = 48
Evaluating:  48%|████▊     | 48/99 [00:19<00:25,  2.02it/s]11/28/2021 01:15:18 - INFO - __main__ -   Batch number = 49
Evaluating:  49%|████▉     | 49/99 [00:20<00:24,  2.02it/s]11/28/2021 01:15:18 - INFO - __main__ -   Batch number = 50
Evaluating:  51%|█████     | 50/99 [00:20<00:24,  2.03it/s]11/28/2021 01:15:19 - INFO - __main__ -   Batch number = 51
Evaluating:  52%|█████▏    | 51/99 [00:21<00:23,  2.03it/s]11/28/2021 01:15:19 - INFO - __main__ -   Batch number = 52
Evaluating:  53%|█████▎    | 52/99 [00:21<00:23,  2.02it/s]11/28/2021 01:15:20 - INFO - __main__ -   Language bxr, split test does not exist
11/28/2021 01:15:20 - INFO - __main__ -   Batch number = 53
Evaluating:  54%|█████▎    | 53/99 [00:22<00:25,  1.80it/s]11/28/2021 01:15:20 - INFO - __main__ -   Batch number = 54
14.52user 5.69system 0:19.75elapsed 102%CPU (0avgtext+0avgdata 3987764maxresident)k
0inputs+40outputs (0major+1286662minor)pagefaults 0swaps
Evaluating:  55%|█████▍    | 54/99 [00:22<00:24,  1.87it/s]11/28/2021 01:15:21 - INFO - __main__ -   Batch number = 55
Evaluating:  56%|█████▌    | 55/99 [00:23<00:22,  1.92it/s]11/28/2021 01:15:21 - INFO - __main__ -   Batch number = 56
Evaluating:  57%|█████▋    | 56/99 [00:23<00:22,  1.95it/s]11/28/2021 01:15:22 - INFO - __main__ -   Batch number = 57
PyTorch version 1.10.0+cu102 available.
Evaluating:  58%|█████▊    | 57/99 [00:24<00:21,  1.97it/s]11/28/2021 01:15:22 - INFO - __main__ -   Batch number = 58
11/28/2021 01:15:22 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bxr', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:15:22 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:15:22 - INFO - __main__ -   Seed = 3
11/28/2021 01:15:22 - INFO - root -   save model
11/28/2021 01:15:22 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bxr', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:15:22 - INFO - __main__ -   Loading pretrained model and tokenizer
Evaluating:  59%|█████▊    | 58/99 [00:24<00:20,  1.99it/s]11/28/2021 01:15:23 - INFO - __main__ -   Batch number = 59
Evaluating:  60%|█████▉    | 59/99 [00:25<00:18,  2.13it/s]11/28/2021 01:15:23 - INFO - __main__ -   Batch number = 60
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  61%|██████    | 60/99 [00:25<00:18,  2.12it/s]11/28/2021 01:15:24 - INFO - __main__ -   Batch number = 61
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  62%|██████▏   | 61/99 [00:26<00:17,  2.15it/s]11/28/2021 01:15:24 - INFO - __main__ -   Batch number = 62
Evaluating:  63%|██████▎   | 62/99 [00:26<00:17,  2.14it/s]11/28/2021 01:15:25 - INFO - __main__ -   Batch number = 63
loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
Evaluating:  64%|██████▎   | 63/99 [00:26<00:16,  2.22it/s]11/28/2021 01:15:25 - INFO - __main__ -   Batch number = 64
11/28/2021 01:15:25 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
Evaluating:  65%|██████▍   | 64/99 [00:27<00:16,  2.12it/s]11/28/2021 01:15:25 - INFO - __main__ -   Batch number = 65
Evaluating:  66%|██████▌   | 65/99 [00:27<00:15,  2.18it/s]11/28/2021 01:15:26 - INFO - __main__ -   Batch number = 66
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  67%|██████▋   | 66/99 [00:28<00:15,  2.16it/s]11/28/2021 01:15:26 - INFO - __main__ -   Batch number = 67
Evaluating:  68%|██████▊   | 67/99 [00:28<00:15,  2.12it/s]11/28/2021 01:15:27 - INFO - __main__ -   Batch number = 68
Evaluating:  69%|██████▊   | 68/99 [00:29<00:14,  2.09it/s]11/28/2021 01:15:27 - INFO - __main__ -   Batch number = 69
Evaluating:  70%|██████▉   | 69/99 [00:29<00:14,  2.08it/s]11/28/2021 01:15:28 - INFO - __main__ -   Batch number = 70
Evaluating:  71%|███████   | 70/99 [00:30<00:14,  2.07it/s]11/28/2021 01:15:28 - INFO - __main__ -   Batch number = 71
Evaluating:  72%|███████▏  | 71/99 [00:30<00:13,  2.06it/s]11/28/2021 01:15:29 - INFO - __main__ -   Batch number = 72
Evaluating:  73%|███████▎  | 72/99 [00:31<00:13,  2.05it/s]11/28/2021 01:15:29 - INFO - __main__ -   Batch number = 73
Evaluating:  74%|███████▎  | 73/99 [00:31<00:12,  2.15it/s]11/28/2021 01:15:30 - INFO - __main__ -   Batch number = 74
Evaluating:  75%|███████▍  | 74/99 [00:32<00:12,  2.02it/s]11/28/2021 01:15:30 - INFO - __main__ -   Batch number = 75
Evaluating:  76%|███████▌  | 75/99 [00:32<00:11,  2.04it/s]11/28/2021 01:15:31 - INFO - __main__ -   Batch number = 76
Evaluating:  77%|███████▋  | 76/99 [00:33<00:11,  2.06it/s]11/28/2021 01:15:31 - INFO - __main__ -   Batch number = 77
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:15:31 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:15:31 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:15:31 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
11/28/2021 01:15:31 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:15:31 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:15:31 - INFO - root -   loading lang adpater bh/wiki@ukp
11/28/2021 01:15:31 - INFO - __main__ -   Adapter Languages : ['bh'], Length : 1
11/28/2021 01:15:31 - INFO - __main__ -   Adapter Names ['bh/wiki@ukp'], Length : 1
11/28/2021 01:15:31 - INFO - __main__ -   Language = bh
11/28/2021 01:15:31 - INFO - __main__ -   Adapter Name = bh/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/bh/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_bh_wiki_pfeiffer.zip.
Evaluating:  78%|███████▊  | 77/99 [00:33<00:10,  2.06it/s]11/28/2021 01:15:32 - INFO - __main__ -   Batch number = 78
Evaluating:  79%|███████▉  | 78/99 [00:34<00:10,  2.05it/s]11/28/2021 01:15:32 - INFO - __main__ -   Batch number = 79
Evaluating:  80%|███████▉  | 79/99 [00:34<00:09,  2.05it/s]11/28/2021 01:15:33 - INFO - __main__ -   Batch number = 80
PyTorch version 1.10.0+cu102 available.
Evaluating:  81%|████████  | 80/99 [00:35<00:09,  2.05it/s]11/28/2021 01:15:33 - INFO - __main__ -   Batch number = 81
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/adapter_config.json
Adding adapter 'bh' of type 'text_lang'.
11/28/2021 01:15:33 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bxr', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:15:33 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:15:33 - INFO - __main__ -   Seed = 1
11/28/2021 01:15:34 - INFO - root -   save model
11/28/2021 01:15:34 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bxr', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:15:34 - INFO - __main__ -   Loading pretrained model and tokenizer
Loading module weights from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/head_config.json
Evaluating:  82%|████████▏ | 81/99 [00:35<00:08,  2.05it/s]11/28/2021 01:15:34 - INFO - __main__ -   Batch number = 82
Evaluating:  83%|████████▎ | 82/99 [00:36<00:08,  2.07it/s]11/28/2021 01:15:34 - INFO - __main__ -   Batch number = 83
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  84%|████████▍ | 83/99 [00:36<00:07,  2.07it/s]11/28/2021 01:15:35 - INFO - __main__ -   Batch number = 84
Evaluating:  85%|████████▍ | 84/99 [00:37<00:07,  2.07it/s]11/28/2021 01:15:35 - INFO - __main__ -   Batch number = 85
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  86%|████████▌ | 85/99 [00:37<00:06,  2.07it/s]11/28/2021 01:15:36 - INFO - __main__ -   Batch number = 86
loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
Evaluating:  87%|████████▋ | 86/99 [00:38<00:06,  2.07it/s]11/28/2021 01:15:36 - INFO - __main__ -   Batch number = 87
11/28/2021 01:15:36 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
Evaluating:  88%|████████▊ | 87/99 [00:38<00:05,  2.07it/s]11/28/2021 01:15:37 - INFO - __main__ -   Batch number = 88
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  89%|████████▉ | 88/99 [00:39<00:05,  2.06it/s]11/28/2021 01:15:37 - INFO - __main__ -   Batch number = 89
Evaluating:  90%|████████▉ | 89/99 [00:39<00:04,  2.07it/s]11/28/2021 01:15:38 - INFO - __main__ -   Batch number = 90
Evaluating:  91%|█████████ | 90/99 [00:40<00:04,  2.06it/s]11/28/2021 01:15:38 - INFO - __main__ -   Batch number = 91
Evaluating:  92%|█████████▏| 91/99 [00:40<00:03,  2.06it/s]11/28/2021 01:15:39 - INFO - __main__ -   Batch number = 92
Evaluating:  93%|█████████▎| 92/99 [00:41<00:03,  2.05it/s]11/28/2021 01:15:39 - INFO - __main__ -   Batch number = 93
Evaluating:  94%|█████████▍| 93/99 [00:41<00:02,  2.05it/s]11/28/2021 01:15:39 - INFO - __main__ -   Batch number = 94
11/28/2021 01:15:40 - INFO - __main__ -   Language bxr, split test does not exist
Evaluating:  95%|█████████▍| 94/99 [00:41<00:02,  2.12it/s]11/28/2021 01:15:40 - INFO - __main__ -   Batch number = 95
Evaluating:  96%|█████████▌| 95/99 [00:42<00:01,  2.10it/s]11/28/2021 01:15:40 - INFO - __main__ -   Batch number = 96
15.27user 6.19system 0:20.13elapsed 106%CPU (0avgtext+0avgdata 3988420maxresident)k
0inputs+32outputs (0major+1374488minor)pagefaults 0swaps
Evaluating:  97%|█████████▋| 96/99 [00:42<00:01,  2.07it/s]11/28/2021 01:15:41 - INFO - __main__ -   Batch number = 97
Evaluating:  98%|█████████▊| 97/99 [00:43<00:00,  2.05it/s]11/28/2021 01:15:41 - INFO - __main__ -   Batch number = 98
Evaluating:  99%|█████████▉| 98/99 [00:43<00:00,  2.03it/s]11/28/2021 01:15:42 - INFO - __main__ -   Batch number = 99
Evaluating: 100%|██████████| 99/99 [00:44<00:00,  2.27it/s]Evaluating: 100%|██████████| 99/99 [00:44<00:00,  2.24it/s]Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:15:42 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:15:42 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:15:42 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
11/28/2021 01:15:42 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:15:42 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:15:42 - INFO - root -   loading lang adpater bh/wiki@ukp
11/28/2021 01:15:42 - INFO - __main__ -   Adapter Languages : ['bh'], Length : 1
11/28/2021 01:15:42 - INFO - __main__ -   Adapter Names ['bh/wiki@ukp'], Length : 1
11/28/2021 01:15:42 - INFO - __main__ -   Language = bh
11/28/2021 01:15:42 - INFO - __main__ -   Adapter Name = bh/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/bh/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_bh_wiki_pfeiffer.zip.

/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:15:44 - INFO - __main__ -   ***** Evaluation result  in es *****
11/28/2021 01:15:44 - INFO - __main__ -     f1 = 0.8576815931292046
11/28/2021 01:15:44 - INFO - __main__ -     loss = 0.44687959401294436
11/28/2021 01:15:44 - INFO - __main__ -     precision = 0.8665789894172741
11/28/2021 01:15:44 - INFO - __main__ -     recall = 0.8489650438908088
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/adapter_config.json
Adding adapter 'bh' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/head_config.json
50.61user 17.26system 1:08.54elapsed 99%CPU (0avgtext+0avgdata 3986100maxresident)k
8inputs+968outputs (0major+1778003minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:15:47 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='es', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:15:47 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:15:47 - INFO - __main__ -   Seed = 3
11/28/2021 01:15:47 - INFO - root -   save model
11/28/2021 01:15:47 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='es', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:15:47 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:15:50 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
11/28/2021 01:15:51 - INFO - __main__ -   Language bxr, split test does not exist
15.17user 7.11system 0:20.24elapsed 110%CPU (0avgtext+0avgdata 3985240maxresident)k
0inputs+48outputs (0major+1321647minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:15:54 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bxr', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:15:54 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:15:54 - INFO - __main__ -   Seed = 2
11/28/2021 01:15:54 - INFO - root -   save model
11/28/2021 01:15:54 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bxr', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:15:54 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:15:56 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:15:56 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:15:56 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
11/28/2021 01:15:56 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:15:56 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:15:56 - INFO - root -   loading lang adpater bh/wiki@ukp
11/28/2021 01:15:56 - INFO - __main__ -   Adapter Languages : ['bh'], Length : 1
11/28/2021 01:15:56 - INFO - __main__ -   Adapter Names ['bh/wiki@ukp'], Length : 1
11/28/2021 01:15:56 - INFO - __main__ -   Language = bh
11/28/2021 01:15:56 - INFO - __main__ -   Adapter Name = bh/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/bh/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_bh_wiki_pfeiffer.zip.
loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:15:56 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/adapter_config.json
Adding adapter 'bh' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/head_config.json
Traceback (most recent call last):
  File "third_party/my_run_tag.py", line 1112, in <module>
    main()
  File "third_party/my_run_tag.py", line 1039, in main
    model, tokenizer, lang2id = load_model(args, num_labels)
  File "third_party/my_run_tag.py", line 796, in load_model
    cache_dir=args.cache_dir,
  File "/home/abhijeet/rohan/cloud-emea/src/transformers/modeling_auto.py", line 1590, in from_pretrained
    pretrained_model_name_or_path, *model_args, config=config, **kwargs
  File "/home/abhijeet/rohan/cloud-emea/src/transformers/modeling_utils.py", line 949, in from_pretrained
    model = cls(config, *model_args, **model_kwargs)
  File "/home/abhijeet/rohan/cloud-emea/src/transformers/modeling_bert.py", line 1710, in __init__
    self.bert = BertModel(config, add_pooling_layer=False)
  File "/home/abhijeet/rohan/cloud-emea/src/transformers/modeling_bert.py", line 840, in __init__
    self.init_weights()
  File "/home/abhijeet/rohan/cloud-emea/src/transformers/modeling_utils.py", line 673, in init_weights
    self.apply(self._init_weights)
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/torch/nn/modules/module.py", line 659, in apply
    module.apply(fn)
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/torch/nn/modules/module.py", line 659, in apply
    module.apply(fn)
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/torch/nn/modules/module.py", line 660, in apply
    fn(self)
  File "/home/abhijeet/rohan/cloud-emea/src/transformers/modeling_bert.py", line 701, in _init_weights
    module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
KeyboardInterrupt
Command terminated by signal 2
4.98user 2.79system 0:08.04elapsed 96%CPU (0avgtext+0avgdata 1052700maxresident)k
0inputs+32outputs (0major+281970minor)pagefaults 0swaps
Traceback (most recent call last):
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/numpy/core/__init__.py", line 22, in <module>
    from . import multiarray
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/numpy/core/multiarray.py", line 12, in <module>
    from . import overrides
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/numpy/core/overrides.py", line 7, in <module>
    from numpy.core._multiarray_umath import (
ImportError: PyCapsule_Import could not import module "datetime"

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "third_party/my_run_tag.py", line 30, in <module>
    import numpy as np
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/numpy/__init__.py", line 150, in <module>
    from . import core
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/numpy/core/__init__.py", line 48, in <module>
    raise ImportError(msg)
ImportError: 

IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!

Importing the numpy C-extensions failed. This error can happen for
many reasons, often due to issues with your setup or how NumPy was
installed.

We have compiled some common reasons and troubleshooting tips at:

    https://numpy.org/devdocs/user/troubleshooting-importerror.html

Please note and check the following:

  * The Python version is: Python3.7 from "/home/abhijeet/rohan/venvs/cloud-emea/bin/python"
  * The NumPy version is: "1.21.4"

and make sure that they are the versions you expect.
Please carefully study the documentation linked above for further help.

Original error was: PyCapsule_Import could not import module "datetime"

Command exited with non-zero status 1
0.23user 0.32system 0:00.08elapsed 629%CPU (0avgtext+0avgdata 18764maxresident)k
0inputs+0outputs (0major+2929minor)pagefaults 0swaps
11/28/2021 01:16:06 - INFO - __main__ -   Language adapter for es not found, using bh instead
11/28/2021 01:16:06 - INFO - __main__ -   Set active language adapter to bh
11/28/2021 01:16:06 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:16:06 - INFO - __main__ -   Adapter Languages = ['bh']
11/28/2021 01:16:06 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_es_bert-base-multilingual-cased_128
11/28/2021 01:16:06 - INFO - __main__ -   ***** Running evaluation  in es *****
11/28/2021 01:16:06 - INFO - __main__ -     Num examples = 3154
11/28/2021 01:16:06 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/99 [00:00<?, ?it/s]11/28/2021 01:16:06 - INFO - __main__ -   Batch number = 1
Evaluating:   1%|          | 1/99 [00:00<00:16,  5.86it/s]11/28/2021 01:16:06 - INFO - __main__ -   Batch number = 2
Evaluating:   2%|▏         | 2/99 [00:00<00:16,  6.03it/s]11/28/2021 01:16:07 - INFO - __main__ -   Batch number = 3
Evaluating:   3%|▎         | 3/99 [00:00<00:15,  6.12it/s]11/28/2021 01:16:07 - INFO - __main__ -   Batch number = 4
Evaluating:   4%|▍         | 4/99 [00:00<00:15,  6.13it/s]11/28/2021 01:16:07 - INFO - __main__ -   Batch number = 5
Evaluating:   5%|▌         | 5/99 [00:00<00:15,  6.16it/s]11/28/2021 01:16:07 - INFO - __main__ -   Batch number = 6
Evaluating:   6%|▌         | 6/99 [00:00<00:15,  6.09it/s]11/28/2021 01:16:07 - INFO - __main__ -   Batch number = 7
Evaluating:   7%|▋         | 7/99 [00:01<00:15,  6.06it/s]11/28/2021 01:16:07 - INFO - __main__ -   Batch number = 8
Evaluating:   8%|▊         | 8/99 [00:01<00:15,  6.06it/s]11/28/2021 01:16:08 - INFO - __main__ -   Batch number = 9
Evaluating:   9%|▉         | 9/99 [00:01<00:14,  6.05it/s]11/28/2021 01:16:08 - INFO - __main__ -   Batch number = 10
Evaluating:  10%|█         | 10/99 [00:01<00:14,  6.08it/s]11/28/2021 01:16:08 - INFO - __main__ -   Batch number = 11
Evaluating:  11%|█         | 11/99 [00:01<00:14,  6.15it/s]11/28/2021 01:16:08 - INFO - __main__ -   Batch number = 12
Evaluating:  12%|█▏        | 12/99 [00:01<00:14,  6.09it/s]11/28/2021 01:16:08 - INFO - __main__ -   Batch number = 13
Evaluating:  13%|█▎        | 13/99 [00:02<00:14,  6.07it/s]11/28/2021 01:16:08 - INFO - __main__ -   Batch number = 14
Evaluating:  14%|█▍        | 14/99 [00:02<00:13,  6.12it/s]11/28/2021 01:16:09 - INFO - __main__ -   Batch number = 15
Evaluating:  15%|█▌        | 15/99 [00:02<00:13,  6.17it/s]11/28/2021 01:16:09 - INFO - __main__ -   Batch number = 16
Evaluating:  16%|█▌        | 16/99 [00:02<00:13,  6.13it/s]11/28/2021 01:16:09 - INFO - __main__ -   Batch number = 17
Evaluating:  17%|█▋        | 17/99 [00:02<00:13,  6.12it/s]11/28/2021 01:16:09 - INFO - __main__ -   Batch number = 18
Evaluating:  18%|█▊        | 18/99 [00:02<00:13,  6.17it/s]11/28/2021 01:16:09 - INFO - __main__ -   Batch number = 19
Evaluating:  19%|█▉        | 19/99 [00:03<00:14,  5.70it/s]11/28/2021 01:16:09 - INFO - __main__ -   Batch number = 20
Evaluating:  20%|██        | 20/99 [00:03<00:16,  4.79it/s]11/28/2021 01:16:10 - INFO - __main__ -   Batch number = 21
Evaluating:  21%|██        | 21/99 [00:03<00:19,  4.08it/s]11/28/2021 01:16:10 - INFO - __main__ -   Batch number = 22
Evaluating:  22%|██▏       | 22/99 [00:04<00:20,  3.68it/s]11/28/2021 01:16:10 - INFO - __main__ -   Batch number = 23
Evaluating:  23%|██▎       | 23/99 [00:04<00:21,  3.47it/s]11/28/2021 01:16:11 - INFO - __main__ -   Batch number = 24
Evaluating:  24%|██▍       | 24/99 [00:04<00:22,  3.32it/s]11/28/2021 01:16:11 - INFO - __main__ -   Batch number = 25
Evaluating:  25%|██▌       | 25/99 [00:05<00:23,  3.21it/s]11/28/2021 01:16:11 - INFO - __main__ -   Batch number = 26
Evaluating:  26%|██▋       | 26/99 [00:05<00:23,  3.15it/s]11/28/2021 01:16:12 - INFO - __main__ -   Batch number = 27
Evaluating:  27%|██▋       | 27/99 [00:05<00:23,  3.11it/s]11/28/2021 01:16:12 - INFO - __main__ -   Batch number = 28
Evaluating:  28%|██▊       | 28/99 [00:06<00:23,  3.08it/s]11/28/2021 01:16:12 - INFO - __main__ -   Batch number = 29
Evaluating:  29%|██▉       | 29/99 [00:06<00:22,  3.06it/s]11/28/2021 01:16:13 - INFO - __main__ -   Batch number = 30
Evaluating:  30%|███       | 30/99 [00:06<00:22,  3.04it/s]11/28/2021 01:16:13 - INFO - __main__ -   Batch number = 31
Evaluating:  31%|███▏      | 31/99 [00:07<00:22,  3.01it/s]11/28/2021 01:16:13 - INFO - __main__ -   Batch number = 32
Evaluating:  32%|███▏      | 32/99 [00:07<00:22,  3.01it/s]11/28/2021 01:16:14 - INFO - __main__ -   Batch number = 33
Evaluating:  33%|███▎      | 33/99 [00:07<00:21,  3.01it/s]11/28/2021 01:16:14 - INFO - __main__ -   Batch number = 34
Evaluating:  34%|███▍      | 34/99 [00:08<00:21,  3.02it/s]11/28/2021 01:16:14 - INFO - __main__ -   Batch number = 35
Evaluating:  35%|███▌      | 35/99 [00:08<00:21,  3.04it/s]11/28/2021 01:16:15 - INFO - __main__ -   Batch number = 36
Evaluating:  36%|███▋      | 36/99 [00:08<00:21,  2.87it/s]11/28/2021 01:16:15 - INFO - __main__ -   Batch number = 37
Evaluating:  37%|███▋      | 37/99 [00:09<00:21,  2.95it/s]11/28/2021 01:16:15 - INFO - __main__ -   Batch number = 38
Evaluating:  38%|███▊      | 38/99 [00:09<00:20,  3.01it/s]11/28/2021 01:16:16 - INFO - __main__ -   Batch number = 39
Evaluating:  39%|███▉      | 39/99 [00:09<00:19,  3.02it/s]11/28/2021 01:16:16 - INFO - __main__ -   Batch number = 40
Evaluating:  40%|████      | 40/99 [00:10<00:19,  3.02it/s]11/28/2021 01:16:16 - INFO - __main__ -   Batch number = 41
Evaluating:  41%|████▏     | 41/99 [00:10<00:19,  3.01it/s]11/28/2021 01:16:17 - INFO - __main__ -   Batch number = 42
Evaluating:  42%|████▏     | 42/99 [00:10<00:18,  3.01it/s]11/28/2021 01:16:17 - INFO - __main__ -   Batch number = 43
Evaluating:  43%|████▎     | 43/99 [00:11<00:18,  3.03it/s]11/28/2021 01:16:17 - INFO - __main__ -   Batch number = 44
Evaluating:  44%|████▍     | 44/99 [00:11<00:18,  3.02it/s]11/28/2021 01:16:18 - INFO - __main__ -   Batch number = 45
Evaluating:  45%|████▌     | 45/99 [00:11<00:17,  3.02it/s]11/28/2021 01:16:18 - INFO - __main__ -   Batch number = 46
Evaluating:  46%|████▋     | 46/99 [00:12<00:17,  3.03it/s]11/28/2021 01:16:18 - INFO - __main__ -   Batch number = 47
Evaluating:  47%|████▋     | 47/99 [00:12<00:17,  3.02it/s]11/28/2021 01:16:19 - INFO - __main__ -   Batch number = 48
Evaluating:  48%|████▊     | 48/99 [00:12<00:16,  3.02it/s]11/28/2021 01:16:19 - INFO - __main__ -   Batch number = 49
Evaluating:  49%|████▉     | 49/99 [00:13<00:16,  3.02it/s]11/28/2021 01:16:19 - INFO - __main__ -   Batch number = 50
Evaluating:  51%|█████     | 50/99 [00:13<00:16,  3.03it/s]11/28/2021 01:16:20 - INFO - __main__ -   Batch number = 51
Evaluating:  52%|█████▏    | 51/99 [00:13<00:13,  3.44it/s]11/28/2021 01:16:20 - INFO - __main__ -   Batch number = 52
Evaluating:  53%|█████▎    | 52/99 [00:13<00:14,  3.32it/s]11/28/2021 01:16:20 - INFO - __main__ -   Batch number = 53
Evaluating:  54%|█████▎    | 53/99 [00:14<00:14,  3.22it/s]11/28/2021 01:16:21 - INFO - __main__ -   Batch number = 54
Evaluating:  55%|█████▍    | 54/99 [00:14<00:14,  3.18it/s]11/28/2021 01:16:21 - INFO - __main__ -   Batch number = 55
Evaluating:  56%|█████▌    | 55/99 [00:14<00:13,  3.15it/s]11/28/2021 01:16:21 - INFO - __main__ -   Batch number = 56
Evaluating:  57%|█████▋    | 56/99 [00:15<00:13,  3.10it/s]11/28/2021 01:16:22 - INFO - __main__ -   Batch number = 57
Evaluating:  58%|█████▊    | 57/99 [00:15<00:13,  3.08it/s]11/28/2021 01:16:22 - INFO - __main__ -   Batch number = 58
Evaluating:  59%|█████▊    | 58/99 [00:15<00:13,  3.06it/s]11/28/2021 01:16:22 - INFO - __main__ -   Batch number = 59
Evaluating:  60%|█████▉    | 59/99 [00:16<00:13,  3.04it/s]11/28/2021 01:16:23 - INFO - __main__ -   Batch number = 60
Evaluating:  61%|██████    | 60/99 [00:16<00:12,  3.03it/s]11/28/2021 01:16:23 - INFO - __main__ -   Batch number = 61
Evaluating:  62%|██████▏   | 61/99 [00:16<00:12,  3.03it/s]11/28/2021 01:16:23 - INFO - __main__ -   Batch number = 62
Evaluating:  63%|██████▎   | 62/99 [00:17<00:12,  3.02it/s]11/28/2021 01:16:24 - INFO - __main__ -   Batch number = 63
Evaluating:  64%|██████▎   | 63/99 [00:17<00:11,  3.02it/s]11/28/2021 01:16:24 - INFO - __main__ -   Batch number = 64
Evaluating:  65%|██████▍   | 64/99 [00:17<00:11,  3.01it/s]11/28/2021 01:16:24 - INFO - __main__ -   Batch number = 65
Evaluating:  66%|██████▌   | 65/99 [00:18<00:11,  3.05it/s]11/28/2021 01:16:25 - INFO - __main__ -   Batch number = 66
Evaluating:  67%|██████▋   | 66/99 [00:18<00:10,  3.03it/s]11/28/2021 01:16:25 - INFO - __main__ -   Batch number = 67
Evaluating:  68%|██████▊   | 67/99 [00:18<00:10,  3.01it/s]11/28/2021 01:16:25 - INFO - __main__ -   Batch number = 68
Evaluating:  69%|██████▊   | 68/99 [00:19<00:10,  3.00it/s]11/28/2021 01:16:26 - INFO - __main__ -   Batch number = 69
Evaluating:  70%|██████▉   | 69/99 [00:19<00:10,  2.96it/s]11/28/2021 01:16:26 - INFO - __main__ -   Batch number = 70
Evaluating:  71%|███████   | 70/99 [00:19<00:09,  2.97it/s]11/28/2021 01:16:26 - INFO - __main__ -   Batch number = 71
Evaluating:  72%|███████▏  | 71/99 [00:20<00:09,  2.98it/s]11/28/2021 01:16:27 - INFO - __main__ -   Batch number = 72
Evaluating:  73%|███████▎  | 72/99 [00:20<00:09,  2.98it/s]11/28/2021 01:16:27 - INFO - __main__ -   Batch number = 73
Evaluating:  74%|███████▎  | 73/99 [00:20<00:08,  2.98it/s]11/28/2021 01:16:27 - INFO - __main__ -   Batch number = 74
Evaluating:  75%|███████▍  | 74/99 [00:21<00:08,  3.00it/s]11/28/2021 01:16:28 - INFO - __main__ -   Batch number = 75
Evaluating:  76%|███████▌  | 75/99 [00:21<00:07,  3.01it/s]11/28/2021 01:16:28 - INFO - __main__ -   Batch number = 76
Evaluating:  77%|███████▋  | 76/99 [00:21<00:07,  3.01it/s]11/28/2021 01:16:28 - INFO - __main__ -   Batch number = 77
Evaluating:  78%|███████▊  | 77/99 [00:22<00:07,  3.00it/s]11/28/2021 01:16:29 - INFO - __main__ -   Batch number = 78
Evaluating:  79%|███████▉  | 78/99 [00:22<00:07,  3.00it/s]11/28/2021 01:16:29 - INFO - __main__ -   Batch number = 79
Evaluating:  80%|███████▉  | 79/99 [00:22<00:06,  2.99it/s]11/28/2021 01:16:29 - INFO - __main__ -   Batch number = 80
Evaluating:  81%|████████  | 80/99 [00:23<00:06,  3.00it/s]11/28/2021 01:16:30 - INFO - __main__ -   Batch number = 81
Evaluating:  82%|████████▏ | 81/99 [00:23<00:05,  3.01it/s]11/28/2021 01:16:30 - INFO - __main__ -   Batch number = 82
Evaluating:  83%|████████▎ | 82/99 [00:23<00:05,  3.02it/s]11/28/2021 01:16:30 - INFO - __main__ -   Batch number = 83
Evaluating:  84%|████████▍ | 83/99 [00:24<00:05,  3.01it/s]11/28/2021 01:16:31 - INFO - __main__ -   Batch number = 84
Evaluating:  85%|████████▍ | 84/99 [00:24<00:04,  3.01it/s]11/28/2021 01:16:31 - INFO - __main__ -   Batch number = 85
Evaluating:  86%|████████▌ | 85/99 [00:24<00:04,  3.02it/s]11/28/2021 01:16:31 - INFO - __main__ -   Batch number = 86
Evaluating:  87%|████████▋ | 86/99 [00:25<00:04,  3.00it/s]11/28/2021 01:16:32 - INFO - __main__ -   Batch number = 87
Evaluating:  88%|████████▊ | 87/99 [00:25<00:04,  2.98it/s]11/28/2021 01:16:32 - INFO - __main__ -   Batch number = 88
Evaluating:  89%|████████▉ | 88/99 [00:25<00:03,  2.99it/s]11/28/2021 01:16:32 - INFO - __main__ -   Batch number = 89
Evaluating:  90%|████████▉ | 89/99 [00:26<00:03,  3.02it/s]11/28/2021 01:16:33 - INFO - __main__ -   Batch number = 90
Evaluating:  91%|█████████ | 90/99 [00:26<00:02,  3.01it/s]11/28/2021 01:16:33 - INFO - __main__ -   Batch number = 91
Evaluating:  92%|█████████▏| 91/99 [00:26<00:02,  3.01it/s]11/28/2021 01:16:33 - INFO - __main__ -   Batch number = 92
Evaluating:  93%|█████████▎| 92/99 [00:27<00:02,  2.97it/s]11/28/2021 01:16:34 - INFO - __main__ -   Batch number = 93
Evaluating:  94%|█████████▍| 93/99 [00:27<00:02,  2.60it/s]11/28/2021 01:16:34 - INFO - __main__ -   Batch number = 94
Evaluating:  95%|█████████▍| 94/99 [00:28<00:02,  2.39it/s]11/28/2021 01:16:35 - INFO - __main__ -   Batch number = 95
Evaluating:  96%|█████████▌| 95/99 [00:28<00:01,  2.26it/s]11/28/2021 01:16:35 - INFO - __main__ -   Batch number = 96
Evaluating:  97%|█████████▋| 96/99 [00:29<00:01,  2.17it/s]11/28/2021 01:16:36 - INFO - __main__ -   Batch number = 97
Evaluating:  98%|█████████▊| 97/99 [00:29<00:00,  2.23it/s]11/28/2021 01:16:36 - INFO - __main__ -   Batch number = 98
Evaluating:  99%|█████████▉| 98/99 [00:30<00:00,  2.16it/s]11/28/2021 01:16:36 - INFO - __main__ -   Batch number = 99
Evaluating: 100%|██████████| 99/99 [00:30<00:00,  2.34it/s]Evaluating: 100%|██████████| 99/99 [00:30<00:00,  3.25it/s]
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:16:39 - INFO - __main__ -   ***** Evaluation result  in es *****
11/28/2021 01:16:39 - INFO - __main__ -     f1 = 0.8723507680342212
11/28/2021 01:16:39 - INFO - __main__ -     loss = 0.3884523372457485
11/28/2021 01:16:39 - INFO - __main__ -     precision = 0.8804719792957107
11/28/2021 01:16:39 - INFO - __main__ -     recall = 0.8643780028176815
41.08user 13.85system 0:54.57elapsed 100%CPU (0avgtext+0avgdata 4003952maxresident)k
0inputs+952outputs (0major+1408432minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:28:40 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='is', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:28:40 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:28:40 - INFO - __main__ -   Seed = 1
11/28/2021 01:28:40 - INFO - root -   save model
11/28/2021 01:28:40 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='is', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:28:40 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:28:43 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:28:49 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:28:49 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:28:49 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
11/28/2021 01:28:49 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:28:49 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:28:49 - INFO - root -   loading lang adpater bh/wiki@ukp
11/28/2021 01:28:49 - INFO - __main__ -   Adapter Languages : ['bh'], Length : 1
11/28/2021 01:28:49 - INFO - __main__ -   Adapter Names ['bh/wiki@ukp'], Length : 1
11/28/2021 01:28:49 - INFO - __main__ -   Language = bh
11/28/2021 01:28:49 - INFO - __main__ -   Adapter Name = bh/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/bh/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_bh_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/adapter_config.json
Adding adapter 'bh' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/head_config.json
11/28/2021 01:29:03 - INFO - __main__ -   Language adapter for is not found, using bh instead
11/28/2021 01:29:03 - INFO - __main__ -   Set active language adapter to bh
11/28/2021 01:29:03 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:29:03 - INFO - __main__ -   Adapter Languages = ['bh']
11/28/2021 01:29:03 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_is_bert-base-multilingual-cased_128
11/28/2021 01:29:04 - INFO - __main__ -   ***** Running evaluation  in is *****
11/28/2021 01:29:04 - INFO - __main__ -     Num examples = 6401
11/28/2021 01:29:04 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/201 [00:00<?, ?it/s]11/28/2021 01:29:04 - INFO - __main__ -   Batch number = 1
Evaluating:   0%|          | 1/201 [00:00<01:41,  1.96it/s]11/28/2021 01:29:05 - INFO - __main__ -   Batch number = 2
Evaluating:   1%|          | 2/201 [00:01<01:40,  1.99it/s]11/28/2021 01:29:05 - INFO - __main__ -   Batch number = 3
Evaluating:   1%|▏         | 3/201 [00:01<01:38,  2.00it/s]11/28/2021 01:29:06 - INFO - __main__ -   Batch number = 4
Evaluating:   2%|▏         | 4/201 [00:01<01:37,  2.01it/s]11/28/2021 01:29:06 - INFO - __main__ -   Batch number = 5
Evaluating:   2%|▏         | 5/201 [00:02<01:37,  2.02it/s]11/28/2021 01:29:07 - INFO - __main__ -   Batch number = 6
Evaluating:   3%|▎         | 6/201 [00:02<01:36,  2.02it/s]11/28/2021 01:29:07 - INFO - __main__ -   Batch number = 7
Evaluating:   3%|▎         | 7/201 [00:03<01:36,  2.02it/s]11/28/2021 01:29:08 - INFO - __main__ -   Batch number = 8
Evaluating:   4%|▍         | 8/201 [00:03<01:35,  2.02it/s]11/28/2021 01:29:08 - INFO - __main__ -   Batch number = 9
Evaluating:   4%|▍         | 9/201 [00:04<01:33,  2.06it/s]11/28/2021 01:29:09 - INFO - __main__ -   Batch number = 10
Evaluating:   5%|▍         | 10/201 [00:04<01:23,  2.29it/s]11/28/2021 01:29:09 - INFO - __main__ -   Batch number = 11
Evaluating:   5%|▌         | 11/201 [00:05<01:16,  2.47it/s]11/28/2021 01:29:09 - INFO - __main__ -   Batch number = 12
Evaluating:   6%|▌         | 12/201 [00:05<01:15,  2.52it/s]11/28/2021 01:29:10 - INFO - __main__ -   Batch number = 13
Evaluating:   6%|▋         | 13/201 [00:05<01:10,  2.66it/s]11/28/2021 01:29:10 - INFO - __main__ -   Batch number = 14
Evaluating:   7%|▋         | 14/201 [00:06<01:07,  2.77it/s]11/28/2021 01:29:10 - INFO - __main__ -   Batch number = 15
Evaluating:   7%|▋         | 15/201 [00:06<01:04,  2.89it/s]11/28/2021 01:29:11 - INFO - __main__ -   Batch number = 16
Evaluating:   8%|▊         | 16/201 [00:06<01:03,  2.94it/s]11/28/2021 01:29:11 - INFO - __main__ -   Batch number = 17
Evaluating:   8%|▊         | 17/201 [00:07<01:01,  2.99it/s]11/28/2021 01:29:11 - INFO - __main__ -   Batch number = 18
Evaluating:   9%|▉         | 18/201 [00:07<01:00,  3.01it/s]11/28/2021 01:29:12 - INFO - __main__ -   Batch number = 19
Evaluating:   9%|▉         | 19/201 [00:07<00:59,  3.04it/s]11/28/2021 01:29:12 - INFO - __main__ -   Batch number = 20
Evaluating:  10%|▉         | 20/201 [00:08<00:59,  3.04it/s]11/28/2021 01:29:12 - INFO - __main__ -   Batch number = 21
Evaluating:  10%|█         | 21/201 [00:08<00:59,  3.04it/s]11/28/2021 01:29:13 - INFO - __main__ -   Batch number = 22
Evaluating:  11%|█         | 22/201 [00:08<00:58,  3.05it/s]11/28/2021 01:29:13 - INFO - __main__ -   Batch number = 23
Evaluating:  11%|█▏        | 23/201 [00:09<00:58,  3.05it/s]11/28/2021 01:29:13 - INFO - __main__ -   Batch number = 24
Evaluating:  12%|█▏        | 24/201 [00:09<00:57,  3.07it/s]11/28/2021 01:29:13 - INFO - __main__ -   Batch number = 25
Evaluating:  12%|█▏        | 25/201 [00:09<00:56,  3.12it/s]11/28/2021 01:29:14 - INFO - __main__ -   Batch number = 26
Evaluating:  13%|█▎        | 26/201 [00:10<00:56,  3.09it/s]11/28/2021 01:29:14 - INFO - __main__ -   Batch number = 27
Evaluating:  13%|█▎        | 27/201 [00:10<00:52,  3.31it/s]11/28/2021 01:29:14 - INFO - __main__ -   Batch number = 28
Evaluating:  14%|█▍        | 28/201 [00:10<00:55,  3.11it/s]11/28/2021 01:29:15 - INFO - __main__ -   Batch number = 29
Evaluating:  14%|█▍        | 29/201 [00:10<00:55,  3.12it/s]11/28/2021 01:29:15 - INFO - __main__ -   Batch number = 30
Evaluating:  15%|█▍        | 30/201 [00:11<00:55,  3.09it/s]11/28/2021 01:29:15 - INFO - __main__ -   Batch number = 31
Evaluating:  15%|█▌        | 31/201 [00:11<00:55,  3.08it/s]11/28/2021 01:29:16 - INFO - __main__ -   Batch number = 32
Evaluating:  16%|█▌        | 32/201 [00:11<00:55,  3.07it/s]11/28/2021 01:29:16 - INFO - __main__ -   Batch number = 33
Evaluating:  16%|█▋        | 33/201 [00:12<00:54,  3.06it/s]11/28/2021 01:29:16 - INFO - __main__ -   Batch number = 34
Evaluating:  17%|█▋        | 34/201 [00:12<00:54,  3.05it/s]11/28/2021 01:29:17 - INFO - __main__ -   Batch number = 35
Evaluating:  17%|█▋        | 35/201 [00:12<00:54,  3.04it/s]11/28/2021 01:29:17 - INFO - __main__ -   Batch number = 36
Evaluating:  18%|█▊        | 36/201 [00:13<00:54,  3.03it/s]11/28/2021 01:29:17 - INFO - __main__ -   Batch number = 37
Evaluating:  18%|█▊        | 37/201 [00:13<00:54,  3.00it/s]11/28/2021 01:29:18 - INFO - __main__ -   Batch number = 38
Evaluating:  19%|█▉        | 38/201 [00:13<00:54,  3.00it/s]11/28/2021 01:29:18 - INFO - __main__ -   Batch number = 39
Evaluating:  19%|█▉        | 39/201 [00:14<00:54,  3.00it/s]11/28/2021 01:29:18 - INFO - __main__ -   Batch number = 40
Evaluating:  20%|█▉        | 40/201 [00:14<00:53,  3.00it/s]11/28/2021 01:29:19 - INFO - __main__ -   Batch number = 41
Evaluating:  20%|██        | 41/201 [00:14<00:53,  3.00it/s]11/28/2021 01:29:19 - INFO - __main__ -   Batch number = 42
Evaluating:  21%|██        | 42/201 [00:15<00:53,  3.00it/s]11/28/2021 01:29:19 - INFO - __main__ -   Batch number = 43
Evaluating:  21%|██▏       | 43/201 [00:15<00:52,  2.99it/s]11/28/2021 01:29:20 - INFO - __main__ -   Batch number = 44
Evaluating:  22%|██▏       | 44/201 [00:15<00:52,  2.99it/s]11/28/2021 01:29:20 - INFO - __main__ -   Batch number = 45
Evaluating:  22%|██▏       | 45/201 [00:16<00:52,  2.99it/s]11/28/2021 01:29:20 - INFO - __main__ -   Batch number = 46
Evaluating:  23%|██▎       | 46/201 [00:16<00:51,  3.00it/s]11/28/2021 01:29:21 - INFO - __main__ -   Batch number = 47
Evaluating:  23%|██▎       | 47/201 [00:16<00:51,  3.01it/s]11/28/2021 01:29:21 - INFO - __main__ -   Batch number = 48
Evaluating:  24%|██▍       | 48/201 [00:17<00:51,  2.99it/s]11/28/2021 01:29:21 - INFO - __main__ -   Batch number = 49
Evaluating:  24%|██▍       | 49/201 [00:17<00:45,  3.38it/s]11/28/2021 01:29:22 - INFO - __main__ -   Batch number = 50
Evaluating:  25%|██▍       | 50/201 [00:17<00:43,  3.49it/s]11/28/2021 01:29:22 - INFO - __main__ -   Batch number = 51
Evaluating:  25%|██▌       | 51/201 [00:18<00:47,  3.16it/s]11/28/2021 01:29:22 - INFO - __main__ -   Batch number = 52
Evaluating:  26%|██▌       | 52/201 [00:18<00:55,  2.67it/s]11/28/2021 01:29:23 - INFO - __main__ -   Batch number = 53
Evaluating:  26%|██▋       | 53/201 [00:19<01:00,  2.46it/s]11/28/2021 01:29:23 - INFO - __main__ -   Batch number = 54
Evaluating:  27%|██▋       | 54/201 [00:19<01:04,  2.28it/s]11/28/2021 01:29:24 - INFO - __main__ -   Batch number = 55
Evaluating:  27%|██▋       | 55/201 [00:20<01:06,  2.18it/s]11/28/2021 01:29:24 - INFO - __main__ -   Batch number = 56
Evaluating:  28%|██▊       | 56/201 [00:20<01:08,  2.10it/s]11/28/2021 01:29:25 - INFO - __main__ -   Batch number = 57
Evaluating:  28%|██▊       | 57/201 [00:21<01:10,  2.06it/s]11/28/2021 01:29:25 - INFO - __main__ -   Batch number = 58
Evaluating:  29%|██▉       | 58/201 [00:21<01:10,  2.03it/s]11/28/2021 01:29:26 - INFO - __main__ -   Batch number = 59
Evaluating:  29%|██▉       | 59/201 [00:22<01:09,  2.05it/s]11/28/2021 01:29:26 - INFO - __main__ -   Batch number = 60
Evaluating:  30%|██▉       | 60/201 [00:22<01:09,  2.02it/s]11/28/2021 01:29:27 - INFO - __main__ -   Batch number = 61
Evaluating:  30%|███       | 61/201 [00:23<01:09,  2.01it/s]11/28/2021 01:29:27 - INFO - __main__ -   Batch number = 62
Evaluating:  31%|███       | 62/201 [00:23<01:09,  2.01it/s]11/28/2021 01:29:28 - INFO - __main__ -   Batch number = 63
Evaluating:  31%|███▏      | 63/201 [00:24<01:06,  2.09it/s]11/28/2021 01:29:28 - INFO - __main__ -   Batch number = 64
Evaluating:  32%|███▏      | 64/201 [00:24<01:06,  2.06it/s]11/28/2021 01:29:29 - INFO - __main__ -   Batch number = 65
Evaluating:  32%|███▏      | 65/201 [00:25<01:06,  2.03it/s]11/28/2021 01:29:29 - INFO - __main__ -   Batch number = 66
Evaluating:  33%|███▎      | 66/201 [00:25<01:14,  1.81it/s]11/28/2021 01:29:30 - INFO - __main__ -   Batch number = 67
Evaluating:  33%|███▎      | 67/201 [00:26<01:12,  1.85it/s]11/28/2021 01:29:30 - INFO - __main__ -   Batch number = 68
Evaluating:  34%|███▍      | 68/201 [00:26<01:10,  1.88it/s]11/28/2021 01:29:31 - INFO - __main__ -   Batch number = 69
Evaluating:  34%|███▍      | 69/201 [00:27<01:02,  2.10it/s]11/28/2021 01:29:31 - INFO - __main__ -   Batch number = 70
Evaluating:  35%|███▍      | 70/201 [00:27<01:03,  2.06it/s]11/28/2021 01:29:32 - INFO - __main__ -   Batch number = 71
Evaluating:  35%|███▌      | 71/201 [00:28<01:03,  2.04it/s]11/28/2021 01:29:32 - INFO - __main__ -   Batch number = 72
Evaluating:  36%|███▌      | 72/201 [00:28<01:03,  2.04it/s]11/28/2021 01:29:33 - INFO - __main__ -   Batch number = 73
Evaluating:  36%|███▋      | 73/201 [00:29<01:03,  2.01it/s]11/28/2021 01:29:33 - INFO - __main__ -   Batch number = 74
Evaluating:  37%|███▋      | 74/201 [00:29<01:03,  2.00it/s]11/28/2021 01:29:34 - INFO - __main__ -   Batch number = 75
Evaluating:  37%|███▋      | 75/201 [00:30<01:02,  2.00it/s]11/28/2021 01:29:34 - INFO - __main__ -   Batch number = 76
Evaluating:  38%|███▊      | 76/201 [00:30<01:01,  2.04it/s]11/28/2021 01:29:35 - INFO - __main__ -   Batch number = 77
Evaluating:  38%|███▊      | 77/201 [00:31<01:04,  1.92it/s]11/28/2021 01:29:35 - INFO - __main__ -   Batch number = 78
Evaluating:  39%|███▉      | 78/201 [00:31<01:09,  1.78it/s]11/28/2021 01:29:36 - INFO - __main__ -   Batch number = 79
Evaluating:  39%|███▉      | 79/201 [00:32<01:12,  1.68it/s]11/28/2021 01:29:37 - INFO - __main__ -   Batch number = 80
Evaluating:  40%|███▉      | 80/201 [00:33<01:14,  1.62it/s]11/28/2021 01:29:37 - INFO - __main__ -   Batch number = 81
Evaluating:  40%|████      | 81/201 [00:33<01:15,  1.58it/s]11/28/2021 01:29:38 - INFO - __main__ -   Batch number = 82
Evaluating:  41%|████      | 82/201 [00:34<01:16,  1.55it/s]11/28/2021 01:29:39 - INFO - __main__ -   Batch number = 83
Evaluating:  41%|████▏     | 83/201 [00:35<01:14,  1.58it/s]11/28/2021 01:29:39 - INFO - __main__ -   Batch number = 84
Evaluating:  42%|████▏     | 84/201 [00:36<01:21,  1.44it/s]11/28/2021 01:29:40 - INFO - __main__ -   Batch number = 85
Evaluating:  42%|████▏     | 85/201 [00:36<01:19,  1.46it/s]11/28/2021 01:29:41 - INFO - __main__ -   Batch number = 86
Evaluating:  43%|████▎     | 86/201 [00:37<01:18,  1.47it/s]11/28/2021 01:29:41 - INFO - __main__ -   Batch number = 87
Evaluating:  43%|████▎     | 87/201 [00:38<01:17,  1.48it/s]11/28/2021 01:29:42 - INFO - __main__ -   Batch number = 88
Evaluating:  44%|████▍     | 88/201 [00:38<01:16,  1.48it/s]11/28/2021 01:29:43 - INFO - __main__ -   Batch number = 89
Evaluating:  44%|████▍     | 89/201 [00:39<01:15,  1.49it/s]11/28/2021 01:29:43 - INFO - __main__ -   Batch number = 90
Evaluating:  45%|████▍     | 90/201 [00:40<01:13,  1.51it/s]11/28/2021 01:29:44 - INFO - __main__ -   Batch number = 91
Evaluating:  45%|████▌     | 91/201 [00:40<01:10,  1.56it/s]11/28/2021 01:29:45 - INFO - __main__ -   Batch number = 92
Evaluating:  46%|████▌     | 92/201 [00:41<01:08,  1.58it/s]11/28/2021 01:29:45 - INFO - __main__ -   Batch number = 93
Evaluating:  46%|████▋     | 93/201 [00:41<01:09,  1.55it/s]11/28/2021 01:29:46 - INFO - __main__ -   Batch number = 94
Evaluating:  47%|████▋     | 94/201 [00:42<01:09,  1.53it/s]11/28/2021 01:29:47 - INFO - __main__ -   Batch number = 95
Evaluating:  47%|████▋     | 95/201 [00:43<01:09,  1.52it/s]11/28/2021 01:29:47 - INFO - __main__ -   Batch number = 96
Evaluating:  48%|████▊     | 96/201 [00:43<01:08,  1.53it/s]11/28/2021 01:29:48 - INFO - __main__ -   Batch number = 97
Evaluating:  48%|████▊     | 97/201 [00:44<01:08,  1.51it/s]11/28/2021 01:29:49 - INFO - __main__ -   Batch number = 98
Evaluating:  49%|████▉     | 98/201 [00:45<01:06,  1.55it/s]11/28/2021 01:29:49 - INFO - __main__ -   Batch number = 99
Evaluating:  49%|████▉     | 99/201 [00:45<01:09,  1.46it/s]11/28/2021 01:29:50 - INFO - __main__ -   Batch number = 100
Evaluating:  50%|████▉     | 100/201 [00:46<01:06,  1.51it/s]11/28/2021 01:29:51 - INFO - __main__ -   Batch number = 101
Evaluating:  50%|█████     | 101/201 [00:47<01:06,  1.50it/s]11/28/2021 01:29:51 - INFO - __main__ -   Batch number = 102
Evaluating:  51%|█████     | 102/201 [00:47<01:05,  1.51it/s]11/28/2021 01:29:52 - INFO - __main__ -   Batch number = 103
Evaluating:  51%|█████     | 103/201 [00:48<01:05,  1.51it/s]11/28/2021 01:29:53 - INFO - __main__ -   Batch number = 104
Evaluating:  52%|█████▏    | 104/201 [00:49<01:03,  1.52it/s]11/28/2021 01:29:53 - INFO - __main__ -   Batch number = 105
Evaluating:  52%|█████▏    | 105/201 [00:49<01:00,  1.58it/s]11/28/2021 01:29:54 - INFO - __main__ -   Batch number = 106
Evaluating:  53%|█████▎    | 106/201 [00:50<00:56,  1.67it/s]11/28/2021 01:29:54 - INFO - __main__ -   Batch number = 107
Evaluating:  53%|█████▎    | 107/201 [00:50<00:50,  1.85it/s]11/28/2021 01:29:55 - INFO - __main__ -   Batch number = 108
Evaluating:  54%|█████▎    | 108/201 [00:50<00:43,  2.13it/s]11/28/2021 01:29:55 - INFO - __main__ -   Batch number = 109
Evaluating:  54%|█████▍    | 109/201 [00:51<00:42,  2.14it/s]11/28/2021 01:29:56 - INFO - __main__ -   Batch number = 110
Evaluating:  55%|█████▍    | 110/201 [00:51<00:42,  2.16it/s]11/28/2021 01:29:56 - INFO - __main__ -   Batch number = 111
Evaluating:  55%|█████▌    | 111/201 [00:52<00:37,  2.39it/s]11/28/2021 01:29:56 - INFO - __main__ -   Batch number = 112
Evaluating:  56%|█████▌    | 112/201 [00:52<00:38,  2.33it/s]11/28/2021 01:29:57 - INFO - __main__ -   Batch number = 113
Evaluating:  56%|█████▌    | 113/201 [00:53<00:38,  2.26it/s]11/28/2021 01:29:57 - INFO - __main__ -   Batch number = 114
Evaluating:  57%|█████▋    | 114/201 [00:53<00:39,  2.21it/s]11/28/2021 01:29:58 - INFO - __main__ -   Batch number = 115
Evaluating:  57%|█████▋    | 115/201 [00:54<00:39,  2.15it/s]11/28/2021 01:29:58 - INFO - __main__ -   Batch number = 116
Evaluating:  58%|█████▊    | 116/201 [00:54<00:38,  2.22it/s]11/28/2021 01:29:59 - INFO - __main__ -   Batch number = 117
Evaluating:  58%|█████▊    | 117/201 [00:55<00:38,  2.15it/s]11/28/2021 01:29:59 - INFO - __main__ -   Batch number = 118
Evaluating:  59%|█████▊    | 118/201 [00:55<00:38,  2.17it/s]11/28/2021 01:30:00 - INFO - __main__ -   Batch number = 119
Evaluating:  59%|█████▉    | 119/201 [00:55<00:38,  2.15it/s]11/28/2021 01:30:00 - INFO - __main__ -   Batch number = 120
Evaluating:  60%|█████▉    | 120/201 [00:56<00:38,  2.11it/s]11/28/2021 01:30:01 - INFO - __main__ -   Batch number = 121
Evaluating:  60%|██████    | 121/201 [00:56<00:37,  2.14it/s]11/28/2021 01:30:01 - INFO - __main__ -   Batch number = 122
Evaluating:  61%|██████    | 122/201 [00:57<00:37,  2.09it/s]11/28/2021 01:30:02 - INFO - __main__ -   Batch number = 123
Evaluating:  61%|██████    | 123/201 [00:57<00:36,  2.16it/s]11/28/2021 01:30:02 - INFO - __main__ -   Batch number = 124
Evaluating:  62%|██████▏   | 124/201 [00:58<00:36,  2.12it/s]11/28/2021 01:30:02 - INFO - __main__ -   Batch number = 125
Evaluating:  62%|██████▏   | 125/201 [00:58<00:36,  2.10it/s]11/28/2021 01:30:03 - INFO - __main__ -   Batch number = 126
Evaluating:  63%|██████▎   | 126/201 [00:59<00:34,  2.19it/s]11/28/2021 01:30:03 - INFO - __main__ -   Batch number = 127
Evaluating:  63%|██████▎   | 127/201 [00:59<00:34,  2.13it/s]11/28/2021 01:30:04 - INFO - __main__ -   Batch number = 128
Evaluating:  64%|██████▎   | 128/201 [01:00<00:32,  2.21it/s]11/28/2021 01:30:04 - INFO - __main__ -   Batch number = 129
Evaluating:  64%|██████▍   | 129/201 [01:00<00:34,  2.07it/s]11/28/2021 01:30:05 - INFO - __main__ -   Batch number = 130
Evaluating:  65%|██████▍   | 130/201 [01:01<00:33,  2.11it/s]11/28/2021 01:30:05 - INFO - __main__ -   Batch number = 131
Evaluating:  65%|██████▌   | 131/201 [01:01<00:33,  2.08it/s]11/28/2021 01:30:06 - INFO - __main__ -   Batch number = 132
Evaluating:  66%|██████▌   | 132/201 [01:02<00:32,  2.12it/s]11/28/2021 01:30:06 - INFO - __main__ -   Batch number = 133
Evaluating:  66%|██████▌   | 133/201 [01:02<00:31,  2.13it/s]11/28/2021 01:30:07 - INFO - __main__ -   Batch number = 134
Evaluating:  67%|██████▋   | 134/201 [01:03<00:32,  2.08it/s]11/28/2021 01:30:07 - INFO - __main__ -   Batch number = 135
Evaluating:  67%|██████▋   | 135/201 [01:03<00:29,  2.23it/s]11/28/2021 01:30:08 - INFO - __main__ -   Batch number = 136
Evaluating:  68%|██████▊   | 136/201 [01:03<00:30,  2.16it/s]11/28/2021 01:30:08 - INFO - __main__ -   Batch number = 137
Evaluating:  68%|██████▊   | 137/201 [01:04<00:30,  2.11it/s]11/28/2021 01:30:09 - INFO - __main__ -   Batch number = 138
Evaluating:  69%|██████▊   | 138/201 [01:04<00:27,  2.30it/s]11/28/2021 01:30:09 - INFO - __main__ -   Batch number = 139
Evaluating:  69%|██████▉   | 139/201 [01:05<00:25,  2.45it/s]11/28/2021 01:30:09 - INFO - __main__ -   Batch number = 140
Evaluating:  70%|██████▉   | 140/201 [01:05<00:26,  2.28it/s]11/28/2021 01:30:10 - INFO - __main__ -   Batch number = 141
Evaluating:  70%|███████   | 141/201 [01:06<00:27,  2.18it/s]11/28/2021 01:30:10 - INFO - __main__ -   Batch number = 142
Evaluating:  71%|███████   | 142/201 [01:06<00:25,  2.36it/s]11/28/2021 01:30:11 - INFO - __main__ -   Batch number = 143
Evaluating:  71%|███████   | 143/201 [01:06<00:25,  2.24it/s]11/28/2021 01:30:11 - INFO - __main__ -   Batch number = 144
Evaluating:  72%|███████▏  | 144/201 [01:07<00:26,  2.16it/s]11/28/2021 01:30:12 - INFO - __main__ -   Batch number = 145
Evaluating:  72%|███████▏  | 145/201 [01:07<00:26,  2.11it/s]11/28/2021 01:30:12 - INFO - __main__ -   Batch number = 146
Evaluating:  73%|███████▎  | 146/201 [01:08<00:26,  2.07it/s]11/28/2021 01:30:13 - INFO - __main__ -   Batch number = 147
Evaluating:  73%|███████▎  | 147/201 [01:08<00:24,  2.24it/s]11/28/2021 01:30:13 - INFO - __main__ -   Batch number = 148
Evaluating:  74%|███████▎  | 148/201 [01:09<00:23,  2.26it/s]11/28/2021 01:30:13 - INFO - __main__ -   Batch number = 149
Evaluating:  74%|███████▍  | 149/201 [01:09<00:24,  2.16it/s]11/28/2021 01:30:14 - INFO - __main__ -   Batch number = 150
Evaluating:  75%|███████▍  | 150/201 [01:10<00:22,  2.23it/s]11/28/2021 01:30:14 - INFO - __main__ -   Batch number = 151
Evaluating:  75%|███████▌  | 151/201 [01:10<00:23,  2.13it/s]11/28/2021 01:30:15 - INFO - __main__ -   Batch number = 152
Evaluating:  76%|███████▌  | 152/201 [01:11<00:23,  2.07it/s]11/28/2021 01:30:15 - INFO - __main__ -   Batch number = 153
Evaluating:  76%|███████▌  | 153/201 [01:11<00:22,  2.16it/s]11/28/2021 01:30:16 - INFO - __main__ -   Batch number = 154
Evaluating:  77%|███████▋  | 154/201 [01:12<00:22,  2.10it/s]11/28/2021 01:30:16 - INFO - __main__ -   Batch number = 155
Evaluating:  77%|███████▋  | 155/201 [01:12<00:21,  2.16it/s]11/28/2021 01:30:17 - INFO - __main__ -   Batch number = 156
Evaluating:  78%|███████▊  | 156/201 [01:13<00:21,  2.11it/s]11/28/2021 01:30:17 - INFO - __main__ -   Batch number = 157
Evaluating:  78%|███████▊  | 157/201 [01:13<00:20,  2.16it/s]11/28/2021 01:30:18 - INFO - __main__ -   Batch number = 158
Evaluating:  79%|███████▊  | 158/201 [01:13<00:16,  2.64it/s]11/28/2021 01:30:18 - INFO - __main__ -   Batch number = 159
Evaluating:  79%|███████▉  | 159/201 [01:13<00:14,  2.85it/s]11/28/2021 01:30:18 - INFO - __main__ -   Batch number = 160
Evaluating:  80%|███████▉  | 160/201 [01:14<00:14,  2.91it/s]11/28/2021 01:30:18 - INFO - __main__ -   Batch number = 161
Evaluating:  80%|████████  | 161/201 [01:14<00:12,  3.33it/s]11/28/2021 01:30:19 - INFO - __main__ -   Batch number = 162
Evaluating:  81%|████████  | 162/201 [01:14<00:10,  3.76it/s]11/28/2021 01:30:19 - INFO - __main__ -   Batch number = 163
Evaluating:  81%|████████  | 163/201 [01:15<00:10,  3.52it/s]11/28/2021 01:30:19 - INFO - __main__ -   Batch number = 164
Evaluating:  82%|████████▏ | 164/201 [01:15<00:10,  3.40it/s]11/28/2021 01:30:19 - INFO - __main__ -   Batch number = 165
Evaluating:  82%|████████▏ | 165/201 [01:15<00:09,  3.71it/s]11/28/2021 01:30:20 - INFO - __main__ -   Batch number = 166
Evaluating:  83%|████████▎ | 166/201 [01:15<00:10,  3.24it/s]11/28/2021 01:30:20 - INFO - __main__ -   Batch number = 167
Evaluating:  83%|████████▎ | 167/201 [01:16<00:10,  3.20it/s]11/28/2021 01:30:20 - INFO - __main__ -   Batch number = 168
Evaluating:  84%|████████▎ | 168/201 [01:16<00:10,  3.13it/s]11/28/2021 01:30:21 - INFO - __main__ -   Batch number = 169
Evaluating:  84%|████████▍ | 169/201 [01:16<00:10,  3.08it/s]11/28/2021 01:30:21 - INFO - __main__ -   Batch number = 170
Evaluating:  85%|████████▍ | 170/201 [01:17<00:10,  3.00it/s]11/28/2021 01:30:21 - INFO - __main__ -   Batch number = 171
Evaluating:  85%|████████▌ | 171/201 [01:17<00:10,  2.97it/s]11/28/2021 01:30:22 - INFO - __main__ -   Batch number = 172
Evaluating:  86%|████████▌ | 172/201 [01:17<00:09,  2.93it/s]11/28/2021 01:30:22 - INFO - __main__ -   Batch number = 173
Evaluating:  86%|████████▌ | 173/201 [01:18<00:09,  2.91it/s]11/28/2021 01:30:22 - INFO - __main__ -   Batch number = 174
Evaluating:  87%|████████▋ | 174/201 [01:18<00:09,  2.88it/s]11/28/2021 01:30:23 - INFO - __main__ -   Batch number = 175
Evaluating:  87%|████████▋ | 175/201 [01:19<00:09,  2.89it/s]11/28/2021 01:30:23 - INFO - __main__ -   Batch number = 176
Evaluating:  88%|████████▊ | 176/201 [01:19<00:08,  2.89it/s]11/28/2021 01:30:24 - INFO - __main__ -   Batch number = 177
Evaluating:  88%|████████▊ | 177/201 [01:19<00:08,  2.89it/s]11/28/2021 01:30:24 - INFO - __main__ -   Batch number = 178
Evaluating:  89%|████████▊ | 178/201 [01:20<00:08,  2.87it/s]11/28/2021 01:30:24 - INFO - __main__ -   Batch number = 179
Evaluating:  89%|████████▉ | 179/201 [01:20<00:07,  3.13it/s]11/28/2021 01:30:24 - INFO - __main__ -   Batch number = 180
Evaluating:  90%|████████▉ | 180/201 [01:20<00:06,  3.04it/s]11/28/2021 01:30:25 - INFO - __main__ -   Batch number = 181
Evaluating:  90%|█████████ | 181/201 [01:21<00:06,  3.00it/s]11/28/2021 01:30:25 - INFO - __main__ -   Batch number = 182
Evaluating:  91%|█████████ | 182/201 [01:21<00:06,  2.95it/s]11/28/2021 01:30:26 - INFO - __main__ -   Batch number = 183
Evaluating:  91%|█████████ | 183/201 [01:21<00:06,  2.95it/s]11/28/2021 01:30:26 - INFO - __main__ -   Batch number = 184
Evaluating:  92%|█████████▏| 184/201 [01:22<00:05,  2.91it/s]11/28/2021 01:30:26 - INFO - __main__ -   Batch number = 185
Evaluating:  92%|█████████▏| 185/201 [01:22<00:05,  2.92it/s]11/28/2021 01:30:27 - INFO - __main__ -   Batch number = 186
Evaluating:  93%|█████████▎| 186/201 [01:22<00:05,  2.90it/s]11/28/2021 01:30:27 - INFO - __main__ -   Batch number = 187
Evaluating:  93%|█████████▎| 187/201 [01:23<00:04,  2.91it/s]11/28/2021 01:30:27 - INFO - __main__ -   Batch number = 188
Evaluating:  94%|█████████▎| 188/201 [01:23<00:04,  2.91it/s]11/28/2021 01:30:28 - INFO - __main__ -   Batch number = 189
Evaluating:  94%|█████████▍| 189/201 [01:23<00:04,  2.91it/s]11/28/2021 01:30:28 - INFO - __main__ -   Batch number = 190
Evaluating:  95%|█████████▍| 190/201 [01:24<00:03,  2.85it/s]11/28/2021 01:30:28 - INFO - __main__ -   Batch number = 191
Evaluating:  95%|█████████▌| 191/201 [01:24<00:03,  2.88it/s]11/28/2021 01:30:29 - INFO - __main__ -   Batch number = 192
Evaluating:  96%|█████████▌| 192/201 [01:24<00:03,  2.86it/s]11/28/2021 01:30:29 - INFO - __main__ -   Batch number = 193
Evaluating:  96%|█████████▌| 193/201 [01:25<00:02,  2.88it/s]11/28/2021 01:30:30 - INFO - __main__ -   Batch number = 194
Evaluating:  97%|█████████▋| 194/201 [01:25<00:02,  2.55it/s]11/28/2021 01:30:30 - INFO - __main__ -   Batch number = 195
Evaluating:  97%|█████████▋| 195/201 [01:26<00:02,  2.72it/s]11/28/2021 01:30:30 - INFO - __main__ -   Batch number = 196
Evaluating:  98%|█████████▊| 196/201 [01:26<00:01,  2.80it/s]11/28/2021 01:30:30 - INFO - __main__ -   Batch number = 197
Evaluating:  98%|█████████▊| 197/201 [01:26<00:01,  2.88it/s]11/28/2021 01:30:31 - INFO - __main__ -   Batch number = 198
Evaluating:  99%|█████████▊| 198/201 [01:27<00:01,  2.90it/s]11/28/2021 01:30:31 - INFO - __main__ -   Batch number = 199
Evaluating:  99%|█████████▉| 199/201 [01:27<00:00,  2.90it/s]11/28/2021 01:30:31 - INFO - __main__ -   Batch number = 200
Evaluating: 100%|█████████▉| 200/201 [01:27<00:00,  2.86it/s]11/28/2021 01:30:32 - INFO - __main__ -   Batch number = 201
Evaluating: 100%|██████████| 201/201 [01:27<00:00,  2.29it/s]
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:30:36 - INFO - __main__ -   ***** Evaluation result  in is *****
11/28/2021 01:30:36 - INFO - __main__ -     f1 = 0.7131712408505172
11/28/2021 01:30:36 - INFO - __main__ -     loss = 0.9542221604888119
11/28/2021 01:30:36 - INFO - __main__ -     precision = 0.7214397390168461
11/28/2021 01:30:36 - INFO - __main__ -     recall = 0.7050901273358691
88.81user 30.16system 1:58.47elapsed 100%CPU (0avgtext+0avgdata 3984888maxresident)k
0inputs+1760outputs (0major+1536326minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:30:39 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='is', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:30:39 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:30:39 - INFO - __main__ -   Seed = 2
11/28/2021 01:30:39 - INFO - root -   save model
11/28/2021 01:30:39 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='is', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:30:39 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:30:42 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:30:48 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:30:48 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:30:48 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
11/28/2021 01:30:48 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:30:48 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:30:48 - INFO - root -   loading lang adpater bh/wiki@ukp
11/28/2021 01:30:48 - INFO - __main__ -   Adapter Languages : ['bh'], Length : 1
11/28/2021 01:30:48 - INFO - __main__ -   Adapter Names ['bh/wiki@ukp'], Length : 1
11/28/2021 01:30:48 - INFO - __main__ -   Language = bh
11/28/2021 01:30:48 - INFO - __main__ -   Adapter Name = bh/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/bh/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_bh_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/adapter_config.json
Adding adapter 'bh' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/head_config.json
11/28/2021 01:30:59 - INFO - __main__ -   Language adapter for is not found, using bh instead
11/28/2021 01:30:59 - INFO - __main__ -   Set active language adapter to bh
11/28/2021 01:30:59 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:30:59 - INFO - __main__ -   Adapter Languages = ['bh']
11/28/2021 01:30:59 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_is_bert-base-multilingual-cased_128
11/28/2021 01:31:00 - INFO - __main__ -   ***** Running evaluation  in is *****
11/28/2021 01:31:00 - INFO - __main__ -     Num examples = 6401
11/28/2021 01:31:00 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/201 [00:00<?, ?it/s]11/28/2021 01:31:00 - INFO - __main__ -   Batch number = 1
Evaluating:   0%|          | 1/201 [00:00<02:15,  1.47it/s]11/28/2021 01:31:00 - INFO - __main__ -   Batch number = 2
Evaluating:   1%|          | 2/201 [00:01<02:12,  1.50it/s]11/28/2021 01:31:01 - INFO - __main__ -   Batch number = 3
Evaluating:   1%|▏         | 3/201 [00:01<02:11,  1.50it/s]11/28/2021 01:31:02 - INFO - __main__ -   Batch number = 4
Evaluating:   2%|▏         | 4/201 [00:02<02:10,  1.50it/s]11/28/2021 01:31:02 - INFO - __main__ -   Batch number = 5
Evaluating:   2%|▏         | 5/201 [00:03<02:10,  1.51it/s]11/28/2021 01:31:03 - INFO - __main__ -   Batch number = 6
Evaluating:   3%|▎         | 6/201 [00:03<02:00,  1.61it/s]11/28/2021 01:31:04 - INFO - __main__ -   Batch number = 7
Evaluating:   3%|▎         | 7/201 [00:04<01:52,  1.72it/s]11/28/2021 01:31:04 - INFO - __main__ -   Batch number = 8
Evaluating:   4%|▍         | 8/201 [00:04<01:46,  1.81it/s]11/28/2021 01:31:05 - INFO - __main__ -   Batch number = 9
Evaluating:   4%|▍         | 9/201 [00:05<01:42,  1.87it/s]11/28/2021 01:31:05 - INFO - __main__ -   Batch number = 10
Evaluating:   5%|▍         | 10/201 [00:05<01:39,  1.91it/s]11/28/2021 01:31:06 - INFO - __main__ -   Batch number = 11
Evaluating:   5%|▌         | 11/201 [00:06<01:37,  1.94it/s]11/28/2021 01:31:06 - INFO - __main__ -   Batch number = 12
Evaluating:   6%|▌         | 12/201 [00:06<01:36,  1.97it/s]11/28/2021 01:31:07 - INFO - __main__ -   Batch number = 13
Evaluating:   6%|▋         | 13/201 [00:07<01:34,  1.98it/s]11/28/2021 01:31:07 - INFO - __main__ -   Batch number = 14
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:31:08 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='pt', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:31:08 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:31:08 - INFO - __main__ -   Seed = 1
11/28/2021 01:31:08 - INFO - root -   save model
11/28/2021 01:31:08 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='pt', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:31:08 - INFO - __main__ -   Loading pretrained model and tokenizer
Evaluating:   7%|▋         | 14/201 [00:07<01:33,  2.00it/s]11/28/2021 01:31:08 - INFO - __main__ -   Batch number = 15
Evaluating:   7%|▋         | 15/201 [00:08<01:32,  2.00it/s]11/28/2021 01:31:08 - INFO - __main__ -   Batch number = 16
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:   8%|▊         | 16/201 [00:08<01:31,  2.02it/s]11/28/2021 01:31:09 - INFO - __main__ -   Batch number = 17
Evaluating:   8%|▊         | 17/201 [00:09<01:31,  2.02it/s]11/28/2021 01:31:09 - INFO - __main__ -   Batch number = 18
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:   9%|▉         | 18/201 [00:09<01:30,  2.02it/s]11/28/2021 01:31:10 - INFO - __main__ -   Batch number = 19
Evaluating:   9%|▉         | 19/201 [00:10<01:30,  2.02it/s]11/28/2021 01:31:10 - INFO - __main__ -   Batch number = 20
loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:31:10 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
Evaluating:  10%|▉         | 20/201 [00:10<01:29,  2.02it/s]11/28/2021 01:31:11 - INFO - __main__ -   Batch number = 21
Evaluating:  10%|█         | 21/201 [00:11<01:29,  2.02it/s]11/28/2021 01:31:11 - INFO - __main__ -   Batch number = 22
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  11%|█         | 22/201 [00:11<01:28,  2.01it/s]11/28/2021 01:31:12 - INFO - __main__ -   Batch number = 23
Evaluating:  11%|█▏        | 23/201 [00:12<01:28,  2.01it/s]11/28/2021 01:31:12 - INFO - __main__ -   Batch number = 24
Evaluating:  12%|█▏        | 24/201 [00:12<01:28,  2.01it/s]11/28/2021 01:31:13 - INFO - __main__ -   Batch number = 25
Evaluating:  12%|█▏        | 25/201 [00:13<01:27,  2.01it/s]11/28/2021 01:31:13 - INFO - __main__ -   Batch number = 26
Evaluating:  13%|█▎        | 26/201 [00:13<01:27,  2.01it/s]11/28/2021 01:31:14 - INFO - __main__ -   Batch number = 27
Evaluating:  13%|█▎        | 27/201 [00:14<01:26,  2.01it/s]11/28/2021 01:31:14 - INFO - __main__ -   Batch number = 28
Evaluating:  14%|█▍        | 28/201 [00:14<01:26,  2.01it/s]11/28/2021 01:31:14 - INFO - __main__ -   Batch number = 29
Evaluating:  14%|█▍        | 29/201 [00:15<01:25,  2.01it/s]11/28/2021 01:31:15 - INFO - __main__ -   Batch number = 30
Evaluating:  15%|█▍        | 30/201 [00:15<01:25,  2.01it/s]11/28/2021 01:31:15 - INFO - __main__ -   Batch number = 31
Evaluating:  15%|█▌        | 31/201 [00:16<01:24,  2.01it/s]11/28/2021 01:31:16 - INFO - __main__ -   Batch number = 32
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:31:16 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:31:16 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:31:16 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
11/28/2021 01:31:16 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:31:16 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:31:16 - INFO - root -   loading lang adpater bh/wiki@ukp
11/28/2021 01:31:16 - INFO - __main__ -   Adapter Languages : ['bh'], Length : 1
11/28/2021 01:31:16 - INFO - __main__ -   Adapter Names ['bh/wiki@ukp'], Length : 1
11/28/2021 01:31:16 - INFO - __main__ -   Language = bh
11/28/2021 01:31:16 - INFO - __main__ -   Adapter Name = bh/wiki@ukp
Evaluating:  16%|█▌        | 32/201 [00:16<01:23,  2.02it/s]11/28/2021 01:31:16 - INFO - __main__ -   Batch number = 33
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/bh/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_bh_wiki_pfeiffer.zip.
Evaluating:  16%|█▋        | 33/201 [00:17<01:23,  2.02it/s]11/28/2021 01:31:17 - INFO - __main__ -   Batch number = 34
Evaluating:  17%|█▋        | 34/201 [00:17<01:22,  2.02it/s]11/28/2021 01:31:17 - INFO - __main__ -   Batch number = 35
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/adapter_config.json
Adding adapter 'bh' of type 'text_lang'.
Evaluating:  17%|█▋        | 35/201 [00:18<01:22,  2.01it/s]11/28/2021 01:31:18 - INFO - __main__ -   Batch number = 36
Loading module weights from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/head_config.json
Evaluating:  18%|█▊        | 36/201 [00:18<01:22,  2.00it/s]11/28/2021 01:31:18 - INFO - __main__ -   Batch number = 37
Evaluating:  18%|█▊        | 37/201 [00:19<01:22,  2.00it/s]11/28/2021 01:31:19 - INFO - __main__ -   Batch number = 38
Evaluating:  19%|█▉        | 38/201 [00:19<01:20,  2.03it/s]11/28/2021 01:31:19 - INFO - __main__ -   Batch number = 39
Evaluating:  19%|█▉        | 39/201 [00:20<01:20,  2.02it/s]11/28/2021 01:31:20 - INFO - __main__ -   Batch number = 40
Evaluating:  20%|█▉        | 40/201 [00:20<01:18,  2.04it/s]11/28/2021 01:31:20 - INFO - __main__ -   Batch number = 41
Evaluating:  20%|██        | 41/201 [00:21<01:17,  2.07it/s]11/28/2021 01:31:21 - INFO - __main__ -   Batch number = 42
Evaluating:  21%|██        | 42/201 [00:21<01:15,  2.10it/s]11/28/2021 01:31:21 - INFO - __main__ -   Batch number = 43
Evaluating:  21%|██▏       | 43/201 [00:22<01:15,  2.09it/s]11/28/2021 01:31:22 - INFO - __main__ -   Batch number = 44
Evaluating:  22%|██▏       | 44/201 [00:22<01:15,  2.08it/s]11/28/2021 01:31:22 - INFO - __main__ -   Batch number = 45
Evaluating:  22%|██▏       | 45/201 [00:23<01:15,  2.07it/s]11/28/2021 01:31:23 - INFO - __main__ -   Batch number = 46
Evaluating:  23%|██▎       | 46/201 [00:23<01:16,  2.04it/s]11/28/2021 01:31:23 - INFO - __main__ -   Batch number = 47
Evaluating:  23%|██▎       | 47/201 [00:24<01:16,  2.02it/s]11/28/2021 01:31:24 - INFO - __main__ -   Batch number = 48
Evaluating:  24%|██▍       | 48/201 [00:24<01:16,  2.01it/s]11/28/2021 01:31:25 - INFO - __main__ -   Batch number = 49
Evaluating:  24%|██▍       | 49/201 [00:25<01:24,  1.79it/s]11/28/2021 01:31:25 - INFO - __main__ -   Batch number = 50
Evaluating:  25%|██▍       | 50/201 [00:25<01:21,  1.86it/s]11/28/2021 01:31:26 - INFO - __main__ -   Batch number = 51
Evaluating:  25%|██▌       | 51/201 [00:26<01:18,  1.90it/s]11/28/2021 01:31:26 - INFO - __main__ -   Batch number = 52
Evaluating:  26%|██▌       | 52/201 [00:26<01:17,  1.93it/s]11/28/2021 01:31:27 - INFO - __main__ -   Batch number = 53
Evaluating:  26%|██▋       | 53/201 [00:27<01:15,  1.96it/s]11/28/2021 01:31:27 - INFO - __main__ -   Batch number = 54
Evaluating:  27%|██▋       | 54/201 [00:27<01:14,  1.97it/s]11/28/2021 01:31:28 - INFO - __main__ -   Batch number = 55
Evaluating:  27%|██▋       | 55/201 [00:28<01:13,  1.98it/s]11/28/2021 01:31:28 - INFO - __main__ -   Batch number = 56
Evaluating:  28%|██▊       | 56/201 [00:28<01:07,  2.14it/s]11/28/2021 01:31:28 - INFO - __main__ -   Batch number = 57
Evaluating:  28%|██▊       | 57/201 [00:29<01:01,  2.33it/s]11/28/2021 01:31:29 - INFO - __main__ -   Batch number = 58
Evaluating:  29%|██▉       | 58/201 [00:29<00:57,  2.50it/s]11/28/2021 01:31:29 - INFO - __main__ -   Batch number = 59
Evaluating:  29%|██▉       | 59/201 [00:29<00:53,  2.64it/s]11/28/2021 01:31:29 - INFO - __main__ -   Batch number = 60
Evaluating:  30%|██▉       | 60/201 [00:30<00:53,  2.65it/s]11/28/2021 01:31:30 - INFO - __main__ -   Batch number = 61
Evaluating:  30%|███       | 61/201 [00:30<00:49,  2.81it/s]11/28/2021 01:31:30 - INFO - __main__ -   Batch number = 62
Evaluating:  31%|███       | 62/201 [00:30<00:47,  2.92it/s]11/28/2021 01:31:30 - INFO - __main__ -   Batch number = 63
Evaluating:  31%|███▏      | 63/201 [00:30<00:46,  2.97it/s]11/28/2021 01:31:31 - INFO - __main__ -   Batch number = 64
Evaluating:  32%|███▏      | 64/201 [00:31<00:45,  2.99it/s]11/28/2021 01:31:31 - INFO - __main__ -   Batch number = 65
11/28/2021 01:31:31 - INFO - __main__ -   Language adapter for pt not found, using bh instead
11/28/2021 01:31:31 - INFO - __main__ -   Set active language adapter to bh
11/28/2021 01:31:31 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:31:31 - INFO - __main__ -   Adapter Languages = ['bh']
11/28/2021 01:31:31 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_pt_bert-base-multilingual-cased_128
Evaluating:  32%|███▏      | 65/201 [00:31<00:45,  3.00it/s]11/28/2021 01:31:31 - INFO - __main__ -   Batch number = 66
Evaluating:  33%|███▎      | 66/201 [00:31<00:44,  3.01it/s]11/28/2021 01:31:32 - INFO - __main__ -   Batch number = 67
11/28/2021 01:31:32 - INFO - __main__ -   ***** Running evaluation  in pt *****
11/28/2021 01:31:32 - INFO - __main__ -     Num examples = 2682
11/28/2021 01:31:32 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/84 [00:00<?, ?it/s]11/28/2021 01:31:32 - INFO - __main__ -   Batch number = 1
Evaluating:  33%|███▎      | 67/201 [00:32<00:48,  2.77it/s]11/28/2021 01:31:32 - INFO - __main__ -   Batch number = 68
Evaluating:   1%|          | 1/84 [00:00<00:43,  1.92it/s]11/28/2021 01:31:32 - INFO - __main__ -   Batch number = 2
Evaluating:  34%|███▍      | 68/201 [00:32<00:53,  2.48it/s]11/28/2021 01:31:33 - INFO - __main__ -   Batch number = 69
Evaluating:   2%|▏         | 2/84 [00:01<00:41,  1.98it/s]11/28/2021 01:31:33 - INFO - __main__ -   Batch number = 3
Evaluating:  34%|███▍      | 69/201 [00:33<00:57,  2.31it/s]11/28/2021 01:31:33 - INFO - __main__ -   Batch number = 70
Evaluating:   4%|▎         | 3/84 [00:01<00:40,  1.99it/s]11/28/2021 01:31:33 - INFO - __main__ -   Batch number = 4
Evaluating:  35%|███▍      | 70/201 [00:33<00:59,  2.20it/s]11/28/2021 01:31:34 - INFO - __main__ -   Batch number = 71
Evaluating:   5%|▍         | 4/84 [00:02<00:39,  2.01it/s]11/28/2021 01:31:34 - INFO - __main__ -   Batch number = 5
Evaluating:  35%|███▌      | 71/201 [00:34<01:00,  2.14it/s]11/28/2021 01:31:34 - INFO - __main__ -   Batch number = 72
Evaluating:   6%|▌         | 5/84 [00:02<00:39,  2.01it/s]11/28/2021 01:31:34 - INFO - __main__ -   Batch number = 6
Evaluating:  36%|███▌      | 72/201 [00:34<01:01,  2.08it/s]11/28/2021 01:31:35 - INFO - __main__ -   Batch number = 73
Evaluating:   7%|▋         | 6/84 [00:02<00:38,  2.03it/s]11/28/2021 01:31:35 - INFO - __main__ -   Batch number = 7
Evaluating:  36%|███▋      | 73/201 [00:35<01:02,  2.05it/s]11/28/2021 01:31:35 - INFO - __main__ -   Batch number = 74
Evaluating:   8%|▊         | 7/84 [00:03<00:37,  2.04it/s]11/28/2021 01:31:35 - INFO - __main__ -   Batch number = 8
Evaluating:  37%|███▋      | 74/201 [00:35<01:01,  2.05it/s]11/28/2021 01:31:36 - INFO - __main__ -   Batch number = 75
Evaluating:  10%|▉         | 8/84 [00:03<00:37,  2.04it/s]11/28/2021 01:31:36 - INFO - __main__ -   Batch number = 9
Evaluating:  37%|███▋      | 75/201 [00:36<01:01,  2.04it/s]11/28/2021 01:31:36 - INFO - __main__ -   Batch number = 76
Evaluating:  11%|█         | 9/84 [00:04<00:36,  2.04it/s]11/28/2021 01:31:36 - INFO - __main__ -   Batch number = 10
Evaluating:  38%|███▊      | 76/201 [00:36<01:01,  2.04it/s]11/28/2021 01:31:37 - INFO - __main__ -   Batch number = 77
Evaluating:  12%|█▏        | 10/84 [00:04<00:36,  2.04it/s]11/28/2021 01:31:37 - INFO - __main__ -   Batch number = 11
Evaluating:  38%|███▊      | 77/201 [00:37<01:00,  2.03it/s]11/28/2021 01:31:37 - INFO - __main__ -   Batch number = 78
Evaluating:  13%|█▎        | 11/84 [00:05<00:35,  2.04it/s]11/28/2021 01:31:37 - INFO - __main__ -   Batch number = 12
Evaluating:  39%|███▉      | 78/201 [00:37<01:00,  2.03it/s]11/28/2021 01:31:38 - INFO - __main__ -   Batch number = 79
Evaluating:  14%|█▍        | 12/84 [00:05<00:35,  2.03it/s]11/28/2021 01:31:38 - INFO - __main__ -   Batch number = 13
Evaluating:  39%|███▉      | 79/201 [00:38<01:00,  2.02it/s]11/28/2021 01:31:38 - INFO - __main__ -   Batch number = 80
Evaluating:  15%|█▌        | 13/84 [00:06<00:34,  2.03it/s]11/28/2021 01:31:38 - INFO - __main__ -   Batch number = 14
Evaluating:  40%|███▉      | 80/201 [00:38<01:00,  2.01it/s]11/28/2021 01:31:39 - INFO - __main__ -   Batch number = 81
Evaluating:  17%|█▋        | 14/84 [00:06<00:34,  2.04it/s]11/28/2021 01:31:39 - INFO - __main__ -   Batch number = 15
Evaluating:  40%|████      | 81/201 [00:39<00:59,  2.02it/s]11/28/2021 01:31:39 - INFO - __main__ -   Batch number = 82
Evaluating:  18%|█▊        | 15/84 [00:07<00:33,  2.03it/s]11/28/2021 01:31:39 - INFO - __main__ -   Batch number = 16
Evaluating:  41%|████      | 82/201 [00:39<00:59,  2.02it/s]11/28/2021 01:31:40 - INFO - __main__ -   Batch number = 83
Evaluating:  19%|█▉        | 16/84 [00:07<00:33,  2.03it/s]11/28/2021 01:31:40 - INFO - __main__ -   Batch number = 17
Evaluating:  41%|████▏     | 83/201 [00:40<00:58,  2.01it/s]11/28/2021 01:31:40 - INFO - __main__ -   Batch number = 84
Evaluating:  20%|██        | 17/84 [00:08<00:33,  2.02it/s]11/28/2021 01:31:40 - INFO - __main__ -   Batch number = 18
Evaluating:  42%|████▏     | 84/201 [00:40<00:58,  2.00it/s]11/28/2021 01:31:41 - INFO - __main__ -   Batch number = 85
Evaluating:  21%|██▏       | 18/84 [00:08<00:32,  2.02it/s]11/28/2021 01:31:41 - INFO - __main__ -   Batch number = 19
Evaluating:  42%|████▏     | 85/201 [00:41<00:57,  2.00it/s]11/28/2021 01:31:41 - INFO - __main__ -   Batch number = 86
Evaluating:  23%|██▎       | 19/84 [00:09<00:31,  2.05it/s]11/28/2021 01:31:41 - INFO - __main__ -   Batch number = 20
Evaluating:  43%|████▎     | 86/201 [00:41<00:57,  1.99it/s]11/28/2021 01:31:42 - INFO - __main__ -   Batch number = 87
Evaluating:  24%|██▍       | 20/84 [00:09<00:31,  2.04it/s]11/28/2021 01:31:42 - INFO - __main__ -   Batch number = 21
Evaluating:  43%|████▎     | 87/201 [00:42<00:57,  1.99it/s]11/28/2021 01:31:42 - INFO - __main__ -   Batch number = 88
Evaluating:  25%|██▌       | 21/84 [00:10<00:31,  2.03it/s]11/28/2021 01:31:42 - INFO - __main__ -   Batch number = 22
Evaluating:  44%|████▍     | 88/201 [00:42<00:56,  2.00it/s]11/28/2021 01:31:43 - INFO - __main__ -   Batch number = 89
Evaluating:  26%|██▌       | 22/84 [00:10<00:30,  2.02it/s]11/28/2021 01:31:43 - INFO - __main__ -   Batch number = 23
Evaluating:  44%|████▍     | 89/201 [00:43<00:55,  2.00it/s]11/28/2021 01:31:43 - INFO - __main__ -   Batch number = 90
Evaluating:  27%|██▋       | 23/84 [00:11<00:30,  2.02it/s]11/28/2021 01:31:43 - INFO - __main__ -   Batch number = 24
Evaluating:  45%|████▍     | 90/201 [00:43<00:55,  2.02it/s]11/28/2021 01:31:44 - INFO - __main__ -   Batch number = 91
Evaluating:  29%|██▊       | 24/84 [00:11<00:29,  2.01it/s]11/28/2021 01:31:44 - INFO - __main__ -   Batch number = 25
Evaluating:  45%|████▌     | 91/201 [00:44<00:54,  2.02it/s]11/28/2021 01:31:44 - INFO - __main__ -   Batch number = 92
Evaluating:  30%|██▉       | 25/84 [00:12<00:29,  2.01it/s]11/28/2021 01:31:44 - INFO - __main__ -   Batch number = 26
Evaluating:  46%|████▌     | 92/201 [00:44<00:53,  2.02it/s]11/28/2021 01:31:45 - INFO - __main__ -   Batch number = 93
Evaluating:  31%|███       | 26/84 [00:12<00:28,  2.01it/s]11/28/2021 01:31:45 - INFO - __main__ -   Batch number = 27
Evaluating:  46%|████▋     | 93/201 [00:45<00:53,  2.01it/s]11/28/2021 01:31:45 - INFO - __main__ -   Batch number = 94
Evaluating:  32%|███▏      | 27/84 [00:13<00:28,  2.01it/s]11/28/2021 01:31:45 - INFO - __main__ -   Batch number = 28
Evaluating:  47%|████▋     | 94/201 [00:45<00:53,  2.00it/s]11/28/2021 01:31:46 - INFO - __main__ -   Batch number = 95
Evaluating:  33%|███▎      | 28/84 [00:13<00:27,  2.00it/s]11/28/2021 01:31:46 - INFO - __main__ -   Batch number = 29
Evaluating:  47%|████▋     | 95/201 [00:46<00:52,  2.01it/s]11/28/2021 01:31:46 - INFO - __main__ -   Batch number = 96
Evaluating:  35%|███▍      | 29/84 [00:14<00:27,  2.01it/s]11/28/2021 01:31:46 - INFO - __main__ -   Batch number = 30
Evaluating:  48%|████▊     | 96/201 [00:46<00:52,  2.00it/s]11/28/2021 01:31:47 - INFO - __main__ -   Batch number = 97
Evaluating:  36%|███▌      | 30/84 [00:14<00:26,  2.01it/s]11/28/2021 01:31:47 - INFO - __main__ -   Batch number = 31
Evaluating:  48%|████▊     | 97/201 [00:47<00:52,  1.99it/s]11/28/2021 01:31:47 - INFO - __main__ -   Batch number = 98
Evaluating:  37%|███▋      | 31/84 [00:15<00:26,  2.00it/s]11/28/2021 01:31:47 - INFO - __main__ -   Batch number = 32
Evaluating:  49%|████▉     | 98/201 [00:47<00:51,  1.99it/s]11/28/2021 01:31:48 - INFO - __main__ -   Batch number = 99
Evaluating:  38%|███▊      | 32/84 [00:15<00:25,  2.01it/s]11/28/2021 01:31:48 - INFO - __main__ -   Batch number = 33
Evaluating:  39%|███▉      | 33/84 [00:16<00:25,  2.03it/s]11/28/2021 01:31:48 - INFO - __main__ -   Batch number = 34
Evaluating:  49%|████▉     | 99/201 [00:48<00:51,  1.97it/s]11/28/2021 01:31:48 - INFO - __main__ -   Batch number = 100
Evaluating:  40%|████      | 34/84 [00:16<00:24,  2.04it/s]11/28/2021 01:31:49 - INFO - __main__ -   Batch number = 35
Evaluating:  50%|████▉     | 100/201 [00:48<00:51,  1.97it/s]11/28/2021 01:31:49 - INFO - __main__ -   Batch number = 101
Evaluating:  42%|████▏     | 35/84 [00:17<00:24,  2.02it/s]11/28/2021 01:31:49 - INFO - __main__ -   Batch number = 36
Evaluating:  50%|█████     | 101/201 [00:49<00:50,  1.98it/s]11/28/2021 01:31:49 - INFO - __main__ -   Batch number = 102
Evaluating:  43%|████▎     | 36/84 [00:17<00:23,  2.01it/s]11/28/2021 01:31:50 - INFO - __main__ -   Batch number = 37
Evaluating:  51%|█████     | 102/201 [00:49<00:50,  1.98it/s]11/28/2021 01:31:50 - INFO - __main__ -   Batch number = 103
Evaluating:  44%|████▍     | 37/84 [00:18<00:23,  2.00it/s]11/28/2021 01:31:50 - INFO - __main__ -   Batch number = 38
Evaluating:  51%|█████     | 103/201 [00:50<00:49,  1.97it/s]11/28/2021 01:31:50 - INFO - __main__ -   Batch number = 104
Evaluating:  45%|████▌     | 38/84 [00:18<00:23,  2.00it/s]11/28/2021 01:31:51 - INFO - __main__ -   Batch number = 39
Evaluating:  52%|█████▏    | 104/201 [00:50<00:49,  1.97it/s]11/28/2021 01:31:51 - INFO - __main__ -   Batch number = 105
Evaluating:  46%|████▋     | 39/84 [00:19<00:22,  2.00it/s]11/28/2021 01:31:51 - INFO - __main__ -   Batch number = 40
Evaluating:  52%|█████▏    | 105/201 [00:51<00:48,  1.97it/s]11/28/2021 01:31:51 - INFO - __main__ -   Batch number = 106
Evaluating:  48%|████▊     | 40/84 [00:19<00:21,  2.02it/s]11/28/2021 01:31:52 - INFO - __main__ -   Batch number = 41
Evaluating:  53%|█████▎    | 106/201 [00:51<00:47,  2.00it/s]11/28/2021 01:31:52 - INFO - __main__ -   Batch number = 107
Evaluating:  49%|████▉     | 41/84 [00:20<00:21,  2.03it/s]11/28/2021 01:31:52 - INFO - __main__ -   Batch number = 42
Evaluating:  53%|█████▎    | 107/201 [00:52<00:47,  1.97it/s]11/28/2021 01:31:52 - INFO - __main__ -   Batch number = 108
Evaluating:  50%|█████     | 42/84 [00:20<00:20,  2.04it/s]11/28/2021 01:31:53 - INFO - __main__ -   Batch number = 43
Evaluating:  54%|█████▎    | 108/201 [00:52<00:47,  1.97it/s]11/28/2021 01:31:53 - INFO - __main__ -   Batch number = 109
Evaluating:  51%|█████     | 43/84 [00:21<00:20,  2.03it/s]11/28/2021 01:31:53 - INFO - __main__ -   Batch number = 44
Evaluating:  54%|█████▍    | 109/201 [00:53<00:46,  1.97it/s]11/28/2021 01:31:53 - INFO - __main__ -   Batch number = 110
Evaluating:  52%|█████▏    | 44/84 [00:21<00:19,  2.03it/s]11/28/2021 01:31:54 - INFO - __main__ -   Batch number = 45
Evaluating:  55%|█████▍    | 110/201 [00:53<00:46,  1.97it/s]11/28/2021 01:31:54 - INFO - __main__ -   Batch number = 111
Evaluating:  54%|█████▎    | 45/84 [00:22<00:19,  2.03it/s]11/28/2021 01:31:54 - INFO - __main__ -   Batch number = 46
Evaluating:  55%|█████▌    | 111/201 [00:54<00:45,  1.96it/s]11/28/2021 01:31:54 - INFO - __main__ -   Batch number = 112
Evaluating:  55%|█████▍    | 46/84 [00:22<00:18,  2.04it/s]11/28/2021 01:31:55 - INFO - __main__ -   Batch number = 47
Evaluating:  56%|█████▌    | 112/201 [00:55<00:45,  1.96it/s]11/28/2021 01:31:55 - INFO - __main__ -   Batch number = 113
Evaluating:  56%|█████▌    | 47/84 [00:23<00:17,  2.07it/s]11/28/2021 01:31:55 - INFO - __main__ -   Batch number = 48
Evaluating:  56%|█████▌    | 113/201 [00:55<00:44,  1.96it/s]11/28/2021 01:31:55 - INFO - __main__ -   Batch number = 114
Evaluating:  57%|█████▋    | 48/84 [00:23<00:17,  2.05it/s]11/28/2021 01:31:56 - INFO - __main__ -   Batch number = 49
Evaluating:  57%|█████▋    | 114/201 [00:56<00:45,  1.91it/s]11/28/2021 01:31:56 - INFO - __main__ -   Batch number = 115
Evaluating:  58%|█████▊    | 49/84 [00:24<00:18,  1.86it/s]11/28/2021 01:31:56 - INFO - __main__ -   Batch number = 50
Evaluating:  57%|█████▋    | 115/201 [00:56<00:48,  1.76it/s]11/28/2021 01:31:56 - INFO - __main__ -   Batch number = 116
Evaluating:  60%|█████▉    | 50/84 [00:25<00:19,  1.74it/s]11/28/2021 01:31:57 - INFO - __main__ -   Batch number = 51
Evaluating:  58%|█████▊    | 116/201 [00:57<00:50,  1.67it/s]11/28/2021 01:31:57 - INFO - __main__ -   Batch number = 117
Evaluating:  61%|██████    | 51/84 [00:25<00:19,  1.66it/s]11/28/2021 01:31:57 - INFO - __main__ -   Batch number = 52
Evaluating:  58%|█████▊    | 117/201 [00:58<00:52,  1.59it/s]11/28/2021 01:31:58 - INFO - __main__ -   Batch number = 118
Evaluating:  62%|██████▏   | 52/84 [00:26<00:19,  1.62it/s]11/28/2021 01:31:58 - INFO - __main__ -   Batch number = 53
Evaluating:  59%|█████▊    | 118/201 [00:58<00:53,  1.55it/s]11/28/2021 01:31:59 - INFO - __main__ -   Batch number = 119
Evaluating:  63%|██████▎   | 53/84 [00:27<00:19,  1.58it/s]11/28/2021 01:31:59 - INFO - __main__ -   Batch number = 54
Evaluating:  59%|█████▉    | 119/201 [00:59<00:53,  1.53it/s]11/28/2021 01:31:59 - INFO - __main__ -   Batch number = 120
Evaluating:  64%|██████▍   | 54/84 [00:27<00:19,  1.55it/s]11/28/2021 01:31:59 - INFO - __main__ -   Batch number = 55
Evaluating:  60%|█████▉    | 120/201 [01:00<00:53,  1.52it/s]11/28/2021 01:32:00 - INFO - __main__ -   Batch number = 121
Evaluating:  65%|██████▌   | 55/84 [00:28<00:18,  1.53it/s]11/28/2021 01:32:00 - INFO - __main__ -   Batch number = 56
Evaluating:  60%|██████    | 121/201 [01:00<00:52,  1.51it/s]11/28/2021 01:32:01 - INFO - __main__ -   Batch number = 122
Evaluating:  67%|██████▋   | 56/84 [00:29<00:18,  1.52it/s]11/28/2021 01:32:01 - INFO - __main__ -   Batch number = 57
Evaluating:  61%|██████    | 122/201 [01:01<00:52,  1.51it/s]11/28/2021 01:32:01 - INFO - __main__ -   Batch number = 123
Evaluating:  68%|██████▊   | 57/84 [00:29<00:17,  1.51it/s]11/28/2021 01:32:01 - INFO - __main__ -   Batch number = 58
Evaluating:  61%|██████    | 123/201 [01:02<00:51,  1.52it/s]11/28/2021 01:32:02 - INFO - __main__ -   Batch number = 124
Evaluating:  69%|██████▉   | 58/84 [00:30<00:17,  1.51it/s]11/28/2021 01:32:02 - INFO - __main__ -   Batch number = 59
Evaluating:  62%|██████▏   | 124/201 [01:02<00:51,  1.51it/s]11/28/2021 01:32:03 - INFO - __main__ -   Batch number = 125
Evaluating:  70%|███████   | 59/84 [00:31<00:16,  1.51it/s]11/28/2021 01:32:03 - INFO - __main__ -   Batch number = 60
Evaluating:  62%|██████▏   | 125/201 [01:03<00:50,  1.51it/s]11/28/2021 01:32:03 - INFO - __main__ -   Batch number = 126
Evaluating:  71%|███████▏  | 60/84 [00:31<00:15,  1.51it/s]11/28/2021 01:32:03 - INFO - __main__ -   Batch number = 61
Evaluating:  63%|██████▎   | 126/201 [01:04<00:49,  1.51it/s]11/28/2021 01:32:04 - INFO - __main__ -   Batch number = 127
Evaluating:  73%|███████▎  | 61/84 [00:32<00:15,  1.51it/s]11/28/2021 01:32:04 - INFO - __main__ -   Batch number = 62
Evaluating:  63%|██████▎   | 127/201 [01:04<00:49,  1.50it/s]11/28/2021 01:32:05 - INFO - __main__ -   Batch number = 128
Evaluating:  74%|███████▍  | 62/84 [00:33<00:14,  1.51it/s]11/28/2021 01:32:05 - INFO - __main__ -   Batch number = 63
Evaluating:  64%|██████▎   | 128/201 [01:05<00:48,  1.50it/s]11/28/2021 01:32:05 - INFO - __main__ -   Batch number = 129
Evaluating:  75%|███████▌  | 63/84 [00:33<00:13,  1.51it/s]11/28/2021 01:32:05 - INFO - __main__ -   Batch number = 64
Evaluating:  64%|██████▍   | 129/201 [01:06<00:47,  1.51it/s]11/28/2021 01:32:06 - INFO - __main__ -   Batch number = 130
Evaluating:  76%|███████▌  | 64/84 [00:34<00:13,  1.51it/s]11/28/2021 01:32:06 - INFO - __main__ -   Batch number = 65
Evaluating:  65%|██████▍   | 130/201 [01:06<00:47,  1.50it/s]11/28/2021 01:32:07 - INFO - __main__ -   Batch number = 131
Evaluating:  77%|███████▋  | 65/84 [00:34<00:12,  1.51it/s]11/28/2021 01:32:07 - INFO - __main__ -   Batch number = 66
Evaluating:  65%|██████▌   | 131/201 [01:07<00:47,  1.49it/s]11/28/2021 01:32:07 - INFO - __main__ -   Batch number = 132
Evaluating:  79%|███████▊  | 66/84 [00:35<00:11,  1.51it/s]11/28/2021 01:32:07 - INFO - __main__ -   Batch number = 67
Evaluating:  66%|██████▌   | 132/201 [01:08<00:46,  1.48it/s]11/28/2021 01:32:08 - INFO - __main__ -   Batch number = 133
Evaluating:  80%|███████▉  | 67/84 [00:36<00:11,  1.51it/s]11/28/2021 01:32:08 - INFO - __main__ -   Batch number = 68
Evaluating:  66%|██████▌   | 133/201 [01:08<00:45,  1.48it/s]11/28/2021 01:32:09 - INFO - __main__ -   Batch number = 134
Evaluating:  81%|████████  | 68/84 [00:36<00:10,  1.51it/s]11/28/2021 01:32:09 - INFO - __main__ -   Batch number = 69
Evaluating:  67%|██████▋   | 134/201 [01:09<00:45,  1.48it/s]11/28/2021 01:32:09 - INFO - __main__ -   Batch number = 135
Evaluating:  82%|████████▏ | 69/84 [00:37<00:09,  1.51it/s]11/28/2021 01:32:09 - INFO - __main__ -   Batch number = 70
Evaluating:  67%|██████▋   | 135/201 [01:10<00:44,  1.49it/s]11/28/2021 01:32:10 - INFO - __main__ -   Batch number = 136
Evaluating:  83%|████████▎ | 70/84 [00:38<00:09,  1.51it/s]11/28/2021 01:32:10 - INFO - __main__ -   Batch number = 71
Evaluating:  68%|██████▊   | 136/201 [01:10<00:43,  1.49it/s]11/28/2021 01:32:11 - INFO - __main__ -   Batch number = 137
Evaluating:  85%|████████▍ | 71/84 [00:38<00:08,  1.51it/s]11/28/2021 01:32:11 - INFO - __main__ -   Batch number = 72
Evaluating:  68%|██████▊   | 137/201 [01:11<00:42,  1.49it/s]11/28/2021 01:32:11 - INFO - __main__ -   Batch number = 138
Evaluating:  86%|████████▌ | 72/84 [00:39<00:07,  1.51it/s]11/28/2021 01:32:11 - INFO - __main__ -   Batch number = 73
Evaluating:  69%|██████▊   | 138/201 [01:12<00:42,  1.49it/s]11/28/2021 01:32:12 - INFO - __main__ -   Batch number = 139
Evaluating:  87%|████████▋ | 73/84 [00:40<00:07,  1.51it/s]11/28/2021 01:32:12 - INFO - __main__ -   Batch number = 74
Evaluating:  69%|██████▉   | 139/201 [01:12<00:41,  1.49it/s]11/28/2021 01:32:13 - INFO - __main__ -   Batch number = 140
Evaluating:  88%|████████▊ | 74/84 [00:40<00:06,  1.51it/s]11/28/2021 01:32:13 - INFO - __main__ -   Batch number = 75
Evaluating:  70%|██████▉   | 140/201 [01:13<00:40,  1.49it/s]11/28/2021 01:32:13 - INFO - __main__ -   Batch number = 141
Evaluating:  89%|████████▉ | 75/84 [00:41<00:05,  1.51it/s]11/28/2021 01:32:13 - INFO - __main__ -   Batch number = 76
Evaluating:  70%|███████   | 141/201 [01:14<00:40,  1.49it/s]11/28/2021 01:32:14 - INFO - __main__ -   Batch number = 142
Evaluating:  90%|█████████ | 76/84 [00:42<00:05,  1.50it/s]11/28/2021 01:32:14 - INFO - __main__ -   Batch number = 77
Evaluating:  71%|███████   | 142/201 [01:14<00:39,  1.49it/s]11/28/2021 01:32:15 - INFO - __main__ -   Batch number = 143
Evaluating:  92%|█████████▏| 77/84 [00:42<00:04,  1.50it/s]11/28/2021 01:32:15 - INFO - __main__ -   Batch number = 78
Evaluating:  71%|███████   | 143/201 [01:15<00:38,  1.50it/s]11/28/2021 01:32:15 - INFO - __main__ -   Batch number = 144
Evaluating:  93%|█████████▎| 78/84 [00:43<00:04,  1.50it/s]11/28/2021 01:32:15 - INFO - __main__ -   Batch number = 79
Evaluating:  72%|███████▏  | 144/201 [01:16<00:38,  1.49it/s]11/28/2021 01:32:16 - INFO - __main__ -   Batch number = 145
Evaluating:  94%|█████████▍| 79/84 [00:44<00:03,  1.49it/s]11/28/2021 01:32:16 - INFO - __main__ -   Batch number = 80
Evaluating:  72%|███████▏  | 145/201 [01:16<00:37,  1.49it/s]11/28/2021 01:32:17 - INFO - __main__ -   Batch number = 146
Evaluating:  95%|█████████▌| 80/84 [00:44<00:02,  1.49it/s]11/28/2021 01:32:17 - INFO - __main__ -   Batch number = 81
Evaluating:  73%|███████▎  | 146/201 [01:17<00:36,  1.49it/s]11/28/2021 01:32:17 - INFO - __main__ -   Batch number = 147
Evaluating:  96%|█████████▋| 81/84 [00:45<00:02,  1.49it/s]11/28/2021 01:32:17 - INFO - __main__ -   Batch number = 82
Evaluating:  73%|███████▎  | 147/201 [01:18<00:36,  1.48it/s]11/28/2021 01:32:18 - INFO - __main__ -   Batch number = 148
Evaluating:  98%|█████████▊| 82/84 [00:46<00:01,  1.49it/s]11/28/2021 01:32:18 - INFO - __main__ -   Batch number = 83
Evaluating:  74%|███████▎  | 148/201 [01:18<00:35,  1.48it/s]11/28/2021 01:32:19 - INFO - __main__ -   Batch number = 149
Evaluating:  99%|█████████▉| 83/84 [00:46<00:00,  1.49it/s]11/28/2021 01:32:19 - INFO - __main__ -   Batch number = 84
Evaluating:  74%|███████▍  | 149/201 [01:19<00:34,  1.51it/s]11/28/2021 01:32:19 - INFO - __main__ -   Batch number = 150
Evaluating: 100%|██████████| 84/84 [00:47<00:00,  1.58it/s]Evaluating: 100%|██████████| 84/84 [00:47<00:00,  1.77it/s]Evaluating:  75%|███████▍  | 150/201 [01:20<00:31,  1.63it/s]11/28/2021 01:32:20 - INFO - __main__ -   Batch number = 151
Evaluating:  75%|███████▌  | 151/201 [01:20<00:29,  1.72it/s]11/28/2021 01:32:20 - INFO - __main__ -   Batch number = 152
Evaluating:  76%|███████▌  | 152/201 [01:21<00:27,  1.80it/s]11/28/2021 01:32:21 - INFO - __main__ -   Batch number = 153

/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:32:21 - INFO - __main__ -   ***** Evaluation result  in pt *****
11/28/2021 01:32:21 - INFO - __main__ -     f1 = 0.872783695671124
11/28/2021 01:32:21 - INFO - __main__ -     loss = 0.4127857493502753
11/28/2021 01:32:21 - INFO - __main__ -     precision = 0.8806445370907876
11/28/2021 01:32:21 - INFO - __main__ -     recall = 0.8650619481260043
Evaluating:  76%|███████▌  | 153/201 [01:21<00:25,  1.86it/s]11/28/2021 01:32:21 - INFO - __main__ -   Batch number = 154
55.10user 20.31system 1:16.06elapsed 99%CPU (0avgtext+0avgdata 3987340maxresident)k
0inputs+688outputs (0major+1517186minor)pagefaults 0swaps
Evaluating:  77%|███████▋  | 154/201 [01:22<00:24,  1.91it/s]11/28/2021 01:32:22 - INFO - __main__ -   Batch number = 155
Evaluating:  77%|███████▋  | 155/201 [01:22<00:23,  1.92it/s]11/28/2021 01:32:22 - INFO - __main__ -   Batch number = 156
Evaluating:  78%|███████▊  | 156/201 [01:22<00:21,  2.11it/s]11/28/2021 01:32:23 - INFO - __main__ -   Batch number = 157
Evaluating:  78%|███████▊  | 157/201 [01:23<00:19,  2.30it/s]11/28/2021 01:32:23 - INFO - __main__ -   Batch number = 158
PyTorch version 1.10.0+cu102 available.
Evaluating:  79%|███████▊  | 158/201 [01:23<00:17,  2.47it/s]11/28/2021 01:32:23 - INFO - __main__ -   Batch number = 159
11/28/2021 01:32:24 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='pt', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:32:24 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:32:24 - INFO - __main__ -   Seed = 2
11/28/2021 01:32:24 - INFO - root -   save model
11/28/2021 01:32:24 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='pt', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:32:24 - INFO - __main__ -   Loading pretrained model and tokenizer
Evaluating:  79%|███████▉  | 159/201 [01:23<00:16,  2.57it/s]11/28/2021 01:32:24 - INFO - __main__ -   Batch number = 160
Evaluating:  80%|███████▉  | 160/201 [01:24<00:15,  2.68it/s]11/28/2021 01:32:24 - INFO - __main__ -   Batch number = 161
Evaluating:  80%|████████  | 161/201 [01:24<00:14,  2.76it/s]11/28/2021 01:32:24 - INFO - __main__ -   Batch number = 162
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  81%|████████  | 162/201 [01:25<00:15,  2.58it/s]11/28/2021 01:32:25 - INFO - __main__ -   Batch number = 163
Evaluating:  81%|████████  | 163/201 [01:25<00:14,  2.67it/s]11/28/2021 01:32:25 - INFO - __main__ -   Batch number = 164
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  82%|████████▏ | 164/201 [01:25<00:13,  2.71it/s]11/28/2021 01:32:25 - INFO - __main__ -   Batch number = 165
Evaluating:  82%|████████▏ | 165/201 [01:26<00:13,  2.76it/s]11/28/2021 01:32:26 - INFO - __main__ -   Batch number = 166
Evaluating:  83%|████████▎ | 166/201 [01:26<00:12,  2.81it/s]11/28/2021 01:32:26 - INFO - __main__ -   Batch number = 167
loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:32:26 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
Evaluating:  83%|████████▎ | 167/201 [01:26<00:11,  2.84it/s]11/28/2021 01:32:27 - INFO - __main__ -   Batch number = 168
Evaluating:  84%|████████▎ | 168/201 [01:27<00:11,  2.88it/s]11/28/2021 01:32:27 - INFO - __main__ -   Batch number = 169
Evaluating:  84%|████████▍ | 169/201 [01:27<00:10,  3.03it/s]11/28/2021 01:32:27 - INFO - __main__ -   Batch number = 170
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  85%|████████▍ | 170/201 [01:27<00:08,  3.55it/s]11/28/2021 01:32:27 - INFO - __main__ -   Batch number = 171
Evaluating:  85%|████████▌ | 171/201 [01:27<00:07,  4.01it/s]11/28/2021 01:32:27 - INFO - __main__ -   Batch number = 172
Evaluating:  86%|████████▌ | 172/201 [01:27<00:06,  4.46it/s]11/28/2021 01:32:28 - INFO - __main__ -   Batch number = 173
Evaluating:  86%|████████▌ | 173/201 [01:28<00:05,  4.79it/s]11/28/2021 01:32:28 - INFO - __main__ -   Batch number = 174
Evaluating:  87%|████████▋ | 174/201 [01:28<00:05,  5.16it/s]11/28/2021 01:32:28 - INFO - __main__ -   Batch number = 175
Evaluating:  87%|████████▋ | 175/201 [01:28<00:04,  5.40it/s]11/28/2021 01:32:28 - INFO - __main__ -   Batch number = 176
Evaluating:  88%|████████▊ | 176/201 [01:28<00:04,  5.68it/s]11/28/2021 01:32:28 - INFO - __main__ -   Batch number = 177
Evaluating:  88%|████████▊ | 177/201 [01:28<00:04,  5.68it/s]11/28/2021 01:32:28 - INFO - __main__ -   Batch number = 178
Evaluating:  89%|████████▊ | 178/201 [01:28<00:03,  5.77it/s]11/28/2021 01:32:29 - INFO - __main__ -   Batch number = 179
Evaluating:  89%|████████▉ | 179/201 [01:29<00:03,  5.80it/s]11/28/2021 01:32:29 - INFO - __main__ -   Batch number = 180
Evaluating:  90%|████████▉ | 180/201 [01:29<00:03,  5.92it/s]11/28/2021 01:32:29 - INFO - __main__ -   Batch number = 181
Evaluating:  90%|█████████ | 181/201 [01:29<00:03,  5.92it/s]11/28/2021 01:32:29 - INFO - __main__ -   Batch number = 182
Evaluating:  91%|█████████ | 182/201 [01:29<00:03,  5.97it/s]11/28/2021 01:32:29 - INFO - __main__ -   Batch number = 183
Evaluating:  91%|█████████ | 183/201 [01:29<00:03,  5.93it/s]11/28/2021 01:32:29 - INFO - __main__ -   Batch number = 184
Evaluating:  92%|█████████▏| 184/201 [01:29<00:02,  6.07it/s]11/28/2021 01:32:30 - INFO - __main__ -   Batch number = 185
Evaluating:  92%|█████████▏| 185/201 [01:30<00:02,  6.09it/s]11/28/2021 01:32:30 - INFO - __main__ -   Batch number = 186
Evaluating:  93%|█████████▎| 186/201 [01:30<00:02,  6.20it/s]11/28/2021 01:32:30 - INFO - __main__ -   Batch number = 187
Evaluating:  93%|█████████▎| 187/201 [01:30<00:02,  6.08it/s]11/28/2021 01:32:30 - INFO - __main__ -   Batch number = 188
Evaluating:  94%|█████████▎| 188/201 [01:30<00:02,  6.13it/s]11/28/2021 01:32:30 - INFO - __main__ -   Batch number = 189
Evaluating:  94%|█████████▍| 189/201 [01:30<00:01,  6.09it/s]11/28/2021 01:32:30 - INFO - __main__ -   Batch number = 190
Evaluating:  95%|█████████▍| 190/201 [01:30<00:01,  6.16it/s]11/28/2021 01:32:31 - INFO - __main__ -   Batch number = 191
Evaluating:  95%|█████████▌| 191/201 [01:31<00:01,  5.99it/s]11/28/2021 01:32:31 - INFO - __main__ -   Batch number = 192
Evaluating:  96%|█████████▌| 192/201 [01:31<00:01,  5.99it/s]11/28/2021 01:32:31 - INFO - __main__ -   Batch number = 193
Evaluating:  96%|█████████▌| 193/201 [01:31<00:01,  5.83it/s]11/28/2021 01:32:31 - INFO - __main__ -   Batch number = 194
Evaluating:  97%|█████████▋| 194/201 [01:31<00:01,  5.87it/s]11/28/2021 01:32:31 - INFO - __main__ -   Batch number = 195
Evaluating:  97%|█████████▋| 195/201 [01:31<00:01,  5.81it/s]11/28/2021 01:32:31 - INFO - __main__ -   Batch number = 196
Evaluating:  98%|█████████▊| 196/201 [01:31<00:00,  5.90it/s]11/28/2021 01:32:32 - INFO - __main__ -   Batch number = 197
Evaluating:  98%|█████████▊| 197/201 [01:32<00:00,  5.93it/s]11/28/2021 01:32:32 - INFO - __main__ -   Batch number = 198
Evaluating:  99%|█████████▊| 198/201 [01:32<00:00,  5.96it/s]11/28/2021 01:32:32 - INFO - __main__ -   Batch number = 199
Evaluating:  99%|█████████▉| 199/201 [01:32<00:00,  5.92it/s]11/28/2021 01:32:32 - INFO - __main__ -   Batch number = 200
Evaluating: 100%|█████████▉| 200/201 [01:32<00:00,  5.99it/s]11/28/2021 01:32:32 - INFO - __main__ -   Batch number = 201
Evaluating: 100%|██████████| 201/201 [01:32<00:00,  2.17it/s]Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:32:32 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:32:32 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:32:32 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
11/28/2021 01:32:32 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:32:32 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:32:32 - INFO - root -   loading lang adpater bh/wiki@ukp
11/28/2021 01:32:32 - INFO - __main__ -   Adapter Languages : ['bh'], Length : 1
11/28/2021 01:32:32 - INFO - __main__ -   Adapter Names ['bh/wiki@ukp'], Length : 1
11/28/2021 01:32:32 - INFO - __main__ -   Language = bh
11/28/2021 01:32:32 - INFO - __main__ -   Adapter Name = bh/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/bh/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_bh_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/adapter_config.json
Adding adapter 'bh' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/head_config.json

/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:32:36 - INFO - __main__ -   ***** Evaluation result  in is *****
11/28/2021 01:32:36 - INFO - __main__ -     f1 = 0.7121519528017299
11/28/2021 01:32:36 - INFO - __main__ -     loss = 0.9775671113782854
11/28/2021 01:32:36 - INFO - __main__ -     precision = 0.7185734391940578
11/28/2021 01:32:36 - INFO - __main__ -     recall = 0.7058442202745163
90.13user 31.69system 2:00.42elapsed 101%CPU (0avgtext+0avgdata 3997448maxresident)k
0inputs+1712outputs (0major+1790152minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:32:39 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='is', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:32:39 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:32:39 - INFO - __main__ -   Seed = 3
11/28/2021 01:32:39 - INFO - root -   save model
11/28/2021 01:32:39 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='is', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:32:39 - INFO - __main__ -   Loading pretrained model and tokenizer
11/28/2021 01:32:39 - INFO - __main__ -   Language adapter for pt not found, using bh instead
11/28/2021 01:32:39 - INFO - __main__ -   Set active language adapter to bh
11/28/2021 01:32:39 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:32:39 - INFO - __main__ -   Adapter Languages = ['bh']
11/28/2021 01:32:39 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_pt_bert-base-multilingual-cased_128
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

11/28/2021 01:32:40 - INFO - __main__ -   ***** Running evaluation  in pt *****
11/28/2021 01:32:40 - INFO - __main__ -     Num examples = 2682
11/28/2021 01:32:40 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/84 [00:00<?, ?it/s]11/28/2021 01:32:40 - INFO - __main__ -   Batch number = 1
Evaluating:   1%|          | 1/84 [00:00<00:14,  5.92it/s]11/28/2021 01:32:40 - INFO - __main__ -   Batch number = 2
Evaluating:   2%|▏         | 2/84 [00:00<00:12,  6.32it/s]11/28/2021 01:32:40 - INFO - __main__ -   Batch number = 3
Evaluating:   4%|▎         | 3/84 [00:00<00:12,  6.44it/s]11/28/2021 01:32:41 - INFO - __main__ -   Batch number = 4
Evaluating:   5%|▍         | 4/84 [00:00<00:12,  6.56it/s]11/28/2021 01:32:41 - INFO - __main__ -   Batch number = 5
Evaluating:   6%|▌         | 5/84 [00:00<00:12,  6.49it/s]11/28/2021 01:32:41 - INFO - __main__ -   Batch number = 6
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:   7%|▋         | 6/84 [00:00<00:12,  6.41it/s]11/28/2021 01:32:41 - INFO - __main__ -   Batch number = 7
Evaluating:   8%|▊         | 7/84 [00:01<00:12,  6.39it/s]11/28/2021 01:32:41 - INFO - __main__ -   Batch number = 8
Evaluating:  10%|▉         | 8/84 [00:01<00:11,  6.40it/s]11/28/2021 01:32:41 - INFO - __main__ -   Batch number = 9
Evaluating:  11%|█         | 9/84 [00:01<00:11,  6.46it/s]11/28/2021 01:32:41 - INFO - __main__ -   Batch number = 10
Evaluating:  12%|█▏        | 10/84 [00:01<00:11,  6.43it/s]11/28/2021 01:32:42 - INFO - __main__ -   Batch number = 11
Evaluating:  13%|█▎        | 11/84 [00:01<00:11,  6.44it/s]11/28/2021 01:32:42 - INFO - __main__ -   Batch number = 12
loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
Evaluating:  14%|█▍        | 12/84 [00:01<00:11,  6.45it/s]11/28/2021 01:32:42 - INFO - __main__ -   Batch number = 13
11/28/2021 01:32:42 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
Evaluating:  15%|█▌        | 13/84 [00:02<00:11,  6.39it/s]11/28/2021 01:32:42 - INFO - __main__ -   Batch number = 14
Evaluating:  17%|█▋        | 14/84 [00:02<00:10,  6.40it/s]11/28/2021 01:32:42 - INFO - __main__ -   Batch number = 15
Evaluating:  18%|█▊        | 15/84 [00:02<00:10,  6.43it/s]11/28/2021 01:32:42 - INFO - __main__ -   Batch number = 16
Evaluating:  19%|█▉        | 16/84 [00:02<00:10,  6.39it/s]11/28/2021 01:32:43 - INFO - __main__ -   Batch number = 17
Evaluating:  20%|██        | 17/84 [00:02<00:10,  6.34it/s]11/28/2021 01:32:43 - INFO - __main__ -   Batch number = 18
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  21%|██▏       | 18/84 [00:02<00:10,  6.33it/s]11/28/2021 01:32:43 - INFO - __main__ -   Batch number = 19
Evaluating:  23%|██▎       | 19/84 [00:02<00:10,  6.26it/s]11/28/2021 01:32:43 - INFO - __main__ -   Batch number = 20
Evaluating:  24%|██▍       | 20/84 [00:03<00:10,  6.20it/s]11/28/2021 01:32:43 - INFO - __main__ -   Batch number = 21
Evaluating:  25%|██▌       | 21/84 [00:03<00:12,  5.23it/s]11/28/2021 01:32:43 - INFO - __main__ -   Batch number = 22
Evaluating:  26%|██▌       | 22/84 [00:03<00:14,  4.30it/s]11/28/2021 01:32:44 - INFO - __main__ -   Batch number = 23
Evaluating:  27%|██▋       | 23/84 [00:04<00:15,  3.82it/s]11/28/2021 01:32:44 - INFO - __main__ -   Batch number = 24
Evaluating:  29%|██▊       | 24/84 [00:04<00:17,  3.52it/s]11/28/2021 01:32:45 - INFO - __main__ -   Batch number = 25
Evaluating:  30%|██▉       | 25/84 [00:04<00:20,  2.91it/s]11/28/2021 01:32:45 - INFO - __main__ -   Batch number = 26
Evaluating:  31%|███       | 26/84 [00:05<00:19,  2.95it/s]11/28/2021 01:32:45 - INFO - __main__ -   Batch number = 27
Evaluating:  32%|███▏      | 27/84 [00:05<00:19,  2.96it/s]11/28/2021 01:32:46 - INFO - __main__ -   Batch number = 28
Evaluating:  33%|███▎      | 28/84 [00:05<00:18,  2.97it/s]11/28/2021 01:32:46 - INFO - __main__ -   Batch number = 29
Evaluating:  35%|███▍      | 29/84 [00:06<00:18,  2.98it/s]11/28/2021 01:32:46 - INFO - __main__ -   Batch number = 30
Evaluating:  36%|███▌      | 30/84 [00:06<00:18,  2.98it/s]11/28/2021 01:32:47 - INFO - __main__ -   Batch number = 31
Evaluating:  37%|███▋      | 31/84 [00:06<00:17,  3.00it/s]11/28/2021 01:32:47 - INFO - __main__ -   Batch number = 32
Evaluating:  38%|███▊      | 32/84 [00:07<00:17,  2.99it/s]11/28/2021 01:32:47 - INFO - __main__ -   Batch number = 33
Evaluating:  39%|███▉      | 33/84 [00:07<00:16,  3.01it/s]11/28/2021 01:32:48 - INFO - __main__ -   Batch number = 34
Evaluating:  40%|████      | 34/84 [00:07<00:16,  3.00it/s]11/28/2021 01:32:48 - INFO - __main__ -   Batch number = 35
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:32:48 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:32:48 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:32:48 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
11/28/2021 01:32:48 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:32:48 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:32:48 - INFO - root -   loading lang adpater bh/wiki@ukp
11/28/2021 01:32:48 - INFO - __main__ -   Adapter Languages : ['bh'], Length : 1
11/28/2021 01:32:48 - INFO - __main__ -   Adapter Names ['bh/wiki@ukp'], Length : 1
11/28/2021 01:32:48 - INFO - __main__ -   Language = bh
11/28/2021 01:32:48 - INFO - __main__ -   Adapter Name = bh/wiki@ukp
Evaluating:  42%|████▏     | 35/84 [00:08<00:16,  2.98it/s]11/28/2021 01:32:48 - INFO - __main__ -   Batch number = 36
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/bh/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_bh_wiki_pfeiffer.zip.
Evaluating:  43%|████▎     | 36/84 [00:08<00:16,  2.98it/s]11/28/2021 01:32:49 - INFO - __main__ -   Batch number = 37
Evaluating:  44%|████▍     | 37/84 [00:08<00:15,  2.97it/s]11/28/2021 01:32:49 - INFO - __main__ -   Batch number = 38
Evaluating:  45%|████▌     | 38/84 [00:09<00:15,  2.98it/s]11/28/2021 01:32:49 - INFO - __main__ -   Batch number = 39
Evaluating:  46%|████▋     | 39/84 [00:09<00:15,  2.99it/s]11/28/2021 01:32:50 - INFO - __main__ -   Batch number = 40
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/adapter_config.json
Adding adapter 'bh' of type 'text_lang'.
Evaluating:  48%|████▊     | 40/84 [00:09<00:14,  2.99it/s]11/28/2021 01:32:50 - INFO - __main__ -   Batch number = 41
Loading module weights from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/head_config.json
Evaluating:  49%|████▉     | 41/84 [00:10<00:14,  3.01it/s]11/28/2021 01:32:50 - INFO - __main__ -   Batch number = 42
Evaluating:  50%|█████     | 42/84 [00:10<00:14,  3.00it/s]11/28/2021 01:32:51 - INFO - __main__ -   Batch number = 43
Evaluating:  51%|█████     | 43/84 [00:10<00:13,  2.99it/s]11/28/2021 01:32:51 - INFO - __main__ -   Batch number = 44
Evaluating:  52%|█████▏    | 44/84 [00:11<00:13,  2.98it/s]11/28/2021 01:32:51 - INFO - __main__ -   Batch number = 45
Evaluating:  54%|█████▎    | 45/84 [00:11<00:13,  2.98it/s]11/28/2021 01:32:52 - INFO - __main__ -   Batch number = 46
Evaluating:  55%|█████▍    | 46/84 [00:11<00:12,  2.98it/s]11/28/2021 01:32:52 - INFO - __main__ -   Batch number = 47
Evaluating:  56%|█████▌    | 47/84 [00:12<00:14,  2.59it/s]11/28/2021 01:32:52 - INFO - __main__ -   Batch number = 48
Evaluating:  57%|█████▋    | 48/84 [00:12<00:15,  2.38it/s]11/28/2021 01:32:53 - INFO - __main__ -   Batch number = 49
Evaluating:  58%|█████▊    | 49/84 [00:13<00:15,  2.24it/s]11/28/2021 01:32:53 - INFO - __main__ -   Batch number = 50
Evaluating:  60%|█████▉    | 50/84 [00:13<00:15,  2.16it/s]11/28/2021 01:32:54 - INFO - __main__ -   Batch number = 51
Evaluating:  61%|██████    | 51/84 [00:14<00:15,  2.10it/s]11/28/2021 01:32:54 - INFO - __main__ -   Batch number = 52
Evaluating:  62%|██████▏   | 52/84 [00:14<00:15,  2.07it/s]11/28/2021 01:32:55 - INFO - __main__ -   Batch number = 53
Evaluating:  63%|██████▎   | 53/84 [00:15<00:15,  2.05it/s]11/28/2021 01:32:55 - INFO - __main__ -   Batch number = 54
Evaluating:  64%|██████▍   | 54/84 [00:15<00:14,  2.03it/s]11/28/2021 01:32:56 - INFO - __main__ -   Batch number = 55
Evaluating:  65%|██████▌   | 55/84 [00:16<00:14,  2.00it/s]11/28/2021 01:32:56 - INFO - __main__ -   Batch number = 56
Evaluating:  67%|██████▋   | 56/84 [00:16<00:14,  1.99it/s]11/28/2021 01:32:57 - INFO - __main__ -   Batch number = 57
Evaluating:  68%|██████▊   | 57/84 [00:17<00:13,  1.98it/s]11/28/2021 01:32:58 - INFO - __main__ -   Batch number = 58
Evaluating:  69%|██████▉   | 58/84 [00:17<00:13,  1.97it/s]11/28/2021 01:32:58 - INFO - __main__ -   Batch number = 59
Evaluating:  70%|███████   | 59/84 [00:18<00:12,  1.98it/s]11/28/2021 01:32:59 - INFO - __main__ -   Batch number = 60
Evaluating:  71%|███████▏  | 60/84 [00:18<00:12,  1.98it/s]11/28/2021 01:32:59 - INFO - __main__ -   Batch number = 61
Evaluating:  73%|███████▎  | 61/84 [00:19<00:11,  1.97it/s]11/28/2021 01:33:00 - INFO - __main__ -   Batch number = 62
Evaluating:  74%|███████▍  | 62/84 [00:19<00:11,  1.98it/s]11/28/2021 01:33:00 - INFO - __main__ -   Batch number = 63
Evaluating:  75%|███████▌  | 63/84 [00:20<00:10,  1.99it/s]11/28/2021 01:33:01 - INFO - __main__ -   Batch number = 64
Evaluating:  76%|███████▌  | 64/84 [00:20<00:10,  1.99it/s]11/28/2021 01:33:01 - INFO - __main__ -   Batch number = 65
11/28/2021 01:33:01 - INFO - __main__ -   Language adapter for is not found, using bh instead
11/28/2021 01:33:01 - INFO - __main__ -   Set active language adapter to bh
11/28/2021 01:33:01 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:33:01 - INFO - __main__ -   Adapter Languages = ['bh']
11/28/2021 01:33:01 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_is_bert-base-multilingual-cased_128
Evaluating:  77%|███████▋  | 65/84 [00:21<00:09,  1.99it/s]11/28/2021 01:33:02 - INFO - __main__ -   Batch number = 66
Evaluating:  79%|███████▊  | 66/84 [00:21<00:09,  2.00it/s]11/28/2021 01:33:02 - INFO - __main__ -   Batch number = 67
11/28/2021 01:33:02 - INFO - __main__ -   ***** Running evaluation  in is *****
11/28/2021 01:33:02 - INFO - __main__ -     Num examples = 6401
11/28/2021 01:33:02 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/201 [00:00<?, ?it/s]11/28/2021 01:33:02 - INFO - __main__ -   Batch number = 1
Evaluating:  80%|███████▉  | 67/84 [00:22<00:08,  1.99it/s]11/28/2021 01:33:03 - INFO - __main__ -   Batch number = 68
Evaluating:   0%|          | 1/201 [00:00<02:13,  1.50it/s]11/28/2021 01:33:03 - INFO - __main__ -   Batch number = 2
Evaluating:  81%|████████  | 68/84 [00:23<00:08,  1.82it/s]11/28/2021 01:33:03 - INFO - __main__ -   Batch number = 69
Evaluating:   1%|          | 2/201 [00:01<02:12,  1.50it/s]11/28/2021 01:33:04 - INFO - __main__ -   Batch number = 3
Evaluating:  82%|████████▏ | 69/84 [00:23<00:08,  1.72it/s]11/28/2021 01:33:04 - INFO - __main__ -   Batch number = 70
Evaluating:   1%|▏         | 3/201 [00:01<02:10,  1.52it/s]11/28/2021 01:33:04 - INFO - __main__ -   Batch number = 4
Evaluating:  83%|████████▎ | 70/84 [00:24<00:08,  1.66it/s]11/28/2021 01:33:05 - INFO - __main__ -   Batch number = 71
PyTorch version 1.10.0+cu102 available.
Evaluating:   2%|▏         | 4/201 [00:02<02:09,  1.52it/s]11/28/2021 01:33:05 - INFO - __main__ -   Batch number = 5
Evaluating:  85%|████████▍ | 71/84 [00:25<00:08,  1.61it/s]11/28/2021 01:33:05 - INFO - __main__ -   Batch number = 72
11/28/2021 01:33:05 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='zh', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:33:05 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:33:05 - INFO - __main__ -   Seed = 1
11/28/2021 01:33:05 - INFO - root -   save model
11/28/2021 01:33:05 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='zh', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:33:05 - INFO - __main__ -   Loading pretrained model and tokenizer
Evaluating:   2%|▏         | 5/201 [00:03<02:09,  1.51it/s]11/28/2021 01:33:06 - INFO - __main__ -   Batch number = 6
Evaluating:  86%|████████▌ | 72/84 [00:25<00:07,  1.58it/s]11/28/2021 01:33:06 - INFO - __main__ -   Batch number = 73
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:   3%|▎         | 6/201 [00:03<02:09,  1.51it/s]11/28/2021 01:33:06 - INFO - __main__ -   Batch number = 7
Evaluating:  87%|████████▋ | 73/84 [00:26<00:07,  1.55it/s]11/28/2021 01:33:07 - INFO - __main__ -   Batch number = 74
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:   3%|▎         | 7/201 [00:04<02:08,  1.51it/s]11/28/2021 01:33:07 - INFO - __main__ -   Batch number = 8
Evaluating:  88%|████████▊ | 74/84 [00:27<00:06,  1.54it/s]11/28/2021 01:33:07 - INFO - __main__ -   Batch number = 75
loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
Evaluating:   4%|▍         | 8/201 [00:05<02:07,  1.51it/s]11/28/2021 01:33:08 - INFO - __main__ -   Batch number = 9
Evaluating:  89%|████████▉ | 75/84 [00:27<00:05,  1.52it/s]11/28/2021 01:33:08 - INFO - __main__ -   Batch number = 76
11/28/2021 01:33:08 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
Evaluating:   4%|▍         | 9/201 [00:05<02:07,  1.51it/s]11/28/2021 01:33:08 - INFO - __main__ -   Batch number = 10
Evaluating:  90%|█████████ | 76/84 [00:28<00:05,  1.52it/s]11/28/2021 01:33:08 - INFO - __main__ -   Batch number = 77
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:   5%|▍         | 10/201 [00:06<02:06,  1.51it/s]11/28/2021 01:33:09 - INFO - __main__ -   Batch number = 11
Evaluating:  92%|█████████▏| 77/84 [00:29<00:04,  1.51it/s]11/28/2021 01:33:09 - INFO - __main__ -   Batch number = 78
Evaluating:   5%|▌         | 11/201 [00:07<02:06,  1.51it/s]11/28/2021 01:33:10 - INFO - __main__ -   Batch number = 12
Evaluating:  93%|█████████▎| 78/84 [00:29<00:03,  1.51it/s]11/28/2021 01:33:10 - INFO - __main__ -   Batch number = 79
Evaluating:   6%|▌         | 12/201 [00:07<02:05,  1.51it/s]11/28/2021 01:33:10 - INFO - __main__ -   Batch number = 13
Evaluating:  94%|█████████▍| 79/84 [00:30<00:03,  1.50it/s]11/28/2021 01:33:11 - INFO - __main__ -   Batch number = 80
Evaluating:   6%|▋         | 13/201 [00:08<02:04,  1.51it/s]11/28/2021 01:33:11 - INFO - __main__ -   Batch number = 14
Evaluating:  95%|█████████▌| 80/84 [00:31<00:02,  1.48it/s]11/28/2021 01:33:11 - INFO - __main__ -   Batch number = 81
Evaluating:   7%|▋         | 14/201 [00:09<02:03,  1.52it/s]11/28/2021 01:33:12 - INFO - __main__ -   Batch number = 15
Evaluating:  96%|█████████▋| 81/84 [00:31<00:02,  1.48it/s]11/28/2021 01:33:12 - INFO - __main__ -   Batch number = 82
Evaluating:   7%|▋         | 15/201 [00:09<02:02,  1.52it/s]11/28/2021 01:33:12 - INFO - __main__ -   Batch number = 16
Evaluating:  98%|█████████▊| 82/84 [00:32<00:01,  1.48it/s]11/28/2021 01:33:13 - INFO - __main__ -   Batch number = 83
Evaluating:   8%|▊         | 16/201 [00:10<02:01,  1.52it/s]11/28/2021 01:33:13 - INFO - __main__ -   Batch number = 17
Evaluating:  99%|█████████▉| 83/84 [00:33<00:00,  1.49it/s]11/28/2021 01:33:13 - INFO - __main__ -   Batch number = 84
Evaluating:   8%|▊         | 17/201 [00:11<02:01,  1.52it/s]11/28/2021 01:33:14 - INFO - __main__ -   Batch number = 18
Evaluating: 100%|██████████| 84/84 [00:33<00:00,  1.57it/s]Evaluating: 100%|██████████| 84/84 [00:33<00:00,  2.49it/s]Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:33:14 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:33:14 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:33:14 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
11/28/2021 01:33:14 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:33:14 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:33:14 - INFO - root -   loading lang adpater bh/wiki@ukp
11/28/2021 01:33:14 - INFO - __main__ -   Adapter Languages : ['bh'], Length : 1
11/28/2021 01:33:14 - INFO - __main__ -   Adapter Names ['bh/wiki@ukp'], Length : 1
11/28/2021 01:33:14 - INFO - __main__ -   Language = bh
11/28/2021 01:33:14 - INFO - __main__ -   Adapter Name = bh/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/bh/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_bh_wiki_pfeiffer.zip.
Evaluating:   9%|▉         | 18/201 [00:11<01:52,  1.63it/s]11/28/2021 01:33:14 - INFO - __main__ -   Batch number = 19
Evaluating:   9%|▉         | 19/201 [00:12<01:42,  1.78it/s]11/28/2021 01:33:15 - INFO - __main__ -   Batch number = 20
Evaluating:  10%|▉         | 20/201 [00:12<01:38,  1.84it/s]11/28/2021 01:33:15 - INFO - __main__ -   Batch number = 21

/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:33:15 - INFO - __main__ -   ***** Evaluation result  in pt *****
11/28/2021 01:33:15 - INFO - __main__ -     f1 = 0.8695826575280351
11/28/2021 01:33:15 - INFO - __main__ -     loss = 0.43682057836226057
11/28/2021 01:33:15 - INFO - __main__ -     precision = 0.8777263365430317
11/28/2021 01:33:15 - INFO - __main__ -     recall = 0.8615887059148797
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/adapter_config.json
Adding adapter 'bh' of type 'text_lang'.
Evaluating:  10%|█         | 21/201 [00:13<01:35,  1.89it/s]11/28/2021 01:33:16 - INFO - __main__ -   Batch number = 22
Loading module weights from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/head_config.json
Evaluating:  11%|█         | 22/201 [00:13<01:33,  1.92it/s]11/28/2021 01:33:16 - INFO - __main__ -   Batch number = 23
41.34user 14.22system 0:54.66elapsed 101%CPU (0avgtext+0avgdata 3998336maxresident)k
0inputs+688outputs (0major+1579302minor)pagefaults 0swaps
Evaluating:  11%|█▏        | 23/201 [00:14<01:31,  1.95it/s]11/28/2021 01:33:17 - INFO - __main__ -   Batch number = 24
Evaluating:  12%|█▏        | 24/201 [00:14<01:29,  1.97it/s]11/28/2021 01:33:17 - INFO - __main__ -   Batch number = 25
Evaluating:  12%|█▏        | 25/201 [00:15<01:28,  1.98it/s]11/28/2021 01:33:18 - INFO - __main__ -   Batch number = 26
PyTorch version 1.10.0+cu102 available.
Evaluating:  13%|█▎        | 26/201 [00:15<01:27,  1.99it/s]11/28/2021 01:33:18 - INFO - __main__ -   Batch number = 27
11/28/2021 01:33:18 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='pt', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:33:18 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:33:18 - INFO - __main__ -   Seed = 3
11/28/2021 01:33:18 - INFO - root -   save model
11/28/2021 01:33:18 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='pt', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:33:18 - INFO - __main__ -   Loading pretrained model and tokenizer
Evaluating:  13%|█▎        | 27/201 [00:16<01:26,  2.01it/s]11/28/2021 01:33:19 - INFO - __main__ -   Batch number = 28
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  14%|█▍        | 28/201 [00:16<01:26,  2.01it/s]11/28/2021 01:33:19 - INFO - __main__ -   Batch number = 29
Evaluating:  14%|█▍        | 29/201 [00:17<01:24,  2.04it/s]11/28/2021 01:33:20 - INFO - __main__ -   Batch number = 30
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  15%|█▍        | 30/201 [00:17<01:23,  2.04it/s]11/28/2021 01:33:20 - INFO - __main__ -   Batch number = 31
Evaluating:  15%|█▌        | 31/201 [00:18<01:23,  2.03it/s]11/28/2021 01:33:21 - INFO - __main__ -   Batch number = 32
loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:33:21 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
Evaluating:  16%|█▌        | 32/201 [00:18<01:23,  2.03it/s]11/28/2021 01:33:21 - INFO - __main__ -   Batch number = 33
Evaluating:  16%|█▋        | 33/201 [00:19<01:22,  2.03it/s]11/28/2021 01:33:22 - INFO - __main__ -   Batch number = 34
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  17%|█▋        | 34/201 [00:19<01:22,  2.03it/s]11/28/2021 01:33:22 - INFO - __main__ -   Batch number = 35
Evaluating:  17%|█▋        | 35/201 [00:20<01:22,  2.02it/s]11/28/2021 01:33:23 - INFO - __main__ -   Batch number = 36
Evaluating:  18%|█▊        | 36/201 [00:20<01:16,  2.16it/s]11/28/2021 01:33:23 - INFO - __main__ -   Batch number = 37
Evaluating:  18%|█▊        | 37/201 [00:20<01:09,  2.36it/s]11/28/2021 01:33:23 - INFO - __main__ -   Batch number = 38
Evaluating:  19%|█▉        | 38/201 [00:21<01:04,  2.53it/s]11/28/2021 01:33:24 - INFO - __main__ -   Batch number = 39
Evaluating:  19%|█▉        | 39/201 [00:21<01:01,  2.65it/s]11/28/2021 01:33:24 - INFO - __main__ -   Batch number = 40
Evaluating:  20%|█▉        | 40/201 [00:21<00:58,  2.75it/s]11/28/2021 01:33:24 - INFO - __main__ -   Batch number = 41
Evaluating:  20%|██        | 41/201 [00:22<00:56,  2.83it/s]11/28/2021 01:33:25 - INFO - __main__ -   Batch number = 42
Evaluating:  21%|██        | 42/201 [00:22<00:54,  2.89it/s]11/28/2021 01:33:25 - INFO - __main__ -   Batch number = 43
Evaluating:  21%|██▏       | 43/201 [00:22<00:53,  2.97it/s]11/28/2021 01:33:25 - INFO - __main__ -   Batch number = 44
Evaluating:  22%|██▏       | 44/201 [00:23<00:52,  2.98it/s]11/28/2021 01:33:26 - INFO - __main__ -   Batch number = 45
Evaluating:  22%|██▏       | 45/201 [00:23<00:51,  3.01it/s]11/28/2021 01:33:26 - INFO - __main__ -   Batch number = 46
Evaluating:  23%|██▎       | 46/201 [00:23<00:50,  3.04it/s]11/28/2021 01:33:26 - INFO - __main__ -   Batch number = 47
Evaluating:  23%|██▎       | 47/201 [00:24<00:50,  3.05it/s]11/28/2021 01:33:27 - INFO - __main__ -   Batch number = 48
Evaluating:  24%|██▍       | 48/201 [00:24<00:50,  3.05it/s]11/28/2021 01:33:27 - INFO - __main__ -   Batch number = 49
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:33:27 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:33:27 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:33:27 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
11/28/2021 01:33:27 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:33:27 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:33:27 - INFO - root -   loading lang adpater bh/wiki@ukp
11/28/2021 01:33:27 - INFO - __main__ -   Adapter Languages : ['bh'], Length : 1
11/28/2021 01:33:27 - INFO - __main__ -   Adapter Names ['bh/wiki@ukp'], Length : 1
11/28/2021 01:33:27 - INFO - __main__ -   Language = bh
11/28/2021 01:33:27 - INFO - __main__ -   Adapter Name = bh/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Evaluating:  24%|██▍       | 49/201 [00:24<00:50,  3.04it/s]11/28/2021 01:33:27 - INFO - __main__ -   Batch number = 50
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/bh/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_bh_wiki_pfeiffer.zip.
11/28/2021 01:33:27 - INFO - __main__ -   Language adapter for zh not found, using bh instead
11/28/2021 01:33:27 - INFO - __main__ -   Set active language adapter to bh
11/28/2021 01:33:27 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:33:27 - INFO - __main__ -   Adapter Languages = ['bh']
11/28/2021 01:33:27 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_zh_bert-base-multilingual-cased_128
Evaluating:  25%|██▍       | 50/201 [00:25<00:49,  3.05it/s]11/28/2021 01:33:28 - INFO - __main__ -   Batch number = 51
Evaluating:  25%|██▌       | 51/201 [00:25<00:48,  3.09it/s]11/28/2021 01:33:28 - INFO - __main__ -   Batch number = 52
11/28/2021 01:33:28 - INFO - __main__ -   ***** Running evaluation  in zh *****
11/28/2021 01:33:28 - INFO - __main__ -     Num examples = 3458
11/28/2021 01:33:28 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/109 [00:00<?, ?it/s]11/28/2021 01:33:28 - INFO - __main__ -   Batch number = 1
Evaluating:  26%|██▌       | 52/201 [00:25<00:48,  3.08it/s]11/28/2021 01:33:28 - INFO - __main__ -   Batch number = 53
Evaluating:  26%|██▋       | 53/201 [00:26<00:48,  3.08it/s]11/28/2021 01:33:29 - INFO - __main__ -   Batch number = 54
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/adapter_config.json
Adding adapter 'bh' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/head_config.json
Evaluating:   1%|          | 1/109 [00:00<01:13,  1.46it/s]11/28/2021 01:33:29 - INFO - __main__ -   Batch number = 2
Evaluating:  27%|██▋       | 54/201 [00:26<00:48,  3.03it/s]11/28/2021 01:33:29 - INFO - __main__ -   Batch number = 55
Evaluating:  27%|██▋       | 55/201 [00:26<00:48,  3.02it/s]11/28/2021 01:33:29 - INFO - __main__ -   Batch number = 56
Evaluating:   2%|▏         | 2/109 [00:01<01:11,  1.50it/s]11/28/2021 01:33:29 - INFO - __main__ -   Batch number = 3
Evaluating:  28%|██▊       | 56/201 [00:27<00:47,  3.08it/s]11/28/2021 01:33:30 - INFO - __main__ -   Batch number = 57
Evaluating:  28%|██▊       | 57/201 [00:27<00:46,  3.08it/s]11/28/2021 01:33:30 - INFO - __main__ -   Batch number = 58
Evaluating:   3%|▎         | 3/109 [00:02<01:10,  1.50it/s]11/28/2021 01:33:30 - INFO - __main__ -   Batch number = 4
Evaluating:  29%|██▉       | 58/201 [00:27<00:46,  3.08it/s]11/28/2021 01:33:30 - INFO - __main__ -   Batch number = 59
Evaluating:  29%|██▉       | 59/201 [00:28<00:46,  3.06it/s]11/28/2021 01:33:31 - INFO - __main__ -   Batch number = 60
Evaluating:   4%|▎         | 4/109 [00:02<01:09,  1.51it/s]11/28/2021 01:33:31 - INFO - __main__ -   Batch number = 5
Evaluating:  30%|██▉       | 60/201 [00:28<00:46,  3.05it/s]11/28/2021 01:33:31 - INFO - __main__ -   Batch number = 61
Evaluating:  30%|███       | 61/201 [00:28<00:45,  3.05it/s]11/28/2021 01:33:31 - INFO - __main__ -   Batch number = 62
Evaluating:   5%|▍         | 5/109 [00:03<01:08,  1.52it/s]11/28/2021 01:33:31 - INFO - __main__ -   Batch number = 6
Evaluating:  31%|███       | 62/201 [00:29<00:45,  3.04it/s]11/28/2021 01:33:31 - INFO - __main__ -   Batch number = 63
Evaluating:  31%|███▏      | 63/201 [00:29<00:45,  3.05it/s]11/28/2021 01:33:32 - INFO - __main__ -   Batch number = 64
Evaluating:   6%|▌         | 6/109 [00:03<01:07,  1.52it/s]11/28/2021 01:33:32 - INFO - __main__ -   Batch number = 7
Evaluating:  32%|███▏      | 64/201 [00:29<00:44,  3.06it/s]11/28/2021 01:33:32 - INFO - __main__ -   Batch number = 65
Evaluating:  32%|███▏      | 65/201 [00:29<00:44,  3.05it/s]11/28/2021 01:33:32 - INFO - __main__ -   Batch number = 66
Evaluating:   6%|▋         | 7/109 [00:04<01:06,  1.52it/s]11/28/2021 01:33:33 - INFO - __main__ -   Batch number = 8
Evaluating:  33%|███▎      | 66/201 [00:30<00:44,  3.06it/s]11/28/2021 01:33:33 - INFO - __main__ -   Batch number = 67
Evaluating:  33%|███▎      | 67/201 [00:30<00:44,  3.03it/s]11/28/2021 01:33:33 - INFO - __main__ -   Batch number = 68
Evaluating:   7%|▋         | 8/109 [00:05<01:06,  1.52it/s]11/28/2021 01:33:33 - INFO - __main__ -   Batch number = 9
Evaluating:  34%|███▍      | 68/201 [00:30<00:44,  3.01it/s]11/28/2021 01:33:33 - INFO - __main__ -   Batch number = 69
Evaluating:  34%|███▍      | 69/201 [00:31<00:43,  3.02it/s]11/28/2021 01:33:34 - INFO - __main__ -   Batch number = 70
Evaluating:   8%|▊         | 9/109 [00:05<01:05,  1.53it/s]11/28/2021 01:33:34 - INFO - __main__ -   Batch number = 10
Evaluating:  35%|███▍      | 70/201 [00:31<00:43,  3.03it/s]11/28/2021 01:33:34 - INFO - __main__ -   Batch number = 71
Evaluating:  35%|███▌      | 71/201 [00:31<00:42,  3.06it/s]11/28/2021 01:33:34 - INFO - __main__ -   Batch number = 72
Evaluating:   9%|▉         | 10/109 [00:06<01:04,  1.53it/s]11/28/2021 01:33:35 - INFO - __main__ -   Batch number = 11
Evaluating:  36%|███▌      | 72/201 [00:32<00:42,  3.02it/s]11/28/2021 01:33:35 - INFO - __main__ -   Batch number = 73
Evaluating:  36%|███▋      | 73/201 [00:32<00:42,  2.99it/s]11/28/2021 01:33:35 - INFO - __main__ -   Batch number = 74
Evaluating:  10%|█         | 11/109 [00:07<01:03,  1.53it/s]11/28/2021 01:33:35 - INFO - __main__ -   Batch number = 12
Evaluating:  37%|███▋      | 74/201 [00:32<00:42,  2.99it/s]11/28/2021 01:33:35 - INFO - __main__ -   Batch number = 75
Evaluating:  37%|███▋      | 75/201 [00:33<00:41,  3.02it/s]11/28/2021 01:33:36 - INFO - __main__ -   Batch number = 76
Evaluating:  11%|█         | 12/109 [00:07<01:03,  1.53it/s]11/28/2021 01:33:36 - INFO - __main__ -   Batch number = 13
Evaluating:  38%|███▊      | 76/201 [00:33<00:41,  3.03it/s]11/28/2021 01:33:36 - INFO - __main__ -   Batch number = 77
Evaluating:  38%|███▊      | 77/201 [00:33<00:40,  3.05it/s]11/28/2021 01:33:36 - INFO - __main__ -   Batch number = 78
Evaluating:  12%|█▏        | 13/109 [00:08<01:02,  1.53it/s]11/28/2021 01:33:37 - INFO - __main__ -   Batch number = 14
Evaluating:  39%|███▉      | 78/201 [00:34<00:40,  3.05it/s]11/28/2021 01:33:37 - INFO - __main__ -   Batch number = 79
Evaluating:  39%|███▉      | 79/201 [00:34<00:39,  3.06it/s]11/28/2021 01:33:37 - INFO - __main__ -   Batch number = 80
11/28/2021 01:33:37 - INFO - __main__ -   Language adapter for pt not found, using bh instead
11/28/2021 01:33:37 - INFO - __main__ -   Set active language adapter to bh
11/28/2021 01:33:37 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:33:37 - INFO - __main__ -   Adapter Languages = ['bh']
11/28/2021 01:33:37 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_pt_bert-base-multilingual-cased_128
Evaluating:  13%|█▎        | 14/109 [00:09<01:01,  1.53it/s]11/28/2021 01:33:37 - INFO - __main__ -   Batch number = 15
Evaluating:  40%|███▉      | 80/201 [00:34<00:40,  3.02it/s]11/28/2021 01:33:37 - INFO - __main__ -   Batch number = 81
Evaluating:  40%|████      | 81/201 [00:35<00:39,  3.03it/s]11/28/2021 01:33:38 - INFO - __main__ -   Batch number = 82
11/28/2021 01:33:38 - INFO - __main__ -   ***** Running evaluation  in pt *****
11/28/2021 01:33:38 - INFO - __main__ -     Num examples = 2682
11/28/2021 01:33:38 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/84 [00:00<?, ?it/s]11/28/2021 01:33:38 - INFO - __main__ -   Batch number = 1
Evaluating:  14%|█▍        | 15/109 [00:09<01:01,  1.53it/s]11/28/2021 01:33:38 - INFO - __main__ -   Batch number = 16
Evaluating:   1%|          | 1/84 [00:00<00:41,  2.01it/s]11/28/2021 01:33:38 - INFO - __main__ -   Batch number = 2
Evaluating:  41%|████      | 82/201 [00:35<00:45,  2.61it/s]11/28/2021 01:33:38 - INFO - __main__ -   Batch number = 83
Evaluating:  15%|█▍        | 16/109 [00:10<01:00,  1.53it/s]11/28/2021 01:33:39 - INFO - __main__ -   Batch number = 17
Evaluating:   2%|▏         | 2/84 [00:00<00:40,  2.03it/s]11/28/2021 01:33:39 - INFO - __main__ -   Batch number = 3
Evaluating:  41%|████▏     | 83/201 [00:36<00:49,  2.38it/s]11/28/2021 01:33:39 - INFO - __main__ -   Batch number = 84
Evaluating:   4%|▎         | 3/84 [00:01<00:40,  2.02it/s]11/28/2021 01:33:39 - INFO - __main__ -   Batch number = 4
Evaluating:  42%|████▏     | 84/201 [00:36<00:52,  2.25it/s]11/28/2021 01:33:39 - INFO - __main__ -   Batch number = 85
Evaluating:  16%|█▌        | 17/109 [00:11<00:59,  1.54it/s]11/28/2021 01:33:39 - INFO - __main__ -   Batch number = 18
Evaluating:   5%|▍         | 4/84 [00:01<00:39,  2.05it/s]11/28/2021 01:33:40 - INFO - __main__ -   Batch number = 5
Evaluating:  42%|████▏     | 85/201 [00:37<00:53,  2.18it/s]11/28/2021 01:33:40 - INFO - __main__ -   Batch number = 86
Evaluating:  17%|█▋        | 18/109 [00:11<00:57,  1.58it/s]11/28/2021 01:33:40 - INFO - __main__ -   Batch number = 19
Evaluating:   6%|▌         | 5/84 [00:02<00:38,  2.04it/s]11/28/2021 01:33:40 - INFO - __main__ -   Batch number = 6
Evaluating:  43%|████▎     | 86/201 [00:37<00:53,  2.13it/s]11/28/2021 01:33:40 - INFO - __main__ -   Batch number = 87
Evaluating:  17%|█▋        | 19/109 [00:12<00:52,  1.70it/s]11/28/2021 01:33:40 - INFO - __main__ -   Batch number = 20
Evaluating:   7%|▋         | 6/84 [00:02<00:38,  2.04it/s]11/28/2021 01:33:41 - INFO - __main__ -   Batch number = 7
Evaluating:  43%|████▎     | 87/201 [00:38<00:54,  2.09it/s]11/28/2021 01:33:41 - INFO - __main__ -   Batch number = 88
Evaluating:  18%|█▊        | 20/109 [00:12<00:49,  1.79it/s]11/28/2021 01:33:41 - INFO - __main__ -   Batch number = 21
Evaluating:   8%|▊         | 7/84 [00:03<00:37,  2.03it/s]11/28/2021 01:33:41 - INFO - __main__ -   Batch number = 8
Evaluating:  44%|████▍     | 88/201 [00:38<00:54,  2.06it/s]11/28/2021 01:33:41 - INFO - __main__ -   Batch number = 89
Evaluating:  19%|█▉        | 21/109 [00:13<00:47,  1.85it/s]11/28/2021 01:33:41 - INFO - __main__ -   Batch number = 22
Evaluating:  10%|▉         | 8/84 [00:03<00:37,  2.04it/s]11/28/2021 01:33:42 - INFO - __main__ -   Batch number = 9
Evaluating:  44%|████▍     | 89/201 [00:39<00:54,  2.05it/s]11/28/2021 01:33:42 - INFO - __main__ -   Batch number = 90
Evaluating:  20%|██        | 22/109 [00:13<00:45,  1.89it/s]11/28/2021 01:33:42 - INFO - __main__ -   Batch number = 23
Evaluating:  11%|█         | 9/84 [00:04<00:36,  2.04it/s]11/28/2021 01:33:42 - INFO - __main__ -   Batch number = 10
Evaluating:  45%|████▍     | 90/201 [00:39<00:54,  2.05it/s]11/28/2021 01:33:42 - INFO - __main__ -   Batch number = 91
Evaluating:  21%|██        | 23/109 [00:14<00:44,  1.95it/s]11/28/2021 01:33:42 - INFO - __main__ -   Batch number = 24
Evaluating:  12%|█▏        | 10/84 [00:04<00:36,  2.03it/s]11/28/2021 01:33:43 - INFO - __main__ -   Batch number = 11
Evaluating:  45%|████▌     | 91/201 [00:40<00:54,  2.03it/s]11/28/2021 01:33:43 - INFO - __main__ -   Batch number = 92
Evaluating:  22%|██▏       | 24/109 [00:14<00:42,  1.99it/s]11/28/2021 01:33:43 - INFO - __main__ -   Batch number = 25
Evaluating:  13%|█▎        | 11/84 [00:05<00:36,  2.03it/s]11/28/2021 01:33:43 - INFO - __main__ -   Batch number = 12
Evaluating:  46%|████▌     | 92/201 [00:40<00:53,  2.03it/s]11/28/2021 01:33:43 - INFO - __main__ -   Batch number = 93
Evaluating:  23%|██▎       | 25/109 [00:15<00:41,  2.00it/s]11/28/2021 01:33:43 - INFO - __main__ -   Batch number = 26
Evaluating:  14%|█▍        | 12/84 [00:05<00:35,  2.02it/s]11/28/2021 01:33:44 - INFO - __main__ -   Batch number = 13
Evaluating:  46%|████▋     | 93/201 [00:41<00:53,  2.03it/s]11/28/2021 01:33:44 - INFO - __main__ -   Batch number = 94
Evaluating:  24%|██▍       | 26/109 [00:15<00:41,  2.02it/s]11/28/2021 01:33:44 - INFO - __main__ -   Batch number = 27
Evaluating:  15%|█▌        | 13/84 [00:06<00:34,  2.03it/s]11/28/2021 01:33:44 - INFO - __main__ -   Batch number = 14
Evaluating:  47%|████▋     | 94/201 [00:41<00:52,  2.02it/s]11/28/2021 01:33:44 - INFO - __main__ -   Batch number = 95
Evaluating:  25%|██▍       | 27/109 [00:16<00:40,  2.03it/s]11/28/2021 01:33:44 - INFO - __main__ -   Batch number = 28
Evaluating:  17%|█▋        | 14/84 [00:06<00:34,  2.04it/s]11/28/2021 01:33:45 - INFO - __main__ -   Batch number = 15
Evaluating:  47%|████▋     | 95/201 [00:42<00:52,  2.01it/s]11/28/2021 01:33:45 - INFO - __main__ -   Batch number = 96
Evaluating:  26%|██▌       | 28/109 [00:16<00:39,  2.05it/s]11/28/2021 01:33:45 - INFO - __main__ -   Batch number = 29
Evaluating:  18%|█▊        | 15/84 [00:07<00:33,  2.05it/s]11/28/2021 01:33:45 - INFO - __main__ -   Batch number = 16
Evaluating:  48%|████▊     | 96/201 [00:42<00:52,  2.00it/s]11/28/2021 01:33:45 - INFO - __main__ -   Batch number = 97
Evaluating:  27%|██▋       | 29/109 [00:17<00:39,  2.05it/s]11/28/2021 01:33:45 - INFO - __main__ -   Batch number = 30
Evaluating:  19%|█▉        | 16/84 [00:07<00:33,  2.04it/s]11/28/2021 01:33:46 - INFO - __main__ -   Batch number = 17
Evaluating:  28%|██▊       | 30/109 [00:17<00:38,  2.06it/s]11/28/2021 01:33:46 - INFO - __main__ -   Batch number = 31
Evaluating:  48%|████▊     | 97/201 [00:43<00:52,  1.97it/s]11/28/2021 01:33:46 - INFO - __main__ -   Batch number = 98
Evaluating:  20%|██        | 17/84 [00:08<00:32,  2.05it/s]11/28/2021 01:33:46 - INFO - __main__ -   Batch number = 18
Evaluating:  28%|██▊       | 31/109 [00:18<00:38,  2.05it/s]11/28/2021 01:33:46 - INFO - __main__ -   Batch number = 32
Evaluating:  49%|████▉     | 98/201 [00:43<00:52,  1.98it/s]11/28/2021 01:33:46 - INFO - __main__ -   Batch number = 99
Evaluating:  21%|██▏       | 18/84 [00:08<00:32,  2.05it/s]11/28/2021 01:33:47 - INFO - __main__ -   Batch number = 19
Evaluating:  29%|██▉       | 32/109 [00:18<00:37,  2.06it/s]11/28/2021 01:33:47 - INFO - __main__ -   Batch number = 33
Evaluating:  49%|████▉     | 99/201 [00:44<00:51,  1.98it/s]11/28/2021 01:33:47 - INFO - __main__ -   Batch number = 100
Evaluating:  23%|██▎       | 19/84 [00:09<00:31,  2.06it/s]11/28/2021 01:33:47 - INFO - __main__ -   Batch number = 20
Evaluating:  30%|███       | 33/109 [00:19<00:36,  2.06it/s]11/28/2021 01:33:47 - INFO - __main__ -   Batch number = 34
Evaluating:  50%|████▉     | 100/201 [00:44<00:51,  1.98it/s]11/28/2021 01:33:47 - INFO - __main__ -   Batch number = 101
Evaluating:  24%|██▍       | 20/84 [00:09<00:31,  2.06it/s]11/28/2021 01:33:48 - INFO - __main__ -   Batch number = 21
Evaluating:  31%|███       | 34/109 [00:19<00:36,  2.06it/s]11/28/2021 01:33:48 - INFO - __main__ -   Batch number = 35
Evaluating:  50%|█████     | 101/201 [00:45<00:50,  1.98it/s]11/28/2021 01:33:48 - INFO - __main__ -   Batch number = 102
Evaluating:  25%|██▌       | 21/84 [00:10<00:30,  2.06it/s]11/28/2021 01:33:48 - INFO - __main__ -   Batch number = 22
Evaluating:  32%|███▏      | 35/109 [00:19<00:35,  2.06it/s]11/28/2021 01:33:48 - INFO - __main__ -   Batch number = 36
Evaluating:  51%|█████     | 102/201 [00:45<00:50,  1.98it/s]11/28/2021 01:33:48 - INFO - __main__ -   Batch number = 103
Evaluating:  26%|██▌       | 22/84 [00:10<00:29,  2.07it/s]11/28/2021 01:33:49 - INFO - __main__ -   Batch number = 23
Evaluating:  33%|███▎      | 36/109 [00:20<00:35,  2.08it/s]11/28/2021 01:33:49 - INFO - __main__ -   Batch number = 37
Evaluating:  51%|█████     | 103/201 [00:46<00:49,  1.98it/s]11/28/2021 01:33:49 - INFO - __main__ -   Batch number = 104
Evaluating:  27%|██▋       | 23/84 [00:11<00:29,  2.07it/s]11/28/2021 01:33:49 - INFO - __main__ -   Batch number = 24
Evaluating:  34%|███▍      | 37/109 [00:20<00:34,  2.07it/s]11/28/2021 01:33:49 - INFO - __main__ -   Batch number = 38
Evaluating:  52%|█████▏    | 104/201 [00:46<00:48,  1.98it/s]11/28/2021 01:33:49 - INFO - __main__ -   Batch number = 105
Evaluating:  29%|██▊       | 24/84 [00:11<00:29,  2.07it/s]Evaluating:  35%|███▍      | 38/109 [00:21<00:34,  2.06it/s]11/28/2021 01:33:50 - INFO - __main__ -   Batch number = 39
11/28/2021 01:33:50 - INFO - __main__ -   Batch number = 25
Evaluating:  52%|█████▏    | 105/201 [00:47<00:47,  2.02it/s]11/28/2021 01:33:50 - INFO - __main__ -   Batch number = 106
Evaluating:  36%|███▌      | 39/109 [00:21<00:33,  2.07it/s]11/28/2021 01:33:50 - INFO - __main__ -   Batch number = 40
Evaluating:  30%|██▉       | 25/84 [00:12<00:33,  1.75it/s]11/28/2021 01:33:50 - INFO - __main__ -   Batch number = 26
Evaluating:  53%|█████▎    | 106/201 [00:47<00:51,  1.84it/s]11/28/2021 01:33:50 - INFO - __main__ -   Batch number = 107
Evaluating:  37%|███▋      | 40/109 [00:22<00:33,  2.07it/s]11/28/2021 01:33:51 - INFO - __main__ -   Batch number = 41
Evaluating:  31%|███       | 26/84 [00:13<00:34,  1.67it/s]11/28/2021 01:33:51 - INFO - __main__ -   Batch number = 27
Evaluating:  38%|███▊      | 41/109 [00:22<00:32,  2.08it/s]11/28/2021 01:33:51 - INFO - __main__ -   Batch number = 42
Evaluating:  53%|█████▎    | 107/201 [00:48<00:54,  1.73it/s]11/28/2021 01:33:51 - INFO - __main__ -   Batch number = 108
Evaluating:  39%|███▊      | 42/109 [00:23<00:32,  2.08it/s]11/28/2021 01:33:52 - INFO - __main__ -   Batch number = 43
Evaluating:  32%|███▏      | 27/84 [00:13<00:34,  1.63it/s]11/28/2021 01:33:52 - INFO - __main__ -   Batch number = 28
Evaluating:  54%|█████▎    | 108/201 [00:49<00:56,  1.65it/s]11/28/2021 01:33:52 - INFO - __main__ -   Batch number = 109
Evaluating:  39%|███▉      | 43/109 [00:23<00:31,  2.07it/s]11/28/2021 01:33:52 - INFO - __main__ -   Batch number = 44
Evaluating:  33%|███▎      | 28/84 [00:14<00:34,  1.61it/s]11/28/2021 01:33:52 - INFO - __main__ -   Batch number = 29
Evaluating:  54%|█████▍    | 109/201 [00:49<00:57,  1.61it/s]11/28/2021 01:33:52 - INFO - __main__ -   Batch number = 110
Evaluating:  40%|████      | 44/109 [00:24<00:31,  2.06it/s]11/28/2021 01:33:53 - INFO - __main__ -   Batch number = 45
Evaluating:  35%|███▍      | 29/84 [00:15<00:34,  1.58it/s]11/28/2021 01:33:53 - INFO - __main__ -   Batch number = 30
Evaluating:  41%|████▏     | 45/109 [00:24<00:31,  2.06it/s]11/28/2021 01:33:53 - INFO - __main__ -   Batch number = 46
Evaluating:  55%|█████▍    | 110/201 [00:50<00:57,  1.57it/s]11/28/2021 01:33:53 - INFO - __main__ -   Batch number = 111
Evaluating:  42%|████▏     | 46/109 [00:25<00:30,  2.05it/s]11/28/2021 01:33:53 - INFO - __main__ -   Batch number = 47
Evaluating:  36%|███▌      | 30/84 [00:15<00:34,  1.57it/s]11/28/2021 01:33:54 - INFO - __main__ -   Batch number = 31
Evaluating:  55%|█████▌    | 111/201 [00:51<00:57,  1.56it/s]11/28/2021 01:33:54 - INFO - __main__ -   Batch number = 112
Evaluating:  43%|████▎     | 47/109 [00:25<00:30,  2.06it/s]11/28/2021 01:33:54 - INFO - __main__ -   Batch number = 48
Evaluating:  37%|███▋      | 31/84 [00:16<00:34,  1.55it/s]11/28/2021 01:33:54 - INFO - __main__ -   Batch number = 32
Evaluating:  56%|█████▌    | 112/201 [00:51<00:57,  1.54it/s]Evaluating:  44%|████▍     | 48/109 [00:26<00:29,  2.05it/s]11/28/2021 01:33:54 - INFO - __main__ -   Batch number = 113
11/28/2021 01:33:55 - INFO - __main__ -   Batch number = 49
Evaluating:  38%|███▊      | 32/84 [00:16<00:31,  1.65it/s]11/28/2021 01:33:55 - INFO - __main__ -   Batch number = 33
Evaluating:  45%|████▍     | 49/109 [00:26<00:29,  2.03it/s]11/28/2021 01:33:55 - INFO - __main__ -   Batch number = 50
Evaluating:  56%|█████▌    | 113/201 [00:52<01:01,  1.42it/s]11/28/2021 01:33:55 - INFO - __main__ -   Batch number = 114
Evaluating:  46%|████▌     | 50/109 [00:27<00:25,  2.28it/s]11/28/2021 01:33:55 - INFO - __main__ -   Batch number = 51
Evaluating:  39%|███▉      | 33/84 [00:17<00:31,  1.61it/s]11/28/2021 01:33:55 - INFO - __main__ -   Batch number = 34
Evaluating:  47%|████▋     | 51/109 [00:27<00:23,  2.51it/s]11/28/2021 01:33:56 - INFO - __main__ -   Batch number = 52
Evaluating:  57%|█████▋    | 114/201 [00:53<01:00,  1.45it/s]11/28/2021 01:33:56 - INFO - __main__ -   Batch number = 115
Evaluating:  48%|████▊     | 52/109 [00:27<00:21,  2.66it/s]11/28/2021 01:33:56 - INFO - __main__ -   Batch number = 53
Evaluating:  40%|████      | 34/84 [00:18<00:31,  1.59it/s]11/28/2021 01:33:56 - INFO - __main__ -   Batch number = 35
Evaluating:  49%|████▊     | 53/109 [00:28<00:19,  2.81it/s]11/28/2021 01:33:56 - INFO - __main__ -   Batch number = 54
Evaluating:  50%|████▉     | 54/109 [00:28<00:18,  2.90it/s]11/28/2021 01:33:57 - INFO - __main__ -   Batch number = 55
Evaluating:  57%|█████▋    | 115/201 [00:54<00:58,  1.46it/s]11/28/2021 01:33:57 - INFO - __main__ -   Batch number = 116
Evaluating:  42%|████▏     | 35/84 [00:18<00:31,  1.57it/s]11/28/2021 01:33:57 - INFO - __main__ -   Batch number = 36
Evaluating:  50%|█████     | 55/109 [00:28<00:18,  2.92it/s]11/28/2021 01:33:57 - INFO - __main__ -   Batch number = 56
Evaluating:  51%|█████▏    | 56/109 [00:29<00:17,  2.99it/s]11/28/2021 01:33:57 - INFO - __main__ -   Batch number = 57
Evaluating:  58%|█████▊    | 116/201 [00:54<00:57,  1.47it/s]11/28/2021 01:33:57 - INFO - __main__ -   Batch number = 117
Evaluating:  43%|████▎     | 36/84 [00:19<00:30,  1.56it/s]11/28/2021 01:33:57 - INFO - __main__ -   Batch number = 37
Evaluating:  52%|█████▏    | 57/109 [00:29<00:17,  3.03it/s]11/28/2021 01:33:58 - INFO - __main__ -   Batch number = 58
Evaluating:  53%|█████▎    | 58/109 [00:29<00:16,  3.14it/s]11/28/2021 01:33:58 - INFO - __main__ -   Batch number = 59
Evaluating:  58%|█████▊    | 117/201 [00:55<00:56,  1.48it/s]11/28/2021 01:33:58 - INFO - __main__ -   Batch number = 118
Evaluating:  44%|████▍     | 37/84 [00:20<00:30,  1.55it/s]11/28/2021 01:33:58 - INFO - __main__ -   Batch number = 38
Evaluating:  54%|█████▍    | 59/109 [00:29<00:15,  3.14it/s]11/28/2021 01:33:58 - INFO - __main__ -   Batch number = 60
Evaluating:  55%|█████▌    | 60/109 [00:30<00:15,  3.18it/s]11/28/2021 01:33:58 - INFO - __main__ -   Batch number = 61
Evaluating:  59%|█████▊    | 118/201 [00:56<00:55,  1.49it/s]11/28/2021 01:33:59 - INFO - __main__ -   Batch number = 119
Evaluating:  45%|████▌     | 38/84 [00:20<00:29,  1.54it/s]11/28/2021 01:33:59 - INFO - __main__ -   Batch number = 39
Evaluating:  56%|█████▌    | 61/109 [00:30<00:15,  3.16it/s]11/28/2021 01:33:59 - INFO - __main__ -   Batch number = 62
Evaluating:  57%|█████▋    | 62/109 [00:30<00:14,  3.20it/s]11/28/2021 01:33:59 - INFO - __main__ -   Batch number = 63
Evaluating:  59%|█████▉    | 119/201 [00:56<00:54,  1.49it/s]11/28/2021 01:33:59 - INFO - __main__ -   Batch number = 120
Evaluating:  46%|████▋     | 39/84 [00:21<00:29,  1.53it/s]11/28/2021 01:33:59 - INFO - __main__ -   Batch number = 40
Evaluating:  58%|█████▊    | 63/109 [00:31<00:14,  3.13it/s]11/28/2021 01:33:59 - INFO - __main__ -   Batch number = 64
Evaluating:  59%|█████▊    | 64/109 [00:31<00:13,  3.38it/s]11/28/2021 01:34:00 - INFO - __main__ -   Batch number = 65
Evaluating:  60%|█████▉    | 120/201 [00:57<00:54,  1.49it/s]11/28/2021 01:34:00 - INFO - __main__ -   Batch number = 121
Evaluating:  60%|█████▉    | 65/109 [00:31<00:13,  3.32it/s]11/28/2021 01:34:00 - INFO - __main__ -   Batch number = 66
Evaluating:  48%|████▊     | 40/84 [00:22<00:28,  1.52it/s]11/28/2021 01:34:00 - INFO - __main__ -   Batch number = 41
Evaluating:  61%|██████    | 66/109 [00:32<00:12,  3.32it/s]11/28/2021 01:34:00 - INFO - __main__ -   Batch number = 67
Evaluating:  61%|██████▏   | 67/109 [00:32<00:11,  3.75it/s]11/28/2021 01:34:00 - INFO - __main__ -   Batch number = 68
Evaluating:  60%|██████    | 121/201 [00:58<00:53,  1.50it/s]11/28/2021 01:34:01 - INFO - __main__ -   Batch number = 122
Evaluating:  62%|██████▏   | 68/109 [00:32<00:09,  4.19it/s]11/28/2021 01:34:01 - INFO - __main__ -   Batch number = 69
Evaluating:  49%|████▉     | 41/84 [00:22<00:28,  1.52it/s]11/28/2021 01:34:01 - INFO - __main__ -   Batch number = 42
Evaluating:  63%|██████▎   | 69/109 [00:32<00:08,  4.57it/s]11/28/2021 01:34:01 - INFO - __main__ -   Batch number = 70
Evaluating:  64%|██████▍   | 70/109 [00:32<00:08,  4.86it/s]11/28/2021 01:34:01 - INFO - __main__ -   Batch number = 71
Evaluating:  65%|██████▌   | 71/109 [00:32<00:07,  5.14it/s]11/28/2021 01:34:01 - INFO - __main__ -   Batch number = 72
Evaluating:  61%|██████    | 122/201 [00:58<00:52,  1.50it/s]11/28/2021 01:34:01 - INFO - __main__ -   Batch number = 123
Evaluating:  66%|██████▌   | 72/109 [00:33<00:06,  5.39it/s]11/28/2021 01:34:01 - INFO - __main__ -   Batch number = 73
Evaluating:  50%|█████     | 42/84 [00:23<00:27,  1.52it/s]11/28/2021 01:34:01 - INFO - __main__ -   Batch number = 43
Evaluating:  67%|██████▋   | 73/109 [00:33<00:06,  5.58it/s]11/28/2021 01:34:01 - INFO - __main__ -   Batch number = 74
Evaluating:  68%|██████▊   | 74/109 [00:33<00:06,  5.71it/s]11/28/2021 01:34:02 - INFO - __main__ -   Batch number = 75
Evaluating:  69%|██████▉   | 75/109 [00:33<00:05,  5.81it/s]11/28/2021 01:34:02 - INFO - __main__ -   Batch number = 76
Evaluating:  61%|██████    | 123/201 [00:59<00:52,  1.49it/s]11/28/2021 01:34:02 - INFO - __main__ -   Batch number = 124
Evaluating:  51%|█████     | 43/84 [00:24<00:26,  1.52it/s]11/28/2021 01:34:02 - INFO - __main__ -   Batch number = 44
Evaluating:  70%|██████▉   | 76/109 [00:33<00:05,  5.88it/s]11/28/2021 01:34:02 - INFO - __main__ -   Batch number = 77
Evaluating:  71%|███████   | 77/109 [00:33<00:05,  5.78it/s]11/28/2021 01:34:02 - INFO - __main__ -   Batch number = 78
Evaluating:  72%|███████▏  | 78/109 [00:34<00:05,  5.87it/s]11/28/2021 01:34:02 - INFO - __main__ -   Batch number = 79
Evaluating:  72%|███████▏  | 79/109 [00:34<00:05,  5.93it/s]11/28/2021 01:34:02 - INFO - __main__ -   Batch number = 80
Evaluating:  62%|██████▏   | 124/201 [01:00<00:51,  1.50it/s]11/28/2021 01:34:03 - INFO - __main__ -   Batch number = 125
Evaluating:  52%|█████▏    | 44/84 [00:24<00:26,  1.52it/s]11/28/2021 01:34:03 - INFO - __main__ -   Batch number = 45
Evaluating:  73%|███████▎  | 80/109 [00:34<00:04,  5.93it/s]11/28/2021 01:34:03 - INFO - __main__ -   Batch number = 81
Evaluating:  74%|███████▍  | 81/109 [00:34<00:04,  5.90it/s]11/28/2021 01:34:03 - INFO - __main__ -   Batch number = 82
Evaluating:  75%|███████▌  | 82/109 [00:34<00:04,  5.99it/s]11/28/2021 01:34:03 - INFO - __main__ -   Batch number = 83
Evaluating:  76%|███████▌  | 83/109 [00:34<00:04,  5.95it/s]11/28/2021 01:34:03 - INFO - __main__ -   Batch number = 84
Evaluating:  62%|██████▏   | 125/201 [01:00<00:50,  1.50it/s]11/28/2021 01:34:03 - INFO - __main__ -   Batch number = 126
Evaluating:  54%|█████▎    | 45/84 [00:25<00:25,  1.52it/s]11/28/2021 01:34:03 - INFO - __main__ -   Batch number = 46
Evaluating:  77%|███████▋  | 84/109 [00:35<00:04,  5.90it/s]11/28/2021 01:34:03 - INFO - __main__ -   Batch number = 85
Evaluating:  78%|███████▊  | 85/109 [00:35<00:04,  5.90it/s]11/28/2021 01:34:03 - INFO - __main__ -   Batch number = 86
Evaluating:  79%|███████▉  | 86/109 [00:35<00:03,  5.88it/s]11/28/2021 01:34:04 - INFO - __main__ -   Batch number = 87
Evaluating:  80%|███████▉  | 87/109 [00:35<00:03,  5.88it/s]11/28/2021 01:34:04 - INFO - __main__ -   Batch number = 88
Evaluating:  63%|██████▎   | 126/201 [01:01<00:49,  1.51it/s]11/28/2021 01:34:04 - INFO - __main__ -   Batch number = 127
Evaluating:  55%|█████▍    | 46/84 [00:26<00:24,  1.52it/s]11/28/2021 01:34:04 - INFO - __main__ -   Batch number = 47
Evaluating:  81%|████████  | 88/109 [00:35<00:03,  5.98it/s]11/28/2021 01:34:04 - INFO - __main__ -   Batch number = 89
Evaluating:  82%|████████▏ | 89/109 [00:35<00:03,  5.83it/s]11/28/2021 01:34:04 - INFO - __main__ -   Batch number = 90
Evaluating:  83%|████████▎ | 90/109 [00:36<00:04,  4.67it/s]11/28/2021 01:34:05 - INFO - __main__ -   Batch number = 91
Evaluating:  63%|██████▎   | 127/201 [01:02<00:49,  1.51it/s]11/28/2021 01:34:05 - INFO - __main__ -   Batch number = 128
Evaluating:  56%|█████▌    | 47/84 [00:26<00:24,  1.52it/s]11/28/2021 01:34:05 - INFO - __main__ -   Batch number = 48
Evaluating:  83%|████████▎ | 91/109 [00:36<00:04,  3.68it/s]11/28/2021 01:34:05 - INFO - __main__ -   Batch number = 92
Evaluating:  84%|████████▍ | 92/109 [00:37<00:04,  3.44it/s]11/28/2021 01:34:05 - INFO - __main__ -   Batch number = 93
Evaluating:  64%|██████▎   | 128/201 [01:02<00:48,  1.50it/s]11/28/2021 01:34:05 - INFO - __main__ -   Batch number = 129
Evaluating:  57%|█████▋    | 48/84 [00:27<00:23,  1.51it/s]11/28/2021 01:34:05 - INFO - __main__ -   Batch number = 49
Evaluating:  85%|████████▌ | 93/109 [00:37<00:04,  3.27it/s]11/28/2021 01:34:06 - INFO - __main__ -   Batch number = 94
Evaluating:  86%|████████▌ | 94/109 [00:37<00:04,  3.17it/s]11/28/2021 01:34:06 - INFO - __main__ -   Batch number = 95
Evaluating:  64%|██████▍   | 129/201 [01:03<00:48,  1.50it/s]11/28/2021 01:34:06 - INFO - __main__ -   Batch number = 130
Evaluating:  58%|█████▊    | 49/84 [00:28<00:23,  1.51it/s]11/28/2021 01:34:06 - INFO - __main__ -   Batch number = 50
Evaluating:  87%|████████▋ | 95/109 [00:38<00:04,  3.10it/s]11/28/2021 01:34:06 - INFO - __main__ -   Batch number = 96
Evaluating:  65%|██████▍   | 130/201 [01:04<00:47,  1.50it/s]Evaluating:  88%|████████▊ | 96/109 [00:38<00:04,  3.07it/s]11/28/2021 01:34:07 - INFO - __main__ -   Batch number = 131
11/28/2021 01:34:07 - INFO - __main__ -   Batch number = 97
Evaluating:  60%|█████▉    | 50/84 [00:28<00:22,  1.51it/s]11/28/2021 01:34:07 - INFO - __main__ -   Batch number = 51
Evaluating:  89%|████████▉ | 97/109 [00:38<00:03,  3.04it/s]11/28/2021 01:34:07 - INFO - __main__ -   Batch number = 98
Evaluating:  65%|██████▌   | 131/201 [01:04<00:46,  1.50it/s]11/28/2021 01:34:07 - INFO - __main__ -   Batch number = 132
Evaluating:  61%|██████    | 51/84 [00:29<00:21,  1.51it/s]11/28/2021 01:34:07 - INFO - __main__ -   Batch number = 52
Evaluating:  90%|████████▉ | 98/109 [00:39<00:03,  3.02it/s]11/28/2021 01:34:07 - INFO - __main__ -   Batch number = 99
Evaluating:  91%|█████████ | 99/109 [00:39<00:03,  3.01it/s]11/28/2021 01:34:08 - INFO - __main__ -   Batch number = 100
Evaluating:  66%|██████▌   | 132/201 [01:05<00:45,  1.52it/s]11/28/2021 01:34:08 - INFO - __main__ -   Batch number = 133
Evaluating:  62%|██████▏   | 52/84 [00:30<00:21,  1.51it/s]11/28/2021 01:34:08 - INFO - __main__ -   Batch number = 53
Evaluating:  92%|█████████▏| 100/109 [00:39<00:02,  3.00it/s]11/28/2021 01:34:08 - INFO - __main__ -   Batch number = 101
Evaluating:  93%|█████████▎| 101/109 [00:40<00:02,  3.00it/s]11/28/2021 01:34:08 - INFO - __main__ -   Batch number = 102
Evaluating:  66%|██████▌   | 133/201 [01:06<00:45,  1.51it/s]11/28/2021 01:34:09 - INFO - __main__ -   Batch number = 134
Evaluating:  63%|██████▎   | 53/84 [00:30<00:20,  1.51it/s]11/28/2021 01:34:09 - INFO - __main__ -   Batch number = 54
Evaluating:  94%|█████████▎| 102/109 [00:40<00:02,  2.99it/s]11/28/2021 01:34:09 - INFO - __main__ -   Batch number = 103
Evaluating:  94%|█████████▍| 103/109 [00:40<00:02,  2.98it/s]11/28/2021 01:34:09 - INFO - __main__ -   Batch number = 104
Evaluating:  67%|██████▋   | 134/201 [01:06<00:44,  1.51it/s]11/28/2021 01:34:09 - INFO - __main__ -   Batch number = 135
Evaluating:  64%|██████▍   | 54/84 [00:31<00:19,  1.51it/s]11/28/2021 01:34:09 - INFO - __main__ -   Batch number = 55
Evaluating:  95%|█████████▌| 104/109 [00:41<00:01,  2.98it/s]11/28/2021 01:34:09 - INFO - __main__ -   Batch number = 105
Evaluating:  96%|█████████▋| 105/109 [00:41<00:01,  3.21it/s]11/28/2021 01:34:10 - INFO - __main__ -   Batch number = 106
Evaluating:  67%|██████▋   | 135/201 [01:07<00:43,  1.51it/s]11/28/2021 01:34:10 - INFO - __main__ -   Batch number = 136
Evaluating:  65%|██████▌   | 55/84 [00:32<00:19,  1.51it/s]11/28/2021 01:34:10 - INFO - __main__ -   Batch number = 56
Evaluating:  97%|█████████▋| 106/109 [00:41<00:01,  2.96it/s]11/28/2021 01:34:10 - INFO - __main__ -   Batch number = 107
Evaluating:  98%|█████████▊| 107/109 [00:42<00:00,  3.00it/s]11/28/2021 01:34:10 - INFO - __main__ -   Batch number = 108
Evaluating:  68%|██████▊   | 136/201 [01:08<00:43,  1.50it/s]11/28/2021 01:34:11 - INFO - __main__ -   Batch number = 137
Evaluating:  67%|██████▋   | 56/84 [00:32<00:18,  1.51it/s]11/28/2021 01:34:11 - INFO - __main__ -   Batch number = 57
Evaluating:  99%|█████████▉| 108/109 [00:42<00:00,  2.99it/s]11/28/2021 01:34:11 - INFO - __main__ -   Batch number = 109
Evaluating: 100%|██████████| 109/109 [00:42<00:00,  2.57it/s]Evaluating:  68%|██████▊   | 137/201 [01:08<00:42,  1.51it/s]11/28/2021 01:34:11 - INFO - __main__ -   Batch number = 138
Evaluating:  68%|██████▊   | 57/84 [00:33<00:17,  1.51it/s]11/28/2021 01:34:11 - INFO - __main__ -   Batch number = 58
Evaluating:  69%|██████▉   | 58/84 [00:34<00:17,  1.51it/s]11/28/2021 01:34:12 - INFO - __main__ -   Batch number = 59
Evaluating:  69%|██████▊   | 138/201 [01:09<00:41,  1.50it/s]11/28/2021 01:34:12 - INFO - __main__ -   Batch number = 139

/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:34:12 - INFO - __main__ -   ***** Evaluation result  in zh *****
11/28/2021 01:34:12 - INFO - __main__ -     f1 = 0.6021949109781589
11/28/2021 01:34:12 - INFO - __main__ -     loss = 1.3358200055743576
11/28/2021 01:34:12 - INFO - __main__ -     precision = 0.6112269534679543
11/28/2021 01:34:12 - INFO - __main__ -     recall = 0.5934259127717005
Evaluating:  70%|███████   | 59/84 [00:34<00:16,  1.51it/s]11/28/2021 01:34:13 - INFO - __main__ -   Batch number = 60
Evaluating:  69%|██████▉   | 139/201 [01:10<00:41,  1.50it/s]11/28/2021 01:34:13 - INFO - __main__ -   Batch number = 140
51.49user 17.71system 1:09.41elapsed 99%CPU (0avgtext+0avgdata 3986792maxresident)k
0inputs+736outputs (0major+1443213minor)pagefaults 0swaps
Evaluating:  71%|███████▏  | 60/84 [00:35<00:15,  1.52it/s]11/28/2021 01:34:13 - INFO - __main__ -   Batch number = 61
Evaluating:  70%|██████▉   | 140/201 [01:10<00:40,  1.50it/s]11/28/2021 01:34:13 - INFO - __main__ -   Batch number = 141
Evaluating:  73%|███████▎  | 61/84 [00:36<00:15,  1.52it/s]11/28/2021 01:34:14 - INFO - __main__ -   Batch number = 62
Evaluating:  70%|███████   | 141/201 [01:11<00:40,  1.50it/s]11/28/2021 01:34:14 - INFO - __main__ -   Batch number = 142
PyTorch version 1.10.0+cu102 available.
Evaluating:  74%|███████▍  | 62/84 [00:36<00:14,  1.52it/s]11/28/2021 01:34:14 - INFO - __main__ -   Batch number = 63
Evaluating:  71%|███████   | 142/201 [01:12<00:39,  1.50it/s]11/28/2021 01:34:15 - INFO - __main__ -   Batch number = 143
11/28/2021 01:34:15 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='zh', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:34:15 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:34:15 - INFO - __main__ -   Seed = 2
11/28/2021 01:34:15 - INFO - root -   save model
11/28/2021 01:34:15 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='zh', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:34:15 - INFO - __main__ -   Loading pretrained model and tokenizer
Evaluating:  75%|███████▌  | 63/84 [00:37<00:13,  1.51it/s]11/28/2021 01:34:15 - INFO - __main__ -   Batch number = 64
Evaluating:  71%|███████   | 143/201 [01:12<00:38,  1.50it/s]11/28/2021 01:34:15 - INFO - __main__ -   Batch number = 144
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  76%|███████▌  | 64/84 [00:38<00:13,  1.52it/s]11/28/2021 01:34:16 - INFO - __main__ -   Batch number = 65
Evaluating:  72%|███████▏  | 144/201 [01:13<00:38,  1.49it/s]11/28/2021 01:34:16 - INFO - __main__ -   Batch number = 145
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  77%|███████▋  | 65/84 [00:38<00:12,  1.52it/s]11/28/2021 01:34:16 - INFO - __main__ -   Batch number = 66
Evaluating:  72%|███████▏  | 145/201 [01:14<00:37,  1.50it/s]11/28/2021 01:34:17 - INFO - __main__ -   Batch number = 146
Evaluating:  79%|███████▊  | 66/84 [00:39<00:11,  1.52it/s]11/28/2021 01:34:17 - INFO - __main__ -   Batch number = 67
Evaluating:  73%|███████▎  | 146/201 [01:14<00:36,  1.50it/s]11/28/2021 01:34:17 - INFO - __main__ -   Batch number = 147
loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:34:17 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
Evaluating:  80%|███████▉  | 67/84 [00:40<00:11,  1.52it/s]11/28/2021 01:34:18 - INFO - __main__ -   Batch number = 68
Evaluating:  73%|███████▎  | 147/201 [01:15<00:36,  1.50it/s]11/28/2021 01:34:18 - INFO - __main__ -   Batch number = 148
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  81%|████████  | 68/84 [00:40<00:10,  1.52it/s]11/28/2021 01:34:18 - INFO - __main__ -   Batch number = 69
Evaluating:  74%|███████▎  | 148/201 [01:16<00:35,  1.51it/s]11/28/2021 01:34:19 - INFO - __main__ -   Batch number = 149
Evaluating:  82%|████████▏ | 69/84 [00:41<00:09,  1.53it/s]11/28/2021 01:34:19 - INFO - __main__ -   Batch number = 70
Evaluating:  74%|███████▍  | 149/201 [01:16<00:34,  1.51it/s]11/28/2021 01:34:19 - INFO - __main__ -   Batch number = 150
Evaluating:  83%|████████▎ | 70/84 [00:41<00:09,  1.54it/s]11/28/2021 01:34:20 - INFO - __main__ -   Batch number = 71
Evaluating:  75%|███████▍  | 150/201 [01:17<00:33,  1.53it/s]11/28/2021 01:34:20 - INFO - __main__ -   Batch number = 151
Evaluating:  85%|████████▍ | 71/84 [00:42<00:08,  1.54it/s]11/28/2021 01:34:20 - INFO - __main__ -   Batch number = 72
Evaluating:  75%|███████▌  | 151/201 [01:17<00:32,  1.52it/s]11/28/2021 01:34:20 - INFO - __main__ -   Batch number = 152
Evaluating:  86%|████████▌ | 72/84 [00:43<00:07,  1.53it/s]11/28/2021 01:34:21 - INFO - __main__ -   Batch number = 73
Evaluating:  76%|███████▌  | 152/201 [01:18<00:32,  1.52it/s]11/28/2021 01:34:21 - INFO - __main__ -   Batch number = 153
Evaluating:  87%|████████▋ | 73/84 [00:43<00:07,  1.53it/s]11/28/2021 01:34:22 - INFO - __main__ -   Batch number = 74
Evaluating:  76%|███████▌  | 153/201 [01:19<00:31,  1.51it/s]11/28/2021 01:34:22 - INFO - __main__ -   Batch number = 154
Evaluating:  88%|████████▊ | 74/84 [00:44<00:06,  1.53it/s]11/28/2021 01:34:22 - INFO - __main__ -   Batch number = 75
Evaluating:  77%|███████▋  | 154/201 [01:19<00:31,  1.51it/s]11/28/2021 01:34:22 - INFO - __main__ -   Batch number = 155
Evaluating:  89%|████████▉ | 75/84 [00:45<00:05,  1.53it/s]11/28/2021 01:34:23 - INFO - __main__ -   Batch number = 76
Evaluating:  77%|███████▋  | 155/201 [01:20<00:30,  1.51it/s]11/28/2021 01:34:23 - INFO - __main__ -   Batch number = 156
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:34:23 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:34:23 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:34:23 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
11/28/2021 01:34:23 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:34:23 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:34:24 - INFO - root -   loading lang adpater bh/wiki@ukp
11/28/2021 01:34:24 - INFO - __main__ -   Adapter Languages : ['bh'], Length : 1
11/28/2021 01:34:24 - INFO - __main__ -   Adapter Names ['bh/wiki@ukp'], Length : 1
11/28/2021 01:34:24 - INFO - __main__ -   Language = bh
11/28/2021 01:34:24 - INFO - __main__ -   Adapter Name = bh/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Evaluating:  90%|█████████ | 76/84 [00:45<00:05,  1.53it/s]11/28/2021 01:34:24 - INFO - __main__ -   Batch number = 77
Evaluating:  78%|███████▊  | 156/201 [01:21<00:29,  1.50it/s]11/28/2021 01:34:24 - INFO - __main__ -   Batch number = 157
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/bh/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_bh_wiki_pfeiffer.zip.
Evaluating:  92%|█████████▏| 77/84 [00:46<00:04,  1.52it/s]11/28/2021 01:34:24 - INFO - __main__ -   Batch number = 78
Evaluating:  78%|███████▊  | 157/201 [01:21<00:29,  1.51it/s]11/28/2021 01:34:24 - INFO - __main__ -   Batch number = 158
Evaluating:  93%|█████████▎| 78/84 [00:47<00:03,  1.52it/s]11/28/2021 01:34:25 - INFO - __main__ -   Batch number = 79
Evaluating:  79%|███████▊  | 158/201 [01:22<00:28,  1.49it/s]11/28/2021 01:34:25 - INFO - __main__ -   Batch number = 159
Evaluating:  94%|█████████▍| 79/84 [00:47<00:03,  1.52it/s]11/28/2021 01:34:26 - INFO - __main__ -   Batch number = 80
Evaluating:  79%|███████▉  | 159/201 [01:23<00:27,  1.50it/s]11/28/2021 01:34:26 - INFO - __main__ -   Batch number = 160
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/adapter_config.json
Adding adapter 'bh' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/pytorch_adapter.bin
Evaluating:  95%|█████████▌| 80/84 [00:48<00:02,  1.56it/s]11/28/2021 01:34:26 - INFO - __main__ -   Batch number = 81
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/head_config.json
Evaluating:  80%|███████▉  | 160/201 [01:23<00:25,  1.59it/s]11/28/2021 01:34:26 - INFO - __main__ -   Batch number = 161
Evaluating:  96%|█████████▋| 81/84 [00:48<00:01,  1.79it/s]11/28/2021 01:34:27 - INFO - __main__ -   Batch number = 82
Evaluating:  80%|████████  | 161/201 [01:24<00:22,  1.81it/s]11/28/2021 01:34:27 - INFO - __main__ -   Batch number = 162
Evaluating:  98%|█████████▊| 82/84 [00:49<00:00,  2.04it/s]11/28/2021 01:34:27 - INFO - __main__ -   Batch number = 83
Evaluating:  81%|████████  | 162/201 [01:24<00:18,  2.06it/s]11/28/2021 01:34:27 - INFO - __main__ -   Batch number = 163
Evaluating:  99%|█████████▉| 83/84 [00:49<00:00,  2.27it/s]11/28/2021 01:34:27 - INFO - __main__ -   Batch number = 84
Evaluating:  81%|████████  | 163/201 [01:24<00:16,  2.25it/s]11/28/2021 01:34:27 - INFO - __main__ -   Batch number = 164
Evaluating: 100%|██████████| 84/84 [00:49<00:00,  2.61it/s]Evaluating: 100%|██████████| 84/84 [00:49<00:00,  1.69it/s]Evaluating:  82%|████████▏ | 164/201 [01:25<00:13,  2.65it/s]11/28/2021 01:34:28 - INFO - __main__ -   Batch number = 165
Evaluating:  82%|████████▏ | 165/201 [01:25<00:11,  3.15it/s]11/28/2021 01:34:28 - INFO - __main__ -   Batch number = 166
Evaluating:  83%|████████▎ | 166/201 [01:25<00:09,  3.69it/s]11/28/2021 01:34:28 - INFO - __main__ -   Batch number = 167
Evaluating:  83%|████████▎ | 167/201 [01:25<00:08,  4.18it/s]11/28/2021 01:34:28 - INFO - __main__ -   Batch number = 168
Evaluating:  84%|████████▎ | 168/201 [01:25<00:07,  4.63it/s]11/28/2021 01:34:28 - INFO - __main__ -   Batch number = 169
Evaluating:  84%|████████▍ | 169/201 [01:25<00:06,  4.97it/s]11/28/2021 01:34:28 - INFO - __main__ -   Batch number = 170
Evaluating:  85%|████████▍ | 170/201 [01:26<00:05,  5.33it/s]11/28/2021 01:34:29 - INFO - __main__ -   Batch number = 171
Evaluating:  85%|████████▌ | 171/201 [01:26<00:05,  5.55it/s]11/28/2021 01:34:29 - INFO - __main__ -   Batch number = 172
Evaluating:  86%|████████▌ | 172/201 [01:26<00:04,  5.83it/s]11/28/2021 01:34:29 - INFO - __main__ -   Batch number = 173

/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:34:29 - INFO - __main__ -   ***** Evaluation result  in pt *****
11/28/2021 01:34:29 - INFO - __main__ -     f1 = 0.8721901315331616
11/28/2021 01:34:29 - INFO - __main__ -     loss = 0.40673663431689855
11/28/2021 01:34:29 - INFO - __main__ -     precision = 0.8782256792978399
11/28/2021 01:34:29 - INFO - __main__ -     recall = 0.8662369753417083
Evaluating:  86%|████████▌ | 173/201 [01:26<00:04,  5.78it/s]11/28/2021 01:34:29 - INFO - __main__ -   Batch number = 174
Evaluating:  87%|████████▋ | 174/201 [01:26<00:04,  5.95it/s]11/28/2021 01:34:29 - INFO - __main__ -   Batch number = 175
Evaluating:  87%|████████▋ | 175/201 [01:26<00:04,  6.00it/s]11/28/2021 01:34:29 - INFO - __main__ -   Batch number = 176
Evaluating:  88%|████████▊ | 176/201 [01:27<00:04,  5.40it/s]11/28/2021 01:34:30 - INFO - __main__ -   Batch number = 177
Evaluating:  88%|████████▊ | 177/201 [01:27<00:04,  5.54it/s]11/28/2021 01:34:30 - INFO - __main__ -   Batch number = 178
Evaluating:  89%|████████▊ | 178/201 [01:27<00:03,  5.81it/s]11/28/2021 01:34:30 - INFO - __main__ -   Batch number = 179
Evaluating:  89%|████████▉ | 179/201 [01:27<00:03,  5.89it/s]11/28/2021 01:34:30 - INFO - __main__ -   Batch number = 180
Evaluating:  90%|████████▉ | 180/201 [01:27<00:03,  6.04it/s]11/28/2021 01:34:30 - INFO - __main__ -   Batch number = 181
54.99user 19.60system 1:14.04elapsed 100%CPU (0avgtext+0avgdata 3989776maxresident)k
0inputs+664outputs (0major+1453103minor)pagefaults 0swaps
Evaluating:  90%|█████████ | 181/201 [01:27<00:03,  6.05it/s]11/28/2021 01:34:30 - INFO - __main__ -   Batch number = 182
Evaluating:  91%|█████████ | 182/201 [01:28<00:03,  6.09it/s]11/28/2021 01:34:31 - INFO - __main__ -   Batch number = 183
Evaluating:  91%|█████████ | 183/201 [01:28<00:03,  5.93it/s]11/28/2021 01:34:31 - INFO - __main__ -   Batch number = 184
Evaluating:  92%|█████████▏| 184/201 [01:28<00:02,  6.04it/s]11/28/2021 01:34:31 - INFO - __main__ -   Batch number = 185
PyTorch version 1.10.0+cu102 available.
Evaluating:  92%|█████████▏| 185/201 [01:28<00:02,  6.02it/s]11/28/2021 01:34:31 - INFO - __main__ -   Batch number = 186
Evaluating:  93%|█████████▎| 186/201 [01:28<00:02,  6.09it/s]11/28/2021 01:34:31 - INFO - __main__ -   Batch number = 187
11/28/2021 01:34:31 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:34:31 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:34:31 - INFO - __main__ -   Seed = 1
11/28/2021 01:34:31 - INFO - root -   save model
11/28/2021 01:34:31 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:34:31 - INFO - __main__ -   Loading pretrained model and tokenizer
Evaluating:  93%|█████████▎| 187/201 [01:28<00:02,  6.04it/s]11/28/2021 01:34:31 - INFO - __main__ -   Batch number = 188
Evaluating:  94%|█████████▎| 188/201 [01:29<00:02,  6.14it/s]11/28/2021 01:34:32 - INFO - __main__ -   Batch number = 189
Evaluating:  94%|█████████▍| 189/201 [01:29<00:01,  6.11it/s]11/28/2021 01:34:32 - INFO - __main__ -   Batch number = 190
Evaluating:  95%|█████████▍| 190/201 [01:29<00:01,  6.18it/s]11/28/2021 01:34:32 - INFO - __main__ -   Batch number = 191
Evaluating:  95%|█████████▌| 191/201 [01:29<00:01,  6.02it/s]11/28/2021 01:34:32 - INFO - __main__ -   Batch number = 192
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  96%|█████████▌| 192/201 [01:29<00:01,  6.04it/s]11/28/2021 01:34:32 - INFO - __main__ -   Batch number = 193
Evaluating:  96%|█████████▌| 193/201 [01:29<00:01,  5.94it/s]11/28/2021 01:34:32 - INFO - __main__ -   Batch number = 194
Evaluating:  97%|█████████▋| 194/201 [01:30<00:01,  6.02it/s]11/28/2021 01:34:33 - INFO - __main__ -   Batch number = 195
Evaluating:  97%|█████████▋| 195/201 [01:30<00:01,  5.84it/s]11/28/2021 01:34:33 - INFO - __main__ -   Batch number = 196
Evaluating:  98%|█████████▊| 196/201 [01:30<00:00,  5.80it/s]11/28/2021 01:34:33 - INFO - __main__ -   Batch number = 197
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  98%|█████████▊| 197/201 [01:30<00:00,  5.70it/s]11/28/2021 01:34:33 - INFO - __main__ -   Batch number = 198
Evaluating:  99%|█████████▊| 198/201 [01:30<00:00,  5.79it/s]11/28/2021 01:34:33 - INFO - __main__ -   Batch number = 199
Evaluating:  99%|█████████▉| 199/201 [01:31<00:00,  5.81it/s]11/28/2021 01:34:33 - INFO - __main__ -   Batch number = 200
Evaluating: 100%|█████████▉| 200/201 [01:31<00:00,  5.91it/s]11/28/2021 01:34:34 - INFO - __main__ -   Batch number = 201
Evaluating: 100%|██████████| 201/201 [01:31<00:00,  2.20it/s]loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:34:34 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052

/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:34:38 - INFO - __main__ -   ***** Evaluation result  in is *****
11/28/2021 01:34:38 - INFO - __main__ -     f1 = 0.7133576083431137
11/28/2021 01:34:38 - INFO - __main__ -     loss = 0.9396549217143462
11/28/2021 01:34:38 - INFO - __main__ -     precision = 0.7212605985206021
11/28/2021 01:34:38 - INFO - __main__ -     recall = 0.705625930213329
11/28/2021 01:34:38 - INFO - __main__ -   Language adapter for zh not found, using bh instead
11/28/2021 01:34:38 - INFO - __main__ -   Set active language adapter to bh
11/28/2021 01:34:38 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:34:38 - INFO - __main__ -   Adapter Languages = ['bh']
11/28/2021 01:34:38 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_zh_bert-base-multilingual-cased_128
90.89user 30.68system 2:01.24elapsed 100%CPU (0avgtext+0avgdata 3983788maxresident)k
0inputs+1720outputs (0major+1829688minor)pagefaults 0swaps
11/28/2021 01:34:39 - INFO - __main__ -   ***** Running evaluation  in zh *****
11/28/2021 01:34:39 - INFO - __main__ -     Num examples = 3458
11/28/2021 01:34:39 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/109 [00:00<?, ?it/s]11/28/2021 01:34:39 - INFO - __main__ -   Batch number = 1
Evaluating:   1%|          | 1/109 [00:00<00:58,  1.83it/s]11/28/2021 01:34:39 - INFO - __main__ -   Batch number = 2
Evaluating:   2%|▏         | 2/109 [00:01<00:55,  1.94it/s]11/28/2021 01:34:40 - INFO - __main__ -   Batch number = 3
Evaluating:   3%|▎         | 3/109 [00:01<00:53,  1.98it/s]11/28/2021 01:34:40 - INFO - __main__ -   Batch number = 4
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:34:41 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:34:41 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:34:41 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
11/28/2021 01:34:41 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:34:41 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:34:41 - INFO - root -   loading lang adpater bh/wiki@ukp
11/28/2021 01:34:41 - INFO - __main__ -   Adapter Languages : ['bh'], Length : 1
11/28/2021 01:34:41 - INFO - __main__ -   Adapter Names ['bh/wiki@ukp'], Length : 1
11/28/2021 01:34:41 - INFO - __main__ -   Language = bh
11/28/2021 01:34:41 - INFO - __main__ -   Adapter Name = bh/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/bh/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_bh_wiki_pfeiffer.zip.
Evaluating:   4%|▎         | 4/109 [00:02<00:52,  2.00it/s]11/28/2021 01:34:41 - INFO - __main__ -   Batch number = 5
Evaluating:   5%|▍         | 5/109 [00:02<00:51,  2.01it/s]11/28/2021 01:34:41 - INFO - __main__ -   Batch number = 6
Evaluating:   6%|▌         | 6/109 [00:03<00:50,  2.02it/s]11/28/2021 01:34:42 - INFO - __main__ -   Batch number = 7
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/adapter_config.json
Adding adapter 'bh' of type 'text_lang'.
Evaluating:   6%|▋         | 7/109 [00:03<00:50,  2.02it/s]11/28/2021 01:34:42 - INFO - __main__ -   Batch number = 8
Loading module weights from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/head_config.json
Evaluating:   7%|▋         | 8/109 [00:03<00:50,  2.02it/s]11/28/2021 01:34:43 - INFO - __main__ -   Batch number = 9
Evaluating:   8%|▊         | 9/109 [00:04<00:49,  2.02it/s]11/28/2021 01:34:43 - INFO - __main__ -   Batch number = 10
Evaluating:   9%|▉         | 10/109 [00:04<00:48,  2.02it/s]11/28/2021 01:34:44 - INFO - __main__ -   Batch number = 11
Evaluating:  10%|█         | 11/109 [00:05<00:48,  2.03it/s]11/28/2021 01:34:44 - INFO - __main__ -   Batch number = 12
Evaluating:  11%|█         | 12/109 [00:05<00:47,  2.03it/s]11/28/2021 01:34:45 - INFO - __main__ -   Batch number = 13
Evaluating:  12%|█▏        | 13/109 [00:06<00:47,  2.02it/s]11/28/2021 01:34:45 - INFO - __main__ -   Batch number = 14
Evaluating:  13%|█▎        | 14/109 [00:06<00:46,  2.03it/s]11/28/2021 01:34:46 - INFO - __main__ -   Batch number = 15
Evaluating:  14%|█▍        | 15/109 [00:07<00:46,  2.03it/s]11/28/2021 01:34:46 - INFO - __main__ -   Batch number = 16
Evaluating:  15%|█▍        | 16/109 [00:07<00:45,  2.04it/s]11/28/2021 01:34:47 - INFO - __main__ -   Batch number = 17
Evaluating:  16%|█▌        | 17/109 [00:08<00:45,  2.04it/s]11/28/2021 01:34:47 - INFO - __main__ -   Batch number = 18
Evaluating:  17%|█▋        | 18/109 [00:08<00:44,  2.04it/s]11/28/2021 01:34:48 - INFO - __main__ -   Batch number = 19
Evaluating:  17%|█▋        | 19/109 [00:09<00:44,  2.04it/s]11/28/2021 01:34:48 - INFO - __main__ -   Batch number = 20
Evaluating:  18%|█▊        | 20/109 [00:09<00:43,  2.04it/s]11/28/2021 01:34:49 - INFO - __main__ -   Batch number = 21
11/28/2021 01:34:49 - INFO - __main__ -   Language adapter for fi not found, using bh instead
11/28/2021 01:34:49 - INFO - __main__ -   Set active language adapter to bh
11/28/2021 01:34:49 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:34:49 - INFO - __main__ -   Adapter Languages = ['bh']
11/28/2021 01:34:49 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_fi_bert-base-multilingual-cased_128
Evaluating:  19%|█▉        | 21/109 [00:10<00:43,  2.03it/s]11/28/2021 01:34:49 - INFO - __main__ -   Batch number = 22
Evaluating:  20%|██        | 22/109 [00:10<00:42,  2.03it/s]11/28/2021 01:34:50 - INFO - __main__ -   Batch number = 23
11/28/2021 01:34:50 - INFO - __main__ -   ***** Running evaluation  in fi *****
11/28/2021 01:34:50 - INFO - __main__ -     Num examples = 6550
11/28/2021 01:34:50 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/205 [00:00<?, ?it/s]11/28/2021 01:34:50 - INFO - __main__ -   Batch number = 1
Evaluating:  21%|██        | 23/109 [00:11<00:42,  2.01it/s]11/28/2021 01:34:50 - INFO - __main__ -   Batch number = 24
Evaluating:   0%|          | 1/205 [00:00<01:32,  2.20it/s]11/28/2021 01:34:50 - INFO - __main__ -   Batch number = 2
Evaluating:  22%|██▏       | 24/109 [00:11<00:40,  2.12it/s]11/28/2021 01:34:51 - INFO - __main__ -   Batch number = 25
Evaluating:   1%|          | 2/205 [00:00<01:31,  2.21it/s]11/28/2021 01:34:51 - INFO - __main__ -   Batch number = 3
Evaluating:  23%|██▎       | 25/109 [00:12<00:35,  2.34it/s]11/28/2021 01:34:51 - INFO - __main__ -   Batch number = 26
Evaluating:   1%|▏         | 3/205 [00:01<01:31,  2.20it/s]11/28/2021 01:34:51 - INFO - __main__ -   Batch number = 4
Evaluating:  24%|██▍       | 26/109 [00:12<00:33,  2.51it/s]11/28/2021 01:34:51 - INFO - __main__ -   Batch number = 27
Evaluating:  25%|██▍       | 27/109 [00:12<00:31,  2.64it/s]11/28/2021 01:34:52 - INFO - __main__ -   Batch number = 28
Evaluating:   2%|▏         | 4/205 [00:01<01:32,  2.18it/s]11/28/2021 01:34:52 - INFO - __main__ -   Batch number = 5
Evaluating:  26%|██▌       | 28/109 [00:13<00:29,  2.76it/s]11/28/2021 01:34:52 - INFO - __main__ -   Batch number = 29
Evaluating:   2%|▏         | 5/205 [00:02<01:33,  2.15it/s]11/28/2021 01:34:52 - INFO - __main__ -   Batch number = 6
Evaluating:  27%|██▋       | 29/109 [00:13<00:28,  2.85it/s]11/28/2021 01:34:52 - INFO - __main__ -   Batch number = 30
Evaluating:  28%|██▊       | 30/109 [00:13<00:27,  2.90it/s]11/28/2021 01:34:53 - INFO - __main__ -   Batch number = 31
Evaluating:   3%|▎         | 6/205 [00:02<01:33,  2.12it/s]11/28/2021 01:34:53 - INFO - __main__ -   Batch number = 7
Evaluating:  28%|██▊       | 31/109 [00:14<00:26,  2.94it/s]11/28/2021 01:34:53 - INFO - __main__ -   Batch number = 32
Evaluating:   3%|▎         | 7/205 [00:03<01:35,  2.07it/s]11/28/2021 01:34:53 - INFO - __main__ -   Batch number = 8
Evaluating:  29%|██▉       | 32/109 [00:14<00:25,  3.00it/s]11/28/2021 01:34:53 - INFO - __main__ -   Batch number = 33
Evaluating:  30%|███       | 33/109 [00:14<00:25,  3.03it/s]11/28/2021 01:34:54 - INFO - __main__ -   Batch number = 34
Evaluating:   4%|▍         | 8/205 [00:03<01:36,  2.04it/s]11/28/2021 01:34:54 - INFO - __main__ -   Batch number = 9
Evaluating:  31%|███       | 34/109 [00:15<00:24,  3.02it/s]11/28/2021 01:34:54 - INFO - __main__ -   Batch number = 35
Evaluating:   4%|▍         | 9/205 [00:04<01:37,  2.01it/s]11/28/2021 01:34:54 - INFO - __main__ -   Batch number = 10
Evaluating:  32%|███▏      | 35/109 [00:15<00:24,  3.04it/s]11/28/2021 01:34:54 - INFO - __main__ -   Batch number = 36
Evaluating:  33%|███▎      | 36/109 [00:15<00:26,  2.76it/s]Evaluating:   5%|▍         | 10/205 [00:04<01:37,  2.00it/s]11/28/2021 01:34:55 - INFO - __main__ -   Batch number = 11
11/28/2021 01:34:55 - INFO - __main__ -   Batch number = 37
Evaluating:  34%|███▍      | 37/109 [00:16<00:30,  2.37it/s]11/28/2021 01:34:55 - INFO - __main__ -   Batch number = 38
Evaluating:   5%|▌         | 11/205 [00:05<01:39,  1.95it/s]11/28/2021 01:34:55 - INFO - __main__ -   Batch number = 12
Evaluating:  35%|███▍      | 38/109 [00:16<00:31,  2.26it/s]11/28/2021 01:34:56 - INFO - __main__ -   Batch number = 39
Evaluating:   6%|▌         | 12/205 [00:05<01:38,  1.97it/s]11/28/2021 01:34:56 - INFO - __main__ -   Batch number = 13
Evaluating:  36%|███▌      | 39/109 [00:17<00:32,  2.18it/s]11/28/2021 01:34:56 - INFO - __main__ -   Batch number = 40
Evaluating:   6%|▋         | 13/205 [00:06<01:36,  1.98it/s]11/28/2021 01:34:56 - INFO - __main__ -   Batch number = 14
Evaluating:  37%|███▋      | 40/109 [00:17<00:32,  2.13it/s]11/28/2021 01:34:57 - INFO - __main__ -   Batch number = 41
Evaluating:   7%|▋         | 14/205 [00:06<01:35,  2.00it/s]11/28/2021 01:34:57 - INFO - __main__ -   Batch number = 15
Evaluating:  38%|███▊      | 41/109 [00:18<00:32,  2.10it/s]11/28/2021 01:34:57 - INFO - __main__ -   Batch number = 42
Evaluating:   7%|▋         | 15/205 [00:07<01:35,  1.99it/s]11/28/2021 01:34:57 - INFO - __main__ -   Batch number = 16
Evaluating:  39%|███▊      | 42/109 [00:18<00:32,  2.08it/s]11/28/2021 01:34:58 - INFO - __main__ -   Batch number = 43
Evaluating:   8%|▊         | 16/205 [00:07<01:34,  2.00it/s]11/28/2021 01:34:58 - INFO - __main__ -   Batch number = 17
Evaluating:  39%|███▉      | 43/109 [00:19<00:32,  2.06it/s]11/28/2021 01:34:58 - INFO - __main__ -   Batch number = 44
Evaluating:   8%|▊         | 17/205 [00:08<01:34,  1.99it/s]11/28/2021 01:34:58 - INFO - __main__ -   Batch number = 18
Evaluating:  40%|████      | 44/109 [00:19<00:31,  2.06it/s]11/28/2021 01:34:59 - INFO - __main__ -   Batch number = 45
Evaluating:   9%|▉         | 18/205 [00:08<01:33,  1.99it/s]11/28/2021 01:34:59 - INFO - __main__ -   Batch number = 19
Evaluating:  41%|████▏     | 45/109 [00:20<00:30,  2.07it/s]11/28/2021 01:34:59 - INFO - __main__ -   Batch number = 46
Evaluating:   9%|▉         | 19/205 [00:09<01:33,  1.99it/s]11/28/2021 01:34:59 - INFO - __main__ -   Batch number = 20
Evaluating:  42%|████▏     | 46/109 [00:20<00:30,  2.06it/s]11/28/2021 01:35:00 - INFO - __main__ -   Batch number = 47
Evaluating:  10%|▉         | 20/205 [00:09<01:32,  1.99it/s]11/28/2021 01:35:00 - INFO - __main__ -   Batch number = 21
Evaluating:  43%|████▎     | 47/109 [00:21<00:30,  2.05it/s]11/28/2021 01:35:00 - INFO - __main__ -   Batch number = 48
Evaluating:  10%|█         | 21/205 [00:10<01:32,  1.99it/s]11/28/2021 01:35:00 - INFO - __main__ -   Batch number = 22
Evaluating:  44%|████▍     | 48/109 [00:21<00:29,  2.04it/s]11/28/2021 01:35:01 - INFO - __main__ -   Batch number = 49
Evaluating:  11%|█         | 22/205 [00:10<01:32,  1.99it/s]11/28/2021 01:35:01 - INFO - __main__ -   Batch number = 23
Evaluating:  45%|████▍     | 49/109 [00:22<00:29,  2.03it/s]11/28/2021 01:35:01 - INFO - __main__ -   Batch number = 50
Evaluating:  11%|█         | 23/205 [00:11<01:31,  1.99it/s]11/28/2021 01:35:01 - INFO - __main__ -   Batch number = 24
Evaluating:  46%|████▌     | 50/109 [00:22<00:29,  2.02it/s]11/28/2021 01:35:02 - INFO - __main__ -   Batch number = 51
Evaluating:  12%|█▏        | 24/205 [00:11<01:31,  1.99it/s]11/28/2021 01:35:02 - INFO - __main__ -   Batch number = 25
Evaluating:  47%|████▋     | 51/109 [00:23<00:28,  2.02it/s]11/28/2021 01:35:02 - INFO - __main__ -   Batch number = 52
Evaluating:  12%|█▏        | 25/205 [00:12<01:30,  1.99it/s]11/28/2021 01:35:02 - INFO - __main__ -   Batch number = 26
Evaluating:  48%|████▊     | 52/109 [00:23<00:28,  2.02it/s]11/28/2021 01:35:03 - INFO - __main__ -   Batch number = 53
Evaluating:  13%|█▎        | 26/205 [00:12<01:30,  1.99it/s]11/28/2021 01:35:03 - INFO - __main__ -   Batch number = 27
Evaluating:  49%|████▊     | 53/109 [00:24<00:27,  2.02it/s]11/28/2021 01:35:03 - INFO - __main__ -   Batch number = 54
Evaluating:  13%|█▎        | 27/205 [00:13<01:29,  1.99it/s]11/28/2021 01:35:03 - INFO - __main__ -   Batch number = 28
Evaluating:  50%|████▉     | 54/109 [00:24<00:27,  2.01it/s]11/28/2021 01:35:04 - INFO - __main__ -   Batch number = 55
Evaluating:  14%|█▎        | 28/205 [00:13<01:30,  1.95it/s]11/28/2021 01:35:04 - INFO - __main__ -   Batch number = 29
Evaluating:  50%|█████     | 55/109 [00:25<00:26,  2.01it/s]11/28/2021 01:35:04 - INFO - __main__ -   Batch number = 56
Evaluating:  14%|█▍        | 29/205 [00:14<01:38,  1.78it/s]11/28/2021 01:35:04 - INFO - __main__ -   Batch number = 30
Evaluating:  51%|█████▏    | 56/109 [00:25<00:26,  2.01it/s]11/28/2021 01:35:05 - INFO - __main__ -   Batch number = 57
Evaluating:  15%|█▍        | 30/205 [00:15<01:41,  1.73it/s]11/28/2021 01:35:05 - INFO - __main__ -   Batch number = 31
Evaluating:  52%|█████▏    | 57/109 [00:26<00:26,  1.95it/s]11/28/2021 01:35:05 - INFO - __main__ -   Batch number = 58
Evaluating:  53%|█████▎    | 58/109 [00:26<00:26,  1.96it/s]11/28/2021 01:35:06 - INFO - __main__ -   Batch number = 59
Evaluating:  15%|█▌        | 31/205 [00:15<01:45,  1.65it/s]11/28/2021 01:35:06 - INFO - __main__ -   Batch number = 32
Evaluating:  54%|█████▍    | 59/109 [00:27<00:25,  1.97it/s]11/28/2021 01:35:06 - INFO - __main__ -   Batch number = 60
Evaluating:  16%|█▌        | 32/205 [00:16<01:47,  1.61it/s]11/28/2021 01:35:06 - INFO - __main__ -   Batch number = 33
Evaluating:  55%|█████▌    | 60/109 [00:27<00:24,  1.98it/s]11/28/2021 01:35:07 - INFO - __main__ -   Batch number = 61
Evaluating:  16%|█▌        | 33/205 [00:17<01:49,  1.57it/s]11/28/2021 01:35:07 - INFO - __main__ -   Batch number = 34
Evaluating:  56%|█████▌    | 61/109 [00:28<00:24,  1.98it/s]11/28/2021 01:35:07 - INFO - __main__ -   Batch number = 62
Evaluating:  57%|█████▋    | 62/109 [00:28<00:23,  1.99it/s]11/28/2021 01:35:08 - INFO - __main__ -   Batch number = 63
Evaluating:  17%|█▋        | 34/205 [00:17<01:50,  1.55it/s]11/28/2021 01:35:08 - INFO - __main__ -   Batch number = 35
Evaluating:  58%|█████▊    | 63/109 [00:29<00:23,  2.00it/s]11/28/2021 01:35:08 - INFO - __main__ -   Batch number = 64
Evaluating:  17%|█▋        | 35/205 [00:18<01:50,  1.53it/s]11/28/2021 01:35:08 - INFO - __main__ -   Batch number = 36
Evaluating:  59%|█████▊    | 64/109 [00:29<00:22,  2.00it/s]11/28/2021 01:35:09 - INFO - __main__ -   Batch number = 65
Evaluating:  18%|█▊        | 36/205 [00:19<01:47,  1.58it/s]11/28/2021 01:35:09 - INFO - __main__ -   Batch number = 37
Evaluating:  60%|█████▉    | 65/109 [00:30<00:21,  2.01it/s]11/28/2021 01:35:09 - INFO - __main__ -   Batch number = 66
Evaluating:  18%|█▊        | 37/205 [00:19<01:40,  1.66it/s]Evaluating:  61%|██████    | 66/109 [00:30<00:21,  2.01it/s]11/28/2021 01:35:10 - INFO - __main__ -   Batch number = 67
11/28/2021 01:35:10 - INFO - __main__ -   Batch number = 38
Evaluating:  19%|█▊        | 38/205 [00:20<01:44,  1.59it/s]11/28/2021 01:35:10 - INFO - __main__ -   Batch number = 39
Evaluating:  61%|██████▏   | 67/109 [00:31<00:21,  1.92it/s]11/28/2021 01:35:10 - INFO - __main__ -   Batch number = 68
Evaluating:  19%|█▉        | 39/205 [00:20<01:35,  1.73it/s]11/28/2021 01:35:11 - INFO - __main__ -   Batch number = 40
Evaluating:  62%|██████▏   | 68/109 [00:31<00:21,  1.95it/s]11/28/2021 01:35:11 - INFO - __main__ -   Batch number = 69
Evaluating:  20%|█▉        | 40/205 [00:21<01:30,  1.83it/s]11/28/2021 01:35:11 - INFO - __main__ -   Batch number = 41
Evaluating:  63%|██████▎   | 69/109 [00:32<00:20,  2.00it/s]11/28/2021 01:35:11 - INFO - __main__ -   Batch number = 70
Evaluating:  20%|██        | 41/205 [00:21<01:25,  1.91it/s]11/28/2021 01:35:12 - INFO - __main__ -   Batch number = 42
Evaluating:  64%|██████▍   | 70/109 [00:32<00:19,  2.00it/s]11/28/2021 01:35:12 - INFO - __main__ -   Batch number = 71
Evaluating:  20%|██        | 42/205 [00:22<01:23,  1.96it/s]11/28/2021 01:35:12 - INFO - __main__ -   Batch number = 43
Evaluating:  65%|██████▌   | 71/109 [00:33<00:18,  2.01it/s]11/28/2021 01:35:12 - INFO - __main__ -   Batch number = 72
Evaluating:  21%|██        | 43/205 [00:22<01:21,  1.98it/s]11/28/2021 01:35:13 - INFO - __main__ -   Batch number = 44
Evaluating:  66%|██████▌   | 72/109 [00:33<00:18,  2.01it/s]11/28/2021 01:35:13 - INFO - __main__ -   Batch number = 73
Evaluating:  21%|██▏       | 44/205 [00:23<01:21,  1.98it/s]11/28/2021 01:35:13 - INFO - __main__ -   Batch number = 45
Evaluating:  67%|██████▋   | 73/109 [00:34<00:17,  2.01it/s]11/28/2021 01:35:13 - INFO - __main__ -   Batch number = 74
Evaluating:  22%|██▏       | 45/205 [00:23<01:20,  1.98it/s]11/28/2021 01:35:14 - INFO - __main__ -   Batch number = 46
Evaluating:  68%|██████▊   | 74/109 [00:34<00:17,  2.01it/s]11/28/2021 01:35:14 - INFO - __main__ -   Batch number = 75
Evaluating:  22%|██▏       | 46/205 [00:24<01:20,  1.98it/s]11/28/2021 01:35:14 - INFO - __main__ -   Batch number = 47
Evaluating:  69%|██████▉   | 75/109 [00:35<00:16,  2.01it/s]11/28/2021 01:35:14 - INFO - __main__ -   Batch number = 76
Evaluating:  23%|██▎       | 47/205 [00:24<01:18,  2.02it/s]Evaluating:  70%|██████▉   | 76/109 [00:35<00:16,  2.05it/s]11/28/2021 01:35:15 - INFO - __main__ -   Batch number = 48
11/28/2021 01:35:15 - INFO - __main__ -   Batch number = 77
Evaluating:  23%|██▎       | 48/205 [00:25<01:32,  1.70it/s]11/28/2021 01:35:15 - INFO - __main__ -   Batch number = 49
Evaluating:  71%|███████   | 77/109 [00:36<00:18,  1.78it/s]11/28/2021 01:35:15 - INFO - __main__ -   Batch number = 78
Evaluating:  24%|██▍       | 49/205 [00:26<01:27,  1.78it/s]11/28/2021 01:35:16 - INFO - __main__ -   Batch number = 50
Evaluating:  72%|███████▏  | 78/109 [00:37<00:16,  1.82it/s]11/28/2021 01:35:16 - INFO - __main__ -   Batch number = 79
Evaluating:  24%|██▍       | 50/205 [00:26<01:24,  1.83it/s]11/28/2021 01:35:16 - INFO - __main__ -   Batch number = 51
Evaluating:  72%|███████▏  | 79/109 [00:37<00:16,  1.86it/s]11/28/2021 01:35:16 - INFO - __main__ -   Batch number = 80
Evaluating:  25%|██▍       | 51/205 [00:27<01:22,  1.87it/s]11/28/2021 01:35:17 - INFO - __main__ -   Batch number = 52
Evaluating:  73%|███████▎  | 80/109 [00:38<00:15,  1.90it/s]11/28/2021 01:35:17 - INFO - __main__ -   Batch number = 81
Evaluating:  25%|██▌       | 52/205 [00:27<01:20,  1.90it/s]11/28/2021 01:35:17 - INFO - __main__ -   Batch number = 53
Evaluating:  74%|███████▍  | 81/109 [00:38<00:14,  1.92it/s]11/28/2021 01:35:17 - INFO - __main__ -   Batch number = 82
Evaluating:  26%|██▌       | 53/205 [00:28<01:19,  1.92it/s]11/28/2021 01:35:18 - INFO - __main__ -   Batch number = 54
Evaluating:  75%|███████▌  | 82/109 [00:39<00:13,  1.93it/s]11/28/2021 01:35:18 - INFO - __main__ -   Batch number = 83
Evaluating:  26%|██▋       | 54/205 [00:28<01:18,  1.93it/s]11/28/2021 01:35:18 - INFO - __main__ -   Batch number = 55
Evaluating:  76%|███████▌  | 83/109 [00:39<00:14,  1.83it/s]11/28/2021 01:35:18 - INFO - __main__ -   Batch number = 84
Evaluating:  27%|██▋       | 55/205 [00:29<01:17,  1.94it/s]11/28/2021 01:35:19 - INFO - __main__ -   Batch number = 56
Evaluating:  77%|███████▋  | 84/109 [00:40<00:14,  1.72it/s]11/28/2021 01:35:19 - INFO - __main__ -   Batch number = 85
Evaluating:  27%|██▋       | 56/205 [00:29<01:16,  1.95it/s]11/28/2021 01:35:19 - INFO - __main__ -   Batch number = 57
Evaluating:  78%|███████▊  | 85/109 [00:41<00:14,  1.67it/s]11/28/2021 01:35:20 - INFO - __main__ -   Batch number = 86
Evaluating:  28%|██▊       | 57/205 [00:30<01:14,  1.98it/s]11/28/2021 01:35:20 - INFO - __main__ -   Batch number = 58
Evaluating:  79%|███████▉  | 86/109 [00:41<00:13,  1.75it/s]11/28/2021 01:35:20 - INFO - __main__ -   Batch number = 87
Evaluating:  28%|██▊       | 58/205 [00:30<01:14,  1.98it/s]11/28/2021 01:35:20 - INFO - __main__ -   Batch number = 59
Evaluating:  29%|██▉       | 59/205 [00:31<01:13,  1.97it/s]11/28/2021 01:35:21 - INFO - __main__ -   Batch number = 60
Evaluating:  80%|███████▉  | 87/109 [00:42<00:13,  1.66it/s]11/28/2021 01:35:21 - INFO - __main__ -   Batch number = 88
Evaluating:  29%|██▉       | 60/205 [00:31<01:13,  1.98it/s]11/28/2021 01:35:21 - INFO - __main__ -   Batch number = 61
Evaluating:  81%|████████  | 88/109 [00:42<00:12,  1.62it/s]11/28/2021 01:35:22 - INFO - __main__ -   Batch number = 89
Evaluating:  30%|██▉       | 61/205 [00:32<01:12,  1.98it/s]11/28/2021 01:35:22 - INFO - __main__ -   Batch number = 62
Evaluating:  82%|████████▏ | 89/109 [00:43<00:12,  1.58it/s]11/28/2021 01:35:22 - INFO - __main__ -   Batch number = 90
Evaluating:  30%|███       | 62/205 [00:32<01:12,  1.97it/s]11/28/2021 01:35:22 - INFO - __main__ -   Batch number = 63
Evaluating:  31%|███       | 63/205 [00:33<01:11,  1.98it/s]11/28/2021 01:35:23 - INFO - __main__ -   Batch number = 64
Evaluating:  83%|████████▎ | 90/109 [00:44<00:12,  1.56it/s]11/28/2021 01:35:23 - INFO - __main__ -   Batch number = 91
Evaluating:  31%|███       | 64/205 [00:33<01:11,  1.98it/s]11/28/2021 01:35:23 - INFO - __main__ -   Batch number = 65
Evaluating:  83%|████████▎ | 91/109 [00:44<00:11,  1.53it/s]11/28/2021 01:35:24 - INFO - __main__ -   Batch number = 92
Evaluating:  32%|███▏      | 65/205 [00:34<01:10,  1.99it/s]11/28/2021 01:35:24 - INFO - __main__ -   Batch number = 66
Evaluating:  84%|████████▍ | 92/109 [00:45<00:11,  1.53it/s]11/28/2021 01:35:24 - INFO - __main__ -   Batch number = 93
Evaluating:  32%|███▏      | 66/205 [00:34<01:09,  2.00it/s]11/28/2021 01:35:24 - INFO - __main__ -   Batch number = 67
Evaluating:  85%|████████▌ | 93/109 [00:46<00:09,  1.60it/s]11/28/2021 01:35:25 - INFO - __main__ -   Batch number = 94
Evaluating:  33%|███▎      | 67/205 [00:35<01:09,  2.00it/s]11/28/2021 01:35:25 - INFO - __main__ -   Batch number = 68
Evaluating:  33%|███▎      | 68/205 [00:35<01:08,  1.99it/s]11/28/2021 01:35:25 - INFO - __main__ -   Batch number = 69
Evaluating:  86%|████████▌ | 94/109 [00:46<00:09,  1.58it/s]11/28/2021 01:35:25 - INFO - __main__ -   Batch number = 95
Evaluating:  34%|███▎      | 69/205 [00:36<01:08,  1.99it/s]11/28/2021 01:35:26 - INFO - __main__ -   Batch number = 70
Evaluating:  87%|████████▋ | 95/109 [00:47<00:08,  1.56it/s]11/28/2021 01:35:26 - INFO - __main__ -   Batch number = 96
Evaluating:  34%|███▍      | 70/205 [00:36<01:08,  1.97it/s]11/28/2021 01:35:26 - INFO - __main__ -   Batch number = 71
Evaluating:  88%|████████▊ | 96/109 [00:48<00:08,  1.56it/s]11/28/2021 01:35:27 - INFO - __main__ -   Batch number = 97
Evaluating:  35%|███▍      | 71/205 [00:37<01:08,  1.96it/s]11/28/2021 01:35:27 - INFO - __main__ -   Batch number = 72
Evaluating:  35%|███▌      | 72/205 [00:37<01:07,  1.96it/s]11/28/2021 01:35:27 - INFO - __main__ -   Batch number = 73
Evaluating:  89%|████████▉ | 97/109 [00:48<00:07,  1.54it/s]11/28/2021 01:35:27 - INFO - __main__ -   Batch number = 98
Evaluating:  36%|███▌      | 73/205 [00:38<01:07,  1.96it/s]11/28/2021 01:35:28 - INFO - __main__ -   Batch number = 74
Evaluating:  90%|████████▉ | 98/109 [00:49<00:07,  1.53it/s]11/28/2021 01:35:28 - INFO - __main__ -   Batch number = 99
Evaluating:  36%|███▌      | 74/205 [00:38<01:06,  1.96it/s]11/28/2021 01:35:28 - INFO - __main__ -   Batch number = 75
Evaluating:  91%|█████████ | 99/109 [00:50<00:06,  1.52it/s]11/28/2021 01:35:29 - INFO - __main__ -   Batch number = 100
Evaluating:  37%|███▋      | 75/205 [00:39<01:06,  1.96it/s]11/28/2021 01:35:29 - INFO - __main__ -   Batch number = 76
Evaluating:  92%|█████████▏| 100/109 [00:50<00:05,  1.51it/s]11/28/2021 01:35:29 - INFO - __main__ -   Batch number = 101
Evaluating:  37%|███▋      | 76/205 [00:39<01:06,  1.95it/s]11/28/2021 01:35:29 - INFO - __main__ -   Batch number = 77
Evaluating:  38%|███▊      | 77/205 [00:40<01:05,  1.94it/s]11/28/2021 01:35:30 - INFO - __main__ -   Batch number = 78
Evaluating:  93%|█████████▎| 101/109 [00:51<00:05,  1.52it/s]11/28/2021 01:35:30 - INFO - __main__ -   Batch number = 102
Evaluating:  38%|███▊      | 78/205 [00:40<01:04,  1.97it/s]11/28/2021 01:35:31 - INFO - __main__ -   Batch number = 79
Evaluating:  94%|█████████▎| 102/109 [00:52<00:04,  1.51it/s]11/28/2021 01:35:31 - INFO - __main__ -   Batch number = 103
Evaluating:  39%|███▊      | 79/205 [00:41<01:03,  1.97it/s]11/28/2021 01:35:31 - INFO - __main__ -   Batch number = 80
Evaluating:  94%|█████████▍| 103/109 [00:52<00:03,  1.51it/s]11/28/2021 01:35:31 - INFO - __main__ -   Batch number = 104
Evaluating:  39%|███▉      | 80/205 [00:41<01:03,  1.98it/s]11/28/2021 01:35:32 - INFO - __main__ -   Batch number = 81
Evaluating:  40%|███▉      | 81/205 [00:42<01:02,  1.98it/s]11/28/2021 01:35:32 - INFO - __main__ -   Batch number = 82
Evaluating:  95%|█████████▌| 104/109 [00:53<00:03,  1.51it/s]11/28/2021 01:35:32 - INFO - __main__ -   Batch number = 105
Evaluating:  40%|████      | 82/205 [00:42<01:02,  1.98it/s]11/28/2021 01:35:33 - INFO - __main__ -   Batch number = 83
Evaluating:  96%|█████████▋| 105/109 [00:54<00:02,  1.51it/s]11/28/2021 01:35:33 - INFO - __main__ -   Batch number = 106
Evaluating:  40%|████      | 83/205 [00:43<01:01,  1.99it/s]11/28/2021 01:35:33 - INFO - __main__ -   Batch number = 84
Evaluating:  97%|█████████▋| 106/109 [00:54<00:01,  1.51it/s]11/28/2021 01:35:33 - INFO - __main__ -   Batch number = 107
Evaluating:  41%|████      | 84/205 [00:43<00:57,  2.09it/s]11/28/2021 01:35:33 - INFO - __main__ -   Batch number = 85
Evaluating:  41%|████▏     | 85/205 [00:43<00:52,  2.28it/s]11/28/2021 01:35:34 - INFO - __main__ -   Batch number = 86
Evaluating:  98%|█████████▊| 107/109 [00:55<00:01,  1.50it/s]11/28/2021 01:35:34 - INFO - __main__ -   Batch number = 108
Evaluating:  42%|████▏     | 86/205 [00:44<00:48,  2.45it/s]11/28/2021 01:35:34 - INFO - __main__ -   Batch number = 87
Evaluating:  42%|████▏     | 87/205 [00:44<00:45,  2.58it/s]11/28/2021 01:35:34 - INFO - __main__ -   Batch number = 88
Evaluating:  99%|█████████▉| 108/109 [00:55<00:00,  1.56it/s]11/28/2021 01:35:35 - INFO - __main__ -   Batch number = 109
Evaluating: 100%|██████████| 109/109 [00:56<00:00,  2.04it/s]Evaluating: 100%|██████████| 109/109 [00:56<00:00,  1.94it/s]Evaluating:  43%|████▎     | 88/205 [00:45<00:46,  2.52it/s]11/28/2021 01:35:35 - INFO - __main__ -   Batch number = 89
Evaluating:  43%|████▎     | 89/205 [00:45<00:46,  2.49it/s]11/28/2021 01:35:35 - INFO - __main__ -   Batch number = 90
Evaluating:  44%|████▍     | 90/205 [00:46<00:49,  2.31it/s]11/28/2021 01:35:36 - INFO - __main__ -   Batch number = 91
Evaluating:  44%|████▍     | 91/205 [00:46<00:51,  2.20it/s]11/28/2021 01:35:36 - INFO - __main__ -   Batch number = 92

/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:35:36 - INFO - __main__ -   ***** Evaluation result  in zh *****
11/28/2021 01:35:36 - INFO - __main__ -     f1 = 0.5964366896973075
11/28/2021 01:35:36 - INFO - __main__ -     loss = 1.36431677078982
11/28/2021 01:35:36 - INFO - __main__ -     precision = 0.6036116314171925
11/28/2021 01:35:36 - INFO - __main__ -     recall = 0.5894303168063646
Evaluating:  45%|████▍     | 92/205 [00:47<00:53,  2.13it/s]11/28/2021 01:35:37 - INFO - __main__ -   Batch number = 93
60.31user 21.39system 1:24.42elapsed 96%CPU (0avgtext+0avgdata 3990272maxresident)k
0inputs+712outputs (0major+1613384minor)pagefaults 0swaps
Evaluating:  45%|████▌     | 93/205 [00:47<00:53,  2.08it/s]11/28/2021 01:35:37 - INFO - __main__ -   Batch number = 94
Evaluating:  46%|████▌     | 94/205 [00:48<00:54,  2.05it/s]11/28/2021 01:35:38 - INFO - __main__ -   Batch number = 95
Evaluating:  46%|████▋     | 95/205 [00:48<00:54,  2.03it/s]11/28/2021 01:35:38 - INFO - __main__ -   Batch number = 96
PyTorch version 1.10.0+cu102 available.
Evaluating:  47%|████▋     | 96/205 [00:49<00:53,  2.03it/s]11/28/2021 01:35:39 - INFO - __main__ -   Batch number = 97
11/28/2021 01:35:39 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='zh', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:35:39 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:35:39 - INFO - __main__ -   Seed = 3
11/28/2021 01:35:39 - INFO - root -   save model
11/28/2021 01:35:39 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='zh', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:35:39 - INFO - __main__ -   Loading pretrained model and tokenizer
Evaluating:  47%|████▋     | 97/205 [00:49<00:53,  2.01it/s]11/28/2021 01:35:39 - INFO - __main__ -   Batch number = 98
Evaluating:  48%|████▊     | 98/205 [00:49<00:52,  2.06it/s]11/28/2021 01:35:40 - INFO - __main__ -   Batch number = 99
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  48%|████▊     | 99/205 [00:50<00:52,  2.04it/s]11/28/2021 01:35:40 - INFO - __main__ -   Batch number = 100
Evaluating:  49%|████▉     | 100/205 [00:50<00:51,  2.02it/s]11/28/2021 01:35:41 - INFO - __main__ -   Batch number = 101
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  49%|████▉     | 101/205 [00:51<00:51,  2.01it/s]11/28/2021 01:35:41 - INFO - __main__ -   Batch number = 102
loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
Evaluating:  50%|████▉     | 102/205 [00:52<00:51,  2.00it/s]11/28/2021 01:35:42 - INFO - __main__ -   Batch number = 103
11/28/2021 01:35:42 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
Evaluating:  50%|█████     | 103/205 [00:52<00:51,  2.00it/s]11/28/2021 01:35:42 - INFO - __main__ -   Batch number = 104
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  51%|█████     | 104/205 [00:53<00:50,  1.99it/s]11/28/2021 01:35:43 - INFO - __main__ -   Batch number = 105
Evaluating:  51%|█████     | 105/205 [00:53<00:50,  1.99it/s]11/28/2021 01:35:43 - INFO - __main__ -   Batch number = 106
Evaluating:  52%|█████▏    | 106/205 [00:54<00:49,  1.99it/s]11/28/2021 01:35:44 - INFO - __main__ -   Batch number = 107
Evaluating:  52%|█████▏    | 107/205 [00:54<00:49,  1.99it/s]11/28/2021 01:35:44 - INFO - __main__ -   Batch number = 108
Evaluating:  53%|█████▎    | 108/205 [00:55<00:48,  1.99it/s]11/28/2021 01:35:45 - INFO - __main__ -   Batch number = 109
Evaluating:  53%|█████▎    | 109/205 [00:55<00:53,  1.79it/s]11/28/2021 01:35:46 - INFO - __main__ -   Batch number = 110
Evaluating:  54%|█████▎    | 110/205 [00:56<00:51,  1.83it/s]11/28/2021 01:35:46 - INFO - __main__ -   Batch number = 111
Evaluating:  54%|█████▍    | 111/205 [00:56<00:50,  1.88it/s]11/28/2021 01:35:47 - INFO - __main__ -   Batch number = 112
Evaluating:  55%|█████▍    | 112/205 [00:57<00:53,  1.75it/s]11/28/2021 01:35:47 - INFO - __main__ -   Batch number = 113
Evaluating:  55%|█████▌    | 113/205 [00:57<00:50,  1.81it/s]11/28/2021 01:35:48 - INFO - __main__ -   Batch number = 114
Evaluating:  56%|█████▌    | 114/205 [00:58<00:48,  1.87it/s]11/28/2021 01:35:48 - INFO - __main__ -   Batch number = 115
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:35:48 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:35:48 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:35:48 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
11/28/2021 01:35:48 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:35:48 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:35:48 - INFO - root -   loading lang adpater bh/wiki@ukp
11/28/2021 01:35:48 - INFO - __main__ -   Adapter Languages : ['bh'], Length : 1
11/28/2021 01:35:48 - INFO - __main__ -   Adapter Names ['bh/wiki@ukp'], Length : 1
11/28/2021 01:35:48 - INFO - __main__ -   Language = bh
11/28/2021 01:35:48 - INFO - __main__ -   Adapter Name = bh/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/bh/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_bh_wiki_pfeiffer.zip.
Evaluating:  56%|█████▌    | 115/205 [00:58<00:49,  1.83it/s]11/28/2021 01:35:49 - INFO - __main__ -   Batch number = 116
Evaluating:  57%|█████▋    | 116/205 [00:59<00:47,  1.86it/s]11/28/2021 01:35:49 - INFO - __main__ -   Batch number = 117
Evaluating:  57%|█████▋    | 117/205 [00:59<00:46,  1.89it/s]11/28/2021 01:35:50 - INFO - __main__ -   Batch number = 118
Evaluating:  58%|█████▊    | 118/205 [01:00<00:46,  1.85it/s]11/28/2021 01:35:50 - INFO - __main__ -   Batch number = 119
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/adapter_config.json
Adding adapter 'bh' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/head_config.json
Evaluating:  58%|█████▊    | 119/205 [01:01<00:45,  1.89it/s]11/28/2021 01:35:51 - INFO - __main__ -   Batch number = 120
Evaluating:  59%|█████▊    | 120/205 [01:01<00:44,  1.92it/s]11/28/2021 01:35:51 - INFO - __main__ -   Batch number = 121
Evaluating:  59%|█████▉    | 121/205 [01:02<00:47,  1.78it/s]11/28/2021 01:35:52 - INFO - __main__ -   Batch number = 122
Evaluating:  60%|█████▉    | 122/205 [01:02<00:45,  1.82it/s]11/28/2021 01:35:53 - INFO - __main__ -   Batch number = 123
Evaluating:  60%|██████    | 123/205 [01:03<00:44,  1.85it/s]11/28/2021 01:35:53 - INFO - __main__ -   Batch number = 124
Evaluating:  60%|██████    | 124/205 [01:03<00:46,  1.74it/s]11/28/2021 01:35:54 - INFO - __main__ -   Batch number = 125
Evaluating:  61%|██████    | 125/205 [01:04<00:44,  1.79it/s]11/28/2021 01:35:54 - INFO - __main__ -   Batch number = 126
Evaluating:  61%|██████▏   | 126/205 [01:04<00:42,  1.86it/s]11/28/2021 01:35:55 - INFO - __main__ -   Batch number = 127
Evaluating:  62%|██████▏   | 127/205 [01:05<00:42,  1.85it/s]11/28/2021 01:35:55 - INFO - __main__ -   Batch number = 128
Evaluating:  62%|██████▏   | 128/205 [01:05<00:40,  1.91it/s]11/28/2021 01:35:56 - INFO - __main__ -   Batch number = 129
Evaluating:  63%|██████▎   | 129/205 [01:06<00:39,  1.93it/s]11/28/2021 01:35:56 - INFO - __main__ -   Batch number = 130
Evaluating:  63%|██████▎   | 130/205 [01:07<00:42,  1.78it/s]11/28/2021 01:35:57 - INFO - __main__ -   Batch number = 131
Evaluating:  64%|██████▍   | 131/205 [01:07<00:40,  1.83it/s]11/28/2021 01:35:57 - INFO - __main__ -   Batch number = 132
Evaluating:  64%|██████▍   | 132/205 [01:08<00:38,  1.88it/s]11/28/2021 01:35:58 - INFO - __main__ -   Batch number = 133
11/28/2021 01:35:58 - INFO - __main__ -   Language adapter for zh not found, using bh instead
11/28/2021 01:35:58 - INFO - __main__ -   Set active language adapter to bh
11/28/2021 01:35:58 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:35:58 - INFO - __main__ -   Adapter Languages = ['bh']
11/28/2021 01:35:58 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_zh_bert-base-multilingual-cased_128
11/28/2021 01:35:59 - INFO - __main__ -   ***** Running evaluation  in zh *****
11/28/2021 01:35:59 - INFO - __main__ -     Num examples = 3458
11/28/2021 01:35:59 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/109 [00:00<?, ?it/s]11/28/2021 01:35:59 - INFO - __main__ -   Batch number = 1
Evaluating:  65%|██████▍   | 133/205 [01:09<00:56,  1.27it/s]11/28/2021 01:35:59 - INFO - __main__ -   Batch number = 134
Evaluating:   1%|          | 1/109 [00:00<01:02,  1.73it/s]11/28/2021 01:36:00 - INFO - __main__ -   Batch number = 2
Evaluating:  65%|██████▌   | 134/205 [01:10<00:52,  1.36it/s]11/28/2021 01:36:00 - INFO - __main__ -   Batch number = 135
Evaluating:   2%|▏         | 2/109 [00:01<00:53,  1.99it/s]11/28/2021 01:36:00 - INFO - __main__ -   Batch number = 3
Evaluating:   3%|▎         | 3/109 [00:01<00:41,  2.53it/s]11/28/2021 01:36:00 - INFO - __main__ -   Batch number = 4
Evaluating:   4%|▎         | 4/109 [00:01<00:37,  2.78it/s]11/28/2021 01:36:01 - INFO - __main__ -   Batch number = 5
Evaluating:  66%|██████▌   | 135/205 [01:10<00:50,  1.38it/s]11/28/2021 01:36:01 - INFO - __main__ -   Batch number = 136
Evaluating:   5%|▍         | 5/109 [00:01<00:32,  3.22it/s]11/28/2021 01:36:01 - INFO - __main__ -   Batch number = 6
Evaluating:   6%|▌         | 6/109 [00:01<00:26,  3.88it/s]11/28/2021 01:36:01 - INFO - __main__ -   Batch number = 7
Evaluating:   6%|▋         | 7/109 [00:02<00:22,  4.46it/s]11/28/2021 01:36:01 - INFO - __main__ -   Batch number = 8
Evaluating:  66%|██████▋   | 136/205 [01:11<00:49,  1.40it/s]11/28/2021 01:36:01 - INFO - __main__ -   Batch number = 137
Evaluating:   7%|▋         | 8/109 [00:02<00:23,  4.34it/s]11/28/2021 01:36:01 - INFO - __main__ -   Batch number = 9
Evaluating:   8%|▊         | 9/109 [00:02<00:31,  3.21it/s]11/28/2021 01:36:02 - INFO - __main__ -   Batch number = 10
Evaluating:  67%|██████▋   | 137/205 [01:12<00:47,  1.43it/s]11/28/2021 01:36:02 - INFO - __main__ -   Batch number = 138
Evaluating:   9%|▉         | 10/109 [00:03<00:36,  2.75it/s]11/28/2021 01:36:02 - INFO - __main__ -   Batch number = 11
Evaluating:  67%|██████▋   | 138/205 [01:12<00:46,  1.44it/s]11/28/2021 01:36:03 - INFO - __main__ -   Batch number = 139
Evaluating:  10%|█         | 11/109 [00:03<00:39,  2.48it/s]11/28/2021 01:36:03 - INFO - __main__ -   Batch number = 12
Evaluating:  68%|██████▊   | 139/205 [01:13<00:44,  1.50it/s]11/28/2021 01:36:03 - INFO - __main__ -   Batch number = 140
Evaluating:  11%|█         | 12/109 [00:04<00:40,  2.38it/s]11/28/2021 01:36:03 - INFO - __main__ -   Batch number = 13
Evaluating:  12%|█▏        | 13/109 [00:04<00:37,  2.53it/s]11/28/2021 01:36:04 - INFO - __main__ -   Batch number = 14
Evaluating:  13%|█▎        | 14/109 [00:05<00:37,  2.57it/s]11/28/2021 01:36:04 - INFO - __main__ -   Batch number = 15
Evaluating:  68%|██████▊   | 140/205 [01:14<00:46,  1.41it/s]11/28/2021 01:36:04 - INFO - __main__ -   Batch number = 141
Evaluating:  14%|█▍        | 15/109 [00:05<00:39,  2.38it/s]Evaluating:  69%|██████▉   | 141/205 [01:14<00:40,  1.57it/s]11/28/2021 01:36:05 - INFO - __main__ -   Batch number = 16
11/28/2021 01:36:05 - INFO - __main__ -   Batch number = 142
Evaluating:  15%|█▍        | 16/109 [00:06<00:44,  2.09it/s]11/28/2021 01:36:05 - INFO - __main__ -   Batch number = 17
Evaluating:  69%|██████▉   | 142/205 [01:15<00:42,  1.49it/s]11/28/2021 01:36:05 - INFO - __main__ -   Batch number = 143
Evaluating:  16%|█▌        | 17/109 [00:06<00:44,  2.06it/s]11/28/2021 01:36:06 - INFO - __main__ -   Batch number = 18
Evaluating:  70%|██████▉   | 143/205 [01:16<00:38,  1.61it/s]11/28/2021 01:36:06 - INFO - __main__ -   Batch number = 144
Evaluating:  17%|█▋        | 18/109 [00:07<00:44,  2.05it/s]11/28/2021 01:36:06 - INFO - __main__ -   Batch number = 19
Evaluating:  70%|███████   | 144/205 [01:16<00:35,  1.70it/s]11/28/2021 01:36:06 - INFO - __main__ -   Batch number = 145
Evaluating:  17%|█▋        | 19/109 [00:07<00:41,  2.18it/s]11/28/2021 01:36:07 - INFO - __main__ -   Batch number = 20
Evaluating:  71%|███████   | 145/205 [01:17<00:33,  1.78it/s]11/28/2021 01:36:07 - INFO - __main__ -   Batch number = 146
Evaluating:  18%|█▊        | 20/109 [00:08<00:41,  2.13it/s]11/28/2021 01:36:07 - INFO - __main__ -   Batch number = 21
Evaluating:  71%|███████   | 146/205 [01:17<00:32,  1.84it/s]11/28/2021 01:36:07 - INFO - __main__ -   Batch number = 147
Evaluating:  19%|█▉        | 21/109 [00:08<00:40,  2.16it/s]11/28/2021 01:36:07 - INFO - __main__ -   Batch number = 22
Evaluating:  20%|██        | 22/109 [00:08<00:37,  2.30it/s]11/28/2021 01:36:08 - INFO - __main__ -   Batch number = 23
Evaluating:  72%|███████▏  | 147/205 [01:18<00:34,  1.69it/s]11/28/2021 01:36:08 - INFO - __main__ -   Batch number = 148
Evaluating:  21%|██        | 23/109 [00:09<00:38,  2.21it/s]11/28/2021 01:36:08 - INFO - __main__ -   Batch number = 24
Evaluating:  72%|███████▏  | 148/205 [01:18<00:32,  1.78it/s]11/28/2021 01:36:09 - INFO - __main__ -   Batch number = 149
Evaluating:  22%|██▏       | 24/109 [00:09<00:38,  2.19it/s]11/28/2021 01:36:09 - INFO - __main__ -   Batch number = 25
Evaluating:  23%|██▎       | 25/109 [00:10<00:35,  2.39it/s]11/28/2021 01:36:09 - INFO - __main__ -   Batch number = 26
Evaluating:  24%|██▍       | 26/109 [00:10<00:32,  2.54it/s]11/28/2021 01:36:09 - INFO - __main__ -   Batch number = 27
Evaluating:  25%|██▍       | 27/109 [00:10<00:30,  2.66it/s]11/28/2021 01:36:10 - INFO - __main__ -   Batch number = 28
Evaluating:  73%|███████▎  | 149/205 [01:20<00:49,  1.14it/s]11/28/2021 01:36:10 - INFO - __main__ -   Batch number = 150
Evaluating:  26%|██▌       | 28/109 [00:11<00:33,  2.42it/s]11/28/2021 01:36:10 - INFO - __main__ -   Batch number = 29
Evaluating:  73%|███████▎  | 150/205 [01:20<00:42,  1.30it/s]11/28/2021 01:36:11 - INFO - __main__ -   Batch number = 151
Evaluating:  27%|██▋       | 29/109 [00:11<00:35,  2.27it/s]11/28/2021 01:36:11 - INFO - __main__ -   Batch number = 30
Evaluating:  74%|███████▎  | 151/205 [01:21<00:37,  1.44it/s]11/28/2021 01:36:11 - INFO - __main__ -   Batch number = 152
Evaluating:  28%|██▊       | 30/109 [00:12<00:35,  2.24it/s]11/28/2021 01:36:11 - INFO - __main__ -   Batch number = 31
Evaluating:  28%|██▊       | 31/109 [00:12<00:35,  2.17it/s]11/28/2021 01:36:12 - INFO - __main__ -   Batch number = 32
Evaluating:  74%|███████▍  | 152/205 [01:22<00:36,  1.45it/s]11/28/2021 01:36:12 - INFO - __main__ -   Batch number = 153
Evaluating:  29%|██▉       | 32/109 [00:13<00:33,  2.28it/s]11/28/2021 01:36:12 - INFO - __main__ -   Batch number = 33
Evaluating:  75%|███████▍  | 153/205 [01:22<00:33,  1.57it/s]11/28/2021 01:36:12 - INFO - __main__ -   Batch number = 154
Evaluating:  30%|███       | 33/109 [00:13<00:34,  2.22it/s]11/28/2021 01:36:13 - INFO - __main__ -   Batch number = 34
Evaluating:  75%|███████▌  | 154/205 [01:23<00:34,  1.49it/s]11/28/2021 01:36:13 - INFO - __main__ -   Batch number = 155
Evaluating:  31%|███       | 34/109 [00:14<00:34,  2.15it/s]11/28/2021 01:36:13 - INFO - __main__ -   Batch number = 35
Evaluating:  32%|███▏      | 35/109 [00:14<00:33,  2.20it/s]11/28/2021 01:36:14 - INFO - __main__ -   Batch number = 36
Evaluating:  76%|███████▌  | 155/205 [01:23<00:30,  1.61it/s]11/28/2021 01:36:14 - INFO - __main__ -   Batch number = 156
Evaluating:  33%|███▎      | 36/109 [00:15<00:34,  2.13it/s]11/28/2021 01:36:14 - INFO - __main__ -   Batch number = 37
Evaluating:  76%|███████▌  | 156/205 [01:24<00:32,  1.53it/s]11/28/2021 01:36:14 - INFO - __main__ -   Batch number = 157
Evaluating:  34%|███▍      | 37/109 [00:15<00:32,  2.21it/s]11/28/2021 01:36:14 - INFO - __main__ -   Batch number = 38
Evaluating:  77%|███████▋  | 157/205 [01:25<00:29,  1.63it/s]11/28/2021 01:36:15 - INFO - __main__ -   Batch number = 158
Evaluating:  35%|███▍      | 38/109 [00:15<00:31,  2.22it/s]11/28/2021 01:36:15 - INFO - __main__ -   Batch number = 39
Evaluating:  36%|███▌      | 39/109 [00:16<00:32,  2.16it/s]11/28/2021 01:36:15 - INFO - __main__ -   Batch number = 40
Evaluating:  37%|███▋      | 40/109 [00:16<00:30,  2.24it/s]11/28/2021 01:36:16 - INFO - __main__ -   Batch number = 41
Evaluating:  77%|███████▋  | 158/205 [01:26<00:36,  1.29it/s]11/28/2021 01:36:16 - INFO - __main__ -   Batch number = 159
Evaluating:  38%|███▊      | 41/109 [00:17<00:28,  2.40it/s]11/28/2021 01:36:16 - INFO - __main__ -   Batch number = 42
Evaluating:  78%|███████▊  | 159/205 [01:26<00:32,  1.43it/s]11/28/2021 01:36:17 - INFO - __main__ -   Batch number = 160
Evaluating:  39%|███▊      | 42/109 [00:17<00:29,  2.27it/s]11/28/2021 01:36:17 - INFO - __main__ -   Batch number = 43
Evaluating:  39%|███▉      | 43/109 [00:18<00:30,  2.18it/s]11/28/2021 01:36:17 - INFO - __main__ -   Batch number = 44
Evaluating:  40%|████      | 44/109 [00:18<00:27,  2.35it/s]11/28/2021 01:36:17 - INFO - __main__ -   Batch number = 45
Evaluating:  78%|███████▊  | 160/205 [01:27<00:35,  1.28it/s]11/28/2021 01:36:18 - INFO - __main__ -   Batch number = 161
Evaluating:  41%|████▏     | 45/109 [00:18<00:28,  2.25it/s]11/28/2021 01:36:18 - INFO - __main__ -   Batch number = 46
Evaluating:  79%|███████▊  | 161/205 [01:28<00:30,  1.44it/s]11/28/2021 01:36:18 - INFO - __main__ -   Batch number = 162
Evaluating:  42%|████▏     | 46/109 [00:19<00:29,  2.17it/s]11/28/2021 01:36:18 - INFO - __main__ -   Batch number = 47
Evaluating:  79%|███████▉  | 162/205 [01:28<00:30,  1.39it/s]11/28/2021 01:36:19 - INFO - __main__ -   Batch number = 163
Evaluating:  43%|████▎     | 47/109 [00:19<00:26,  2.32it/s]11/28/2021 01:36:19 - INFO - __main__ -   Batch number = 48
Evaluating:  44%|████▍     | 48/109 [00:20<00:25,  2.39it/s]11/28/2021 01:36:19 - INFO - __main__ -   Batch number = 49
Evaluating:  80%|███████▉  | 163/205 [01:29<00:27,  1.53it/s]11/28/2021 01:36:19 - INFO - __main__ -   Batch number = 164
Evaluating:  45%|████▍     | 49/109 [00:20<00:26,  2.26it/s]11/28/2021 01:36:20 - INFO - __main__ -   Batch number = 50
Evaluating:  80%|████████  | 164/205 [01:30<00:25,  1.63it/s]11/28/2021 01:36:20 - INFO - __main__ -   Batch number = 165
Evaluating:  46%|████▌     | 50/109 [00:21<00:26,  2.23it/s]11/28/2021 01:36:20 - INFO - __main__ -   Batch number = 51
Evaluating:  80%|████████  | 165/205 [01:30<00:23,  1.72it/s]11/28/2021 01:36:20 - INFO - __main__ -   Batch number = 166
Evaluating:  47%|████▋     | 51/109 [00:21<00:25,  2.28it/s]11/28/2021 01:36:21 - INFO - __main__ -   Batch number = 52
Evaluating:  48%|████▊     | 52/109 [00:22<00:25,  2.21it/s]11/28/2021 01:36:21 - INFO - __main__ -   Batch number = 53
Evaluating:  49%|████▊     | 53/109 [00:22<00:25,  2.22it/s]11/28/2021 01:36:22 - INFO - __main__ -   Batch number = 54
Evaluating:  81%|████████  | 166/205 [01:31<00:32,  1.21it/s]11/28/2021 01:36:22 - INFO - __main__ -   Batch number = 167
Evaluating:  50%|████▉     | 54/109 [00:22<00:22,  2.41it/s]11/28/2021 01:36:22 - INFO - __main__ -   Batch number = 55
Evaluating:  81%|████████▏ | 167/205 [01:32<00:27,  1.37it/s]11/28/2021 01:36:22 - INFO - __main__ -   Batch number = 168
Evaluating:  50%|█████     | 55/109 [00:23<00:23,  2.30it/s]11/28/2021 01:36:22 - INFO - __main__ -   Batch number = 56
Evaluating:  51%|█████▏    | 56/109 [00:23<00:24,  2.21it/s]11/28/2021 01:36:23 - INFO - __main__ -   Batch number = 57
Evaluating:  82%|████████▏ | 168/205 [01:33<00:26,  1.38it/s]11/28/2021 01:36:23 - INFO - __main__ -   Batch number = 169
Evaluating:  52%|█████▏    | 57/109 [00:24<00:22,  2.33it/s]11/28/2021 01:36:23 - INFO - __main__ -   Batch number = 58
Evaluating:  82%|████████▏ | 169/205 [01:33<00:23,  1.51it/s]11/28/2021 01:36:23 - INFO - __main__ -   Batch number = 170
Evaluating:  53%|█████▎    | 58/109 [00:24<00:20,  2.50it/s]11/28/2021 01:36:24 - INFO - __main__ -   Batch number = 59
Evaluating:  83%|████████▎ | 170/205 [01:34<00:21,  1.60it/s]11/28/2021 01:36:24 - INFO - __main__ -   Batch number = 171
Evaluating:  54%|█████▍    | 59/109 [00:25<00:22,  2.22it/s]11/28/2021 01:36:24 - INFO - __main__ -   Batch number = 60
Evaluating:  83%|████████▎ | 171/205 [01:34<00:20,  1.68it/s]11/28/2021 01:36:24 - INFO - __main__ -   Batch number = 172
Evaluating:  55%|█████▌    | 60/109 [00:25<00:24,  1.98it/s]11/28/2021 01:36:25 - INFO - __main__ -   Batch number = 61
Evaluating:  56%|█████▌    | 61/109 [00:26<00:24,  1.98it/s]11/28/2021 01:36:25 - INFO - __main__ -   Batch number = 62
Evaluating:  84%|████████▍ | 172/205 [01:35<00:22,  1.49it/s]11/28/2021 01:36:25 - INFO - __main__ -   Batch number = 173
Evaluating:  84%|████████▍ | 173/205 [01:36<00:19,  1.61it/s]11/28/2021 01:36:26 - INFO - __main__ -   Batch number = 174
Evaluating:  57%|█████▋    | 62/109 [00:26<00:25,  1.83it/s]11/28/2021 01:36:26 - INFO - __main__ -   Batch number = 63
Evaluating:  58%|█████▊    | 63/109 [00:27<00:26,  1.72it/s]11/28/2021 01:36:27 - INFO - __main__ -   Batch number = 64
Evaluating:  85%|████████▍ | 174/205 [01:36<00:21,  1.42it/s]11/28/2021 01:36:27 - INFO - __main__ -   Batch number = 175
Evaluating:  59%|█████▊    | 64/109 [00:28<00:25,  1.77it/s]11/28/2021 01:36:27 - INFO - __main__ -   Batch number = 65
Evaluating:  85%|████████▌ | 175/205 [01:37<00:19,  1.55it/s]11/28/2021 01:36:27 - INFO - __main__ -   Batch number = 176
Evaluating:  60%|█████▉    | 65/109 [00:28<00:26,  1.69it/s]11/28/2021 01:36:28 - INFO - __main__ -   Batch number = 66
Evaluating:  86%|████████▌ | 176/205 [01:38<00:19,  1.50it/s]11/28/2021 01:36:28 - INFO - __main__ -   Batch number = 177
Evaluating:  61%|██████    | 66/109 [00:29<00:26,  1.64it/s]11/28/2021 01:36:28 - INFO - __main__ -   Batch number = 67
Evaluating:  86%|████████▋ | 177/205 [01:38<00:18,  1.50it/s]11/28/2021 01:36:29 - INFO - __main__ -   Batch number = 178
Evaluating:  61%|██████▏   | 67/109 [00:29<00:25,  1.68it/s]11/28/2021 01:36:29 - INFO - __main__ -   Batch number = 68
Evaluating:  87%|████████▋ | 178/205 [01:39<00:19,  1.37it/s]Evaluating:  62%|██████▏   | 68/109 [00:30<00:25,  1.63it/s]11/28/2021 01:36:30 - INFO - __main__ -   Batch number = 179
11/28/2021 01:36:30 - INFO - __main__ -   Batch number = 69
Evaluating:  63%|██████▎   | 69/109 [00:31<00:24,  1.61it/s]11/28/2021 01:36:30 - INFO - __main__ -   Batch number = 70
Evaluating:  87%|████████▋ | 179/205 [01:40<00:19,  1.35it/s]11/28/2021 01:36:30 - INFO - __main__ -   Batch number = 180
Evaluating:  64%|██████▍   | 70/109 [00:31<00:24,  1.62it/s]11/28/2021 01:36:31 - INFO - __main__ -   Batch number = 71
Evaluating:  88%|████████▊ | 180/205 [01:41<00:19,  1.28it/s]11/28/2021 01:36:31 - INFO - __main__ -   Batch number = 181
Evaluating:  65%|██████▌   | 71/109 [00:32<00:24,  1.58it/s]11/28/2021 01:36:32 - INFO - __main__ -   Batch number = 72
Evaluating:  88%|████████▊ | 181/205 [01:42<00:17,  1.34it/s]11/28/2021 01:36:32 - INFO - __main__ -   Batch number = 182
Evaluating:  66%|██████▌   | 72/109 [00:33<00:22,  1.62it/s]11/28/2021 01:36:32 - INFO - __main__ -   Batch number = 73
Evaluating:  89%|████████▉ | 182/205 [01:42<00:18,  1.28it/s]11/28/2021 01:36:33 - INFO - __main__ -   Batch number = 183
Evaluating:  67%|██████▋   | 73/109 [00:33<00:22,  1.59it/s]11/28/2021 01:36:33 - INFO - __main__ -   Batch number = 74
Evaluating:  89%|████████▉ | 183/205 [01:43<00:16,  1.33it/s]11/28/2021 01:36:33 - INFO - __main__ -   Batch number = 184
Evaluating:  68%|██████▊   | 74/109 [00:34<00:22,  1.56it/s]11/28/2021 01:36:33 - INFO - __main__ -   Batch number = 75
Evaluating:  69%|██████▉   | 75/109 [00:35<00:21,  1.58it/s]11/28/2021 01:36:34 - INFO - __main__ -   Batch number = 76
Evaluating:  90%|████████▉ | 184/205 [01:44<00:15,  1.35it/s]11/28/2021 01:36:34 - INFO - __main__ -   Batch number = 185
Evaluating:  70%|██████▉   | 76/109 [00:35<00:21,  1.56it/s]11/28/2021 01:36:35 - INFO - __main__ -   Batch number = 77
Evaluating:  90%|█████████ | 185/205 [01:44<00:14,  1.41it/s]11/28/2021 01:36:35 - INFO - __main__ -   Batch number = 186
Evaluating:  71%|███████   | 77/109 [00:36<00:19,  1.62it/s]11/28/2021 01:36:35 - INFO - __main__ -   Batch number = 78
Evaluating:  72%|███████▏  | 78/109 [00:36<00:18,  1.69it/s]11/28/2021 01:36:36 - INFO - __main__ -   Batch number = 79
Evaluating:  91%|█████████ | 186/205 [01:46<00:16,  1.15it/s]11/28/2021 01:36:36 - INFO - __main__ -   Batch number = 187
Evaluating:  72%|███████▏  | 79/109 [00:37<00:17,  1.67it/s]11/28/2021 01:36:36 - INFO - __main__ -   Batch number = 80
Evaluating:  91%|█████████ | 187/205 [01:46<00:14,  1.22it/s]11/28/2021 01:36:37 - INFO - __main__ -   Batch number = 188
Evaluating:  73%|███████▎  | 80/109 [00:38<00:17,  1.63it/s]11/28/2021 01:36:37 - INFO - __main__ -   Batch number = 81
Evaluating:  92%|█████████▏| 188/205 [01:47<00:14,  1.19it/s]11/28/2021 01:36:38 - INFO - __main__ -   Batch number = 189
Evaluating:  74%|███████▍  | 81/109 [00:38<00:16,  1.65it/s]11/28/2021 01:36:38 - INFO - __main__ -   Batch number = 82
Evaluating:  92%|█████████▏| 189/205 [01:48<00:12,  1.27it/s]11/28/2021 01:36:38 - INFO - __main__ -   Batch number = 190
Evaluating:  75%|███████▌  | 82/109 [00:39<00:16,  1.60it/s]11/28/2021 01:36:38 - INFO - __main__ -   Batch number = 83
Evaluating:  76%|███████▌  | 83/109 [00:39<00:16,  1.57it/s]11/28/2021 01:36:39 - INFO - __main__ -   Batch number = 84
Evaluating:  93%|█████████▎| 190/205 [01:49<00:13,  1.12it/s]11/28/2021 01:36:39 - INFO - __main__ -   Batch number = 191
Evaluating:  77%|███████▋  | 84/109 [00:40<00:15,  1.65it/s]11/28/2021 01:36:40 - INFO - __main__ -   Batch number = 85
Evaluating:  93%|█████████▎| 191/205 [01:50<00:11,  1.24it/s]11/28/2021 01:36:40 - INFO - __main__ -   Batch number = 192
Evaluating:  78%|███████▊  | 85/109 [00:41<00:14,  1.61it/s]11/28/2021 01:36:40 - INFO - __main__ -   Batch number = 86
Evaluating:  79%|███████▉  | 86/109 [00:41<00:14,  1.59it/s]11/28/2021 01:36:41 - INFO - __main__ -   Batch number = 87
Evaluating:  80%|███████▉  | 87/109 [00:42<00:13,  1.69it/s]11/28/2021 01:36:41 - INFO - __main__ -   Batch number = 88
Evaluating:  94%|█████████▎| 192/205 [01:51<00:12,  1.01it/s]11/28/2021 01:36:41 - INFO - __main__ -   Batch number = 193
Evaluating:  81%|████████  | 88/109 [00:42<00:12,  1.64it/s]11/28/2021 01:36:42 - INFO - __main__ -   Batch number = 89
Evaluating:  94%|█████████▍| 193/205 [01:52<00:10,  1.13it/s]11/28/2021 01:36:42 - INFO - __main__ -   Batch number = 194
Evaluating:  95%|█████████▍| 194/205 [01:52<00:08,  1.28it/s]11/28/2021 01:36:43 - INFO - __main__ -   Batch number = 195
Evaluating:  82%|████████▏ | 89/109 [00:43<00:12,  1.60it/s]11/28/2021 01:36:43 - INFO - __main__ -   Batch number = 90
Evaluating:  95%|█████████▌| 195/205 [01:53<00:07,  1.43it/s]11/28/2021 01:36:43 - INFO - __main__ -   Batch number = 196
Evaluating:  83%|████████▎ | 90/109 [00:44<00:11,  1.67it/s]11/28/2021 01:36:43 - INFO - __main__ -   Batch number = 91
Evaluating:  83%|████████▎ | 91/109 [00:44<00:09,  1.87it/s]11/28/2021 01:36:44 - INFO - __main__ -   Batch number = 92
Evaluating:  96%|█████████▌| 196/205 [01:54<00:06,  1.41it/s]11/28/2021 01:36:44 - INFO - __main__ -   Batch number = 197
Evaluating:  84%|████████▍ | 92/109 [00:44<00:08,  2.05it/s]11/28/2021 01:36:44 - INFO - __main__ -   Batch number = 93
Evaluating:  96%|█████████▌| 197/205 [01:54<00:05,  1.55it/s]11/28/2021 01:36:44 - INFO - __main__ -   Batch number = 198
Evaluating:  85%|████████▌ | 93/109 [00:45<00:07,  2.04it/s]11/28/2021 01:36:44 - INFO - __main__ -   Batch number = 94
Evaluating:  86%|████████▌ | 94/109 [00:45<00:07,  2.08it/s]11/28/2021 01:36:45 - INFO - __main__ -   Batch number = 95
Evaluating:  97%|█████████▋| 198/205 [01:55<00:04,  1.42it/s]11/28/2021 01:36:45 - INFO - __main__ -   Batch number = 199
Evaluating:  87%|████████▋ | 95/109 [00:46<00:06,  2.29it/s]11/28/2021 01:36:45 - INFO - __main__ -   Batch number = 96
Evaluating:  88%|████████▊ | 96/109 [00:46<00:05,  2.46it/s]11/28/2021 01:36:46 - INFO - __main__ -   Batch number = 97
Evaluating:  97%|█████████▋| 199/205 [01:55<00:03,  1.59it/s]11/28/2021 01:36:46 - INFO - __main__ -   Batch number = 200
Evaluating:  98%|█████████▊| 200/205 [01:56<00:02,  1.81it/s]11/28/2021 01:36:46 - INFO - __main__ -   Batch number = 201
Evaluating:  89%|████████▉ | 97/109 [00:47<00:05,  2.31it/s]11/28/2021 01:36:46 - INFO - __main__ -   Batch number = 98
Evaluating:  98%|█████████▊| 201/205 [01:56<00:01,  2.03it/s]11/28/2021 01:36:46 - INFO - __main__ -   Batch number = 202
Evaluating:  90%|████████▉ | 98/109 [00:47<00:04,  2.20it/s]11/28/2021 01:36:47 - INFO - __main__ -   Batch number = 99
Evaluating:  91%|█████████ | 99/109 [00:47<00:04,  2.39it/s]11/28/2021 01:36:47 - INFO - __main__ -   Batch number = 100
Evaluating:  99%|█████████▊| 202/205 [01:57<00:01,  1.67it/s]11/28/2021 01:36:47 - INFO - __main__ -   Batch number = 203
Evaluating:  92%|█████████▏| 100/109 [00:48<00:03,  2.25it/s]11/28/2021 01:36:47 - INFO - __main__ -   Batch number = 101
Evaluating:  99%|█████████▉| 203/205 [01:57<00:01,  1.90it/s]11/28/2021 01:36:48 - INFO - __main__ -   Batch number = 204
Evaluating: 100%|█████████▉| 204/205 [01:58<00:00,  2.09it/s]11/28/2021 01:36:48 - INFO - __main__ -   Batch number = 205
Evaluating:  93%|█████████▎| 101/109 [00:48<00:03,  2.17it/s]11/28/2021 01:36:48 - INFO - __main__ -   Batch number = 102
Evaluating: 100%|██████████| 205/205 [01:58<00:00,  2.41it/s]Evaluating: 100%|██████████| 205/205 [01:58<00:00,  1.73it/s]Evaluating:  94%|█████████▎| 102/109 [00:49<00:02,  2.36it/s]11/28/2021 01:36:48 - INFO - __main__ -   Batch number = 103
Evaluating:  94%|█████████▍| 103/109 [00:49<00:02,  2.25it/s]11/28/2021 01:36:49 - INFO - __main__ -   Batch number = 104
Evaluating:  95%|█████████▌| 104/109 [00:50<00:02,  2.18it/s]11/28/2021 01:36:49 - INFO - __main__ -   Batch number = 105
Evaluating:  96%|█████████▋| 105/109 [00:50<00:01,  2.21it/s]11/28/2021 01:36:50 - INFO - __main__ -   Batch number = 106
Evaluating:  97%|█████████▋| 106/109 [00:51<00:01,  2.17it/s]11/28/2021 01:36:50 - INFO - __main__ -   Batch number = 107

/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:36:50 - INFO - __main__ -   ***** Evaluation result  in fi *****
11/28/2021 01:36:50 - INFO - __main__ -     f1 = 0.7113520871845246
11/28/2021 01:36:50 - INFO - __main__ -     loss = 1.0031439997800966
11/28/2021 01:36:50 - INFO - __main__ -     precision = 0.7171821145058407
11/28/2021 01:36:50 - INFO - __main__ -     recall = 0.7056160810188737
Evaluating:  98%|█████████▊| 107/109 [00:51<00:00,  2.26it/s]11/28/2021 01:36:51 - INFO - __main__ -   Batch number = 108
Evaluating:  99%|█████████▉| 108/109 [00:51<00:00,  2.41it/s]11/28/2021 01:36:51 - INFO - __main__ -   Batch number = 109
Evaluating: 100%|██████████| 109/109 [00:51<00:00,  2.10it/s]96.01user 43.82system 2:21.87elapsed 98%CPU (0avgtext+0avgdata 3986516maxresident)k
0inputs+944outputs (0major+2259755minor)pagefaults 0swaps

/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:36:53 - INFO - __main__ -   ***** Evaluation result  in zh *****
11/28/2021 01:36:53 - INFO - __main__ -     f1 = 0.6164459955386055
11/28/2021 01:36:53 - INFO - __main__ -     loss = 1.4206433804757004
11/28/2021 01:36:53 - INFO - __main__ -     precision = 0.6245807815689706
11/28/2021 01:36:53 - INFO - __main__ -     recall = 0.6085203864185253
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:36:53 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:36:53 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:36:53 - INFO - __main__ -   Seed = 2
11/28/2021 01:36:53 - INFO - root -   save model
11/28/2021 01:36:53 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:36:53 - INFO - __main__ -   Loading pretrained model and tokenizer
56.81user 18.87system 1:16.59elapsed 98%CPU (0avgtext+0avgdata 3984596maxresident)k
0inputs+728outputs (0major+1481864minor)pagefaults 0swaps
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:36:56 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:37:02 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:37:02 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:37:02 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
11/28/2021 01:37:02 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:37:02 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:37:02 - INFO - root -   loading lang adpater bh/wiki@ukp
11/28/2021 01:37:02 - INFO - __main__ -   Adapter Languages : ['bh'], Length : 1
11/28/2021 01:37:02 - INFO - __main__ -   Adapter Names ['bh/wiki@ukp'], Length : 1
11/28/2021 01:37:02 - INFO - __main__ -   Language = bh
11/28/2021 01:37:02 - INFO - __main__ -   Adapter Name = bh/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/bh/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_bh_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/adapter_config.json
Adding adapter 'bh' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/head_config.json
11/28/2021 01:37:14 - INFO - __main__ -   Language adapter for fi not found, using bh instead
11/28/2021 01:37:14 - INFO - __main__ -   Set active language adapter to bh
11/28/2021 01:37:14 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:37:14 - INFO - __main__ -   Adapter Languages = ['bh']
11/28/2021 01:37:14 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_fi_bert-base-multilingual-cased_128
11/28/2021 01:37:15 - INFO - __main__ -   ***** Running evaluation  in fi *****
11/28/2021 01:37:15 - INFO - __main__ -     Num examples = 6550
11/28/2021 01:37:15 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/205 [00:00<?, ?it/s]11/28/2021 01:37:15 - INFO - __main__ -   Batch number = 1
Evaluating:   0%|          | 1/205 [00:00<02:05,  1.62it/s]11/28/2021 01:37:16 - INFO - __main__ -   Batch number = 2
Evaluating:   1%|          | 2/205 [00:01<02:05,  1.62it/s]11/28/2021 01:37:16 - INFO - __main__ -   Batch number = 3
Evaluating:   1%|▏         | 3/205 [00:01<02:05,  1.61it/s]11/28/2021 01:37:17 - INFO - __main__ -   Batch number = 4
Evaluating:   2%|▏         | 4/205 [00:02<02:08,  1.57it/s]11/28/2021 01:37:18 - INFO - __main__ -   Batch number = 5
Evaluating:   2%|▏         | 5/205 [00:03<02:07,  1.57it/s]11/28/2021 01:37:18 - INFO - __main__ -   Batch number = 6
Evaluating:   3%|▎         | 6/205 [00:03<02:08,  1.55it/s]11/28/2021 01:37:19 - INFO - __main__ -   Batch number = 7
Evaluating:   3%|▎         | 7/205 [00:04<02:09,  1.53it/s]11/28/2021 01:37:20 - INFO - __main__ -   Batch number = 8
Evaluating:   4%|▍         | 8/205 [00:05<02:09,  1.52it/s]11/28/2021 01:37:20 - INFO - __main__ -   Batch number = 9
Evaluating:   4%|▍         | 9/205 [00:05<02:09,  1.51it/s]11/28/2021 01:37:21 - INFO - __main__ -   Batch number = 10
Evaluating:   5%|▍         | 10/205 [00:06<02:09,  1.51it/s]11/28/2021 01:37:22 - INFO - __main__ -   Batch number = 11
Evaluating:   5%|▌         | 11/205 [00:07<02:08,  1.50it/s]11/28/2021 01:37:22 - INFO - __main__ -   Batch number = 12
Evaluating:   6%|▌         | 12/205 [00:07<02:07,  1.51it/s]11/28/2021 01:37:23 - INFO - __main__ -   Batch number = 13
Evaluating:   6%|▋         | 13/205 [00:08<02:07,  1.51it/s]11/28/2021 01:37:24 - INFO - __main__ -   Batch number = 14
Evaluating:   7%|▋         | 14/205 [00:09<02:07,  1.50it/s]11/28/2021 01:37:24 - INFO - __main__ -   Batch number = 15
Evaluating:   7%|▋         | 15/205 [00:09<02:06,  1.50it/s]11/28/2021 01:37:25 - INFO - __main__ -   Batch number = 16
Evaluating:   8%|▊         | 16/205 [00:10<02:06,  1.50it/s]11/28/2021 01:37:26 - INFO - __main__ -   Batch number = 17
Evaluating:   8%|▊         | 17/205 [00:11<02:05,  1.50it/s]11/28/2021 01:37:26 - INFO - __main__ -   Batch number = 18
Evaluating:   9%|▉         | 18/205 [00:11<02:02,  1.53it/s]11/28/2021 01:37:27 - INFO - __main__ -   Batch number = 19
Evaluating:   9%|▉         | 19/205 [00:12<01:53,  1.64it/s]11/28/2021 01:37:28 - INFO - __main__ -   Batch number = 20
Evaluating:  10%|▉         | 20/205 [00:12<01:46,  1.74it/s]11/28/2021 01:37:28 - INFO - __main__ -   Batch number = 21
Evaluating:  10%|█         | 21/205 [00:13<01:41,  1.81it/s]11/28/2021 01:37:29 - INFO - __main__ -   Batch number = 22
Evaluating:  11%|█         | 22/205 [00:13<01:38,  1.85it/s]11/28/2021 01:37:29 - INFO - __main__ -   Batch number = 23
Evaluating:  11%|█         | 23/205 [00:14<01:36,  1.88it/s]11/28/2021 01:37:30 - INFO - __main__ -   Batch number = 24
Evaluating:  12%|█▏        | 24/205 [00:14<01:39,  1.81it/s]11/28/2021 01:37:30 - INFO - __main__ -   Batch number = 25
Evaluating:  12%|█▏        | 25/205 [00:15<01:36,  1.86it/s]11/28/2021 01:37:31 - INFO - __main__ -   Batch number = 26
Evaluating:  13%|█▎        | 26/205 [00:15<01:34,  1.90it/s]11/28/2021 01:37:31 - INFO - __main__ -   Batch number = 27
Evaluating:  13%|█▎        | 27/205 [00:16<01:32,  1.92it/s]11/28/2021 01:37:32 - INFO - __main__ -   Batch number = 28
Evaluating:  14%|█▎        | 28/205 [00:16<01:30,  1.95it/s]11/28/2021 01:37:32 - INFO - __main__ -   Batch number = 29
Evaluating:  14%|█▍        | 29/205 [00:17<01:30,  1.95it/s]11/28/2021 01:37:33 - INFO - __main__ -   Batch number = 30
Evaluating:  15%|█▍        | 30/205 [00:17<01:29,  1.96it/s]11/28/2021 01:37:33 - INFO - __main__ -   Batch number = 31
Evaluating:  15%|█▌        | 31/205 [00:18<01:28,  1.97it/s]11/28/2021 01:37:34 - INFO - __main__ -   Batch number = 32
Evaluating:  16%|█▌        | 32/205 [00:18<01:27,  1.98it/s]11/28/2021 01:37:34 - INFO - __main__ -   Batch number = 33
Evaluating:  16%|█▌        | 33/205 [00:19<01:26,  1.98it/s]11/28/2021 01:37:35 - INFO - __main__ -   Batch number = 34
Evaluating:  17%|█▋        | 34/205 [00:19<01:26,  1.98it/s]11/28/2021 01:37:35 - INFO - __main__ -   Batch number = 35
Evaluating:  17%|█▋        | 35/205 [00:20<01:25,  1.98it/s]11/28/2021 01:37:36 - INFO - __main__ -   Batch number = 36
Evaluating:  18%|█▊        | 36/205 [00:20<01:25,  1.99it/s]11/28/2021 01:37:36 - INFO - __main__ -   Batch number = 37
Evaluating:  18%|█▊        | 37/205 [00:21<01:24,  1.98it/s]11/28/2021 01:37:37 - INFO - __main__ -   Batch number = 38
Evaluating:  19%|█▊        | 38/205 [00:21<01:25,  1.96it/s]11/28/2021 01:37:37 - INFO - __main__ -   Batch number = 39
Evaluating:  19%|█▉        | 39/205 [00:22<01:24,  1.97it/s]11/28/2021 01:37:38 - INFO - __main__ -   Batch number = 40
Evaluating:  20%|█▉        | 40/205 [00:22<01:23,  1.97it/s]11/28/2021 01:37:38 - INFO - __main__ -   Batch number = 41
Evaluating:  20%|██        | 41/205 [00:23<01:22,  1.98it/s]11/28/2021 01:37:39 - INFO - __main__ -   Batch number = 42
Evaluating:  20%|██        | 42/205 [00:23<01:21,  1.99it/s]11/28/2021 01:37:39 - INFO - __main__ -   Batch number = 43
Evaluating:  21%|██        | 43/205 [00:24<01:21,  1.99it/s]11/28/2021 01:37:40 - INFO - __main__ -   Batch number = 44
Evaluating:  21%|██▏       | 44/205 [00:25<01:24,  1.91it/s]11/28/2021 01:37:40 - INFO - __main__ -   Batch number = 45
Evaluating:  22%|██▏       | 45/205 [00:25<01:23,  1.93it/s]11/28/2021 01:37:41 - INFO - __main__ -   Batch number = 46
Evaluating:  22%|██▏       | 46/205 [00:26<01:21,  1.95it/s]11/28/2021 01:37:41 - INFO - __main__ -   Batch number = 47
Evaluating:  23%|██▎       | 47/205 [00:26<01:20,  1.96it/s]11/28/2021 01:37:42 - INFO - __main__ -   Batch number = 48
Evaluating:  23%|██▎       | 48/205 [00:27<01:19,  1.97it/s]11/28/2021 01:37:42 - INFO - __main__ -   Batch number = 49
Evaluating:  24%|██▍       | 49/205 [00:27<01:18,  1.98it/s]11/28/2021 01:37:43 - INFO - __main__ -   Batch number = 50
Evaluating:  24%|██▍       | 50/205 [00:28<01:18,  1.98it/s]11/28/2021 01:37:43 - INFO - __main__ -   Batch number = 51
Evaluating:  25%|██▍       | 51/205 [00:28<01:17,  1.98it/s]11/28/2021 01:37:44 - INFO - __main__ -   Batch number = 52
Evaluating:  25%|██▌       | 52/205 [00:29<01:16,  1.99it/s]11/28/2021 01:37:44 - INFO - __main__ -   Batch number = 53
Evaluating:  26%|██▌       | 53/205 [00:29<01:12,  2.09it/s]11/28/2021 01:37:45 - INFO - __main__ -   Batch number = 54
Evaluating:  26%|██▋       | 54/205 [00:29<01:06,  2.26it/s]11/28/2021 01:37:45 - INFO - __main__ -   Batch number = 55
Evaluating:  27%|██▋       | 55/205 [00:30<01:01,  2.44it/s]11/28/2021 01:37:45 - INFO - __main__ -   Batch number = 56
Evaluating:  27%|██▋       | 56/205 [00:30<00:57,  2.57it/s]11/28/2021 01:37:46 - INFO - __main__ -   Batch number = 57
Evaluating:  28%|██▊       | 57/205 [00:30<00:55,  2.69it/s]11/28/2021 01:37:46 - INFO - __main__ -   Batch number = 58
Evaluating:  28%|██▊       | 58/205 [00:31<00:53,  2.77it/s]11/28/2021 01:37:46 - INFO - __main__ -   Batch number = 59
Evaluating:  29%|██▉       | 59/205 [00:31<00:51,  2.83it/s]11/28/2021 01:37:47 - INFO - __main__ -   Batch number = 60
Evaluating:  29%|██▉       | 60/205 [00:31<00:52,  2.79it/s]11/28/2021 01:37:47 - INFO - __main__ -   Batch number = 61
Evaluating:  30%|██▉       | 61/205 [00:32<00:50,  2.85it/s]11/28/2021 01:37:48 - INFO - __main__ -   Batch number = 62
Evaluating:  30%|███       | 62/205 [00:32<00:49,  2.90it/s]11/28/2021 01:37:48 - INFO - __main__ -   Batch number = 63
Evaluating:  31%|███       | 63/205 [00:32<00:49,  2.89it/s]11/28/2021 01:37:48 - INFO - __main__ -   Batch number = 64
Evaluating:  31%|███       | 64/205 [00:33<00:48,  2.91it/s]11/28/2021 01:37:49 - INFO - __main__ -   Batch number = 65
Evaluating:  32%|███▏      | 65/205 [00:33<00:48,  2.91it/s]11/28/2021 01:37:49 - INFO - __main__ -   Batch number = 66
Evaluating:  32%|███▏      | 66/205 [00:33<00:47,  2.92it/s]11/28/2021 01:37:49 - INFO - __main__ -   Batch number = 67
Evaluating:  33%|███▎      | 67/205 [00:34<00:46,  2.94it/s]11/28/2021 01:37:50 - INFO - __main__ -   Batch number = 68
Evaluating:  33%|███▎      | 68/205 [00:34<00:46,  2.95it/s]11/28/2021 01:37:50 - INFO - __main__ -   Batch number = 69
Evaluating:  34%|███▎      | 69/205 [00:34<00:45,  2.96it/s]11/28/2021 01:37:50 - INFO - __main__ -   Batch number = 70
Evaluating:  34%|███▍      | 70/205 [00:35<00:46,  2.93it/s]11/28/2021 01:37:51 - INFO - __main__ -   Batch number = 71
Evaluating:  35%|███▍      | 71/205 [00:35<00:45,  2.93it/s]11/28/2021 01:37:51 - INFO - __main__ -   Batch number = 72
Evaluating:  35%|███▌      | 72/205 [00:35<00:45,  2.94it/s]11/28/2021 01:37:51 - INFO - __main__ -   Batch number = 73
Evaluating:  36%|███▌      | 73/205 [00:36<00:44,  2.94it/s]11/28/2021 01:37:52 - INFO - __main__ -   Batch number = 74
Evaluating:  36%|███▌      | 74/205 [00:36<00:44,  2.95it/s]11/28/2021 01:37:52 - INFO - __main__ -   Batch number = 75
Evaluating:  37%|███▋      | 75/205 [00:37<00:50,  2.57it/s]11/28/2021 01:37:52 - INFO - __main__ -   Batch number = 76
Evaluating:  37%|███▋      | 76/205 [00:37<00:47,  2.70it/s]11/28/2021 01:37:53 - INFO - __main__ -   Batch number = 77
Evaluating:  38%|███▊      | 77/205 [00:37<00:50,  2.52it/s]11/28/2021 01:37:53 - INFO - __main__ -   Batch number = 78
Evaluating:  38%|███▊      | 78/205 [00:38<00:54,  2.31it/s]11/28/2021 01:37:54 - INFO - __main__ -   Batch number = 79
Evaluating:  39%|███▊      | 79/205 [00:38<00:57,  2.20it/s]11/28/2021 01:37:54 - INFO - __main__ -   Batch number = 80
Evaluating:  39%|███▉      | 80/205 [00:39<00:56,  2.20it/s]11/28/2021 01:37:55 - INFO - __main__ -   Batch number = 81
Evaluating:  40%|███▉      | 81/205 [00:39<00:58,  2.12it/s]11/28/2021 01:37:55 - INFO - __main__ -   Batch number = 82
Evaluating:  40%|████      | 82/205 [00:40<00:59,  2.08it/s]11/28/2021 01:37:56 - INFO - __main__ -   Batch number = 83
Evaluating:  40%|████      | 83/205 [00:40<00:59,  2.05it/s]11/28/2021 01:37:56 - INFO - __main__ -   Batch number = 84
Evaluating:  41%|████      | 84/205 [00:41<00:59,  2.03it/s]11/28/2021 01:37:57 - INFO - __main__ -   Batch number = 85
Evaluating:  41%|████▏     | 85/205 [00:41<00:59,  2.02it/s]11/28/2021 01:37:57 - INFO - __main__ -   Batch number = 86
Evaluating:  42%|████▏     | 86/205 [00:42<00:59,  2.00it/s]11/28/2021 01:37:58 - INFO - __main__ -   Batch number = 87
Evaluating:  42%|████▏     | 87/205 [00:42<00:58,  2.00it/s]11/28/2021 01:37:58 - INFO - __main__ -   Batch number = 88
Evaluating:  43%|████▎     | 88/205 [00:43<00:58,  1.99it/s]11/28/2021 01:37:59 - INFO - __main__ -   Batch number = 89
Evaluating:  43%|████▎     | 89/205 [00:43<00:58,  1.99it/s]11/28/2021 01:37:59 - INFO - __main__ -   Batch number = 90
Evaluating:  44%|████▍     | 90/205 [00:44<00:57,  1.99it/s]11/28/2021 01:38:00 - INFO - __main__ -   Batch number = 91
Evaluating:  44%|████▍     | 91/205 [00:44<00:57,  1.98it/s]11/28/2021 01:38:00 - INFO - __main__ -   Batch number = 92
Evaluating:  45%|████▍     | 92/205 [00:45<00:57,  1.97it/s]11/28/2021 01:38:01 - INFO - __main__ -   Batch number = 93
Evaluating:  45%|████▌     | 93/205 [00:45<00:56,  1.97it/s]11/28/2021 01:38:01 - INFO - __main__ -   Batch number = 94
Evaluating:  46%|████▌     | 94/205 [00:46<00:56,  1.97it/s]11/28/2021 01:38:02 - INFO - __main__ -   Batch number = 95
Evaluating:  46%|████▋     | 95/205 [00:47<00:55,  1.97it/s]11/28/2021 01:38:02 - INFO - __main__ -   Batch number = 96
Evaluating:  47%|████▋     | 96/205 [00:47<00:55,  1.96it/s]11/28/2021 01:38:03 - INFO - __main__ -   Batch number = 97
Evaluating:  47%|████▋     | 97/205 [00:48<00:55,  1.96it/s]11/28/2021 01:38:03 - INFO - __main__ -   Batch number = 98
Evaluating:  48%|████▊     | 98/205 [00:48<00:54,  1.96it/s]11/28/2021 01:38:04 - INFO - __main__ -   Batch number = 99
Evaluating:  48%|████▊     | 99/205 [00:49<00:54,  1.96it/s]11/28/2021 01:38:04 - INFO - __main__ -   Batch number = 100
Evaluating:  49%|████▉     | 100/205 [00:49<00:53,  1.96it/s]11/28/2021 01:38:05 - INFO - __main__ -   Batch number = 101
Evaluating:  49%|████▉     | 101/205 [00:50<00:52,  1.96it/s]11/28/2021 01:38:05 - INFO - __main__ -   Batch number = 102
Evaluating:  50%|████▉     | 102/205 [00:50<00:52,  1.97it/s]11/28/2021 01:38:06 - INFO - __main__ -   Batch number = 103
Evaluating:  50%|█████     | 103/205 [00:51<00:52,  1.96it/s]11/28/2021 01:38:06 - INFO - __main__ -   Batch number = 104
Evaluating:  51%|█████     | 104/205 [00:51<00:51,  1.97it/s]11/28/2021 01:38:07 - INFO - __main__ -   Batch number = 105
Evaluating:  51%|█████     | 105/205 [00:52<00:50,  1.97it/s]11/28/2021 01:38:07 - INFO - __main__ -   Batch number = 106
Evaluating:  52%|█████▏    | 106/205 [00:52<00:50,  1.96it/s]11/28/2021 01:38:08 - INFO - __main__ -   Batch number = 107
Evaluating:  52%|█████▏    | 107/205 [00:53<00:51,  1.89it/s]11/28/2021 01:38:08 - INFO - __main__ -   Batch number = 108
Evaluating:  53%|█████▎    | 108/205 [00:53<00:52,  1.86it/s]11/28/2021 01:38:09 - INFO - __main__ -   Batch number = 109
Evaluating:  53%|█████▎    | 109/205 [00:54<00:51,  1.85it/s]11/28/2021 01:38:10 - INFO - __main__ -   Batch number = 110
Evaluating:  54%|█████▎    | 110/205 [00:54<00:50,  1.88it/s]11/28/2021 01:38:10 - INFO - __main__ -   Batch number = 111
Evaluating:  54%|█████▍    | 111/205 [00:55<00:51,  1.83it/s]11/28/2021 01:38:11 - INFO - __main__ -   Batch number = 112
Evaluating:  55%|█████▍    | 112/205 [00:56<01:14,  1.25it/s]11/28/2021 01:38:12 - INFO - __main__ -   Batch number = 113
Evaluating:  55%|█████▌    | 113/205 [00:57<01:10,  1.30it/s]11/28/2021 01:38:13 - INFO - __main__ -   Batch number = 114
Evaluating:  56%|█████▌    | 114/205 [00:58<01:28,  1.03it/s]11/28/2021 01:38:14 - INFO - __main__ -   Batch number = 115
Evaluating:  56%|█████▌    | 115/205 [00:59<01:19,  1.14it/s]11/28/2021 01:38:15 - INFO - __main__ -   Batch number = 116
Evaluating:  57%|█████▋    | 116/205 [01:00<01:13,  1.22it/s]11/28/2021 01:38:16 - INFO - __main__ -   Batch number = 117
Evaluating:  57%|█████▋    | 117/205 [01:01<01:16,  1.15it/s]11/28/2021 01:38:17 - INFO - __main__ -   Batch number = 118
Evaluating:  58%|█████▊    | 118/205 [01:02<01:13,  1.19it/s]11/28/2021 01:38:17 - INFO - __main__ -   Batch number = 119
Evaluating:  58%|█████▊    | 119/205 [01:03<01:31,  1.06s/it]11/28/2021 01:38:19 - INFO - __main__ -   Batch number = 120
Evaluating:  59%|█████▊    | 120/205 [01:04<01:21,  1.05it/s]11/28/2021 01:38:20 - INFO - __main__ -   Batch number = 121
Evaluating:  59%|█████▉    | 121/205 [01:06<01:45,  1.25s/it]11/28/2021 01:38:22 - INFO - __main__ -   Batch number = 122
Evaluating:  60%|█████▉    | 122/205 [01:07<01:43,  1.25s/it]11/28/2021 01:38:23 - INFO - __main__ -   Batch number = 123
Evaluating:  60%|██████    | 123/205 [01:08<01:35,  1.16s/it]11/28/2021 01:38:24 - INFO - __main__ -   Batch number = 124
Evaluating:  60%|██████    | 124/205 [01:09<01:22,  1.02s/it]11/28/2021 01:38:24 - INFO - __main__ -   Batch number = 125
Evaluating:  61%|██████    | 125/205 [01:10<01:21,  1.02s/it]11/28/2021 01:38:25 - INFO - __main__ -   Batch number = 126
Evaluating:  61%|██████▏   | 126/205 [01:10<01:12,  1.09it/s]11/28/2021 01:38:26 - INFO - __main__ -   Batch number = 127
Evaluating:  62%|██████▏   | 127/205 [01:11<01:11,  1.10it/s]11/28/2021 01:38:27 - INFO - __main__ -   Batch number = 128
Evaluating:  62%|██████▏   | 128/205 [01:12<01:05,  1.18it/s]11/28/2021 01:38:28 - INFO - __main__ -   Batch number = 129
Evaluating:  63%|██████▎   | 129/205 [01:13<01:01,  1.25it/s]11/28/2021 01:38:28 - INFO - __main__ -   Batch number = 130
Evaluating:  63%|██████▎   | 130/205 [01:13<00:57,  1.30it/s]11/28/2021 01:38:29 - INFO - __main__ -   Batch number = 131
Evaluating:  64%|██████▍   | 131/205 [01:14<01:00,  1.22it/s]11/28/2021 01:38:30 - INFO - __main__ -   Batch number = 132
Evaluating:  64%|██████▍   | 132/205 [01:15<00:57,  1.27it/s]11/28/2021 01:38:31 - INFO - __main__ -   Batch number = 133
Evaluating:  65%|██████▍   | 133/205 [01:16<01:10,  1.03it/s]11/28/2021 01:38:32 - INFO - __main__ -   Batch number = 134
Evaluating:  65%|██████▌   | 134/205 [01:17<01:02,  1.13it/s]11/28/2021 01:38:33 - INFO - __main__ -   Batch number = 135
Evaluating:  66%|██████▌   | 135/205 [01:18<01:04,  1.09it/s]11/28/2021 01:38:34 - INFO - __main__ -   Batch number = 136
Evaluating:  66%|██████▋   | 136/205 [01:19<01:11,  1.04s/it]11/28/2021 01:38:35 - INFO - __main__ -   Batch number = 137
Evaluating:  67%|██████▋   | 137/205 [01:20<01:03,  1.06it/s]11/28/2021 01:38:36 - INFO - __main__ -   Batch number = 138
Evaluating:  67%|██████▋   | 138/205 [01:21<01:08,  1.02s/it]11/28/2021 01:38:37 - INFO - __main__ -   Batch number = 139
Evaluating:  68%|██████▊   | 139/205 [01:22<01:00,  1.09it/s]11/28/2021 01:38:38 - INFO - __main__ -   Batch number = 140
Evaluating:  68%|██████▊   | 140/205 [01:23<01:02,  1.04it/s]11/28/2021 01:38:39 - INFO - __main__ -   Batch number = 141
Evaluating:  69%|██████▉   | 141/205 [01:24<01:02,  1.02it/s]11/28/2021 01:38:40 - INFO - __main__ -   Batch number = 142
Evaluating:  69%|██████▉   | 142/205 [01:26<01:14,  1.18s/it]11/28/2021 01:38:41 - INFO - __main__ -   Batch number = 143
Evaluating:  70%|██████▉   | 143/205 [01:26<01:01,  1.02it/s]11/28/2021 01:38:42 - INFO - __main__ -   Batch number = 144
Evaluating:  70%|███████   | 144/205 [01:27<01:04,  1.06s/it]11/28/2021 01:38:43 - INFO - __main__ -   Batch number = 145
Evaluating:  71%|███████   | 145/205 [01:28<00:50,  1.18it/s]11/28/2021 01:38:44 - INFO - __main__ -   Batch number = 146
Evaluating:  71%|███████   | 146/205 [01:29<00:47,  1.24it/s]11/28/2021 01:38:44 - INFO - __main__ -   Batch number = 147
Evaluating:  72%|███████▏  | 147/205 [01:29<00:45,  1.26it/s]11/28/2021 01:38:45 - INFO - __main__ -   Batch number = 148
Evaluating:  72%|███████▏  | 148/205 [01:31<00:55,  1.03it/s]11/28/2021 01:38:46 - INFO - __main__ -   Batch number = 149
Evaluating:  73%|███████▎  | 149/205 [01:31<00:44,  1.26it/s]11/28/2021 01:38:47 - INFO - __main__ -   Batch number = 150
Evaluating:  73%|███████▎  | 150/205 [01:31<00:36,  1.52it/s]11/28/2021 01:38:47 - INFO - __main__ -   Batch number = 151
Evaluating:  74%|███████▎  | 151/205 [01:33<00:42,  1.26it/s]11/28/2021 01:38:48 - INFO - __main__ -   Batch number = 152
Evaluating:  74%|███████▍  | 152/205 [01:34<00:48,  1.10it/s]11/28/2021 01:38:49 - INFO - __main__ -   Batch number = 153
Evaluating:  75%|███████▍  | 153/205 [01:34<00:37,  1.39it/s]11/28/2021 01:38:50 - INFO - __main__ -   Batch number = 154
Evaluating:  75%|███████▌  | 154/205 [01:34<00:29,  1.71it/s]11/28/2021 01:38:50 - INFO - __main__ -   Batch number = 155
Evaluating:  76%|███████▌  | 155/205 [01:34<00:23,  2.14it/s]11/28/2021 01:38:50 - INFO - __main__ -   Batch number = 156
Evaluating:  76%|███████▌  | 156/205 [01:35<00:18,  2.63it/s]11/28/2021 01:38:50 - INFO - __main__ -   Batch number = 157
Evaluating:  77%|███████▋  | 157/205 [01:36<00:26,  1.80it/s]11/28/2021 01:38:51 - INFO - __main__ -   Batch number = 158
Evaluating:  77%|███████▋  | 158/205 [01:36<00:21,  2.23it/s]11/28/2021 01:38:52 - INFO - __main__ -   Batch number = 159
Evaluating:  78%|███████▊  | 159/205 [01:36<00:17,  2.70it/s]11/28/2021 01:38:52 - INFO - __main__ -   Batch number = 160
Evaluating:  78%|███████▊  | 160/205 [01:38<00:41,  1.10it/s]11/28/2021 01:38:54 - INFO - __main__ -   Batch number = 161
Evaluating:  79%|███████▊  | 161/205 [01:38<00:31,  1.38it/s]11/28/2021 01:38:54 - INFO - __main__ -   Batch number = 162
Evaluating:  79%|███████▉  | 162/205 [01:39<00:24,  1.76it/s]11/28/2021 01:38:54 - INFO - __main__ -   Batch number = 163
Evaluating:  80%|███████▉  | 163/205 [01:40<00:33,  1.26it/s]11/28/2021 01:38:56 - INFO - __main__ -   Batch number = 164
Evaluating:  80%|████████  | 164/205 [01:40<00:27,  1.47it/s]11/28/2021 01:38:56 - INFO - __main__ -   Batch number = 165
Evaluating:  80%|████████  | 165/205 [01:41<00:23,  1.72it/s]11/28/2021 01:38:56 - INFO - __main__ -   Batch number = 166
Evaluating:  81%|████████  | 166/205 [01:41<00:18,  2.16it/s]11/28/2021 01:38:57 - INFO - __main__ -   Batch number = 167
Evaluating:  81%|████████▏ | 167/205 [01:43<00:33,  1.13it/s]11/28/2021 01:38:59 - INFO - __main__ -   Batch number = 168
Evaluating:  82%|████████▏ | 168/205 [01:43<00:25,  1.47it/s]11/28/2021 01:38:59 - INFO - __main__ -   Batch number = 169
Evaluating:  82%|████████▏ | 169/205 [01:43<00:19,  1.87it/s]11/28/2021 01:38:59 - INFO - __main__ -   Batch number = 170
Evaluating:  83%|████████▎ | 170/205 [01:44<00:16,  2.12it/s]11/28/2021 01:38:59 - INFO - __main__ -   Batch number = 171
Evaluating:  83%|████████▎ | 171/205 [01:45<00:23,  1.44it/s]11/28/2021 01:39:00 - INFO - __main__ -   Batch number = 172
Evaluating:  84%|████████▍ | 172/205 [01:45<00:20,  1.61it/s]11/28/2021 01:39:01 - INFO - __main__ -   Batch number = 173
Evaluating:  84%|████████▍ | 173/205 [01:46<00:18,  1.74it/s]11/28/2021 01:39:01 - INFO - __main__ -   Batch number = 174
Evaluating:  85%|████████▍ | 174/205 [01:46<00:18,  1.68it/s]11/28/2021 01:39:02 - INFO - __main__ -   Batch number = 175
Evaluating:  85%|████████▌ | 175/205 [01:47<00:16,  1.87it/s]11/28/2021 01:39:02 - INFO - __main__ -   Batch number = 176
Evaluating:  86%|████████▌ | 176/205 [01:47<00:16,  1.80it/s]11/28/2021 01:39:03 - INFO - __main__ -   Batch number = 177
Evaluating:  86%|████████▋ | 177/205 [01:48<00:17,  1.57it/s]11/28/2021 01:39:04 - INFO - __main__ -   Batch number = 178
Evaluating:  87%|████████▋ | 178/205 [01:49<00:18,  1.46it/s]11/28/2021 01:39:05 - INFO - __main__ -   Batch number = 179
Evaluating:  87%|████████▋ | 179/205 [01:50<00:18,  1.40it/s]11/28/2021 01:39:05 - INFO - __main__ -   Batch number = 180
Evaluating:  88%|████████▊ | 180/205 [01:51<00:20,  1.24it/s]11/28/2021 01:39:06 - INFO - __main__ -   Batch number = 181
Evaluating:  88%|████████▊ | 181/205 [01:51<00:18,  1.27it/s]11/28/2021 01:39:07 - INFO - __main__ -   Batch number = 182
Evaluating:  89%|████████▉ | 182/205 [01:54<00:27,  1.21s/it]11/28/2021 01:39:09 - INFO - __main__ -   Batch number = 183
Evaluating:  89%|████████▉ | 183/205 [01:54<00:23,  1.09s/it]11/28/2021 01:39:10 - INFO - __main__ -   Batch number = 184
Evaluating:  90%|████████▉ | 184/205 [01:55<00:21,  1.03s/it]11/28/2021 01:39:11 - INFO - __main__ -   Batch number = 185
Evaluating:  90%|█████████ | 185/205 [01:56<00:18,  1.06it/s]11/28/2021 01:39:12 - INFO - __main__ -   Batch number = 186
Evaluating:  91%|█████████ | 186/205 [01:57<00:18,  1.04it/s]11/28/2021 01:39:13 - INFO - __main__ -   Batch number = 187
Evaluating:  91%|█████████ | 187/205 [01:58<00:16,  1.08it/s]11/28/2021 01:39:14 - INFO - __main__ -   Batch number = 188
Evaluating:  92%|█████████▏| 188/205 [01:58<00:13,  1.23it/s]11/28/2021 01:39:14 - INFO - __main__ -   Batch number = 189
Evaluating:  92%|█████████▏| 189/205 [02:00<00:17,  1.09s/it]11/28/2021 01:39:16 - INFO - __main__ -   Batch number = 190
Evaluating:  93%|█████████▎| 190/205 [02:01<00:16,  1.13s/it]11/28/2021 01:39:17 - INFO - __main__ -   Batch number = 191
Evaluating:  93%|█████████▎| 191/205 [02:02<00:13,  1.04it/s]11/28/2021 01:39:18 - INFO - __main__ -   Batch number = 192
Evaluating:  94%|█████████▎| 192/205 [02:03<00:10,  1.21it/s]11/28/2021 01:39:18 - INFO - __main__ -   Batch number = 193
Evaluating:  94%|█████████▍| 193/205 [02:04<00:11,  1.05it/s]11/28/2021 01:39:20 - INFO - __main__ -   Batch number = 194
Evaluating:  95%|█████████▍| 194/205 [02:05<00:10,  1.09it/s]11/28/2021 01:39:20 - INFO - __main__ -   Batch number = 195
Evaluating:  95%|█████████▌| 195/205 [02:06<00:09,  1.03it/s]11/28/2021 01:39:21 - INFO - __main__ -   Batch number = 196
Evaluating:  96%|█████████▌| 196/205 [02:07<00:09,  1.02s/it]11/28/2021 01:39:23 - INFO - __main__ -   Batch number = 197
Evaluating:  96%|█████████▌| 197/205 [02:08<00:07,  1.01it/s]11/28/2021 01:39:24 - INFO - __main__ -   Batch number = 198
Evaluating:  97%|█████████▋| 198/205 [02:09<00:07,  1.03s/it]11/28/2021 01:39:25 - INFO - __main__ -   Batch number = 199
Evaluating:  97%|█████████▋| 199/205 [02:11<00:08,  1.36s/it]11/28/2021 01:39:27 - INFO - __main__ -   Batch number = 200
Evaluating:  98%|█████████▊| 200/205 [02:12<00:06,  1.30s/it]11/28/2021 01:39:28 - INFO - __main__ -   Batch number = 201
Evaluating:  98%|█████████▊| 201/205 [02:13<00:04,  1.20s/it]11/28/2021 01:39:29 - INFO - __main__ -   Batch number = 202
Evaluating:  99%|█████████▊| 202/205 [02:14<00:03,  1.18s/it]11/28/2021 01:39:30 - INFO - __main__ -   Batch number = 203
Evaluating:  99%|█████████▉| 203/205 [02:15<00:02,  1.01s/it]11/28/2021 01:39:31 - INFO - __main__ -   Batch number = 204
Evaluating: 100%|█████████▉| 204/205 [02:17<00:01,  1.31s/it]11/28/2021 01:39:33 - INFO - __main__ -   Batch number = 205
Evaluating: 100%|██████████| 205/205 [02:17<00:00,  1.04s/it]Evaluating: 100%|██████████| 205/205 [02:17<00:00,  1.49it/s]
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:39:35 - INFO - __main__ -   ***** Evaluation result  in fi *****
11/28/2021 01:39:35 - INFO - __main__ -     f1 = 0.717162567068497
11/28/2021 01:39:35 - INFO - __main__ -     loss = 1.0134120856843343
11/28/2021 01:39:35 - INFO - __main__ -     precision = 0.7237806497991842
11/28/2021 01:39:35 - INFO - __main__ -     recall = 0.7106644161423968
96.57user 68.01system 2:44.86elapsed 99%CPU (0avgtext+0avgdata 3988220maxresident)k
0inputs+936outputs (0major+2353162minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:39:38 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:39:38 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:39:38 - INFO - __main__ -   Seed = 3
11/28/2021 01:39:38 - INFO - root -   save model
11/28/2021 01:39:38 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:39:38 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:39:41 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:39:48 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:39:48 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:39:48 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
11/28/2021 01:39:48 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:39:48 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:39:48 - INFO - root -   loading lang adpater bh/wiki@ukp
11/28/2021 01:39:48 - INFO - __main__ -   Adapter Languages : ['bh'], Length : 1
11/28/2021 01:39:48 - INFO - __main__ -   Adapter Names ['bh/wiki@ukp'], Length : 1
11/28/2021 01:39:48 - INFO - __main__ -   Language = bh
11/28/2021 01:39:48 - INFO - __main__ -   Adapter Name = bh/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/bh/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_bh_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/adapter_config.json
Adding adapter 'bh' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/head_config.json
11/28/2021 01:39:59 - INFO - __main__ -   Language adapter for fi not found, using bh instead
11/28/2021 01:39:59 - INFO - __main__ -   Set active language adapter to bh
11/28/2021 01:39:59 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:39:59 - INFO - __main__ -   Adapter Languages = ['bh']
11/28/2021 01:39:59 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_fi_bert-base-multilingual-cased_128
11/28/2021 01:40:00 - INFO - __main__ -   ***** Running evaluation  in fi *****
11/28/2021 01:40:00 - INFO - __main__ -     Num examples = 6550
11/28/2021 01:40:00 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/205 [00:00<?, ?it/s]11/28/2021 01:40:00 - INFO - __main__ -   Batch number = 1
Evaluating:   0%|          | 1/205 [00:00<02:16,  1.49it/s]11/28/2021 01:40:01 - INFO - __main__ -   Batch number = 2
Evaluating:   1%|          | 2/205 [00:01<02:15,  1.50it/s]11/28/2021 01:40:02 - INFO - __main__ -   Batch number = 3
Evaluating:   1%|▏         | 3/205 [00:01<02:13,  1.51it/s]11/28/2021 01:40:02 - INFO - __main__ -   Batch number = 4
Evaluating:   2%|▏         | 4/205 [00:02<02:00,  1.67it/s]11/28/2021 01:40:03 - INFO - __main__ -   Batch number = 5
Evaluating:   2%|▏         | 5/205 [00:03<02:04,  1.60it/s]11/28/2021 01:40:03 - INFO - __main__ -   Batch number = 6
Evaluating:   3%|▎         | 6/205 [00:03<02:06,  1.57it/s]11/28/2021 01:40:04 - INFO - __main__ -   Batch number = 7
Evaluating:   3%|▎         | 7/205 [00:04<02:07,  1.56it/s]11/28/2021 01:40:05 - INFO - __main__ -   Batch number = 8
Evaluating:   4%|▍         | 8/205 [00:05<02:12,  1.49it/s]11/28/2021 01:40:05 - INFO - __main__ -   Batch number = 9
Evaluating:   4%|▍         | 9/205 [00:05<02:10,  1.50it/s]11/28/2021 01:40:06 - INFO - __main__ -   Batch number = 10
Evaluating:   5%|▍         | 10/205 [00:06<02:01,  1.61it/s]11/28/2021 01:40:07 - INFO - __main__ -   Batch number = 11
Evaluating:   5%|▌         | 11/205 [00:06<01:53,  1.71it/s]11/28/2021 01:40:07 - INFO - __main__ -   Batch number = 12
Evaluating:   6%|▌         | 12/205 [00:07<01:49,  1.77it/s]11/28/2021 01:40:08 - INFO - __main__ -   Batch number = 13
Evaluating:   6%|▋         | 13/205 [00:07<01:45,  1.83it/s]11/28/2021 01:40:08 - INFO - __main__ -   Batch number = 14
Evaluating:   7%|▋         | 14/205 [00:08<01:44,  1.82it/s]11/28/2021 01:40:09 - INFO - __main__ -   Batch number = 15
Evaluating:   7%|▋         | 15/205 [00:09<01:51,  1.71it/s]11/28/2021 01:40:09 - INFO - __main__ -   Batch number = 16
Evaluating:   8%|▊         | 16/205 [00:09<01:52,  1.68it/s]11/28/2021 01:40:10 - INFO - __main__ -   Batch number = 17
Evaluating:   8%|▊         | 17/205 [00:10<01:58,  1.59it/s]11/28/2021 01:40:11 - INFO - __main__ -   Batch number = 18
Evaluating:   9%|▉         | 18/205 [00:11<01:59,  1.56it/s]11/28/2021 01:40:11 - INFO - __main__ -   Batch number = 19
Evaluating:   9%|▉         | 19/205 [00:11<01:59,  1.55it/s]11/28/2021 01:40:12 - INFO - __main__ -   Batch number = 20
Evaluating:  10%|▉         | 20/205 [00:12<02:00,  1.54it/s]11/28/2021 01:40:13 - INFO - __main__ -   Batch number = 21
Evaluating:  10%|█         | 21/205 [00:13<02:00,  1.53it/s]11/28/2021 01:40:13 - INFO - __main__ -   Batch number = 22
Evaluating:  11%|█         | 22/205 [00:13<02:01,  1.51it/s]11/28/2021 01:40:14 - INFO - __main__ -   Batch number = 23
Evaluating:  11%|█         | 23/205 [00:14<02:00,  1.51it/s]11/28/2021 01:40:15 - INFO - __main__ -   Batch number = 24
Evaluating:  12%|█▏        | 24/205 [00:15<02:00,  1.50it/s]11/28/2021 01:40:15 - INFO - __main__ -   Batch number = 25
Evaluating:  12%|█▏        | 25/205 [00:15<01:59,  1.50it/s]11/28/2021 01:40:16 - INFO - __main__ -   Batch number = 26
Evaluating:  13%|█▎        | 26/205 [00:16<01:51,  1.60it/s]11/28/2021 01:40:17 - INFO - __main__ -   Batch number = 27
Evaluating:  13%|█▎        | 27/205 [00:16<01:43,  1.71it/s]11/28/2021 01:40:17 - INFO - __main__ -   Batch number = 28
Evaluating:  14%|█▎        | 28/205 [00:17<01:38,  1.79it/s]11/28/2021 01:40:18 - INFO - __main__ -   Batch number = 29
Evaluating:  14%|█▍        | 29/205 [00:17<01:31,  1.93it/s]11/28/2021 01:40:18 - INFO - __main__ -   Batch number = 30
Evaluating:  15%|█▍        | 30/205 [00:18<01:29,  1.95it/s]11/28/2021 01:40:18 - INFO - __main__ -   Batch number = 31
Evaluating:  15%|█▌        | 31/205 [00:18<01:26,  2.00it/s]11/28/2021 01:40:19 - INFO - __main__ -   Batch number = 32
Evaluating:  16%|█▌        | 32/205 [00:19<01:18,  2.21it/s]11/28/2021 01:40:19 - INFO - __main__ -   Batch number = 33
Evaluating:  16%|█▌        | 33/205 [00:19<01:19,  2.16it/s]11/28/2021 01:40:20 - INFO - __main__ -   Batch number = 34
Evaluating:  17%|█▋        | 34/205 [00:20<01:21,  2.11it/s]11/28/2021 01:40:20 - INFO - __main__ -   Batch number = 35
Evaluating:  17%|█▋        | 35/205 [00:20<01:21,  2.08it/s]11/28/2021 01:40:21 - INFO - __main__ -   Batch number = 36
Evaluating:  18%|█▊        | 36/205 [00:21<01:22,  2.06it/s]11/28/2021 01:40:21 - INFO - __main__ -   Batch number = 37
Evaluating:  18%|█▊        | 37/205 [00:21<01:14,  2.25it/s]11/28/2021 01:40:22 - INFO - __main__ -   Batch number = 38
Evaluating:  19%|█▊        | 38/205 [00:21<01:17,  2.16it/s]11/28/2021 01:40:22 - INFO - __main__ -   Batch number = 39
Evaluating:  19%|█▉        | 39/205 [00:22<01:18,  2.11it/s]11/28/2021 01:40:23 - INFO - __main__ -   Batch number = 40
Evaluating:  20%|█▉        | 40/205 [00:22<01:18,  2.09it/s]11/28/2021 01:40:23 - INFO - __main__ -   Batch number = 41
Evaluating:  20%|██        | 41/205 [00:23<01:18,  2.08it/s]11/28/2021 01:40:24 - INFO - __main__ -   Batch number = 42
Evaluating:  20%|██        | 42/205 [00:23<01:17,  2.10it/s]11/28/2021 01:40:24 - INFO - __main__ -   Batch number = 43
Evaluating:  21%|██        | 43/205 [00:24<01:17,  2.08it/s]11/28/2021 01:40:25 - INFO - __main__ -   Batch number = 44
Evaluating:  21%|██▏       | 44/205 [00:24<01:17,  2.07it/s]11/28/2021 01:40:25 - INFO - __main__ -   Batch number = 45
Evaluating:  22%|██▏       | 45/205 [00:25<01:18,  2.05it/s]11/28/2021 01:40:26 - INFO - __main__ -   Batch number = 46
Evaluating:  22%|██▏       | 46/205 [00:25<01:14,  2.12it/s]11/28/2021 01:40:26 - INFO - __main__ -   Batch number = 47
Evaluating:  23%|██▎       | 47/205 [00:26<01:15,  2.09it/s]11/28/2021 01:40:26 - INFO - __main__ -   Batch number = 48
Evaluating:  23%|██▎       | 48/205 [00:26<01:16,  2.06it/s]11/28/2021 01:40:27 - INFO - __main__ -   Batch number = 49
Evaluating:  24%|██▍       | 49/205 [00:27<01:15,  2.06it/s]11/28/2021 01:40:27 - INFO - __main__ -   Batch number = 50
Evaluating:  24%|██▍       | 50/205 [00:27<01:15,  2.04it/s]11/28/2021 01:40:28 - INFO - __main__ -   Batch number = 51
Evaluating:  25%|██▍       | 51/205 [00:28<01:11,  2.17it/s]11/28/2021 01:40:28 - INFO - __main__ -   Batch number = 52
Evaluating:  25%|██▌       | 52/205 [00:28<01:12,  2.12it/s]11/28/2021 01:40:29 - INFO - __main__ -   Batch number = 53
Evaluating:  26%|██▌       | 53/205 [00:29<01:12,  2.11it/s]11/28/2021 01:40:29 - INFO - __main__ -   Batch number = 54
Evaluating:  26%|██▋       | 54/205 [00:29<01:12,  2.08it/s]11/28/2021 01:40:30 - INFO - __main__ -   Batch number = 55
Evaluating:  27%|██▋       | 55/205 [00:30<01:15,  1.99it/s]11/28/2021 01:40:30 - INFO - __main__ -   Batch number = 56
Evaluating:  27%|██▋       | 56/205 [00:30<01:07,  2.22it/s]11/28/2021 01:40:31 - INFO - __main__ -   Batch number = 57
Evaluating:  28%|██▊       | 57/205 [00:30<01:00,  2.44it/s]11/28/2021 01:40:31 - INFO - __main__ -   Batch number = 58
Evaluating:  28%|██▊       | 58/205 [00:31<00:56,  2.62it/s]11/28/2021 01:40:31 - INFO - __main__ -   Batch number = 59
Evaluating:  29%|██▉       | 59/205 [00:31<00:53,  2.75it/s]11/28/2021 01:40:32 - INFO - __main__ -   Batch number = 60
Evaluating:  29%|██▉       | 60/205 [00:31<00:51,  2.84it/s]11/28/2021 01:40:32 - INFO - __main__ -   Batch number = 61
Evaluating:  30%|██▉       | 61/205 [00:32<00:49,  2.90it/s]11/28/2021 01:40:32 - INFO - __main__ -   Batch number = 62
Evaluating:  30%|███       | 62/205 [00:32<00:49,  2.91it/s]11/28/2021 01:40:33 - INFO - __main__ -   Batch number = 63
Evaluating:  31%|███       | 63/205 [00:32<00:48,  2.93it/s]11/28/2021 01:40:33 - INFO - __main__ -   Batch number = 64
Evaluating:  31%|███       | 64/205 [00:33<00:47,  2.95it/s]11/28/2021 01:40:33 - INFO - __main__ -   Batch number = 65
Evaluating:  32%|███▏      | 65/205 [00:33<00:47,  2.95it/s]11/28/2021 01:40:34 - INFO - __main__ -   Batch number = 66
Evaluating:  32%|███▏      | 66/205 [00:33<00:47,  2.95it/s]11/28/2021 01:40:34 - INFO - __main__ -   Batch number = 67
Evaluating:  33%|███▎      | 67/205 [00:34<00:46,  2.95it/s]11/28/2021 01:40:34 - INFO - __main__ -   Batch number = 68
Evaluating:  33%|███▎      | 68/205 [00:34<00:46,  2.95it/s]11/28/2021 01:40:35 - INFO - __main__ -   Batch number = 69
Evaluating:  34%|███▎      | 69/205 [00:34<00:46,  2.95it/s]11/28/2021 01:40:35 - INFO - __main__ -   Batch number = 70
Evaluating:  34%|███▍      | 70/205 [00:35<00:45,  2.95it/s]11/28/2021 01:40:35 - INFO - __main__ -   Batch number = 71
Evaluating:  35%|███▍      | 71/205 [00:35<00:45,  2.95it/s]11/28/2021 01:40:36 - INFO - __main__ -   Batch number = 72
Evaluating:  35%|███▌      | 72/205 [00:35<00:44,  2.96it/s]11/28/2021 01:40:36 - INFO - __main__ -   Batch number = 73
Evaluating:  36%|███▌      | 73/205 [00:36<00:44,  2.94it/s]11/28/2021 01:40:36 - INFO - __main__ -   Batch number = 74
Evaluating:  36%|███▌      | 74/205 [00:36<00:44,  2.94it/s]11/28/2021 01:40:37 - INFO - __main__ -   Batch number = 75
Evaluating:  37%|███▋      | 75/205 [00:36<00:44,  2.95it/s]11/28/2021 01:40:37 - INFO - __main__ -   Batch number = 76
Evaluating:  37%|███▋      | 76/205 [00:37<00:43,  2.96it/s]11/28/2021 01:40:37 - INFO - __main__ -   Batch number = 77
Evaluating:  38%|███▊      | 77/205 [00:37<00:43,  2.96it/s]11/28/2021 01:40:38 - INFO - __main__ -   Batch number = 78
Evaluating:  38%|███▊      | 78/205 [00:37<00:43,  2.93it/s]11/28/2021 01:40:38 - INFO - __main__ -   Batch number = 79
Evaluating:  39%|███▊      | 79/205 [00:38<00:42,  2.94it/s]11/28/2021 01:40:38 - INFO - __main__ -   Batch number = 80
Evaluating:  39%|███▉      | 80/205 [00:38<00:40,  3.06it/s]11/28/2021 01:40:39 - INFO - __main__ -   Batch number = 81
Evaluating:  40%|███▉      | 81/205 [00:38<00:34,  3.54it/s]11/28/2021 01:40:39 - INFO - __main__ -   Batch number = 82
Evaluating:  40%|████      | 82/205 [00:38<00:30,  3.99it/s]11/28/2021 01:40:39 - INFO - __main__ -   Batch number = 83
Evaluating:  40%|████      | 83/205 [00:38<00:27,  4.44it/s]11/28/2021 01:40:39 - INFO - __main__ -   Batch number = 84
Evaluating:  41%|████      | 84/205 [00:39<00:25,  4.84it/s]11/28/2021 01:40:39 - INFO - __main__ -   Batch number = 85
Evaluating:  41%|████▏     | 85/205 [00:39<00:23,  5.17it/s]11/28/2021 01:40:40 - INFO - __main__ -   Batch number = 86
Evaluating:  42%|████▏     | 86/205 [00:39<00:24,  4.77it/s]11/28/2021 01:40:40 - INFO - __main__ -   Batch number = 87
Evaluating:  42%|████▏     | 87/205 [00:39<00:30,  3.92it/s]11/28/2021 01:40:40 - INFO - __main__ -   Batch number = 88
Evaluating:  43%|████▎     | 88/205 [00:40<00:32,  3.58it/s]11/28/2021 01:40:41 - INFO - __main__ -   Batch number = 89
Evaluating:  43%|████▎     | 89/205 [00:40<00:34,  3.33it/s]11/28/2021 01:40:41 - INFO - __main__ -   Batch number = 90
Evaluating:  44%|████▍     | 90/205 [00:40<00:35,  3.21it/s]11/28/2021 01:40:41 - INFO - __main__ -   Batch number = 91
Evaluating:  44%|████▍     | 91/205 [00:41<00:36,  3.13it/s]11/28/2021 01:40:42 - INFO - __main__ -   Batch number = 92
Evaluating:  45%|████▍     | 92/205 [00:41<00:36,  3.06it/s]11/28/2021 01:40:42 - INFO - __main__ -   Batch number = 93
Evaluating:  45%|████▌     | 93/205 [00:41<00:36,  3.04it/s]11/28/2021 01:40:42 - INFO - __main__ -   Batch number = 94
Evaluating:  46%|████▌     | 94/205 [00:42<00:36,  3.01it/s]11/28/2021 01:40:43 - INFO - __main__ -   Batch number = 95
Evaluating:  46%|████▋     | 95/205 [00:42<00:36,  2.98it/s]11/28/2021 01:40:43 - INFO - __main__ -   Batch number = 96
Evaluating:  47%|████▋     | 96/205 [00:42<00:36,  2.97it/s]11/28/2021 01:40:43 - INFO - __main__ -   Batch number = 97
Evaluating:  47%|████▋     | 97/205 [00:43<00:36,  2.96it/s]11/28/2021 01:40:44 - INFO - __main__ -   Batch number = 98
Evaluating:  48%|████▊     | 98/205 [00:43<00:36,  2.96it/s]11/28/2021 01:40:44 - INFO - __main__ -   Batch number = 99
Evaluating:  48%|████▊     | 99/205 [00:44<00:35,  2.96it/s]11/28/2021 01:40:44 - INFO - __main__ -   Batch number = 100
Evaluating:  49%|████▉     | 100/205 [00:44<00:35,  2.95it/s]11/28/2021 01:40:45 - INFO - __main__ -   Batch number = 101
Evaluating:  49%|████▉     | 101/205 [00:44<00:35,  2.95it/s]11/28/2021 01:40:45 - INFO - __main__ -   Batch number = 102
Evaluating:  50%|████▉     | 102/205 [00:45<00:35,  2.94it/s]11/28/2021 01:40:45 - INFO - __main__ -   Batch number = 103
Evaluating:  50%|█████     | 103/205 [00:45<00:34,  2.94it/s]11/28/2021 01:40:46 - INFO - __main__ -   Batch number = 104
Evaluating:  51%|█████     | 104/205 [00:45<00:34,  2.92it/s]11/28/2021 01:40:46 - INFO - __main__ -   Batch number = 105
Evaluating:  51%|█████     | 105/205 [00:46<00:34,  2.92it/s]11/28/2021 01:40:46 - INFO - __main__ -   Batch number = 106
Evaluating:  52%|█████▏    | 106/205 [00:46<00:33,  2.93it/s]11/28/2021 01:40:47 - INFO - __main__ -   Batch number = 107
Evaluating:  52%|█████▏    | 107/205 [00:46<00:33,  2.92it/s]11/28/2021 01:40:47 - INFO - __main__ -   Batch number = 108
Evaluating:  53%|█████▎    | 108/205 [00:47<00:33,  2.93it/s]11/28/2021 01:40:47 - INFO - __main__ -   Batch number = 109
Evaluating:  53%|█████▎    | 109/205 [00:47<00:32,  2.92it/s]11/28/2021 01:40:48 - INFO - __main__ -   Batch number = 110
Evaluating:  54%|█████▎    | 110/205 [00:47<00:32,  2.93it/s]11/28/2021 01:40:48 - INFO - __main__ -   Batch number = 111
Evaluating:  54%|█████▍    | 111/205 [00:48<00:32,  2.92it/s]11/28/2021 01:40:48 - INFO - __main__ -   Batch number = 112
Evaluating:  55%|█████▍    | 112/205 [00:48<00:31,  2.91it/s]11/28/2021 01:40:49 - INFO - __main__ -   Batch number = 113
Evaluating:  55%|█████▌    | 113/205 [00:48<00:31,  2.91it/s]11/28/2021 01:40:49 - INFO - __main__ -   Batch number = 114
Evaluating:  56%|█████▌    | 114/205 [00:49<00:31,  2.91it/s]11/28/2021 01:40:49 - INFO - __main__ -   Batch number = 115
Evaluating:  56%|█████▌    | 115/205 [00:49<00:30,  2.91it/s]11/28/2021 01:40:50 - INFO - __main__ -   Batch number = 116
Evaluating:  57%|█████▋    | 116/205 [00:49<00:33,  2.70it/s]11/28/2021 01:40:50 - INFO - __main__ -   Batch number = 117
Evaluating:  57%|█████▋    | 117/205 [00:50<00:31,  2.80it/s]11/28/2021 01:40:50 - INFO - __main__ -   Batch number = 118
Evaluating:  58%|█████▊    | 118/205 [00:50<00:30,  2.89it/s]11/28/2021 01:40:51 - INFO - __main__ -   Batch number = 119
Evaluating:  58%|█████▊    | 119/205 [00:50<00:29,  2.94it/s]11/28/2021 01:40:51 - INFO - __main__ -   Batch number = 120
Evaluating:  59%|█████▊    | 120/205 [00:51<00:28,  2.97it/s]11/28/2021 01:40:51 - INFO - __main__ -   Batch number = 121
Evaluating:  59%|█████▉    | 121/205 [00:51<00:28,  2.97it/s]11/28/2021 01:40:52 - INFO - __main__ -   Batch number = 122
Evaluating:  60%|█████▉    | 122/205 [00:51<00:27,  3.01it/s]11/28/2021 01:40:52 - INFO - __main__ -   Batch number = 123
Evaluating:  60%|██████    | 123/205 [00:52<00:27,  2.97it/s]11/28/2021 01:40:52 - INFO - __main__ -   Batch number = 124
Evaluating:  60%|██████    | 124/205 [00:52<00:27,  2.97it/s]11/28/2021 01:40:53 - INFO - __main__ -   Batch number = 125
Evaluating:  61%|██████    | 125/205 [00:52<00:27,  2.90it/s]11/28/2021 01:40:53 - INFO - __main__ -   Batch number = 126
Evaluating:  61%|██████▏   | 126/205 [00:53<00:38,  2.06it/s]11/28/2021 01:40:54 - INFO - __main__ -   Batch number = 127
Evaluating:  62%|██████▏   | 127/205 [00:54<00:34,  2.25it/s]11/28/2021 01:40:54 - INFO - __main__ -   Batch number = 128
Evaluating:  62%|██████▏   | 128/205 [00:54<00:32,  2.40it/s]11/28/2021 01:40:55 - INFO - __main__ -   Batch number = 129
Evaluating:  63%|██████▎   | 129/205 [00:55<00:45,  1.67it/s]11/28/2021 01:40:56 - INFO - __main__ -   Batch number = 130
Evaluating:  63%|██████▎   | 130/205 [00:55<00:39,  1.91it/s]11/28/2021 01:40:56 - INFO - __main__ -   Batch number = 131
Evaluating:  64%|██████▍   | 131/205 [00:56<00:34,  2.12it/s]11/28/2021 01:40:56 - INFO - __main__ -   Batch number = 132
Evaluating:  64%|██████▍   | 132/205 [00:56<00:39,  1.84it/s]11/28/2021 01:40:57 - INFO - __main__ -   Batch number = 133
Evaluating:  65%|██████▍   | 133/205 [00:57<00:31,  2.31it/s]11/28/2021 01:40:57 - INFO - __main__ -   Batch number = 134
Evaluating:  65%|██████▌   | 134/205 [00:57<00:25,  2.80it/s]11/28/2021 01:40:57 - INFO - __main__ -   Batch number = 135
Evaluating:  66%|██████▌   | 135/205 [00:57<00:21,  3.21it/s]11/28/2021 01:40:58 - INFO - __main__ -   Batch number = 136
Evaluating:  66%|██████▋   | 136/205 [00:57<00:18,  3.72it/s]11/28/2021 01:40:58 - INFO - __main__ -   Batch number = 137
Evaluating:  67%|██████▋   | 137/205 [00:57<00:16,  4.03it/s]11/28/2021 01:40:58 - INFO - __main__ -   Batch number = 138
Evaluating:  67%|██████▋   | 138/205 [00:58<00:32,  2.06it/s]11/28/2021 01:40:59 - INFO - __main__ -   Batch number = 139
Evaluating:  68%|██████▊   | 139/205 [00:59<00:29,  2.26it/s]11/28/2021 01:40:59 - INFO - __main__ -   Batch number = 140
Evaluating:  68%|██████▊   | 140/205 [00:59<00:27,  2.41it/s]11/28/2021 01:41:00 - INFO - __main__ -   Batch number = 141
Evaluating:  69%|██████▉   | 141/205 [01:00<00:29,  2.20it/s]11/28/2021 01:41:00 - INFO - __main__ -   Batch number = 142
Evaluating:  69%|██████▉   | 142/205 [01:00<00:26,  2.37it/s]11/28/2021 01:41:01 - INFO - __main__ -   Batch number = 143
Evaluating:  70%|██████▉   | 143/205 [01:00<00:24,  2.49it/s]11/28/2021 01:41:01 - INFO - __main__ -   Batch number = 144
Evaluating:  70%|███████   | 144/205 [01:01<00:23,  2.59it/s]11/28/2021 01:41:01 - INFO - __main__ -   Batch number = 145
Evaluating:  71%|███████   | 145/205 [01:01<00:22,  2.67it/s]11/28/2021 01:41:02 - INFO - __main__ -   Batch number = 146
Evaluating:  71%|███████   | 146/205 [01:02<00:35,  1.68it/s]11/28/2021 01:41:03 - INFO - __main__ -   Batch number = 147
Evaluating:  72%|███████▏  | 147/205 [01:02<00:30,  1.92it/s]11/28/2021 01:41:03 - INFO - __main__ -   Batch number = 148
Evaluating:  72%|███████▏  | 148/205 [01:03<00:27,  2.06it/s]11/28/2021 01:41:04 - INFO - __main__ -   Batch number = 149
Evaluating:  73%|███████▎  | 149/205 [01:03<00:27,  2.02it/s]11/28/2021 01:41:04 - INFO - __main__ -   Batch number = 150
Evaluating:  73%|███████▎  | 150/205 [01:04<00:27,  1.99it/s]11/28/2021 01:41:05 - INFO - __main__ -   Batch number = 151
Evaluating:  74%|███████▎  | 151/205 [01:04<00:27,  1.96it/s]11/28/2021 01:41:05 - INFO - __main__ -   Batch number = 152
Evaluating:  74%|███████▍  | 152/205 [01:05<00:27,  1.95it/s]11/28/2021 01:41:06 - INFO - __main__ -   Batch number = 153
Evaluating:  75%|███████▍  | 153/205 [01:05<00:26,  1.95it/s]11/28/2021 01:41:06 - INFO - __main__ -   Batch number = 154
Evaluating:  75%|███████▌  | 154/205 [01:06<00:26,  1.95it/s]11/28/2021 01:41:07 - INFO - __main__ -   Batch number = 155
Evaluating:  76%|███████▌  | 155/205 [01:06<00:25,  1.95it/s]11/28/2021 01:41:07 - INFO - __main__ -   Batch number = 156
Evaluating:  76%|███████▌  | 156/205 [01:07<00:25,  1.95it/s]11/28/2021 01:41:08 - INFO - __main__ -   Batch number = 157
Evaluating:  77%|███████▋  | 157/205 [01:07<00:24,  1.95it/s]11/28/2021 01:41:08 - INFO - __main__ -   Batch number = 158
Evaluating:  77%|███████▋  | 158/205 [01:08<00:24,  1.96it/s]11/28/2021 01:41:09 - INFO - __main__ -   Batch number = 159
Evaluating:  78%|███████▊  | 159/205 [01:09<00:23,  1.96it/s]11/28/2021 01:41:09 - INFO - __main__ -   Batch number = 160
Evaluating:  78%|███████▊  | 160/205 [01:09<00:23,  1.96it/s]11/28/2021 01:41:10 - INFO - __main__ -   Batch number = 161
Evaluating:  79%|███████▊  | 161/205 [01:10<00:23,  1.86it/s]11/28/2021 01:41:10 - INFO - __main__ -   Batch number = 162
Evaluating:  79%|███████▉  | 162/205 [01:10<00:22,  1.89it/s]11/28/2021 01:41:11 - INFO - __main__ -   Batch number = 163
Evaluating:  80%|███████▉  | 163/205 [01:11<00:21,  1.91it/s]11/28/2021 01:41:11 - INFO - __main__ -   Batch number = 164
Evaluating:  80%|████████  | 164/205 [01:11<00:21,  1.92it/s]11/28/2021 01:41:12 - INFO - __main__ -   Batch number = 165
Evaluating:  80%|████████  | 165/205 [01:12<00:20,  1.94it/s]11/28/2021 01:41:12 - INFO - __main__ -   Batch number = 166
Evaluating:  81%|████████  | 166/205 [01:12<00:20,  1.94it/s]11/28/2021 01:41:13 - INFO - __main__ -   Batch number = 167
Evaluating:  81%|████████▏ | 167/205 [01:13<00:19,  1.94it/s]11/28/2021 01:41:13 - INFO - __main__ -   Batch number = 168
Evaluating:  82%|████████▏ | 168/205 [01:13<00:19,  1.94it/s]11/28/2021 01:41:14 - INFO - __main__ -   Batch number = 169
Evaluating:  82%|████████▏ | 169/205 [01:14<00:22,  1.58it/s]11/28/2021 01:41:15 - INFO - __main__ -   Batch number = 170
Evaluating:  83%|████████▎ | 170/205 [01:15<00:21,  1.60it/s]11/28/2021 01:41:15 - INFO - __main__ -   Batch number = 171
Evaluating:  83%|████████▎ | 171/205 [01:15<00:20,  1.69it/s]11/28/2021 01:41:16 - INFO - __main__ -   Batch number = 172
Evaluating:  84%|████████▍ | 172/205 [01:16<00:18,  1.75it/s]11/28/2021 01:41:16 - INFO - __main__ -   Batch number = 173
Evaluating:  84%|████████▍ | 173/205 [01:16<00:17,  1.80it/s]11/28/2021 01:41:17 - INFO - __main__ -   Batch number = 174
Evaluating:  85%|████████▍ | 174/205 [01:17<00:16,  1.83it/s]11/28/2021 01:41:18 - INFO - __main__ -   Batch number = 175
Evaluating:  85%|████████▌ | 175/205 [01:17<00:16,  1.84it/s]11/28/2021 01:41:18 - INFO - __main__ -   Batch number = 176
Evaluating:  86%|████████▌ | 176/205 [01:18<00:15,  1.87it/s]11/28/2021 01:41:19 - INFO - __main__ -   Batch number = 177
Evaluating:  86%|████████▋ | 177/205 [01:19<00:16,  1.65it/s]11/28/2021 01:41:19 - INFO - __main__ -   Batch number = 178
Evaluating:  87%|████████▋ | 178/205 [01:19<00:15,  1.73it/s]11/28/2021 01:41:20 - INFO - __main__ -   Batch number = 179
Evaluating:  87%|████████▋ | 179/205 [01:20<00:16,  1.58it/s]11/28/2021 01:41:21 - INFO - __main__ -   Batch number = 180
Evaluating:  88%|████████▊ | 180/205 [01:20<00:14,  1.67it/s]11/28/2021 01:41:21 - INFO - __main__ -   Batch number = 181
Evaluating:  88%|████████▊ | 181/205 [01:21<00:16,  1.49it/s]11/28/2021 01:41:22 - INFO - __main__ -   Batch number = 182
Evaluating:  89%|████████▉ | 182/205 [01:22<00:14,  1.60it/s]11/28/2021 01:41:23 - INFO - __main__ -   Batch number = 183
Evaluating:  89%|████████▉ | 183/205 [01:23<00:14,  1.51it/s]11/28/2021 01:41:23 - INFO - __main__ -   Batch number = 184
Evaluating:  90%|████████▉ | 184/205 [01:23<00:13,  1.50it/s]11/28/2021 01:41:24 - INFO - __main__ -   Batch number = 185
Evaluating:  90%|█████████ | 185/205 [01:24<00:13,  1.49it/s]11/28/2021 01:41:25 - INFO - __main__ -   Batch number = 186
Evaluating:  91%|█████████ | 186/205 [01:25<00:12,  1.48it/s]11/28/2021 01:41:25 - INFO - __main__ -   Batch number = 187
Evaluating:  91%|█████████ | 187/205 [01:25<00:12,  1.45it/s]11/28/2021 01:41:26 - INFO - __main__ -   Batch number = 188
Evaluating:  92%|█████████▏| 188/205 [01:26<00:11,  1.45it/s]11/28/2021 01:41:27 - INFO - __main__ -   Batch number = 189
Evaluating:  92%|█████████▏| 189/205 [01:27<00:11,  1.45it/s]11/28/2021 01:41:27 - INFO - __main__ -   Batch number = 190
Evaluating:  93%|█████████▎| 190/205 [01:27<00:10,  1.46it/s]11/28/2021 01:41:28 - INFO - __main__ -   Batch number = 191
Evaluating:  93%|█████████▎| 191/205 [01:28<00:09,  1.45it/s]11/28/2021 01:41:29 - INFO - __main__ -   Batch number = 192
Evaluating:  94%|█████████▎| 192/205 [01:29<00:08,  1.45it/s]11/28/2021 01:41:29 - INFO - __main__ -   Batch number = 193
Evaluating:  94%|█████████▍| 193/205 [01:29<00:08,  1.45it/s]11/28/2021 01:41:30 - INFO - __main__ -   Batch number = 194
Evaluating:  95%|█████████▍| 194/205 [01:30<00:07,  1.46it/s]11/28/2021 01:41:31 - INFO - __main__ -   Batch number = 195
Evaluating:  95%|█████████▌| 195/205 [01:31<00:06,  1.46it/s]11/28/2021 01:41:32 - INFO - __main__ -   Batch number = 196
Evaluating:  96%|█████████▌| 196/205 [01:31<00:06,  1.46it/s]11/28/2021 01:41:32 - INFO - __main__ -   Batch number = 197
Evaluating:  96%|█████████▌| 197/205 [01:32<00:05,  1.46it/s]11/28/2021 01:41:33 - INFO - __main__ -   Batch number = 198
Evaluating:  97%|█████████▋| 198/205 [01:33<00:04,  1.46it/s]11/28/2021 01:41:34 - INFO - __main__ -   Batch number = 199
Evaluating:  97%|█████████▋| 199/205 [01:34<00:04,  1.46it/s]11/28/2021 01:41:34 - INFO - __main__ -   Batch number = 200
Evaluating:  98%|█████████▊| 200/205 [01:34<00:03,  1.46it/s]11/28/2021 01:41:35 - INFO - __main__ -   Batch number = 201
Evaluating:  98%|█████████▊| 201/205 [01:35<00:02,  1.46it/s]11/28/2021 01:41:36 - INFO - __main__ -   Batch number = 202
Evaluating:  99%|█████████▊| 202/205 [01:36<00:02,  1.46it/s]11/28/2021 01:41:36 - INFO - __main__ -   Batch number = 203
Evaluating:  99%|█████████▉| 203/205 [01:36<00:01,  1.46it/s]11/28/2021 01:41:37 - INFO - __main__ -   Batch number = 204
Evaluating: 100%|█████████▉| 204/205 [01:37<00:00,  1.46it/s]11/28/2021 01:41:38 - INFO - __main__ -   Batch number = 205
Evaluating: 100%|██████████| 205/205 [01:38<00:00,  1.54it/s]Evaluating: 100%|██████████| 205/205 [01:38<00:00,  2.09it/s]
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:41:40 - INFO - __main__ -   ***** Evaluation result  in fi *****
11/28/2021 01:41:40 - INFO - __main__ -     f1 = 0.7219417809020761
11/28/2021 01:41:40 - INFO - __main__ -     loss = 0.9519045682941994
11/28/2021 01:41:40 - INFO - __main__ -     precision = 0.7343692957254647
11/28/2021 01:41:40 - INFO - __main__ -     recall = 0.7099278809268068
89.19user 36.56system 2:05.01elapsed 100%CPU (0avgtext+0avgdata 3991636maxresident)k
0inputs+928outputs (0major+1983611minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 16:23:44 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='zh', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 16:23:45 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 16:23:45 - INFO - __main__ -   Seed = 1
11/28/2021 16:23:45 - INFO - root -   save model
11/28/2021 16:23:45 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='zh', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 16:23:45 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 16:23:47 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 16:23:53 - INFO - __main__ -   Using lang2id = None
11/28/2021 16:23:53 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 16:23:53 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
11/28/2021 16:23:53 - INFO - root -   Trying to decide if add adapter
11/28/2021 16:23:53 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 16:23:53 - INFO - root -   loading lang adpater bh/wiki@ukp
11/28/2021 16:23:53 - INFO - __main__ -   Adapter Languages : ['bh'], Length : 1
11/28/2021 16:23:53 - INFO - __main__ -   Adapter Names ['bh/wiki@ukp'], Length : 1
11/28/2021 16:23:53 - INFO - __main__ -   Language = bh
11/28/2021 16:23:53 - INFO - __main__ -   Adapter Name = bh/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/bh/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_bh_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/adapter_config.json
Adding adapter 'bh' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/head_config.json
11/28/2021 16:24:01 - INFO - __main__ -   Language adapter for zh not found, using bh instead
11/28/2021 16:24:01 - INFO - __main__ -   Set active language adapter to bh
11/28/2021 16:24:01 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 16:24:01 - INFO - __main__ -   Adapter Languages = ['bh']
11/28/2021 16:24:01 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_zh_bert-base-multilingual-cased_128
11/28/2021 16:24:02 - INFO - __main__ -   ***** Running evaluation  in zh *****
11/28/2021 16:24:02 - INFO - __main__ -     Num examples = 3458
11/28/2021 16:24:02 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/109 [00:00<?, ?it/s]11/28/2021 16:24:02 - INFO - __main__ -   Batch number = 1
Evaluating:   1%|          | 1/109 [00:00<00:52,  2.07it/s]11/28/2021 16:24:03 - INFO - __main__ -   Batch number = 2
Evaluating:   2%|▏         | 2/109 [00:00<00:51,  2.06it/s]11/28/2021 16:24:03 - INFO - __main__ -   Batch number = 3
Evaluating:   3%|▎         | 3/109 [00:01<00:51,  2.05it/s]11/28/2021 16:24:04 - INFO - __main__ -   Batch number = 4
Evaluating:   4%|▎         | 4/109 [00:01<00:51,  2.06it/s]11/28/2021 16:24:04 - INFO - __main__ -   Batch number = 5
Evaluating:   5%|▍         | 5/109 [00:02<00:50,  2.04it/s]11/28/2021 16:24:04 - INFO - __main__ -   Batch number = 6
Evaluating:   6%|▌         | 6/109 [00:02<00:50,  2.04it/s]11/28/2021 16:24:05 - INFO - __main__ -   Batch number = 7
Evaluating:   6%|▋         | 7/109 [00:03<00:49,  2.04it/s]11/28/2021 16:24:05 - INFO - __main__ -   Batch number = 8
Evaluating:   7%|▋         | 8/109 [00:03<00:49,  2.04it/s]11/28/2021 16:24:06 - INFO - __main__ -   Batch number = 9
Evaluating:   8%|▊         | 9/109 [00:04<00:49,  2.04it/s]11/28/2021 16:24:06 - INFO - __main__ -   Batch number = 10
Evaluating:   9%|▉         | 10/109 [00:04<00:48,  2.03it/s]11/28/2021 16:24:07 - INFO - __main__ -   Batch number = 11
Evaluating:  10%|█         | 11/109 [00:05<00:48,  2.03it/s]11/28/2021 16:24:07 - INFO - __main__ -   Batch number = 12
Evaluating:  11%|█         | 12/109 [00:05<00:47,  2.03it/s]11/28/2021 16:24:08 - INFO - __main__ -   Batch number = 13
Evaluating:  12%|█▏        | 13/109 [00:06<00:45,  2.12it/s]11/28/2021 16:24:08 - INFO - __main__ -   Batch number = 14
Evaluating:  13%|█▎        | 14/109 [00:06<00:45,  2.09it/s]11/28/2021 16:24:09 - INFO - __main__ -   Batch number = 15
Evaluating:  14%|█▍        | 15/109 [00:07<00:45,  2.08it/s]11/28/2021 16:24:09 - INFO - __main__ -   Batch number = 16
Evaluating:  15%|█▍        | 16/109 [00:07<00:44,  2.07it/s]11/28/2021 16:24:10 - INFO - __main__ -   Batch number = 17
Evaluating:  16%|█▌        | 17/109 [00:08<00:44,  2.06it/s]11/28/2021 16:24:10 - INFO - __main__ -   Batch number = 18
Evaluating:  17%|█▋        | 18/109 [00:08<00:44,  2.05it/s]11/28/2021 16:24:11 - INFO - __main__ -   Batch number = 19
Evaluating:  17%|█▋        | 19/109 [00:09<00:44,  2.04it/s]11/28/2021 16:24:11 - INFO - __main__ -   Batch number = 20
Evaluating:  18%|█▊        | 20/109 [00:09<00:43,  2.04it/s]11/28/2021 16:24:12 - INFO - __main__ -   Batch number = 21
Evaluating:  19%|█▉        | 21/109 [00:10<00:43,  2.04it/s]11/28/2021 16:24:12 - INFO - __main__ -   Batch number = 22
Evaluating:  20%|██        | 22/109 [00:10<00:42,  2.05it/s]11/28/2021 16:24:13 - INFO - __main__ -   Batch number = 23
Evaluating:  21%|██        | 23/109 [00:11<00:41,  2.07it/s]11/28/2021 16:24:13 - INFO - __main__ -   Batch number = 24
Evaluating:  22%|██▏       | 24/109 [00:11<00:46,  1.83it/s]11/28/2021 16:24:14 - INFO - __main__ -   Batch number = 25
Evaluating:  23%|██▎       | 25/109 [00:12<00:43,  1.93it/s]11/28/2021 16:24:14 - INFO - __main__ -   Batch number = 26
Evaluating:  24%|██▍       | 26/109 [00:12<00:41,  1.98it/s]11/28/2021 16:24:15 - INFO - __main__ -   Batch number = 27
Evaluating:  25%|██▍       | 27/109 [00:13<00:40,  2.01it/s]11/28/2021 16:24:15 - INFO - __main__ -   Batch number = 28
Evaluating:  26%|██▌       | 28/109 [00:13<00:40,  2.02it/s]11/28/2021 16:24:16 - INFO - __main__ -   Batch number = 29
Evaluating:  27%|██▋       | 29/109 [00:14<00:39,  2.03it/s]11/28/2021 16:24:16 - INFO - __main__ -   Batch number = 30
Evaluating:  28%|██▊       | 30/109 [00:14<00:38,  2.04it/s]11/28/2021 16:24:17 - INFO - __main__ -   Batch number = 31
Evaluating:  28%|██▊       | 31/109 [00:15<00:38,  2.03it/s]11/28/2021 16:24:17 - INFO - __main__ -   Batch number = 32
Evaluating:  29%|██▉       | 32/109 [00:15<00:37,  2.03it/s]11/28/2021 16:24:18 - INFO - __main__ -   Batch number = 33
Evaluating:  30%|███       | 33/109 [00:16<00:37,  2.04it/s]11/28/2021 16:24:18 - INFO - __main__ -   Batch number = 34
Evaluating:  31%|███       | 34/109 [00:16<00:36,  2.04it/s]11/28/2021 16:24:19 - INFO - __main__ -   Batch number = 35
Evaluating:  32%|███▏      | 35/109 [00:17<00:36,  2.04it/s]11/28/2021 16:24:19 - INFO - __main__ -   Batch number = 36
Evaluating:  33%|███▎      | 36/109 [00:17<00:35,  2.04it/s]11/28/2021 16:24:20 - INFO - __main__ -   Batch number = 37
Evaluating:  34%|███▍      | 37/109 [00:18<00:35,  2.03it/s]11/28/2021 16:24:20 - INFO - __main__ -   Batch number = 38
Evaluating:  35%|███▍      | 38/109 [00:18<00:34,  2.04it/s]11/28/2021 16:24:21 - INFO - __main__ -   Batch number = 39
Evaluating:  36%|███▌      | 39/109 [00:19<00:34,  2.05it/s]11/28/2021 16:24:21 - INFO - __main__ -   Batch number = 40
Evaluating:  37%|███▋      | 40/109 [00:19<00:33,  2.05it/s]11/28/2021 16:24:22 - INFO - __main__ -   Batch number = 41
Evaluating:  38%|███▊      | 41/109 [00:20<00:33,  2.05it/s]11/28/2021 16:24:22 - INFO - __main__ -   Batch number = 42
Evaluating:  39%|███▊      | 42/109 [00:20<00:32,  2.05it/s]11/28/2021 16:24:23 - INFO - __main__ -   Batch number = 43
Evaluating:  39%|███▉      | 43/109 [00:21<00:32,  2.04it/s]11/28/2021 16:24:23 - INFO - __main__ -   Batch number = 44
Evaluating:  40%|████      | 44/109 [00:21<00:31,  2.04it/s]11/28/2021 16:24:24 - INFO - __main__ -   Batch number = 45
Evaluating:  41%|████▏     | 45/109 [00:22<00:31,  2.05it/s]11/28/2021 16:24:24 - INFO - __main__ -   Batch number = 46
Evaluating:  42%|████▏     | 46/109 [00:22<00:30,  2.04it/s]11/28/2021 16:24:25 - INFO - __main__ -   Batch number = 47
Evaluating:  43%|████▎     | 47/109 [00:23<00:30,  2.03it/s]11/28/2021 16:24:25 - INFO - __main__ -   Batch number = 48
Evaluating:  44%|████▍     | 48/109 [00:23<00:30,  1.99it/s]11/28/2021 16:24:26 - INFO - __main__ -   Batch number = 49
Evaluating:  45%|████▍     | 49/109 [00:24<00:29,  2.03it/s]11/28/2021 16:24:26 - INFO - __main__ -   Batch number = 50
Evaluating:  46%|████▌     | 50/109 [00:24<00:29,  2.03it/s]11/28/2021 16:24:27 - INFO - __main__ -   Batch number = 51
Evaluating:  47%|████▋     | 51/109 [00:25<00:28,  2.04it/s]11/28/2021 16:24:27 - INFO - __main__ -   Batch number = 52
Evaluating:  48%|████▊     | 52/109 [00:25<00:27,  2.05it/s]11/28/2021 16:24:28 - INFO - __main__ -   Batch number = 53
Evaluating:  49%|████▊     | 53/109 [00:26<00:27,  2.05it/s]11/28/2021 16:24:28 - INFO - __main__ -   Batch number = 54
Evaluating:  50%|████▉     | 54/109 [00:26<00:26,  2.05it/s]11/28/2021 16:24:29 - INFO - __main__ -   Batch number = 55
Evaluating:  50%|█████     | 55/109 [00:27<00:26,  2.05it/s]11/28/2021 16:24:29 - INFO - __main__ -   Batch number = 56
Evaluating:  51%|█████▏    | 56/109 [00:27<00:25,  2.05it/s]11/28/2021 16:24:30 - INFO - __main__ -   Batch number = 57
Evaluating:  52%|█████▏    | 57/109 [00:27<00:25,  2.05it/s]11/28/2021 16:24:30 - INFO - __main__ -   Batch number = 58
Evaluating:  53%|█████▎    | 58/109 [00:28<00:24,  2.04it/s]11/28/2021 16:24:31 - INFO - __main__ -   Batch number = 59
Evaluating:  54%|█████▍    | 59/109 [00:28<00:24,  2.04it/s]11/28/2021 16:24:31 - INFO - __main__ -   Batch number = 60
Evaluating:  55%|█████▌    | 60/109 [00:29<00:23,  2.05it/s]11/28/2021 16:24:31 - INFO - __main__ -   Batch number = 61
Evaluating:  56%|█████▌    | 61/109 [00:29<00:23,  2.05it/s]11/28/2021 16:24:32 - INFO - __main__ -   Batch number = 62
Evaluating:  57%|█████▋    | 62/109 [00:30<00:22,  2.05it/s]11/28/2021 16:24:32 - INFO - __main__ -   Batch number = 63
Evaluating:  58%|█████▊    | 63/109 [00:30<00:22,  2.04it/s]11/28/2021 16:24:33 - INFO - __main__ -   Batch number = 64
Evaluating:  59%|█████▊    | 64/109 [00:31<00:21,  2.05it/s]11/28/2021 16:24:33 - INFO - __main__ -   Batch number = 65
Evaluating:  60%|█████▉    | 65/109 [00:31<00:21,  2.05it/s]11/28/2021 16:24:34 - INFO - __main__ -   Batch number = 66
Evaluating:  61%|██████    | 66/109 [00:32<00:21,  2.05it/s]11/28/2021 16:24:34 - INFO - __main__ -   Batch number = 67
Evaluating:  61%|██████▏   | 67/109 [00:32<00:20,  2.05it/s]11/28/2021 16:24:35 - INFO - __main__ -   Batch number = 68
Evaluating:  62%|██████▏   | 68/109 [00:33<00:20,  2.03it/s]11/28/2021 16:24:35 - INFO - __main__ -   Batch number = 69
Evaluating:  63%|██████▎   | 69/109 [00:33<00:19,  2.02it/s]11/28/2021 16:24:36 - INFO - __main__ -   Batch number = 70
Evaluating:  64%|██████▍   | 70/109 [00:34<00:19,  1.99it/s]11/28/2021 16:24:36 - INFO - __main__ -   Batch number = 71
Evaluating:  65%|██████▌   | 71/109 [00:34<00:18,  2.00it/s]11/28/2021 16:24:37 - INFO - __main__ -   Batch number = 72
Evaluating:  66%|██████▌   | 72/109 [00:35<00:18,  2.03it/s]11/28/2021 16:24:37 - INFO - __main__ -   Batch number = 73
Evaluating:  67%|██████▋   | 73/109 [00:35<00:17,  2.03it/s]11/28/2021 16:24:38 - INFO - __main__ -   Batch number = 74
Evaluating:  68%|██████▊   | 74/109 [00:36<00:17,  2.04it/s]11/28/2021 16:24:38 - INFO - __main__ -   Batch number = 75
Evaluating:  69%|██████▉   | 75/109 [00:36<00:16,  2.04it/s]11/28/2021 16:24:39 - INFO - __main__ -   Batch number = 76
Evaluating:  70%|██████▉   | 76/109 [00:37<00:16,  2.04it/s]11/28/2021 16:24:39 - INFO - __main__ -   Batch number = 77
Evaluating:  71%|███████   | 77/109 [00:37<00:15,  2.04it/s]11/28/2021 16:24:40 - INFO - __main__ -   Batch number = 78
Evaluating:  72%|███████▏  | 78/109 [00:38<00:15,  2.04it/s]11/28/2021 16:24:40 - INFO - __main__ -   Batch number = 79
Evaluating:  72%|███████▏  | 79/109 [00:38<00:14,  2.04it/s]11/28/2021 16:24:41 - INFO - __main__ -   Batch number = 80
Evaluating:  73%|███████▎  | 80/109 [00:39<00:13,  2.08it/s]11/28/2021 16:24:41 - INFO - __main__ -   Batch number = 81
Evaluating:  74%|███████▍  | 81/109 [00:39<00:13,  2.07it/s]11/28/2021 16:24:42 - INFO - __main__ -   Batch number = 82
Evaluating:  75%|███████▌  | 82/109 [00:40<00:13,  2.07it/s]11/28/2021 16:24:42 - INFO - __main__ -   Batch number = 83
Evaluating:  76%|███████▌  | 83/109 [00:40<00:12,  2.07it/s]11/28/2021 16:24:43 - INFO - __main__ -   Batch number = 84
Evaluating:  77%|███████▋  | 84/109 [00:41<00:12,  2.06it/s]11/28/2021 16:24:43 - INFO - __main__ -   Batch number = 85
Evaluating:  78%|███████▊  | 85/109 [00:41<00:11,  2.05it/s]11/28/2021 16:24:44 - INFO - __main__ -   Batch number = 86
Evaluating:  79%|███████▉  | 86/109 [00:42<00:11,  2.05it/s]11/28/2021 16:24:44 - INFO - __main__ -   Batch number = 87
Evaluating:  80%|███████▉  | 87/109 [00:42<00:10,  2.04it/s]11/28/2021 16:24:45 - INFO - __main__ -   Batch number = 88
Evaluating:  81%|████████  | 88/109 [00:43<00:10,  2.03it/s]11/28/2021 16:24:45 - INFO - __main__ -   Batch number = 89
Evaluating:  82%|████████▏ | 89/109 [00:43<00:09,  2.04it/s]11/28/2021 16:24:46 - INFO - __main__ -   Batch number = 90
Evaluating:  83%|████████▎ | 90/109 [00:44<00:09,  2.05it/s]11/28/2021 16:24:46 - INFO - __main__ -   Batch number = 91
Evaluating:  83%|████████▎ | 91/109 [00:44<00:08,  2.05it/s]11/28/2021 16:24:47 - INFO - __main__ -   Batch number = 92
Evaluating:  84%|████████▍ | 92/109 [00:45<00:08,  2.05it/s]11/28/2021 16:24:47 - INFO - __main__ -   Batch number = 93
Evaluating:  85%|████████▌ | 93/109 [00:45<00:07,  2.05it/s]11/28/2021 16:24:48 - INFO - __main__ -   Batch number = 94
Evaluating:  86%|████████▌ | 94/109 [00:46<00:07,  2.05it/s]11/28/2021 16:24:48 - INFO - __main__ -   Batch number = 95
Evaluating:  87%|████████▋ | 95/109 [00:46<00:06,  2.15it/s]11/28/2021 16:24:49 - INFO - __main__ -   Batch number = 96
Evaluating:  88%|████████▊ | 96/109 [00:46<00:06,  2.11it/s]11/28/2021 16:24:49 - INFO - __main__ -   Batch number = 97
Evaluating:  89%|████████▉ | 97/109 [00:47<00:05,  2.09it/s]11/28/2021 16:24:50 - INFO - __main__ -   Batch number = 98
Evaluating:  90%|████████▉ | 98/109 [00:47<00:05,  2.08it/s]11/28/2021 16:24:50 - INFO - __main__ -   Batch number = 99
Evaluating:  91%|█████████ | 99/109 [00:48<00:04,  2.06it/s]11/28/2021 16:24:51 - INFO - __main__ -   Batch number = 100
Evaluating:  92%|█████████▏| 100/109 [00:48<00:04,  2.04it/s]11/28/2021 16:24:51 - INFO - __main__ -   Batch number = 101
Evaluating:  93%|█████████▎| 101/109 [00:49<00:03,  2.05it/s]11/28/2021 16:24:51 - INFO - __main__ -   Batch number = 102
Evaluating:  94%|█████████▎| 102/109 [00:49<00:03,  2.23it/s]11/28/2021 16:24:52 - INFO - __main__ -   Batch number = 103
Evaluating:  94%|█████████▍| 103/109 [00:50<00:02,  2.44it/s]11/28/2021 16:24:52 - INFO - __main__ -   Batch number = 104
Evaluating:  95%|█████████▌| 104/109 [00:50<00:01,  2.59it/s]11/28/2021 16:24:52 - INFO - __main__ -   Batch number = 105
Evaluating:  96%|█████████▋| 105/109 [00:50<00:01,  2.72it/s]11/28/2021 16:24:53 - INFO - __main__ -   Batch number = 106
Evaluating:  97%|█████████▋| 106/109 [00:51<00:01,  2.80it/s]11/28/2021 16:24:53 - INFO - __main__ -   Batch number = 107
Evaluating:  98%|█████████▊| 107/109 [00:51<00:00,  2.88it/s]11/28/2021 16:24:53 - INFO - __main__ -   Batch number = 108
Evaluating:  99%|█████████▉| 108/109 [00:51<00:00,  2.93it/s]11/28/2021 16:24:54 - INFO - __main__ -   Batch number = 109
Evaluating: 100%|██████████| 109/109 [00:51<00:00,  2.10it/s]
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 16:24:55 - INFO - __main__ -   ***** Evaluation result  in zh *****
11/28/2021 16:24:55 - INFO - __main__ -     f1 = 0.6021949109781589
11/28/2021 16:24:55 - INFO - __main__ -     loss = 1.3358200055743576
11/28/2021 16:24:55 - INFO - __main__ -     precision = 0.6112269534679543
11/28/2021 16:24:55 - INFO - __main__ -     recall = 0.5934259127717005
54.61user 19.94system 1:13.37elapsed 101%CPU (0avgtext+0avgdata 3982924maxresident)k
16inputs+776outputs (0major+1881916minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 16:24:58 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='zh', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 16:24:58 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 16:24:58 - INFO - __main__ -   Seed = 2
11/28/2021 16:24:58 - INFO - root -   save model
11/28/2021 16:24:58 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='zh', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 16:24:58 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 16:25:00 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 16:25:06 - INFO - __main__ -   Using lang2id = None
11/28/2021 16:25:06 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 16:25:06 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
11/28/2021 16:25:06 - INFO - root -   Trying to decide if add adapter
11/28/2021 16:25:06 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 16:25:06 - INFO - root -   loading lang adpater bh/wiki@ukp
11/28/2021 16:25:06 - INFO - __main__ -   Adapter Languages : ['bh'], Length : 1
11/28/2021 16:25:06 - INFO - __main__ -   Adapter Names ['bh/wiki@ukp'], Length : 1
11/28/2021 16:25:06 - INFO - __main__ -   Language = bh
11/28/2021 16:25:06 - INFO - __main__ -   Adapter Name = bh/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/bh/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_bh_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/adapter_config.json
Adding adapter 'bh' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/head_config.json
11/28/2021 16:25:13 - INFO - __main__ -   Language adapter for zh not found, using bh instead
11/28/2021 16:25:13 - INFO - __main__ -   Set active language adapter to bh
11/28/2021 16:25:13 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 16:25:13 - INFO - __main__ -   Adapter Languages = ['bh']
11/28/2021 16:25:13 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_zh_bert-base-multilingual-cased_128
11/28/2021 16:25:13 - INFO - __main__ -   ***** Running evaluation  in zh *****
11/28/2021 16:25:13 - INFO - __main__ -     Num examples = 3458
11/28/2021 16:25:13 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/109 [00:00<?, ?it/s]11/28/2021 16:25:13 - INFO - __main__ -   Batch number = 1
Evaluating:   1%|          | 1/109 [00:00<00:36,  2.94it/s]11/28/2021 16:25:14 - INFO - __main__ -   Batch number = 2
Evaluating:   2%|▏         | 2/109 [00:00<00:35,  2.99it/s]11/28/2021 16:25:14 - INFO - __main__ -   Batch number = 3
Evaluating:   3%|▎         | 3/109 [00:01<00:35,  3.00it/s]11/28/2021 16:25:14 - INFO - __main__ -   Batch number = 4
Evaluating:   4%|▎         | 4/109 [00:01<00:34,  3.01it/s]11/28/2021 16:25:15 - INFO - __main__ -   Batch number = 5
Evaluating:   5%|▍         | 5/109 [00:01<00:34,  3.02it/s]11/28/2021 16:25:15 - INFO - __main__ -   Batch number = 6
Evaluating:   6%|▌         | 6/109 [00:01<00:34,  3.02it/s]11/28/2021 16:25:15 - INFO - __main__ -   Batch number = 7
Evaluating:   6%|▋         | 7/109 [00:02<00:33,  3.02it/s]11/28/2021 16:25:16 - INFO - __main__ -   Batch number = 8
Evaluating:   7%|▋         | 8/109 [00:02<00:33,  3.01it/s]11/28/2021 16:25:16 - INFO - __main__ -   Batch number = 9
Evaluating:   8%|▊         | 9/109 [00:02<00:33,  3.01it/s]11/28/2021 16:25:16 - INFO - __main__ -   Batch number = 10
Evaluating:   9%|▉         | 10/109 [00:03<00:32,  3.01it/s]11/28/2021 16:25:17 - INFO - __main__ -   Batch number = 11
Evaluating:  10%|█         | 11/109 [00:03<00:32,  3.01it/s]11/28/2021 16:25:17 - INFO - __main__ -   Batch number = 12
Evaluating:  11%|█         | 12/109 [00:03<00:32,  3.00it/s]11/28/2021 16:25:17 - INFO - __main__ -   Batch number = 13
Evaluating:  12%|█▏        | 13/109 [00:04<00:31,  3.01it/s]11/28/2021 16:25:18 - INFO - __main__ -   Batch number = 14
Evaluating:  13%|█▎        | 14/109 [00:04<00:31,  3.01it/s]11/28/2021 16:25:18 - INFO - __main__ -   Batch number = 15
Evaluating:  14%|█▍        | 15/109 [00:04<00:31,  3.01it/s]11/28/2021 16:25:18 - INFO - __main__ -   Batch number = 16
Evaluating:  15%|█▍        | 16/109 [00:05<00:30,  3.02it/s]11/28/2021 16:25:19 - INFO - __main__ -   Batch number = 17
Evaluating:  16%|█▌        | 17/109 [00:05<00:30,  3.02it/s]11/28/2021 16:25:19 - INFO - __main__ -   Batch number = 18
Evaluating:  17%|█▋        | 18/109 [00:05<00:30,  3.02it/s]11/28/2021 16:25:19 - INFO - __main__ -   Batch number = 19
Evaluating:  17%|█▋        | 19/109 [00:06<00:29,  3.02it/s]11/28/2021 16:25:20 - INFO - __main__ -   Batch number = 20
Evaluating:  18%|█▊        | 20/109 [00:06<00:27,  3.18it/s]11/28/2021 16:25:20 - INFO - __main__ -   Batch number = 21
Evaluating:  19%|█▉        | 21/109 [00:06<00:28,  3.12it/s]11/28/2021 16:25:20 - INFO - __main__ -   Batch number = 22
Evaluating:  20%|██        | 22/109 [00:07<00:28,  3.09it/s]11/28/2021 16:25:21 - INFO - __main__ -   Batch number = 23
Evaluating:  21%|██        | 23/109 [00:07<00:28,  3.06it/s]11/28/2021 16:25:21 - INFO - __main__ -   Batch number = 24
Evaluating:  22%|██▏       | 24/109 [00:07<00:28,  3.02it/s]11/28/2021 16:25:21 - INFO - __main__ -   Batch number = 25
Evaluating:  23%|██▎       | 25/109 [00:08<00:27,  3.01it/s]11/28/2021 16:25:22 - INFO - __main__ -   Batch number = 26
Evaluating:  24%|██▍       | 26/109 [00:08<00:27,  3.02it/s]11/28/2021 16:25:22 - INFO - __main__ -   Batch number = 27
Evaluating:  25%|██▍       | 27/109 [00:08<00:27,  3.01it/s]11/28/2021 16:25:22 - INFO - __main__ -   Batch number = 28
Evaluating:  26%|██▌       | 28/109 [00:09<00:26,  3.02it/s]11/28/2021 16:25:23 - INFO - __main__ -   Batch number = 29
Evaluating:  27%|██▋       | 29/109 [00:09<00:26,  3.03it/s]11/28/2021 16:25:23 - INFO - __main__ -   Batch number = 30
Evaluating:  28%|██▊       | 30/109 [00:09<00:26,  3.01it/s]11/28/2021 16:25:23 - INFO - __main__ -   Batch number = 31
Evaluating:  28%|██▊       | 31/109 [00:10<00:25,  3.01it/s]11/28/2021 16:25:24 - INFO - __main__ -   Batch number = 32
Evaluating:  29%|██▉       | 32/109 [00:10<00:25,  3.01it/s]11/28/2021 16:25:24 - INFO - __main__ -   Batch number = 33
Evaluating:  30%|███       | 33/109 [00:10<00:25,  3.01it/s]11/28/2021 16:25:24 - INFO - __main__ -   Batch number = 34
Evaluating:  31%|███       | 34/109 [00:11<00:24,  3.00it/s]11/28/2021 16:25:25 - INFO - __main__ -   Batch number = 35
Evaluating:  32%|███▏      | 35/109 [00:11<00:24,  3.00it/s]11/28/2021 16:25:25 - INFO - __main__ -   Batch number = 36
Evaluating:  33%|███▎      | 36/109 [00:12<00:28,  2.52it/s]11/28/2021 16:25:25 - INFO - __main__ -   Batch number = 37
Evaluating:  34%|███▍      | 37/109 [00:12<00:27,  2.63it/s]11/28/2021 16:25:26 - INFO - __main__ -   Batch number = 38
Evaluating:  35%|███▍      | 38/109 [00:12<00:25,  2.74it/s]11/28/2021 16:25:26 - INFO - __main__ -   Batch number = 39
Evaluating:  36%|███▌      | 39/109 [00:13<00:24,  2.83it/s]11/28/2021 16:25:26 - INFO - __main__ -   Batch number = 40
Evaluating:  37%|███▋      | 40/109 [00:13<00:23,  2.89it/s]11/28/2021 16:25:27 - INFO - __main__ -   Batch number = 41
Evaluating:  38%|███▊      | 41/109 [00:13<00:23,  2.94it/s]11/28/2021 16:25:27 - INFO - __main__ -   Batch number = 42
Evaluating:  39%|███▊      | 42/109 [00:14<00:22,  2.95it/s]11/28/2021 16:25:27 - INFO - __main__ -   Batch number = 43
Evaluating:  39%|███▉      | 43/109 [00:14<00:22,  2.96it/s]11/28/2021 16:25:28 - INFO - __main__ -   Batch number = 44
Evaluating:  40%|████      | 44/109 [00:14<00:21,  2.98it/s]11/28/2021 16:25:28 - INFO - __main__ -   Batch number = 45
Evaluating:  41%|████▏     | 45/109 [00:15<00:21,  2.98it/s]11/28/2021 16:25:28 - INFO - __main__ -   Batch number = 46
Evaluating:  42%|████▏     | 46/109 [00:15<00:20,  3.01it/s]11/28/2021 16:25:29 - INFO - __main__ -   Batch number = 47
Evaluating:  43%|████▎     | 47/109 [00:15<00:20,  3.02it/s]11/28/2021 16:25:29 - INFO - __main__ -   Batch number = 48
Evaluating:  44%|████▍     | 48/109 [00:16<00:20,  3.02it/s]11/28/2021 16:25:29 - INFO - __main__ -   Batch number = 49
Evaluating:  45%|████▍     | 49/109 [00:16<00:20,  2.99it/s]11/28/2021 16:25:30 - INFO - __main__ -   Batch number = 50
Evaluating:  46%|████▌     | 50/109 [00:16<00:19,  2.99it/s]11/28/2021 16:25:30 - INFO - __main__ -   Batch number = 51
Evaluating:  47%|████▋     | 51/109 [00:17<00:19,  3.00it/s]11/28/2021 16:25:30 - INFO - __main__ -   Batch number = 52
Evaluating:  48%|████▊     | 52/109 [00:17<00:19,  2.99it/s]11/28/2021 16:25:31 - INFO - __main__ -   Batch number = 53
Evaluating:  49%|████▊     | 53/109 [00:17<00:18,  2.99it/s]11/28/2021 16:25:31 - INFO - __main__ -   Batch number = 54
Evaluating:  50%|████▉     | 54/109 [00:18<00:18,  3.00it/s]11/28/2021 16:25:31 - INFO - __main__ -   Batch number = 55
Evaluating:  50%|█████     | 55/109 [00:18<00:18,  2.91it/s]11/28/2021 16:25:32 - INFO - __main__ -   Batch number = 56
Evaluating:  51%|█████▏    | 56/109 [00:18<00:20,  2.56it/s]11/28/2021 16:25:32 - INFO - __main__ -   Batch number = 57
Evaluating:  52%|█████▏    | 57/109 [00:19<00:21,  2.37it/s]11/28/2021 16:25:33 - INFO - __main__ -   Batch number = 58
Evaluating:  53%|█████▎    | 58/109 [00:19<00:22,  2.25it/s]11/28/2021 16:25:33 - INFO - __main__ -   Batch number = 59
Evaluating:  54%|█████▍    | 59/109 [00:20<00:23,  2.17it/s]11/28/2021 16:25:34 - INFO - __main__ -   Batch number = 60
Evaluating:  55%|█████▌    | 60/109 [00:20<00:23,  2.12it/s]11/28/2021 16:25:34 - INFO - __main__ -   Batch number = 61
Evaluating:  56%|█████▌    | 61/109 [00:21<00:23,  2.08it/s]11/28/2021 16:25:35 - INFO - __main__ -   Batch number = 62
Evaluating:  57%|█████▋    | 62/109 [00:21<00:22,  2.06it/s]11/28/2021 16:25:35 - INFO - __main__ -   Batch number = 63
Evaluating:  58%|█████▊    | 63/109 [00:22<00:23,  1.92it/s]11/28/2021 16:25:36 - INFO - __main__ -   Batch number = 64
Evaluating:  59%|█████▊    | 64/109 [00:23<00:23,  1.94it/s]11/28/2021 16:25:36 - INFO - __main__ -   Batch number = 65
Evaluating:  60%|█████▉    | 65/109 [00:23<00:22,  1.96it/s]11/28/2021 16:25:37 - INFO - __main__ -   Batch number = 66
Evaluating:  61%|██████    | 66/109 [00:24<00:21,  1.98it/s]11/28/2021 16:25:37 - INFO - __main__ -   Batch number = 67
Evaluating:  61%|██████▏   | 67/109 [00:24<00:21,  1.98it/s]11/28/2021 16:25:38 - INFO - __main__ -   Batch number = 68
Evaluating:  62%|██████▏   | 68/109 [00:25<00:20,  1.99it/s]11/28/2021 16:25:38 - INFO - __main__ -   Batch number = 69
Evaluating:  63%|██████▎   | 69/109 [00:25<00:20,  1.99it/s]11/28/2021 16:25:39 - INFO - __main__ -   Batch number = 70
Evaluating:  64%|██████▍   | 70/109 [00:26<00:19,  2.00it/s]11/28/2021 16:25:39 - INFO - __main__ -   Batch number = 71
Evaluating:  65%|██████▌   | 71/109 [00:26<00:18,  2.00it/s]11/28/2021 16:25:40 - INFO - __main__ -   Batch number = 72
Evaluating:  66%|██████▌   | 72/109 [00:27<00:18,  2.00it/s]11/28/2021 16:25:40 - INFO - __main__ -   Batch number = 73
Evaluating:  67%|██████▋   | 73/109 [00:27<00:17,  2.01it/s]11/28/2021 16:25:41 - INFO - __main__ -   Batch number = 74
Evaluating:  68%|██████▊   | 74/109 [00:28<00:17,  2.01it/s]11/28/2021 16:25:41 - INFO - __main__ -   Batch number = 75
Evaluating:  69%|██████▉   | 75/109 [00:28<00:16,  2.00it/s]11/28/2021 16:25:42 - INFO - __main__ -   Batch number = 76
Evaluating:  70%|██████▉   | 76/109 [00:29<00:16,  2.00it/s]11/28/2021 16:25:42 - INFO - __main__ -   Batch number = 77
Evaluating:  71%|███████   | 77/109 [00:29<00:15,  2.00it/s]11/28/2021 16:25:43 - INFO - __main__ -   Batch number = 78
Evaluating:  72%|███████▏  | 78/109 [00:30<00:15,  2.00it/s]11/28/2021 16:25:43 - INFO - __main__ -   Batch number = 79
Evaluating:  72%|███████▏  | 79/109 [00:30<00:15,  2.00it/s]11/28/2021 16:25:44 - INFO - __main__ -   Batch number = 80
Evaluating:  73%|███████▎  | 80/109 [00:31<00:14,  2.00it/s]11/28/2021 16:25:44 - INFO - __main__ -   Batch number = 81
Evaluating:  74%|███████▍  | 81/109 [00:31<00:13,  2.00it/s]11/28/2021 16:25:45 - INFO - __main__ -   Batch number = 82
Evaluating:  75%|███████▌  | 82/109 [00:32<00:13,  2.00it/s]11/28/2021 16:25:45 - INFO - __main__ -   Batch number = 83
Evaluating:  76%|███████▌  | 83/109 [00:32<00:13,  2.00it/s]11/28/2021 16:25:46 - INFO - __main__ -   Batch number = 84
Evaluating:  77%|███████▋  | 84/109 [00:33<00:12,  1.99it/s]11/28/2021 16:25:46 - INFO - __main__ -   Batch number = 85
Evaluating:  78%|███████▊  | 85/109 [00:33<00:12,  1.98it/s]11/28/2021 16:25:47 - INFO - __main__ -   Batch number = 86
Evaluating:  79%|███████▉  | 86/109 [00:34<00:11,  1.99it/s]11/28/2021 16:25:47 - INFO - __main__ -   Batch number = 87
Evaluating:  80%|███████▉  | 87/109 [00:34<00:11,  2.00it/s]11/28/2021 16:25:48 - INFO - __main__ -   Batch number = 88
Evaluating:  81%|████████  | 88/109 [00:35<00:10,  2.00it/s]11/28/2021 16:25:48 - INFO - __main__ -   Batch number = 89
Evaluating:  82%|████████▏ | 89/109 [00:35<00:10,  2.00it/s]11/28/2021 16:25:49 - INFO - __main__ -   Batch number = 90
Evaluating:  83%|████████▎ | 90/109 [00:36<00:09,  1.99it/s]11/28/2021 16:25:49 - INFO - __main__ -   Batch number = 91
Evaluating:  83%|████████▎ | 91/109 [00:36<00:09,  2.00it/s]11/28/2021 16:25:50 - INFO - __main__ -   Batch number = 92
Evaluating:  84%|████████▍ | 92/109 [00:37<00:08,  2.00it/s]11/28/2021 16:25:50 - INFO - __main__ -   Batch number = 93
Evaluating:  85%|████████▌ | 93/109 [00:37<00:07,  2.01it/s]11/28/2021 16:25:51 - INFO - __main__ -   Batch number = 94
Evaluating:  86%|████████▌ | 94/109 [00:38<00:07,  2.01it/s]11/28/2021 16:25:51 - INFO - __main__ -   Batch number = 95
Evaluating:  87%|████████▋ | 95/109 [00:38<00:06,  2.01it/s]11/28/2021 16:25:52 - INFO - __main__ -   Batch number = 96
Evaluating:  88%|████████▊ | 96/109 [00:38<00:06,  2.08it/s]11/28/2021 16:25:52 - INFO - __main__ -   Batch number = 97
Evaluating:  89%|████████▉ | 97/109 [00:39<00:05,  2.05it/s]11/28/2021 16:25:53 - INFO - __main__ -   Batch number = 98
Evaluating:  90%|████████▉ | 98/109 [00:39<00:05,  2.04it/s]11/28/2021 16:25:53 - INFO - __main__ -   Batch number = 99
Evaluating:  91%|█████████ | 99/109 [00:40<00:04,  2.04it/s]11/28/2021 16:25:54 - INFO - __main__ -   Batch number = 100
Evaluating:  92%|█████████▏| 100/109 [00:40<00:04,  2.03it/s]11/28/2021 16:25:54 - INFO - __main__ -   Batch number = 101
Evaluating:  93%|█████████▎| 101/109 [00:41<00:03,  2.02it/s]11/28/2021 16:25:55 - INFO - __main__ -   Batch number = 102
Evaluating:  94%|█████████▎| 102/109 [00:41<00:03,  2.01it/s]11/28/2021 16:25:55 - INFO - __main__ -   Batch number = 103
Evaluating:  94%|█████████▍| 103/109 [00:42<00:02,  2.01it/s]11/28/2021 16:25:56 - INFO - __main__ -   Batch number = 104
Evaluating:  95%|█████████▌| 104/109 [00:42<00:02,  2.01it/s]11/28/2021 16:25:56 - INFO - __main__ -   Batch number = 105
Evaluating:  96%|█████████▋| 105/109 [00:43<00:01,  2.00it/s]11/28/2021 16:25:57 - INFO - __main__ -   Batch number = 106
Evaluating:  97%|█████████▋| 106/109 [00:43<00:01,  2.00it/s]11/28/2021 16:25:57 - INFO - __main__ -   Batch number = 107
Evaluating:  98%|█████████▊| 107/109 [00:44<00:01,  1.92it/s]11/28/2021 16:25:58 - INFO - __main__ -   Batch number = 108
Evaluating:  99%|█████████▉| 108/109 [00:45<00:00,  1.94it/s]11/28/2021 16:25:58 - INFO - __main__ -   Batch number = 109
Evaluating: 100%|██████████| 109/109 [00:45<00:00,  2.41it/s]
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 16:26:00 - INFO - __main__ -   ***** Evaluation result  in zh *****
11/28/2021 16:26:00 - INFO - __main__ -     f1 = 0.5964366896973075
11/28/2021 16:26:00 - INFO - __main__ -     loss = 1.36431677078982
11/28/2021 16:26:00 - INFO - __main__ -     precision = 0.6036116314171925
11/28/2021 16:26:00 - INFO - __main__ -     recall = 0.5894303168063646
47.89user 17.07system 1:04.66elapsed 100%CPU (0avgtext+0avgdata 3985456maxresident)k
0inputs+744outputs (0major+1658737minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 16:26:02 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='zh', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 16:26:02 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 16:26:02 - INFO - __main__ -   Seed = 3
11/28/2021 16:26:02 - INFO - root -   save model
11/28/2021 16:26:02 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='zh', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bh//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 16:26:02 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 16:26:05 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 16:26:11 - INFO - __main__ -   Using lang2id = None
11/28/2021 16:26:11 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 16:26:11 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
11/28/2021 16:26:11 - INFO - root -   Trying to decide if add adapter
11/28/2021 16:26:11 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 16:26:11 - INFO - root -   loading lang adpater bh/wiki@ukp
11/28/2021 16:26:11 - INFO - __main__ -   Adapter Languages : ['bh'], Length : 1
11/28/2021 16:26:11 - INFO - __main__ -   Adapter Names ['bh/wiki@ukp'], Length : 1
11/28/2021 16:26:11 - INFO - __main__ -   Language = bh
11/28/2021 16:26:11 - INFO - __main__ -   Adapter Name = bh/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/bh/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_bh_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/adapter_config.json
Adding adapter 'bh' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/1aaa301517b310ef94f706667bc395687c6db23a351ed9581f9dc6e6ea6f2e95-70ebaa536fbc1e2421bb400c78f88f405c3959401ecbd71b3c51e565c92a7607-extracted/head_config.json
11/28/2021 16:26:21 - INFO - __main__ -   Language adapter for zh not found, using bh instead
11/28/2021 16:26:21 - INFO - __main__ -   Set active language adapter to bh
11/28/2021 16:26:21 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 16:26:21 - INFO - __main__ -   Adapter Languages = ['bh']
11/28/2021 16:26:21 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_zh_bert-base-multilingual-cased_128
11/28/2021 16:26:22 - INFO - __main__ -   ***** Running evaluation  in zh *****
11/28/2021 16:26:22 - INFO - __main__ -     Num examples = 3458
11/28/2021 16:26:22 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/109 [00:00<?, ?it/s]11/28/2021 16:26:22 - INFO - __main__ -   Batch number = 1
Evaluating:   1%|          | 1/109 [00:00<00:52,  2.04it/s]11/28/2021 16:26:23 - INFO - __main__ -   Batch number = 2
Evaluating:   2%|▏         | 2/109 [00:01<01:04,  1.67it/s]11/28/2021 16:26:23 - INFO - __main__ -   Batch number = 3
Evaluating:   3%|▎         | 3/109 [00:01<00:54,  1.94it/s]11/28/2021 16:26:24 - INFO - __main__ -   Batch number = 4
Evaluating:   4%|▎         | 4/109 [00:02<00:51,  2.03it/s]11/28/2021 16:26:24 - INFO - __main__ -   Batch number = 5
Evaluating:   5%|▍         | 5/109 [00:02<00:49,  2.08it/s]11/28/2021 16:26:25 - INFO - __main__ -   Batch number = 6
Evaluating:   6%|▌         | 6/109 [00:02<00:49,  2.09it/s]11/28/2021 16:26:25 - INFO - __main__ -   Batch number = 7
Evaluating:   6%|▋         | 7/109 [00:03<00:48,  2.10it/s]11/28/2021 16:26:26 - INFO - __main__ -   Batch number = 8
Evaluating:   7%|▋         | 8/109 [00:03<00:48,  2.09it/s]11/28/2021 16:26:26 - INFO - __main__ -   Batch number = 9
Evaluating:   8%|▊         | 9/109 [00:04<00:48,  2.08it/s]11/28/2021 16:26:26 - INFO - __main__ -   Batch number = 10
Evaluating:   9%|▉         | 10/109 [00:04<00:47,  2.07it/s]11/28/2021 16:26:27 - INFO - __main__ -   Batch number = 11
Evaluating:  10%|█         | 11/109 [00:05<00:47,  2.07it/s]11/28/2021 16:26:27 - INFO - __main__ -   Batch number = 12
Evaluating:  11%|█         | 12/109 [00:05<00:41,  2.36it/s]11/28/2021 16:26:28 - INFO - __main__ -   Batch number = 13
Evaluating:  12%|█▏        | 13/109 [00:06<00:44,  2.14it/s]11/28/2021 16:26:28 - INFO - __main__ -   Batch number = 14
Evaluating:  13%|█▎        | 14/109 [00:06<00:44,  2.15it/s]11/28/2021 16:26:29 - INFO - __main__ -   Batch number = 15
Evaluating:  14%|█▍        | 15/109 [00:07<00:44,  2.12it/s]11/28/2021 16:26:29 - INFO - __main__ -   Batch number = 16
Evaluating:  15%|█▍        | 16/109 [00:07<00:44,  2.11it/s]11/28/2021 16:26:30 - INFO - __main__ -   Batch number = 17
Evaluating:  16%|█▌        | 17/109 [00:08<00:43,  2.09it/s]11/28/2021 16:26:30 - INFO - __main__ -   Batch number = 18
Evaluating:  17%|█▋        | 18/109 [00:08<00:43,  2.08it/s]11/28/2021 16:26:31 - INFO - __main__ -   Batch number = 19
Evaluating:  17%|█▋        | 19/109 [00:09<00:43,  2.07it/s]11/28/2021 16:26:31 - INFO - __main__ -   Batch number = 20
Evaluating:  18%|█▊        | 20/109 [00:09<00:42,  2.07it/s]11/28/2021 16:26:32 - INFO - __main__ -   Batch number = 21
Evaluating:  19%|█▉        | 21/109 [00:10<00:42,  2.07it/s]11/28/2021 16:26:32 - INFO - __main__ -   Batch number = 22
Evaluating:  20%|██        | 22/109 [00:10<00:42,  2.07it/s]11/28/2021 16:26:33 - INFO - __main__ -   Batch number = 23
Evaluating:  21%|██        | 23/109 [00:11<00:41,  2.09it/s]11/28/2021 16:26:33 - INFO - __main__ -   Batch number = 24
Evaluating:  22%|██▏       | 24/109 [00:11<00:36,  2.32it/s]11/28/2021 16:26:33 - INFO - __main__ -   Batch number = 25
Evaluating:  23%|██▎       | 25/109 [00:11<00:33,  2.50it/s]11/28/2021 16:26:34 - INFO - __main__ -   Batch number = 26
Evaluating:  24%|██▍       | 26/109 [00:12<00:31,  2.65it/s]11/28/2021 16:26:34 - INFO - __main__ -   Batch number = 27
Evaluating:  25%|██▍       | 27/109 [00:12<00:29,  2.77it/s]11/28/2021 16:26:34 - INFO - __main__ -   Batch number = 28
Evaluating:  26%|██▌       | 28/109 [00:12<00:28,  2.86it/s]11/28/2021 16:26:35 - INFO - __main__ -   Batch number = 29
Evaluating:  27%|██▋       | 29/109 [00:12<00:27,  2.92it/s]11/28/2021 16:26:35 - INFO - __main__ -   Batch number = 30
Evaluating:  28%|██▊       | 30/109 [00:13<00:26,  2.96it/s]11/28/2021 16:26:35 - INFO - __main__ -   Batch number = 31
Evaluating:  28%|██▊       | 31/109 [00:13<00:25,  3.01it/s]11/28/2021 16:26:36 - INFO - __main__ -   Batch number = 32
Evaluating:  29%|██▉       | 32/109 [00:13<00:25,  3.03it/s]11/28/2021 16:26:36 - INFO - __main__ -   Batch number = 33
Evaluating:  30%|███       | 33/109 [00:14<00:24,  3.05it/s]11/28/2021 16:26:36 - INFO - __main__ -   Batch number = 34
Evaluating:  31%|███       | 34/109 [00:14<00:24,  3.06it/s]11/28/2021 16:26:37 - INFO - __main__ -   Batch number = 35
Evaluating:  32%|███▏      | 35/109 [00:14<00:24,  3.07it/s]11/28/2021 16:26:37 - INFO - __main__ -   Batch number = 36
Evaluating:  33%|███▎      | 36/109 [00:15<00:23,  3.06it/s]11/28/2021 16:26:37 - INFO - __main__ -   Batch number = 37
Evaluating:  34%|███▍      | 37/109 [00:15<00:23,  3.07it/s]11/28/2021 16:26:38 - INFO - __main__ -   Batch number = 38
Evaluating:  35%|███▍      | 38/109 [00:15<00:23,  3.02it/s]11/28/2021 16:26:38 - INFO - __main__ -   Batch number = 39
Evaluating:  36%|███▌      | 39/109 [00:16<00:23,  3.04it/s]11/28/2021 16:26:38 - INFO - __main__ -   Batch number = 40
Evaluating:  37%|███▋      | 40/109 [00:16<00:22,  3.13it/s]11/28/2021 16:26:39 - INFO - __main__ -   Batch number = 41
Evaluating:  38%|███▊      | 41/109 [00:16<00:21,  3.16it/s]11/28/2021 16:26:39 - INFO - __main__ -   Batch number = 42
Evaluating:  39%|███▊      | 42/109 [00:17<00:21,  3.14it/s]11/28/2021 16:26:39 - INFO - __main__ -   Batch number = 43
Evaluating:  39%|███▉      | 43/109 [00:17<00:21,  3.12it/s]11/28/2021 16:26:40 - INFO - __main__ -   Batch number = 44
Evaluating:  40%|████      | 44/109 [00:17<00:21,  3.09it/s]11/28/2021 16:26:40 - INFO - __main__ -   Batch number = 45
Evaluating:  41%|████▏     | 45/109 [00:18<00:20,  3.18it/s]11/28/2021 16:26:40 - INFO - __main__ -   Batch number = 46
Evaluating:  42%|████▏     | 46/109 [00:18<00:19,  3.16it/s]11/28/2021 16:26:41 - INFO - __main__ -   Batch number = 47
Evaluating:  43%|████▎     | 47/109 [00:18<00:19,  3.15it/s]11/28/2021 16:26:41 - INFO - __main__ -   Batch number = 48
Evaluating:  44%|████▍     | 48/109 [00:19<00:19,  3.13it/s]11/28/2021 16:26:41 - INFO - __main__ -   Batch number = 49
Evaluating:  45%|████▍     | 49/109 [00:19<00:19,  3.14it/s]11/28/2021 16:26:41 - INFO - __main__ -   Batch number = 50
Evaluating:  46%|████▌     | 50/109 [00:19<00:18,  3.13it/s]11/28/2021 16:26:42 - INFO - __main__ -   Batch number = 51
Evaluating:  47%|████▋     | 51/109 [00:20<00:18,  3.19it/s]11/28/2021 16:26:42 - INFO - __main__ -   Batch number = 52
Evaluating:  48%|████▊     | 52/109 [00:20<00:17,  3.20it/s]11/28/2021 16:26:42 - INFO - __main__ -   Batch number = 53
Evaluating:  49%|████▊     | 53/109 [00:20<00:17,  3.24it/s]11/28/2021 16:26:43 - INFO - __main__ -   Batch number = 54
Evaluating:  50%|████▉     | 54/109 [00:21<00:18,  2.95it/s]11/28/2021 16:26:43 - INFO - __main__ -   Batch number = 55
Evaluating:  50%|█████     | 55/109 [00:21<00:18,  3.00it/s]11/28/2021 16:26:43 - INFO - __main__ -   Batch number = 56
Evaluating:  51%|█████▏    | 56/109 [00:21<00:17,  3.10it/s]11/28/2021 16:26:44 - INFO - __main__ -   Batch number = 57
Evaluating:  52%|█████▏    | 57/109 [00:21<00:16,  3.16it/s]11/28/2021 16:26:44 - INFO - __main__ -   Batch number = 58
Evaluating:  53%|█████▎    | 58/109 [00:22<00:15,  3.20it/s]11/28/2021 16:26:44 - INFO - __main__ -   Batch number = 59
Evaluating:  54%|█████▍    | 59/109 [00:22<00:15,  3.23it/s]11/28/2021 16:26:45 - INFO - __main__ -   Batch number = 60
Evaluating:  55%|█████▌    | 60/109 [00:22<00:15,  3.23it/s]11/28/2021 16:26:45 - INFO - __main__ -   Batch number = 61
Evaluating:  56%|█████▌    | 61/109 [00:23<00:14,  3.21it/s]11/28/2021 16:26:45 - INFO - __main__ -   Batch number = 62
Evaluating:  57%|█████▋    | 62/109 [00:23<00:14,  3.22it/s]11/28/2021 16:26:46 - INFO - __main__ -   Batch number = 63
Evaluating:  58%|█████▊    | 63/109 [00:23<00:14,  3.19it/s]11/28/2021 16:26:46 - INFO - __main__ -   Batch number = 64
Evaluating:  59%|█████▊    | 64/109 [00:24<00:13,  3.23it/s]11/28/2021 16:26:46 - INFO - __main__ -   Batch number = 65
Evaluating:  60%|█████▉    | 65/109 [00:24<00:13,  3.23it/s]11/28/2021 16:26:47 - INFO - __main__ -   Batch number = 66
Evaluating:  61%|██████    | 66/109 [00:24<00:13,  3.19it/s]11/28/2021 16:26:47 - INFO - __main__ -   Batch number = 67
Evaluating:  61%|██████▏   | 67/109 [00:25<00:13,  3.17it/s]11/28/2021 16:26:47 - INFO - __main__ -   Batch number = 68
Evaluating:  62%|██████▏   | 68/109 [00:25<00:13,  3.10it/s]11/28/2021 16:26:47 - INFO - __main__ -   Batch number = 69
Evaluating:  63%|██████▎   | 69/109 [00:25<00:12,  3.10it/s]11/28/2021 16:26:48 - INFO - __main__ -   Batch number = 70
Evaluating:  64%|██████▍   | 70/109 [00:26<00:12,  3.11it/s]11/28/2021 16:26:48 - INFO - __main__ -   Batch number = 71
Evaluating:  65%|██████▌   | 71/109 [00:26<00:12,  3.11it/s]11/28/2021 16:26:48 - INFO - __main__ -   Batch number = 72
Evaluating:  66%|██████▌   | 72/109 [00:26<00:11,  3.11it/s]11/28/2021 16:26:49 - INFO - __main__ -   Batch number = 73
Evaluating:  67%|██████▋   | 73/109 [00:27<00:11,  3.08it/s]11/28/2021 16:26:49 - INFO - __main__ -   Batch number = 74
Evaluating:  68%|██████▊   | 74/109 [00:27<00:11,  3.11it/s]11/28/2021 16:26:49 - INFO - __main__ -   Batch number = 75
Evaluating:  69%|██████▉   | 75/109 [00:27<00:11,  3.08it/s]11/28/2021 16:26:50 - INFO - __main__ -   Batch number = 76
Evaluating:  70%|██████▉   | 76/109 [00:27<00:10,  3.12it/s]11/28/2021 16:26:50 - INFO - __main__ -   Batch number = 77
Evaluating:  71%|███████   | 77/109 [00:28<00:10,  3.09it/s]11/28/2021 16:26:50 - INFO - __main__ -   Batch number = 78
Evaluating:  72%|███████▏  | 78/109 [00:28<00:09,  3.12it/s]11/28/2021 16:26:51 - INFO - __main__ -   Batch number = 79
Evaluating:  72%|███████▏  | 79/109 [00:28<00:09,  3.09it/s]11/28/2021 16:26:51 - INFO - __main__ -   Batch number = 80
Evaluating:  73%|███████▎  | 80/109 [00:29<00:09,  3.13it/s]11/28/2021 16:26:51 - INFO - __main__ -   Batch number = 81
Evaluating:  74%|███████▍  | 81/109 [00:29<00:09,  3.09it/s]11/28/2021 16:26:52 - INFO - __main__ -   Batch number = 82
Evaluating:  75%|███████▌  | 82/109 [00:29<00:08,  3.30it/s]11/28/2021 16:26:52 - INFO - __main__ -   Batch number = 83
Evaluating:  76%|███████▌  | 83/109 [00:30<00:06,  3.80it/s]11/28/2021 16:26:52 - INFO - __main__ -   Batch number = 84
Evaluating:  77%|███████▋  | 84/109 [00:30<00:05,  4.26it/s]11/28/2021 16:26:52 - INFO - __main__ -   Batch number = 85
Evaluating:  78%|███████▊  | 85/109 [00:30<00:05,  4.67it/s]11/28/2021 16:26:52 - INFO - __main__ -   Batch number = 86
Evaluating:  79%|███████▉  | 86/109 [00:30<00:04,  4.98it/s]11/28/2021 16:26:53 - INFO - __main__ -   Batch number = 87
Evaluating:  80%|███████▉  | 87/109 [00:30<00:04,  5.22it/s]11/28/2021 16:26:53 - INFO - __main__ -   Batch number = 88
Evaluating:  81%|████████  | 88/109 [00:30<00:04,  5.16it/s]11/28/2021 16:26:53 - INFO - __main__ -   Batch number = 89
Evaluating:  82%|████████▏ | 89/109 [00:31<00:03,  5.34it/s]11/28/2021 16:26:53 - INFO - __main__ -   Batch number = 90
Evaluating:  83%|████████▎ | 90/109 [00:31<00:03,  5.53it/s]11/28/2021 16:26:53 - INFO - __main__ -   Batch number = 91
Evaluating:  83%|████████▎ | 91/109 [00:31<00:03,  5.65it/s]11/28/2021 16:26:53 - INFO - __main__ -   Batch number = 92
Evaluating:  84%|████████▍ | 92/109 [00:31<00:02,  5.80it/s]11/28/2021 16:26:54 - INFO - __main__ -   Batch number = 93
Evaluating:  85%|████████▌ | 93/109 [00:31<00:02,  5.94it/s]11/28/2021 16:26:54 - INFO - __main__ -   Batch number = 94
Evaluating:  86%|████████▌ | 94/109 [00:31<00:02,  6.11it/s]11/28/2021 16:26:54 - INFO - __main__ -   Batch number = 95
Evaluating:  87%|████████▋ | 95/109 [00:32<00:02,  6.17it/s]11/28/2021 16:26:54 - INFO - __main__ -   Batch number = 96
Evaluating:  88%|████████▊ | 96/109 [00:32<00:02,  6.24it/s]11/28/2021 16:26:54 - INFO - __main__ -   Batch number = 97
Evaluating:  89%|████████▉ | 97/109 [00:32<00:01,  6.30it/s]11/28/2021 16:26:54 - INFO - __main__ -   Batch number = 98
Evaluating:  90%|████████▉ | 98/109 [00:32<00:01,  6.40it/s]11/28/2021 16:26:55 - INFO - __main__ -   Batch number = 99
Evaluating:  91%|█████████ | 99/109 [00:32<00:01,  6.47it/s]11/28/2021 16:26:55 - INFO - __main__ -   Batch number = 100
Evaluating:  92%|█████████▏| 100/109 [00:32<00:01,  6.53it/s]11/28/2021 16:26:55 - INFO - __main__ -   Batch number = 101
Evaluating:  93%|█████████▎| 101/109 [00:32<00:01,  6.56it/s]11/28/2021 16:26:55 - INFO - __main__ -   Batch number = 102
Evaluating:  94%|█████████▎| 102/109 [00:33<00:01,  6.61it/s]11/28/2021 16:26:55 - INFO - __main__ -   Batch number = 103
Evaluating:  94%|█████████▍| 103/109 [00:33<00:00,  6.53it/s]11/28/2021 16:26:55 - INFO - __main__ -   Batch number = 104
Evaluating:  95%|█████████▌| 104/109 [00:33<00:00,  6.51it/s]11/28/2021 16:26:56 - INFO - __main__ -   Batch number = 105
Evaluating:  96%|█████████▋| 105/109 [00:33<00:00,  6.46it/s]11/28/2021 16:26:56 - INFO - __main__ -   Batch number = 106
Evaluating:  97%|█████████▋| 106/109 [00:33<00:00,  6.42it/s]11/28/2021 16:26:56 - INFO - __main__ -   Batch number = 107
Evaluating:  98%|█████████▊| 107/109 [00:33<00:00,  6.50it/s]11/28/2021 16:26:56 - INFO - __main__ -   Batch number = 108
Evaluating:  99%|█████████▉| 108/109 [00:34<00:00,  6.45it/s]11/28/2021 16:26:56 - INFO - __main__ -   Batch number = 109
Evaluating: 100%|██████████| 109/109 [00:34<00:00,  3.20it/s]
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 16:26:58 - INFO - __main__ -   ***** Evaluation result  in zh *****
11/28/2021 16:26:58 - INFO - __main__ -     f1 = 0.6164459955386055
11/28/2021 16:26:58 - INFO - __main__ -     loss = 1.4206433804757004
11/28/2021 16:26:58 - INFO - __main__ -     precision = 0.6245807815689706
11/28/2021 16:26:58 - INFO - __main__ -     recall = 0.6085203864185253
43.92user 14.87system 0:57.65elapsed 101%CPU (0avgtext+0avgdata 3980128maxresident)k
0inputs+752outputs (0major+1627344minor)pagefaults 0swaps
