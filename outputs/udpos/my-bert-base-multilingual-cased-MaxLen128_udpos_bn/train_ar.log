11/21/2021 11:13:49 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ar', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/21/2021 11:13:49 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/21/2021 11:13:49 - INFO - __main__ -   Seed = 1
11/21/2021 11:13:49 - INFO - root -   save model
11/21/2021 11:13:49 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ar', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/21/2021 11:13:49 - INFO - __main__ -   Loading pretrained model and tokenizer
11/21/2021 11:13:51 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
11/21/2021 11:13:57 - INFO - __main__ -   Using lang2id = None
11/21/2021 11:13:57 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/21/2021 11:13:57 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
11/21/2021 11:13:57 - INFO - root -   Trying to decide if add adapter
11/21/2021 11:13:57 - INFO - root -   loading task adapter
11/21/2021 11:13:57 - INFO - root -   loading lang adpater bn/wiki@ukp
11/21/2021 11:13:57 - INFO - __main__ -   Adapter Languages : ['bn'], Length : 1
11/21/2021 11:13:57 - INFO - __main__ -   Adapter Names ['bn/wiki@ukp'], Length : 1
11/21/2021 11:13:57 - INFO - __main__ -   Language = bn
11/21/2021 11:13:57 - INFO - __main__ -   Adapter Name = bn/wiki@ukp
11/21/2021 11:14:02 - INFO - __main__ -   Language adapter for ar not found, using bn instead
11/21/2021 11:14:02 - INFO - __main__ -   Set active language adapter to bn
11/21/2021 11:14:02 - INFO - __main__ -   Args Adapter Weight = None
11/21/2021 11:14:02 - INFO - __main__ -   Adapter Languages = ['bn']
11/21/2021 11:14:02 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_ar_bert-base-multilingual-cased_128
11/21/2021 11:14:02 - INFO - __main__ -   ***** Running evaluation  in ar *****
11/21/2021 11:14:02 - INFO - __main__ -     Num examples = 1784
11/21/2021 11:14:02 - INFO - __main__ -     Batch size = 32
11/21/2021 11:14:02 - INFO - __main__ -   Batch number = 1
11/21/2021 11:14:03 - INFO - __main__ -   Batch number = 2
11/21/2021 11:14:03 - INFO - __main__ -   Batch number = 3
11/21/2021 11:14:03 - INFO - __main__ -   Batch number = 4
11/21/2021 11:14:03 - INFO - __main__ -   Batch number = 5
11/21/2021 11:14:03 - INFO - __main__ -   Batch number = 6
11/21/2021 11:14:03 - INFO - __main__ -   Batch number = 7
11/21/2021 11:14:03 - INFO - __main__ -   Batch number = 8
11/21/2021 11:14:04 - INFO - __main__ -   Batch number = 9
11/21/2021 11:14:04 - INFO - __main__ -   Batch number = 10
11/21/2021 11:14:04 - INFO - __main__ -   Batch number = 11
11/21/2021 11:14:04 - INFO - __main__ -   Batch number = 12
11/21/2021 11:14:04 - INFO - __main__ -   Batch number = 13
11/21/2021 11:14:04 - INFO - __main__ -   Batch number = 14
11/21/2021 11:14:04 - INFO - __main__ -   Batch number = 15
11/21/2021 11:14:05 - INFO - __main__ -   Batch number = 16
11/21/2021 11:14:05 - INFO - __main__ -   Batch number = 17
11/21/2021 11:14:05 - INFO - __main__ -   Batch number = 18
11/21/2021 11:14:05 - INFO - __main__ -   Batch number = 19
11/21/2021 11:14:05 - INFO - __main__ -   Batch number = 20
11/21/2021 11:14:05 - INFO - __main__ -   Batch number = 21
11/21/2021 11:14:05 - INFO - __main__ -   Batch number = 22
11/21/2021 11:14:05 - INFO - __main__ -   Batch number = 23
11/21/2021 11:14:06 - INFO - __main__ -   Batch number = 24
11/21/2021 11:14:06 - INFO - __main__ -   Batch number = 25
11/21/2021 11:14:06 - INFO - __main__ -   Batch number = 26
11/21/2021 11:14:06 - INFO - __main__ -   Batch number = 27
11/21/2021 11:14:06 - INFO - __main__ -   Batch number = 28
11/21/2021 11:14:06 - INFO - __main__ -   Batch number = 29
11/21/2021 11:14:06 - INFO - __main__ -   Batch number = 30
11/21/2021 11:14:07 - INFO - __main__ -   Batch number = 31
11/21/2021 11:14:07 - INFO - __main__ -   Batch number = 32
11/21/2021 11:14:07 - INFO - __main__ -   Batch number = 33
11/21/2021 11:14:07 - INFO - __main__ -   Batch number = 34
11/21/2021 11:14:07 - INFO - __main__ -   Batch number = 35
11/21/2021 11:14:07 - INFO - __main__ -   Batch number = 36
11/21/2021 11:14:07 - INFO - __main__ -   Batch number = 37
11/21/2021 11:14:08 - INFO - __main__ -   Batch number = 38
11/21/2021 11:14:08 - INFO - __main__ -   Batch number = 39
11/21/2021 11:14:08 - INFO - __main__ -   Batch number = 40
11/21/2021 11:14:08 - INFO - __main__ -   Batch number = 41
11/21/2021 11:14:08 - INFO - __main__ -   Batch number = 42
11/21/2021 11:14:08 - INFO - __main__ -   Batch number = 43
11/21/2021 11:14:08 - INFO - __main__ -   Batch number = 44
11/21/2021 11:14:09 - INFO - __main__ -   Batch number = 45
11/21/2021 11:14:09 - INFO - __main__ -   Batch number = 46
11/21/2021 11:14:09 - INFO - __main__ -   Batch number = 47
11/21/2021 11:14:09 - INFO - __main__ -   Batch number = 48
11/21/2021 11:14:09 - INFO - __main__ -   Batch number = 49
11/21/2021 11:14:09 - INFO - __main__ -   Batch number = 50
11/21/2021 11:14:09 - INFO - __main__ -   Batch number = 51
11/21/2021 11:14:10 - INFO - __main__ -   Batch number = 52
11/21/2021 11:14:10 - INFO - __main__ -   Batch number = 53
11/21/2021 11:14:10 - INFO - __main__ -   Batch number = 54
11/21/2021 11:14:10 - INFO - __main__ -   Batch number = 55
11/21/2021 11:14:10 - INFO - __main__ -   Batch number = 56
11/21/2021 11:14:11 - INFO - __main__ -   ***** Evaluation result  in ar *****
11/21/2021 11:14:11 - INFO - __main__ -     f1 = 0.5580247548295053
11/21/2021 11:14:11 - INFO - __main__ -     loss = 1.5517014392784663
11/21/2021 11:14:11 - INFO - __main__ -     precision = 0.5448795180722892
11/21/2021 11:14:11 - INFO - __main__ -     recall = 0.5718199299317757
11/21/2021 11:14:13 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ar', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/21/2021 11:14:13 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/21/2021 11:14:13 - INFO - __main__ -   Seed = 2
11/21/2021 11:14:13 - INFO - root -   save model
11/21/2021 11:14:13 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ar', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/21/2021 11:14:13 - INFO - __main__ -   Loading pretrained model and tokenizer
11/21/2021 11:14:16 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
11/21/2021 11:14:21 - INFO - __main__ -   Using lang2id = None
11/21/2021 11:14:21 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/21/2021 11:14:21 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
11/21/2021 11:14:21 - INFO - root -   Trying to decide if add adapter
11/21/2021 11:14:21 - INFO - root -   loading task adapter
11/21/2021 11:14:21 - INFO - root -   loading lang adpater bn/wiki@ukp
11/21/2021 11:14:21 - INFO - __main__ -   Adapter Languages : ['bn'], Length : 1
11/21/2021 11:14:21 - INFO - __main__ -   Adapter Names ['bn/wiki@ukp'], Length : 1
11/21/2021 11:14:21 - INFO - __main__ -   Language = bn
11/21/2021 11:14:21 - INFO - __main__ -   Adapter Name = bn/wiki@ukp
11/21/2021 11:14:26 - INFO - __main__ -   Language adapter for ar not found, using bn instead
11/21/2021 11:14:26 - INFO - __main__ -   Set active language adapter to bn
11/21/2021 11:14:26 - INFO - __main__ -   Args Adapter Weight = None
11/21/2021 11:14:26 - INFO - __main__ -   Adapter Languages = ['bn']
11/21/2021 11:14:26 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_ar_bert-base-multilingual-cased_128
11/21/2021 11:14:26 - INFO - __main__ -   ***** Running evaluation  in ar *****
11/21/2021 11:14:26 - INFO - __main__ -     Num examples = 1784
11/21/2021 11:14:26 - INFO - __main__ -     Batch size = 32
11/21/2021 11:14:26 - INFO - __main__ -   Batch number = 1
11/21/2021 11:14:26 - INFO - __main__ -   Batch number = 2
11/21/2021 11:14:27 - INFO - __main__ -   Batch number = 3
11/21/2021 11:14:27 - INFO - __main__ -   Batch number = 4
11/21/2021 11:14:27 - INFO - __main__ -   Batch number = 5
11/21/2021 11:14:27 - INFO - __main__ -   Batch number = 6
11/21/2021 11:14:27 - INFO - __main__ -   Batch number = 7
11/21/2021 11:14:27 - INFO - __main__ -   Batch number = 8
11/21/2021 11:14:27 - INFO - __main__ -   Batch number = 9
11/21/2021 11:14:28 - INFO - __main__ -   Batch number = 10
11/21/2021 11:14:28 - INFO - __main__ -   Batch number = 11
11/21/2021 11:14:28 - INFO - __main__ -   Batch number = 12
11/21/2021 11:14:28 - INFO - __main__ -   Batch number = 13
11/21/2021 11:14:28 - INFO - __main__ -   Batch number = 14
11/21/2021 11:14:28 - INFO - __main__ -   Batch number = 15
11/21/2021 11:14:29 - INFO - __main__ -   Batch number = 16
11/21/2021 11:14:29 - INFO - __main__ -   Batch number = 17
11/21/2021 11:14:29 - INFO - __main__ -   Batch number = 18
11/21/2021 11:14:29 - INFO - __main__ -   Batch number = 19
11/21/2021 11:14:29 - INFO - __main__ -   Batch number = 20
11/21/2021 11:14:29 - INFO - __main__ -   Batch number = 21
11/21/2021 11:14:29 - INFO - __main__ -   Batch number = 22
11/21/2021 11:14:30 - INFO - __main__ -   Batch number = 23
11/21/2021 11:14:30 - INFO - __main__ -   Batch number = 24
11/21/2021 11:14:30 - INFO - __main__ -   Batch number = 25
11/21/2021 11:14:30 - INFO - __main__ -   Batch number = 26
11/21/2021 11:14:30 - INFO - __main__ -   Batch number = 27
11/21/2021 11:14:30 - INFO - __main__ -   Batch number = 28
11/21/2021 11:14:30 - INFO - __main__ -   Batch number = 29
11/21/2021 11:14:30 - INFO - __main__ -   Batch number = 30
11/21/2021 11:14:31 - INFO - __main__ -   Batch number = 31
11/21/2021 11:14:31 - INFO - __main__ -   Batch number = 32
11/21/2021 11:14:31 - INFO - __main__ -   Batch number = 33
11/21/2021 11:14:31 - INFO - __main__ -   Batch number = 34
11/21/2021 11:14:31 - INFO - __main__ -   Batch number = 35
11/21/2021 11:14:31 - INFO - __main__ -   Batch number = 36
11/21/2021 11:14:31 - INFO - __main__ -   Batch number = 37
11/21/2021 11:14:32 - INFO - __main__ -   Batch number = 38
11/21/2021 11:14:32 - INFO - __main__ -   Batch number = 39
11/21/2021 11:14:32 - INFO - __main__ -   Batch number = 40
11/21/2021 11:14:32 - INFO - __main__ -   Batch number = 41
11/21/2021 11:14:32 - INFO - __main__ -   Batch number = 42
11/21/2021 11:14:32 - INFO - __main__ -   Batch number = 43
11/21/2021 11:14:32 - INFO - __main__ -   Batch number = 44
11/21/2021 11:14:33 - INFO - __main__ -   Batch number = 45
11/21/2021 11:14:33 - INFO - __main__ -   Batch number = 46
11/21/2021 11:14:33 - INFO - __main__ -   Batch number = 47
11/21/2021 11:14:33 - INFO - __main__ -   Batch number = 48
11/21/2021 11:14:33 - INFO - __main__ -   Batch number = 49
11/21/2021 11:14:33 - INFO - __main__ -   Batch number = 50
11/21/2021 11:14:33 - INFO - __main__ -   Batch number = 51
11/21/2021 11:14:34 - INFO - __main__ -   Batch number = 52
11/21/2021 11:14:34 - INFO - __main__ -   Batch number = 53
11/21/2021 11:14:34 - INFO - __main__ -   Batch number = 54
11/21/2021 11:14:34 - INFO - __main__ -   Batch number = 55
11/21/2021 11:14:34 - INFO - __main__ -   Batch number = 56
11/21/2021 11:14:35 - INFO - __main__ -   ***** Evaluation result  in ar *****
11/21/2021 11:14:35 - INFO - __main__ -     f1 = 0.5548093837876705
11/21/2021 11:14:35 - INFO - __main__ -     loss = 1.8253483804208892
11/21/2021 11:14:35 - INFO - __main__ -     precision = 0.5412365968533921
11/21/2021 11:14:35 - INFO - __main__ -     recall = 0.5690804204093459
11/21/2021 11:14:37 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ar', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/21/2021 11:14:37 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/21/2021 11:14:37 - INFO - __main__ -   Seed = 3
11/21/2021 11:14:37 - INFO - root -   save model
11/21/2021 11:14:37 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ar', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/21/2021 11:14:37 - INFO - __main__ -   Loading pretrained model and tokenizer
11/21/2021 11:14:40 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
11/21/2021 11:14:45 - INFO - __main__ -   Using lang2id = None
11/21/2021 11:14:45 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/21/2021 11:14:45 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
11/21/2021 11:14:45 - INFO - root -   Trying to decide if add adapter
11/21/2021 11:14:45 - INFO - root -   loading task adapter
11/21/2021 11:14:45 - INFO - root -   loading lang adpater bn/wiki@ukp
11/21/2021 11:14:45 - INFO - __main__ -   Adapter Languages : ['bn'], Length : 1
11/21/2021 11:14:45 - INFO - __main__ -   Adapter Names ['bn/wiki@ukp'], Length : 1
11/21/2021 11:14:45 - INFO - __main__ -   Language = bn
11/21/2021 11:14:45 - INFO - __main__ -   Adapter Name = bn/wiki@ukp
11/21/2021 11:14:50 - INFO - __main__ -   Language adapter for ar not found, using bn instead
11/21/2021 11:14:50 - INFO - __main__ -   Set active language adapter to bn
11/21/2021 11:14:50 - INFO - __main__ -   Args Adapter Weight = None
11/21/2021 11:14:50 - INFO - __main__ -   Adapter Languages = ['bn']
11/21/2021 11:14:50 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_ar_bert-base-multilingual-cased_128
11/21/2021 11:14:50 - INFO - __main__ -   ***** Running evaluation  in ar *****
11/21/2021 11:14:50 - INFO - __main__ -     Num examples = 1784
11/21/2021 11:14:50 - INFO - __main__ -     Batch size = 32
11/21/2021 11:14:50 - INFO - __main__ -   Batch number = 1
11/21/2021 11:14:50 - INFO - __main__ -   Batch number = 2
11/21/2021 11:14:50 - INFO - __main__ -   Batch number = 3
11/21/2021 11:14:51 - INFO - __main__ -   Batch number = 4
11/21/2021 11:14:51 - INFO - __main__ -   Batch number = 5
11/21/2021 11:14:51 - INFO - __main__ -   Batch number = 6
11/21/2021 11:14:51 - INFO - __main__ -   Batch number = 7
11/21/2021 11:14:51 - INFO - __main__ -   Batch number = 8
11/21/2021 11:14:51 - INFO - __main__ -   Batch number = 9
11/21/2021 11:14:51 - INFO - __main__ -   Batch number = 10
11/21/2021 11:14:52 - INFO - __main__ -   Batch number = 11
11/21/2021 11:14:52 - INFO - __main__ -   Batch number = 12
11/21/2021 11:14:52 - INFO - __main__ -   Batch number = 13
11/21/2021 11:14:52 - INFO - __main__ -   Batch number = 14
11/21/2021 11:14:52 - INFO - __main__ -   Batch number = 15
11/21/2021 11:14:52 - INFO - __main__ -   Batch number = 16
11/21/2021 11:14:52 - INFO - __main__ -   Batch number = 17
11/21/2021 11:14:53 - INFO - __main__ -   Batch number = 18
11/21/2021 11:14:53 - INFO - __main__ -   Batch number = 19
11/21/2021 11:14:53 - INFO - __main__ -   Batch number = 20
11/21/2021 11:14:53 - INFO - __main__ -   Batch number = 21
11/21/2021 11:14:53 - INFO - __main__ -   Batch number = 22
11/21/2021 11:14:53 - INFO - __main__ -   Batch number = 23
11/21/2021 11:14:53 - INFO - __main__ -   Batch number = 24
11/21/2021 11:14:53 - INFO - __main__ -   Batch number = 25
11/21/2021 11:14:54 - INFO - __main__ -   Batch number = 26
11/21/2021 11:14:54 - INFO - __main__ -   Batch number = 27
11/21/2021 11:14:54 - INFO - __main__ -   Batch number = 28
11/21/2021 11:14:54 - INFO - __main__ -   Batch number = 29
11/21/2021 11:14:54 - INFO - __main__ -   Batch number = 30
11/21/2021 11:14:54 - INFO - __main__ -   Batch number = 31
11/21/2021 11:14:54 - INFO - __main__ -   Batch number = 32
11/21/2021 11:14:55 - INFO - __main__ -   Batch number = 33
11/21/2021 11:14:55 - INFO - __main__ -   Batch number = 34
11/21/2021 11:14:55 - INFO - __main__ -   Batch number = 35
11/21/2021 11:14:55 - INFO - __main__ -   Batch number = 36
11/21/2021 11:14:55 - INFO - __main__ -   Batch number = 37
11/21/2021 11:14:55 - INFO - __main__ -   Batch number = 38
11/21/2021 11:14:55 - INFO - __main__ -   Batch number = 39
11/21/2021 11:14:56 - INFO - __main__ -   Batch number = 40
11/21/2021 11:14:56 - INFO - __main__ -   Batch number = 41
11/21/2021 11:14:56 - INFO - __main__ -   Batch number = 42
11/21/2021 11:14:56 - INFO - __main__ -   Batch number = 43
11/21/2021 11:14:56 - INFO - __main__ -   Batch number = 44
11/21/2021 11:14:56 - INFO - __main__ -   Batch number = 45
11/21/2021 11:14:56 - INFO - __main__ -   Batch number = 46
11/21/2021 11:14:57 - INFO - __main__ -   Batch number = 47
11/21/2021 11:14:57 - INFO - __main__ -   Batch number = 48
11/21/2021 11:14:57 - INFO - __main__ -   Batch number = 49
11/21/2021 11:14:57 - INFO - __main__ -   Batch number = 50
11/21/2021 11:14:57 - INFO - __main__ -   Batch number = 51
11/21/2021 11:14:57 - INFO - __main__ -   Batch number = 52
11/21/2021 11:14:57 - INFO - __main__ -   Batch number = 53
11/21/2021 11:14:58 - INFO - __main__ -   Batch number = 54
11/21/2021 11:14:58 - INFO - __main__ -   Batch number = 55
11/21/2021 11:14:58 - INFO - __main__ -   Batch number = 56
11/21/2021 11:14:59 - INFO - __main__ -   ***** Evaluation result  in ar *****
11/21/2021 11:14:59 - INFO - __main__ -     f1 = 0.5609145289163974
11/21/2021 11:14:59 - INFO - __main__ -     loss = 1.458224597786154
11/21/2021 11:14:59 - INFO - __main__ -     precision = 0.552731178396072
11/21/2021 11:14:59 - INFO - __main__ -     recall = 0.5693438347865026
11/28/2021 01:05:58 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='cdo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:05:58 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:05:58 - INFO - __main__ -   Seed = 1
11/28/2021 01:05:58 - INFO - root -   save model
11/28/2021 01:05:58 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='cdo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:05:58 - INFO - __main__ -   Loading pretrained model and tokenizer
11/28/2021 01:06:01 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
11/28/2021 01:06:04 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='hi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:06:04 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:06:04 - INFO - __main__ -   Seed = 1
11/28/2021 01:06:04 - INFO - root -   save model
11/28/2021 01:06:04 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='hi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:06:04 - INFO - __main__ -   Loading pretrained model and tokenizer
11/28/2021 01:06:06 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
11/28/2021 01:06:07 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:06:07 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:06:07 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
11/28/2021 01:06:07 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:06:07 - INFO - root -   loading task adapter
11/28/2021 01:06:07 - INFO - root -   loading lang adpater bn/wiki@ukp
11/28/2021 01:06:07 - INFO - __main__ -   Adapter Languages : ['bn'], Length : 1
11/28/2021 01:06:07 - INFO - __main__ -   Adapter Names ['bn/wiki@ukp'], Length : 1
11/28/2021 01:06:07 - INFO - __main__ -   Language = bn
11/28/2021 01:06:07 - INFO - __main__ -   Adapter Name = bn/wiki@ukp
11/28/2021 01:06:12 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:06:12 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:06:12 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
11/28/2021 01:06:12 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:06:12 - INFO - root -   loading task adapter
11/28/2021 01:06:12 - INFO - root -   loading lang adpater bn/wiki@ukp
11/28/2021 01:06:12 - INFO - __main__ -   Adapter Languages : ['bn'], Length : 1
11/28/2021 01:06:12 - INFO - __main__ -   Adapter Names ['bn/wiki@ukp'], Length : 1
11/28/2021 01:06:12 - INFO - __main__ -   Language = bn
11/28/2021 01:06:12 - INFO - __main__ -   Adapter Name = bn/wiki@ukp
11/28/2021 01:06:16 - INFO - __main__ -   Language cdo, split test does not exist
11/28/2021 01:06:18 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='cdo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:06:18 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:06:18 - INFO - __main__ -   Seed = 2
11/28/2021 01:06:18 - INFO - root -   save model
11/28/2021 01:06:18 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='cdo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:06:18 - INFO - __main__ -   Loading pretrained model and tokenizer
11/28/2021 01:06:21 - INFO - __main__ -   Language adapter for hi not found, using bn instead
11/28/2021 01:06:21 - INFO - __main__ -   Set active language adapter to bn
11/28/2021 01:06:21 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:06:21 - INFO - __main__ -   Adapter Languages = ['bn']
11/28/2021 01:06:21 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_hi_bert-base-multilingual-cased_128
11/28/2021 01:06:21 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
11/28/2021 01:06:22 - INFO - __main__ -   ***** Running evaluation  in hi *****
11/28/2021 01:06:22 - INFO - __main__ -     Num examples = 2685
11/28/2021 01:06:22 - INFO - __main__ -     Batch size = 32
11/28/2021 01:06:22 - INFO - __main__ -   Batch number = 1
11/28/2021 01:06:22 - INFO - __main__ -   Batch number = 2
11/28/2021 01:06:22 - INFO - __main__ -   Batch number = 3
11/28/2021 01:06:23 - INFO - __main__ -   Batch number = 4
11/28/2021 01:06:23 - INFO - __main__ -   Batch number = 5
11/28/2021 01:06:23 - INFO - __main__ -   Batch number = 6
11/28/2021 01:06:24 - INFO - __main__ -   Batch number = 7
11/28/2021 01:06:24 - INFO - __main__ -   Batch number = 8
11/28/2021 01:06:24 - INFO - __main__ -   Batch number = 9
11/28/2021 01:06:25 - INFO - __main__ -   Batch number = 10
11/28/2021 01:06:25 - INFO - __main__ -   Batch number = 11
11/28/2021 01:06:25 - INFO - __main__ -   Batch number = 12
11/28/2021 01:06:26 - INFO - __main__ -   Batch number = 13
11/28/2021 01:06:26 - INFO - __main__ -   Batch number = 14
11/28/2021 01:06:26 - INFO - __main__ -   Batch number = 15
11/28/2021 01:06:26 - INFO - __main__ -   Batch number = 16
11/28/2021 01:06:27 - INFO - __main__ -   Batch number = 17
11/28/2021 01:06:27 - INFO - __main__ -   Batch number = 18
11/28/2021 01:06:27 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:06:27 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:06:27 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
11/28/2021 01:06:27 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:06:27 - INFO - root -   loading task adapter
11/28/2021 01:06:27 - INFO - root -   loading lang adpater bn/wiki@ukp
11/28/2021 01:06:27 - INFO - __main__ -   Adapter Languages : ['bn'], Length : 1
11/28/2021 01:06:27 - INFO - __main__ -   Adapter Names ['bn/wiki@ukp'], Length : 1
11/28/2021 01:06:27 - INFO - __main__ -   Language = bn
11/28/2021 01:06:27 - INFO - __main__ -   Adapter Name = bn/wiki@ukp
11/28/2021 01:06:27 - INFO - __main__ -   Batch number = 19
11/28/2021 01:06:28 - INFO - __main__ -   Batch number = 20
11/28/2021 01:06:28 - INFO - __main__ -   Batch number = 21
11/28/2021 01:06:28 - INFO - __main__ -   Batch number = 22
11/28/2021 01:06:29 - INFO - __main__ -   Batch number = 23
11/28/2021 01:06:29 - INFO - __main__ -   Batch number = 24
11/28/2021 01:06:29 - INFO - __main__ -   Batch number = 25
11/28/2021 01:06:30 - INFO - __main__ -   Batch number = 26
11/28/2021 01:06:30 - INFO - __main__ -   Batch number = 27
11/28/2021 01:06:30 - INFO - __main__ -   Batch number = 28
11/28/2021 01:06:31 - INFO - __main__ -   Batch number = 29
11/28/2021 01:06:31 - INFO - __main__ -   Batch number = 30
11/28/2021 01:06:31 - INFO - __main__ -   Batch number = 31
11/28/2021 01:06:32 - INFO - __main__ -   Batch number = 32
11/28/2021 01:06:32 - INFO - __main__ -   Batch number = 33
11/28/2021 01:06:32 - INFO - __main__ -   Batch number = 34
11/28/2021 01:06:33 - INFO - __main__ -   Batch number = 35
11/28/2021 01:06:33 - INFO - __main__ -   Batch number = 36
11/28/2021 01:06:33 - INFO - __main__ -   Batch number = 37
11/28/2021 01:06:34 - INFO - __main__ -   Batch number = 38
11/28/2021 01:06:34 - INFO - __main__ -   Batch number = 39
11/28/2021 01:06:34 - INFO - __main__ -   Batch number = 40
11/28/2021 01:06:35 - INFO - __main__ -   Batch number = 41
11/28/2021 01:06:35 - INFO - __main__ -   Batch number = 42
11/28/2021 01:06:35 - INFO - __main__ -   Batch number = 43
11/28/2021 01:06:36 - INFO - __main__ -   Batch number = 44
11/28/2021 01:06:36 - INFO - __main__ -   Batch number = 45
11/28/2021 01:06:36 - INFO - __main__ -   Batch number = 46
11/28/2021 01:06:36 - INFO - __main__ -   Batch number = 47
11/28/2021 01:06:37 - INFO - __main__ -   Batch number = 48
11/28/2021 01:06:37 - INFO - __main__ -   Batch number = 49
11/28/2021 01:06:37 - INFO - __main__ -   Language cdo, split test does not exist
11/28/2021 01:06:37 - INFO - __main__ -   Batch number = 50
11/28/2021 01:06:38 - INFO - __main__ -   Batch number = 51
11/28/2021 01:06:38 - INFO - __main__ -   Batch number = 52
11/28/2021 01:06:38 - INFO - __main__ -   Batch number = 53
11/28/2021 01:06:39 - INFO - __main__ -   Batch number = 54
11/28/2021 01:06:39 - INFO - __main__ -   Batch number = 55
11/28/2021 01:06:39 - INFO - __main__ -   Batch number = 56
11/28/2021 01:06:40 - INFO - __main__ -   Batch number = 57
11/28/2021 01:06:40 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='cdo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:06:40 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:06:40 - INFO - __main__ -   Seed = 3
11/28/2021 01:06:40 - INFO - root -   save model
11/28/2021 01:06:40 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='cdo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:06:40 - INFO - __main__ -   Loading pretrained model and tokenizer
11/28/2021 01:06:40 - INFO - __main__ -   Batch number = 58
11/28/2021 01:06:40 - INFO - __main__ -   Batch number = 59
11/28/2021 01:06:41 - INFO - __main__ -   Batch number = 60
11/28/2021 01:06:41 - INFO - __main__ -   Batch number = 61
11/28/2021 01:06:41 - INFO - __main__ -   Batch number = 62
11/28/2021 01:06:42 - INFO - __main__ -   Batch number = 63
11/28/2021 01:06:42 - INFO - __main__ -   Batch number = 64
11/28/2021 01:06:42 - INFO - __main__ -   Batch number = 65
11/28/2021 01:06:43 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
11/28/2021 01:06:43 - INFO - __main__ -   Batch number = 66
11/28/2021 01:06:43 - INFO - __main__ -   Batch number = 67
11/28/2021 01:06:43 - INFO - __main__ -   Batch number = 68
11/28/2021 01:06:44 - INFO - __main__ -   Batch number = 69
11/28/2021 01:06:44 - INFO - __main__ -   Batch number = 70
11/28/2021 01:06:44 - INFO - __main__ -   Batch number = 71
11/28/2021 01:06:45 - INFO - __main__ -   Batch number = 72
11/28/2021 01:06:45 - INFO - __main__ -   Batch number = 73
11/28/2021 01:06:45 - INFO - __main__ -   Batch number = 74
11/28/2021 01:06:46 - INFO - __main__ -   Batch number = 75
11/28/2021 01:06:46 - INFO - __main__ -   Batch number = 76
11/28/2021 01:06:46 - INFO - __main__ -   Batch number = 77
11/28/2021 01:06:46 - INFO - __main__ -   Batch number = 78
11/28/2021 01:06:47 - INFO - __main__ -   Batch number = 79
11/28/2021 01:06:47 - INFO - __main__ -   Batch number = 80
11/28/2021 01:06:47 - INFO - __main__ -   Batch number = 81
11/28/2021 01:06:48 - INFO - __main__ -   Batch number = 82
11/28/2021 01:06:48 - INFO - __main__ -   Batch number = 83
11/28/2021 01:06:48 - INFO - __main__ -   Batch number = 84
11/28/2021 01:06:49 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:06:49 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:06:49 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
11/28/2021 01:06:49 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:06:49 - INFO - root -   loading task adapter
11/28/2021 01:06:49 - INFO - root -   loading lang adpater bn/wiki@ukp
11/28/2021 01:06:49 - INFO - __main__ -   Adapter Languages : ['bn'], Length : 1
11/28/2021 01:06:49 - INFO - __main__ -   Adapter Names ['bn/wiki@ukp'], Length : 1
11/28/2021 01:06:49 - INFO - __main__ -   Language = bn
11/28/2021 01:06:49 - INFO - __main__ -   Adapter Name = bn/wiki@ukp
11/28/2021 01:06:50 - INFO - __main__ -   ***** Evaluation result  in hi *****
11/28/2021 01:06:50 - INFO - __main__ -     f1 = 0.6349727720632724
11/28/2021 01:06:50 - INFO - __main__ -     loss = 1.2339117505720683
11/28/2021 01:06:50 - INFO - __main__ -     precision = 0.6368259839334219
11/28/2021 01:06:50 - INFO - __main__ -     recall = 0.6331303148701448
11/28/2021 01:06:50 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='gn', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:06:50 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:06:50 - INFO - __main__ -   Seed = 1
11/28/2021 01:06:50 - INFO - root -   save model
11/28/2021 01:06:50 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='gn', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:06:50 - INFO - __main__ -   Loading pretrained model and tokenizer
11/28/2021 01:06:53 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='hi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:06:53 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:06:53 - INFO - __main__ -   Seed = 2
11/28/2021 01:06:53 - INFO - root -   save model
11/28/2021 01:06:53 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='hi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:06:53 - INFO - __main__ -   Loading pretrained model and tokenizer
11/28/2021 01:06:53 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
11/28/2021 01:06:55 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
11/28/2021 01:06:57 - INFO - __main__ -   Language cdo, split test does not exist
11/28/2021 01:06:59 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:06:59 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:06:59 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
11/28/2021 01:06:59 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:06:59 - INFO - root -   loading task adapter
11/28/2021 01:06:59 - INFO - root -   loading lang adpater bn/wiki@ukp
11/28/2021 01:06:59 - INFO - __main__ -   Adapter Languages : ['bn'], Length : 1
11/28/2021 01:06:59 - INFO - __main__ -   Adapter Names ['bn/wiki@ukp'], Length : 1
11/28/2021 01:06:59 - INFO - __main__ -   Language = bn
11/28/2021 01:06:59 - INFO - __main__ -   Adapter Name = bn/wiki@ukp
11/28/2021 01:07:02 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:07:02 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:07:02 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
11/28/2021 01:07:02 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:07:02 - INFO - root -   loading task adapter
11/28/2021 01:07:02 - INFO - root -   loading lang adpater bn/wiki@ukp
11/28/2021 01:07:02 - INFO - __main__ -   Adapter Languages : ['bn'], Length : 1
11/28/2021 01:07:02 - INFO - __main__ -   Adapter Names ['bn/wiki@ukp'], Length : 1
11/28/2021 01:07:02 - INFO - __main__ -   Language = bn
11/28/2021 01:07:02 - INFO - __main__ -   Adapter Name = bn/wiki@ukp
11/28/2021 01:07:08 - INFO - __main__ -   Language gn, split test does not exist
11/28/2021 01:07:09 - INFO - __main__ -   Language adapter for hi not found, using bn instead
11/28/2021 01:07:09 - INFO - __main__ -   Set active language adapter to bn
11/28/2021 01:07:09 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:07:09 - INFO - __main__ -   Adapter Languages = ['bn']
11/28/2021 01:07:09 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_hi_bert-base-multilingual-cased_128
11/28/2021 01:07:10 - INFO - __main__ -   ***** Running evaluation  in hi *****
11/28/2021 01:07:10 - INFO - __main__ -     Num examples = 2685
11/28/2021 01:07:10 - INFO - __main__ -     Batch size = 32
11/28/2021 01:07:10 - INFO - __main__ -   Batch number = 1
11/28/2021 01:07:10 - INFO - __main__ -   Batch number = 2
11/28/2021 01:07:10 - INFO - __main__ -   Batch number = 3
11/28/2021 01:07:10 - INFO - __main__ -   Batch number = 4
11/28/2021 01:07:10 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='gn', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:07:10 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:07:10 - INFO - __main__ -   Seed = 2
11/28/2021 01:07:10 - INFO - root -   save model
11/28/2021 01:07:10 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='gn', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:07:10 - INFO - __main__ -   Loading pretrained model and tokenizer
11/28/2021 01:07:10 - INFO - __main__ -   Batch number = 5
11/28/2021 01:07:10 - INFO - __main__ -   Batch number = 6
11/28/2021 01:07:11 - INFO - __main__ -   Batch number = 7
11/28/2021 01:07:11 - INFO - __main__ -   Batch number = 8
11/28/2021 01:07:11 - INFO - __main__ -   Batch number = 9
11/28/2021 01:07:11 - INFO - __main__ -   Batch number = 10
11/28/2021 01:07:11 - INFO - __main__ -   Batch number = 11
11/28/2021 01:07:11 - INFO - __main__ -   Batch number = 12
11/28/2021 01:07:12 - INFO - __main__ -   Batch number = 13
11/28/2021 01:07:12 - INFO - __main__ -   Batch number = 14
11/28/2021 01:07:12 - INFO - __main__ -   Batch number = 15
11/28/2021 01:07:12 - INFO - __main__ -   Batch number = 16
11/28/2021 01:07:12 - INFO - __main__ -   Batch number = 17
11/28/2021 01:07:12 - INFO - __main__ -   Batch number = 18
11/28/2021 01:07:12 - INFO - __main__ -   Batch number = 19
11/28/2021 01:07:13 - INFO - __main__ -   Batch number = 20
11/28/2021 01:07:13 - INFO - __main__ -   Batch number = 21
11/28/2021 01:07:13 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
11/28/2021 01:07:13 - INFO - __main__ -   Batch number = 22
11/28/2021 01:07:13 - INFO - __main__ -   Batch number = 23
11/28/2021 01:07:13 - INFO - __main__ -   Batch number = 24
11/28/2021 01:07:13 - INFO - __main__ -   Batch number = 25
11/28/2021 01:07:14 - INFO - __main__ -   Batch number = 26
11/28/2021 01:07:14 - INFO - __main__ -   Batch number = 27
11/28/2021 01:07:14 - INFO - __main__ -   Batch number = 28
11/28/2021 01:07:14 - INFO - __main__ -   Batch number = 29
11/28/2021 01:07:14 - INFO - __main__ -   Batch number = 30
11/28/2021 01:07:14 - INFO - __main__ -   Batch number = 31
11/28/2021 01:07:15 - INFO - __main__ -   Batch number = 32
11/28/2021 01:07:15 - INFO - __main__ -   Batch number = 33
11/28/2021 01:07:15 - INFO - __main__ -   Batch number = 34
11/28/2021 01:07:15 - INFO - __main__ -   Batch number = 35
11/28/2021 01:07:15 - INFO - __main__ -   Batch number = 36
11/28/2021 01:07:15 - INFO - __main__ -   Batch number = 37
11/28/2021 01:07:15 - INFO - __main__ -   Batch number = 38
11/28/2021 01:07:16 - INFO - __main__ -   Batch number = 39
11/28/2021 01:07:16 - INFO - __main__ -   Batch number = 40
11/28/2021 01:07:16 - INFO - __main__ -   Batch number = 41
11/28/2021 01:07:16 - INFO - __main__ -   Batch number = 42
11/28/2021 01:07:16 - INFO - __main__ -   Batch number = 43
11/28/2021 01:07:16 - INFO - __main__ -   Batch number = 44
11/28/2021 01:07:17 - INFO - __main__ -   Batch number = 45
11/28/2021 01:07:17 - INFO - __main__ -   Batch number = 46
11/28/2021 01:07:17 - INFO - __main__ -   Batch number = 47
11/28/2021 01:07:17 - INFO - __main__ -   Batch number = 48
11/28/2021 01:07:17 - INFO - __main__ -   Batch number = 49
11/28/2021 01:07:17 - INFO - __main__ -   Batch number = 50
11/28/2021 01:07:18 - INFO - __main__ -   Batch number = 51
11/28/2021 01:07:18 - INFO - __main__ -   Batch number = 52
11/28/2021 01:07:18 - INFO - __main__ -   Batch number = 53
11/28/2021 01:07:18 - INFO - __main__ -   Batch number = 54
11/28/2021 01:07:18 - INFO - __main__ -   Batch number = 55
11/28/2021 01:07:18 - INFO - __main__ -   Batch number = 56
11/28/2021 01:07:18 - INFO - __main__ -   Batch number = 57
11/28/2021 01:07:19 - INFO - __main__ -   Batch number = 58
11/28/2021 01:07:19 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:07:19 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:07:19 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
11/28/2021 01:07:19 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:07:19 - INFO - root -   loading task adapter
11/28/2021 01:07:19 - INFO - root -   loading lang adpater bn/wiki@ukp
11/28/2021 01:07:19 - INFO - __main__ -   Adapter Languages : ['bn'], Length : 1
11/28/2021 01:07:19 - INFO - __main__ -   Adapter Names ['bn/wiki@ukp'], Length : 1
11/28/2021 01:07:19 - INFO - __main__ -   Language = bn
11/28/2021 01:07:19 - INFO - __main__ -   Adapter Name = bn/wiki@ukp
11/28/2021 01:07:19 - INFO - __main__ -   Batch number = 59
11/28/2021 01:07:19 - INFO - __main__ -   Batch number = 60
11/28/2021 01:07:19 - INFO - __main__ -   Batch number = 61
11/28/2021 01:07:20 - INFO - __main__ -   Batch number = 62
11/28/2021 01:07:20 - INFO - __main__ -   Batch number = 63
11/28/2021 01:07:20 - INFO - __main__ -   Batch number = 64
11/28/2021 01:07:20 - INFO - __main__ -   Batch number = 65
11/28/2021 01:07:20 - INFO - __main__ -   Batch number = 66
11/28/2021 01:07:20 - INFO - __main__ -   Batch number = 67
11/28/2021 01:07:20 - INFO - __main__ -   Batch number = 68
11/28/2021 01:07:21 - INFO - __main__ -   Batch number = 69
11/28/2021 01:07:21 - INFO - __main__ -   Batch number = 70
11/28/2021 01:07:21 - INFO - __main__ -   Batch number = 71
11/28/2021 01:07:21 - INFO - __main__ -   Batch number = 72
11/28/2021 01:07:21 - INFO - __main__ -   Batch number = 73
11/28/2021 01:07:21 - INFO - __main__ -   Batch number = 74
11/28/2021 01:07:21 - INFO - __main__ -   Batch number = 75
11/28/2021 01:07:22 - INFO - __main__ -   Batch number = 76
11/28/2021 01:07:22 - INFO - __main__ -   Batch number = 77
11/28/2021 01:07:22 - INFO - __main__ -   Batch number = 78
11/28/2021 01:07:22 - INFO - __main__ -   Batch number = 79
11/28/2021 01:07:22 - INFO - __main__ -   Batch number = 80
11/28/2021 01:07:22 - INFO - __main__ -   Batch number = 81
11/28/2021 01:07:22 - INFO - __main__ -   Batch number = 82
11/28/2021 01:07:23 - INFO - __main__ -   Batch number = 83
11/28/2021 01:07:23 - INFO - __main__ -   Batch number = 84
11/28/2021 01:07:24 - INFO - __main__ -   ***** Evaluation result  in hi *****
11/28/2021 01:07:24 - INFO - __main__ -     f1 = 0.6212333465369493
11/28/2021 01:07:24 - INFO - __main__ -     loss = 1.3100808731147222
11/28/2021 01:07:24 - INFO - __main__ -     precision = 0.62678623647529
11/28/2021 01:07:24 - INFO - __main__ -     recall = 0.6157779820730866
11/28/2021 01:07:26 - INFO - __main__ -   Language gn, split test does not exist
11/28/2021 01:07:27 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='hi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:07:27 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:07:27 - INFO - __main__ -   Seed = 3
11/28/2021 01:07:27 - INFO - root -   save model
11/28/2021 01:07:27 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='hi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:07:27 - INFO - __main__ -   Loading pretrained model and tokenizer
11/28/2021 01:07:29 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='gn', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:07:29 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:07:29 - INFO - __main__ -   Seed = 3
11/28/2021 01:07:29 - INFO - root -   save model
11/28/2021 01:07:29 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='gn', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:07:29 - INFO - __main__ -   Loading pretrained model and tokenizer
11/28/2021 01:07:30 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
11/28/2021 01:07:31 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
11/28/2021 01:07:36 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:07:36 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:07:36 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
11/28/2021 01:07:36 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:07:36 - INFO - root -   loading task adapter
11/28/2021 01:07:36 - INFO - root -   loading lang adpater bn/wiki@ukp
11/28/2021 01:07:36 - INFO - __main__ -   Adapter Languages : ['bn'], Length : 1
11/28/2021 01:07:36 - INFO - __main__ -   Adapter Names ['bn/wiki@ukp'], Length : 1
11/28/2021 01:07:36 - INFO - __main__ -   Language = bn
11/28/2021 01:07:36 - INFO - __main__ -   Adapter Name = bn/wiki@ukp
11/28/2021 01:07:38 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:07:38 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:07:38 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
11/28/2021 01:07:38 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:07:38 - INFO - root -   loading task adapter
11/28/2021 01:07:38 - INFO - root -   loading lang adpater bn/wiki@ukp
11/28/2021 01:07:38 - INFO - __main__ -   Adapter Languages : ['bn'], Length : 1
11/28/2021 01:07:38 - INFO - __main__ -   Adapter Names ['bn/wiki@ukp'], Length : 1
11/28/2021 01:07:38 - INFO - __main__ -   Language = bn
11/28/2021 01:07:38 - INFO - __main__ -   Adapter Name = bn/wiki@ukp
11/28/2021 01:07:45 - INFO - __main__ -   Language adapter for hi not found, using bn instead
11/28/2021 01:07:45 - INFO - __main__ -   Set active language adapter to bn
11/28/2021 01:07:45 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:07:45 - INFO - __main__ -   Adapter Languages = ['bn']
11/28/2021 01:07:45 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_hi_bert-base-multilingual-cased_128
11/28/2021 01:07:45 - INFO - __main__ -   Language gn, split test does not exist
11/28/2021 01:07:45 - INFO - __main__ -   ***** Running evaluation  in hi *****
11/28/2021 01:07:45 - INFO - __main__ -     Num examples = 2685
11/28/2021 01:07:45 - INFO - __main__ -     Batch size = 32
11/28/2021 01:07:45 - INFO - __main__ -   Batch number = 1
11/28/2021 01:07:46 - INFO - __main__ -   Batch number = 2
11/28/2021 01:07:46 - INFO - __main__ -   Batch number = 3
11/28/2021 01:07:46 - INFO - __main__ -   Batch number = 4
11/28/2021 01:07:47 - INFO - __main__ -   Batch number = 5
11/28/2021 01:07:47 - INFO - __main__ -   Batch number = 6
11/28/2021 01:07:47 - INFO - __main__ -   Batch number = 7
11/28/2021 01:07:47 - INFO - __main__ -   Batch number = 8
11/28/2021 01:07:48 - INFO - __main__ -   Batch number = 9
11/28/2021 01:07:48 - INFO - __main__ -   Batch number = 10
11/28/2021 01:07:48 - INFO - __main__ -   Batch number = 11
11/28/2021 01:07:49 - INFO - __main__ -   Batch number = 12
11/28/2021 01:07:49 - INFO - __main__ -   Batch number = 13
11/28/2021 01:07:50 - INFO - __main__ -   Batch number = 14
11/28/2021 01:07:50 - INFO - __main__ -   Batch number = 15
11/28/2021 01:07:50 - INFO - __main__ -   Batch number = 16
11/28/2021 01:07:50 - INFO - __main__ -   Batch number = 17
11/28/2021 01:07:51 - INFO - __main__ -   Batch number = 18
11/28/2021 01:07:51 - INFO - __main__ -   Batch number = 19
11/28/2021 01:07:51 - INFO - __main__ -   Batch number = 20
11/28/2021 01:07:52 - INFO - __main__ -   Batch number = 21
11/28/2021 01:07:52 - INFO - __main__ -   Batch number = 22
11/28/2021 01:07:52 - INFO - __main__ -   Batch number = 23
11/28/2021 01:07:53 - INFO - __main__ -   Batch number = 24
11/28/2021 01:07:53 - INFO - __main__ -   Batch number = 25
11/28/2021 01:07:53 - INFO - __main__ -   Batch number = 26
11/28/2021 01:07:54 - INFO - __main__ -   Batch number = 27
11/28/2021 01:07:54 - INFO - __main__ -   Batch number = 28
11/28/2021 01:07:54 - INFO - __main__ -   Batch number = 29
11/28/2021 01:07:55 - INFO - __main__ -   Batch number = 30
11/28/2021 01:07:55 - INFO - __main__ -   Batch number = 31
11/28/2021 01:07:55 - INFO - __main__ -   Batch number = 32
11/28/2021 01:07:55 - INFO - __main__ -   Batch number = 33
11/28/2021 01:07:56 - INFO - __main__ -   Batch number = 34
11/28/2021 01:07:56 - INFO - __main__ -   Batch number = 35
11/28/2021 01:07:56 - INFO - __main__ -   Batch number = 36
11/28/2021 01:07:57 - INFO - __main__ -   Batch number = 37
11/28/2021 01:07:57 - INFO - __main__ -   Batch number = 38
11/28/2021 01:07:57 - INFO - __main__ -   Batch number = 39
11/28/2021 01:07:58 - INFO - __main__ -   Batch number = 40
11/28/2021 01:07:58 - INFO - __main__ -   Batch number = 41
11/28/2021 01:07:58 - INFO - __main__ -   Batch number = 42
11/28/2021 01:07:59 - INFO - __main__ -   Batch number = 43
11/28/2021 01:07:59 - INFO - __main__ -   Batch number = 44
11/28/2021 01:07:59 - INFO - __main__ -   Batch number = 45
11/28/2021 01:08:00 - INFO - __main__ -   Batch number = 46
11/28/2021 01:08:00 - INFO - __main__ -   Batch number = 47
11/28/2021 01:08:00 - INFO - __main__ -   Batch number = 48
11/28/2021 01:08:01 - INFO - __main__ -   Batch number = 49
11/28/2021 01:08:01 - INFO - __main__ -   Batch number = 50
11/28/2021 01:08:01 - INFO - __main__ -   Batch number = 51
11/28/2021 01:08:01 - INFO - __main__ -   Batch number = 52
11/28/2021 01:08:02 - INFO - __main__ -   Batch number = 53
11/28/2021 01:08:02 - INFO - __main__ -   Batch number = 54
11/28/2021 01:08:02 - INFO - __main__ -   Batch number = 55
11/28/2021 01:08:03 - INFO - __main__ -   Batch number = 56
11/28/2021 01:08:03 - INFO - __main__ -   Batch number = 57
11/28/2021 01:08:03 - INFO - __main__ -   Batch number = 58
11/28/2021 01:08:04 - INFO - __main__ -   Batch number = 59
11/28/2021 01:08:04 - INFO - __main__ -   Batch number = 60
11/28/2021 01:08:04 - INFO - __main__ -   Batch number = 61
11/28/2021 01:08:05 - INFO - __main__ -   Batch number = 62
11/28/2021 01:08:05 - INFO - __main__ -   Batch number = 63
11/28/2021 01:08:05 - INFO - __main__ -   Batch number = 64
11/28/2021 01:08:06 - INFO - __main__ -   Batch number = 65
11/28/2021 01:08:06 - INFO - __main__ -   Batch number = 66
11/28/2021 01:08:06 - INFO - __main__ -   Batch number = 67
11/28/2021 01:08:07 - INFO - __main__ -   Batch number = 68
11/28/2021 01:08:07 - INFO - __main__ -   Batch number = 69
11/28/2021 01:08:07 - INFO - __main__ -   Batch number = 70
11/28/2021 01:08:07 - INFO - __main__ -   Batch number = 71
11/28/2021 01:08:08 - INFO - __main__ -   Batch number = 72
11/28/2021 01:08:08 - INFO - __main__ -   Batch number = 73
11/28/2021 01:08:08 - INFO - __main__ -   Batch number = 74
11/28/2021 01:08:09 - INFO - __main__ -   Batch number = 75
11/28/2021 01:08:09 - INFO - __main__ -   Batch number = 76
11/28/2021 01:08:09 - INFO - __main__ -   Batch number = 77
11/28/2021 01:08:10 - INFO - __main__ -   Batch number = 78
11/28/2021 01:08:10 - INFO - __main__ -   Batch number = 79
11/28/2021 01:08:10 - INFO - __main__ -   Batch number = 80
11/28/2021 01:08:11 - INFO - __main__ -   Batch number = 81
11/28/2021 01:08:11 - INFO - __main__ -   Batch number = 82
11/28/2021 01:08:11 - INFO - __main__ -   Batch number = 83
11/28/2021 01:08:12 - INFO - __main__ -   Batch number = 84
11/28/2021 01:08:13 - INFO - __main__ -   ***** Evaluation result  in hi *****
11/28/2021 01:08:13 - INFO - __main__ -     f1 = 0.6623297106995254
11/28/2021 01:08:13 - INFO - __main__ -     loss = 1.163644783553623
11/28/2021 01:08:13 - INFO - __main__ -     precision = 0.6616713816040982
11/28/2021 01:08:13 - INFO - __main__ -     recall = 0.6629893511070252
11/28/2021 01:09:12 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:09:12 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:09:12 - INFO - __main__ -   Seed = 1
11/28/2021 01:09:12 - INFO - root -   save model
11/28/2021 01:09:12 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:09:12 - INFO - __main__ -   Loading pretrained model and tokenizer
11/28/2021 01:09:15 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
11/28/2021 01:09:22 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:09:22 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:09:22 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
11/28/2021 01:09:22 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:09:22 - INFO - root -   loading task adapter
11/28/2021 01:09:22 - INFO - root -   loading lang adpater bn/wiki@ukp
11/28/2021 01:09:22 - INFO - __main__ -   Adapter Languages : ['bn'], Length : 1
11/28/2021 01:09:22 - INFO - __main__ -   Adapter Names ['bn/wiki@ukp'], Length : 1
11/28/2021 01:09:22 - INFO - __main__ -   Language = bn
11/28/2021 01:09:22 - INFO - __main__ -   Adapter Name = bn/wiki@ukp
11/28/2021 01:09:32 - INFO - __main__ -   Language mi, split test does not exist
11/28/2021 01:09:35 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:09:35 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:09:35 - INFO - __main__ -   Seed = 2
11/28/2021 01:09:35 - INFO - root -   save model
11/28/2021 01:09:35 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:09:35 - INFO - __main__ -   Loading pretrained model and tokenizer
11/28/2021 01:09:37 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
11/28/2021 01:09:43 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:09:43 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:09:43 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
11/28/2021 01:09:43 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:09:43 - INFO - root -   loading task adapter
11/28/2021 01:09:43 - INFO - root -   loading lang adpater bn/wiki@ukp
11/28/2021 01:09:43 - INFO - __main__ -   Adapter Languages : ['bn'], Length : 1
11/28/2021 01:09:43 - INFO - __main__ -   Adapter Names ['bn/wiki@ukp'], Length : 1
11/28/2021 01:09:43 - INFO - __main__ -   Language = bn
11/28/2021 01:09:43 - INFO - __main__ -   Adapter Name = bn/wiki@ukp
11/28/2021 01:09:57 - INFO - __main__ -   Language mi, split test does not exist
11/28/2021 01:09:59 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:09:59 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:09:59 - INFO - __main__ -   Seed = 3
11/28/2021 01:09:59 - INFO - root -   save model
11/28/2021 01:09:59 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:09:59 - INFO - __main__ -   Loading pretrained model and tokenizer
11/28/2021 01:10:02 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
11/28/2021 01:10:08 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:10:08 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:10:08 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
11/28/2021 01:10:08 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:10:08 - INFO - root -   loading task adapter
11/28/2021 01:10:08 - INFO - root -   loading lang adpater bn/wiki@ukp
11/28/2021 01:10:08 - INFO - __main__ -   Adapter Languages : ['bn'], Length : 1
11/28/2021 01:10:08 - INFO - __main__ -   Adapter Names ['bn/wiki@ukp'], Length : 1
11/28/2021 01:10:08 - INFO - __main__ -   Language = bn
11/28/2021 01:10:08 - INFO - __main__ -   Adapter Name = bn/wiki@ukp
11/28/2021 01:10:18 - INFO - __main__ -   Language mi, split test does not exist
11/28/2021 01:10:26 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ru', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:10:26 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:10:26 - INFO - __main__ -   Seed = 1
11/28/2021 01:10:26 - INFO - root -   save model
11/28/2021 01:10:26 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ru', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:10:26 - INFO - __main__ -   Loading pretrained model and tokenizer
11/28/2021 01:10:29 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
11/28/2021 01:10:35 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:10:35 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:10:35 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
11/28/2021 01:10:35 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:10:35 - INFO - root -   loading task adapter
11/28/2021 01:10:35 - INFO - root -   loading lang adpater bn/wiki@ukp
11/28/2021 01:10:35 - INFO - __main__ -   Adapter Languages : ['bn'], Length : 1
11/28/2021 01:10:35 - INFO - __main__ -   Adapter Names ['bn/wiki@ukp'], Length : 1
11/28/2021 01:10:35 - INFO - __main__ -   Language = bn
11/28/2021 01:10:35 - INFO - __main__ -   Adapter Name = bn/wiki@ukp
11/28/2021 01:10:44 - INFO - __main__ -   Language adapter for ru not found, using bn instead
11/28/2021 01:10:44 - INFO - __main__ -   Set active language adapter to bn
11/28/2021 01:10:44 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:10:44 - INFO - __main__ -   Adapter Languages = ['bn']
11/28/2021 01:10:44 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_ru_bert-base-multilingual-cased_128
11/28/2021 01:10:45 - INFO - __main__ -   ***** Running evaluation  in ru *****
11/28/2021 01:10:45 - INFO - __main__ -     Num examples = 8995
11/28/2021 01:10:45 - INFO - __main__ -     Batch size = 32
11/28/2021 01:10:45 - INFO - __main__ -   Batch number = 1
11/28/2021 01:10:46 - INFO - __main__ -   Batch number = 2
11/28/2021 01:10:46 - INFO - __main__ -   Batch number = 3
11/28/2021 01:10:46 - INFO - __main__ -   Batch number = 4
11/28/2021 01:10:46 - INFO - __main__ -   Batch number = 5
11/28/2021 01:10:47 - INFO - __main__ -   Batch number = 6
11/28/2021 01:10:47 - INFO - __main__ -   Batch number = 7
11/28/2021 01:10:47 - INFO - __main__ -   Batch number = 8
11/28/2021 01:10:48 - INFO - __main__ -   Batch number = 9
11/28/2021 01:10:48 - INFO - __main__ -   Batch number = 10
11/28/2021 01:10:48 - INFO - __main__ -   Batch number = 11
11/28/2021 01:10:49 - INFO - __main__ -   Batch number = 12
11/28/2021 01:10:49 - INFO - __main__ -   Batch number = 13
11/28/2021 01:10:49 - INFO - __main__ -   Batch number = 14
11/28/2021 01:10:50 - INFO - __main__ -   Batch number = 15
11/28/2021 01:10:50 - INFO - __main__ -   Batch number = 16
11/28/2021 01:10:50 - INFO - __main__ -   Batch number = 17
11/28/2021 01:10:51 - INFO - __main__ -   Batch number = 18
11/28/2021 01:10:51 - INFO - __main__ -   Batch number = 19
11/28/2021 01:10:51 - INFO - __main__ -   Batch number = 20
11/28/2021 01:10:52 - INFO - __main__ -   Batch number = 21
11/28/2021 01:10:52 - INFO - __main__ -   Batch number = 22
11/28/2021 01:10:52 - INFO - __main__ -   Batch number = 23
11/28/2021 01:10:53 - INFO - __main__ -   Batch number = 24
11/28/2021 01:10:53 - INFO - __main__ -   Batch number = 25
11/28/2021 01:10:53 - INFO - __main__ -   Batch number = 26
11/28/2021 01:10:54 - INFO - __main__ -   Batch number = 27
11/28/2021 01:10:54 - INFO - __main__ -   Batch number = 28
11/28/2021 01:10:54 - INFO - __main__ -   Batch number = 29
11/28/2021 01:10:55 - INFO - __main__ -   Batch number = 30
11/28/2021 01:10:55 - INFO - __main__ -   Batch number = 31
11/28/2021 01:10:55 - INFO - __main__ -   Batch number = 32
11/28/2021 01:10:55 - INFO - __main__ -   Batch number = 33
11/28/2021 01:10:55 - INFO - __main__ -   Batch number = 34
11/28/2021 01:10:55 - INFO - __main__ -   Batch number = 35
11/28/2021 01:10:56 - INFO - __main__ -   Batch number = 36
11/28/2021 01:10:56 - INFO - __main__ -   Batch number = 37
11/28/2021 01:10:56 - INFO - __main__ -   Batch number = 38
11/28/2021 01:10:56 - INFO - __main__ -   Batch number = 39
11/28/2021 01:10:56 - INFO - __main__ -   Batch number = 40
11/28/2021 01:10:56 - INFO - __main__ -   Batch number = 41
11/28/2021 01:10:57 - INFO - __main__ -   Batch number = 42
11/28/2021 01:10:57 - INFO - __main__ -   Batch number = 43
11/28/2021 01:10:57 - INFO - __main__ -   Batch number = 44
11/28/2021 01:10:57 - INFO - __main__ -   Batch number = 45
11/28/2021 01:10:58 - INFO - __main__ -   Batch number = 46
11/28/2021 01:10:58 - INFO - __main__ -   Batch number = 47
11/28/2021 01:10:58 - INFO - __main__ -   Batch number = 48
11/28/2021 01:10:59 - INFO - __main__ -   Batch number = 49
11/28/2021 01:10:59 - INFO - __main__ -   Batch number = 50
11/28/2021 01:10:59 - INFO - __main__ -   Batch number = 51
11/28/2021 01:11:00 - INFO - __main__ -   Batch number = 52
11/28/2021 01:11:00 - INFO - __main__ -   Batch number = 53
11/28/2021 01:11:00 - INFO - __main__ -   Batch number = 54
11/28/2021 01:11:01 - INFO - __main__ -   Batch number = 55
11/28/2021 01:11:01 - INFO - __main__ -   Batch number = 56
11/28/2021 01:11:01 - INFO - __main__ -   Batch number = 57
11/28/2021 01:11:02 - INFO - __main__ -   Batch number = 58
11/28/2021 01:11:02 - INFO - __main__ -   Batch number = 59
11/28/2021 01:11:02 - INFO - __main__ -   Batch number = 60
11/28/2021 01:11:03 - INFO - __main__ -   Batch number = 61
11/28/2021 01:11:03 - INFO - __main__ -   Batch number = 62
11/28/2021 01:11:03 - INFO - __main__ -   Batch number = 63
11/28/2021 01:11:04 - INFO - __main__ -   Batch number = 64
11/28/2021 01:11:04 - INFO - __main__ -   Batch number = 65
11/28/2021 01:11:04 - INFO - __main__ -   Batch number = 66
11/28/2021 01:11:05 - INFO - __main__ -   Batch number = 67
11/28/2021 01:11:05 - INFO - __main__ -   Batch number = 68
11/28/2021 01:11:05 - INFO - __main__ -   Batch number = 69
11/28/2021 01:11:06 - INFO - __main__ -   Batch number = 70
11/28/2021 01:11:06 - INFO - __main__ -   Batch number = 71
11/28/2021 01:11:06 - INFO - __main__ -   Batch number = 72
11/28/2021 01:11:07 - INFO - __main__ -   Batch number = 73
11/28/2021 01:11:07 - INFO - __main__ -   Batch number = 74
11/28/2021 01:11:07 - INFO - __main__ -   Batch number = 75
11/28/2021 01:11:08 - INFO - __main__ -   Batch number = 76
11/28/2021 01:11:08 - INFO - __main__ -   Batch number = 77
11/28/2021 01:11:08 - INFO - __main__ -   Batch number = 78
11/28/2021 01:11:09 - INFO - __main__ -   Batch number = 79
11/28/2021 01:11:09 - INFO - __main__ -   Batch number = 80
11/28/2021 01:11:10 - INFO - __main__ -   Batch number = 81
11/28/2021 01:11:10 - INFO - __main__ -   Batch number = 82
11/28/2021 01:11:10 - INFO - __main__ -   Batch number = 83
11/28/2021 01:11:11 - INFO - __main__ -   Batch number = 84
11/28/2021 01:11:11 - INFO - __main__ -   Batch number = 85
11/28/2021 01:11:11 - INFO - __main__ -   Batch number = 86
11/28/2021 01:11:12 - INFO - __main__ -   Batch number = 87
11/28/2021 01:11:12 - INFO - __main__ -   Batch number = 88
11/28/2021 01:11:12 - INFO - __main__ -   Batch number = 89
11/28/2021 01:11:13 - INFO - __main__ -   Batch number = 90
11/28/2021 01:11:13 - INFO - __main__ -   Batch number = 91
11/28/2021 01:11:13 - INFO - __main__ -   Batch number = 92
11/28/2021 01:11:14 - INFO - __main__ -   Batch number = 93
11/28/2021 01:11:14 - INFO - __main__ -   Batch number = 94
11/28/2021 01:11:15 - INFO - __main__ -   Batch number = 95
11/28/2021 01:11:15 - INFO - __main__ -   Batch number = 96
11/28/2021 01:11:15 - INFO - __main__ -   Batch number = 97
11/28/2021 01:11:16 - INFO - __main__ -   Batch number = 98
11/28/2021 01:11:16 - INFO - __main__ -   Batch number = 99
11/28/2021 01:11:16 - INFO - __main__ -   Batch number = 100
11/28/2021 01:11:17 - INFO - __main__ -   Batch number = 101
11/28/2021 01:11:17 - INFO - __main__ -   Batch number = 102
11/28/2021 01:11:17 - INFO - __main__ -   Batch number = 103
11/28/2021 01:11:18 - INFO - __main__ -   Batch number = 104
11/28/2021 01:11:18 - INFO - __main__ -   Batch number = 105
11/28/2021 01:11:18 - INFO - __main__ -   Batch number = 106
11/28/2021 01:11:19 - INFO - __main__ -   Batch number = 107
11/28/2021 01:11:19 - INFO - __main__ -   Batch number = 108
11/28/2021 01:11:19 - INFO - __main__ -   Batch number = 109
11/28/2021 01:11:20 - INFO - __main__ -   Batch number = 110
11/28/2021 01:11:20 - INFO - __main__ -   Batch number = 111
11/28/2021 01:11:21 - INFO - __main__ -   Batch number = 112
11/28/2021 01:11:21 - INFO - __main__ -   Batch number = 113
11/28/2021 01:11:22 - INFO - __main__ -   Batch number = 114
11/28/2021 01:11:22 - INFO - __main__ -   Batch number = 115
11/28/2021 01:11:23 - INFO - __main__ -   Batch number = 116
11/28/2021 01:11:23 - INFO - __main__ -   Batch number = 117
11/28/2021 01:11:24 - INFO - __main__ -   Batch number = 118
11/28/2021 01:11:24 - INFO - __main__ -   Batch number = 119
11/28/2021 01:11:25 - INFO - __main__ -   Batch number = 120
11/28/2021 01:11:25 - INFO - __main__ -   Batch number = 121
11/28/2021 01:11:26 - INFO - __main__ -   Batch number = 122
11/28/2021 01:11:26 - INFO - __main__ -   Batch number = 123
11/28/2021 01:11:27 - INFO - __main__ -   Batch number = 124
11/28/2021 01:11:27 - INFO - __main__ -   Batch number = 125
11/28/2021 01:11:28 - INFO - __main__ -   Batch number = 126
11/28/2021 01:11:28 - INFO - __main__ -   Batch number = 127
11/28/2021 01:11:28 - INFO - __main__ -   Batch number = 128
11/28/2021 01:11:29 - INFO - __main__ -   Batch number = 129
11/28/2021 01:11:29 - INFO - __main__ -   Batch number = 130
11/28/2021 01:11:29 - INFO - __main__ -   Batch number = 131
11/28/2021 01:11:30 - INFO - __main__ -   Batch number = 132
11/28/2021 01:11:30 - INFO - __main__ -   Batch number = 133
11/28/2021 01:11:30 - INFO - __main__ -   Batch number = 134
11/28/2021 01:11:31 - INFO - __main__ -   Batch number = 135
11/28/2021 01:11:31 - INFO - __main__ -   Batch number = 136
11/28/2021 01:11:31 - INFO - __main__ -   Batch number = 137
11/28/2021 01:11:32 - INFO - __main__ -   Batch number = 138
11/28/2021 01:11:32 - INFO - __main__ -   Batch number = 139
11/28/2021 01:11:32 - INFO - __main__ -   Batch number = 140
11/28/2021 01:11:33 - INFO - __main__ -   Batch number = 141
11/28/2021 01:11:33 - INFO - __main__ -   Batch number = 142
11/28/2021 01:11:33 - INFO - __main__ -   Batch number = 143
11/28/2021 01:11:34 - INFO - __main__ -   Batch number = 144
11/28/2021 01:11:34 - INFO - __main__ -   Batch number = 145
11/28/2021 01:11:34 - INFO - __main__ -   Batch number = 146
11/28/2021 01:11:35 - INFO - __main__ -   Batch number = 147
11/28/2021 01:11:35 - INFO - __main__ -   Batch number = 148
11/28/2021 01:11:36 - INFO - __main__ -   Batch number = 149
11/28/2021 01:11:36 - INFO - __main__ -   Batch number = 150
11/28/2021 01:11:36 - INFO - __main__ -   Batch number = 151
11/28/2021 01:11:37 - INFO - __main__ -   Batch number = 152
11/28/2021 01:11:37 - INFO - __main__ -   Batch number = 153
11/28/2021 01:11:37 - INFO - __main__ -   Batch number = 154
11/28/2021 01:11:38 - INFO - __main__ -   Batch number = 155
11/28/2021 01:11:38 - INFO - __main__ -   Batch number = 156
11/28/2021 01:11:38 - INFO - __main__ -   Batch number = 157
11/28/2021 01:11:39 - INFO - __main__ -   Batch number = 158
11/28/2021 01:11:39 - INFO - __main__ -   Batch number = 159
11/28/2021 01:11:39 - INFO - __main__ -   Batch number = 160
11/28/2021 01:11:40 - INFO - __main__ -   Batch number = 161
11/28/2021 01:11:40 - INFO - __main__ -   Batch number = 162
11/28/2021 01:11:40 - INFO - __main__ -   Batch number = 163
11/28/2021 01:11:41 - INFO - __main__ -   Batch number = 164
11/28/2021 01:11:41 - INFO - __main__ -   Batch number = 165
11/28/2021 01:11:41 - INFO - __main__ -   Batch number = 166
11/28/2021 01:11:42 - INFO - __main__ -   Batch number = 167
11/28/2021 01:11:42 - INFO - __main__ -   Batch number = 168
11/28/2021 01:11:42 - INFO - __main__ -   Batch number = 169
11/28/2021 01:11:43 - INFO - __main__ -   Batch number = 170
11/28/2021 01:11:43 - INFO - __main__ -   Batch number = 171
11/28/2021 01:11:43 - INFO - __main__ -   Batch number = 172
11/28/2021 01:11:44 - INFO - __main__ -   Batch number = 173
11/28/2021 01:11:44 - INFO - __main__ -   Batch number = 174
11/28/2021 01:11:44 - INFO - __main__ -   Batch number = 175
11/28/2021 01:11:45 - INFO - __main__ -   Batch number = 176
11/28/2021 01:11:45 - INFO - __main__ -   Batch number = 177
11/28/2021 01:11:45 - INFO - __main__ -   Batch number = 178
11/28/2021 01:11:46 - INFO - __main__ -   Batch number = 179
11/28/2021 01:11:46 - INFO - __main__ -   Batch number = 180
11/28/2021 01:11:46 - INFO - __main__ -   Batch number = 181
11/28/2021 01:11:47 - INFO - __main__ -   Batch number = 182
11/28/2021 01:11:47 - INFO - __main__ -   Batch number = 183
11/28/2021 01:11:47 - INFO - __main__ -   Batch number = 184
11/28/2021 01:11:48 - INFO - __main__ -   Batch number = 185
11/28/2021 01:11:48 - INFO - __main__ -   Batch number = 186
11/28/2021 01:11:49 - INFO - __main__ -   Batch number = 187
11/28/2021 01:11:49 - INFO - __main__ -   Batch number = 188
11/28/2021 01:11:49 - INFO - __main__ -   Batch number = 189
11/28/2021 01:11:50 - INFO - __main__ -   Batch number = 190
11/28/2021 01:11:50 - INFO - __main__ -   Batch number = 191
11/28/2021 01:11:50 - INFO - __main__ -   Batch number = 192
11/28/2021 01:11:51 - INFO - __main__ -   Batch number = 193
11/28/2021 01:11:51 - INFO - __main__ -   Batch number = 194
11/28/2021 01:11:52 - INFO - __main__ -   Batch number = 195
11/28/2021 01:11:52 - INFO - __main__ -   Batch number = 196
11/28/2021 01:11:53 - INFO - __main__ -   Batch number = 197
11/28/2021 01:11:53 - INFO - __main__ -   Batch number = 198
11/28/2021 01:11:54 - INFO - __main__ -   Batch number = 199
11/28/2021 01:11:54 - INFO - __main__ -   Batch number = 200
11/28/2021 01:11:55 - INFO - __main__ -   Batch number = 201
11/28/2021 01:11:55 - INFO - __main__ -   Batch number = 202
11/28/2021 01:11:55 - INFO - __main__ -   Batch number = 203
11/28/2021 01:11:56 - INFO - __main__ -   Batch number = 204
11/28/2021 01:11:56 - INFO - __main__ -   Batch number = 205
11/28/2021 01:11:57 - INFO - __main__ -   Batch number = 206
11/28/2021 01:11:57 - INFO - __main__ -   Batch number = 207
11/28/2021 01:11:57 - INFO - __main__ -   Batch number = 208
11/28/2021 01:11:58 - INFO - __main__ -   Batch number = 209
11/28/2021 01:11:58 - INFO - __main__ -   Batch number = 210
11/28/2021 01:11:58 - INFO - __main__ -   Batch number = 211
11/28/2021 01:11:59 - INFO - __main__ -   Batch number = 212
11/28/2021 01:11:59 - INFO - __main__ -   Batch number = 213
11/28/2021 01:11:59 - INFO - __main__ -   Batch number = 214
11/28/2021 01:12:00 - INFO - __main__ -   Batch number = 215
11/28/2021 01:12:00 - INFO - __main__ -   Batch number = 216
11/28/2021 01:12:00 - INFO - __main__ -   Batch number = 217
11/28/2021 01:12:01 - INFO - __main__ -   Batch number = 218
11/28/2021 01:12:01 - INFO - __main__ -   Batch number = 219
11/28/2021 01:12:01 - INFO - __main__ -   Batch number = 220
11/28/2021 01:12:02 - INFO - __main__ -   Batch number = 221
11/28/2021 01:12:02 - INFO - __main__ -   Batch number = 222
11/28/2021 01:12:03 - INFO - __main__ -   Batch number = 223
11/28/2021 01:12:03 - INFO - __main__ -   Batch number = 224
11/28/2021 01:12:03 - INFO - __main__ -   Batch number = 225
11/28/2021 01:12:04 - INFO - __main__ -   Batch number = 226
11/28/2021 01:12:04 - INFO - __main__ -   Batch number = 227
11/28/2021 01:12:05 - INFO - __main__ -   Batch number = 228
11/28/2021 01:12:05 - INFO - __main__ -   Batch number = 229
11/28/2021 01:12:05 - INFO - __main__ -   Batch number = 230
11/28/2021 01:12:06 - INFO - __main__ -   Batch number = 231
11/28/2021 01:12:06 - INFO - __main__ -   Batch number = 232
11/28/2021 01:12:06 - INFO - __main__ -   Batch number = 233
11/28/2021 01:12:07 - INFO - __main__ -   Batch number = 234
11/28/2021 01:12:07 - INFO - __main__ -   Batch number = 235
11/28/2021 01:12:07 - INFO - __main__ -   Batch number = 236
11/28/2021 01:12:08 - INFO - __main__ -   Batch number = 237
11/28/2021 01:12:08 - INFO - __main__ -   Batch number = 238
11/28/2021 01:12:08 - INFO - __main__ -   Batch number = 239
11/28/2021 01:12:09 - INFO - __main__ -   Batch number = 240
11/28/2021 01:12:09 - INFO - __main__ -   Batch number = 241
11/28/2021 01:12:09 - INFO - __main__ -   Batch number = 242
11/28/2021 01:12:10 - INFO - __main__ -   Batch number = 243
11/28/2021 01:12:10 - INFO - __main__ -   Batch number = 244
11/28/2021 01:12:10 - INFO - __main__ -   Batch number = 245
11/28/2021 01:12:11 - INFO - __main__ -   Batch number = 246
11/28/2021 01:12:11 - INFO - __main__ -   Batch number = 247
11/28/2021 01:12:12 - INFO - __main__ -   Batch number = 248
11/28/2021 01:12:12 - INFO - __main__ -   Batch number = 249
11/28/2021 01:12:12 - INFO - __main__ -   Batch number = 250
11/28/2021 01:12:13 - INFO - __main__ -   Batch number = 251
11/28/2021 01:12:13 - INFO - __main__ -   Batch number = 252
11/28/2021 01:12:13 - INFO - __main__ -   Batch number = 253
11/28/2021 01:12:14 - INFO - __main__ -   Batch number = 254
11/28/2021 01:12:14 - INFO - __main__ -   Batch number = 255
11/28/2021 01:12:14 - INFO - __main__ -   Batch number = 256
11/28/2021 01:12:15 - INFO - __main__ -   Batch number = 257
11/28/2021 01:12:15 - INFO - __main__ -   Batch number = 258
11/28/2021 01:12:15 - INFO - __main__ -   Batch number = 259
11/28/2021 01:12:16 - INFO - __main__ -   Batch number = 260
11/28/2021 01:12:16 - INFO - __main__ -   Batch number = 261
11/28/2021 01:12:16 - INFO - __main__ -   Batch number = 262
11/28/2021 01:12:17 - INFO - __main__ -   Batch number = 263
11/28/2021 01:12:17 - INFO - __main__ -   Batch number = 264
11/28/2021 01:12:18 - INFO - __main__ -   Batch number = 265
11/28/2021 01:12:18 - INFO - __main__ -   Batch number = 266
11/28/2021 01:12:18 - INFO - __main__ -   Batch number = 267
11/28/2021 01:12:18 - INFO - __main__ -   Batch number = 268
11/28/2021 01:12:18 - INFO - __main__ -   Batch number = 269
11/28/2021 01:12:19 - INFO - __main__ -   Batch number = 270
11/28/2021 01:12:19 - INFO - __main__ -   Batch number = 271
11/28/2021 01:12:19 - INFO - __main__ -   Batch number = 272
11/28/2021 01:12:20 - INFO - __main__ -   Batch number = 273
11/28/2021 01:12:20 - INFO - __main__ -   Batch number = 274
11/28/2021 01:12:21 - INFO - __main__ -   Batch number = 275
11/28/2021 01:12:21 - INFO - __main__ -   Batch number = 276
11/28/2021 01:12:21 - INFO - __main__ -   Batch number = 277
11/28/2021 01:12:22 - INFO - __main__ -   Batch number = 278
11/28/2021 01:12:22 - INFO - __main__ -   Batch number = 279
11/28/2021 01:12:22 - INFO - __main__ -   Batch number = 280
11/28/2021 01:12:23 - INFO - __main__ -   Batch number = 281
11/28/2021 01:12:23 - INFO - __main__ -   Batch number = 282
11/28/2021 01:12:28 - INFO - __main__ -   ***** Evaluation result  in ru *****
11/28/2021 01:12:28 - INFO - __main__ -     f1 = 0.8479576217257451
11/28/2021 01:12:28 - INFO - __main__ -     loss = 0.5581662943388553
11/28/2021 01:12:28 - INFO - __main__ -     precision = 0.850335630661396
11/28/2021 01:12:28 - INFO - __main__ -     recall = 0.8455928761559781
11/28/2021 01:12:30 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ru', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:12:30 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:12:30 - INFO - __main__ -   Seed = 2
11/28/2021 01:12:30 - INFO - root -   save model
11/28/2021 01:12:30 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ru', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:12:30 - INFO - __main__ -   Loading pretrained model and tokenizer
11/28/2021 01:12:33 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
11/28/2021 01:12:39 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:12:39 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:12:39 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
11/28/2021 01:12:39 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:12:39 - INFO - root -   loading task adapter
11/28/2021 01:12:39 - INFO - root -   loading lang adpater bn/wiki@ukp
11/28/2021 01:12:39 - INFO - __main__ -   Adapter Languages : ['bn'], Length : 1
11/28/2021 01:12:39 - INFO - __main__ -   Adapter Names ['bn/wiki@ukp'], Length : 1
11/28/2021 01:12:39 - INFO - __main__ -   Language = bn
11/28/2021 01:12:39 - INFO - __main__ -   Adapter Name = bn/wiki@ukp
11/28/2021 01:12:49 - INFO - __main__ -   Language adapter for ru not found, using bn instead
11/28/2021 01:12:49 - INFO - __main__ -   Set active language adapter to bn
11/28/2021 01:12:49 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:12:49 - INFO - __main__ -   Adapter Languages = ['bn']
11/28/2021 01:12:49 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_ru_bert-base-multilingual-cased_128
11/28/2021 01:12:50 - INFO - __main__ -   ***** Running evaluation  in ru *****
11/28/2021 01:12:50 - INFO - __main__ -     Num examples = 8995
11/28/2021 01:12:50 - INFO - __main__ -     Batch size = 32
11/28/2021 01:12:50 - INFO - __main__ -   Batch number = 1
11/28/2021 01:12:51 - INFO - __main__ -   Batch number = 2
11/28/2021 01:12:51 - INFO - __main__ -   Batch number = 3
11/28/2021 01:12:51 - INFO - __main__ -   Batch number = 4
11/28/2021 01:12:52 - INFO - __main__ -   Batch number = 5
11/28/2021 01:12:52 - INFO - __main__ -   Batch number = 6
11/28/2021 01:12:52 - INFO - __main__ -   Batch number = 7
11/28/2021 01:12:53 - INFO - __main__ -   Batch number = 8
11/28/2021 01:12:53 - INFO - __main__ -   Batch number = 9
11/28/2021 01:12:53 - INFO - __main__ -   Batch number = 10
11/28/2021 01:12:54 - INFO - __main__ -   Batch number = 11
11/28/2021 01:12:54 - INFO - __main__ -   Batch number = 12
11/28/2021 01:12:54 - INFO - __main__ -   Batch number = 13
11/28/2021 01:12:54 - INFO - __main__ -   Batch number = 14
11/28/2021 01:12:55 - INFO - __main__ -   Batch number = 15
11/28/2021 01:12:55 - INFO - __main__ -   Batch number = 16
11/28/2021 01:12:55 - INFO - __main__ -   Batch number = 17
11/28/2021 01:12:55 - INFO - __main__ -   Batch number = 18
11/28/2021 01:12:55 - INFO - __main__ -   Batch number = 19
11/28/2021 01:12:55 - INFO - __main__ -   Batch number = 20
11/28/2021 01:12:56 - INFO - __main__ -   Batch number = 21
11/28/2021 01:12:56 - INFO - __main__ -   Batch number = 22
11/28/2021 01:12:56 - INFO - __main__ -   Batch number = 23
11/28/2021 01:12:56 - INFO - __main__ -   Batch number = 24
11/28/2021 01:12:56 - INFO - __main__ -   Batch number = 25
11/28/2021 01:12:56 - INFO - __main__ -   Batch number = 26
11/28/2021 01:12:57 - INFO - __main__ -   Batch number = 27
11/28/2021 01:12:57 - INFO - __main__ -   Batch number = 28
11/28/2021 01:12:57 - INFO - __main__ -   Batch number = 29
11/28/2021 01:12:57 - INFO - __main__ -   Batch number = 30
11/28/2021 01:12:57 - INFO - __main__ -   Batch number = 31
11/28/2021 01:12:57 - INFO - __main__ -   Batch number = 32
11/28/2021 01:12:58 - INFO - __main__ -   Batch number = 33
11/28/2021 01:12:58 - INFO - __main__ -   Batch number = 34
11/28/2021 01:12:58 - INFO - __main__ -   Batch number = 35
11/28/2021 01:12:58 - INFO - __main__ -   Batch number = 36
11/28/2021 01:12:58 - INFO - __main__ -   Batch number = 37
11/28/2021 01:12:58 - INFO - __main__ -   Batch number = 38
11/28/2021 01:12:59 - INFO - __main__ -   Batch number = 39
11/28/2021 01:12:59 - INFO - __main__ -   Batch number = 40
11/28/2021 01:12:59 - INFO - __main__ -   Batch number = 41
11/28/2021 01:12:59 - INFO - __main__ -   Batch number = 42
11/28/2021 01:12:59 - INFO - __main__ -   Batch number = 43
11/28/2021 01:13:00 - INFO - __main__ -   Batch number = 44
11/28/2021 01:13:00 - INFO - __main__ -   Batch number = 45
11/28/2021 01:13:00 - INFO - __main__ -   Batch number = 46
11/28/2021 01:13:00 - INFO - __main__ -   Batch number = 47
11/28/2021 01:13:00 - INFO - __main__ -   Batch number = 48
11/28/2021 01:13:01 - INFO - __main__ -   Batch number = 49
11/28/2021 01:13:01 - INFO - __main__ -   Batch number = 50
11/28/2021 01:13:01 - INFO - __main__ -   Batch number = 51
11/28/2021 01:13:02 - INFO - __main__ -   Batch number = 52
11/28/2021 01:13:02 - INFO - __main__ -   Batch number = 53
11/28/2021 01:13:02 - INFO - __main__ -   Batch number = 54
11/28/2021 01:13:03 - INFO - __main__ -   Batch number = 55
11/28/2021 01:13:03 - INFO - __main__ -   Batch number = 56
11/28/2021 01:13:03 - INFO - __main__ -   Batch number = 57
11/28/2021 01:13:04 - INFO - __main__ -   Batch number = 58
11/28/2021 01:13:04 - INFO - __main__ -   Batch number = 59
11/28/2021 01:13:04 - INFO - __main__ -   Batch number = 60
11/28/2021 01:13:05 - INFO - __main__ -   Batch number = 61
11/28/2021 01:13:05 - INFO - __main__ -   Batch number = 62
11/28/2021 01:13:05 - INFO - __main__ -   Batch number = 63
11/28/2021 01:13:06 - INFO - __main__ -   Batch number = 64
11/28/2021 01:13:06 - INFO - __main__ -   Batch number = 65
11/28/2021 01:13:06 - INFO - __main__ -   Batch number = 66
11/28/2021 01:13:07 - INFO - __main__ -   Batch number = 67
11/28/2021 01:13:07 - INFO - __main__ -   Batch number = 68
11/28/2021 01:13:07 - INFO - __main__ -   Batch number = 69
11/28/2021 01:13:08 - INFO - __main__ -   Batch number = 70
11/28/2021 01:13:08 - INFO - __main__ -   Batch number = 71
11/28/2021 01:13:08 - INFO - __main__ -   Batch number = 72
11/28/2021 01:13:09 - INFO - __main__ -   Batch number = 73
11/28/2021 01:13:09 - INFO - __main__ -   Batch number = 74
11/28/2021 01:13:09 - INFO - __main__ -   Batch number = 75
11/28/2021 01:13:10 - INFO - __main__ -   Batch number = 76
11/28/2021 01:13:10 - INFO - __main__ -   Batch number = 77
11/28/2021 01:13:10 - INFO - __main__ -   Batch number = 78
11/28/2021 01:13:11 - INFO - __main__ -   Batch number = 79
11/28/2021 01:13:11 - INFO - __main__ -   Batch number = 80
11/28/2021 01:13:12 - INFO - __main__ -   Batch number = 81
11/28/2021 01:13:12 - INFO - __main__ -   Batch number = 82
11/28/2021 01:13:12 - INFO - __main__ -   Batch number = 83
11/28/2021 01:13:13 - INFO - __main__ -   Batch number = 84
11/28/2021 01:13:13 - INFO - __main__ -   Batch number = 85
11/28/2021 01:13:13 - INFO - __main__ -   Batch number = 86
11/28/2021 01:13:14 - INFO - __main__ -   Batch number = 87
11/28/2021 01:13:14 - INFO - __main__ -   Batch number = 88
11/28/2021 01:13:14 - INFO - __main__ -   Batch number = 89
11/28/2021 01:13:14 - INFO - __main__ -   Batch number = 90
11/28/2021 01:13:15 - INFO - __main__ -   Batch number = 91
11/28/2021 01:13:15 - INFO - __main__ -   Batch number = 92
11/28/2021 01:13:15 - INFO - __main__ -   Batch number = 93
11/28/2021 01:13:16 - INFO - __main__ -   Batch number = 94
11/28/2021 01:13:16 - INFO - __main__ -   Batch number = 95
11/28/2021 01:13:17 - INFO - __main__ -   Batch number = 96
11/28/2021 01:13:17 - INFO - __main__ -   Batch number = 97
11/28/2021 01:13:17 - INFO - __main__ -   Batch number = 98
11/28/2021 01:13:18 - INFO - __main__ -   Batch number = 99
11/28/2021 01:13:18 - INFO - __main__ -   Batch number = 100
11/28/2021 01:13:18 - INFO - __main__ -   Batch number = 101
11/28/2021 01:13:19 - INFO - __main__ -   Batch number = 102
11/28/2021 01:13:19 - INFO - __main__ -   Batch number = 103
11/28/2021 01:13:19 - INFO - __main__ -   Batch number = 104
11/28/2021 01:13:20 - INFO - __main__ -   Batch number = 105
11/28/2021 01:13:20 - INFO - __main__ -   Batch number = 106
11/28/2021 01:13:20 - INFO - __main__ -   Batch number = 107
11/28/2021 01:13:21 - INFO - __main__ -   Batch number = 108
11/28/2021 01:13:21 - INFO - __main__ -   Batch number = 109
11/28/2021 01:13:22 - INFO - __main__ -   Batch number = 110
11/28/2021 01:13:22 - INFO - __main__ -   Batch number = 111
11/28/2021 01:13:23 - INFO - __main__ -   Batch number = 112
11/28/2021 01:13:23 - INFO - __main__ -   Batch number = 113
11/28/2021 01:13:24 - INFO - __main__ -   Batch number = 114
11/28/2021 01:13:24 - INFO - __main__ -   Batch number = 115
11/28/2021 01:13:25 - INFO - __main__ -   Batch number = 116
11/28/2021 01:13:25 - INFO - __main__ -   Batch number = 117
11/28/2021 01:13:26 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='eu', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:13:26 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:13:26 - INFO - __main__ -   Seed = 1
11/28/2021 01:13:26 - INFO - root -   save model
11/28/2021 01:13:26 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='eu', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:13:26 - INFO - __main__ -   Loading pretrained model and tokenizer
11/28/2021 01:13:26 - INFO - __main__ -   Batch number = 118
11/28/2021 01:13:26 - INFO - __main__ -   Batch number = 119
11/28/2021 01:13:27 - INFO - __main__ -   Batch number = 120
11/28/2021 01:13:27 - INFO - __main__ -   Batch number = 121
11/28/2021 01:13:28 - INFO - __main__ -   Batch number = 122
11/28/2021 01:13:28 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
11/28/2021 01:13:28 - INFO - __main__ -   Batch number = 123
11/28/2021 01:13:29 - INFO - __main__ -   Batch number = 124
11/28/2021 01:13:29 - INFO - __main__ -   Batch number = 125
11/28/2021 01:13:30 - INFO - __main__ -   Batch number = 126
11/28/2021 01:13:30 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='wo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:13:30 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:13:30 - INFO - __main__ -   Seed = 1
11/28/2021 01:13:30 - INFO - root -   save model
11/28/2021 01:13:30 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='wo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:13:30 - INFO - __main__ -   Loading pretrained model and tokenizer
11/28/2021 01:13:30 - INFO - __main__ -   Batch number = 127
11/28/2021 01:13:31 - INFO - __main__ -   Batch number = 128
11/28/2021 01:13:31 - INFO - __main__ -   Batch number = 129
11/28/2021 01:13:32 - INFO - __main__ -   Batch number = 130
11/28/2021 01:13:33 - INFO - __main__ -   Batch number = 131
11/28/2021 01:13:33 - INFO - __main__ -   Batch number = 132
11/28/2021 01:13:33 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
11/28/2021 01:13:34 - INFO - __main__ -   Batch number = 133
11/28/2021 01:13:34 - INFO - __main__ -   Batch number = 134
11/28/2021 01:13:35 - INFO - __main__ -   Batch number = 135
11/28/2021 01:13:35 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:13:35 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:13:35 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
11/28/2021 01:13:35 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:13:35 - INFO - root -   loading task adapter
11/28/2021 01:13:35 - INFO - root -   loading lang adpater bn/wiki@ukp
11/28/2021 01:13:35 - INFO - __main__ -   Adapter Languages : ['bn'], Length : 1
11/28/2021 01:13:35 - INFO - __main__ -   Adapter Names ['bn/wiki@ukp'], Length : 1
11/28/2021 01:13:35 - INFO - __main__ -   Language = bn
11/28/2021 01:13:35 - INFO - __main__ -   Adapter Name = bn/wiki@ukp
11/28/2021 01:13:35 - INFO - __main__ -   Batch number = 136
11/28/2021 01:13:36 - INFO - __main__ -   Batch number = 137
11/28/2021 01:13:36 - INFO - __main__ -   Batch number = 138
11/28/2021 01:13:37 - INFO - __main__ -   Batch number = 139
11/28/2021 01:13:37 - INFO - __main__ -   Batch number = 140
11/28/2021 01:13:37 - INFO - __main__ -   Batch number = 141
11/28/2021 01:13:38 - INFO - __main__ -   Batch number = 142
11/28/2021 01:13:38 - INFO - __main__ -   Batch number = 143
11/28/2021 01:13:39 - INFO - __main__ -   Batch number = 144
11/28/2021 01:13:39 - INFO - __main__ -   Batch number = 145
11/28/2021 01:13:39 - INFO - __main__ -   Batch number = 146
11/28/2021 01:13:40 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:13:40 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:13:40 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
11/28/2021 01:13:40 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:13:40 - INFO - root -   loading task adapter
11/28/2021 01:13:40 - INFO - __main__ -   Batch number = 147
11/28/2021 01:13:40 - INFO - root -   loading lang adpater bn/wiki@ukp
11/28/2021 01:13:40 - INFO - __main__ -   Adapter Languages : ['bn'], Length : 1
11/28/2021 01:13:40 - INFO - __main__ -   Adapter Names ['bn/wiki@ukp'], Length : 1
11/28/2021 01:13:40 - INFO - __main__ -   Language = bn
11/28/2021 01:13:40 - INFO - __main__ -   Adapter Name = bn/wiki@ukp
11/28/2021 01:13:40 - INFO - __main__ -   Batch number = 148
11/28/2021 01:13:40 - INFO - __main__ -   Batch number = 149
11/28/2021 01:13:41 - INFO - __main__ -   Batch number = 150
11/28/2021 01:13:41 - INFO - __main__ -   Batch number = 151
11/28/2021 01:13:41 - INFO - __main__ -   Batch number = 152
11/28/2021 01:13:42 - INFO - __main__ -   Batch number = 153
11/28/2021 01:13:42 - INFO - __main__ -   Batch number = 154
11/28/2021 01:13:42 - INFO - __main__ -   Language adapter for eu not found, using bn instead
11/28/2021 01:13:42 - INFO - __main__ -   Set active language adapter to bn
11/28/2021 01:13:42 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:13:42 - INFO - __main__ -   Adapter Languages = ['bn']
11/28/2021 01:13:42 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_eu_bert-base-multilingual-cased_128
11/28/2021 01:13:42 - INFO - __main__ -   Batch number = 155
11/28/2021 01:13:43 - INFO - __main__ -   ***** Running evaluation  in eu *****
11/28/2021 01:13:43 - INFO - __main__ -     Num examples = 1799
11/28/2021 01:13:43 - INFO - __main__ -     Batch size = 32
11/28/2021 01:13:43 - INFO - __main__ -   Batch number = 1
11/28/2021 01:13:43 - INFO - __main__ -   Batch number = 156
11/28/2021 01:13:43 - INFO - __main__ -   Batch number = 2
11/28/2021 01:13:43 - INFO - __main__ -   Batch number = 3
11/28/2021 01:13:43 - INFO - __main__ -   Batch number = 157
11/28/2021 01:13:43 - INFO - __main__ -   Batch number = 4
11/28/2021 01:13:43 - INFO - __main__ -   Batch number = 5
11/28/2021 01:13:43 - INFO - __main__ -   Batch number = 6
11/28/2021 01:13:43 - INFO - __main__ -   Batch number = 158
11/28/2021 01:13:44 - INFO - __main__ -   Batch number = 7
11/28/2021 01:13:44 - INFO - __main__ -   Batch number = 8
11/28/2021 01:13:44 - INFO - __main__ -   Batch number = 159
11/28/2021 01:13:44 - INFO - __main__ -   Batch number = 9
11/28/2021 01:13:44 - INFO - __main__ -   Batch number = 10
11/28/2021 01:13:44 - INFO - __main__ -   Batch number = 160
11/28/2021 01:13:44 - INFO - __main__ -   Batch number = 11
11/28/2021 01:13:44 - INFO - __main__ -   Batch number = 12
11/28/2021 01:13:44 - INFO - __main__ -   Batch number = 161
11/28/2021 01:13:44 - INFO - __main__ -   Batch number = 13
11/28/2021 01:13:45 - INFO - __main__ -   Batch number = 14
11/28/2021 01:13:45 - INFO - __main__ -   Batch number = 15
11/28/2021 01:13:45 - INFO - __main__ -   Batch number = 162
11/28/2021 01:13:45 - INFO - __main__ -   Batch number = 16
11/28/2021 01:13:45 - INFO - __main__ -   Batch number = 17
11/28/2021 01:13:45 - INFO - __main__ -   Batch number = 163
11/28/2021 01:13:45 - INFO - __main__ -   Batch number = 18
11/28/2021 01:13:45 - INFO - __main__ -   Batch number = 19
11/28/2021 01:13:45 - INFO - __main__ -   Batch number = 164
11/28/2021 01:13:45 - INFO - __main__ -   Batch number = 20
11/28/2021 01:13:46 - INFO - __main__ -   Batch number = 21
11/28/2021 01:13:46 - INFO - __main__ -   Batch number = 22
11/28/2021 01:13:46 - INFO - __main__ -   Batch number = 165
11/28/2021 01:13:46 - INFO - __main__ -   Batch number = 23
11/28/2021 01:13:46 - INFO - __main__ -   Batch number = 24
11/28/2021 01:13:46 - INFO - __main__ -   Batch number = 166
11/28/2021 01:13:46 - INFO - __main__ -   Batch number = 25
11/28/2021 01:13:46 - INFO - __main__ -   Batch number = 26
11/28/2021 01:13:46 - INFO - __main__ -   Batch number = 167
11/28/2021 01:13:47 - INFO - __main__ -   Batch number = 27
11/28/2021 01:13:47 - INFO - __main__ -   Batch number = 28
11/28/2021 01:13:47 - INFO - __main__ -   Batch number = 29
11/28/2021 01:13:47 - INFO - __main__ -   Batch number = 168
11/28/2021 01:13:47 - INFO - __main__ -   Batch number = 30
11/28/2021 01:13:47 - INFO - __main__ -   Batch number = 31
11/28/2021 01:13:47 - INFO - __main__ -   Batch number = 169
11/28/2021 01:13:47 - INFO - __main__ -   Batch number = 32
11/28/2021 01:13:47 - INFO - __main__ -   Batch number = 33
11/28/2021 01:13:48 - INFO - __main__ -   Batch number = 170
11/28/2021 01:13:48 - INFO - __main__ -   Batch number = 34
11/28/2021 01:13:48 - INFO - __main__ -   Batch number = 35
11/28/2021 01:13:48 - INFO - __main__ -   Batch number = 171
11/28/2021 01:13:48 - INFO - __main__ -   Batch number = 36
11/28/2021 01:13:48 - INFO - __main__ -   Batch number = 37
11/28/2021 01:13:48 - INFO - __main__ -   Batch number = 38
11/28/2021 01:13:48 - INFO - __main__ -   Batch number = 172
11/28/2021 01:13:48 - INFO - __main__ -   Batch number = 39
11/28/2021 01:13:48 - INFO - __main__ -   Batch number = 40
11/28/2021 01:13:49 - INFO - __main__ -   Batch number = 173
11/28/2021 01:13:49 - INFO - __main__ -   Batch number = 41
11/28/2021 01:13:49 - INFO - __main__ -   Language wo, split test does not exist
11/28/2021 01:13:49 - INFO - __main__ -   Batch number = 42
11/28/2021 01:13:49 - INFO - __main__ -   Batch number = 174
11/28/2021 01:13:49 - INFO - __main__ -   Batch number = 43
11/28/2021 01:13:49 - INFO - __main__ -   Batch number = 44
11/28/2021 01:13:49 - INFO - __main__ -   Batch number = 45
11/28/2021 01:13:49 - INFO - __main__ -   Batch number = 175
11/28/2021 01:13:49 - INFO - __main__ -   Batch number = 46
11/28/2021 01:13:50 - INFO - __main__ -   Batch number = 47
11/28/2021 01:13:50 - INFO - __main__ -   Batch number = 176
11/28/2021 01:13:50 - INFO - __main__ -   Batch number = 48
11/28/2021 01:13:50 - INFO - __main__ -   Batch number = 49
11/28/2021 01:13:50 - INFO - __main__ -   Batch number = 50
11/28/2021 01:13:50 - INFO - __main__ -   Batch number = 177
11/28/2021 01:13:50 - INFO - __main__ -   Batch number = 51
11/28/2021 01:13:50 - INFO - __main__ -   Batch number = 52
11/28/2021 01:13:50 - INFO - __main__ -   Batch number = 178
11/28/2021 01:13:50 - INFO - __main__ -   Batch number = 53
11/28/2021 01:13:51 - INFO - __main__ -   Batch number = 54
11/28/2021 01:13:51 - INFO - __main__ -   Batch number = 179
11/28/2021 01:13:51 - INFO - __main__ -   Batch number = 55
11/28/2021 01:13:51 - INFO - __main__ -   Batch number = 56
11/28/2021 01:13:51 - INFO - __main__ -   Batch number = 57
11/28/2021 01:13:51 - INFO - __main__ -   Batch number = 180
11/28/2021 01:13:51 - INFO - __main__ -   Batch number = 181
11/28/2021 01:13:52 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='wo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:13:52 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:13:52 - INFO - __main__ -   Seed = 2
11/28/2021 01:13:52 - INFO - root -   save model
11/28/2021 01:13:52 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='wo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:13:52 - INFO - __main__ -   Loading pretrained model and tokenizer
11/28/2021 01:13:52 - INFO - __main__ -   ***** Evaluation result  in eu *****
11/28/2021 01:13:52 - INFO - __main__ -     f1 = 0.5573555166374782
11/28/2021 01:13:52 - INFO - __main__ -     loss = 1.4115445781172367
11/28/2021 01:13:52 - INFO - __main__ -     precision = 0.5951997506363967
11/28/2021 01:13:52 - INFO - __main__ -     recall = 0.5240360426290994
11/28/2021 01:13:52 - INFO - __main__ -   Batch number = 182
11/28/2021 01:13:52 - INFO - __main__ -   Batch number = 183
11/28/2021 01:13:52 - INFO - __main__ -   Batch number = 184
11/28/2021 01:13:53 - INFO - __main__ -   Batch number = 185
11/28/2021 01:13:53 - INFO - __main__ -   Batch number = 186
11/28/2021 01:13:53 - INFO - __main__ -   Batch number = 187
11/28/2021 01:13:53 - INFO - __main__ -   Batch number = 188
11/28/2021 01:13:54 - INFO - __main__ -   Batch number = 189
11/28/2021 01:13:54 - INFO - __main__ -   Batch number = 190
11/28/2021 01:13:54 - INFO - __main__ -   Batch number = 191
11/28/2021 01:13:54 - INFO - __main__ -   Batch number = 192
11/28/2021 01:13:54 - INFO - __main__ -   Batch number = 193
11/28/2021 01:13:54 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
11/28/2021 01:13:54 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='eu', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:13:54 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:13:54 - INFO - __main__ -   Seed = 2
11/28/2021 01:13:54 - INFO - root -   save model
11/28/2021 01:13:54 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='eu', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:13:54 - INFO - __main__ -   Loading pretrained model and tokenizer
11/28/2021 01:13:54 - INFO - __main__ -   Batch number = 194
11/28/2021 01:13:55 - INFO - __main__ -   Batch number = 195
11/28/2021 01:13:55 - INFO - __main__ -   Batch number = 196
11/28/2021 01:13:55 - INFO - __main__ -   Batch number = 197
11/28/2021 01:13:55 - INFO - __main__ -   Batch number = 198
11/28/2021 01:13:55 - INFO - __main__ -   Batch number = 199
11/28/2021 01:13:55 - INFO - __main__ -   Batch number = 200
11/28/2021 01:13:56 - INFO - __main__ -   Batch number = 201
11/28/2021 01:13:56 - INFO - __main__ -   Batch number = 202
11/28/2021 01:13:56 - INFO - __main__ -   Batch number = 203
11/28/2021 01:13:56 - INFO - __main__ -   Batch number = 204
11/28/2021 01:13:56 - INFO - __main__ -   Batch number = 205
11/28/2021 01:13:57 - INFO - __main__ -   Batch number = 206
11/28/2021 01:13:57 - INFO - __main__ -   Batch number = 207
11/28/2021 01:13:57 - INFO - __main__ -   Batch number = 208
11/28/2021 01:13:57 - INFO - __main__ -   Batch number = 209
11/28/2021 01:13:57 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
11/28/2021 01:13:57 - INFO - __main__ -   Batch number = 210
11/28/2021 01:13:57 - INFO - __main__ -   Batch number = 211
11/28/2021 01:13:58 - INFO - __main__ -   Batch number = 212
11/28/2021 01:13:58 - INFO - __main__ -   Batch number = 213
11/28/2021 01:13:58 - INFO - __main__ -   Batch number = 214
11/28/2021 01:13:58 - INFO - __main__ -   Batch number = 215
11/28/2021 01:13:59 - INFO - __main__ -   Batch number = 216
11/28/2021 01:13:59 - INFO - __main__ -   Batch number = 217
11/28/2021 01:13:59 - INFO - __main__ -   Batch number = 218
11/28/2021 01:13:59 - INFO - __main__ -   Batch number = 219
11/28/2021 01:13:59 - INFO - __main__ -   Batch number = 220
11/28/2021 01:14:00 - INFO - __main__ -   Batch number = 221
11/28/2021 01:14:00 - INFO - __main__ -   Batch number = 222
11/28/2021 01:14:00 - INFO - __main__ -   Batch number = 223
11/28/2021 01:14:00 - INFO - __main__ -   Batch number = 224
11/28/2021 01:14:00 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:14:00 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:14:00 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
11/28/2021 01:14:00 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:14:00 - INFO - root -   loading task adapter
11/28/2021 01:14:00 - INFO - __main__ -   Batch number = 225
11/28/2021 01:14:00 - INFO - root -   loading lang adpater bn/wiki@ukp
11/28/2021 01:14:00 - INFO - __main__ -   Adapter Languages : ['bn'], Length : 1
11/28/2021 01:14:00 - INFO - __main__ -   Adapter Names ['bn/wiki@ukp'], Length : 1
11/28/2021 01:14:00 - INFO - __main__ -   Language = bn
11/28/2021 01:14:00 - INFO - __main__ -   Adapter Name = bn/wiki@ukp
11/28/2021 01:14:00 - INFO - __main__ -   Batch number = 226
11/28/2021 01:14:01 - INFO - __main__ -   Batch number = 227
11/28/2021 01:14:01 - INFO - __main__ -   Batch number = 228
11/28/2021 01:14:01 - INFO - __main__ -   Batch number = 229
11/28/2021 01:14:01 - INFO - __main__ -   Batch number = 230
11/28/2021 01:14:01 - INFO - __main__ -   Batch number = 231
11/28/2021 01:14:02 - INFO - __main__ -   Batch number = 232
11/28/2021 01:14:02 - INFO - __main__ -   Batch number = 233
11/28/2021 01:14:02 - INFO - __main__ -   Batch number = 234
11/28/2021 01:14:02 - INFO - __main__ -   Batch number = 235
11/28/2021 01:14:02 - INFO - __main__ -   Batch number = 236
11/28/2021 01:14:03 - INFO - __main__ -   Batch number = 237
11/28/2021 01:14:03 - INFO - __main__ -   Batch number = 238
11/28/2021 01:14:03 - INFO - __main__ -   Batch number = 239
11/28/2021 01:14:03 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:14:03 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:14:03 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
11/28/2021 01:14:03 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:14:03 - INFO - root -   loading task adapter
11/28/2021 01:14:03 - INFO - root -   loading lang adpater bn/wiki@ukp
11/28/2021 01:14:03 - INFO - __main__ -   Adapter Languages : ['bn'], Length : 1
11/28/2021 01:14:03 - INFO - __main__ -   Adapter Names ['bn/wiki@ukp'], Length : 1
11/28/2021 01:14:03 - INFO - __main__ -   Language = bn
11/28/2021 01:14:03 - INFO - __main__ -   Adapter Name = bn/wiki@ukp
11/28/2021 01:14:04 - INFO - __main__ -   Batch number = 240
11/28/2021 01:14:04 - INFO - __main__ -   Batch number = 241
11/28/2021 01:14:04 - INFO - __main__ -   Batch number = 242
11/28/2021 01:14:05 - INFO - __main__ -   Batch number = 243
11/28/2021 01:14:05 - INFO - __main__ -   Batch number = 244
11/28/2021 01:14:05 - INFO - __main__ -   Batch number = 245
11/28/2021 01:14:06 - INFO - __main__ -   Batch number = 246
11/28/2021 01:14:06 - INFO - __main__ -   Batch number = 247
11/28/2021 01:14:06 - INFO - __main__ -   Batch number = 248
11/28/2021 01:14:07 - INFO - __main__ -   Batch number = 249
11/28/2021 01:14:07 - INFO - __main__ -   Batch number = 250
11/28/2021 01:14:07 - INFO - __main__ -   Batch number = 251
11/28/2021 01:14:08 - INFO - __main__ -   Language wo, split test does not exist
11/28/2021 01:14:08 - INFO - __main__ -   Batch number = 252
11/28/2021 01:14:08 - INFO - __main__ -   Batch number = 253
11/28/2021 01:14:08 - INFO - __main__ -   Batch number = 254
11/28/2021 01:14:09 - INFO - __main__ -   Batch number = 255
11/28/2021 01:14:09 - INFO - __main__ -   Batch number = 256
11/28/2021 01:14:10 - INFO - __main__ -   Batch number = 257
11/28/2021 01:14:10 - INFO - __main__ -   Batch number = 258
11/28/2021 01:14:10 - INFO - __main__ -   Batch number = 259
11/28/2021 01:14:11 - INFO - __main__ -   Language adapter for eu not found, using bn instead
11/28/2021 01:14:11 - INFO - __main__ -   Set active language adapter to bn
11/28/2021 01:14:11 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:14:11 - INFO - __main__ -   Adapter Languages = ['bn']
11/28/2021 01:14:11 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_eu_bert-base-multilingual-cased_128
11/28/2021 01:14:11 - INFO - __main__ -   Batch number = 260
11/28/2021 01:14:11 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='wo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:14:11 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:14:11 - INFO - __main__ -   Seed = 3
11/28/2021 01:14:11 - INFO - root -   save model
11/28/2021 01:14:11 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='wo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:14:11 - INFO - __main__ -   Loading pretrained model and tokenizer
11/28/2021 01:14:11 - INFO - __main__ -   ***** Running evaluation  in eu *****
11/28/2021 01:14:11 - INFO - __main__ -     Num examples = 1799
11/28/2021 01:14:11 - INFO - __main__ -     Batch size = 32
11/28/2021 01:14:11 - INFO - __main__ -   Batch number = 1
11/28/2021 01:14:11 - INFO - __main__ -   Batch number = 261
11/28/2021 01:14:11 - INFO - __main__ -   Batch number = 2
11/28/2021 01:14:11 - INFO - __main__ -   Batch number = 3
11/28/2021 01:14:11 - INFO - __main__ -   Batch number = 262
11/28/2021 01:14:11 - INFO - __main__ -   Batch number = 4
11/28/2021 01:14:12 - INFO - __main__ -   Batch number = 5
11/28/2021 01:14:12 - INFO - __main__ -   Batch number = 6
11/28/2021 01:14:12 - INFO - __main__ -   Batch number = 263
11/28/2021 01:14:12 - INFO - __main__ -   Batch number = 7
11/28/2021 01:14:12 - INFO - __main__ -   Batch number = 8
11/28/2021 01:14:12 - INFO - __main__ -   Batch number = 264
11/28/2021 01:14:12 - INFO - __main__ -   Batch number = 9
11/28/2021 01:14:12 - INFO - __main__ -   Batch number = 10
11/28/2021 01:14:12 - INFO - __main__ -   Batch number = 11
11/28/2021 01:14:12 - INFO - __main__ -   Batch number = 265
11/28/2021 01:14:13 - INFO - __main__ -   Batch number = 12
11/28/2021 01:14:13 - INFO - __main__ -   Batch number = 13
11/28/2021 01:14:13 - INFO - __main__ -   Batch number = 266
11/28/2021 01:14:13 - INFO - __main__ -   Batch number = 14
11/28/2021 01:14:13 - INFO - __main__ -   Batch number = 15
11/28/2021 01:14:13 - INFO - __main__ -   Batch number = 267
11/28/2021 01:14:13 - INFO - __main__ -   Batch number = 16
11/28/2021 01:14:13 - INFO - __main__ -   Batch number = 17
11/28/2021 01:14:13 - INFO - __main__ -   Batch number = 18
11/28/2021 01:14:13 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
11/28/2021 01:14:14 - INFO - __main__ -   Batch number = 268
11/28/2021 01:14:14 - INFO - __main__ -   Batch number = 19
11/28/2021 01:14:14 - INFO - __main__ -   Batch number = 20
11/28/2021 01:14:14 - INFO - __main__ -   Batch number = 269
11/28/2021 01:14:14 - INFO - __main__ -   Batch number = 21
11/28/2021 01:14:14 - INFO - __main__ -   Batch number = 22
11/28/2021 01:14:14 - INFO - __main__ -   Batch number = 23
11/28/2021 01:14:14 - INFO - __main__ -   Batch number = 270
11/28/2021 01:14:15 - INFO - __main__ -   Batch number = 24
11/28/2021 01:14:15 - INFO - __main__ -   Batch number = 271
11/28/2021 01:14:15 - INFO - __main__ -   Batch number = 25
11/28/2021 01:14:15 - INFO - __main__ -   Batch number = 26
11/28/2021 01:14:15 - INFO - __main__ -   Batch number = 27
11/28/2021 01:14:15 - INFO - __main__ -   Batch number = 28
11/28/2021 01:14:15 - INFO - __main__ -   Batch number = 29
11/28/2021 01:14:15 - INFO - __main__ -   Batch number = 272
11/28/2021 01:14:15 - INFO - __main__ -   Batch number = 30
11/28/2021 01:14:16 - INFO - __main__ -   Batch number = 31
11/28/2021 01:14:16 - INFO - __main__ -   Batch number = 32
11/28/2021 01:14:16 - INFO - __main__ -   Batch number = 273
11/28/2021 01:14:16 - INFO - __main__ -   Batch number = 33
11/28/2021 01:14:16 - INFO - __main__ -   Batch number = 34
11/28/2021 01:14:16 - INFO - __main__ -   Batch number = 274
11/28/2021 01:14:16 - INFO - __main__ -   Batch number = 35
11/28/2021 01:14:16 - INFO - __main__ -   Batch number = 36
11/28/2021 01:14:16 - INFO - __main__ -   Batch number = 37
11/28/2021 01:14:17 - INFO - __main__ -   Batch number = 275
11/28/2021 01:14:17 - INFO - __main__ -   Batch number = 38
11/28/2021 01:14:17 - INFO - __main__ -   Batch number = 39
11/28/2021 01:14:17 - INFO - __main__ -   Batch number = 276
11/28/2021 01:14:17 - INFO - __main__ -   Batch number = 40
11/28/2021 01:14:17 - INFO - __main__ -   Batch number = 41
11/28/2021 01:14:17 - INFO - __main__ -   Batch number = 42
11/28/2021 01:14:17 - INFO - __main__ -   Batch number = 277
11/28/2021 01:14:17 - INFO - __main__ -   Batch number = 43
11/28/2021 01:14:18 - INFO - __main__ -   Batch number = 44
11/28/2021 01:14:18 - INFO - __main__ -   Batch number = 278
11/28/2021 01:14:18 - INFO - __main__ -   Batch number = 45
11/28/2021 01:14:18 - INFO - __main__ -   Batch number = 46
11/28/2021 01:14:18 - INFO - __main__ -   Batch number = 279
11/28/2021 01:14:18 - INFO - __main__ -   Batch number = 47
11/28/2021 01:14:18 - INFO - __main__ -   Batch number = 48
11/28/2021 01:14:18 - INFO - __main__ -   Batch number = 49
11/28/2021 01:14:18 - INFO - __main__ -   Batch number = 280
11/28/2021 01:14:18 - INFO - __main__ -   Batch number = 50
11/28/2021 01:14:19 - INFO - __main__ -   Batch number = 51
11/28/2021 01:14:19 - INFO - __main__ -   Batch number = 281
11/28/2021 01:14:19 - INFO - __main__ -   Batch number = 52
11/28/2021 01:14:19 - INFO - __main__ -   Batch number = 53
11/28/2021 01:14:19 - INFO - __main__ -   Batch number = 54
11/28/2021 01:14:19 - INFO - __main__ -   Batch number = 282
11/28/2021 01:14:19 - INFO - __main__ -   Batch number = 55
11/28/2021 01:14:19 - INFO - __main__ -   Batch number = 56
11/28/2021 01:14:20 - INFO - __main__ -   Batch number = 57
11/28/2021 01:14:20 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:14:20 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:14:20 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
11/28/2021 01:14:20 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:14:20 - INFO - root -   loading task adapter
11/28/2021 01:14:20 - INFO - root -   loading lang adpater bn/wiki@ukp
11/28/2021 01:14:20 - INFO - __main__ -   Adapter Languages : ['bn'], Length : 1
11/28/2021 01:14:20 - INFO - __main__ -   Adapter Names ['bn/wiki@ukp'], Length : 1
11/28/2021 01:14:20 - INFO - __main__ -   Language = bn
11/28/2021 01:14:20 - INFO - __main__ -   Adapter Name = bn/wiki@ukp
11/28/2021 01:14:20 - INFO - __main__ -   ***** Evaluation result  in eu *****
11/28/2021 01:14:20 - INFO - __main__ -     f1 = 0.5780911584925639
11/28/2021 01:14:20 - INFO - __main__ -     loss = 1.322387180830303
11/28/2021 01:14:20 - INFO - __main__ -     precision = 0.6099740998425677
11/28/2021 01:14:20 - INFO - __main__ -     recall = 0.5493756575035448
11/28/2021 01:14:23 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='eu', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:14:23 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:14:23 - INFO - __main__ -   Seed = 3
11/28/2021 01:14:23 - INFO - root -   save model
11/28/2021 01:14:23 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='eu', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:14:23 - INFO - __main__ -   Loading pretrained model and tokenizer
11/28/2021 01:14:24 - INFO - __main__ -   ***** Evaluation result  in ru *****
11/28/2021 01:14:24 - INFO - __main__ -     f1 = 0.8453560632955457
11/28/2021 01:14:24 - INFO - __main__ -     loss = 0.5946703818461574
11/28/2021 01:14:24 - INFO - __main__ -     precision = 0.8494462030796511
11/28/2021 01:14:24 - INFO - __main__ -     recall = 0.8413051233500727
11/28/2021 01:14:26 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
11/28/2021 01:14:27 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ru', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:14:27 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:14:27 - INFO - __main__ -   Seed = 3
11/28/2021 01:14:27 - INFO - root -   save model
11/28/2021 01:14:27 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ru', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:14:27 - INFO - __main__ -   Loading pretrained model and tokenizer
11/28/2021 01:14:27 - INFO - __main__ -   Language wo, split test does not exist
11/28/2021 01:14:29 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
11/28/2021 01:14:32 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:14:32 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:14:32 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
11/28/2021 01:14:32 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:14:32 - INFO - root -   loading task adapter
11/28/2021 01:14:32 - INFO - root -   loading lang adpater bn/wiki@ukp
11/28/2021 01:14:32 - INFO - __main__ -   Adapter Languages : ['bn'], Length : 1
11/28/2021 01:14:32 - INFO - __main__ -   Adapter Names ['bn/wiki@ukp'], Length : 1
11/28/2021 01:14:32 - INFO - __main__ -   Language = bn
11/28/2021 01:14:32 - INFO - __main__ -   Adapter Name = bn/wiki@ukp
11/28/2021 01:14:36 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:14:36 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:14:36 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
11/28/2021 01:14:36 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:14:36 - INFO - root -   loading task adapter
11/28/2021 01:14:36 - INFO - root -   loading lang adpater bn/wiki@ukp
11/28/2021 01:14:36 - INFO - __main__ -   Adapter Languages : ['bn'], Length : 1
11/28/2021 01:14:36 - INFO - __main__ -   Adapter Names ['bn/wiki@ukp'], Length : 1
11/28/2021 01:14:36 - INFO - __main__ -   Language = bn
11/28/2021 01:14:36 - INFO - __main__ -   Adapter Name = bn/wiki@ukp
11/28/2021 01:14:39 - INFO - __main__ -   Language adapter for eu not found, using bn instead
11/28/2021 01:14:39 - INFO - __main__ -   Set active language adapter to bn
11/28/2021 01:14:39 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:14:39 - INFO - __main__ -   Adapter Languages = ['bn']
11/28/2021 01:14:39 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_eu_bert-base-multilingual-cased_128
11/28/2021 01:14:39 - INFO - __main__ -   ***** Running evaluation  in eu *****
11/28/2021 01:14:39 - INFO - __main__ -     Num examples = 1799
11/28/2021 01:14:39 - INFO - __main__ -     Batch size = 32
11/28/2021 01:14:39 - INFO - __main__ -   Batch number = 1
11/28/2021 01:14:39 - INFO - __main__ -   Batch number = 2
11/28/2021 01:14:39 - INFO - __main__ -   Batch number = 3
11/28/2021 01:14:40 - INFO - __main__ -   Batch number = 4
11/28/2021 01:14:40 - INFO - __main__ -   Batch number = 5
11/28/2021 01:14:40 - INFO - __main__ -   Batch number = 6
11/28/2021 01:14:40 - INFO - __main__ -   Batch number = 7
11/28/2021 01:14:40 - INFO - __main__ -   Batch number = 8
11/28/2021 01:14:40 - INFO - __main__ -   Batch number = 9
11/28/2021 01:14:40 - INFO - __main__ -   Batch number = 10
11/28/2021 01:14:41 - INFO - __main__ -   Batch number = 11
11/28/2021 01:14:41 - INFO - __main__ -   Batch number = 12
11/28/2021 01:14:41 - INFO - __main__ -   Batch number = 13
11/28/2021 01:14:41 - INFO - __main__ -   Batch number = 14
11/28/2021 01:14:41 - INFO - __main__ -   Batch number = 15
11/28/2021 01:14:41 - INFO - __main__ -   Batch number = 16
11/28/2021 01:14:41 - INFO - __main__ -   Batch number = 17
11/28/2021 01:14:42 - INFO - __main__ -   Batch number = 18
11/28/2021 01:14:42 - INFO - __main__ -   Batch number = 19
11/28/2021 01:14:42 - INFO - __main__ -   Batch number = 20
11/28/2021 01:14:42 - INFO - __main__ -   Batch number = 21
11/28/2021 01:14:42 - INFO - __main__ -   Batch number = 22
11/28/2021 01:14:42 - INFO - __main__ -   Batch number = 23
11/28/2021 01:14:42 - INFO - __main__ -   Batch number = 24
11/28/2021 01:14:43 - INFO - __main__ -   Batch number = 25
11/28/2021 01:14:43 - INFO - __main__ -   Batch number = 26
11/28/2021 01:14:43 - INFO - __main__ -   Batch number = 27
11/28/2021 01:14:43 - INFO - __main__ -   Batch number = 28
11/28/2021 01:14:43 - INFO - __main__ -   Batch number = 29
11/28/2021 01:14:43 - INFO - __main__ -   Batch number = 30
11/28/2021 01:14:44 - INFO - __main__ -   Batch number = 31
11/28/2021 01:14:44 - INFO - __main__ -   Batch number = 32
11/28/2021 01:14:44 - INFO - __main__ -   Batch number = 33
11/28/2021 01:14:44 - INFO - __main__ -   Batch number = 34
11/28/2021 01:14:44 - INFO - __main__ -   Batch number = 35
11/28/2021 01:14:44 - INFO - __main__ -   Batch number = 36
11/28/2021 01:14:44 - INFO - __main__ -   Language adapter for ru not found, using bn instead
11/28/2021 01:14:44 - INFO - __main__ -   Set active language adapter to bn
11/28/2021 01:14:44 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:14:44 - INFO - __main__ -   Adapter Languages = ['bn']
11/28/2021 01:14:44 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_ru_bert-base-multilingual-cased_128
11/28/2021 01:14:44 - INFO - __main__ -   Batch number = 37
11/28/2021 01:14:45 - INFO - __main__ -   Batch number = 38
11/28/2021 01:14:45 - INFO - __main__ -   Batch number = 39
11/28/2021 01:14:45 - INFO - __main__ -   Batch number = 40
11/28/2021 01:14:45 - INFO - __main__ -   Batch number = 41
11/28/2021 01:14:45 - INFO - __main__ -   Batch number = 42
11/28/2021 01:14:45 - INFO - __main__ -   Batch number = 43
11/28/2021 01:14:46 - INFO - __main__ -   Batch number = 44
11/28/2021 01:14:46 - INFO - __main__ -   ***** Running evaluation  in ru *****
11/28/2021 01:14:46 - INFO - __main__ -     Num examples = 8995
11/28/2021 01:14:46 - INFO - __main__ -     Batch size = 32
11/28/2021 01:14:46 - INFO - __main__ -   Batch number = 1
11/28/2021 01:14:46 - INFO - __main__ -   Batch number = 45
11/28/2021 01:14:46 - INFO - __main__ -   Batch number = 46
11/28/2021 01:14:46 - INFO - __main__ -   Batch number = 2
11/28/2021 01:14:46 - INFO - __main__ -   Batch number = 47
11/28/2021 01:14:46 - INFO - __main__ -   Batch number = 3
11/28/2021 01:14:46 - INFO - __main__ -   Batch number = 48
11/28/2021 01:14:46 - INFO - __main__ -   Batch number = 4
11/28/2021 01:14:46 - INFO - __main__ -   Batch number = 49
11/28/2021 01:14:46 - INFO - __main__ -   Batch number = 5
11/28/2021 01:14:46 - INFO - __main__ -   Batch number = 50
11/28/2021 01:14:47 - INFO - __main__ -   Batch number = 6
11/28/2021 01:14:47 - INFO - __main__ -   Batch number = 51
11/28/2021 01:14:47 - INFO - __main__ -   Batch number = 7
11/28/2021 01:14:47 - INFO - __main__ -   Batch number = 52
11/28/2021 01:14:47 - INFO - __main__ -   Batch number = 8
11/28/2021 01:14:47 - INFO - __main__ -   Batch number = 53
11/28/2021 01:14:47 - INFO - __main__ -   Batch number = 9
11/28/2021 01:14:47 - INFO - __main__ -   Batch number = 54
11/28/2021 01:14:47 - INFO - __main__ -   Batch number = 10
11/28/2021 01:14:47 - INFO - __main__ -   Batch number = 55
11/28/2021 01:14:47 - INFO - __main__ -   Batch number = 11
11/28/2021 01:14:47 - INFO - __main__ -   Batch number = 56
11/28/2021 01:14:47 - INFO - __main__ -   Batch number = 12
11/28/2021 01:14:48 - INFO - __main__ -   Batch number = 57
11/28/2021 01:14:48 - INFO - __main__ -   Batch number = 13
11/28/2021 01:14:48 - INFO - __main__ -   Batch number = 14
11/28/2021 01:14:48 - INFO - __main__ -   Batch number = 15
11/28/2021 01:14:48 - INFO - __main__ -   Batch number = 16
11/28/2021 01:14:48 - INFO - __main__ -   Batch number = 17
11/28/2021 01:14:48 - INFO - __main__ -   ***** Evaluation result  in eu *****
11/28/2021 01:14:48 - INFO - __main__ -     f1 = 0.5665962971051417
11/28/2021 01:14:48 - INFO - __main__ -     loss = 1.3420254284875435
11/28/2021 01:14:48 - INFO - __main__ -     precision = 0.6034215422782716
11/28/2021 01:14:48 - INFO - __main__ -     recall = 0.5340072268215706
11/28/2021 01:14:48 - INFO - __main__ -   Batch number = 18
11/28/2021 01:14:48 - INFO - __main__ -   Batch number = 19
11/28/2021 01:14:49 - INFO - __main__ -   Batch number = 20
11/28/2021 01:14:49 - INFO - __main__ -   Batch number = 21
11/28/2021 01:14:49 - INFO - __main__ -   Batch number = 22
11/28/2021 01:14:49 - INFO - __main__ -   Batch number = 23
11/28/2021 01:14:49 - INFO - __main__ -   Batch number = 24
11/28/2021 01:14:49 - INFO - __main__ -   Batch number = 25
11/28/2021 01:14:50 - INFO - __main__ -   Batch number = 26
11/28/2021 01:14:50 - INFO - __main__ -   Batch number = 27
11/28/2021 01:14:50 - INFO - __main__ -   Batch number = 28
11/28/2021 01:14:50 - INFO - __main__ -   Batch number = 29
11/28/2021 01:14:50 - INFO - __main__ -   Batch number = 30
11/28/2021 01:14:50 - INFO - __main__ -   Batch number = 31
11/28/2021 01:14:51 - INFO - __main__ -   Batch number = 32
11/28/2021 01:14:51 - INFO - __main__ -   Batch number = 33
11/28/2021 01:14:51 - INFO - __main__ -   Batch number = 34
11/28/2021 01:14:51 - INFO - __main__ -   Batch number = 35
11/28/2021 01:14:51 - INFO - __main__ -   Batch number = 36
11/28/2021 01:14:51 - INFO - __main__ -   Batch number = 37
11/28/2021 01:14:51 - INFO - __main__ -   Batch number = 38
11/28/2021 01:14:52 - INFO - __main__ -   Batch number = 39
11/28/2021 01:14:52 - INFO - __main__ -   Batch number = 40
11/28/2021 01:14:52 - INFO - __main__ -   Batch number = 41
11/28/2021 01:14:52 - INFO - __main__ -   Batch number = 42
11/28/2021 01:14:52 - INFO - __main__ -   Batch number = 43
11/28/2021 01:14:52 - INFO - __main__ -   Batch number = 44
11/28/2021 01:14:53 - INFO - __main__ -   Batch number = 45
11/28/2021 01:14:53 - INFO - __main__ -   Batch number = 46
11/28/2021 01:14:53 - INFO - __main__ -   Batch number = 47
11/28/2021 01:14:53 - INFO - __main__ -   Batch number = 48
11/28/2021 01:14:53 - INFO - __main__ -   Batch number = 49
11/28/2021 01:14:53 - INFO - __main__ -   Batch number = 50
11/28/2021 01:14:54 - INFO - __main__ -   Batch number = 51
11/28/2021 01:14:54 - INFO - __main__ -   Batch number = 52
11/28/2021 01:14:54 - INFO - __main__ -   Batch number = 53
11/28/2021 01:14:54 - INFO - __main__ -   Batch number = 54
11/28/2021 01:14:54 - INFO - __main__ -   Batch number = 55
11/28/2021 01:14:54 - INFO - __main__ -   Batch number = 56
11/28/2021 01:14:55 - INFO - __main__ -   Batch number = 57
11/28/2021 01:14:55 - INFO - __main__ -   Batch number = 58
11/28/2021 01:14:55 - INFO - __main__ -   Batch number = 59
11/28/2021 01:14:55 - INFO - __main__ -   Batch number = 60
11/28/2021 01:14:56 - INFO - __main__ -   Batch number = 61
11/28/2021 01:14:56 - INFO - __main__ -   Batch number = 62
11/28/2021 01:14:56 - INFO - __main__ -   Batch number = 63
11/28/2021 01:14:56 - INFO - __main__ -   Batch number = 64
11/28/2021 01:14:56 - INFO - __main__ -   Batch number = 65
11/28/2021 01:14:56 - INFO - __main__ -   Batch number = 66
11/28/2021 01:14:57 - INFO - __main__ -   Batch number = 67
11/28/2021 01:14:57 - INFO - __main__ -   Batch number = 68
11/28/2021 01:14:57 - INFO - __main__ -   Batch number = 69
11/28/2021 01:14:57 - INFO - __main__ -   Batch number = 70
11/28/2021 01:14:57 - INFO - __main__ -   Batch number = 71
11/28/2021 01:14:57 - INFO - __main__ -   Batch number = 72
11/28/2021 01:14:57 - INFO - __main__ -   Batch number = 73
11/28/2021 01:14:58 - INFO - __main__ -   Batch number = 74
11/28/2021 01:14:58 - INFO - __main__ -   Batch number = 75
11/28/2021 01:14:58 - INFO - __main__ -   Batch number = 76
11/28/2021 01:14:58 - INFO - __main__ -   Batch number = 77
11/28/2021 01:14:59 - INFO - __main__ -   Batch number = 78
11/28/2021 01:14:59 - INFO - __main__ -   Batch number = 79
11/28/2021 01:14:59 - INFO - __main__ -   Batch number = 80
11/28/2021 01:15:00 - INFO - __main__ -   Batch number = 81
11/28/2021 01:15:00 - INFO - __main__ -   Batch number = 82
11/28/2021 01:15:00 - INFO - __main__ -   Batch number = 83
11/28/2021 01:15:01 - INFO - __main__ -   Batch number = 84
11/28/2021 01:15:01 - INFO - __main__ -   Batch number = 85
11/28/2021 01:15:01 - INFO - __main__ -   Batch number = 86
11/28/2021 01:15:02 - INFO - __main__ -   Batch number = 87
11/28/2021 01:15:02 - INFO - __main__ -   Batch number = 88
11/28/2021 01:15:02 - INFO - __main__ -   Batch number = 89
11/28/2021 01:15:03 - INFO - __main__ -   Batch number = 90
11/28/2021 01:15:03 - INFO - __main__ -   Batch number = 91
11/28/2021 01:15:03 - INFO - __main__ -   Batch number = 92
11/28/2021 01:15:04 - INFO - __main__ -   Batch number = 93
11/28/2021 01:15:04 - INFO - __main__ -   Batch number = 94
11/28/2021 01:15:05 - INFO - __main__ -   Batch number = 95
11/28/2021 01:15:05 - INFO - __main__ -   Batch number = 96
11/28/2021 01:15:05 - INFO - __main__ -   Batch number = 97
11/28/2021 01:15:05 - INFO - __main__ -   Batch number = 98
11/28/2021 01:15:06 - INFO - __main__ -   Batch number = 99
11/28/2021 01:15:06 - INFO - __main__ -   Batch number = 100
11/28/2021 01:15:06 - INFO - __main__ -   Batch number = 101
11/28/2021 01:15:07 - INFO - __main__ -   Batch number = 102
11/28/2021 01:15:08 - INFO - __main__ -   Batch number = 103
11/28/2021 01:15:08 - INFO - __main__ -   Batch number = 104
11/28/2021 01:15:09 - INFO - __main__ -   Batch number = 105
11/28/2021 01:15:09 - INFO - __main__ -   Batch number = 106
11/28/2021 01:15:10 - INFO - __main__ -   Batch number = 107
11/28/2021 01:15:10 - INFO - __main__ -   Batch number = 108
11/28/2021 01:15:11 - INFO - __main__ -   Batch number = 109
11/28/2021 01:15:11 - INFO - __main__ -   Batch number = 110
11/28/2021 01:15:12 - INFO - __main__ -   Batch number = 111
11/28/2021 01:15:12 - INFO - __main__ -   Batch number = 112
11/28/2021 01:15:13 - INFO - __main__ -   Batch number = 113
11/28/2021 01:15:13 - INFO - __main__ -   Batch number = 114
11/28/2021 01:15:14 - INFO - __main__ -   Batch number = 115
11/28/2021 01:15:14 - INFO - __main__ -   Batch number = 116
11/28/2021 01:15:15 - INFO - __main__ -   Batch number = 117
11/28/2021 01:15:15 - INFO - __main__ -   Batch number = 118
11/28/2021 01:15:16 - INFO - __main__ -   Batch number = 119
11/28/2021 01:15:16 - INFO - __main__ -   Batch number = 120
11/28/2021 01:15:17 - INFO - __main__ -   Batch number = 121
11/28/2021 01:15:17 - INFO - __main__ -   Batch number = 122
11/28/2021 01:15:18 - INFO - __main__ -   Batch number = 123
11/28/2021 01:15:18 - INFO - __main__ -   Batch number = 124
11/28/2021 01:15:19 - INFO - __main__ -   Batch number = 125
11/28/2021 01:15:19 - INFO - __main__ -   Batch number = 126
11/28/2021 01:15:20 - INFO - __main__ -   Batch number = 127
11/28/2021 01:15:20 - INFO - __main__ -   Batch number = 128
11/28/2021 01:15:21 - INFO - __main__ -   Batch number = 129
11/28/2021 01:15:21 - INFO - __main__ -   Batch number = 130
11/28/2021 01:15:22 - INFO - __main__ -   Batch number = 131
11/28/2021 01:15:22 - INFO - __main__ -   Batch number = 132
11/28/2021 01:15:23 - INFO - __main__ -   Batch number = 133
11/28/2021 01:15:24 - INFO - __main__ -   Batch number = 134
11/28/2021 01:15:24 - INFO - __main__ -   Batch number = 135
11/28/2021 01:15:25 - INFO - __main__ -   Batch number = 136
11/28/2021 01:15:26 - INFO - __main__ -   Batch number = 137
11/28/2021 01:15:26 - INFO - __main__ -   Batch number = 138
11/28/2021 01:15:27 - INFO - __main__ -   Batch number = 139
11/28/2021 01:15:27 - INFO - __main__ -   Batch number = 140
11/28/2021 01:15:28 - INFO - __main__ -   Batch number = 141
11/28/2021 01:15:28 - INFO - __main__ -   Batch number = 142
11/28/2021 01:15:29 - INFO - __main__ -   Batch number = 143
11/28/2021 01:15:29 - INFO - __main__ -   Batch number = 144
11/28/2021 01:15:30 - INFO - __main__ -   Batch number = 145
11/28/2021 01:15:30 - INFO - __main__ -   Batch number = 146
11/28/2021 01:15:31 - INFO - __main__ -   Batch number = 147
11/28/2021 01:15:31 - INFO - __main__ -   Batch number = 148
11/28/2021 01:15:32 - INFO - __main__ -   Batch number = 149
11/28/2021 01:15:32 - INFO - __main__ -   Batch number = 150
11/28/2021 01:15:33 - INFO - __main__ -   Batch number = 151
11/28/2021 01:15:33 - INFO - __main__ -   Batch number = 152
11/28/2021 01:15:34 - INFO - __main__ -   Batch number = 153
11/28/2021 01:15:34 - INFO - __main__ -   Batch number = 154
11/28/2021 01:15:35 - INFO - __main__ -   Batch number = 155
11/28/2021 01:15:35 - INFO - __main__ -   Batch number = 156
11/28/2021 01:15:36 - INFO - __main__ -   Batch number = 157
11/28/2021 01:15:36 - INFO - __main__ -   Batch number = 158
11/28/2021 01:15:37 - INFO - __main__ -   Batch number = 159
11/28/2021 01:15:37 - INFO - __main__ -   Batch number = 160
11/28/2021 01:15:38 - INFO - __main__ -   Batch number = 161
11/28/2021 01:15:38 - INFO - __main__ -   Batch number = 162
11/28/2021 01:15:39 - INFO - __main__ -   Batch number = 163
11/28/2021 01:15:39 - INFO - __main__ -   Batch number = 164
11/28/2021 01:15:40 - INFO - __main__ -   Batch number = 165
11/28/2021 01:15:40 - INFO - __main__ -   Batch number = 166
11/28/2021 01:15:41 - INFO - __main__ -   Batch number = 167
11/28/2021 01:15:41 - INFO - __main__ -   Batch number = 168
11/28/2021 01:15:42 - INFO - __main__ -   Batch number = 169
11/28/2021 01:15:42 - INFO - __main__ -   Batch number = 170
11/28/2021 01:15:43 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bxr', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:15:43 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:15:43 - INFO - __main__ -   Seed = 1
11/28/2021 01:15:43 - INFO - root -   save model
11/28/2021 01:15:43 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bxr', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:15:43 - INFO - __main__ -   Loading pretrained model and tokenizer
11/28/2021 01:15:43 - INFO - __main__ -   Batch number = 171
11/28/2021 01:15:43 - INFO - __main__ -   Batch number = 172
11/28/2021 01:15:43 - INFO - __main__ -   Batch number = 173
11/28/2021 01:15:44 - INFO - __main__ -   Batch number = 174
11/28/2021 01:15:44 - INFO - __main__ -   Batch number = 175
11/28/2021 01:15:44 - INFO - __main__ -   Batch number = 176
11/28/2021 01:15:45 - INFO - __main__ -   Batch number = 177
11/28/2021 01:15:45 - INFO - __main__ -   Batch number = 178
11/28/2021 01:15:45 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
11/28/2021 01:15:45 - INFO - __main__ -   Batch number = 179
11/28/2021 01:15:46 - INFO - __main__ -   Batch number = 180
11/28/2021 01:15:46 - INFO - __main__ -   Batch number = 181
11/28/2021 01:15:46 - INFO - __main__ -   Batch number = 182
11/28/2021 01:15:46 - INFO - __main__ -   Batch number = 183
11/28/2021 01:15:46 - INFO - __main__ -   Batch number = 184
11/28/2021 01:15:47 - INFO - __main__ -   Batch number = 185
11/28/2021 01:15:47 - INFO - __main__ -   Batch number = 186
11/28/2021 01:15:47 - INFO - __main__ -   Batch number = 187
11/28/2021 01:15:47 - INFO - __main__ -   Batch number = 188
11/28/2021 01:15:47 - INFO - __main__ -   Batch number = 189
11/28/2021 01:15:48 - INFO - __main__ -   Batch number = 190
11/28/2021 01:15:48 - INFO - __main__ -   Batch number = 191
11/28/2021 01:15:48 - INFO - __main__ -   Batch number = 192
11/28/2021 01:15:48 - INFO - __main__ -   Batch number = 193
11/28/2021 01:15:48 - INFO - __main__ -   Batch number = 194
11/28/2021 01:15:49 - INFO - __main__ -   Batch number = 195
11/28/2021 01:15:49 - INFO - __main__ -   Batch number = 196
11/28/2021 01:15:49 - INFO - __main__ -   Batch number = 197
11/28/2021 01:15:49 - INFO - __main__ -   Batch number = 198
11/28/2021 01:15:49 - INFO - __main__ -   Batch number = 199
11/28/2021 01:15:49 - INFO - __main__ -   Batch number = 200
11/28/2021 01:15:50 - INFO - __main__ -   Batch number = 201
11/28/2021 01:15:50 - INFO - __main__ -   Batch number = 202
11/28/2021 01:15:50 - INFO - __main__ -   Batch number = 203
11/28/2021 01:15:50 - INFO - __main__ -   Batch number = 204
11/28/2021 01:15:50 - INFO - __main__ -   Batch number = 205
11/28/2021 01:15:51 - INFO - __main__ -   Batch number = 206
11/28/2021 01:15:51 - INFO - __main__ -   Batch number = 207
11/28/2021 01:15:51 - INFO - __main__ -   Batch number = 208
11/28/2021 01:15:51 - INFO - __main__ -   Batch number = 209
11/28/2021 01:15:51 - INFO - __main__ -   Batch number = 210
11/28/2021 01:15:51 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:15:51 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:15:51 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
11/28/2021 01:15:51 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:15:51 - INFO - root -   loading task adapter
11/28/2021 01:15:51 - INFO - root -   loading lang adpater bn/wiki@ukp
11/28/2021 01:15:51 - INFO - __main__ -   Adapter Languages : ['bn'], Length : 1
11/28/2021 01:15:51 - INFO - __main__ -   Adapter Names ['bn/wiki@ukp'], Length : 1
11/28/2021 01:15:51 - INFO - __main__ -   Language = bn
11/28/2021 01:15:51 - INFO - __main__ -   Adapter Name = bn/wiki@ukp
11/28/2021 01:15:52 - INFO - __main__ -   Batch number = 211
11/28/2021 01:15:52 - INFO - __main__ -   Batch number = 212
11/28/2021 01:15:52 - INFO - __main__ -   Batch number = 213
11/28/2021 01:15:52 - INFO - __main__ -   Batch number = 214
11/28/2021 01:15:53 - INFO - __main__ -   Batch number = 215
11/28/2021 01:15:53 - INFO - __main__ -   Batch number = 216
11/28/2021 01:15:53 - INFO - __main__ -   Batch number = 217
11/28/2021 01:15:53 - INFO - __main__ -   Batch number = 218
11/28/2021 01:15:53 - INFO - __main__ -   Batch number = 219
11/28/2021 01:15:54 - INFO - __main__ -   Batch number = 220
11/28/2021 01:15:54 - INFO - __main__ -   Batch number = 221
11/28/2021 01:15:54 - INFO - __main__ -   Batch number = 222
11/28/2021 01:15:54 - INFO - __main__ -   Batch number = 223
11/28/2021 01:15:54 - INFO - __main__ -   Batch number = 224
11/28/2021 01:15:55 - INFO - __main__ -   Batch number = 225
11/28/2021 01:15:55 - INFO - __main__ -   Batch number = 226
11/28/2021 01:15:55 - INFO - __main__ -   Batch number = 227
11/28/2021 01:15:55 - INFO - __main__ -   Batch number = 228
11/28/2021 01:15:55 - INFO - __main__ -   Batch number = 229
11/28/2021 01:15:55 - INFO - __main__ -   Batch number = 230
11/28/2021 01:15:56 - INFO - __main__ -   Batch number = 231
11/28/2021 01:15:56 - INFO - __main__ -   Batch number = 232
11/28/2021 01:15:56 - INFO - __main__ -   Batch number = 233
11/28/2021 01:15:56 - INFO - __main__ -   Batch number = 234
11/28/2021 01:15:56 - INFO - __main__ -   Batch number = 235
11/28/2021 01:15:56 - INFO - __main__ -   Batch number = 236
11/28/2021 01:15:57 - INFO - __main__ -   Batch number = 237
11/28/2021 01:15:57 - INFO - __main__ -   Batch number = 238
11/28/2021 01:15:57 - INFO - __main__ -   Batch number = 239
11/28/2021 01:15:57 - INFO - __main__ -   Batch number = 240
11/28/2021 01:15:57 - INFO - __main__ -   Batch number = 241
11/28/2021 01:15:58 - INFO - __main__ -   Batch number = 242
11/28/2021 01:15:58 - INFO - __main__ -   Batch number = 243
11/28/2021 01:15:58 - INFO - __main__ -   Batch number = 244
11/28/2021 01:15:58 - INFO - __main__ -   Batch number = 245
11/28/2021 01:15:58 - INFO - __main__ -   Batch number = 246
11/28/2021 01:15:58 - INFO - __main__ -   Batch number = 247
11/28/2021 01:15:59 - INFO - __main__ -   Batch number = 248
11/28/2021 01:15:59 - INFO - __main__ -   Batch number = 249
11/28/2021 01:15:59 - INFO - __main__ -   Batch number = 250
11/28/2021 01:15:59 - INFO - __main__ -   Batch number = 251
11/28/2021 01:15:59 - INFO - __main__ -   Batch number = 252
11/28/2021 01:16:00 - INFO - __main__ -   Batch number = 253
11/28/2021 01:16:00 - INFO - __main__ -   Language bxr, split test does not exist
11/28/2021 01:16:00 - INFO - __main__ -   Batch number = 254
11/28/2021 01:16:00 - INFO - __main__ -   Batch number = 255
11/28/2021 01:16:00 - INFO - __main__ -   Batch number = 256
11/28/2021 01:16:00 - INFO - __main__ -   Batch number = 257
11/28/2021 01:16:01 - INFO - __main__ -   Batch number = 258
11/28/2021 01:16:01 - INFO - __main__ -   Batch number = 259
11/28/2021 01:16:01 - INFO - __main__ -   Batch number = 260
11/28/2021 01:16:01 - INFO - __main__ -   Batch number = 261
11/28/2021 01:16:01 - INFO - __main__ -   Batch number = 262
11/28/2021 01:16:01 - INFO - __main__ -   Batch number = 263
11/28/2021 01:16:02 - INFO - __main__ -   Batch number = 264
11/28/2021 01:16:02 - INFO - __main__ -   Batch number = 265
11/28/2021 01:16:02 - INFO - __main__ -   Batch number = 266
11/28/2021 01:16:02 - INFO - __main__ -   Batch number = 267
11/28/2021 01:16:02 - INFO - __main__ -   Batch number = 268
11/28/2021 01:16:03 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bxr', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:16:03 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:16:03 - INFO - __main__ -   Seed = 2
11/28/2021 01:16:03 - INFO - root -   save model
11/28/2021 01:16:03 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bxr', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:16:03 - INFO - __main__ -   Loading pretrained model and tokenizer
11/28/2021 01:16:03 - INFO - __main__ -   Batch number = 269
11/28/2021 01:16:03 - INFO - __main__ -   Batch number = 270
11/28/2021 01:16:03 - INFO - __main__ -   Batch number = 271
11/28/2021 01:16:03 - INFO - __main__ -   Batch number = 272
11/28/2021 01:16:03 - INFO - __main__ -   Batch number = 273
11/28/2021 01:16:04 - INFO - __main__ -   Batch number = 274
11/28/2021 01:16:04 - INFO - __main__ -   Batch number = 275
11/28/2021 01:16:04 - INFO - __main__ -   Batch number = 276
11/28/2021 01:16:04 - INFO - __main__ -   Batch number = 277
11/28/2021 01:16:04 - INFO - __main__ -   Batch number = 278
11/28/2021 01:16:05 - INFO - __main__ -   Batch number = 279
11/28/2021 01:16:05 - INFO - __main__ -   Batch number = 280
11/28/2021 01:16:05 - INFO - __main__ -   Batch number = 281
11/28/2021 01:16:05 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
11/28/2021 01:16:05 - INFO - __main__ -   Batch number = 282
11/28/2021 01:16:10 - INFO - __main__ -   ***** Evaluation result  in ru *****
11/28/2021 01:16:10 - INFO - __main__ -     f1 = 0.8542054528153572
11/28/2021 01:16:10 - INFO - __main__ -     loss = 0.5292767325402997
11/28/2021 01:16:10 - INFO - __main__ -     precision = 0.8585745176314039
11/28/2021 01:16:10 - INFO - __main__ -     recall = 0.8498806289618835
11/28/2021 01:16:11 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:16:11 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:16:11 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
11/28/2021 01:16:11 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:16:11 - INFO - root -   loading task adapter
11/28/2021 01:16:11 - INFO - root -   loading lang adpater bn/wiki@ukp
11/28/2021 01:16:11 - INFO - __main__ -   Adapter Languages : ['bn'], Length : 1
11/28/2021 01:16:11 - INFO - __main__ -   Adapter Names ['bn/wiki@ukp'], Length : 1
11/28/2021 01:16:11 - INFO - __main__ -   Language = bn
11/28/2021 01:16:11 - INFO - __main__ -   Adapter Name = bn/wiki@ukp
11/28/2021 01:16:18 - INFO - __main__ -   Language bxr, split test does not exist
11/28/2021 01:16:21 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bxr', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:16:21 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:16:21 - INFO - __main__ -   Seed = 3
11/28/2021 01:16:21 - INFO - root -   save model
11/28/2021 01:16:21 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bxr', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:16:21 - INFO - __main__ -   Loading pretrained model and tokenizer
11/28/2021 01:16:23 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
11/28/2021 01:16:29 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:16:29 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:16:29 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
11/28/2021 01:16:29 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:16:29 - INFO - root -   loading task adapter
11/28/2021 01:16:29 - INFO - root -   loading lang adpater bn/wiki@ukp
11/28/2021 01:16:29 - INFO - __main__ -   Adapter Languages : ['bn'], Length : 1
11/28/2021 01:16:29 - INFO - __main__ -   Adapter Names ['bn/wiki@ukp'], Length : 1
11/28/2021 01:16:29 - INFO - __main__ -   Language = bn
11/28/2021 01:16:29 - INFO - __main__ -   Adapter Name = bn/wiki@ukp
11/28/2021 01:16:38 - INFO - __main__ -   Language bxr, split test does not exist
11/28/2021 01:16:42 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='es', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:16:42 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:16:42 - INFO - __main__ -   Seed = 1
11/28/2021 01:16:42 - INFO - root -   save model
11/28/2021 01:16:42 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='es', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:16:42 - INFO - __main__ -   Loading pretrained model and tokenizer
11/28/2021 01:16:45 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
11/28/2021 01:16:51 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:16:51 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:16:51 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
11/28/2021 01:16:51 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:16:51 - INFO - root -   loading task adapter
11/28/2021 01:16:51 - INFO - root -   loading lang adpater bn/wiki@ukp
11/28/2021 01:16:51 - INFO - __main__ -   Adapter Languages : ['bn'], Length : 1
11/28/2021 01:16:51 - INFO - __main__ -   Adapter Names ['bn/wiki@ukp'], Length : 1
11/28/2021 01:16:51 - INFO - __main__ -   Language = bn
11/28/2021 01:16:51 - INFO - __main__ -   Adapter Name = bn/wiki@ukp
11/28/2021 01:17:02 - INFO - __main__ -   Language adapter for es not found, using bn instead
11/28/2021 01:17:02 - INFO - __main__ -   Set active language adapter to bn
11/28/2021 01:17:02 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:17:02 - INFO - __main__ -   Adapter Languages = ['bn']
11/28/2021 01:17:02 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_es_bert-base-multilingual-cased_128
11/28/2021 01:17:03 - INFO - __main__ -   ***** Running evaluation  in es *****
11/28/2021 01:17:03 - INFO - __main__ -     Num examples = 3154
11/28/2021 01:17:03 - INFO - __main__ -     Batch size = 32
11/28/2021 01:17:03 - INFO - __main__ -   Batch number = 1
11/28/2021 01:17:03 - INFO - __main__ -   Batch number = 2
11/28/2021 01:17:04 - INFO - __main__ -   Batch number = 3
11/28/2021 01:17:04 - INFO - __main__ -   Batch number = 4
11/28/2021 01:17:05 - INFO - __main__ -   Batch number = 5
11/28/2021 01:17:05 - INFO - __main__ -   Batch number = 6
11/28/2021 01:17:06 - INFO - __main__ -   Batch number = 7
11/28/2021 01:17:06 - INFO - __main__ -   Batch number = 8
11/28/2021 01:17:07 - INFO - __main__ -   Batch number = 9
11/28/2021 01:17:07 - INFO - __main__ -   Batch number = 10
11/28/2021 01:17:07 - INFO - __main__ -   Batch number = 11
11/28/2021 01:17:08 - INFO - __main__ -   Batch number = 12
11/28/2021 01:17:08 - INFO - __main__ -   Batch number = 13
11/28/2021 01:17:09 - INFO - __main__ -   Batch number = 14
11/28/2021 01:17:09 - INFO - __main__ -   Batch number = 15
11/28/2021 01:17:10 - INFO - __main__ -   Batch number = 16
11/28/2021 01:17:10 - INFO - __main__ -   Batch number = 17
11/28/2021 01:17:11 - INFO - __main__ -   Batch number = 18
11/28/2021 01:17:12 - INFO - __main__ -   Batch number = 19
11/28/2021 01:17:12 - INFO - __main__ -   Batch number = 20
11/28/2021 01:17:12 - INFO - __main__ -   Batch number = 21
11/28/2021 01:17:13 - INFO - __main__ -   Batch number = 22
11/28/2021 01:17:13 - INFO - __main__ -   Batch number = 23
11/28/2021 01:17:14 - INFO - __main__ -   Batch number = 24
11/28/2021 01:17:14 - INFO - __main__ -   Batch number = 25
11/28/2021 01:17:15 - INFO - __main__ -   Batch number = 26
11/28/2021 01:17:15 - INFO - __main__ -   Batch number = 27
11/28/2021 01:17:16 - INFO - __main__ -   Batch number = 28
11/28/2021 01:17:17 - INFO - __main__ -   Batch number = 29
11/28/2021 01:17:17 - INFO - __main__ -   Batch number = 30
11/28/2021 01:17:18 - INFO - __main__ -   Batch number = 31
11/28/2021 01:17:18 - INFO - __main__ -   Batch number = 32
11/28/2021 01:17:19 - INFO - __main__ -   Batch number = 33
11/28/2021 01:17:19 - INFO - __main__ -   Batch number = 34
11/28/2021 01:17:20 - INFO - __main__ -   Batch number = 35
11/28/2021 01:17:20 - INFO - __main__ -   Batch number = 36
11/28/2021 01:17:21 - INFO - __main__ -   Batch number = 37
11/28/2021 01:17:21 - INFO - __main__ -   Batch number = 38
11/28/2021 01:17:21 - INFO - __main__ -   Batch number = 39
11/28/2021 01:17:22 - INFO - __main__ -   Batch number = 40
11/28/2021 01:17:22 - INFO - __main__ -   Batch number = 41
11/28/2021 01:17:23 - INFO - __main__ -   Batch number = 42
11/28/2021 01:17:23 - INFO - __main__ -   Batch number = 43
11/28/2021 01:17:24 - INFO - __main__ -   Batch number = 44
11/28/2021 01:17:24 - INFO - __main__ -   Batch number = 45
11/28/2021 01:17:25 - INFO - __main__ -   Batch number = 46
11/28/2021 01:17:25 - INFO - __main__ -   Batch number = 47
11/28/2021 01:17:26 - INFO - __main__ -   Batch number = 48
11/28/2021 01:17:26 - INFO - __main__ -   Batch number = 49
11/28/2021 01:17:27 - INFO - __main__ -   Batch number = 50
11/28/2021 01:17:27 - INFO - __main__ -   Batch number = 51
11/28/2021 01:17:28 - INFO - __main__ -   Batch number = 52
11/28/2021 01:17:28 - INFO - __main__ -   Batch number = 53
11/28/2021 01:17:29 - INFO - __main__ -   Batch number = 54
11/28/2021 01:17:29 - INFO - __main__ -   Batch number = 55
11/28/2021 01:17:30 - INFO - __main__ -   Batch number = 56
11/28/2021 01:17:30 - INFO - __main__ -   Batch number = 57
11/28/2021 01:17:31 - INFO - __main__ -   Batch number = 58
11/28/2021 01:17:31 - INFO - __main__ -   Batch number = 59
11/28/2021 01:17:32 - INFO - __main__ -   Batch number = 60
11/28/2021 01:17:32 - INFO - __main__ -   Batch number = 61
11/28/2021 01:17:33 - INFO - __main__ -   Batch number = 62
11/28/2021 01:17:33 - INFO - __main__ -   Batch number = 63
11/28/2021 01:17:34 - INFO - __main__ -   Batch number = 64
11/28/2021 01:17:34 - INFO - __main__ -   Batch number = 65
11/28/2021 01:17:35 - INFO - __main__ -   Batch number = 66
11/28/2021 01:17:35 - INFO - __main__ -   Batch number = 67
11/28/2021 01:17:36 - INFO - __main__ -   Batch number = 68
11/28/2021 01:17:36 - INFO - __main__ -   Batch number = 69
11/28/2021 01:17:37 - INFO - __main__ -   Batch number = 70
11/28/2021 01:17:37 - INFO - __main__ -   Batch number = 71
11/28/2021 01:17:38 - INFO - __main__ -   Batch number = 72
11/28/2021 01:17:38 - INFO - __main__ -   Batch number = 73
11/28/2021 01:17:39 - INFO - __main__ -   Batch number = 74
11/28/2021 01:17:39 - INFO - __main__ -   Batch number = 75
11/28/2021 01:17:40 - INFO - __main__ -   Batch number = 76
11/28/2021 01:17:40 - INFO - __main__ -   Batch number = 77
11/28/2021 01:17:40 - INFO - __main__ -   Batch number = 78
11/28/2021 01:17:41 - INFO - __main__ -   Batch number = 79
11/28/2021 01:17:42 - INFO - __main__ -   Batch number = 80
11/28/2021 01:17:42 - INFO - __main__ -   Batch number = 81
11/28/2021 01:17:42 - INFO - __main__ -   Batch number = 82
11/28/2021 01:17:43 - INFO - __main__ -   Batch number = 83
11/28/2021 01:17:43 - INFO - __main__ -   Batch number = 84
11/28/2021 01:17:44 - INFO - __main__ -   Batch number = 85
11/28/2021 01:17:44 - INFO - __main__ -   Batch number = 86
11/28/2021 01:17:45 - INFO - __main__ -   Batch number = 87
11/28/2021 01:17:45 - INFO - __main__ -   Batch number = 88
11/28/2021 01:17:45 - INFO - __main__ -   Batch number = 89
11/28/2021 01:17:46 - INFO - __main__ -   Batch number = 90
11/28/2021 01:17:46 - INFO - __main__ -   Batch number = 91
11/28/2021 01:17:46 - INFO - __main__ -   Batch number = 92
11/28/2021 01:17:47 - INFO - __main__ -   Batch number = 93
11/28/2021 01:17:47 - INFO - __main__ -   Batch number = 94
11/28/2021 01:17:47 - INFO - __main__ -   Batch number = 95
11/28/2021 01:17:48 - INFO - __main__ -   Batch number = 96
11/28/2021 01:17:48 - INFO - __main__ -   Batch number = 97
11/28/2021 01:17:48 - INFO - __main__ -   Batch number = 98
11/28/2021 01:17:48 - INFO - __main__ -   Batch number = 99
11/28/2021 01:17:51 - INFO - __main__ -   ***** Evaluation result  in es *****
11/28/2021 01:17:51 - INFO - __main__ -     f1 = 0.8735057160723935
11/28/2021 01:17:51 - INFO - __main__ -     loss = 0.4111606367308684
11/28/2021 01:17:51 - INFO - __main__ -     precision = 0.8809022335423198
11/28/2021 01:17:51 - INFO - __main__ -     recall = 0.8662323744385709
11/28/2021 01:17:54 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='es', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:17:54 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:17:54 - INFO - __main__ -   Seed = 2
11/28/2021 01:17:54 - INFO - root -   save model
11/28/2021 01:17:54 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='es', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:17:54 - INFO - __main__ -   Loading pretrained model and tokenizer
11/28/2021 01:17:56 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
11/28/2021 01:18:03 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:18:03 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:18:03 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
11/28/2021 01:18:03 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:18:03 - INFO - root -   loading task adapter
11/28/2021 01:18:03 - INFO - root -   loading lang adpater bn/wiki@ukp
11/28/2021 01:18:03 - INFO - __main__ -   Adapter Languages : ['bn'], Length : 1
11/28/2021 01:18:03 - INFO - __main__ -   Adapter Names ['bn/wiki@ukp'], Length : 1
11/28/2021 01:18:03 - INFO - __main__ -   Language = bn
11/28/2021 01:18:03 - INFO - __main__ -   Adapter Name = bn/wiki@ukp
11/28/2021 01:18:14 - INFO - __main__ -   Language adapter for es not found, using bn instead
11/28/2021 01:18:14 - INFO - __main__ -   Set active language adapter to bn
11/28/2021 01:18:14 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:18:14 - INFO - __main__ -   Adapter Languages = ['bn']
11/28/2021 01:18:14 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_es_bert-base-multilingual-cased_128
11/28/2021 01:18:15 - INFO - __main__ -   ***** Running evaluation  in es *****
11/28/2021 01:18:15 - INFO - __main__ -     Num examples = 3154
11/28/2021 01:18:15 - INFO - __main__ -     Batch size = 32
11/28/2021 01:18:15 - INFO - __main__ -   Batch number = 1
11/28/2021 01:18:15 - INFO - __main__ -   Batch number = 2
11/28/2021 01:18:15 - INFO - __main__ -   Batch number = 3
11/28/2021 01:18:16 - INFO - __main__ -   Batch number = 4
11/28/2021 01:18:16 - INFO - __main__ -   Batch number = 5
11/28/2021 01:18:16 - INFO - __main__ -   Batch number = 6
11/28/2021 01:18:16 - INFO - __main__ -   Batch number = 7
11/28/2021 01:18:17 - INFO - __main__ -   Batch number = 8
11/28/2021 01:18:17 - INFO - __main__ -   Batch number = 9
11/28/2021 01:18:17 - INFO - __main__ -   Batch number = 10
11/28/2021 01:18:18 - INFO - __main__ -   Batch number = 11
11/28/2021 01:18:18 - INFO - __main__ -   Batch number = 12
11/28/2021 01:18:18 - INFO - __main__ -   Batch number = 13
11/28/2021 01:18:19 - INFO - __main__ -   Batch number = 14
11/28/2021 01:18:19 - INFO - __main__ -   Batch number = 15
11/28/2021 01:18:19 - INFO - __main__ -   Batch number = 16
11/28/2021 01:18:20 - INFO - __main__ -   Batch number = 17
11/28/2021 01:18:20 - INFO - __main__ -   Batch number = 18
11/28/2021 01:18:20 - INFO - __main__ -   Batch number = 19
11/28/2021 01:18:20 - INFO - __main__ -   Batch number = 20
11/28/2021 01:18:20 - INFO - __main__ -   Batch number = 21
11/28/2021 01:18:20 - INFO - __main__ -   Batch number = 22
11/28/2021 01:18:21 - INFO - __main__ -   Batch number = 23
11/28/2021 01:18:21 - INFO - __main__ -   Batch number = 24
11/28/2021 01:18:21 - INFO - __main__ -   Batch number = 25
11/28/2021 01:18:21 - INFO - __main__ -   Batch number = 26
11/28/2021 01:18:21 - INFO - __main__ -   Batch number = 27
11/28/2021 01:18:22 - INFO - __main__ -   Batch number = 28
11/28/2021 01:18:22 - INFO - __main__ -   Batch number = 29
11/28/2021 01:18:22 - INFO - __main__ -   Batch number = 30
11/28/2021 01:18:22 - INFO - __main__ -   Batch number = 31
11/28/2021 01:18:22 - INFO - __main__ -   Batch number = 32
11/28/2021 01:18:22 - INFO - __main__ -   Batch number = 33
11/28/2021 01:18:23 - INFO - __main__ -   Batch number = 34
11/28/2021 01:18:23 - INFO - __main__ -   Batch number = 35
11/28/2021 01:18:23 - INFO - __main__ -   Batch number = 36
11/28/2021 01:18:23 - INFO - __main__ -   Batch number = 37
11/28/2021 01:18:23 - INFO - __main__ -   Batch number = 38
11/28/2021 01:18:23 - INFO - __main__ -   Batch number = 39
11/28/2021 01:18:24 - INFO - __main__ -   Batch number = 40
11/28/2021 01:18:24 - INFO - __main__ -   Batch number = 41
11/28/2021 01:18:24 - INFO - __main__ -   Batch number = 42
11/28/2021 01:18:24 - INFO - __main__ -   Batch number = 43
11/28/2021 01:18:25 - INFO - __main__ -   Batch number = 44
11/28/2021 01:18:25 - INFO - __main__ -   Batch number = 45
11/28/2021 01:18:25 - INFO - __main__ -   Batch number = 46
11/28/2021 01:18:26 - INFO - __main__ -   Batch number = 47
11/28/2021 01:18:26 - INFO - __main__ -   Batch number = 48
11/28/2021 01:18:27 - INFO - __main__ -   Batch number = 49
11/28/2021 01:18:27 - INFO - __main__ -   Batch number = 50
11/28/2021 01:18:27 - INFO - __main__ -   Batch number = 51
11/28/2021 01:18:28 - INFO - __main__ -   Batch number = 52
11/28/2021 01:18:28 - INFO - __main__ -   Batch number = 53
11/28/2021 01:18:28 - INFO - __main__ -   Batch number = 54
11/28/2021 01:18:29 - INFO - __main__ -   Batch number = 55
11/28/2021 01:18:29 - INFO - __main__ -   Batch number = 56
11/28/2021 01:18:29 - INFO - __main__ -   Batch number = 57
11/28/2021 01:18:30 - INFO - __main__ -   Batch number = 58
11/28/2021 01:18:30 - INFO - __main__ -   Batch number = 59
11/28/2021 01:18:30 - INFO - __main__ -   Batch number = 60
11/28/2021 01:18:31 - INFO - __main__ -   Batch number = 61
11/28/2021 01:18:31 - INFO - __main__ -   Batch number = 62
11/28/2021 01:18:32 - INFO - __main__ -   Batch number = 63
11/28/2021 01:18:32 - INFO - __main__ -   Batch number = 64
11/28/2021 01:18:32 - INFO - __main__ -   Batch number = 65
11/28/2021 01:18:33 - INFO - __main__ -   Batch number = 66
11/28/2021 01:18:33 - INFO - __main__ -   Batch number = 67
11/28/2021 01:18:33 - INFO - __main__ -   Batch number = 68
11/28/2021 01:18:34 - INFO - __main__ -   Batch number = 69
11/28/2021 01:18:34 - INFO - __main__ -   Batch number = 70
11/28/2021 01:18:34 - INFO - __main__ -   Batch number = 71
11/28/2021 01:18:35 - INFO - __main__ -   Batch number = 72
11/28/2021 01:18:35 - INFO - __main__ -   Batch number = 73
11/28/2021 01:18:35 - INFO - __main__ -   Batch number = 74
11/28/2021 01:18:36 - INFO - __main__ -   Batch number = 75
11/28/2021 01:18:36 - INFO - __main__ -   Batch number = 76
11/28/2021 01:18:36 - INFO - __main__ -   Batch number = 77
11/28/2021 01:18:37 - INFO - __main__ -   Batch number = 78
11/28/2021 01:18:37 - INFO - __main__ -   Batch number = 79
11/28/2021 01:18:37 - INFO - __main__ -   Batch number = 80
11/28/2021 01:18:38 - INFO - __main__ -   Batch number = 81
11/28/2021 01:18:38 - INFO - __main__ -   Batch number = 82
11/28/2021 01:18:38 - INFO - __main__ -   Batch number = 83
11/28/2021 01:18:39 - INFO - __main__ -   Batch number = 84
11/28/2021 01:18:39 - INFO - __main__ -   Batch number = 85
11/28/2021 01:18:39 - INFO - __main__ -   Batch number = 86
11/28/2021 01:18:40 - INFO - __main__ -   Batch number = 87
11/28/2021 01:18:40 - INFO - __main__ -   Batch number = 88
11/28/2021 01:18:40 - INFO - __main__ -   Batch number = 89
11/28/2021 01:18:41 - INFO - __main__ -   Batch number = 90
11/28/2021 01:18:41 - INFO - __main__ -   Batch number = 91
11/28/2021 01:18:42 - INFO - __main__ -   Batch number = 92
11/28/2021 01:18:42 - INFO - __main__ -   Batch number = 93
11/28/2021 01:18:42 - INFO - __main__ -   Batch number = 94
11/28/2021 01:18:43 - INFO - __main__ -   Batch number = 95
11/28/2021 01:18:43 - INFO - __main__ -   Batch number = 96
11/28/2021 01:18:43 - INFO - __main__ -   Batch number = 97
11/28/2021 01:18:44 - INFO - __main__ -   Batch number = 98
11/28/2021 01:18:44 - INFO - __main__ -   Batch number = 99
11/28/2021 01:18:47 - INFO - __main__ -   ***** Evaluation result  in es *****
11/28/2021 01:18:47 - INFO - __main__ -     f1 = 0.860760417426841
11/28/2021 01:18:47 - INFO - __main__ -     loss = 0.4603135889828807
11/28/2021 01:18:47 - INFO - __main__ -     precision = 0.8695278231702073
11/28/2021 01:18:47 - INFO - __main__ -     recall = 0.8521680494177996
11/28/2021 01:18:49 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='es', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:18:49 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:18:49 - INFO - __main__ -   Seed = 3
11/28/2021 01:18:49 - INFO - root -   save model
11/28/2021 01:18:49 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='es', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:18:49 - INFO - __main__ -   Loading pretrained model and tokenizer
11/28/2021 01:18:52 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
11/28/2021 01:18:59 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:18:59 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:18:59 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
11/28/2021 01:18:59 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:18:59 - INFO - root -   loading task adapter
11/28/2021 01:18:59 - INFO - root -   loading lang adpater bn/wiki@ukp
11/28/2021 01:18:59 - INFO - __main__ -   Adapter Languages : ['bn'], Length : 1
11/28/2021 01:18:59 - INFO - __main__ -   Adapter Names ['bn/wiki@ukp'], Length : 1
11/28/2021 01:18:59 - INFO - __main__ -   Language = bn
11/28/2021 01:18:59 - INFO - __main__ -   Adapter Name = bn/wiki@ukp
11/28/2021 01:19:10 - INFO - __main__ -   Language adapter for es not found, using bn instead
11/28/2021 01:19:10 - INFO - __main__ -   Set active language adapter to bn
11/28/2021 01:19:10 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:19:10 - INFO - __main__ -   Adapter Languages = ['bn']
11/28/2021 01:19:10 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_es_bert-base-multilingual-cased_128
11/28/2021 01:19:10 - INFO - __main__ -   ***** Running evaluation  in es *****
11/28/2021 01:19:10 - INFO - __main__ -     Num examples = 3154
11/28/2021 01:19:10 - INFO - __main__ -     Batch size = 32
11/28/2021 01:19:10 - INFO - __main__ -   Batch number = 1
11/28/2021 01:19:11 - INFO - __main__ -   Batch number = 2
11/28/2021 01:19:11 - INFO - __main__ -   Batch number = 3
11/28/2021 01:19:12 - INFO - __main__ -   Batch number = 4
11/28/2021 01:19:12 - INFO - __main__ -   Batch number = 5
11/28/2021 01:19:13 - INFO - __main__ -   Batch number = 6
11/28/2021 01:19:13 - INFO - __main__ -   Batch number = 7
11/28/2021 01:19:13 - INFO - __main__ -   Batch number = 8
11/28/2021 01:19:14 - INFO - __main__ -   Batch number = 9
11/28/2021 01:19:14 - INFO - __main__ -   Batch number = 10
11/28/2021 01:19:14 - INFO - __main__ -   Batch number = 11
11/28/2021 01:19:15 - INFO - __main__ -   Batch number = 12
11/28/2021 01:19:15 - INFO - __main__ -   Batch number = 13
11/28/2021 01:19:15 - INFO - __main__ -   Batch number = 14
11/28/2021 01:19:16 - INFO - __main__ -   Batch number = 15
11/28/2021 01:19:16 - INFO - __main__ -   Batch number = 16
11/28/2021 01:19:16 - INFO - __main__ -   Batch number = 17
11/28/2021 01:19:17 - INFO - __main__ -   Batch number = 18
11/28/2021 01:19:17 - INFO - __main__ -   Batch number = 19
11/28/2021 01:19:17 - INFO - __main__ -   Batch number = 20
11/28/2021 01:19:17 - INFO - __main__ -   Batch number = 21
11/28/2021 01:19:18 - INFO - __main__ -   Batch number = 22
11/28/2021 01:19:18 - INFO - __main__ -   Batch number = 23
11/28/2021 01:19:18 - INFO - __main__ -   Batch number = 24
11/28/2021 01:19:19 - INFO - __main__ -   Batch number = 25
11/28/2021 01:19:19 - INFO - __main__ -   Batch number = 26
11/28/2021 01:19:19 - INFO - __main__ -   Batch number = 27
11/28/2021 01:19:20 - INFO - __main__ -   Batch number = 28
11/28/2021 01:19:20 - INFO - __main__ -   Batch number = 29
11/28/2021 01:19:20 - INFO - __main__ -   Batch number = 30
11/28/2021 01:19:21 - INFO - __main__ -   Batch number = 31
11/28/2021 01:19:21 - INFO - __main__ -   Batch number = 32
11/28/2021 01:19:21 - INFO - __main__ -   Batch number = 33
11/28/2021 01:19:22 - INFO - __main__ -   Batch number = 34
11/28/2021 01:19:22 - INFO - __main__ -   Batch number = 35
11/28/2021 01:19:22 - INFO - __main__ -   Batch number = 36
11/28/2021 01:19:23 - INFO - __main__ -   Batch number = 37
11/28/2021 01:19:23 - INFO - __main__ -   Batch number = 38
11/28/2021 01:19:23 - INFO - __main__ -   Batch number = 39
11/28/2021 01:19:24 - INFO - __main__ -   Batch number = 40
11/28/2021 01:19:24 - INFO - __main__ -   Batch number = 41
11/28/2021 01:19:24 - INFO - __main__ -   Batch number = 42
11/28/2021 01:19:25 - INFO - __main__ -   Batch number = 43
11/28/2021 01:19:25 - INFO - __main__ -   Batch number = 44
11/28/2021 01:19:25 - INFO - __main__ -   Batch number = 45
11/28/2021 01:19:26 - INFO - __main__ -   Batch number = 46
11/28/2021 01:19:26 - INFO - __main__ -   Batch number = 47
11/28/2021 01:19:26 - INFO - __main__ -   Batch number = 48
11/28/2021 01:19:27 - INFO - __main__ -   Batch number = 49
11/28/2021 01:19:27 - INFO - __main__ -   Batch number = 50
11/28/2021 01:19:27 - INFO - __main__ -   Batch number = 51
11/28/2021 01:19:27 - INFO - __main__ -   Batch number = 52
11/28/2021 01:19:28 - INFO - __main__ -   Batch number = 53
11/28/2021 01:19:28 - INFO - __main__ -   Batch number = 54
11/28/2021 01:19:28 - INFO - __main__ -   Batch number = 55
11/28/2021 01:19:29 - INFO - __main__ -   Batch number = 56
11/28/2021 01:19:29 - INFO - __main__ -   Batch number = 57
11/28/2021 01:19:29 - INFO - __main__ -   Batch number = 58
11/28/2021 01:19:30 - INFO - __main__ -   Batch number = 59
11/28/2021 01:19:30 - INFO - __main__ -   Batch number = 60
11/28/2021 01:19:30 - INFO - __main__ -   Batch number = 61
11/28/2021 01:19:31 - INFO - __main__ -   Batch number = 62
11/28/2021 01:19:31 - INFO - __main__ -   Batch number = 63
11/28/2021 01:19:31 - INFO - __main__ -   Batch number = 64
11/28/2021 01:19:32 - INFO - __main__ -   Batch number = 65
11/28/2021 01:19:32 - INFO - __main__ -   Batch number = 66
11/28/2021 01:19:32 - INFO - __main__ -   Batch number = 67
11/28/2021 01:19:33 - INFO - __main__ -   Batch number = 68
11/28/2021 01:19:33 - INFO - __main__ -   Batch number = 69
11/28/2021 01:19:33 - INFO - __main__ -   Batch number = 70
11/28/2021 01:19:34 - INFO - __main__ -   Batch number = 71
11/28/2021 01:19:34 - INFO - __main__ -   Batch number = 72
11/28/2021 01:19:34 - INFO - __main__ -   Batch number = 73
11/28/2021 01:19:35 - INFO - __main__ -   Batch number = 74
11/28/2021 01:19:35 - INFO - __main__ -   Batch number = 75
11/28/2021 01:19:35 - INFO - __main__ -   Batch number = 76
11/28/2021 01:19:36 - INFO - __main__ -   Batch number = 77
11/28/2021 01:19:36 - INFO - __main__ -   Batch number = 78
11/28/2021 01:19:36 - INFO - __main__ -   Batch number = 79
11/28/2021 01:19:37 - INFO - __main__ -   Batch number = 80
11/28/2021 01:19:37 - INFO - __main__ -   Batch number = 81
11/28/2021 01:19:38 - INFO - __main__ -   Batch number = 82
11/28/2021 01:19:38 - INFO - __main__ -   Batch number = 83
11/28/2021 01:19:39 - INFO - __main__ -   Batch number = 84
11/28/2021 01:19:39 - INFO - __main__ -   Batch number = 85
11/28/2021 01:19:40 - INFO - __main__ -   Batch number = 86
11/28/2021 01:19:40 - INFO - __main__ -   Batch number = 87
11/28/2021 01:19:41 - INFO - __main__ -   Batch number = 88
11/28/2021 01:19:41 - INFO - __main__ -   Batch number = 89
11/28/2021 01:19:42 - INFO - __main__ -   Batch number = 90
11/28/2021 01:19:42 - INFO - __main__ -   Batch number = 91
11/28/2021 01:19:43 - INFO - __main__ -   Batch number = 92
11/28/2021 01:19:43 - INFO - __main__ -   Batch number = 93
11/28/2021 01:19:44 - INFO - __main__ -   Batch number = 94
11/28/2021 01:19:44 - INFO - __main__ -   Batch number = 95
11/28/2021 01:19:45 - INFO - __main__ -   Batch number = 96
11/28/2021 01:19:45 - INFO - __main__ -   Batch number = 97
11/28/2021 01:19:45 - INFO - __main__ -   Batch number = 98
11/28/2021 01:19:46 - INFO - __main__ -   Batch number = 99
11/28/2021 01:19:48 - INFO - __main__ -   ***** Evaluation result  in es *****
11/28/2021 01:19:48 - INFO - __main__ -     f1 = 0.874314617248268
11/28/2021 01:19:48 - INFO - __main__ -     loss = 0.4036746145498873
11/28/2021 01:19:48 - INFO - __main__ -     precision = 0.8818377795742387
11/28/2021 01:19:48 - INFO - __main__ -     recall = 0.8669187327657832
11/28/2021 01:34:32 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='pt', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:34:32 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:34:32 - INFO - __main__ -   Seed = 1
11/28/2021 01:34:32 - INFO - root -   save model
11/28/2021 01:34:32 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='pt', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:34:32 - INFO - __main__ -   Loading pretrained model and tokenizer
11/28/2021 01:34:35 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
11/28/2021 01:34:41 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='is', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:34:41 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:34:41 - INFO - __main__ -   Seed = 1
11/28/2021 01:34:41 - INFO - root -   save model
11/28/2021 01:34:41 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='is', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:34:41 - INFO - __main__ -   Loading pretrained model and tokenizer
11/28/2021 01:34:41 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:34:41 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:34:41 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
11/28/2021 01:34:41 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:34:41 - INFO - root -   loading task adapter
11/28/2021 01:34:41 - INFO - root -   loading lang adpater bn/wiki@ukp
11/28/2021 01:34:41 - INFO - __main__ -   Adapter Languages : ['bn'], Length : 1
11/28/2021 01:34:41 - INFO - __main__ -   Adapter Names ['bn/wiki@ukp'], Length : 1
11/28/2021 01:34:41 - INFO - __main__ -   Language = bn
11/28/2021 01:34:41 - INFO - __main__ -   Adapter Name = bn/wiki@ukp
11/28/2021 01:34:43 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
11/28/2021 01:34:49 - INFO - __main__ -   Language adapter for pt not found, using bn instead
11/28/2021 01:34:49 - INFO - __main__ -   Set active language adapter to bn
11/28/2021 01:34:49 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:34:49 - INFO - __main__ -   Adapter Languages = ['bn']
11/28/2021 01:34:49 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_pt_bert-base-multilingual-cased_128
11/28/2021 01:34:50 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:34:50 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:34:50 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
11/28/2021 01:34:50 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:34:50 - INFO - root -   loading task adapter
11/28/2021 01:34:50 - INFO - root -   loading lang adpater bn/wiki@ukp
11/28/2021 01:34:50 - INFO - __main__ -   Adapter Languages : ['bn'], Length : 1
11/28/2021 01:34:50 - INFO - __main__ -   Adapter Names ['bn/wiki@ukp'], Length : 1
11/28/2021 01:34:50 - INFO - __main__ -   Language = bn
11/28/2021 01:34:50 - INFO - __main__ -   Adapter Name = bn/wiki@ukp
11/28/2021 01:34:50 - INFO - __main__ -   ***** Running evaluation  in pt *****
11/28/2021 01:34:50 - INFO - __main__ -     Num examples = 2682
11/28/2021 01:34:50 - INFO - __main__ -     Batch size = 32
11/28/2021 01:34:50 - INFO - __main__ -   Batch number = 1
11/28/2021 01:34:50 - INFO - __main__ -   Batch number = 2
11/28/2021 01:34:51 - INFO - __main__ -   Batch number = 3
11/28/2021 01:34:51 - INFO - __main__ -   Batch number = 4
11/28/2021 01:34:52 - INFO - __main__ -   Batch number = 5
11/28/2021 01:34:52 - INFO - __main__ -   Batch number = 6
11/28/2021 01:34:53 - INFO - __main__ -   Batch number = 7
11/28/2021 01:34:53 - INFO - __main__ -   Batch number = 8
11/28/2021 01:34:54 - INFO - __main__ -   Batch number = 9
11/28/2021 01:34:54 - INFO - __main__ -   Batch number = 10
11/28/2021 01:34:55 - INFO - __main__ -   Batch number = 11
11/28/2021 01:34:55 - INFO - __main__ -   Batch number = 12
11/28/2021 01:34:56 - INFO - __main__ -   Batch number = 13
11/28/2021 01:34:56 - INFO - __main__ -   Batch number = 14
11/28/2021 01:34:57 - INFO - __main__ -   Batch number = 15
11/28/2021 01:34:57 - INFO - __main__ -   Batch number = 16
11/28/2021 01:34:58 - INFO - __main__ -   Batch number = 17
11/28/2021 01:34:58 - INFO - __main__ -   Batch number = 18
11/28/2021 01:34:59 - INFO - __main__ -   Batch number = 19
11/28/2021 01:34:59 - INFO - __main__ -   Batch number = 20
11/28/2021 01:35:00 - INFO - __main__ -   Batch number = 21
11/28/2021 01:35:00 - INFO - __main__ -   Batch number = 22
11/28/2021 01:35:01 - INFO - __main__ -   Batch number = 23
11/28/2021 01:35:01 - INFO - __main__ -   Batch number = 24
11/28/2021 01:35:02 - INFO - __main__ -   Batch number = 25
11/28/2021 01:35:02 - INFO - __main__ -   Batch number = 26
11/28/2021 01:35:02 - INFO - __main__ -   Language adapter for is not found, using bn instead
11/28/2021 01:35:02 - INFO - __main__ -   Set active language adapter to bn
11/28/2021 01:35:02 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:35:02 - INFO - __main__ -   Adapter Languages = ['bn']
11/28/2021 01:35:02 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_is_bert-base-multilingual-cased_128
11/28/2021 01:35:03 - INFO - __main__ -   Batch number = 27
11/28/2021 01:35:03 - INFO - __main__ -   Batch number = 28
11/28/2021 01:35:04 - INFO - __main__ -   ***** Running evaluation  in is *****
11/28/2021 01:35:04 - INFO - __main__ -     Num examples = 6401
11/28/2021 01:35:04 - INFO - __main__ -     Batch size = 32
11/28/2021 01:35:04 - INFO - __main__ -   Batch number = 1
11/28/2021 01:35:04 - INFO - __main__ -   Batch number = 29
11/28/2021 01:35:04 - INFO - __main__ -   Batch number = 2
11/28/2021 01:35:04 - INFO - __main__ -   Batch number = 30
11/28/2021 01:35:05 - INFO - __main__ -   Batch number = 3
11/28/2021 01:35:05 - INFO - __main__ -   Batch number = 31
11/28/2021 01:35:05 - INFO - __main__ -   Batch number = 4
11/28/2021 01:35:06 - INFO - __main__ -   Batch number = 32
11/28/2021 01:35:06 - INFO - __main__ -   Batch number = 5
11/28/2021 01:35:06 - INFO - __main__ -   Batch number = 33
11/28/2021 01:35:07 - INFO - __main__ -   Batch number = 6
11/28/2021 01:35:07 - INFO - __main__ -   Batch number = 34
11/28/2021 01:35:07 - INFO - __main__ -   Batch number = 7
11/28/2021 01:35:08 - INFO - __main__ -   Batch number = 35
11/28/2021 01:35:08 - INFO - __main__ -   Batch number = 8
11/28/2021 01:35:08 - INFO - __main__ -   Batch number = 36
11/28/2021 01:35:09 - INFO - __main__ -   Batch number = 9
11/28/2021 01:35:09 - INFO - __main__ -   Batch number = 37
11/28/2021 01:35:09 - INFO - __main__ -   Batch number = 10
11/28/2021 01:35:10 - INFO - __main__ -   Batch number = 11
11/28/2021 01:35:10 - INFO - __main__ -   Batch number = 38
11/28/2021 01:35:10 - INFO - __main__ -   Batch number = 12
11/28/2021 01:35:10 - INFO - __main__ -   Batch number = 39
11/28/2021 01:35:11 - INFO - __main__ -   Batch number = 13
11/28/2021 01:35:11 - INFO - __main__ -   Batch number = 40
11/28/2021 01:35:11 - INFO - __main__ -   Batch number = 14
11/28/2021 01:35:11 - INFO - __main__ -   Batch number = 41
11/28/2021 01:35:12 - INFO - __main__ -   Batch number = 15
11/28/2021 01:35:12 - INFO - __main__ -   Batch number = 42
11/28/2021 01:35:12 - INFO - __main__ -   Batch number = 16
11/28/2021 01:35:12 - INFO - __main__ -   Batch number = 43
11/28/2021 01:35:12 - INFO - __main__ -   Batch number = 17
11/28/2021 01:35:13 - INFO - __main__ -   Batch number = 44
11/28/2021 01:35:13 - INFO - __main__ -   Batch number = 18
11/28/2021 01:35:13 - INFO - __main__ -   Batch number = 45
11/28/2021 01:35:13 - INFO - __main__ -   Batch number = 19
11/28/2021 01:35:14 - INFO - __main__ -   Batch number = 46
11/28/2021 01:35:14 - INFO - __main__ -   Batch number = 20
11/28/2021 01:35:14 - INFO - __main__ -   Batch number = 47
11/28/2021 01:35:14 - INFO - __main__ -   Batch number = 21
11/28/2021 01:35:15 - INFO - __main__ -   Batch number = 48
11/28/2021 01:35:15 - INFO - __main__ -   Batch number = 22
11/28/2021 01:35:15 - INFO - __main__ -   Batch number = 49
11/28/2021 01:35:16 - INFO - __main__ -   Batch number = 23
11/28/2021 01:35:16 - INFO - __main__ -   Batch number = 50
11/28/2021 01:35:16 - INFO - __main__ -   Batch number = 24
11/28/2021 01:35:16 - INFO - __main__ -   Batch number = 51
11/28/2021 01:35:17 - INFO - __main__ -   Batch number = 25
11/28/2021 01:35:17 - INFO - __main__ -   Batch number = 52
11/28/2021 01:35:17 - INFO - __main__ -   Batch number = 26
11/28/2021 01:35:17 - INFO - __main__ -   Batch number = 53
11/28/2021 01:35:18 - INFO - __main__ -   Batch number = 27
11/28/2021 01:35:18 - INFO - __main__ -   Batch number = 54
11/28/2021 01:35:18 - INFO - __main__ -   Batch number = 28
11/28/2021 01:35:18 - INFO - __main__ -   Batch number = 55
11/28/2021 01:35:19 - INFO - __main__ -   Batch number = 29
11/28/2021 01:35:19 - INFO - __main__ -   Batch number = 56
11/28/2021 01:35:19 - INFO - __main__ -   Batch number = 30
11/28/2021 01:35:19 - INFO - __main__ -   Batch number = 57
11/28/2021 01:35:20 - INFO - __main__ -   Batch number = 31
11/28/2021 01:35:20 - INFO - __main__ -   Batch number = 58
11/28/2021 01:35:20 - INFO - __main__ -   Batch number = 32
11/28/2021 01:35:20 - INFO - __main__ -   Batch number = 59
11/28/2021 01:35:21 - INFO - __main__ -   Batch number = 33
11/28/2021 01:35:21 - INFO - __main__ -   Batch number = 60
11/28/2021 01:35:21 - INFO - __main__ -   Batch number = 34
11/28/2021 01:35:21 - INFO - __main__ -   Batch number = 61
11/28/2021 01:35:22 - INFO - __main__ -   Batch number = 35
11/28/2021 01:35:22 - INFO - __main__ -   Batch number = 62
11/28/2021 01:35:22 - INFO - __main__ -   Batch number = 36
11/28/2021 01:35:22 - INFO - __main__ -   Batch number = 63
11/28/2021 01:35:23 - INFO - __main__ -   Batch number = 37
11/28/2021 01:35:23 - INFO - __main__ -   Batch number = 64
11/28/2021 01:35:23 - INFO - __main__ -   Batch number = 38
11/28/2021 01:35:23 - INFO - __main__ -   Batch number = 65
11/28/2021 01:35:24 - INFO - __main__ -   Batch number = 39
11/28/2021 01:35:24 - INFO - __main__ -   Batch number = 66
11/28/2021 01:35:24 - INFO - __main__ -   Batch number = 40
11/28/2021 01:35:24 - INFO - __main__ -   Batch number = 67
11/28/2021 01:35:25 - INFO - __main__ -   Batch number = 41
11/28/2021 01:35:25 - INFO - __main__ -   Batch number = 68
11/28/2021 01:35:25 - INFO - __main__ -   Batch number = 42
11/28/2021 01:35:25 - INFO - __main__ -   Batch number = 69
11/28/2021 01:35:26 - INFO - __main__ -   Batch number = 43
11/28/2021 01:35:26 - INFO - __main__ -   Batch number = 70
11/28/2021 01:35:26 - INFO - __main__ -   Batch number = 44
11/28/2021 01:35:26 - INFO - __main__ -   Batch number = 71
11/28/2021 01:35:27 - INFO - __main__ -   Batch number = 45
11/28/2021 01:35:27 - INFO - __main__ -   Batch number = 72
11/28/2021 01:35:27 - INFO - __main__ -   Batch number = 46
11/28/2021 01:35:27 - INFO - __main__ -   Batch number = 73
11/28/2021 01:35:28 - INFO - __main__ -   Batch number = 47
11/28/2021 01:35:28 - INFO - __main__ -   Batch number = 74
11/28/2021 01:35:28 - INFO - __main__ -   Batch number = 48
11/28/2021 01:35:28 - INFO - __main__ -   Batch number = 75
11/28/2021 01:35:29 - INFO - __main__ -   Batch number = 49
11/28/2021 01:35:29 - INFO - __main__ -   Batch number = 76
11/28/2021 01:35:29 - INFO - __main__ -   Batch number = 50
11/28/2021 01:35:29 - INFO - __main__ -   Batch number = 77
11/28/2021 01:35:30 - INFO - __main__ -   Batch number = 51
11/28/2021 01:35:30 - INFO - __main__ -   Batch number = 78
11/28/2021 01:35:30 - INFO - __main__ -   Batch number = 52
11/28/2021 01:35:30 - INFO - __main__ -   Batch number = 79
11/28/2021 01:35:31 - INFO - __main__ -   Batch number = 53
11/28/2021 01:35:31 - INFO - __main__ -   Batch number = 80
11/28/2021 01:35:31 - INFO - __main__ -   Batch number = 54
11/28/2021 01:35:31 - INFO - __main__ -   Batch number = 81
11/28/2021 01:35:32 - INFO - __main__ -   Batch number = 55
11/28/2021 01:35:32 - INFO - __main__ -   Batch number = 82
11/28/2021 01:35:32 - INFO - __main__ -   Batch number = 56
11/28/2021 01:35:32 - INFO - __main__ -   Batch number = 83
11/28/2021 01:35:33 - INFO - __main__ -   Batch number = 57
11/28/2021 01:35:33 - INFO - __main__ -   Batch number = 84
11/28/2021 01:35:33 - INFO - __main__ -   Batch number = 58
11/28/2021 01:35:34 - INFO - __main__ -   Batch number = 59
11/28/2021 01:35:34 - INFO - __main__ -   Batch number = 60
11/28/2021 01:35:34 - INFO - __main__ -   Batch number = 61
11/28/2021 01:35:35 - INFO - __main__ -   Batch number = 62
11/28/2021 01:35:35 - INFO - __main__ -   ***** Evaluation result  in pt *****
11/28/2021 01:35:35 - INFO - __main__ -     f1 = 0.87944805025318
11/28/2021 01:35:35 - INFO - __main__ -     loss = 0.4262663768160911
11/28/2021 01:35:35 - INFO - __main__ -     precision = 0.8856431913364992
11/28/2021 01:35:35 - INFO - __main__ -     recall = 0.8733389780719186
11/28/2021 01:35:35 - INFO - __main__ -   Batch number = 63
11/28/2021 01:35:35 - INFO - __main__ -   Batch number = 64
11/28/2021 01:35:36 - INFO - __main__ -   Batch number = 65
11/28/2021 01:35:36 - INFO - __main__ -   Batch number = 66
11/28/2021 01:35:37 - INFO - __main__ -   Batch number = 67
11/28/2021 01:35:37 - INFO - __main__ -   Batch number = 68
11/28/2021 01:35:38 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='pt', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:35:38 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:35:38 - INFO - __main__ -   Seed = 2
11/28/2021 01:35:38 - INFO - root -   save model
11/28/2021 01:35:38 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='pt', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:35:38 - INFO - __main__ -   Loading pretrained model and tokenizer
11/28/2021 01:35:38 - INFO - __main__ -   Batch number = 69
11/28/2021 01:35:38 - INFO - __main__ -   Batch number = 70
11/28/2021 01:35:39 - INFO - __main__ -   Batch number = 71
11/28/2021 01:35:39 - INFO - __main__ -   Batch number = 72
11/28/2021 01:35:40 - INFO - __main__ -   Batch number = 73
11/28/2021 01:35:40 - INFO - __main__ -   Batch number = 74
11/28/2021 01:35:40 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
11/28/2021 01:35:41 - INFO - __main__ -   Batch number = 75
11/28/2021 01:35:41 - INFO - __main__ -   Batch number = 76
11/28/2021 01:35:42 - INFO - __main__ -   Batch number = 77
11/28/2021 01:35:42 - INFO - __main__ -   Batch number = 78
11/28/2021 01:35:43 - INFO - __main__ -   Batch number = 79
11/28/2021 01:35:43 - INFO - __main__ -   Batch number = 80
11/28/2021 01:35:44 - INFO - __main__ -   Batch number = 81
11/28/2021 01:35:44 - INFO - __main__ -   Batch number = 82
11/28/2021 01:35:45 - INFO - __main__ -   Batch number = 83
11/28/2021 01:35:45 - INFO - __main__ -   Batch number = 84
11/28/2021 01:35:46 - INFO - __main__ -   Batch number = 85
11/28/2021 01:35:46 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:35:46 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:35:46 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
11/28/2021 01:35:46 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:35:46 - INFO - root -   loading task adapter
11/28/2021 01:35:46 - INFO - __main__ -   Batch number = 86
11/28/2021 01:35:46 - INFO - root -   loading lang adpater bn/wiki@ukp
11/28/2021 01:35:46 - INFO - __main__ -   Adapter Languages : ['bn'], Length : 1
11/28/2021 01:35:46 - INFO - __main__ -   Adapter Names ['bn/wiki@ukp'], Length : 1
11/28/2021 01:35:46 - INFO - __main__ -   Language = bn
11/28/2021 01:35:46 - INFO - __main__ -   Adapter Name = bn/wiki@ukp
11/28/2021 01:35:47 - INFO - __main__ -   Batch number = 87
11/28/2021 01:35:47 - INFO - __main__ -   Batch number = 88
11/28/2021 01:35:48 - INFO - __main__ -   Batch number = 89
11/28/2021 01:35:48 - INFO - __main__ -   Batch number = 90
11/28/2021 01:35:49 - INFO - __main__ -   Batch number = 91
11/28/2021 01:35:49 - INFO - __main__ -   Batch number = 92
11/28/2021 01:35:50 - INFO - __main__ -   Batch number = 93
11/28/2021 01:35:50 - INFO - __main__ -   Batch number = 94
11/28/2021 01:35:51 - INFO - __main__ -   Batch number = 95
11/28/2021 01:35:51 - INFO - __main__ -   Batch number = 96
11/28/2021 01:35:52 - INFO - __main__ -   Batch number = 97
11/28/2021 01:35:52 - INFO - __main__ -   Batch number = 98
11/28/2021 01:35:53 - INFO - __main__ -   Batch number = 99
11/28/2021 01:35:53 - INFO - __main__ -   Batch number = 100
11/28/2021 01:35:54 - INFO - __main__ -   Batch number = 101
11/28/2021 01:35:54 - INFO - __main__ -   Batch number = 102
11/28/2021 01:35:55 - INFO - __main__ -   Batch number = 103
11/28/2021 01:35:55 - INFO - __main__ -   Batch number = 104
11/28/2021 01:35:56 - INFO - __main__ -   Batch number = 105
11/28/2021 01:35:56 - INFO - __main__ -   Batch number = 106
11/28/2021 01:35:57 - INFO - __main__ -   Batch number = 107
11/28/2021 01:35:57 - INFO - __main__ -   Batch number = 108
11/28/2021 01:35:58 - INFO - __main__ -   Batch number = 109
11/28/2021 01:35:58 - INFO - __main__ -   Batch number = 110
11/28/2021 01:35:59 - INFO - __main__ -   Batch number = 111
11/28/2021 01:35:59 - INFO - __main__ -   Language adapter for pt not found, using bn instead
11/28/2021 01:35:59 - INFO - __main__ -   Set active language adapter to bn
11/28/2021 01:35:59 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:35:59 - INFO - __main__ -   Adapter Languages = ['bn']
11/28/2021 01:35:59 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_pt_bert-base-multilingual-cased_128
11/28/2021 01:35:59 - INFO - __main__ -   Batch number = 112
11/28/2021 01:35:59 - INFO - __main__ -   Batch number = 113
11/28/2021 01:35:59 - INFO - __main__ -   ***** Running evaluation  in pt *****
11/28/2021 01:35:59 - INFO - __main__ -     Num examples = 2682
11/28/2021 01:35:59 - INFO - __main__ -     Batch size = 32
11/28/2021 01:35:59 - INFO - __main__ -   Batch number = 1
11/28/2021 01:36:00 - INFO - __main__ -   Batch number = 114
11/28/2021 01:36:00 - INFO - __main__ -   Batch number = 2
11/28/2021 01:36:00 - INFO - __main__ -   Batch number = 115
11/28/2021 01:36:01 - INFO - __main__ -   Batch number = 3
11/28/2021 01:36:01 - INFO - __main__ -   Batch number = 116
11/28/2021 01:36:01 - INFO - __main__ -   Batch number = 4
11/28/2021 01:36:02 - INFO - __main__ -   Batch number = 117
11/28/2021 01:36:02 - INFO - __main__ -   Batch number = 5
11/28/2021 01:36:02 - INFO - __main__ -   Batch number = 118
11/28/2021 01:36:03 - INFO - __main__ -   Batch number = 6
11/28/2021 01:36:03 - INFO - __main__ -   Batch number = 119
11/28/2021 01:36:03 - INFO - __main__ -   Batch number = 7
11/28/2021 01:36:04 - INFO - __main__ -   Batch number = 120
11/28/2021 01:36:04 - INFO - __main__ -   Batch number = 8
11/28/2021 01:36:04 - INFO - __main__ -   Batch number = 9
11/28/2021 01:36:04 - INFO - __main__ -   Batch number = 121
11/28/2021 01:36:04 - INFO - __main__ -   Batch number = 10
11/28/2021 01:36:05 - INFO - __main__ -   Batch number = 122
11/28/2021 01:36:05 - INFO - __main__ -   Batch number = 11
11/28/2021 01:36:05 - INFO - __main__ -   Batch number = 123
11/28/2021 01:36:06 - INFO - __main__ -   Batch number = 12
11/28/2021 01:36:06 - INFO - __main__ -   Batch number = 124
11/28/2021 01:36:06 - INFO - __main__ -   Batch number = 13
11/28/2021 01:36:06 - INFO - __main__ -   Batch number = 125
11/28/2021 01:36:07 - INFO - __main__ -   Batch number = 14
11/28/2021 01:36:07 - INFO - __main__ -   Batch number = 126
11/28/2021 01:36:07 - INFO - __main__ -   Batch number = 15
11/28/2021 01:36:07 - INFO - __main__ -   Batch number = 127
11/28/2021 01:36:08 - INFO - __main__ -   Batch number = 16
11/28/2021 01:36:08 - INFO - __main__ -   Batch number = 128
11/28/2021 01:36:08 - INFO - __main__ -   Batch number = 17
11/28/2021 01:36:08 - INFO - __main__ -   Batch number = 129
11/28/2021 01:36:09 - INFO - __main__ -   Batch number = 18
11/28/2021 01:36:09 - INFO - __main__ -   Batch number = 130
11/28/2021 01:36:09 - INFO - __main__ -   Batch number = 19
11/28/2021 01:36:09 - INFO - __main__ -   Batch number = 131
11/28/2021 01:36:09 - INFO - __main__ -   Batch number = 20
11/28/2021 01:36:10 - INFO - __main__ -   Batch number = 132
11/28/2021 01:36:10 - INFO - __main__ -   Batch number = 21
11/28/2021 01:36:10 - INFO - __main__ -   Batch number = 22
11/28/2021 01:36:10 - INFO - __main__ -   Batch number = 133
11/28/2021 01:36:11 - INFO - __main__ -   Batch number = 23
11/28/2021 01:36:11 - INFO - __main__ -   Batch number = 134
11/28/2021 01:36:11 - INFO - __main__ -   Batch number = 24
11/28/2021 01:36:11 - INFO - __main__ -   Batch number = 135
11/28/2021 01:36:12 - INFO - __main__ -   Batch number = 25
11/28/2021 01:36:12 - INFO - __main__ -   Batch number = 136
11/28/2021 01:36:12 - INFO - __main__ -   Batch number = 26
11/28/2021 01:36:12 - INFO - __main__ -   Batch number = 137
11/28/2021 01:36:12 - INFO - __main__ -   Batch number = 27
11/28/2021 01:36:13 - INFO - __main__ -   Batch number = 138
11/28/2021 01:36:13 - INFO - __main__ -   Batch number = 28
11/28/2021 01:36:13 - INFO - __main__ -   Batch number = 139
11/28/2021 01:36:13 - INFO - __main__ -   Batch number = 29
11/28/2021 01:36:13 - INFO - __main__ -   Batch number = 140
11/28/2021 01:36:14 - INFO - __main__ -   Batch number = 30
11/28/2021 01:36:14 - INFO - __main__ -   Batch number = 141
11/28/2021 01:36:14 - INFO - __main__ -   Batch number = 31
11/28/2021 01:36:14 - INFO - __main__ -   Batch number = 142
11/28/2021 01:36:15 - INFO - __main__ -   Batch number = 32
11/28/2021 01:36:15 - INFO - __main__ -   Batch number = 143
11/28/2021 01:36:15 - INFO - __main__ -   Batch number = 33
11/28/2021 01:36:15 - INFO - __main__ -   Batch number = 144
11/28/2021 01:36:16 - INFO - __main__ -   Batch number = 34
11/28/2021 01:36:16 - INFO - __main__ -   Batch number = 145
11/28/2021 01:36:16 - INFO - __main__ -   Batch number = 35
11/28/2021 01:36:16 - INFO - __main__ -   Batch number = 146
11/28/2021 01:36:16 - INFO - __main__ -   Batch number = 36
11/28/2021 01:36:17 - INFO - __main__ -   Batch number = 147
11/28/2021 01:36:17 - INFO - __main__ -   Batch number = 37
11/28/2021 01:36:17 - INFO - __main__ -   Batch number = 148
11/28/2021 01:36:17 - INFO - __main__ -   Batch number = 38
11/28/2021 01:36:17 - INFO - __main__ -   Batch number = 149
11/28/2021 01:36:17 - INFO - __main__ -   Batch number = 39
11/28/2021 01:36:18 - INFO - __main__ -   Batch number = 150
11/28/2021 01:36:18 - INFO - __main__ -   Batch number = 40
11/28/2021 01:36:18 - INFO - __main__ -   Batch number = 151
11/28/2021 01:36:18 - INFO - __main__ -   Batch number = 41
11/28/2021 01:36:19 - INFO - __main__ -   Batch number = 152
11/28/2021 01:36:19 - INFO - __main__ -   Batch number = 42
11/28/2021 01:36:19 - INFO - __main__ -   Batch number = 43
11/28/2021 01:36:19 - INFO - __main__ -   Batch number = 153
11/28/2021 01:36:20 - INFO - __main__ -   Batch number = 44
11/28/2021 01:36:20 - INFO - __main__ -   Batch number = 154
11/28/2021 01:36:20 - INFO - __main__ -   Batch number = 45
11/28/2021 01:36:20 - INFO - __main__ -   Batch number = 155
11/28/2021 01:36:21 - INFO - __main__ -   Batch number = 46
11/28/2021 01:36:21 - INFO - __main__ -   Batch number = 156
11/28/2021 01:36:21 - INFO - __main__ -   Batch number = 47
11/28/2021 01:36:21 - INFO - __main__ -   Batch number = 157
11/28/2021 01:36:21 - INFO - __main__ -   Batch number = 48
11/28/2021 01:36:22 - INFO - __main__ -   Batch number = 158
11/28/2021 01:36:22 - INFO - __main__ -   Batch number = 49
11/28/2021 01:36:22 - INFO - __main__ -   Batch number = 159
11/28/2021 01:36:22 - INFO - __main__ -   Batch number = 50
11/28/2021 01:36:23 - INFO - __main__ -   Batch number = 160
11/28/2021 01:36:23 - INFO - __main__ -   Batch number = 51
11/28/2021 01:36:23 - INFO - __main__ -   Batch number = 161
11/28/2021 01:36:23 - INFO - __main__ -   Batch number = 52
11/28/2021 01:36:23 - INFO - __main__ -   Batch number = 162
11/28/2021 01:36:24 - INFO - __main__ -   Batch number = 53
11/28/2021 01:36:24 - INFO - __main__ -   Batch number = 163
11/28/2021 01:36:24 - INFO - __main__ -   Batch number = 54
11/28/2021 01:36:24 - INFO - __main__ -   Batch number = 164
11/28/2021 01:36:25 - INFO - __main__ -   Batch number = 55
11/28/2021 01:36:25 - INFO - __main__ -   Batch number = 165
11/28/2021 01:36:25 - INFO - __main__ -   Batch number = 56
11/28/2021 01:36:25 - INFO - __main__ -   Batch number = 166
11/28/2021 01:36:25 - INFO - __main__ -   Batch number = 57
11/28/2021 01:36:26 - INFO - __main__ -   Batch number = 167
11/28/2021 01:36:26 - INFO - __main__ -   Batch number = 58
11/28/2021 01:36:26 - INFO - __main__ -   Batch number = 168
11/28/2021 01:36:26 - INFO - __main__ -   Batch number = 59
11/28/2021 01:36:27 - INFO - __main__ -   Batch number = 169
11/28/2021 01:36:27 - INFO - __main__ -   Batch number = 60
11/28/2021 01:36:27 - INFO - __main__ -   Batch number = 170
11/28/2021 01:36:27 - INFO - __main__ -   Batch number = 61
11/28/2021 01:36:28 - INFO - __main__ -   Batch number = 171
11/28/2021 01:36:28 - INFO - __main__ -   Batch number = 62
11/28/2021 01:36:28 - INFO - __main__ -   Batch number = 172
11/28/2021 01:36:28 - INFO - __main__ -   Batch number = 63
11/28/2021 01:36:29 - INFO - __main__ -   Batch number = 173
11/28/2021 01:36:29 - INFO - __main__ -   Batch number = 64
11/28/2021 01:36:29 - INFO - __main__ -   Batch number = 174
11/28/2021 01:36:30 - INFO - __main__ -   Batch number = 65
11/28/2021 01:36:30 - INFO - __main__ -   Batch number = 175
11/28/2021 01:36:30 - INFO - __main__ -   Batch number = 66
11/28/2021 01:36:31 - INFO - __main__ -   Batch number = 176
11/28/2021 01:36:31 - INFO - __main__ -   Batch number = 67
11/28/2021 01:36:31 - INFO - __main__ -   Batch number = 68
11/28/2021 01:36:31 - INFO - __main__ -   Batch number = 177
11/28/2021 01:36:32 - INFO - __main__ -   Batch number = 69
11/28/2021 01:36:32 - INFO - __main__ -   Batch number = 178
11/28/2021 01:36:33 - INFO - __main__ -   Batch number = 70
11/28/2021 01:36:33 - INFO - __main__ -   Batch number = 179
11/28/2021 01:36:33 - INFO - __main__ -   Batch number = 71
11/28/2021 01:36:33 - INFO - __main__ -   Batch number = 180
11/28/2021 01:36:34 - INFO - __main__ -   Batch number = 72
11/28/2021 01:36:34 - INFO - __main__ -   Batch number = 181
11/28/2021 01:36:35 - INFO - __main__ -   Batch number = 73
11/28/2021 01:36:35 - INFO - __main__ -   Batch number = 182
11/28/2021 01:36:35 - INFO - __main__ -   Batch number = 74
11/28/2021 01:36:35 - INFO - __main__ -   Batch number = 183
11/28/2021 01:36:36 - INFO - __main__ -   Batch number = 75
11/28/2021 01:36:36 - INFO - __main__ -   Batch number = 184
11/28/2021 01:36:36 - INFO - __main__ -   Batch number = 76
11/28/2021 01:36:37 - INFO - __main__ -   Batch number = 185
11/28/2021 01:36:37 - INFO - __main__ -   Batch number = 77
11/28/2021 01:36:37 - INFO - __main__ -   Batch number = 186
11/28/2021 01:36:38 - INFO - __main__ -   Batch number = 78
11/28/2021 01:36:38 - INFO - __main__ -   Batch number = 187
11/28/2021 01:36:38 - INFO - __main__ -   Batch number = 79
11/28/2021 01:36:39 - INFO - __main__ -   Batch number = 188
11/28/2021 01:36:39 - INFO - __main__ -   Batch number = 80
11/28/2021 01:36:39 - INFO - __main__ -   Batch number = 189
11/28/2021 01:36:40 - INFO - __main__ -   Batch number = 81
11/28/2021 01:36:40 - INFO - __main__ -   Batch number = 190
11/28/2021 01:36:40 - INFO - __main__ -   Batch number = 82
11/28/2021 01:36:40 - INFO - __main__ -   Batch number = 191
11/28/2021 01:36:41 - INFO - __main__ -   Batch number = 83
11/28/2021 01:36:41 - INFO - __main__ -   Batch number = 192
11/28/2021 01:36:41 - INFO - __main__ -   Batch number = 84
11/28/2021 01:36:41 - INFO - __main__ -   Batch number = 193
11/28/2021 01:36:42 - INFO - __main__ -   Batch number = 194
11/28/2021 01:36:43 - INFO - __main__ -   Batch number = 195
11/28/2021 01:36:43 - INFO - __main__ -   Batch number = 196
11/28/2021 01:36:43 - INFO - __main__ -   ***** Evaluation result  in pt *****
11/28/2021 01:36:43 - INFO - __main__ -     f1 = 0.8745490981963927
11/28/2021 01:36:43 - INFO - __main__ -     loss = 0.4402315229887054
11/28/2021 01:36:43 - INFO - __main__ -     precision = 0.8820190161514262
11/28/2021 01:36:43 - INFO - __main__ -     recall = 0.8672046448134644
11/28/2021 01:36:44 - INFO - __main__ -   Batch number = 197
11/28/2021 01:36:44 - INFO - __main__ -   Batch number = 198
11/28/2021 01:36:45 - INFO - __main__ -   Batch number = 199
11/28/2021 01:36:45 - INFO - __main__ -   Batch number = 200
11/28/2021 01:36:45 - INFO - __main__ -   Batch number = 201
11/28/2021 01:36:46 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='pt', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:36:46 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:36:46 - INFO - __main__ -   Seed = 3
11/28/2021 01:36:46 - INFO - root -   save model
11/28/2021 01:36:46 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='pt', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:36:46 - INFO - __main__ -   Loading pretrained model and tokenizer
11/28/2021 01:36:49 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
11/28/2021 01:36:49 - INFO - __main__ -   ***** Evaluation result  in is *****
11/28/2021 01:36:49 - INFO - __main__ -     f1 = 0.7336906375848986
11/28/2021 01:36:49 - INFO - __main__ -     loss = 0.9165598638911745
11/28/2021 01:36:49 - INFO - __main__ -     precision = 0.738955688558469
11/28/2021 01:36:49 - INFO - __main__ -     recall = 0.7285000826856293
11/28/2021 01:36:53 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='is', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:36:53 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:36:53 - INFO - __main__ -   Seed = 2
11/28/2021 01:36:53 - INFO - root -   save model
11/28/2021 01:36:53 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='is', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:36:53 - INFO - __main__ -   Loading pretrained model and tokenizer
11/28/2021 01:36:55 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
11/28/2021 01:36:55 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:36:55 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:36:55 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
11/28/2021 01:36:55 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:36:55 - INFO - root -   loading task adapter
11/28/2021 01:36:55 - INFO - root -   loading lang adpater bn/wiki@ukp
11/28/2021 01:36:55 - INFO - __main__ -   Adapter Languages : ['bn'], Length : 1
11/28/2021 01:36:55 - INFO - __main__ -   Adapter Names ['bn/wiki@ukp'], Length : 1
11/28/2021 01:36:55 - INFO - __main__ -   Language = bn
11/28/2021 01:36:55 - INFO - __main__ -   Adapter Name = bn/wiki@ukp
11/28/2021 01:36:56 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='zh', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:36:56 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:36:56 - INFO - __main__ -   Seed = 1
11/28/2021 01:36:56 - INFO - root -   save model
11/28/2021 01:36:56 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='zh', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:36:56 - INFO - __main__ -   Loading pretrained model and tokenizer
11/28/2021 01:36:58 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
11/28/2021 01:37:02 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:37:02 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:37:02 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
11/28/2021 01:37:02 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:37:02 - INFO - root -   loading task adapter
11/28/2021 01:37:02 - INFO - root -   loading lang adpater bn/wiki@ukp
11/28/2021 01:37:02 - INFO - __main__ -   Adapter Languages : ['bn'], Length : 1
11/28/2021 01:37:02 - INFO - __main__ -   Adapter Names ['bn/wiki@ukp'], Length : 1
11/28/2021 01:37:02 - INFO - __main__ -   Language = bn
11/28/2021 01:37:02 - INFO - __main__ -   Adapter Name = bn/wiki@ukp
11/28/2021 01:37:03 - INFO - __main__ -   Language adapter for pt not found, using bn instead
11/28/2021 01:37:03 - INFO - __main__ -   Set active language adapter to bn
11/28/2021 01:37:03 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:37:03 - INFO - __main__ -   Adapter Languages = ['bn']
11/28/2021 01:37:03 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_pt_bert-base-multilingual-cased_128
11/28/2021 01:37:03 - INFO - __main__ -   ***** Running evaluation  in pt *****
11/28/2021 01:37:03 - INFO - __main__ -     Num examples = 2682
11/28/2021 01:37:03 - INFO - __main__ -     Batch size = 32
11/28/2021 01:37:03 - INFO - __main__ -   Batch number = 1
11/28/2021 01:37:03 - INFO - __main__ -   Batch number = 2
11/28/2021 01:37:03 - INFO - __main__ -   Batch number = 3
11/28/2021 01:37:04 - INFO - __main__ -   Batch number = 4
11/28/2021 01:37:04 - INFO - __main__ -   Batch number = 5
11/28/2021 01:37:04 - INFO - __main__ -   Batch number = 6
11/28/2021 01:37:04 - INFO - __main__ -   Batch number = 7
11/28/2021 01:37:04 - INFO - __main__ -   Batch number = 8
11/28/2021 01:37:04 - INFO - __main__ -   Batch number = 9
11/28/2021 01:37:05 - INFO - __main__ -   Batch number = 10
11/28/2021 01:37:05 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:37:05 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:37:05 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
11/28/2021 01:37:05 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:37:05 - INFO - root -   loading task adapter
11/28/2021 01:37:05 - INFO - root -   loading lang adpater bn/wiki@ukp
11/28/2021 01:37:05 - INFO - __main__ -   Adapter Languages : ['bn'], Length : 1
11/28/2021 01:37:05 - INFO - __main__ -   Adapter Names ['bn/wiki@ukp'], Length : 1
11/28/2021 01:37:05 - INFO - __main__ -   Language = bn
11/28/2021 01:37:05 - INFO - __main__ -   Adapter Name = bn/wiki@ukp
11/28/2021 01:37:05 - INFO - __main__ -   Batch number = 11
11/28/2021 01:37:05 - INFO - __main__ -   Batch number = 12
11/28/2021 01:37:05 - INFO - __main__ -   Batch number = 13
11/28/2021 01:37:05 - INFO - __main__ -   Batch number = 14
11/28/2021 01:37:05 - INFO - __main__ -   Batch number = 15
11/28/2021 01:37:05 - INFO - __main__ -   Batch number = 16
11/28/2021 01:37:06 - INFO - __main__ -   Batch number = 17
11/28/2021 01:37:06 - INFO - __main__ -   Batch number = 18
11/28/2021 01:37:06 - INFO - __main__ -   Batch number = 19
11/28/2021 01:37:06 - INFO - __main__ -   Batch number = 20
11/28/2021 01:37:06 - INFO - __main__ -   Batch number = 21
11/28/2021 01:37:06 - INFO - __main__ -   Batch number = 22
11/28/2021 01:37:07 - INFO - __main__ -   Batch number = 23
11/28/2021 01:37:07 - INFO - __main__ -   Batch number = 24
11/28/2021 01:37:07 - INFO - __main__ -   Batch number = 25
11/28/2021 01:37:07 - INFO - __main__ -   Batch number = 26
11/28/2021 01:37:07 - INFO - __main__ -   Batch number = 27
11/28/2021 01:37:07 - INFO - __main__ -   Batch number = 28
11/28/2021 01:37:07 - INFO - __main__ -   Batch number = 29
11/28/2021 01:37:08 - INFO - __main__ -   Batch number = 30
11/28/2021 01:37:08 - INFO - __main__ -   Batch number = 31
11/28/2021 01:37:08 - INFO - __main__ -   Batch number = 32
11/28/2021 01:37:08 - INFO - __main__ -   Batch number = 33
11/28/2021 01:37:08 - INFO - __main__ -   Batch number = 34
11/28/2021 01:37:08 - INFO - __main__ -   Batch number = 35
11/28/2021 01:37:09 - INFO - __main__ -   Batch number = 36
11/28/2021 01:37:09 - INFO - __main__ -   Batch number = 37
11/28/2021 01:37:09 - INFO - __main__ -   Batch number = 38
11/28/2021 01:37:09 - INFO - __main__ -   Batch number = 39
11/28/2021 01:37:09 - INFO - __main__ -   Batch number = 40
11/28/2021 01:37:09 - INFO - __main__ -   Batch number = 41
11/28/2021 01:37:10 - INFO - __main__ -   Batch number = 42
11/28/2021 01:37:10 - INFO - __main__ -   Batch number = 43
11/28/2021 01:37:10 - INFO - __main__ -   Batch number = 44
11/28/2021 01:37:10 - INFO - __main__ -   Batch number = 45
11/28/2021 01:37:10 - INFO - __main__ -   Batch number = 46
11/28/2021 01:37:10 - INFO - __main__ -   Batch number = 47
11/28/2021 01:37:11 - INFO - __main__ -   Batch number = 48
11/28/2021 01:37:11 - INFO - __main__ -   Batch number = 49
11/28/2021 01:37:11 - INFO - __main__ -   Batch number = 50
11/28/2021 01:37:11 - INFO - __main__ -   Batch number = 51
11/28/2021 01:37:11 - INFO - __main__ -   Batch number = 52
11/28/2021 01:37:12 - INFO - __main__ -   Batch number = 53
11/28/2021 01:37:12 - INFO - __main__ -   Batch number = 54
11/28/2021 01:37:12 - INFO - __main__ -   Language adapter for is not found, using bn instead
11/28/2021 01:37:12 - INFO - __main__ -   Set active language adapter to bn
11/28/2021 01:37:12 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:37:12 - INFO - __main__ -   Adapter Languages = ['bn']
11/28/2021 01:37:12 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_is_bert-base-multilingual-cased_128
11/28/2021 01:37:12 - INFO - __main__ -   Batch number = 55
11/28/2021 01:37:12 - INFO - __main__ -   Batch number = 56
11/28/2021 01:37:12 - INFO - __main__ -   Batch number = 57
11/28/2021 01:37:12 - INFO - __main__ -   Batch number = 58
11/28/2021 01:37:13 - INFO - __main__ -   Batch number = 59
11/28/2021 01:37:13 - INFO - __main__ -   Batch number = 60
11/28/2021 01:37:13 - INFO - __main__ -   Batch number = 61
11/28/2021 01:37:13 - INFO - __main__ -   ***** Running evaluation  in is *****
11/28/2021 01:37:13 - INFO - __main__ -     Num examples = 6401
11/28/2021 01:37:13 - INFO - __main__ -     Batch size = 32
11/28/2021 01:37:13 - INFO - __main__ -   Batch number = 1
11/28/2021 01:37:13 - INFO - __main__ -   Batch number = 62
11/28/2021 01:37:13 - INFO - __main__ -   Batch number = 2
11/28/2021 01:37:14 - INFO - __main__ -   Batch number = 63
11/28/2021 01:37:14 - INFO - __main__ -   Batch number = 3
11/28/2021 01:37:14 - INFO - __main__ -   Batch number = 64
11/28/2021 01:37:14 - INFO - __main__ -   Batch number = 4
11/28/2021 01:37:15 - INFO - __main__ -   Batch number = 65
11/28/2021 01:37:15 - INFO - __main__ -   Batch number = 66
11/28/2021 01:37:15 - INFO - __main__ -   Batch number = 5
11/28/2021 01:37:15 - INFO - __main__ -   Batch number = 67
11/28/2021 01:37:15 - INFO - __main__ -   Language adapter for zh not found, using bn instead
11/28/2021 01:37:15 - INFO - __main__ -   Set active language adapter to bn
11/28/2021 01:37:15 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:37:15 - INFO - __main__ -   Adapter Languages = ['bn']
11/28/2021 01:37:15 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_zh_bert-base-multilingual-cased_128
11/28/2021 01:37:15 - INFO - __main__ -   Batch number = 6
11/28/2021 01:37:16 - INFO - __main__ -   Batch number = 68
11/28/2021 01:37:16 - INFO - __main__ -   Batch number = 7
11/28/2021 01:37:16 - INFO - __main__ -   ***** Running evaluation  in zh *****
11/28/2021 01:37:16 - INFO - __main__ -     Num examples = 3458
11/28/2021 01:37:16 - INFO - __main__ -     Batch size = 32
11/28/2021 01:37:16 - INFO - __main__ -   Batch number = 1
11/28/2021 01:37:16 - INFO - __main__ -   Batch number = 2
11/28/2021 01:37:16 - INFO - __main__ -   Batch number = 69
11/28/2021 01:37:16 - INFO - __main__ -   Batch number = 8
11/28/2021 01:37:17 - INFO - __main__ -   Batch number = 3
11/28/2021 01:37:17 - INFO - __main__ -   Batch number = 70
11/28/2021 01:37:17 - INFO - __main__ -   Batch number = 9
11/28/2021 01:37:17 - INFO - __main__ -   Batch number = 4
11/28/2021 01:37:18 - INFO - __main__ -   Batch number = 5
11/28/2021 01:37:18 - INFO - __main__ -   Batch number = 71
11/28/2021 01:37:18 - INFO - __main__ -   Batch number = 10
11/28/2021 01:37:18 - INFO - __main__ -   Batch number = 6
11/28/2021 01:37:18 - INFO - __main__ -   Batch number = 7
11/28/2021 01:37:18 - INFO - __main__ -   Batch number = 72
11/28/2021 01:37:18 - INFO - __main__ -   Batch number = 11
11/28/2021 01:37:18 - INFO - __main__ -   Batch number = 8
11/28/2021 01:37:19 - INFO - __main__ -   Batch number = 9
11/28/2021 01:37:19 - INFO - __main__ -   Batch number = 73
11/28/2021 01:37:19 - INFO - __main__ -   Batch number = 12
11/28/2021 01:37:19 - INFO - __main__ -   Batch number = 10
11/28/2021 01:37:19 - INFO - __main__ -   Batch number = 11
11/28/2021 01:37:20 - INFO - __main__ -   Batch number = 74
11/28/2021 01:37:20 - INFO - __main__ -   Batch number = 13
11/28/2021 01:37:20 - INFO - __main__ -   Batch number = 12
11/28/2021 01:37:20 - INFO - __main__ -   Batch number = 13
11/28/2021 01:37:20 - INFO - __main__ -   Batch number = 75
11/28/2021 01:37:20 - INFO - __main__ -   Batch number = 14
11/28/2021 01:37:21 - INFO - __main__ -   Batch number = 14
11/28/2021 01:37:21 - INFO - __main__ -   Batch number = 15
11/28/2021 01:37:21 - INFO - __main__ -   Batch number = 76
11/28/2021 01:37:21 - INFO - __main__ -   Batch number = 15
11/28/2021 01:37:21 - INFO - __main__ -   Batch number = 16
11/28/2021 01:37:22 - INFO - __main__ -   Batch number = 17
11/28/2021 01:37:22 - INFO - __main__ -   Batch number = 16
11/28/2021 01:37:22 - INFO - __main__ -   Batch number = 77
11/28/2021 01:37:22 - INFO - __main__ -   Batch number = 18
11/28/2021 01:37:22 - INFO - __main__ -   Batch number = 19
11/28/2021 01:37:22 - INFO - __main__ -   Batch number = 17
11/28/2021 01:37:22 - INFO - __main__ -   Batch number = 78
11/28/2021 01:37:23 - INFO - __main__ -   Batch number = 20
11/28/2021 01:37:23 - INFO - __main__ -   Batch number = 21
11/28/2021 01:37:23 - INFO - __main__ -   Batch number = 18
11/28/2021 01:37:23 - INFO - __main__ -   Batch number = 79
11/28/2021 01:37:23 - INFO - __main__ -   Batch number = 22
11/28/2021 01:37:24 - INFO - __main__ -   Batch number = 23
11/28/2021 01:37:24 - INFO - __main__ -   Batch number = 19
11/28/2021 01:37:24 - INFO - __main__ -   Batch number = 80
11/28/2021 01:37:24 - INFO - __main__ -   Batch number = 24
11/28/2021 01:37:24 - INFO - __main__ -   Batch number = 25
11/28/2021 01:37:24 - INFO - __main__ -   Batch number = 20
11/28/2021 01:37:24 - INFO - __main__ -   Batch number = 81
11/28/2021 01:37:25 - INFO - __main__ -   Batch number = 26
11/28/2021 01:37:25 - INFO - __main__ -   Batch number = 21
11/28/2021 01:37:25 - INFO - __main__ -   Batch number = 82
11/28/2021 01:37:25 - INFO - __main__ -   Batch number = 27
11/28/2021 01:37:25 - INFO - __main__ -   Batch number = 28
11/28/2021 01:37:26 - INFO - __main__ -   Batch number = 22
11/28/2021 01:37:26 - INFO - __main__ -   Batch number = 83
11/28/2021 01:37:26 - INFO - __main__ -   Batch number = 29
11/28/2021 01:37:26 - INFO - __main__ -   Batch number = 30
11/28/2021 01:37:26 - INFO - __main__ -   Batch number = 23
11/28/2021 01:37:26 - INFO - __main__ -   Batch number = 84
11/28/2021 01:37:26 - INFO - __main__ -   Batch number = 31
11/28/2021 01:37:27 - INFO - __main__ -   Batch number = 32
11/28/2021 01:37:27 - INFO - __main__ -   Batch number = 24
11/28/2021 01:37:27 - INFO - __main__ -   Batch number = 33
11/28/2021 01:37:27 - INFO - __main__ -   Batch number = 34
11/28/2021 01:37:27 - INFO - __main__ -   Batch number = 35
11/28/2021 01:37:27 - INFO - __main__ -   Batch number = 25
11/28/2021 01:37:28 - INFO - __main__ -   Batch number = 36
11/28/2021 01:37:28 - INFO - __main__ -   Batch number = 26
11/28/2021 01:37:28 - INFO - __main__ -   Batch number = 37
11/28/2021 01:37:28 - INFO - __main__ -   Batch number = 38
11/28/2021 01:37:28 - INFO - __main__ -   Batch number = 27
11/28/2021 01:37:29 - INFO - __main__ -   ***** Evaluation result  in pt *****
11/28/2021 01:37:29 - INFO - __main__ -     f1 = 0.8830346267617888
11/28/2021 01:37:29 - INFO - __main__ -     loss = 0.39384881939206806
11/28/2021 01:37:29 - INFO - __main__ -     precision = 0.8892393418493403
11/28/2021 01:37:29 - INFO - __main__ -     recall = 0.8769158991550172
11/28/2021 01:37:29 - INFO - __main__ -   Batch number = 39
11/28/2021 01:37:29 - INFO - __main__ -   Batch number = 28
11/28/2021 01:37:29 - INFO - __main__ -   Batch number = 40
11/28/2021 01:37:29 - INFO - __main__ -   Batch number = 29
11/28/2021 01:37:29 - INFO - __main__ -   Batch number = 41
11/28/2021 01:37:30 - INFO - __main__ -   Batch number = 42
11/28/2021 01:37:30 - INFO - __main__ -   Batch number = 30
11/28/2021 01:37:30 - INFO - __main__ -   Batch number = 43
11/28/2021 01:37:30 - INFO - __main__ -   Batch number = 44
11/28/2021 01:37:30 - INFO - __main__ -   Batch number = 31
11/28/2021 01:37:31 - INFO - __main__ -   Batch number = 45
11/28/2021 01:37:31 - INFO - __main__ -   Batch number = 32
11/28/2021 01:37:31 - INFO - __main__ -   Batch number = 46
11/28/2021 01:37:31 - INFO - __main__ -   Batch number = 47
11/28/2021 01:37:31 - INFO - __main__ -   Batch number = 33
11/28/2021 01:37:32 - INFO - __main__ -   Batch number = 48
11/28/2021 01:37:32 - INFO - __main__ -   Batch number = 34
11/28/2021 01:37:32 - INFO - __main__ -   Batch number = 49
11/28/2021 01:37:32 - INFO - __main__ -   Batch number = 50
11/28/2021 01:37:32 - INFO - __main__ -   Batch number = 35
11/28/2021 01:37:33 - INFO - __main__ -   Batch number = 51
11/28/2021 01:37:33 - INFO - __main__ -   Batch number = 36
11/28/2021 01:37:33 - INFO - __main__ -   Batch number = 52
11/28/2021 01:37:33 - INFO - __main__ -   Batch number = 53
11/28/2021 01:37:33 - INFO - __main__ -   Batch number = 37
11/28/2021 01:37:34 - INFO - __main__ -   Batch number = 54
11/28/2021 01:37:34 - INFO - __main__ -   Batch number = 38
11/28/2021 01:37:34 - INFO - __main__ -   Batch number = 55
11/28/2021 01:37:34 - INFO - __main__ -   Batch number = 56
11/28/2021 01:37:34 - INFO - __main__ -   Batch number = 39
11/28/2021 01:37:35 - INFO - __main__ -   Batch number = 57
11/28/2021 01:37:35 - INFO - __main__ -   Batch number = 40
11/28/2021 01:37:35 - INFO - __main__ -   Batch number = 58
11/28/2021 01:37:35 - INFO - __main__ -   Batch number = 59
11/28/2021 01:37:35 - INFO - __main__ -   Batch number = 41
11/28/2021 01:37:36 - INFO - __main__ -   Batch number = 60
11/28/2021 01:37:36 - INFO - __main__ -   Batch number = 42
11/28/2021 01:37:36 - INFO - __main__ -   Batch number = 61
11/28/2021 01:37:36 - INFO - __main__ -   Batch number = 62
11/28/2021 01:37:36 - INFO - __main__ -   Batch number = 43
11/28/2021 01:37:37 - INFO - __main__ -   Batch number = 63
11/28/2021 01:37:37 - INFO - __main__ -   Batch number = 44
11/28/2021 01:37:37 - INFO - __main__ -   Batch number = 64
11/28/2021 01:37:37 - INFO - __main__ -   Batch number = 65
11/28/2021 01:37:37 - INFO - __main__ -   Batch number = 45
11/28/2021 01:37:38 - INFO - __main__ -   Batch number = 66
11/28/2021 01:37:38 - INFO - __main__ -   Batch number = 46
11/28/2021 01:37:38 - INFO - __main__ -   Batch number = 67
11/28/2021 01:37:38 - INFO - __main__ -   Batch number = 68
11/28/2021 01:37:38 - INFO - __main__ -   Batch number = 47
11/28/2021 01:37:39 - INFO - __main__ -   Batch number = 69
11/28/2021 01:37:39 - INFO - __main__ -   Batch number = 48
11/28/2021 01:37:39 - INFO - __main__ -   Batch number = 70
11/28/2021 01:37:39 - INFO - __main__ -   Batch number = 71
11/28/2021 01:37:39 - INFO - __main__ -   Batch number = 49
11/28/2021 01:37:40 - INFO - __main__ -   Batch number = 72
11/28/2021 01:37:40 - INFO - __main__ -   Batch number = 50
11/28/2021 01:37:40 - INFO - __main__ -   Batch number = 73
11/28/2021 01:37:40 - INFO - __main__ -   Batch number = 51
11/28/2021 01:37:40 - INFO - __main__ -   Batch number = 74
11/28/2021 01:37:41 - INFO - __main__ -   Batch number = 75
11/28/2021 01:37:41 - INFO - __main__ -   Batch number = 52
11/28/2021 01:37:41 - INFO - __main__ -   Batch number = 76
11/28/2021 01:37:41 - INFO - __main__ -   Batch number = 53
11/28/2021 01:37:41 - INFO - __main__ -   Batch number = 77
11/28/2021 01:37:42 - INFO - __main__ -   Batch number = 78
11/28/2021 01:37:42 - INFO - __main__ -   Batch number = 54
11/28/2021 01:37:42 - INFO - __main__ -   Batch number = 79
11/28/2021 01:37:42 - INFO - __main__ -   Batch number = 55
11/28/2021 01:37:42 - INFO - __main__ -   Batch number = 80
11/28/2021 01:37:43 - INFO - __main__ -   Batch number = 81
11/28/2021 01:37:43 - INFO - __main__ -   Batch number = 56
11/28/2021 01:37:43 - INFO - __main__ -   Batch number = 82
11/28/2021 01:37:43 - INFO - __main__ -   Batch number = 57
11/28/2021 01:37:43 - INFO - __main__ -   Batch number = 83
11/28/2021 01:37:44 - INFO - __main__ -   Batch number = 84
11/28/2021 01:37:44 - INFO - __main__ -   Batch number = 58
11/28/2021 01:37:44 - INFO - __main__ -   Batch number = 85
11/28/2021 01:37:44 - INFO - __main__ -   Batch number = 59
11/28/2021 01:37:45 - INFO - __main__ -   Batch number = 86
11/28/2021 01:37:45 - INFO - __main__ -   Batch number = 60
11/28/2021 01:37:45 - INFO - __main__ -   Batch number = 87
11/28/2021 01:37:45 - INFO - __main__ -   Batch number = 61
11/28/2021 01:37:45 - INFO - __main__ -   Batch number = 88
11/28/2021 01:37:45 - INFO - __main__ -   Batch number = 62
11/28/2021 01:37:46 - INFO - __main__ -   Batch number = 89
11/28/2021 01:37:46 - INFO - __main__ -   Batch number = 63
11/28/2021 01:37:46 - INFO - __main__ -   Batch number = 90
11/28/2021 01:37:46 - INFO - __main__ -   Batch number = 64
11/28/2021 01:37:46 - INFO - __main__ -   Batch number = 91
11/28/2021 01:37:46 - INFO - __main__ -   Batch number = 65
11/28/2021 01:37:47 - INFO - __main__ -   Batch number = 92
11/28/2021 01:37:47 - INFO - __main__ -   Batch number = 66
11/28/2021 01:37:47 - INFO - __main__ -   Batch number = 93
11/28/2021 01:37:47 - INFO - __main__ -   Batch number = 67
11/28/2021 01:37:47 - INFO - __main__ -   Batch number = 94
11/28/2021 01:37:47 - INFO - __main__ -   Batch number = 68
11/28/2021 01:37:48 - INFO - __main__ -   Batch number = 95
11/28/2021 01:37:48 - INFO - __main__ -   Batch number = 69
11/28/2021 01:37:48 - INFO - __main__ -   Batch number = 96
11/28/2021 01:37:48 - INFO - __main__ -   Batch number = 70
11/28/2021 01:37:48 - INFO - __main__ -   Batch number = 97
11/28/2021 01:37:48 - INFO - __main__ -   Batch number = 71
11/28/2021 01:37:49 - INFO - __main__ -   Batch number = 98
11/28/2021 01:37:49 - INFO - __main__ -   Batch number = 72
11/28/2021 01:37:49 - INFO - __main__ -   Batch number = 99
11/28/2021 01:37:49 - INFO - __main__ -   Batch number = 73
11/28/2021 01:37:49 - INFO - __main__ -   Batch number = 100
11/28/2021 01:37:49 - INFO - __main__ -   Batch number = 74
11/28/2021 01:37:50 - INFO - __main__ -   Batch number = 101
11/28/2021 01:37:50 - INFO - __main__ -   Batch number = 75
11/28/2021 01:37:50 - INFO - __main__ -   Batch number = 102
11/28/2021 01:37:50 - INFO - __main__ -   Batch number = 76
11/28/2021 01:37:50 - INFO - __main__ -   Batch number = 103
11/28/2021 01:37:50 - INFO - __main__ -   Batch number = 77
11/28/2021 01:37:51 - INFO - __main__ -   Batch number = 104
11/28/2021 01:37:51 - INFO - __main__ -   Batch number = 78
11/28/2021 01:37:51 - INFO - __main__ -   Batch number = 105
11/28/2021 01:37:51 - INFO - __main__ -   Batch number = 79
11/28/2021 01:37:51 - INFO - __main__ -   Batch number = 80
11/28/2021 01:37:52 - INFO - __main__ -   Batch number = 106
11/28/2021 01:37:52 - INFO - __main__ -   Batch number = 81
11/28/2021 01:37:52 - INFO - __main__ -   Batch number = 82
11/28/2021 01:37:52 - INFO - __main__ -   Batch number = 83
11/28/2021 01:37:52 - INFO - __main__ -   Batch number = 107
11/28/2021 01:37:53 - INFO - __main__ -   Batch number = 84
11/28/2021 01:37:53 - INFO - __main__ -   Batch number = 85
11/28/2021 01:37:53 - INFO - __main__ -   Batch number = 108
11/28/2021 01:37:54 - INFO - __main__ -   Batch number = 86
11/28/2021 01:37:54 - INFO - __main__ -   Batch number = 109
11/28/2021 01:37:54 - INFO - __main__ -   Batch number = 87
11/28/2021 01:37:55 - INFO - __main__ -   Batch number = 88
11/28/2021 01:37:55 - INFO - __main__ -   Batch number = 89
11/28/2021 01:37:55 - INFO - __main__ -   ***** Evaluation result  in zh *****
11/28/2021 01:37:55 - INFO - __main__ -     f1 = 0.6023805447681176
11/28/2021 01:37:55 - INFO - __main__ -     loss = 1.443457361755021
11/28/2021 01:37:55 - INFO - __main__ -     precision = 0.6114210458927036
11/28/2021 01:37:55 - INFO - __main__ -     recall = 0.5936034948146044
11/28/2021 01:37:56 - INFO - __main__ -   Batch number = 90
11/28/2021 01:37:56 - INFO - __main__ -   Batch number = 91
11/28/2021 01:37:57 - INFO - __main__ -   Batch number = 92
11/28/2021 01:37:57 - INFO - __main__ -   Batch number = 93
11/28/2021 01:37:58 - INFO - __main__ -   Batch number = 94
11/28/2021 01:37:58 - INFO - __main__ -   Batch number = 95
11/28/2021 01:37:58 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='zh', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:37:58 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:37:58 - INFO - __main__ -   Seed = 2
11/28/2021 01:37:58 - INFO - root -   save model
11/28/2021 01:37:58 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='zh', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:37:58 - INFO - __main__ -   Loading pretrained model and tokenizer
11/28/2021 01:37:59 - INFO - __main__ -   Batch number = 96
11/28/2021 01:37:59 - INFO - __main__ -   Batch number = 97
11/28/2021 01:38:00 - INFO - __main__ -   Batch number = 98
11/28/2021 01:38:00 - INFO - __main__ -   Batch number = 99
11/28/2021 01:38:01 - INFO - __main__ -   Batch number = 100
11/28/2021 01:38:01 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
11/28/2021 01:38:01 - INFO - __main__ -   Batch number = 101
11/28/2021 01:38:02 - INFO - __main__ -   Batch number = 102
11/28/2021 01:38:02 - INFO - __main__ -   Batch number = 103
11/28/2021 01:38:03 - INFO - __main__ -   Batch number = 104
11/28/2021 01:38:03 - INFO - __main__ -   Batch number = 105
11/28/2021 01:38:04 - INFO - __main__ -   Batch number = 106
11/28/2021 01:38:04 - INFO - __main__ -   Batch number = 107
11/28/2021 01:38:05 - INFO - __main__ -   Batch number = 108
11/28/2021 01:38:05 - INFO - __main__ -   Batch number = 109
11/28/2021 01:38:06 - INFO - __main__ -   Batch number = 110
11/28/2021 01:38:06 - INFO - __main__ -   Batch number = 111
11/28/2021 01:38:07 - INFO - __main__ -   Batch number = 112
11/28/2021 01:38:07 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:38:07 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:38:07 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
11/28/2021 01:38:07 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:38:07 - INFO - root -   loading task adapter
11/28/2021 01:38:07 - INFO - root -   loading lang adpater bn/wiki@ukp
11/28/2021 01:38:07 - INFO - __main__ -   Adapter Languages : ['bn'], Length : 1
11/28/2021 01:38:07 - INFO - __main__ -   Adapter Names ['bn/wiki@ukp'], Length : 1
11/28/2021 01:38:07 - INFO - __main__ -   Language = bn
11/28/2021 01:38:07 - INFO - __main__ -   Adapter Name = bn/wiki@ukp
11/28/2021 01:38:07 - INFO - __main__ -   Batch number = 113
11/28/2021 01:38:08 - INFO - __main__ -   Batch number = 114
11/28/2021 01:38:08 - INFO - __main__ -   Batch number = 115
11/28/2021 01:38:09 - INFO - __main__ -   Batch number = 116
11/28/2021 01:38:09 - INFO - __main__ -   Batch number = 117
11/28/2021 01:38:10 - INFO - __main__ -   Batch number = 118
11/28/2021 01:38:10 - INFO - __main__ -   Batch number = 119
11/28/2021 01:38:11 - INFO - __main__ -   Batch number = 120
11/28/2021 01:38:12 - INFO - __main__ -   Batch number = 121
11/28/2021 01:38:12 - INFO - __main__ -   Batch number = 122
11/28/2021 01:38:13 - INFO - __main__ -   Batch number = 123
11/28/2021 01:38:13 - INFO - __main__ -   Batch number = 124
11/28/2021 01:38:14 - INFO - __main__ -   Batch number = 125
11/28/2021 01:38:15 - INFO - __main__ -   Batch number = 126
11/28/2021 01:38:15 - INFO - __main__ -   Batch number = 127
11/28/2021 01:38:16 - INFO - __main__ -   Batch number = 128
11/28/2021 01:38:16 - INFO - __main__ -   Batch number = 129
11/28/2021 01:38:17 - INFO - __main__ -   Batch number = 130
11/28/2021 01:38:18 - INFO - __main__ -   Batch number = 131
11/28/2021 01:38:18 - INFO - __main__ -   Language adapter for zh not found, using bn instead
11/28/2021 01:38:18 - INFO - __main__ -   Set active language adapter to bn
11/28/2021 01:38:18 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:38:18 - INFO - __main__ -   Adapter Languages = ['bn']
11/28/2021 01:38:18 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_zh_bert-base-multilingual-cased_128
11/28/2021 01:38:18 - INFO - __main__ -   Batch number = 132
11/28/2021 01:38:19 - INFO - __main__ -   ***** Running evaluation  in zh *****
11/28/2021 01:38:19 - INFO - __main__ -     Num examples = 3458
11/28/2021 01:38:19 - INFO - __main__ -     Batch size = 32
11/28/2021 01:38:19 - INFO - __main__ -   Batch number = 1
11/28/2021 01:38:19 - INFO - __main__ -   Batch number = 133
11/28/2021 01:38:19 - INFO - __main__ -   Batch number = 2
11/28/2021 01:38:20 - INFO - __main__ -   Batch number = 3
11/28/2021 01:38:20 - INFO - __main__ -   Batch number = 134
11/28/2021 01:38:20 - INFO - __main__ -   Batch number = 4
11/28/2021 01:38:20 - INFO - __main__ -   Batch number = 135
11/28/2021 01:38:20 - INFO - __main__ -   Batch number = 5
11/28/2021 01:38:21 - INFO - __main__ -   Batch number = 136
11/28/2021 01:38:21 - INFO - __main__ -   Batch number = 6
11/28/2021 01:38:21 - INFO - __main__ -   Batch number = 137
11/28/2021 01:38:21 - INFO - __main__ -   Batch number = 7
11/28/2021 01:38:22 - INFO - __main__ -   Batch number = 138
11/28/2021 01:38:22 - INFO - __main__ -   Batch number = 8
11/28/2021 01:38:22 - INFO - __main__ -   Batch number = 139
11/28/2021 01:38:22 - INFO - __main__ -   Batch number = 9
11/28/2021 01:38:23 - INFO - __main__ -   Batch number = 10
11/28/2021 01:38:23 - INFO - __main__ -   Batch number = 140
11/28/2021 01:38:23 - INFO - __main__ -   Batch number = 11
11/28/2021 01:38:24 - INFO - __main__ -   Batch number = 141
11/28/2021 01:38:24 - INFO - __main__ -   Batch number = 12
11/28/2021 01:38:24 - INFO - __main__ -   Batch number = 142
11/28/2021 01:38:24 - INFO - __main__ -   Batch number = 13
11/28/2021 01:38:25 - INFO - __main__ -   Batch number = 143
11/28/2021 01:38:25 - INFO - __main__ -   Batch number = 14
11/28/2021 01:38:25 - INFO - __main__ -   Batch number = 15
11/28/2021 01:38:25 - INFO - __main__ -   Batch number = 144
11/28/2021 01:38:26 - INFO - __main__ -   Batch number = 16
11/28/2021 01:38:26 - INFO - __main__ -   Batch number = 145
11/28/2021 01:38:26 - INFO - __main__ -   Batch number = 17
11/28/2021 01:38:27 - INFO - __main__ -   Batch number = 146
11/28/2021 01:38:27 - INFO - __main__ -   Batch number = 18
11/28/2021 01:38:27 - INFO - __main__ -   Batch number = 19
11/28/2021 01:38:27 - INFO - __main__ -   Batch number = 147
11/28/2021 01:38:28 - INFO - __main__ -   Batch number = 20
11/28/2021 01:38:28 - INFO - __main__ -   Batch number = 21
11/28/2021 01:38:28 - INFO - __main__ -   Batch number = 148
11/28/2021 01:38:28 - INFO - __main__ -   Batch number = 22
11/28/2021 01:38:29 - INFO - __main__ -   Batch number = 23
11/28/2021 01:38:29 - INFO - __main__ -   Batch number = 149
11/28/2021 01:38:29 - INFO - __main__ -   Batch number = 24
11/28/2021 01:38:29 - INFO - __main__ -   Batch number = 25
11/28/2021 01:38:29 - INFO - __main__ -   Batch number = 150
11/28/2021 01:38:30 - INFO - __main__ -   Batch number = 26
11/28/2021 01:38:30 - INFO - __main__ -   Batch number = 27
11/28/2021 01:38:30 - INFO - __main__ -   Batch number = 151
11/28/2021 01:38:30 - INFO - __main__ -   Batch number = 28
11/28/2021 01:38:31 - INFO - __main__ -   Batch number = 29
11/28/2021 01:38:31 - INFO - __main__ -   Batch number = 152
11/28/2021 01:38:31 - INFO - __main__ -   Batch number = 30
11/28/2021 01:38:31 - INFO - __main__ -   Batch number = 31
11/28/2021 01:38:31 - INFO - __main__ -   Batch number = 153
11/28/2021 01:38:32 - INFO - __main__ -   Batch number = 32
11/28/2021 01:38:32 - INFO - __main__ -   Batch number = 154
11/28/2021 01:38:32 - INFO - __main__ -   Batch number = 33
11/28/2021 01:38:33 - INFO - __main__ -   Batch number = 155
11/28/2021 01:38:33 - INFO - __main__ -   Batch number = 34
11/28/2021 01:38:33 - INFO - __main__ -   Batch number = 35
11/28/2021 01:38:33 - INFO - __main__ -   Batch number = 156
11/28/2021 01:38:34 - INFO - __main__ -   Batch number = 36
11/28/2021 01:38:34 - INFO - __main__ -   Batch number = 157
11/28/2021 01:38:34 - INFO - __main__ -   Batch number = 37
11/28/2021 01:38:34 - INFO - __main__ -   Batch number = 158
11/28/2021 01:38:35 - INFO - __main__ -   Batch number = 38
11/28/2021 01:38:35 - INFO - __main__ -   Batch number = 159
11/28/2021 01:38:35 - INFO - __main__ -   Batch number = 39
11/28/2021 01:38:36 - INFO - __main__ -   Batch number = 160
11/28/2021 01:38:36 - INFO - __main__ -   Batch number = 40
11/28/2021 01:38:36 - INFO - __main__ -   Batch number = 41
11/28/2021 01:38:36 - INFO - __main__ -   Batch number = 161
11/28/2021 01:38:37 - INFO - __main__ -   Batch number = 42
11/28/2021 01:38:37 - INFO - __main__ -   Batch number = 162
11/28/2021 01:38:37 - INFO - __main__ -   Batch number = 43
11/28/2021 01:38:37 - INFO - __main__ -   Batch number = 163
11/28/2021 01:38:38 - INFO - __main__ -   Batch number = 44
11/28/2021 01:38:38 - INFO - __main__ -   Batch number = 164
11/28/2021 01:38:38 - INFO - __main__ -   Batch number = 45
11/28/2021 01:38:39 - INFO - __main__ -   Batch number = 46
11/28/2021 01:38:39 - INFO - __main__ -   Batch number = 165
11/28/2021 01:38:39 - INFO - __main__ -   Batch number = 47
11/28/2021 01:38:39 - INFO - __main__ -   Batch number = 166
11/28/2021 01:38:40 - INFO - __main__ -   Batch number = 48
11/28/2021 01:38:40 - INFO - __main__ -   Batch number = 167
11/28/2021 01:38:40 - INFO - __main__ -   Batch number = 49
11/28/2021 01:38:40 - INFO - __main__ -   Batch number = 168
11/28/2021 01:38:41 - INFO - __main__ -   Batch number = 169
11/28/2021 01:38:41 - INFO - __main__ -   Batch number = 50
11/28/2021 01:38:41 - INFO - __main__ -   Batch number = 170
11/28/2021 01:38:41 - INFO - __main__ -   Batch number = 51
11/28/2021 01:38:41 - INFO - __main__ -   Batch number = 171
11/28/2021 01:38:42 - INFO - __main__ -   Batch number = 52
11/28/2021 01:38:42 - INFO - __main__ -   Batch number = 172
11/28/2021 01:38:42 - INFO - __main__ -   Batch number = 53
11/28/2021 01:38:42 - INFO - __main__ -   Batch number = 173
11/28/2021 01:38:43 - INFO - __main__ -   Batch number = 174
11/28/2021 01:38:43 - INFO - __main__ -   Batch number = 54
11/28/2021 01:38:43 - INFO - __main__ -   Batch number = 175
11/28/2021 01:38:43 - INFO - __main__ -   Batch number = 176
11/28/2021 01:38:43 - INFO - __main__ -   Batch number = 177
11/28/2021 01:38:43 - INFO - __main__ -   Batch number = 55
11/28/2021 01:38:44 - INFO - __main__ -   Batch number = 178
11/28/2021 01:38:44 - INFO - __main__ -   Batch number = 56
11/28/2021 01:38:44 - INFO - __main__ -   Batch number = 179
11/28/2021 01:38:44 - INFO - __main__ -   Batch number = 180
11/28/2021 01:38:44 - INFO - __main__ -   Batch number = 57
11/28/2021 01:38:44 - INFO - __main__ -   Batch number = 181
11/28/2021 01:38:45 - INFO - __main__ -   Batch number = 182
11/28/2021 01:38:45 - INFO - __main__ -   Batch number = 58
11/28/2021 01:38:45 - INFO - __main__ -   Batch number = 183
11/28/2021 01:38:45 - INFO - __main__ -   Batch number = 184
11/28/2021 01:38:45 - INFO - __main__ -   Batch number = 185
11/28/2021 01:38:45 - INFO - __main__ -   Batch number = 59
11/28/2021 01:38:45 - INFO - __main__ -   Batch number = 186
11/28/2021 01:38:46 - INFO - __main__ -   Batch number = 187
11/28/2021 01:38:46 - INFO - __main__ -   Batch number = 60
11/28/2021 01:38:46 - INFO - __main__ -   Batch number = 188
11/28/2021 01:38:46 - INFO - __main__ -   Batch number = 189
11/28/2021 01:38:46 - INFO - __main__ -   Batch number = 190
11/28/2021 01:38:46 - INFO - __main__ -   Batch number = 61
11/28/2021 01:38:46 - INFO - __main__ -   Batch number = 191
11/28/2021 01:38:47 - INFO - __main__ -   Batch number = 192
11/28/2021 01:38:47 - INFO - __main__ -   Batch number = 62
11/28/2021 01:38:47 - INFO - __main__ -   Batch number = 193
11/28/2021 01:38:47 - INFO - __main__ -   Batch number = 194
11/28/2021 01:38:47 - INFO - __main__ -   Batch number = 63
11/28/2021 01:38:47 - INFO - __main__ -   Batch number = 195
11/28/2021 01:38:48 - INFO - __main__ -   Batch number = 196
11/28/2021 01:38:48 - INFO - __main__ -   Batch number = 64
11/28/2021 01:38:48 - INFO - __main__ -   Batch number = 197
11/28/2021 01:38:48 - INFO - __main__ -   Batch number = 198
11/28/2021 01:38:48 - INFO - __main__ -   Batch number = 199
11/28/2021 01:38:48 - INFO - __main__ -   Batch number = 65
11/28/2021 01:38:48 - INFO - __main__ -   Batch number = 200
11/28/2021 01:38:49 - INFO - __main__ -   Batch number = 201
11/28/2021 01:38:49 - INFO - __main__ -   Batch number = 66
11/28/2021 01:38:49 - INFO - __main__ -   Batch number = 67
11/28/2021 01:38:50 - INFO - __main__ -   Batch number = 68
11/28/2021 01:38:50 - INFO - __main__ -   Batch number = 69
11/28/2021 01:38:51 - INFO - __main__ -   Batch number = 70
11/28/2021 01:38:51 - INFO - __main__ -   Batch number = 71
11/28/2021 01:38:52 - INFO - __main__ -   Batch number = 72
11/28/2021 01:38:52 - INFO - __main__ -   Batch number = 73
11/28/2021 01:38:53 - INFO - __main__ -   ***** Evaluation result  in is *****
11/28/2021 01:38:53 - INFO - __main__ -     f1 = 0.7106669377931416
11/28/2021 01:38:53 - INFO - __main__ -     loss = 1.0205595457731789
11/28/2021 01:38:53 - INFO - __main__ -     precision = 0.7163190151067147
11/28/2021 01:38:53 - INFO - __main__ -     recall = 0.705103357036547
11/28/2021 01:38:53 - INFO - __main__ -   Batch number = 74
11/28/2021 01:38:53 - INFO - __main__ -   Batch number = 75
11/28/2021 01:38:54 - INFO - __main__ -   Batch number = 76
11/28/2021 01:38:54 - INFO - __main__ -   Batch number = 77
11/28/2021 01:38:55 - INFO - __main__ -   Batch number = 78
11/28/2021 01:38:55 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='is', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:38:55 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:38:55 - INFO - __main__ -   Seed = 3
11/28/2021 01:38:55 - INFO - root -   save model
11/28/2021 01:38:55 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='is', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:38:55 - INFO - __main__ -   Loading pretrained model and tokenizer
11/28/2021 01:38:56 - INFO - __main__ -   Batch number = 79
11/28/2021 01:38:56 - INFO - __main__ -   Batch number = 80
11/28/2021 01:38:57 - INFO - __main__ -   Batch number = 81
11/28/2021 01:38:58 - INFO - __main__ -   Batch number = 82
11/28/2021 01:38:58 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
11/28/2021 01:38:58 - INFO - __main__ -   Batch number = 83
11/28/2021 01:38:59 - INFO - __main__ -   Batch number = 84
11/28/2021 01:38:59 - INFO - __main__ -   Batch number = 85
11/28/2021 01:39:00 - INFO - __main__ -   Batch number = 86
11/28/2021 01:39:01 - INFO - __main__ -   Batch number = 87
11/28/2021 01:39:01 - INFO - __main__ -   Batch number = 88
11/28/2021 01:39:02 - INFO - __main__ -   Batch number = 89
11/28/2021 01:39:03 - INFO - __main__ -   Batch number = 90
11/28/2021 01:39:03 - INFO - __main__ -   Batch number = 91
11/28/2021 01:39:04 - INFO - __main__ -   Batch number = 92
11/28/2021 01:39:05 - INFO - __main__ -   Batch number = 93
11/28/2021 01:39:05 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:39:05 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:39:05 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
11/28/2021 01:39:05 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:39:05 - INFO - root -   loading task adapter
11/28/2021 01:39:05 - INFO - root -   loading lang adpater bn/wiki@ukp
11/28/2021 01:39:05 - INFO - __main__ -   Adapter Languages : ['bn'], Length : 1
11/28/2021 01:39:05 - INFO - __main__ -   Adapter Names ['bn/wiki@ukp'], Length : 1
11/28/2021 01:39:05 - INFO - __main__ -   Language = bn
11/28/2021 01:39:05 - INFO - __main__ -   Adapter Name = bn/wiki@ukp
11/28/2021 01:39:05 - INFO - __main__ -   Batch number = 94
11/28/2021 01:39:06 - INFO - __main__ -   Batch number = 95
11/28/2021 01:39:06 - INFO - __main__ -   Batch number = 96
11/28/2021 01:39:07 - INFO - __main__ -   Batch number = 97
11/28/2021 01:39:08 - INFO - __main__ -   Batch number = 98
11/28/2021 01:39:08 - INFO - __main__ -   Batch number = 99
11/28/2021 01:39:09 - INFO - __main__ -   Batch number = 100
11/28/2021 01:39:10 - INFO - __main__ -   Batch number = 101
11/28/2021 01:39:10 - INFO - __main__ -   Batch number = 102
11/28/2021 01:39:11 - INFO - __main__ -   Batch number = 103
11/28/2021 01:39:12 - INFO - __main__ -   Batch number = 104
11/28/2021 01:39:12 - INFO - __main__ -   Batch number = 105
11/28/2021 01:39:13 - INFO - __main__ -   Batch number = 106
11/28/2021 01:39:14 - INFO - __main__ -   Batch number = 107
11/28/2021 01:39:14 - INFO - __main__ -   Batch number = 108
11/28/2021 01:39:15 - INFO - __main__ -   Batch number = 109
11/28/2021 01:39:17 - INFO - __main__ -   ***** Evaluation result  in zh *****
11/28/2021 01:39:17 - INFO - __main__ -     f1 = 0.6134583037248253
11/28/2021 01:39:17 - INFO - __main__ -     loss = 1.3601835725504323
11/28/2021 01:39:17 - INFO - __main__ -     precision = 0.6215708791308945
11/28/2021 01:39:17 - INFO - __main__ -     recall = 0.6055547663020315
11/28/2021 01:39:17 - INFO - __main__ -   Language adapter for is not found, using bn instead
11/28/2021 01:39:17 - INFO - __main__ -   Set active language adapter to bn
11/28/2021 01:39:17 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:39:17 - INFO - __main__ -   Adapter Languages = ['bn']
11/28/2021 01:39:17 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_is_bert-base-multilingual-cased_128
11/28/2021 01:39:18 - INFO - __main__ -   ***** Running evaluation  in is *****
11/28/2021 01:39:18 - INFO - __main__ -     Num examples = 6401
11/28/2021 01:39:18 - INFO - __main__ -     Batch size = 32
11/28/2021 01:39:18 - INFO - __main__ -   Batch number = 1
11/28/2021 01:39:19 - INFO - __main__ -   Batch number = 2
11/28/2021 01:39:19 - INFO - __main__ -   Batch number = 3
11/28/2021 01:39:20 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='zh', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:39:20 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:39:20 - INFO - __main__ -   Seed = 3
11/28/2021 01:39:20 - INFO - root -   save model
11/28/2021 01:39:20 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='zh', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:39:20 - INFO - __main__ -   Loading pretrained model and tokenizer
11/28/2021 01:39:20 - INFO - __main__ -   Batch number = 4
11/28/2021 01:39:21 - INFO - __main__ -   Batch number = 5
11/28/2021 01:39:21 - INFO - __main__ -   Batch number = 6
11/28/2021 01:39:22 - INFO - __main__ -   Batch number = 7
11/28/2021 01:39:22 - INFO - __main__ -   Batch number = 8
11/28/2021 01:39:22 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
11/28/2021 01:39:23 - INFO - __main__ -   Batch number = 9
11/28/2021 01:39:24 - INFO - __main__ -   Batch number = 10
11/28/2021 01:39:24 - INFO - __main__ -   Batch number = 11
11/28/2021 01:39:25 - INFO - __main__ -   Batch number = 12
11/28/2021 01:39:25 - INFO - __main__ -   Batch number = 13
11/28/2021 01:39:26 - INFO - __main__ -   Batch number = 14
11/28/2021 01:39:26 - INFO - __main__ -   Batch number = 15
11/28/2021 01:39:27 - INFO - __main__ -   Batch number = 16
11/28/2021 01:39:28 - INFO - __main__ -   Batch number = 17
11/28/2021 01:39:28 - INFO - __main__ -   Batch number = 18
11/28/2021 01:39:29 - INFO - __main__ -   Batch number = 19
11/28/2021 01:39:29 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:39:29 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:39:29 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
11/28/2021 01:39:29 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:39:29 - INFO - root -   loading task adapter
11/28/2021 01:39:29 - INFO - root -   loading lang adpater bn/wiki@ukp
11/28/2021 01:39:29 - INFO - __main__ -   Adapter Languages : ['bn'], Length : 1
11/28/2021 01:39:29 - INFO - __main__ -   Adapter Names ['bn/wiki@ukp'], Length : 1
11/28/2021 01:39:29 - INFO - __main__ -   Language = bn
11/28/2021 01:39:29 - INFO - __main__ -   Adapter Name = bn/wiki@ukp
11/28/2021 01:39:29 - INFO - __main__ -   Batch number = 20
11/28/2021 01:39:30 - INFO - __main__ -   Batch number = 21
11/28/2021 01:39:30 - INFO - __main__ -   Batch number = 22
11/28/2021 01:39:31 - INFO - __main__ -   Batch number = 23
11/28/2021 01:39:31 - INFO - __main__ -   Batch number = 24
11/28/2021 01:39:31 - INFO - __main__ -   Batch number = 25
11/28/2021 01:39:32 - INFO - __main__ -   Batch number = 26
11/28/2021 01:39:32 - INFO - __main__ -   Batch number = 27
11/28/2021 01:39:32 - INFO - __main__ -   Batch number = 28
11/28/2021 01:39:33 - INFO - __main__ -   Batch number = 29
11/28/2021 01:39:33 - INFO - __main__ -   Batch number = 30
11/28/2021 01:39:33 - INFO - __main__ -   Batch number = 31
11/28/2021 01:39:34 - INFO - __main__ -   Batch number = 32
11/28/2021 01:39:34 - INFO - __main__ -   Batch number = 33
11/28/2021 01:39:34 - INFO - __main__ -   Batch number = 34
11/28/2021 01:39:34 - INFO - __main__ -   Batch number = 35
11/28/2021 01:39:35 - INFO - __main__ -   Batch number = 36
11/28/2021 01:39:35 - INFO - __main__ -   Batch number = 37
11/28/2021 01:39:35 - INFO - __main__ -   Batch number = 38
11/28/2021 01:39:35 - INFO - __main__ -   Batch number = 39
11/28/2021 01:39:35 - INFO - __main__ -   Batch number = 40
11/28/2021 01:39:35 - INFO - __main__ -   Batch number = 41
11/28/2021 01:39:36 - INFO - __main__ -   Batch number = 42
11/28/2021 01:39:36 - INFO - __main__ -   Batch number = 43
11/28/2021 01:39:36 - INFO - __main__ -   Batch number = 44
11/28/2021 01:39:36 - INFO - __main__ -   Batch number = 45
11/28/2021 01:39:36 - INFO - __main__ -   Batch number = 46
11/28/2021 01:39:36 - INFO - __main__ -   Batch number = 47
11/28/2021 01:39:37 - INFO - __main__ -   Batch number = 48
11/28/2021 01:39:37 - INFO - __main__ -   Batch number = 49
11/28/2021 01:39:37 - INFO - __main__ -   Batch number = 50
11/28/2021 01:39:37 - INFO - __main__ -   Batch number = 51
11/28/2021 01:39:37 - INFO - __main__ -   Batch number = 52
11/28/2021 01:39:37 - INFO - __main__ -   Batch number = 53
11/28/2021 01:39:37 - INFO - __main__ -   Batch number = 54
11/28/2021 01:39:38 - INFO - __main__ -   Batch number = 55
11/28/2021 01:39:38 - INFO - __main__ -   Batch number = 56
11/28/2021 01:39:38 - INFO - __main__ -   Batch number = 57
11/28/2021 01:39:38 - INFO - __main__ -   Batch number = 58
11/28/2021 01:39:38 - INFO - __main__ -   Batch number = 59
11/28/2021 01:39:38 - INFO - __main__ -   Batch number = 60
11/28/2021 01:39:39 - INFO - __main__ -   Batch number = 61
11/28/2021 01:39:39 - INFO - __main__ -   Batch number = 62
11/28/2021 01:39:39 - INFO - __main__ -   Batch number = 63
11/28/2021 01:39:39 - INFO - __main__ -   Batch number = 64
11/28/2021 01:39:39 - INFO - __main__ -   Batch number = 65
11/28/2021 01:39:39 - INFO - __main__ -   Batch number = 66
11/28/2021 01:39:40 - INFO - __main__ -   Batch number = 67
11/28/2021 01:39:40 - INFO - __main__ -   Batch number = 68
11/28/2021 01:39:40 - INFO - __main__ -   Batch number = 69
11/28/2021 01:39:40 - INFO - __main__ -   Batch number = 70
11/28/2021 01:39:40 - INFO - __main__ -   Batch number = 71
11/28/2021 01:39:40 - INFO - __main__ -   Batch number = 72
11/28/2021 01:39:41 - INFO - __main__ -   Batch number = 73
11/28/2021 01:39:41 - INFO - __main__ -   Batch number = 74
11/28/2021 01:39:41 - INFO - __main__ -   Batch number = 75
11/28/2021 01:39:41 - INFO - __main__ -   Batch number = 76
11/28/2021 01:39:41 - INFO - __main__ -   Language adapter for zh not found, using bn instead
11/28/2021 01:39:41 - INFO - __main__ -   Set active language adapter to bn
11/28/2021 01:39:41 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:39:41 - INFO - __main__ -   Adapter Languages = ['bn']
11/28/2021 01:39:41 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_zh_bert-base-multilingual-cased_128
11/28/2021 01:39:41 - INFO - __main__ -   Batch number = 77
11/28/2021 01:39:41 - INFO - __main__ -   Batch number = 78
11/28/2021 01:39:41 - INFO - __main__ -   Batch number = 79
11/28/2021 01:39:42 - INFO - __main__ -   Batch number = 80
11/28/2021 01:39:42 - INFO - __main__ -   Batch number = 81
11/28/2021 01:39:42 - INFO - __main__ -   ***** Running evaluation  in zh *****
11/28/2021 01:39:42 - INFO - __main__ -     Num examples = 3458
11/28/2021 01:39:42 - INFO - __main__ -     Batch size = 32
11/28/2021 01:39:42 - INFO - __main__ -   Batch number = 1
11/28/2021 01:39:42 - INFO - __main__ -   Batch number = 82
11/28/2021 01:39:42 - INFO - __main__ -   Batch number = 83
11/28/2021 01:39:42 - INFO - __main__ -   Batch number = 2
11/28/2021 01:39:42 - INFO - __main__ -   Batch number = 84
11/28/2021 01:39:42 - INFO - __main__ -   Batch number = 85
11/28/2021 01:39:42 - INFO - __main__ -   Batch number = 3
11/28/2021 01:39:43 - INFO - __main__ -   Batch number = 86
11/28/2021 01:39:43 - INFO - __main__ -   Batch number = 87
11/28/2021 01:39:43 - INFO - __main__ -   Batch number = 4
11/28/2021 01:39:43 - INFO - __main__ -   Batch number = 88
11/28/2021 01:39:43 - INFO - __main__ -   Batch number = 89
11/28/2021 01:39:43 - INFO - __main__ -   Batch number = 5
11/28/2021 01:39:43 - INFO - __main__ -   Batch number = 90
11/28/2021 01:39:43 - INFO - __main__ -   Batch number = 91
11/28/2021 01:39:43 - INFO - __main__ -   Batch number = 6
11/28/2021 01:39:43 - INFO - __main__ -   Batch number = 92
11/28/2021 01:39:44 - INFO - __main__ -   Batch number = 93
11/28/2021 01:39:44 - INFO - __main__ -   Batch number = 7
11/28/2021 01:39:44 - INFO - __main__ -   Batch number = 94
11/28/2021 01:39:44 - INFO - __main__ -   Batch number = 95
11/28/2021 01:39:44 - INFO - __main__ -   Batch number = 8
11/28/2021 01:39:44 - INFO - __main__ -   Batch number = 96
11/28/2021 01:39:44 - INFO - __main__ -   Batch number = 97
11/28/2021 01:39:44 - INFO - __main__ -   Batch number = 9
11/28/2021 01:39:44 - INFO - __main__ -   Batch number = 98
11/28/2021 01:39:44 - INFO - __main__ -   Batch number = 99
11/28/2021 01:39:45 - INFO - __main__ -   Batch number = 10
11/28/2021 01:39:45 - INFO - __main__ -   Batch number = 100
11/28/2021 01:39:45 - INFO - __main__ -   Batch number = 11
11/28/2021 01:39:45 - INFO - __main__ -   Batch number = 101
11/28/2021 01:39:45 - INFO - __main__ -   Batch number = 102
11/28/2021 01:39:45 - INFO - __main__ -   Batch number = 103
11/28/2021 01:39:45 - INFO - __main__ -   Batch number = 12
11/28/2021 01:39:45 - INFO - __main__ -   Batch number = 104
11/28/2021 01:39:45 - INFO - __main__ -   Batch number = 13
11/28/2021 01:39:45 - INFO - __main__ -   Batch number = 105
11/28/2021 01:39:46 - INFO - __main__ -   Batch number = 106
11/28/2021 01:39:46 - INFO - __main__ -   Batch number = 107
11/28/2021 01:39:46 - INFO - __main__ -   Batch number = 14
11/28/2021 01:39:46 - INFO - __main__ -   Batch number = 108
11/28/2021 01:39:46 - INFO - __main__ -   Batch number = 15
11/28/2021 01:39:46 - INFO - __main__ -   Batch number = 109
11/28/2021 01:39:46 - INFO - __main__ -   Batch number = 110
11/28/2021 01:39:46 - INFO - __main__ -   Batch number = 16
11/28/2021 01:39:46 - INFO - __main__ -   Batch number = 111
11/28/2021 01:39:47 - INFO - __main__ -   Batch number = 112
11/28/2021 01:39:47 - INFO - __main__ -   Batch number = 17
11/28/2021 01:39:47 - INFO - __main__ -   Batch number = 113
11/28/2021 01:39:47 - INFO - __main__ -   Batch number = 114
11/28/2021 01:39:47 - INFO - __main__ -   Batch number = 18
11/28/2021 01:39:47 - INFO - __main__ -   Batch number = 115
11/28/2021 01:39:47 - INFO - __main__ -   Batch number = 116
11/28/2021 01:39:47 - INFO - __main__ -   Batch number = 19
11/28/2021 01:39:47 - INFO - __main__ -   Batch number = 117
11/28/2021 01:39:48 - INFO - __main__ -   Batch number = 118
11/28/2021 01:39:48 - INFO - __main__ -   Batch number = 20
11/28/2021 01:39:48 - INFO - __main__ -   Batch number = 119
11/28/2021 01:39:48 - INFO - __main__ -   Batch number = 120
11/28/2021 01:39:48 - INFO - __main__ -   Batch number = 21
11/28/2021 01:39:48 - INFO - __main__ -   Batch number = 121
11/28/2021 01:39:48 - INFO - __main__ -   Batch number = 122
11/28/2021 01:39:48 - INFO - __main__ -   Batch number = 22
11/28/2021 01:39:48 - INFO - __main__ -   Batch number = 123
11/28/2021 01:39:49 - INFO - __main__ -   Batch number = 124
11/28/2021 01:39:49 - INFO - __main__ -   Batch number = 23
11/28/2021 01:39:49 - INFO - __main__ -   Batch number = 125
11/28/2021 01:39:49 - INFO - __main__ -   Batch number = 126
11/28/2021 01:39:49 - INFO - __main__ -   Batch number = 24
11/28/2021 01:39:49 - INFO - __main__ -   Batch number = 127
11/28/2021 01:39:49 - INFO - __main__ -   Batch number = 128
11/28/2021 01:39:49 - INFO - __main__ -   Batch number = 25
11/28/2021 01:39:49 - INFO - __main__ -   Batch number = 129
11/28/2021 01:39:50 - INFO - __main__ -   Batch number = 26
11/28/2021 01:39:50 - INFO - __main__ -   Batch number = 130
11/28/2021 01:39:50 - INFO - __main__ -   Batch number = 27
11/28/2021 01:39:50 - INFO - __main__ -   Batch number = 131
11/28/2021 01:39:50 - INFO - __main__ -   Batch number = 132
11/28/2021 01:39:50 - INFO - __main__ -   Batch number = 28
11/28/2021 01:39:50 - INFO - __main__ -   Batch number = 133
11/28/2021 01:39:50 - INFO - __main__ -   Batch number = 29
11/28/2021 01:39:50 - INFO - __main__ -   Batch number = 134
11/28/2021 01:39:51 - INFO - __main__ -   Batch number = 135
11/28/2021 01:39:51 - INFO - __main__ -   Batch number = 30
11/28/2021 01:39:51 - INFO - __main__ -   Batch number = 136
11/28/2021 01:39:51 - INFO - __main__ -   Batch number = 137
11/28/2021 01:39:51 - INFO - __main__ -   Batch number = 31
11/28/2021 01:39:51 - INFO - __main__ -   Batch number = 138
11/28/2021 01:39:51 - INFO - __main__ -   Batch number = 139
11/28/2021 01:39:52 - INFO - __main__ -   Batch number = 140
11/28/2021 01:39:52 - INFO - __main__ -   Batch number = 32
11/28/2021 01:39:52 - INFO - __main__ -   Batch number = 33
11/28/2021 01:39:52 - INFO - __main__ -   Batch number = 34
11/28/2021 01:39:53 - INFO - __main__ -   Batch number = 35
11/28/2021 01:39:53 - INFO - __main__ -   Batch number = 141
11/28/2021 01:39:53 - INFO - __main__ -   Batch number = 36
11/28/2021 01:39:53 - INFO - __main__ -   Batch number = 142
11/28/2021 01:39:54 - INFO - __main__ -   Batch number = 37
11/28/2021 01:39:54 - INFO - __main__ -   Batch number = 143
11/28/2021 01:39:54 - INFO - __main__ -   Batch number = 38
11/28/2021 01:39:54 - INFO - __main__ -   Batch number = 144
11/28/2021 01:39:54 - INFO - __main__ -   Batch number = 39
11/28/2021 01:39:55 - INFO - __main__ -   Batch number = 40
11/28/2021 01:39:55 - INFO - __main__ -   Batch number = 145
11/28/2021 01:39:55 - INFO - __main__ -   Batch number = 146
11/28/2021 01:39:55 - INFO - __main__ -   Batch number = 41
11/28/2021 01:39:55 - INFO - __main__ -   Batch number = 147
11/28/2021 01:39:56 - INFO - __main__ -   Batch number = 42
11/28/2021 01:39:56 - INFO - __main__ -   Batch number = 148
11/28/2021 01:39:56 - INFO - __main__ -   Batch number = 43
11/28/2021 01:39:56 - INFO - __main__ -   Batch number = 149
11/28/2021 01:39:57 - INFO - __main__ -   Batch number = 150
11/28/2021 01:39:57 - INFO - __main__ -   Batch number = 44
11/28/2021 01:39:57 - INFO - __main__ -   Batch number = 45
11/28/2021 01:39:58 - INFO - __main__ -   Batch number = 151
11/28/2021 01:39:58 - INFO - __main__ -   Batch number = 46
11/28/2021 01:39:58 - INFO - __main__ -   Batch number = 152
11/28/2021 01:39:58 - INFO - __main__ -   Batch number = 47
11/28/2021 01:39:58 - INFO - __main__ -   Batch number = 153
11/28/2021 01:39:59 - INFO - __main__ -   Batch number = 154
11/28/2021 01:39:59 - INFO - __main__ -   Batch number = 48
11/28/2021 01:39:59 - INFO - __main__ -   Batch number = 155
11/28/2021 01:39:59 - INFO - __main__ -   Batch number = 49
11/28/2021 01:40:00 - INFO - __main__ -   Batch number = 156
11/28/2021 01:40:00 - INFO - __main__ -   Batch number = 50
11/28/2021 01:40:00 - INFO - __main__ -   Batch number = 157
11/28/2021 01:40:01 - INFO - __main__ -   Batch number = 51
11/28/2021 01:40:01 - INFO - __main__ -   Batch number = 158
11/28/2021 01:40:01 - INFO - __main__ -   Batch number = 52
11/28/2021 01:40:01 - INFO - __main__ -   Batch number = 159
11/28/2021 01:40:02 - INFO - __main__ -   Batch number = 53
11/28/2021 01:40:02 - INFO - __main__ -   Batch number = 54
11/28/2021 01:40:03 - INFO - __main__ -   Batch number = 55
11/28/2021 01:40:03 - INFO - __main__ -   Batch number = 160
11/28/2021 01:40:03 - INFO - __main__ -   Batch number = 56
11/28/2021 01:40:03 - INFO - __main__ -   Batch number = 161
11/28/2021 01:40:04 - INFO - __main__ -   Batch number = 57
11/28/2021 01:40:04 - INFO - __main__ -   Batch number = 162
11/28/2021 01:40:04 - INFO - __main__ -   Batch number = 58
11/28/2021 01:40:05 - INFO - __main__ -   Batch number = 163
11/28/2021 01:40:05 - INFO - __main__ -   Batch number = 59
11/28/2021 01:40:05 - INFO - __main__ -   Batch number = 60
11/28/2021 01:40:05 - INFO - __main__ -   Batch number = 164
11/28/2021 01:40:06 - INFO - __main__ -   Batch number = 61
11/28/2021 01:40:06 - INFO - __main__ -   Batch number = 62
11/28/2021 01:40:07 - INFO - __main__ -   Batch number = 63
11/28/2021 01:40:07 - INFO - __main__ -   Batch number = 64
11/28/2021 01:40:08 - INFO - __main__ -   Batch number = 65
11/28/2021 01:40:08 - INFO - __main__ -   Batch number = 66
11/28/2021 01:40:08 - INFO - __main__ -   Batch number = 165
11/28/2021 01:40:09 - INFO - __main__ -   Batch number = 67
11/28/2021 01:40:09 - INFO - __main__ -   Batch number = 166
11/28/2021 01:40:09 - INFO - __main__ -   Batch number = 68
11/28/2021 01:40:10 - INFO - __main__ -   Batch number = 69
11/28/2021 01:40:10 - INFO - __main__ -   Batch number = 167
11/28/2021 01:40:11 - INFO - __main__ -   Batch number = 70
11/28/2021 01:40:11 - INFO - __main__ -   Batch number = 168
11/28/2021 01:40:11 - INFO - __main__ -   Batch number = 71
11/28/2021 01:40:11 - INFO - __main__ -   Batch number = 169
11/28/2021 01:40:12 - INFO - __main__ -   Batch number = 72
11/28/2021 01:40:12 - INFO - __main__ -   Batch number = 73
11/28/2021 01:40:12 - INFO - __main__ -   Batch number = 170
11/28/2021 01:40:13 - INFO - __main__ -   Batch number = 74
11/28/2021 01:40:13 - INFO - __main__ -   Batch number = 171
11/28/2021 01:40:13 - INFO - __main__ -   Batch number = 75
11/28/2021 01:40:13 - INFO - __main__ -   Batch number = 172
11/28/2021 01:40:14 - INFO - __main__ -   Batch number = 76
11/28/2021 01:40:14 - INFO - __main__ -   Batch number = 77
11/28/2021 01:40:14 - INFO - __main__ -   Batch number = 173
11/28/2021 01:40:15 - INFO - __main__ -   Batch number = 78
11/28/2021 01:40:15 - INFO - __main__ -   Batch number = 174
11/28/2021 01:40:15 - INFO - __main__ -   Batch number = 79
11/28/2021 01:40:15 - INFO - __main__ -   Batch number = 175
11/28/2021 01:40:16 - INFO - __main__ -   Batch number = 80
11/28/2021 01:40:16 - INFO - __main__ -   Batch number = 81
11/28/2021 01:40:16 - INFO - __main__ -   Batch number = 176
11/28/2021 01:40:17 - INFO - __main__ -   Batch number = 82
11/28/2021 01:40:17 - INFO - __main__ -   Batch number = 177
11/28/2021 01:40:17 - INFO - __main__ -   Batch number = 83
11/28/2021 01:40:17 - INFO - __main__ -   Batch number = 178
11/28/2021 01:40:18 - INFO - __main__ -   Batch number = 84
11/28/2021 01:40:18 - INFO - __main__ -   Batch number = 179
11/28/2021 01:40:18 - INFO - __main__ -   Batch number = 85
11/28/2021 01:40:18 - INFO - __main__ -   Batch number = 180
11/28/2021 01:40:19 - INFO - __main__ -   Batch number = 86
11/28/2021 01:40:19 - INFO - __main__ -   Batch number = 87
11/28/2021 01:40:19 - INFO - __main__ -   Batch number = 181
11/28/2021 01:40:20 - INFO - __main__ -   Batch number = 88
11/28/2021 01:40:20 - INFO - __main__ -   Batch number = 182
11/28/2021 01:40:20 - INFO - __main__ -   Batch number = 89
11/28/2021 01:40:20 - INFO - __main__ -   Batch number = 183
11/28/2021 01:40:20 - INFO - __main__ -   Batch number = 90
11/28/2021 01:40:21 - INFO - __main__ -   Batch number = 184
11/28/2021 01:40:21 - INFO - __main__ -   Batch number = 91
11/28/2021 01:40:21 - INFO - __main__ -   Batch number = 92
11/28/2021 01:40:22 - INFO - __main__ -   Batch number = 185
11/28/2021 01:40:22 - INFO - __main__ -   Batch number = 93
11/28/2021 01:40:22 - INFO - __main__ -   Batch number = 186
11/28/2021 01:40:22 - INFO - __main__ -   Batch number = 94
11/28/2021 01:40:23 - INFO - __main__ -   Batch number = 187
11/28/2021 01:40:23 - INFO - __main__ -   Batch number = 95
11/28/2021 01:40:23 - INFO - __main__ -   Batch number = 188
11/28/2021 01:40:23 - INFO - __main__ -   Batch number = 96
11/28/2021 01:40:24 - INFO - __main__ -   Batch number = 189
11/28/2021 01:40:24 - INFO - __main__ -   Batch number = 97
11/28/2021 01:40:24 - INFO - __main__ -   Batch number = 190
11/28/2021 01:40:24 - INFO - __main__ -   Batch number = 98
11/28/2021 01:40:25 - INFO - __main__ -   Batch number = 191
11/28/2021 01:40:25 - INFO - __main__ -   Batch number = 99
11/28/2021 01:40:25 - INFO - __main__ -   Batch number = 192
11/28/2021 01:40:25 - INFO - __main__ -   Batch number = 100
11/28/2021 01:40:26 - INFO - __main__ -   Batch number = 101
11/28/2021 01:40:26 - INFO - __main__ -   Batch number = 193
11/28/2021 01:40:26 - INFO - __main__ -   Batch number = 102
11/28/2021 01:40:27 - INFO - __main__ -   Batch number = 194
11/28/2021 01:40:27 - INFO - __main__ -   Batch number = 103
11/28/2021 01:40:27 - INFO - __main__ -   Batch number = 195
11/28/2021 01:40:27 - INFO - __main__ -   Batch number = 104
11/28/2021 01:40:28 - INFO - __main__ -   Batch number = 196
11/28/2021 01:40:28 - INFO - __main__ -   Batch number = 105
11/28/2021 01:40:28 - INFO - __main__ -   Batch number = 197
11/28/2021 01:40:28 - INFO - __main__ -   Batch number = 106
11/28/2021 01:40:29 - INFO - __main__ -   Batch number = 198
11/28/2021 01:40:29 - INFO - __main__ -   Batch number = 107
11/28/2021 01:40:29 - INFO - __main__ -   Batch number = 199
11/28/2021 01:40:29 - INFO - __main__ -   Batch number = 108
11/28/2021 01:40:30 - INFO - __main__ -   Batch number = 200
11/28/2021 01:40:30 - INFO - __main__ -   Batch number = 109
11/28/2021 01:40:30 - INFO - __main__ -   Batch number = 201
11/28/2021 01:40:32 - INFO - __main__ -   ***** Evaluation result  in zh *****
11/28/2021 01:40:32 - INFO - __main__ -     f1 = 0.6169134524506052
11/28/2021 01:40:32 - INFO - __main__ -     loss = 1.416018894506157
11/28/2021 01:40:32 - INFO - __main__ -     precision = 0.6255225161090119
11/28/2021 01:40:32 - INFO - __main__ -     recall = 0.6085381446228157
11/28/2021 01:40:34 - INFO - __main__ -   ***** Evaluation result  in is *****
11/28/2021 01:40:34 - INFO - __main__ -     f1 = 0.7374850139869454
11/28/2021 01:40:34 - INFO - __main__ -     loss = 0.8852148607595643
11/28/2021 01:40:34 - INFO - __main__ -     precision = 0.7426042050903725
11/28/2021 01:40:34 - INFO - __main__ -     recall = 0.7324359186373408
11/28/2021 01:41:43 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:41:43 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:41:43 - INFO - __main__ -   Seed = 1
11/28/2021 01:41:43 - INFO - root -   save model
11/28/2021 01:41:43 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:41:43 - INFO - __main__ -   Loading pretrained model and tokenizer
11/28/2021 01:41:46 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
11/28/2021 01:41:52 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:41:52 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:41:52 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
11/28/2021 01:41:52 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:41:52 - INFO - root -   loading task adapter
11/28/2021 01:41:52 - INFO - root -   loading lang adpater bn/wiki@ukp
11/28/2021 01:41:52 - INFO - __main__ -   Adapter Languages : ['bn'], Length : 1
11/28/2021 01:41:52 - INFO - __main__ -   Adapter Names ['bn/wiki@ukp'], Length : 1
11/28/2021 01:41:52 - INFO - __main__ -   Language = bn
11/28/2021 01:41:52 - INFO - __main__ -   Adapter Name = bn/wiki@ukp
11/28/2021 01:42:02 - INFO - __main__ -   Language adapter for fi not found, using bn instead
11/28/2021 01:42:02 - INFO - __main__ -   Set active language adapter to bn
11/28/2021 01:42:02 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:42:02 - INFO - __main__ -   Adapter Languages = ['bn']
11/28/2021 01:42:02 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_fi_bert-base-multilingual-cased_128
11/28/2021 01:42:04 - INFO - __main__ -   ***** Running evaluation  in fi *****
11/28/2021 01:42:04 - INFO - __main__ -     Num examples = 6550
11/28/2021 01:42:04 - INFO - __main__ -     Batch size = 32
11/28/2021 01:42:04 - INFO - __main__ -   Batch number = 1
11/28/2021 01:42:04 - INFO - __main__ -   Batch number = 2
11/28/2021 01:42:04 - INFO - __main__ -   Batch number = 3
11/28/2021 01:42:04 - INFO - __main__ -   Batch number = 4
11/28/2021 01:42:04 - INFO - __main__ -   Batch number = 5
11/28/2021 01:42:05 - INFO - __main__ -   Batch number = 6
11/28/2021 01:42:05 - INFO - __main__ -   Batch number = 7
11/28/2021 01:42:05 - INFO - __main__ -   Batch number = 8
11/28/2021 01:42:05 - INFO - __main__ -   Batch number = 9
11/28/2021 01:42:06 - INFO - __main__ -   Batch number = 10
11/28/2021 01:42:06 - INFO - __main__ -   Batch number = 11
11/28/2021 01:42:06 - INFO - __main__ -   Batch number = 12
11/28/2021 01:42:06 - INFO - __main__ -   Batch number = 13
11/28/2021 01:42:06 - INFO - __main__ -   Batch number = 14
11/28/2021 01:42:06 - INFO - __main__ -   Batch number = 15
11/28/2021 01:42:07 - INFO - __main__ -   Batch number = 16
11/28/2021 01:42:07 - INFO - __main__ -   Batch number = 17
11/28/2021 01:42:07 - INFO - __main__ -   Batch number = 18
11/28/2021 01:42:07 - INFO - __main__ -   Batch number = 19
11/28/2021 01:42:08 - INFO - __main__ -   Batch number = 20
11/28/2021 01:42:08 - INFO - __main__ -   Batch number = 21
11/28/2021 01:42:08 - INFO - __main__ -   Batch number = 22
11/28/2021 01:42:09 - INFO - __main__ -   Batch number = 23
11/28/2021 01:42:09 - INFO - __main__ -   Batch number = 24
11/28/2021 01:42:09 - INFO - __main__ -   Batch number = 25
11/28/2021 01:42:10 - INFO - __main__ -   Batch number = 26
11/28/2021 01:42:10 - INFO - __main__ -   Batch number = 27
11/28/2021 01:42:10 - INFO - __main__ -   Batch number = 28
11/28/2021 01:42:11 - INFO - __main__ -   Batch number = 29
11/28/2021 01:42:11 - INFO - __main__ -   Batch number = 30
11/28/2021 01:42:11 - INFO - __main__ -   Batch number = 31
11/28/2021 01:42:12 - INFO - __main__ -   Batch number = 32
11/28/2021 01:42:12 - INFO - __main__ -   Batch number = 33
11/28/2021 01:42:12 - INFO - __main__ -   Batch number = 34
11/28/2021 01:42:13 - INFO - __main__ -   Batch number = 35
11/28/2021 01:42:13 - INFO - __main__ -   Batch number = 36
11/28/2021 01:42:13 - INFO - __main__ -   Batch number = 37
11/28/2021 01:42:13 - INFO - __main__ -   Batch number = 38
11/28/2021 01:42:14 - INFO - __main__ -   Batch number = 39
11/28/2021 01:42:14 - INFO - __main__ -   Batch number = 40
11/28/2021 01:42:14 - INFO - __main__ -   Batch number = 41
11/28/2021 01:42:14 - INFO - __main__ -   Batch number = 42
11/28/2021 01:42:14 - INFO - __main__ -   Batch number = 43
11/28/2021 01:42:15 - INFO - __main__ -   Batch number = 44
11/28/2021 01:42:15 - INFO - __main__ -   Batch number = 45
11/28/2021 01:42:16 - INFO - __main__ -   Batch number = 46
11/28/2021 01:42:16 - INFO - __main__ -   Batch number = 47
11/28/2021 01:42:16 - INFO - __main__ -   Batch number = 48
11/28/2021 01:42:17 - INFO - __main__ -   Batch number = 49
11/28/2021 01:42:17 - INFO - __main__ -   Batch number = 50
11/28/2021 01:42:18 - INFO - __main__ -   Batch number = 51
11/28/2021 01:42:18 - INFO - __main__ -   Batch number = 52
11/28/2021 01:42:19 - INFO - __main__ -   Batch number = 53
11/28/2021 01:42:19 - INFO - __main__ -   Batch number = 54
11/28/2021 01:42:20 - INFO - __main__ -   Batch number = 55
11/28/2021 01:42:20 - INFO - __main__ -   Batch number = 56
11/28/2021 01:42:21 - INFO - __main__ -   Batch number = 57
11/28/2021 01:42:21 - INFO - __main__ -   Batch number = 58
11/28/2021 01:42:22 - INFO - __main__ -   Batch number = 59
11/28/2021 01:42:22 - INFO - __main__ -   Batch number = 60
11/28/2021 01:42:23 - INFO - __main__ -   Batch number = 61
11/28/2021 01:42:23 - INFO - __main__ -   Batch number = 62
11/28/2021 01:42:24 - INFO - __main__ -   Batch number = 63
11/28/2021 01:42:24 - INFO - __main__ -   Batch number = 64
11/28/2021 01:42:25 - INFO - __main__ -   Batch number = 65
11/28/2021 01:42:25 - INFO - __main__ -   Batch number = 66
11/28/2021 01:42:26 - INFO - __main__ -   Batch number = 67
11/28/2021 01:42:26 - INFO - __main__ -   Batch number = 68
11/28/2021 01:42:27 - INFO - __main__ -   Batch number = 69
11/28/2021 01:42:27 - INFO - __main__ -   Batch number = 70
11/28/2021 01:42:28 - INFO - __main__ -   Batch number = 71
11/28/2021 01:42:28 - INFO - __main__ -   Batch number = 72
11/28/2021 01:42:29 - INFO - __main__ -   Batch number = 73
11/28/2021 01:42:29 - INFO - __main__ -   Batch number = 74
11/28/2021 01:42:30 - INFO - __main__ -   Batch number = 75
11/28/2021 01:42:30 - INFO - __main__ -   Batch number = 76
11/28/2021 01:42:31 - INFO - __main__ -   Batch number = 77
11/28/2021 01:42:31 - INFO - __main__ -   Batch number = 78
11/28/2021 01:42:32 - INFO - __main__ -   Batch number = 79
11/28/2021 01:42:32 - INFO - __main__ -   Batch number = 80
11/28/2021 01:42:33 - INFO - __main__ -   Batch number = 81
11/28/2021 01:42:33 - INFO - __main__ -   Batch number = 82
11/28/2021 01:42:34 - INFO - __main__ -   Batch number = 83
11/28/2021 01:42:34 - INFO - __main__ -   Batch number = 84
11/28/2021 01:42:35 - INFO - __main__ -   Batch number = 85
11/28/2021 01:42:35 - INFO - __main__ -   Batch number = 86
11/28/2021 01:42:36 - INFO - __main__ -   Batch number = 87
11/28/2021 01:42:36 - INFO - __main__ -   Batch number = 88
11/28/2021 01:42:37 - INFO - __main__ -   Batch number = 89
11/28/2021 01:42:37 - INFO - __main__ -   Batch number = 90
11/28/2021 01:42:38 - INFO - __main__ -   Batch number = 91
11/28/2021 01:42:38 - INFO - __main__ -   Batch number = 92
11/28/2021 01:42:39 - INFO - __main__ -   Batch number = 93
11/28/2021 01:42:39 - INFO - __main__ -   Batch number = 94
11/28/2021 01:42:40 - INFO - __main__ -   Batch number = 95
11/28/2021 01:42:40 - INFO - __main__ -   Batch number = 96
11/28/2021 01:42:41 - INFO - __main__ -   Batch number = 97
11/28/2021 01:42:41 - INFO - __main__ -   Batch number = 98
11/28/2021 01:42:42 - INFO - __main__ -   Batch number = 99
11/28/2021 01:42:42 - INFO - __main__ -   Batch number = 100
11/28/2021 01:42:43 - INFO - __main__ -   Batch number = 101
11/28/2021 01:42:43 - INFO - __main__ -   Batch number = 102
11/28/2021 01:42:44 - INFO - __main__ -   Batch number = 103
11/28/2021 01:42:44 - INFO - __main__ -   Batch number = 104
11/28/2021 01:42:45 - INFO - __main__ -   Batch number = 105
11/28/2021 01:42:45 - INFO - __main__ -   Batch number = 106
11/28/2021 01:42:46 - INFO - __main__ -   Batch number = 107
11/28/2021 01:42:46 - INFO - __main__ -   Batch number = 108
11/28/2021 01:42:47 - INFO - __main__ -   Batch number = 109
11/28/2021 01:42:47 - INFO - __main__ -   Batch number = 110
11/28/2021 01:42:47 - INFO - __main__ -   Batch number = 111
11/28/2021 01:42:48 - INFO - __main__ -   Batch number = 112
11/28/2021 01:42:48 - INFO - __main__ -   Batch number = 113
11/28/2021 01:42:48 - INFO - __main__ -   Batch number = 114
11/28/2021 01:42:49 - INFO - __main__ -   Batch number = 115
11/28/2021 01:42:49 - INFO - __main__ -   Batch number = 116
11/28/2021 01:42:49 - INFO - __main__ -   Batch number = 117
11/28/2021 01:42:50 - INFO - __main__ -   Batch number = 118
11/28/2021 01:42:50 - INFO - __main__ -   Batch number = 119
11/28/2021 01:42:51 - INFO - __main__ -   Batch number = 120
11/28/2021 01:42:51 - INFO - __main__ -   Batch number = 121
11/28/2021 01:42:51 - INFO - __main__ -   Batch number = 122
11/28/2021 01:42:52 - INFO - __main__ -   Batch number = 123
11/28/2021 01:42:52 - INFO - __main__ -   Batch number = 124
11/28/2021 01:42:53 - INFO - __main__ -   Batch number = 125
11/28/2021 01:42:53 - INFO - __main__ -   Batch number = 126
11/28/2021 01:42:54 - INFO - __main__ -   Batch number = 127
11/28/2021 01:42:54 - INFO - __main__ -   Batch number = 128
11/28/2021 01:42:54 - INFO - __main__ -   Batch number = 129
11/28/2021 01:42:55 - INFO - __main__ -   Batch number = 130
11/28/2021 01:42:55 - INFO - __main__ -   Batch number = 131
11/28/2021 01:42:55 - INFO - __main__ -   Batch number = 132
11/28/2021 01:42:56 - INFO - __main__ -   Batch number = 133
11/28/2021 01:42:56 - INFO - __main__ -   Batch number = 134
11/28/2021 01:42:56 - INFO - __main__ -   Batch number = 135
11/28/2021 01:42:57 - INFO - __main__ -   Batch number = 136
11/28/2021 01:42:57 - INFO - __main__ -   Batch number = 137
11/28/2021 01:42:57 - INFO - __main__ -   Batch number = 138
11/28/2021 01:42:58 - INFO - __main__ -   Batch number = 139
11/28/2021 01:42:58 - INFO - __main__ -   Batch number = 140
11/28/2021 01:42:58 - INFO - __main__ -   Batch number = 141
11/28/2021 01:42:59 - INFO - __main__ -   Batch number = 142
11/28/2021 01:42:59 - INFO - __main__ -   Batch number = 143
11/28/2021 01:42:59 - INFO - __main__ -   Batch number = 144
11/28/2021 01:43:00 - INFO - __main__ -   Batch number = 145
11/28/2021 01:43:00 - INFO - __main__ -   Batch number = 146
11/28/2021 01:43:01 - INFO - __main__ -   Batch number = 147
11/28/2021 01:43:01 - INFO - __main__ -   Batch number = 148
11/28/2021 01:43:01 - INFO - __main__ -   Batch number = 149
11/28/2021 01:43:02 - INFO - __main__ -   Batch number = 150
11/28/2021 01:43:02 - INFO - __main__ -   Batch number = 151
11/28/2021 01:43:02 - INFO - __main__ -   Batch number = 152
11/28/2021 01:43:03 - INFO - __main__ -   Batch number = 153
11/28/2021 01:43:03 - INFO - __main__ -   Batch number = 154
11/28/2021 01:43:03 - INFO - __main__ -   Batch number = 155
11/28/2021 01:43:04 - INFO - __main__ -   Batch number = 156
11/28/2021 01:43:04 - INFO - __main__ -   Batch number = 157
11/28/2021 01:43:04 - INFO - __main__ -   Batch number = 158
11/28/2021 01:43:05 - INFO - __main__ -   Batch number = 159
11/28/2021 01:43:05 - INFO - __main__ -   Batch number = 160
11/28/2021 01:43:06 - INFO - __main__ -   Batch number = 161
11/28/2021 01:43:06 - INFO - __main__ -   Batch number = 162
11/28/2021 01:43:07 - INFO - __main__ -   Batch number = 163
11/28/2021 01:43:07 - INFO - __main__ -   Batch number = 164
11/28/2021 01:43:07 - INFO - __main__ -   Batch number = 165
11/28/2021 01:43:08 - INFO - __main__ -   Batch number = 166
11/28/2021 01:43:08 - INFO - __main__ -   Batch number = 167
11/28/2021 01:43:08 - INFO - __main__ -   Batch number = 168
11/28/2021 01:43:09 - INFO - __main__ -   Batch number = 169
11/28/2021 01:43:09 - INFO - __main__ -   Batch number = 170
11/28/2021 01:43:09 - INFO - __main__ -   Batch number = 171
11/28/2021 01:43:10 - INFO - __main__ -   Batch number = 172
11/28/2021 01:43:10 - INFO - __main__ -   Batch number = 173
11/28/2021 01:43:11 - INFO - __main__ -   Batch number = 174
11/28/2021 01:43:11 - INFO - __main__ -   Batch number = 175
11/28/2021 01:43:12 - INFO - __main__ -   Batch number = 176
11/28/2021 01:43:12 - INFO - __main__ -   Batch number = 177
11/28/2021 01:43:13 - INFO - __main__ -   Batch number = 178
11/28/2021 01:43:13 - INFO - __main__ -   Batch number = 179
11/28/2021 01:43:14 - INFO - __main__ -   Batch number = 180
11/28/2021 01:43:15 - INFO - __main__ -   Batch number = 181
11/28/2021 01:43:15 - INFO - __main__ -   Batch number = 182
11/28/2021 01:43:16 - INFO - __main__ -   Batch number = 183
11/28/2021 01:43:17 - INFO - __main__ -   Batch number = 184
11/28/2021 01:43:17 - INFO - __main__ -   Batch number = 185
11/28/2021 01:43:18 - INFO - __main__ -   Batch number = 186
11/28/2021 01:43:18 - INFO - __main__ -   Batch number = 187
11/28/2021 01:43:19 - INFO - __main__ -   Batch number = 188
11/28/2021 01:43:19 - INFO - __main__ -   Batch number = 189
11/28/2021 01:43:20 - INFO - __main__ -   Batch number = 190
11/28/2021 01:43:20 - INFO - __main__ -   Batch number = 191
11/28/2021 01:43:21 - INFO - __main__ -   Batch number = 192
11/28/2021 01:43:22 - INFO - __main__ -   Batch number = 193
11/28/2021 01:43:23 - INFO - __main__ -   Batch number = 194
11/28/2021 01:43:23 - INFO - __main__ -   Batch number = 195
11/28/2021 01:43:24 - INFO - __main__ -   Batch number = 196
11/28/2021 01:43:25 - INFO - __main__ -   Batch number = 197
11/28/2021 01:43:25 - INFO - __main__ -   Batch number = 198
11/28/2021 01:43:26 - INFO - __main__ -   Batch number = 199
11/28/2021 01:43:27 - INFO - __main__ -   Batch number = 200
11/28/2021 01:43:27 - INFO - __main__ -   Batch number = 201
11/28/2021 01:43:28 - INFO - __main__ -   Batch number = 202
11/28/2021 01:43:29 - INFO - __main__ -   Batch number = 203
11/28/2021 01:43:29 - INFO - __main__ -   Batch number = 204
11/28/2021 01:43:30 - INFO - __main__ -   Batch number = 205
11/28/2021 01:43:32 - INFO - __main__ -   ***** Evaluation result  in fi *****
11/28/2021 01:43:32 - INFO - __main__ -     f1 = 0.7432686383240913
11/28/2021 01:43:32 - INFO - __main__ -     loss = 0.9258227913844876
11/28/2021 01:43:32 - INFO - __main__ -     precision = 0.7461419514458018
11/28/2021 01:43:32 - INFO - __main__ -     recall = 0.740417369955501
11/28/2021 01:43:36 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:43:36 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:43:36 - INFO - __main__ -   Seed = 2
11/28/2021 01:43:36 - INFO - root -   save model
11/28/2021 01:43:36 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:43:36 - INFO - __main__ -   Loading pretrained model and tokenizer
11/28/2021 01:43:38 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
11/28/2021 01:43:45 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:43:45 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:43:45 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
11/28/2021 01:43:45 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:43:45 - INFO - root -   loading task adapter
11/28/2021 01:43:45 - INFO - root -   loading lang adpater bn/wiki@ukp
11/28/2021 01:43:45 - INFO - __main__ -   Adapter Languages : ['bn'], Length : 1
11/28/2021 01:43:45 - INFO - __main__ -   Adapter Names ['bn/wiki@ukp'], Length : 1
11/28/2021 01:43:45 - INFO - __main__ -   Language = bn
11/28/2021 01:43:45 - INFO - __main__ -   Adapter Name = bn/wiki@ukp
11/28/2021 01:43:55 - INFO - __main__ -   Language adapter for fi not found, using bn instead
11/28/2021 01:43:55 - INFO - __main__ -   Set active language adapter to bn
11/28/2021 01:43:55 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:43:55 - INFO - __main__ -   Adapter Languages = ['bn']
11/28/2021 01:43:55 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_fi_bert-base-multilingual-cased_128
11/28/2021 01:43:57 - INFO - __main__ -   ***** Running evaluation  in fi *****
11/28/2021 01:43:57 - INFO - __main__ -     Num examples = 6550
11/28/2021 01:43:57 - INFO - __main__ -     Batch size = 32
11/28/2021 01:43:57 - INFO - __main__ -   Batch number = 1
11/28/2021 01:43:57 - INFO - __main__ -   Batch number = 2
11/28/2021 01:43:57 - INFO - __main__ -   Batch number = 3
11/28/2021 01:43:58 - INFO - __main__ -   Batch number = 4
11/28/2021 01:43:58 - INFO - __main__ -   Batch number = 5
11/28/2021 01:43:58 - INFO - __main__ -   Batch number = 6
11/28/2021 01:43:59 - INFO - __main__ -   Batch number = 7
11/28/2021 01:43:59 - INFO - __main__ -   Batch number = 8
11/28/2021 01:43:59 - INFO - __main__ -   Batch number = 9
11/28/2021 01:44:00 - INFO - __main__ -   Batch number = 10
11/28/2021 01:44:00 - INFO - __main__ -   Batch number = 11
11/28/2021 01:44:00 - INFO - __main__ -   Batch number = 12
11/28/2021 01:44:01 - INFO - __main__ -   Batch number = 13
11/28/2021 01:44:01 - INFO - __main__ -   Batch number = 14
11/28/2021 01:44:01 - INFO - __main__ -   Batch number = 15
11/28/2021 01:44:02 - INFO - __main__ -   Batch number = 16
11/28/2021 01:44:02 - INFO - __main__ -   Batch number = 17
11/28/2021 01:44:02 - INFO - __main__ -   Batch number = 18
11/28/2021 01:44:03 - INFO - __main__ -   Batch number = 19
11/28/2021 01:44:03 - INFO - __main__ -   Batch number = 20
11/28/2021 01:44:04 - INFO - __main__ -   Batch number = 21
11/28/2021 01:44:04 - INFO - __main__ -   Batch number = 22
11/28/2021 01:44:05 - INFO - __main__ -   Batch number = 23
11/28/2021 01:44:05 - INFO - __main__ -   Batch number = 24
11/28/2021 01:44:06 - INFO - __main__ -   Batch number = 25
11/28/2021 01:44:06 - INFO - __main__ -   Batch number = 26
11/28/2021 01:44:07 - INFO - __main__ -   Batch number = 27
11/28/2021 01:44:07 - INFO - __main__ -   Batch number = 28
11/28/2021 01:44:08 - INFO - __main__ -   Batch number = 29
11/28/2021 01:44:08 - INFO - __main__ -   Batch number = 30
11/28/2021 01:44:09 - INFO - __main__ -   Batch number = 31
11/28/2021 01:44:09 - INFO - __main__ -   Batch number = 32
11/28/2021 01:44:10 - INFO - __main__ -   Batch number = 33
11/28/2021 01:44:10 - INFO - __main__ -   Batch number = 34
11/28/2021 01:44:11 - INFO - __main__ -   Batch number = 35
11/28/2021 01:44:11 - INFO - __main__ -   Batch number = 36
11/28/2021 01:44:12 - INFO - __main__ -   Batch number = 37
11/28/2021 01:44:12 - INFO - __main__ -   Batch number = 38
11/28/2021 01:44:12 - INFO - __main__ -   Batch number = 39
11/28/2021 01:44:13 - INFO - __main__ -   Batch number = 40
11/28/2021 01:44:13 - INFO - __main__ -   Batch number = 41
11/28/2021 01:44:14 - INFO - __main__ -   Batch number = 42
11/28/2021 01:44:14 - INFO - __main__ -   Batch number = 43
11/28/2021 01:44:14 - INFO - __main__ -   Batch number = 44
11/28/2021 01:44:15 - INFO - __main__ -   Batch number = 45
11/28/2021 01:44:15 - INFO - __main__ -   Batch number = 46
11/28/2021 01:44:16 - INFO - __main__ -   Batch number = 47
11/28/2021 01:44:16 - INFO - __main__ -   Batch number = 48
11/28/2021 01:44:17 - INFO - __main__ -   Batch number = 49
11/28/2021 01:44:17 - INFO - __main__ -   Batch number = 50
11/28/2021 01:44:17 - INFO - __main__ -   Batch number = 51
11/28/2021 01:44:18 - INFO - __main__ -   Batch number = 52
11/28/2021 01:44:18 - INFO - __main__ -   Batch number = 53
11/28/2021 01:44:18 - INFO - __main__ -   Batch number = 54
11/28/2021 01:44:19 - INFO - __main__ -   Batch number = 55
11/28/2021 01:44:19 - INFO - __main__ -   Batch number = 56
11/28/2021 01:44:19 - INFO - __main__ -   Batch number = 57
11/28/2021 01:44:20 - INFO - __main__ -   Batch number = 58
11/28/2021 01:44:20 - INFO - __main__ -   Batch number = 59
11/28/2021 01:44:21 - INFO - __main__ -   Batch number = 60
11/28/2021 01:44:21 - INFO - __main__ -   Batch number = 61
11/28/2021 01:44:21 - INFO - __main__ -   Batch number = 62
11/28/2021 01:44:22 - INFO - __main__ -   Batch number = 63
11/28/2021 01:44:22 - INFO - __main__ -   Batch number = 64
11/28/2021 01:44:22 - INFO - __main__ -   Batch number = 65
11/28/2021 01:44:23 - INFO - __main__ -   Batch number = 66
11/28/2021 01:44:23 - INFO - __main__ -   Batch number = 67
11/28/2021 01:44:24 - INFO - __main__ -   Batch number = 68
11/28/2021 01:44:24 - INFO - __main__ -   Batch number = 69
11/28/2021 01:44:25 - INFO - __main__ -   Batch number = 70
11/28/2021 01:44:25 - INFO - __main__ -   Batch number = 71
11/28/2021 01:44:26 - INFO - __main__ -   Batch number = 72
11/28/2021 01:44:26 - INFO - __main__ -   Batch number = 73
11/28/2021 01:44:26 - INFO - __main__ -   Batch number = 74
11/28/2021 01:44:27 - INFO - __main__ -   Batch number = 75
11/28/2021 01:44:27 - INFO - __main__ -   Batch number = 76
11/28/2021 01:44:27 - INFO - __main__ -   Batch number = 77
11/28/2021 01:44:28 - INFO - __main__ -   Batch number = 78
11/28/2021 01:44:28 - INFO - __main__ -   Batch number = 79
11/28/2021 01:44:28 - INFO - __main__ -   Batch number = 80
11/28/2021 01:44:29 - INFO - __main__ -   Batch number = 81
11/28/2021 01:44:29 - INFO - __main__ -   Batch number = 82
11/28/2021 01:44:29 - INFO - __main__ -   Batch number = 83
11/28/2021 01:44:30 - INFO - __main__ -   Batch number = 84
11/28/2021 01:44:30 - INFO - __main__ -   Batch number = 85
11/28/2021 01:44:30 - INFO - __main__ -   Batch number = 86
11/28/2021 01:44:31 - INFO - __main__ -   Batch number = 87
11/28/2021 01:44:31 - INFO - __main__ -   Batch number = 88
11/28/2021 01:44:31 - INFO - __main__ -   Batch number = 89
11/28/2021 01:44:32 - INFO - __main__ -   Batch number = 90
11/28/2021 01:44:32 - INFO - __main__ -   Batch number = 91
11/28/2021 01:44:33 - INFO - __main__ -   Batch number = 92
11/28/2021 01:44:33 - INFO - __main__ -   Batch number = 93
11/28/2021 01:44:33 - INFO - __main__ -   Batch number = 94
11/28/2021 01:44:34 - INFO - __main__ -   Batch number = 95
11/28/2021 01:44:34 - INFO - __main__ -   Batch number = 96
11/28/2021 01:44:34 - INFO - __main__ -   Batch number = 97
11/28/2021 01:44:35 - INFO - __main__ -   Batch number = 98
11/28/2021 01:44:35 - INFO - __main__ -   Batch number = 99
11/28/2021 01:44:35 - INFO - __main__ -   Batch number = 100
11/28/2021 01:44:36 - INFO - __main__ -   Batch number = 101
11/28/2021 01:44:36 - INFO - __main__ -   Batch number = 102
11/28/2021 01:44:36 - INFO - __main__ -   Batch number = 103
11/28/2021 01:44:37 - INFO - __main__ -   Batch number = 104
11/28/2021 01:44:37 - INFO - __main__ -   Batch number = 105
11/28/2021 01:44:37 - INFO - __main__ -   Batch number = 106
11/28/2021 01:44:38 - INFO - __main__ -   Batch number = 107
11/28/2021 01:44:38 - INFO - __main__ -   Batch number = 108
11/28/2021 01:44:38 - INFO - __main__ -   Batch number = 109
11/28/2021 01:44:39 - INFO - __main__ -   Batch number = 110
11/28/2021 01:44:39 - INFO - __main__ -   Batch number = 111
11/28/2021 01:44:39 - INFO - __main__ -   Batch number = 112
11/28/2021 01:44:40 - INFO - __main__ -   Batch number = 113
11/28/2021 01:44:40 - INFO - __main__ -   Batch number = 114
11/28/2021 01:44:40 - INFO - __main__ -   Batch number = 115
11/28/2021 01:44:41 - INFO - __main__ -   Batch number = 116
11/28/2021 01:44:41 - INFO - __main__ -   Batch number = 117
11/28/2021 01:44:41 - INFO - __main__ -   Batch number = 118
11/28/2021 01:44:42 - INFO - __main__ -   Batch number = 119
11/28/2021 01:44:42 - INFO - __main__ -   Batch number = 120
11/28/2021 01:44:42 - INFO - __main__ -   Batch number = 121
11/28/2021 01:44:43 - INFO - __main__ -   Batch number = 122
11/28/2021 01:44:43 - INFO - __main__ -   Batch number = 123
11/28/2021 01:44:43 - INFO - __main__ -   Batch number = 124
11/28/2021 01:44:44 - INFO - __main__ -   Batch number = 125
11/28/2021 01:44:44 - INFO - __main__ -   Batch number = 126
11/28/2021 01:44:44 - INFO - __main__ -   Batch number = 127
11/28/2021 01:44:45 - INFO - __main__ -   Batch number = 128
11/28/2021 01:44:45 - INFO - __main__ -   Batch number = 129
11/28/2021 01:44:46 - INFO - __main__ -   Batch number = 130
11/28/2021 01:44:46 - INFO - __main__ -   Batch number = 131
11/28/2021 01:44:46 - INFO - __main__ -   Batch number = 132
11/28/2021 01:44:47 - INFO - __main__ -   Batch number = 133
11/28/2021 01:44:47 - INFO - __main__ -   Batch number = 134
11/28/2021 01:44:48 - INFO - __main__ -   Batch number = 135
11/28/2021 01:44:48 - INFO - __main__ -   Batch number = 136
11/28/2021 01:44:49 - INFO - __main__ -   Batch number = 137
11/28/2021 01:44:49 - INFO - __main__ -   Batch number = 138
11/28/2021 01:44:50 - INFO - __main__ -   Batch number = 139
11/28/2021 01:44:50 - INFO - __main__ -   Batch number = 140
11/28/2021 01:44:51 - INFO - __main__ -   Batch number = 141
11/28/2021 01:44:52 - INFO - __main__ -   Batch number = 142
11/28/2021 01:44:52 - INFO - __main__ -   Batch number = 143
11/28/2021 01:44:53 - INFO - __main__ -   Batch number = 144
11/28/2021 01:44:54 - INFO - __main__ -   Batch number = 145
11/28/2021 01:44:54 - INFO - __main__ -   Batch number = 146
11/28/2021 01:44:55 - INFO - __main__ -   Batch number = 147
11/28/2021 01:44:56 - INFO - __main__ -   Batch number = 148
11/28/2021 01:44:56 - INFO - __main__ -   Batch number = 149
11/28/2021 01:44:57 - INFO - __main__ -   Batch number = 150
11/28/2021 01:44:57 - INFO - __main__ -   Batch number = 151
11/28/2021 01:44:58 - INFO - __main__ -   Batch number = 152
11/28/2021 01:44:58 - INFO - __main__ -   Batch number = 153
11/28/2021 01:44:59 - INFO - __main__ -   Batch number = 154
11/28/2021 01:44:59 - INFO - __main__ -   Batch number = 155
11/28/2021 01:45:00 - INFO - __main__ -   Batch number = 156
11/28/2021 01:45:00 - INFO - __main__ -   Batch number = 157
11/28/2021 01:45:01 - INFO - __main__ -   Batch number = 158
11/28/2021 01:45:02 - INFO - __main__ -   Batch number = 159
11/28/2021 01:45:03 - INFO - __main__ -   Batch number = 160
11/28/2021 01:45:03 - INFO - __main__ -   Batch number = 161
11/28/2021 01:45:04 - INFO - __main__ -   Batch number = 162
11/28/2021 01:45:04 - INFO - __main__ -   Batch number = 163
11/28/2021 01:45:05 - INFO - __main__ -   Batch number = 164
11/28/2021 01:45:05 - INFO - __main__ -   Batch number = 165
11/28/2021 01:45:06 - INFO - __main__ -   Batch number = 166
11/28/2021 01:45:06 - INFO - __main__ -   Batch number = 167
11/28/2021 01:45:07 - INFO - __main__ -   Batch number = 168
11/28/2021 01:45:07 - INFO - __main__ -   Batch number = 169
11/28/2021 01:45:08 - INFO - __main__ -   Batch number = 170
11/28/2021 01:45:09 - INFO - __main__ -   Batch number = 171
11/28/2021 01:45:09 - INFO - __main__ -   Batch number = 172
11/28/2021 01:45:10 - INFO - __main__ -   Batch number = 173
11/28/2021 01:45:10 - INFO - __main__ -   Batch number = 174
11/28/2021 01:45:10 - INFO - __main__ -   Batch number = 175
11/28/2021 01:45:11 - INFO - __main__ -   Batch number = 176
11/28/2021 01:45:11 - INFO - __main__ -   Batch number = 177
11/28/2021 01:45:12 - INFO - __main__ -   Batch number = 178
11/28/2021 01:45:13 - INFO - __main__ -   Batch number = 179
11/28/2021 01:45:14 - INFO - __main__ -   Batch number = 180
11/28/2021 01:45:15 - INFO - __main__ -   Batch number = 181
11/28/2021 01:45:16 - INFO - __main__ -   Batch number = 182
11/28/2021 01:45:16 - INFO - __main__ -   Batch number = 183
11/28/2021 01:45:17 - INFO - __main__ -   Batch number = 184
11/28/2021 01:45:17 - INFO - __main__ -   Batch number = 185
11/28/2021 01:45:18 - INFO - __main__ -   Batch number = 186
11/28/2021 01:45:18 - INFO - __main__ -   Batch number = 187
11/28/2021 01:45:19 - INFO - __main__ -   Batch number = 188
11/28/2021 01:45:19 - INFO - __main__ -   Batch number = 189
11/28/2021 01:45:19 - INFO - __main__ -   Batch number = 190
11/28/2021 01:45:20 - INFO - __main__ -   Batch number = 191
11/28/2021 01:45:20 - INFO - __main__ -   Batch number = 192
11/28/2021 01:45:20 - INFO - __main__ -   Batch number = 193
11/28/2021 01:45:21 - INFO - __main__ -   Batch number = 194
11/28/2021 01:45:21 - INFO - __main__ -   Batch number = 195
11/28/2021 01:45:22 - INFO - __main__ -   Batch number = 196
11/28/2021 01:45:22 - INFO - __main__ -   Batch number = 197
11/28/2021 01:45:22 - INFO - __main__ -   Batch number = 198
11/28/2021 01:45:23 - INFO - __main__ -   Batch number = 199
11/28/2021 01:45:23 - INFO - __main__ -   Batch number = 200
11/28/2021 01:45:25 - INFO - __main__ -   Batch number = 201
11/28/2021 01:45:25 - INFO - __main__ -   Batch number = 202
11/28/2021 01:45:26 - INFO - __main__ -   Batch number = 203
11/28/2021 01:45:26 - INFO - __main__ -   Batch number = 204
11/28/2021 01:45:27 - INFO - __main__ -   Batch number = 205
11/28/2021 01:45:29 - INFO - __main__ -   ***** Evaluation result  in fi *****
11/28/2021 01:45:29 - INFO - __main__ -     f1 = 0.7351535258211265
11/28/2021 01:45:29 - INFO - __main__ -     loss = 0.9611444428199675
11/28/2021 01:45:29 - INFO - __main__ -     precision = 0.7375778076394359
11/28/2021 01:45:29 - INFO - __main__ -     recall = 0.7327451281264385
11/28/2021 01:45:32 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:45:32 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:45:32 - INFO - __main__ -   Seed = 3
11/28/2021 01:45:32 - INFO - root -   save model
11/28/2021 01:45:32 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:45:32 - INFO - __main__ -   Loading pretrained model and tokenizer
11/28/2021 01:45:35 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
11/28/2021 01:45:42 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:45:42 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:45:42 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
11/28/2021 01:45:42 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:45:42 - INFO - root -   loading task adapter
11/28/2021 01:45:42 - INFO - root -   loading lang adpater bn/wiki@ukp
11/28/2021 01:45:42 - INFO - __main__ -   Adapter Languages : ['bn'], Length : 1
11/28/2021 01:45:42 - INFO - __main__ -   Adapter Names ['bn/wiki@ukp'], Length : 1
11/28/2021 01:45:42 - INFO - __main__ -   Language = bn
11/28/2021 01:45:42 - INFO - __main__ -   Adapter Name = bn/wiki@ukp
11/28/2021 01:45:56 - INFO - __main__ -   Language adapter for fi not found, using bn instead
11/28/2021 01:45:56 - INFO - __main__ -   Set active language adapter to bn
11/28/2021 01:45:56 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:45:56 - INFO - __main__ -   Adapter Languages = ['bn']
11/28/2021 01:45:56 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_fi_bert-base-multilingual-cased_128
11/28/2021 01:45:57 - INFO - __main__ -   ***** Running evaluation  in fi *****
11/28/2021 01:45:57 - INFO - __main__ -     Num examples = 6550
11/28/2021 01:45:57 - INFO - __main__ -     Batch size = 32
11/28/2021 01:45:57 - INFO - __main__ -   Batch number = 1
11/28/2021 01:45:57 - INFO - __main__ -   Batch number = 2
11/28/2021 01:45:58 - INFO - __main__ -   Batch number = 3
11/28/2021 01:45:58 - INFO - __main__ -   Batch number = 4
11/28/2021 01:45:59 - INFO - __main__ -   Batch number = 5
11/28/2021 01:45:59 - INFO - __main__ -   Batch number = 6
11/28/2021 01:46:00 - INFO - __main__ -   Batch number = 7
11/28/2021 01:46:00 - INFO - __main__ -   Batch number = 8
11/28/2021 01:46:01 - INFO - __main__ -   Batch number = 9
11/28/2021 01:46:01 - INFO - __main__ -   Batch number = 10
11/28/2021 01:46:02 - INFO - __main__ -   Batch number = 11
11/28/2021 01:46:02 - INFO - __main__ -   Batch number = 12
11/28/2021 01:46:03 - INFO - __main__ -   Batch number = 13
11/28/2021 01:46:03 - INFO - __main__ -   Batch number = 14
11/28/2021 01:46:04 - INFO - __main__ -   Batch number = 15
11/28/2021 01:46:04 - INFO - __main__ -   Batch number = 16
11/28/2021 01:46:05 - INFO - __main__ -   Batch number = 17
11/28/2021 01:46:05 - INFO - __main__ -   Batch number = 18
11/28/2021 01:46:06 - INFO - __main__ -   Batch number = 19
11/28/2021 01:46:06 - INFO - __main__ -   Batch number = 20
11/28/2021 01:46:07 - INFO - __main__ -   Batch number = 21
11/28/2021 01:46:07 - INFO - __main__ -   Batch number = 22
11/28/2021 01:46:08 - INFO - __main__ -   Batch number = 23
11/28/2021 01:46:08 - INFO - __main__ -   Batch number = 24
11/28/2021 01:46:09 - INFO - __main__ -   Batch number = 25
11/28/2021 01:46:09 - INFO - __main__ -   Batch number = 26
11/28/2021 01:46:09 - INFO - __main__ -   Batch number = 27
11/28/2021 01:46:10 - INFO - __main__ -   Batch number = 28
11/28/2021 01:46:10 - INFO - __main__ -   Batch number = 29
11/28/2021 01:46:10 - INFO - __main__ -   Batch number = 30
11/28/2021 01:46:10 - INFO - __main__ -   Batch number = 31
11/28/2021 01:46:11 - INFO - __main__ -   Batch number = 32
11/28/2021 01:46:11 - INFO - __main__ -   Batch number = 33
11/28/2021 01:46:11 - INFO - __main__ -   Batch number = 34
11/28/2021 01:46:12 - INFO - __main__ -   Batch number = 35
11/28/2021 01:46:12 - INFO - __main__ -   Batch number = 36
11/28/2021 01:46:12 - INFO - __main__ -   Batch number = 37
11/28/2021 01:46:13 - INFO - __main__ -   Batch number = 38
11/28/2021 01:46:13 - INFO - __main__ -   Batch number = 39
11/28/2021 01:46:13 - INFO - __main__ -   Batch number = 40
11/28/2021 01:46:14 - INFO - __main__ -   Batch number = 41
11/28/2021 01:46:14 - INFO - __main__ -   Batch number = 42
11/28/2021 01:46:14 - INFO - __main__ -   Batch number = 43
11/28/2021 01:46:15 - INFO - __main__ -   Batch number = 44
11/28/2021 01:46:15 - INFO - __main__ -   Batch number = 45
11/28/2021 01:46:15 - INFO - __main__ -   Batch number = 46
11/28/2021 01:46:15 - INFO - __main__ -   Batch number = 47
11/28/2021 01:46:15 - INFO - __main__ -   Batch number = 48
11/28/2021 01:46:15 - INFO - __main__ -   Batch number = 49
11/28/2021 01:46:16 - INFO - __main__ -   Batch number = 50
11/28/2021 01:46:16 - INFO - __main__ -   Batch number = 51
11/28/2021 01:46:16 - INFO - __main__ -   Batch number = 52
11/28/2021 01:46:16 - INFO - __main__ -   Batch number = 53
11/28/2021 01:46:16 - INFO - __main__ -   Batch number = 54
11/28/2021 01:46:16 - INFO - __main__ -   Batch number = 55
11/28/2021 01:46:17 - INFO - __main__ -   Batch number = 56
11/28/2021 01:46:17 - INFO - __main__ -   Batch number = 57
11/28/2021 01:46:17 - INFO - __main__ -   Batch number = 58
11/28/2021 01:46:17 - INFO - __main__ -   Batch number = 59
11/28/2021 01:46:17 - INFO - __main__ -   Batch number = 60
11/28/2021 01:46:17 - INFO - __main__ -   Batch number = 61
11/28/2021 01:46:18 - INFO - __main__ -   Batch number = 62
11/28/2021 01:46:18 - INFO - __main__ -   Batch number = 63
11/28/2021 01:46:18 - INFO - __main__ -   Batch number = 64
11/28/2021 01:46:18 - INFO - __main__ -   Batch number = 65
11/28/2021 01:46:18 - INFO - __main__ -   Batch number = 66
11/28/2021 01:46:18 - INFO - __main__ -   Batch number = 67
11/28/2021 01:46:19 - INFO - __main__ -   Batch number = 68
11/28/2021 01:46:19 - INFO - __main__ -   Batch number = 69
11/28/2021 01:46:19 - INFO - __main__ -   Batch number = 70
11/28/2021 01:46:19 - INFO - __main__ -   Batch number = 71
11/28/2021 01:46:19 - INFO - __main__ -   Batch number = 72
11/28/2021 01:46:19 - INFO - __main__ -   Batch number = 73
11/28/2021 01:46:20 - INFO - __main__ -   Batch number = 74
11/28/2021 01:46:20 - INFO - __main__ -   Batch number = 75
11/28/2021 01:46:20 - INFO - __main__ -   Batch number = 76
11/28/2021 01:46:20 - INFO - __main__ -   Batch number = 77
11/28/2021 01:46:20 - INFO - __main__ -   Batch number = 78
11/28/2021 01:46:21 - INFO - __main__ -   Batch number = 79
11/28/2021 01:46:21 - INFO - __main__ -   Batch number = 80
11/28/2021 01:46:21 - INFO - __main__ -   Batch number = 81
11/28/2021 01:46:21 - INFO - __main__ -   Batch number = 82
11/28/2021 01:46:22 - INFO - __main__ -   Batch number = 83
11/28/2021 01:46:22 - INFO - __main__ -   Batch number = 84
11/28/2021 01:46:22 - INFO - __main__ -   Batch number = 85
11/28/2021 01:46:23 - INFO - __main__ -   Batch number = 86
11/28/2021 01:46:23 - INFO - __main__ -   Batch number = 87
11/28/2021 01:46:23 - INFO - __main__ -   Batch number = 88
11/28/2021 01:46:24 - INFO - __main__ -   Batch number = 89
11/28/2021 01:46:24 - INFO - __main__ -   Batch number = 90
11/28/2021 01:46:24 - INFO - __main__ -   Batch number = 91
11/28/2021 01:46:25 - INFO - __main__ -   Batch number = 92
11/28/2021 01:46:25 - INFO - __main__ -   Batch number = 93
11/28/2021 01:46:25 - INFO - __main__ -   Batch number = 94
11/28/2021 01:46:26 - INFO - __main__ -   Batch number = 95
11/28/2021 01:46:26 - INFO - __main__ -   Batch number = 96
11/28/2021 01:46:26 - INFO - __main__ -   Batch number = 97
11/28/2021 01:46:27 - INFO - __main__ -   Batch number = 98
11/28/2021 01:46:27 - INFO - __main__ -   Batch number = 99
11/28/2021 01:46:27 - INFO - __main__ -   Batch number = 100
11/28/2021 01:46:28 - INFO - __main__ -   Batch number = 101
11/28/2021 01:46:28 - INFO - __main__ -   Batch number = 102
11/28/2021 01:46:28 - INFO - __main__ -   Batch number = 103
11/28/2021 01:46:29 - INFO - __main__ -   Batch number = 104
11/28/2021 01:46:29 - INFO - __main__ -   Batch number = 105
11/28/2021 01:46:29 - INFO - __main__ -   Batch number = 106
11/28/2021 01:46:30 - INFO - __main__ -   Batch number = 107
11/28/2021 01:46:30 - INFO - __main__ -   Batch number = 108
11/28/2021 01:46:31 - INFO - __main__ -   Batch number = 109
11/28/2021 01:46:31 - INFO - __main__ -   Batch number = 110
11/28/2021 01:46:31 - INFO - __main__ -   Batch number = 111
11/28/2021 01:46:32 - INFO - __main__ -   Batch number = 112
11/28/2021 01:46:32 - INFO - __main__ -   Batch number = 113
11/28/2021 01:46:32 - INFO - __main__ -   Batch number = 114
11/28/2021 01:46:33 - INFO - __main__ -   Batch number = 115
11/28/2021 01:46:33 - INFO - __main__ -   Batch number = 116
11/28/2021 01:46:34 - INFO - __main__ -   Batch number = 117
11/28/2021 01:46:34 - INFO - __main__ -   Batch number = 118
11/28/2021 01:46:35 - INFO - __main__ -   Batch number = 119
11/28/2021 01:46:36 - INFO - __main__ -   Batch number = 120
11/28/2021 01:46:36 - INFO - __main__ -   Batch number = 121
11/28/2021 01:46:37 - INFO - __main__ -   Batch number = 122
11/28/2021 01:46:37 - INFO - __main__ -   Batch number = 123
11/28/2021 01:46:38 - INFO - __main__ -   Batch number = 124
11/28/2021 01:46:38 - INFO - __main__ -   Batch number = 125
11/28/2021 01:46:39 - INFO - __main__ -   Batch number = 126
11/28/2021 01:46:39 - INFO - __main__ -   Batch number = 127
11/28/2021 01:46:40 - INFO - __main__ -   Batch number = 128
11/28/2021 01:46:40 - INFO - __main__ -   Batch number = 129
11/28/2021 01:46:41 - INFO - __main__ -   Batch number = 130
11/28/2021 01:46:41 - INFO - __main__ -   Batch number = 131
11/28/2021 01:46:42 - INFO - __main__ -   Batch number = 132
11/28/2021 01:46:42 - INFO - __main__ -   Batch number = 133
11/28/2021 01:46:43 - INFO - __main__ -   Batch number = 134
11/28/2021 01:46:44 - INFO - __main__ -   Batch number = 135
11/28/2021 01:46:44 - INFO - __main__ -   Batch number = 136
11/28/2021 01:46:45 - INFO - __main__ -   Batch number = 137
11/28/2021 01:46:46 - INFO - __main__ -   Batch number = 138
11/28/2021 01:46:46 - INFO - __main__ -   Batch number = 139
11/28/2021 01:46:47 - INFO - __main__ -   Batch number = 140
11/28/2021 01:46:48 - INFO - __main__ -   Batch number = 141
11/28/2021 01:46:48 - INFO - __main__ -   Batch number = 142
11/28/2021 01:46:49 - INFO - __main__ -   Batch number = 143
11/28/2021 01:46:50 - INFO - __main__ -   Batch number = 144
11/28/2021 01:46:50 - INFO - __main__ -   Batch number = 145
11/28/2021 01:46:51 - INFO - __main__ -   Batch number = 146
11/28/2021 01:46:52 - INFO - __main__ -   Batch number = 147
11/28/2021 01:46:53 - INFO - __main__ -   Batch number = 148
11/28/2021 01:46:53 - INFO - __main__ -   Batch number = 149
11/28/2021 01:46:54 - INFO - __main__ -   Batch number = 150
11/28/2021 01:46:55 - INFO - __main__ -   Batch number = 151
11/28/2021 01:46:55 - INFO - __main__ -   Batch number = 152
11/28/2021 01:46:56 - INFO - __main__ -   Batch number = 153
11/28/2021 01:46:57 - INFO - __main__ -   Batch number = 154
11/28/2021 01:46:57 - INFO - __main__ -   Batch number = 155
11/28/2021 01:46:58 - INFO - __main__ -   Batch number = 156
11/28/2021 01:46:59 - INFO - __main__ -   Batch number = 157
11/28/2021 01:46:59 - INFO - __main__ -   Batch number = 158
11/28/2021 01:47:00 - INFO - __main__ -   Batch number = 159
11/28/2021 01:47:01 - INFO - __main__ -   Batch number = 160
11/28/2021 01:47:02 - INFO - __main__ -   Batch number = 161
11/28/2021 01:47:02 - INFO - __main__ -   Batch number = 162
11/28/2021 01:47:03 - INFO - __main__ -   Batch number = 163
11/28/2021 01:47:04 - INFO - __main__ -   Batch number = 164
11/28/2021 01:47:04 - INFO - __main__ -   Batch number = 165
11/28/2021 01:47:04 - INFO - __main__ -   Batch number = 166
11/28/2021 01:47:05 - INFO - __main__ -   Batch number = 167
11/28/2021 01:47:05 - INFO - __main__ -   Batch number = 168
11/28/2021 01:47:06 - INFO - __main__ -   Batch number = 169
11/28/2021 01:47:06 - INFO - __main__ -   Batch number = 170
11/28/2021 01:47:07 - INFO - __main__ -   Batch number = 171
11/28/2021 01:47:07 - INFO - __main__ -   Batch number = 172
11/28/2021 01:47:08 - INFO - __main__ -   Batch number = 173
11/28/2021 01:47:08 - INFO - __main__ -   Batch number = 174
11/28/2021 01:47:09 - INFO - __main__ -   Batch number = 175
11/28/2021 01:47:09 - INFO - __main__ -   Batch number = 176
11/28/2021 01:47:10 - INFO - __main__ -   Batch number = 177
11/28/2021 01:47:10 - INFO - __main__ -   Batch number = 178
11/28/2021 01:47:11 - INFO - __main__ -   Batch number = 179
11/28/2021 01:47:11 - INFO - __main__ -   Batch number = 180
11/28/2021 01:47:12 - INFO - __main__ -   Batch number = 181
11/28/2021 01:47:12 - INFO - __main__ -   Batch number = 182
11/28/2021 01:47:12 - INFO - __main__ -   Batch number = 183
11/28/2021 01:47:13 - INFO - __main__ -   Batch number = 184
11/28/2021 01:47:14 - INFO - __main__ -   Batch number = 185
11/28/2021 01:47:14 - INFO - __main__ -   Batch number = 186
11/28/2021 01:47:14 - INFO - __main__ -   Batch number = 187
11/28/2021 01:47:15 - INFO - __main__ -   Batch number = 188
11/28/2021 01:47:15 - INFO - __main__ -   Batch number = 189
11/28/2021 01:47:15 - INFO - __main__ -   Batch number = 190
11/28/2021 01:47:16 - INFO - __main__ -   Batch number = 191
11/28/2021 01:47:16 - INFO - __main__ -   Batch number = 192
11/28/2021 01:47:17 - INFO - __main__ -   Batch number = 193
11/28/2021 01:47:17 - INFO - __main__ -   Batch number = 194
11/28/2021 01:47:17 - INFO - __main__ -   Batch number = 195
11/28/2021 01:47:18 - INFO - __main__ -   Batch number = 196
11/28/2021 01:47:18 - INFO - __main__ -   Batch number = 197
11/28/2021 01:47:19 - INFO - __main__ -   Batch number = 198
11/28/2021 01:47:19 - INFO - __main__ -   Batch number = 199
11/28/2021 01:47:19 - INFO - __main__ -   Batch number = 200
11/28/2021 01:47:20 - INFO - __main__ -   Batch number = 201
11/28/2021 01:47:20 - INFO - __main__ -   Batch number = 202
11/28/2021 01:47:21 - INFO - __main__ -   Batch number = 203
11/28/2021 01:47:21 - INFO - __main__ -   Batch number = 204
11/28/2021 01:47:22 - INFO - __main__ -   Batch number = 205
11/28/2021 01:47:24 - INFO - __main__ -   ***** Evaluation result  in fi *****
11/28/2021 01:47:24 - INFO - __main__ -     f1 = 0.7520622255831718
11/28/2021 01:47:24 - INFO - __main__ -     loss = 0.8831770639594009
11/28/2021 01:47:24 - INFO - __main__ -     precision = 0.7564223957282337
11/28/2021 01:47:24 - INFO - __main__ -     recall = 0.7477520331440847
11/28/2021 16:27:00 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='zh', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 16:27:00 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 16:27:00 - INFO - __main__ -   Seed = 1
11/28/2021 16:27:00 - INFO - root -   save model
11/28/2021 16:27:00 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='zh', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 16:27:00 - INFO - __main__ -   Loading pretrained model and tokenizer
11/28/2021 16:27:03 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
11/28/2021 16:27:09 - INFO - __main__ -   Using lang2id = None
11/28/2021 16:27:09 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 16:27:09 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
11/28/2021 16:27:09 - INFO - root -   Trying to decide if add adapter
11/28/2021 16:27:09 - INFO - root -   loading task adapter
11/28/2021 16:27:09 - INFO - root -   loading lang adpater bn/wiki@ukp
11/28/2021 16:27:09 - INFO - __main__ -   Adapter Languages : ['bn'], Length : 1
11/28/2021 16:27:09 - INFO - __main__ -   Adapter Names ['bn/wiki@ukp'], Length : 1
11/28/2021 16:27:09 - INFO - __main__ -   Language = bn
11/28/2021 16:27:09 - INFO - __main__ -   Adapter Name = bn/wiki@ukp
11/28/2021 16:27:18 - INFO - __main__ -   Language adapter for zh not found, using bn instead
11/28/2021 16:27:18 - INFO - __main__ -   Set active language adapter to bn
11/28/2021 16:27:18 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 16:27:18 - INFO - __main__ -   Adapter Languages = ['bn']
11/28/2021 16:27:18 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_zh_bert-base-multilingual-cased_128
11/28/2021 16:27:19 - INFO - __main__ -   ***** Running evaluation  in zh *****
11/28/2021 16:27:19 - INFO - __main__ -     Num examples = 3458
11/28/2021 16:27:19 - INFO - __main__ -     Batch size = 32
11/28/2021 16:27:19 - INFO - __main__ -   Batch number = 1
11/28/2021 16:27:19 - INFO - __main__ -   Batch number = 2
11/28/2021 16:27:20 - INFO - __main__ -   Batch number = 3
11/28/2021 16:27:20 - INFO - __main__ -   Batch number = 4
11/28/2021 16:27:21 - INFO - __main__ -   Batch number = 5
11/28/2021 16:27:21 - INFO - __main__ -   Batch number = 6
11/28/2021 16:27:22 - INFO - __main__ -   Batch number = 7
11/28/2021 16:27:22 - INFO - __main__ -   Batch number = 8
11/28/2021 16:27:23 - INFO - __main__ -   Batch number = 9
11/28/2021 16:27:23 - INFO - __main__ -   Batch number = 10
11/28/2021 16:27:24 - INFO - __main__ -   Batch number = 11
11/28/2021 16:27:24 - INFO - __main__ -   Batch number = 12
11/28/2021 16:27:25 - INFO - __main__ -   Batch number = 13
11/28/2021 16:27:25 - INFO - __main__ -   Batch number = 14
11/28/2021 16:27:26 - INFO - __main__ -   Batch number = 15
11/28/2021 16:27:26 - INFO - __main__ -   Batch number = 16
11/28/2021 16:27:27 - INFO - __main__ -   Batch number = 17
11/28/2021 16:27:27 - INFO - __main__ -   Batch number = 18
11/28/2021 16:27:28 - INFO - __main__ -   Batch number = 19
11/28/2021 16:27:28 - INFO - __main__ -   Batch number = 20
11/28/2021 16:27:28 - INFO - __main__ -   Batch number = 21
11/28/2021 16:27:29 - INFO - __main__ -   Batch number = 22
11/28/2021 16:27:29 - INFO - __main__ -   Batch number = 23
11/28/2021 16:27:30 - INFO - __main__ -   Batch number = 24
11/28/2021 16:27:30 - INFO - __main__ -   Batch number = 25
11/28/2021 16:27:31 - INFO - __main__ -   Batch number = 26
11/28/2021 16:27:31 - INFO - __main__ -   Batch number = 27
11/28/2021 16:27:32 - INFO - __main__ -   Batch number = 28
11/28/2021 16:27:32 - INFO - __main__ -   Batch number = 29
11/28/2021 16:27:33 - INFO - __main__ -   Batch number = 30
11/28/2021 16:27:33 - INFO - __main__ -   Batch number = 31
11/28/2021 16:27:34 - INFO - __main__ -   Batch number = 32
11/28/2021 16:27:34 - INFO - __main__ -   Batch number = 33
11/28/2021 16:27:35 - INFO - __main__ -   Batch number = 34
11/28/2021 16:27:35 - INFO - __main__ -   Batch number = 35
11/28/2021 16:27:36 - INFO - __main__ -   Batch number = 36
11/28/2021 16:27:36 - INFO - __main__ -   Batch number = 37
11/28/2021 16:27:37 - INFO - __main__ -   Batch number = 38
11/28/2021 16:27:37 - INFO - __main__ -   Batch number = 39
11/28/2021 16:27:38 - INFO - __main__ -   Batch number = 40
11/28/2021 16:27:38 - INFO - __main__ -   Batch number = 41
11/28/2021 16:27:39 - INFO - __main__ -   Batch number = 42
11/28/2021 16:27:39 - INFO - __main__ -   Batch number = 43
11/28/2021 16:27:40 - INFO - __main__ -   Batch number = 44
11/28/2021 16:27:40 - INFO - __main__ -   Batch number = 45
11/28/2021 16:27:41 - INFO - __main__ -   Batch number = 46
11/28/2021 16:27:41 - INFO - __main__ -   Batch number = 47
11/28/2021 16:27:42 - INFO - __main__ -   Batch number = 48
11/28/2021 16:27:42 - INFO - __main__ -   Batch number = 49
11/28/2021 16:27:43 - INFO - __main__ -   Batch number = 50
11/28/2021 16:27:43 - INFO - __main__ -   Batch number = 51
11/28/2021 16:27:44 - INFO - __main__ -   Batch number = 52
11/28/2021 16:27:44 - INFO - __main__ -   Batch number = 53
11/28/2021 16:27:45 - INFO - __main__ -   Batch number = 54
11/28/2021 16:27:45 - INFO - __main__ -   Batch number = 55
11/28/2021 16:27:46 - INFO - __main__ -   Batch number = 56
11/28/2021 16:27:46 - INFO - __main__ -   Batch number = 57
11/28/2021 16:27:47 - INFO - __main__ -   Batch number = 58
11/28/2021 16:27:47 - INFO - __main__ -   Batch number = 59
11/28/2021 16:27:48 - INFO - __main__ -   Batch number = 60
11/28/2021 16:27:48 - INFO - __main__ -   Batch number = 61
11/28/2021 16:27:49 - INFO - __main__ -   Batch number = 62
11/28/2021 16:27:49 - INFO - __main__ -   Batch number = 63
11/28/2021 16:27:50 - INFO - __main__ -   Batch number = 64
11/28/2021 16:27:50 - INFO - __main__ -   Batch number = 65
11/28/2021 16:27:51 - INFO - __main__ -   Batch number = 66
11/28/2021 16:27:51 - INFO - __main__ -   Batch number = 67
11/28/2021 16:27:52 - INFO - __main__ -   Batch number = 68
11/28/2021 16:27:52 - INFO - __main__ -   Batch number = 69
11/28/2021 16:27:53 - INFO - __main__ -   Batch number = 70
11/28/2021 16:27:53 - INFO - __main__ -   Batch number = 71
11/28/2021 16:27:54 - INFO - __main__ -   Batch number = 72
11/28/2021 16:27:54 - INFO - __main__ -   Batch number = 73
11/28/2021 16:27:55 - INFO - __main__ -   Batch number = 74
11/28/2021 16:27:55 - INFO - __main__ -   Batch number = 75
11/28/2021 16:27:55 - INFO - __main__ -   Batch number = 76
11/28/2021 16:27:56 - INFO - __main__ -   Batch number = 77
11/28/2021 16:27:56 - INFO - __main__ -   Batch number = 78
11/28/2021 16:27:57 - INFO - __main__ -   Batch number = 79
11/28/2021 16:27:57 - INFO - __main__ -   Batch number = 80
11/28/2021 16:27:58 - INFO - __main__ -   Batch number = 81
11/28/2021 16:27:58 - INFO - __main__ -   Batch number = 82
11/28/2021 16:27:59 - INFO - __main__ -   Batch number = 83
11/28/2021 16:27:59 - INFO - __main__ -   Batch number = 84
11/28/2021 16:28:00 - INFO - __main__ -   Batch number = 85
11/28/2021 16:28:00 - INFO - __main__ -   Batch number = 86
11/28/2021 16:28:01 - INFO - __main__ -   Batch number = 87
11/28/2021 16:28:01 - INFO - __main__ -   Batch number = 88
11/28/2021 16:28:01 - INFO - __main__ -   Batch number = 89
11/28/2021 16:28:02 - INFO - __main__ -   Batch number = 90
11/28/2021 16:28:02 - INFO - __main__ -   Batch number = 91
11/28/2021 16:28:02 - INFO - __main__ -   Batch number = 92
11/28/2021 16:28:03 - INFO - __main__ -   Batch number = 93
11/28/2021 16:28:03 - INFO - __main__ -   Batch number = 94
11/28/2021 16:28:03 - INFO - __main__ -   Batch number = 95
11/28/2021 16:28:03 - INFO - __main__ -   Batch number = 96
11/28/2021 16:28:04 - INFO - __main__ -   Batch number = 97
11/28/2021 16:28:04 - INFO - __main__ -   Batch number = 98
11/28/2021 16:28:04 - INFO - __main__ -   Batch number = 99
11/28/2021 16:28:05 - INFO - __main__ -   Batch number = 100
11/28/2021 16:28:05 - INFO - __main__ -   Batch number = 101
11/28/2021 16:28:05 - INFO - __main__ -   Batch number = 102
11/28/2021 16:28:06 - INFO - __main__ -   Batch number = 103
11/28/2021 16:28:06 - INFO - __main__ -   Batch number = 104
11/28/2021 16:28:06 - INFO - __main__ -   Batch number = 105
11/28/2021 16:28:07 - INFO - __main__ -   Batch number = 106
11/28/2021 16:28:07 - INFO - __main__ -   Batch number = 107
11/28/2021 16:28:07 - INFO - __main__ -   Batch number = 108
11/28/2021 16:28:08 - INFO - __main__ -   Batch number = 109
11/28/2021 16:28:09 - INFO - __main__ -   ***** Evaluation result  in zh *****
11/28/2021 16:28:09 - INFO - __main__ -     f1 = 0.6023805447681176
11/28/2021 16:28:09 - INFO - __main__ -     loss = 1.443457361755021
11/28/2021 16:28:09 - INFO - __main__ -     precision = 0.6114210458927036
11/28/2021 16:28:09 - INFO - __main__ -     recall = 0.5936034948146044
11/28/2021 16:28:12 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='zh', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 16:28:12 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 16:28:12 - INFO - __main__ -   Seed = 2
11/28/2021 16:28:12 - INFO - root -   save model
11/28/2021 16:28:12 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='zh', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 16:28:12 - INFO - __main__ -   Loading pretrained model and tokenizer
11/28/2021 16:28:14 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
11/28/2021 16:28:20 - INFO - __main__ -   Using lang2id = None
11/28/2021 16:28:20 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 16:28:20 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
11/28/2021 16:28:20 - INFO - root -   Trying to decide if add adapter
11/28/2021 16:28:20 - INFO - root -   loading task adapter
11/28/2021 16:28:20 - INFO - root -   loading lang adpater bn/wiki@ukp
11/28/2021 16:28:20 - INFO - __main__ -   Adapter Languages : ['bn'], Length : 1
11/28/2021 16:28:20 - INFO - __main__ -   Adapter Names ['bn/wiki@ukp'], Length : 1
11/28/2021 16:28:20 - INFO - __main__ -   Language = bn
11/28/2021 16:28:20 - INFO - __main__ -   Adapter Name = bn/wiki@ukp
11/28/2021 16:28:28 - INFO - __main__ -   Language adapter for zh not found, using bn instead
11/28/2021 16:28:28 - INFO - __main__ -   Set active language adapter to bn
11/28/2021 16:28:28 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 16:28:28 - INFO - __main__ -   Adapter Languages = ['bn']
11/28/2021 16:28:28 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_zh_bert-base-multilingual-cased_128
11/28/2021 16:28:29 - INFO - __main__ -   ***** Running evaluation  in zh *****
11/28/2021 16:28:29 - INFO - __main__ -     Num examples = 3458
11/28/2021 16:28:29 - INFO - __main__ -     Batch size = 32
11/28/2021 16:28:29 - INFO - __main__ -   Batch number = 1
11/28/2021 16:28:29 - INFO - __main__ -   Batch number = 2
11/28/2021 16:28:29 - INFO - __main__ -   Batch number = 3
11/28/2021 16:28:30 - INFO - __main__ -   Batch number = 4
11/28/2021 16:28:30 - INFO - __main__ -   Batch number = 5
11/28/2021 16:28:30 - INFO - __main__ -   Batch number = 6
11/28/2021 16:28:31 - INFO - __main__ -   Batch number = 7
11/28/2021 16:28:31 - INFO - __main__ -   Batch number = 8
11/28/2021 16:28:31 - INFO - __main__ -   Batch number = 9
11/28/2021 16:28:32 - INFO - __main__ -   Batch number = 10
11/28/2021 16:28:32 - INFO - __main__ -   Batch number = 11
11/28/2021 16:28:32 - INFO - __main__ -   Batch number = 12
11/28/2021 16:28:33 - INFO - __main__ -   Batch number = 13
11/28/2021 16:28:33 - INFO - __main__ -   Batch number = 14
11/28/2021 16:28:33 - INFO - __main__ -   Batch number = 15
11/28/2021 16:28:34 - INFO - __main__ -   Batch number = 16
11/28/2021 16:28:34 - INFO - __main__ -   Batch number = 17
11/28/2021 16:28:34 - INFO - __main__ -   Batch number = 18
11/28/2021 16:28:34 - INFO - __main__ -   Batch number = 19
11/28/2021 16:28:35 - INFO - __main__ -   Batch number = 20
11/28/2021 16:28:35 - INFO - __main__ -   Batch number = 21
11/28/2021 16:28:35 - INFO - __main__ -   Batch number = 22
11/28/2021 16:28:36 - INFO - __main__ -   Batch number = 23
11/28/2021 16:28:36 - INFO - __main__ -   Batch number = 24
11/28/2021 16:28:36 - INFO - __main__ -   Batch number = 25
11/28/2021 16:28:37 - INFO - __main__ -   Batch number = 26
11/28/2021 16:28:37 - INFO - __main__ -   Batch number = 27
11/28/2021 16:28:37 - INFO - __main__ -   Batch number = 28
11/28/2021 16:28:38 - INFO - __main__ -   Batch number = 29
11/28/2021 16:28:38 - INFO - __main__ -   Batch number = 30
11/28/2021 16:28:38 - INFO - __main__ -   Batch number = 31
11/28/2021 16:28:39 - INFO - __main__ -   Batch number = 32
11/28/2021 16:28:39 - INFO - __main__ -   Batch number = 33
11/28/2021 16:28:39 - INFO - __main__ -   Batch number = 34
11/28/2021 16:28:40 - INFO - __main__ -   Batch number = 35
11/28/2021 16:28:40 - INFO - __main__ -   Batch number = 36
11/28/2021 16:28:40 - INFO - __main__ -   Batch number = 37
11/28/2021 16:28:41 - INFO - __main__ -   Batch number = 38
11/28/2021 16:28:41 - INFO - __main__ -   Batch number = 39
11/28/2021 16:28:41 - INFO - __main__ -   Batch number = 40
11/28/2021 16:28:42 - INFO - __main__ -   Batch number = 41
11/28/2021 16:28:42 - INFO - __main__ -   Batch number = 42
11/28/2021 16:28:42 - INFO - __main__ -   Batch number = 43
11/28/2021 16:28:43 - INFO - __main__ -   Batch number = 44
11/28/2021 16:28:43 - INFO - __main__ -   Batch number = 45
11/28/2021 16:28:43 - INFO - __main__ -   Batch number = 46
11/28/2021 16:28:44 - INFO - __main__ -   Batch number = 47
11/28/2021 16:28:44 - INFO - __main__ -   Batch number = 48
11/28/2021 16:28:44 - INFO - __main__ -   Batch number = 49
11/28/2021 16:28:45 - INFO - __main__ -   Batch number = 50
11/28/2021 16:28:45 - INFO - __main__ -   Batch number = 51
11/28/2021 16:28:45 - INFO - __main__ -   Batch number = 52
11/28/2021 16:28:46 - INFO - __main__ -   Batch number = 53
11/28/2021 16:28:46 - INFO - __main__ -   Batch number = 54
11/28/2021 16:28:46 - INFO - __main__ -   Batch number = 55
11/28/2021 16:28:47 - INFO - __main__ -   Batch number = 56
11/28/2021 16:28:47 - INFO - __main__ -   Batch number = 57
11/28/2021 16:28:47 - INFO - __main__ -   Batch number = 58
11/28/2021 16:28:48 - INFO - __main__ -   Batch number = 59
11/28/2021 16:28:48 - INFO - __main__ -   Batch number = 60
11/28/2021 16:28:48 - INFO - __main__ -   Batch number = 61
11/28/2021 16:28:49 - INFO - __main__ -   Batch number = 62
11/28/2021 16:28:49 - INFO - __main__ -   Batch number = 63
11/28/2021 16:28:49 - INFO - __main__ -   Batch number = 64
11/28/2021 16:28:50 - INFO - __main__ -   Batch number = 65
11/28/2021 16:28:50 - INFO - __main__ -   Batch number = 66
11/28/2021 16:28:50 - INFO - __main__ -   Batch number = 67
11/28/2021 16:28:51 - INFO - __main__ -   Batch number = 68
11/28/2021 16:28:51 - INFO - __main__ -   Batch number = 69
11/28/2021 16:28:51 - INFO - __main__ -   Batch number = 70
11/28/2021 16:28:51 - INFO - __main__ -   Batch number = 71
11/28/2021 16:28:52 - INFO - __main__ -   Batch number = 72
11/28/2021 16:28:52 - INFO - __main__ -   Batch number = 73
11/28/2021 16:28:52 - INFO - __main__ -   Batch number = 74
11/28/2021 16:28:53 - INFO - __main__ -   Batch number = 75
11/28/2021 16:28:53 - INFO - __main__ -   Batch number = 76
11/28/2021 16:28:53 - INFO - __main__ -   Batch number = 77
11/28/2021 16:28:54 - INFO - __main__ -   Batch number = 78
11/28/2021 16:28:54 - INFO - __main__ -   Batch number = 79
11/28/2021 16:28:54 - INFO - __main__ -   Batch number = 80
11/28/2021 16:28:55 - INFO - __main__ -   Batch number = 81
11/28/2021 16:28:55 - INFO - __main__ -   Batch number = 82
11/28/2021 16:28:55 - INFO - __main__ -   Batch number = 83
11/28/2021 16:28:56 - INFO - __main__ -   Batch number = 84
11/28/2021 16:28:56 - INFO - __main__ -   Batch number = 85
11/28/2021 16:28:56 - INFO - __main__ -   Batch number = 86
11/28/2021 16:28:56 - INFO - __main__ -   Batch number = 87
11/28/2021 16:28:57 - INFO - __main__ -   Batch number = 88
11/28/2021 16:28:57 - INFO - __main__ -   Batch number = 89
11/28/2021 16:28:57 - INFO - __main__ -   Batch number = 90
11/28/2021 16:28:58 - INFO - __main__ -   Batch number = 91
11/28/2021 16:28:58 - INFO - __main__ -   Batch number = 92
11/28/2021 16:28:58 - INFO - __main__ -   Batch number = 93
11/28/2021 16:28:59 - INFO - __main__ -   Batch number = 94
11/28/2021 16:28:59 - INFO - __main__ -   Batch number = 95
11/28/2021 16:28:59 - INFO - __main__ -   Batch number = 96
11/28/2021 16:29:00 - INFO - __main__ -   Batch number = 97
11/28/2021 16:29:00 - INFO - __main__ -   Batch number = 98
11/28/2021 16:29:00 - INFO - __main__ -   Batch number = 99
11/28/2021 16:29:01 - INFO - __main__ -   Batch number = 100
11/28/2021 16:29:01 - INFO - __main__ -   Batch number = 101
11/28/2021 16:29:01 - INFO - __main__ -   Batch number = 102
11/28/2021 16:29:02 - INFO - __main__ -   Batch number = 103
11/28/2021 16:29:02 - INFO - __main__ -   Batch number = 104
11/28/2021 16:29:02 - INFO - __main__ -   Batch number = 105
11/28/2021 16:29:03 - INFO - __main__ -   Batch number = 106
11/28/2021 16:29:03 - INFO - __main__ -   Batch number = 107
11/28/2021 16:29:03 - INFO - __main__ -   Batch number = 108
11/28/2021 16:29:04 - INFO - __main__ -   Batch number = 109
11/28/2021 16:29:05 - INFO - __main__ -   ***** Evaluation result  in zh *****
11/28/2021 16:29:05 - INFO - __main__ -     f1 = 0.6134583037248253
11/28/2021 16:29:05 - INFO - __main__ -     loss = 1.3601835725504323
11/28/2021 16:29:05 - INFO - __main__ -     precision = 0.6215708791308945
11/28/2021 16:29:05 - INFO - __main__ -     recall = 0.6055547663020315
11/28/2021 16:29:08 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='zh', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 16:29:08 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 16:29:08 - INFO - __main__ -   Seed = 3
11/28/2021 16:29:08 - INFO - root -   save model
11/28/2021 16:29:08 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='zh', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_bn//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 16:29:08 - INFO - __main__ -   Loading pretrained model and tokenizer
11/28/2021 16:29:10 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
11/28/2021 16:29:16 - INFO - __main__ -   Using lang2id = None
11/28/2021 16:29:16 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 16:29:16 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
11/28/2021 16:29:16 - INFO - root -   Trying to decide if add adapter
11/28/2021 16:29:16 - INFO - root -   loading task adapter
11/28/2021 16:29:16 - INFO - root -   loading lang adpater bn/wiki@ukp
11/28/2021 16:29:16 - INFO - __main__ -   Adapter Languages : ['bn'], Length : 1
11/28/2021 16:29:16 - INFO - __main__ -   Adapter Names ['bn/wiki@ukp'], Length : 1
11/28/2021 16:29:16 - INFO - __main__ -   Language = bn
11/28/2021 16:29:16 - INFO - __main__ -   Adapter Name = bn/wiki@ukp
11/28/2021 16:29:22 - INFO - __main__ -   Language adapter for zh not found, using bn instead
11/28/2021 16:29:22 - INFO - __main__ -   Set active language adapter to bn
11/28/2021 16:29:22 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 16:29:22 - INFO - __main__ -   Adapter Languages = ['bn']
11/28/2021 16:29:22 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_zh_bert-base-multilingual-cased_128
11/28/2021 16:29:23 - INFO - __main__ -   ***** Running evaluation  in zh *****
11/28/2021 16:29:23 - INFO - __main__ -     Num examples = 3458
11/28/2021 16:29:23 - INFO - __main__ -     Batch size = 32
11/28/2021 16:29:23 - INFO - __main__ -   Batch number = 1
11/28/2021 16:29:23 - INFO - __main__ -   Batch number = 2
11/28/2021 16:29:23 - INFO - __main__ -   Batch number = 3
11/28/2021 16:29:23 - INFO - __main__ -   Batch number = 4
11/28/2021 16:29:23 - INFO - __main__ -   Batch number = 5
11/28/2021 16:29:24 - INFO - __main__ -   Batch number = 6
11/28/2021 16:29:24 - INFO - __main__ -   Batch number = 7
11/28/2021 16:29:24 - INFO - __main__ -   Batch number = 8
11/28/2021 16:29:24 - INFO - __main__ -   Batch number = 9
11/28/2021 16:29:24 - INFO - __main__ -   Batch number = 10
11/28/2021 16:29:24 - INFO - __main__ -   Batch number = 11
11/28/2021 16:29:24 - INFO - __main__ -   Batch number = 12
11/28/2021 16:29:25 - INFO - __main__ -   Batch number = 13
11/28/2021 16:29:25 - INFO - __main__ -   Batch number = 14
11/28/2021 16:29:25 - INFO - __main__ -   Batch number = 15
11/28/2021 16:29:25 - INFO - __main__ -   Batch number = 16
11/28/2021 16:29:25 - INFO - __main__ -   Batch number = 17
11/28/2021 16:29:25 - INFO - __main__ -   Batch number = 18
11/28/2021 16:29:25 - INFO - __main__ -   Batch number = 19
11/28/2021 16:29:26 - INFO - __main__ -   Batch number = 20
11/28/2021 16:29:26 - INFO - __main__ -   Batch number = 21
11/28/2021 16:29:26 - INFO - __main__ -   Batch number = 22
11/28/2021 16:29:26 - INFO - __main__ -   Batch number = 23
11/28/2021 16:29:26 - INFO - __main__ -   Batch number = 24
11/28/2021 16:29:26 - INFO - __main__ -   Batch number = 25
11/28/2021 16:29:26 - INFO - __main__ -   Batch number = 26
11/28/2021 16:29:27 - INFO - __main__ -   Batch number = 27
11/28/2021 16:29:27 - INFO - __main__ -   Batch number = 28
11/28/2021 16:29:27 - INFO - __main__ -   Batch number = 29
11/28/2021 16:29:27 - INFO - __main__ -   Batch number = 30
11/28/2021 16:29:27 - INFO - __main__ -   Batch number = 31
11/28/2021 16:29:27 - INFO - __main__ -   Batch number = 32
11/28/2021 16:29:27 - INFO - __main__ -   Batch number = 33
11/28/2021 16:29:28 - INFO - __main__ -   Batch number = 34
11/28/2021 16:29:28 - INFO - __main__ -   Batch number = 35
11/28/2021 16:29:28 - INFO - __main__ -   Batch number = 36
11/28/2021 16:29:28 - INFO - __main__ -   Batch number = 37
11/28/2021 16:29:28 - INFO - __main__ -   Batch number = 38
11/28/2021 16:29:28 - INFO - __main__ -   Batch number = 39
11/28/2021 16:29:29 - INFO - __main__ -   Batch number = 40
11/28/2021 16:29:29 - INFO - __main__ -   Batch number = 41
11/28/2021 16:29:29 - INFO - __main__ -   Batch number = 42
11/28/2021 16:29:29 - INFO - __main__ -   Batch number = 43
11/28/2021 16:29:29 - INFO - __main__ -   Batch number = 44
11/28/2021 16:29:29 - INFO - __main__ -   Batch number = 45
11/28/2021 16:29:29 - INFO - __main__ -   Batch number = 46
11/28/2021 16:29:30 - INFO - __main__ -   Batch number = 47
11/28/2021 16:29:30 - INFO - __main__ -   Batch number = 48
11/28/2021 16:29:30 - INFO - __main__ -   Batch number = 49
11/28/2021 16:29:30 - INFO - __main__ -   Batch number = 50
11/28/2021 16:29:30 - INFO - __main__ -   Batch number = 51
11/28/2021 16:29:30 - INFO - __main__ -   Batch number = 52
11/28/2021 16:29:31 - INFO - __main__ -   Batch number = 53
11/28/2021 16:29:31 - INFO - __main__ -   Batch number = 54
11/28/2021 16:29:31 - INFO - __main__ -   Batch number = 55
11/28/2021 16:29:31 - INFO - __main__ -   Batch number = 56
11/28/2021 16:29:31 - INFO - __main__ -   Batch number = 57
11/28/2021 16:29:31 - INFO - __main__ -   Batch number = 58
11/28/2021 16:29:32 - INFO - __main__ -   Batch number = 59
11/28/2021 16:29:32 - INFO - __main__ -   Batch number = 60
11/28/2021 16:29:32 - INFO - __main__ -   Batch number = 61
11/28/2021 16:29:32 - INFO - __main__ -   Batch number = 62
11/28/2021 16:29:32 - INFO - __main__ -   Batch number = 63
11/28/2021 16:29:32 - INFO - __main__ -   Batch number = 64
11/28/2021 16:29:32 - INFO - __main__ -   Batch number = 65
11/28/2021 16:29:33 - INFO - __main__ -   Batch number = 66
11/28/2021 16:29:33 - INFO - __main__ -   Batch number = 67
11/28/2021 16:29:33 - INFO - __main__ -   Batch number = 68
11/28/2021 16:29:33 - INFO - __main__ -   Batch number = 69
11/28/2021 16:29:33 - INFO - __main__ -   Batch number = 70
11/28/2021 16:29:33 - INFO - __main__ -   Batch number = 71
11/28/2021 16:29:34 - INFO - __main__ -   Batch number = 72
11/28/2021 16:29:34 - INFO - __main__ -   Batch number = 73
11/28/2021 16:29:34 - INFO - __main__ -   Batch number = 74
11/28/2021 16:29:34 - INFO - __main__ -   Batch number = 75
11/28/2021 16:29:34 - INFO - __main__ -   Batch number = 76
11/28/2021 16:29:34 - INFO - __main__ -   Batch number = 77
11/28/2021 16:29:35 - INFO - __main__ -   Batch number = 78
11/28/2021 16:29:35 - INFO - __main__ -   Batch number = 79
11/28/2021 16:29:35 - INFO - __main__ -   Batch number = 80
11/28/2021 16:29:35 - INFO - __main__ -   Batch number = 81
11/28/2021 16:29:35 - INFO - __main__ -   Batch number = 82
11/28/2021 16:29:35 - INFO - __main__ -   Batch number = 83
11/28/2021 16:29:36 - INFO - __main__ -   Batch number = 84
11/28/2021 16:29:36 - INFO - __main__ -   Batch number = 85
11/28/2021 16:29:36 - INFO - __main__ -   Batch number = 86
11/28/2021 16:29:36 - INFO - __main__ -   Batch number = 87
11/28/2021 16:29:37 - INFO - __main__ -   Batch number = 88
11/28/2021 16:29:37 - INFO - __main__ -   Batch number = 89
11/28/2021 16:29:37 - INFO - __main__ -   Batch number = 90
11/28/2021 16:29:38 - INFO - __main__ -   Batch number = 91
11/28/2021 16:29:38 - INFO - __main__ -   Batch number = 92
11/28/2021 16:29:38 - INFO - __main__ -   Batch number = 93
11/28/2021 16:29:39 - INFO - __main__ -   Batch number = 94
11/28/2021 16:29:39 - INFO - __main__ -   Batch number = 95
11/28/2021 16:29:39 - INFO - __main__ -   Batch number = 96
11/28/2021 16:29:40 - INFO - __main__ -   Batch number = 97
11/28/2021 16:29:40 - INFO - __main__ -   Batch number = 98
11/28/2021 16:29:40 - INFO - __main__ -   Batch number = 99
11/28/2021 16:29:41 - INFO - __main__ -   Batch number = 100
11/28/2021 16:29:41 - INFO - __main__ -   Batch number = 101
11/28/2021 16:29:41 - INFO - __main__ -   Batch number = 102
11/28/2021 16:29:42 - INFO - __main__ -   Batch number = 103
11/28/2021 16:29:42 - INFO - __main__ -   Batch number = 104
11/28/2021 16:29:42 - INFO - __main__ -   Batch number = 105
11/28/2021 16:29:43 - INFO - __main__ -   Batch number = 106
11/28/2021 16:29:43 - INFO - __main__ -   Batch number = 107
11/28/2021 16:29:43 - INFO - __main__ -   Batch number = 108
11/28/2021 16:29:44 - INFO - __main__ -   Batch number = 109
11/28/2021 16:29:45 - INFO - __main__ -   ***** Evaluation result  in zh *****
11/28/2021 16:29:45 - INFO - __main__ -     f1 = 0.6169134524506052
11/28/2021 16:29:45 - INFO - __main__ -     loss = 1.416018894506157
11/28/2021 16:29:45 - INFO - __main__ -     precision = 0.6255225161090119
11/28/2021 16:29:45 - INFO - __main__ -     recall = 0.6085381446228157
