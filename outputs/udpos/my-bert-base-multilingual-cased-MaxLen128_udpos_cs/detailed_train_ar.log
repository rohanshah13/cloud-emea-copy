PyTorch version 1.9.0+cu102 available.
11/21/2021 11:17:26 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ar', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/21/2021 11:17:26 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/21/2021 11:17:26 - INFO - __main__ -   Seed = 1
11/21/2021 11:17:26 - INFO - root -   save model
11/21/2021 11:17:26 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ar', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/21/2021 11:17:26 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/21/2021 11:17:28 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/21/2021 11:17:34 - INFO - __main__ -   Using lang2id = None
11/21/2021 11:17:34 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/21/2021 11:17:34 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
11/21/2021 11:17:34 - INFO - root -   Trying to decide if add adapter
11/21/2021 11:17:34 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_model_head.bin
11/21/2021 11:17:34 - INFO - root -   loading lang adpater cs/wiki@ukp
11/21/2021 11:17:34 - INFO - __main__ -   Adapter Languages : ['cs'], Length : 1
11/21/2021 11:17:34 - INFO - __main__ -   Adapter Names ['cs/wiki@ukp'], Length : 1
11/21/2021 11:17:34 - INFO - __main__ -   Language = cs
11/21/2021 11:17:34 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/cs/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_cs_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/adapter_config.json
Adding adapter 'cs' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/head_config.json
11/21/2021 11:17:39 - INFO - __main__ -   Language adapter for ar not found, using cs instead
11/21/2021 11:17:39 - INFO - __main__ -   Set active language adapter to cs
11/21/2021 11:17:39 - INFO - __main__ -   Args Adapter Weight = None
11/21/2021 11:17:39 - INFO - __main__ -   Adapter Languages = ['cs']
11/21/2021 11:17:39 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_ar_bert-base-multilingual-cased_128
11/21/2021 11:17:40 - INFO - __main__ -   ***** Running evaluation  in ar *****
11/21/2021 11:17:40 - INFO - __main__ -     Num examples = 1784
11/21/2021 11:17:40 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/56 [00:00<?, ?it/s]11/21/2021 11:17:40 - INFO - __main__ -   Batch number = 1
Evaluating:   2%|▏         | 1/56 [00:00<00:07,  7.06it/s]11/21/2021 11:17:40 - INFO - __main__ -   Batch number = 2
Evaluating:   4%|▎         | 2/56 [00:00<00:07,  7.21it/s]11/21/2021 11:17:40 - INFO - __main__ -   Batch number = 3
Evaluating:   5%|▌         | 3/56 [00:00<00:07,  7.26it/s]11/21/2021 11:17:40 - INFO - __main__ -   Batch number = 4
Evaluating:   7%|▋         | 4/56 [00:00<00:07,  7.25it/s]11/21/2021 11:17:40 - INFO - __main__ -   Batch number = 5
Evaluating:   9%|▉         | 5/56 [00:00<00:07,  7.25it/s]11/21/2021 11:17:40 - INFO - __main__ -   Batch number = 6
Evaluating:  11%|█         | 6/56 [00:00<00:07,  7.11it/s]11/21/2021 11:17:40 - INFO - __main__ -   Batch number = 7
Evaluating:  12%|█▎        | 7/56 [00:00<00:06,  7.14it/s]11/21/2021 11:17:41 - INFO - __main__ -   Batch number = 8
Evaluating:  14%|█▍        | 8/56 [00:01<00:06,  7.18it/s]11/21/2021 11:17:41 - INFO - __main__ -   Batch number = 9
Evaluating:  16%|█▌        | 9/56 [00:01<00:06,  7.19it/s]11/21/2021 11:17:41 - INFO - __main__ -   Batch number = 10
Evaluating:  18%|█▊        | 10/56 [00:01<00:06,  7.22it/s]11/21/2021 11:17:41 - INFO - __main__ -   Batch number = 11
Evaluating:  20%|█▉        | 11/56 [00:01<00:06,  7.22it/s]11/21/2021 11:17:41 - INFO - __main__ -   Batch number = 12
Evaluating:  21%|██▏       | 12/56 [00:01<00:06,  7.21it/s]11/21/2021 11:17:41 - INFO - __main__ -   Batch number = 13
Evaluating:  23%|██▎       | 13/56 [00:01<00:05,  7.19it/s]11/21/2021 11:17:41 - INFO - __main__ -   Batch number = 14
Evaluating:  25%|██▌       | 14/56 [00:01<00:05,  7.18it/s]11/21/2021 11:17:42 - INFO - __main__ -   Batch number = 15
Evaluating:  27%|██▋       | 15/56 [00:02<00:05,  7.16it/s]11/21/2021 11:17:42 - INFO - __main__ -   Batch number = 16
Evaluating:  29%|██▊       | 16/56 [00:02<00:05,  7.17it/s]11/21/2021 11:17:42 - INFO - __main__ -   Batch number = 17
Evaluating:  30%|███       | 17/56 [00:02<00:05,  7.11it/s]11/21/2021 11:17:42 - INFO - __main__ -   Batch number = 18
Evaluating:  32%|███▏      | 18/56 [00:02<00:05,  7.10it/s]11/21/2021 11:17:42 - INFO - __main__ -   Batch number = 19
Evaluating:  34%|███▍      | 19/56 [00:02<00:05,  7.11it/s]11/21/2021 11:17:42 - INFO - __main__ -   Batch number = 20
Evaluating:  36%|███▌      | 20/56 [00:02<00:05,  7.10it/s]11/21/2021 11:17:42 - INFO - __main__ -   Batch number = 21
Evaluating:  38%|███▊      | 21/56 [00:02<00:04,  7.06it/s]11/21/2021 11:17:43 - INFO - __main__ -   Batch number = 22
Evaluating:  39%|███▉      | 22/56 [00:03<00:04,  7.02it/s]11/21/2021 11:17:43 - INFO - __main__ -   Batch number = 23
Evaluating:  41%|████      | 23/56 [00:03<00:04,  7.02it/s]11/21/2021 11:17:43 - INFO - __main__ -   Batch number = 24
Evaluating:  43%|████▎     | 24/56 [00:03<00:04,  6.97it/s]11/21/2021 11:17:43 - INFO - __main__ -   Batch number = 25
Evaluating:  45%|████▍     | 25/56 [00:03<00:04,  6.97it/s]11/21/2021 11:17:43 - INFO - __main__ -   Batch number = 26
Evaluating:  46%|████▋     | 26/56 [00:03<00:04,  6.98it/s]11/21/2021 11:17:43 - INFO - __main__ -   Batch number = 27
Evaluating:  48%|████▊     | 27/56 [00:03<00:04,  6.95it/s]11/21/2021 11:17:43 - INFO - __main__ -   Batch number = 28
Evaluating:  50%|█████     | 28/56 [00:03<00:04,  6.94it/s]11/21/2021 11:17:44 - INFO - __main__ -   Batch number = 29
Evaluating:  52%|█████▏    | 29/56 [00:04<00:03,  6.92it/s]11/21/2021 11:17:44 - INFO - __main__ -   Batch number = 30
Evaluating:  54%|█████▎    | 30/56 [00:04<00:03,  6.88it/s]11/21/2021 11:17:44 - INFO - __main__ -   Batch number = 31
Evaluating:  55%|█████▌    | 31/56 [00:04<00:03,  6.87it/s]11/21/2021 11:17:44 - INFO - __main__ -   Batch number = 32
Evaluating:  57%|█████▋    | 32/56 [00:04<00:03,  6.87it/s]11/21/2021 11:17:44 - INFO - __main__ -   Batch number = 33
Evaluating:  59%|█████▉    | 33/56 [00:04<00:03,  6.85it/s]11/21/2021 11:17:44 - INFO - __main__ -   Batch number = 34
Evaluating:  61%|██████    | 34/56 [00:04<00:03,  6.84it/s]11/21/2021 11:17:44 - INFO - __main__ -   Batch number = 35
Evaluating:  62%|██████▎   | 35/56 [00:04<00:03,  6.85it/s]11/21/2021 11:17:45 - INFO - __main__ -   Batch number = 36
Evaluating:  64%|██████▍   | 36/56 [00:05<00:02,  6.70it/s]11/21/2021 11:17:45 - INFO - __main__ -   Batch number = 37
Evaluating:  66%|██████▌   | 37/56 [00:05<00:02,  6.74it/s]11/21/2021 11:17:45 - INFO - __main__ -   Batch number = 38
Evaluating:  68%|██████▊   | 38/56 [00:05<00:02,  6.76it/s]11/21/2021 11:17:45 - INFO - __main__ -   Batch number = 39
Evaluating:  70%|██████▉   | 39/56 [00:05<00:02,  6.77it/s]11/21/2021 11:17:45 - INFO - __main__ -   Batch number = 40
Evaluating:  71%|███████▏  | 40/56 [00:05<00:02,  6.78it/s]11/21/2021 11:17:45 - INFO - __main__ -   Batch number = 41
Evaluating:  73%|███████▎  | 41/56 [00:05<00:02,  6.69it/s]11/21/2021 11:17:45 - INFO - __main__ -   Batch number = 42
Evaluating:  75%|███████▌  | 42/56 [00:06<00:02,  6.66it/s]11/21/2021 11:17:46 - INFO - __main__ -   Batch number = 43
Evaluating:  77%|███████▋  | 43/56 [00:06<00:01,  6.66it/s]11/21/2021 11:17:46 - INFO - __main__ -   Batch number = 44
Evaluating:  79%|███████▊  | 44/56 [00:06<00:01,  6.66it/s]11/21/2021 11:17:46 - INFO - __main__ -   Batch number = 45
Evaluating:  80%|████████  | 45/56 [00:06<00:01,  6.65it/s]11/21/2021 11:17:46 - INFO - __main__ -   Batch number = 46
Evaluating:  82%|████████▏ | 46/56 [00:06<00:01,  6.67it/s]11/21/2021 11:17:46 - INFO - __main__ -   Batch number = 47
Evaluating:  84%|████████▍ | 47/56 [00:06<00:01,  6.68it/s]11/21/2021 11:17:46 - INFO - __main__ -   Batch number = 48
Evaluating:  86%|████████▌ | 48/56 [00:06<00:01,  6.68it/s]11/21/2021 11:17:46 - INFO - __main__ -   Batch number = 49
Evaluating:  88%|████████▊ | 49/56 [00:07<00:01,  6.66it/s]11/21/2021 11:17:47 - INFO - __main__ -   Batch number = 50
Evaluating:  89%|████████▉ | 50/56 [00:07<00:00,  6.65it/s]11/21/2021 11:17:47 - INFO - __main__ -   Batch number = 51
Evaluating:  91%|█████████ | 51/56 [00:07<00:00,  6.64it/s]11/21/2021 11:17:47 - INFO - __main__ -   Batch number = 52
Evaluating:  93%|█████████▎| 52/56 [00:07<00:00,  6.64it/s]11/21/2021 11:17:47 - INFO - __main__ -   Batch number = 53
Evaluating:  95%|█████████▍| 53/56 [00:07<00:00,  6.63it/s]11/21/2021 11:17:47 - INFO - __main__ -   Batch number = 54
Evaluating:  96%|█████████▋| 54/56 [00:07<00:00,  6.60it/s]11/21/2021 11:17:47 - INFO - __main__ -   Batch number = 55
Evaluating:  98%|█████████▊| 55/56 [00:07<00:00,  6.62it/s]11/21/2021 11:17:48 - INFO - __main__ -   Batch number = 56
Evaluating: 100%|██████████| 56/56 [00:08<00:00,  7.06it/s]Evaluating: 100%|██████████| 56/56 [00:08<00:00,  6.92it/s]
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/21/2021 11:17:49 - INFO - __main__ -   ***** Evaluation result  in ar *****
11/21/2021 11:17:49 - INFO - __main__ -     f1 = 0.5873128824830836
11/21/2021 11:17:49 - INFO - __main__ -     loss = 1.4576366820505686
11/21/2021 11:17:49 - INFO - __main__ -     precision = 0.5771206265256306
11/21/2021 11:17:49 - INFO - __main__ -     recall = 0.5978716118325739
23.11user 11.05system 0:25.13elapsed 135%CPU (0avgtext+0avgdata 3874452maxresident)k
0inputs+520outputs (0major+1345020minor)pagefaults 0swaps
PyTorch version 1.9.0+cu102 available.
11/21/2021 11:17:51 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ar', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/21/2021 11:17:51 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/21/2021 11:17:51 - INFO - __main__ -   Seed = 2
11/21/2021 11:17:51 - INFO - root -   save model
11/21/2021 11:17:51 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ar', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/21/2021 11:17:51 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/21/2021 11:17:53 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/21/2021 11:17:59 - INFO - __main__ -   Using lang2id = None
11/21/2021 11:17:59 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/21/2021 11:17:59 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
11/21/2021 11:17:59 - INFO - root -   Trying to decide if add adapter
11/21/2021 11:17:59 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_model_head.bin
11/21/2021 11:17:59 - INFO - root -   loading lang adpater cs/wiki@ukp
11/21/2021 11:17:59 - INFO - __main__ -   Adapter Languages : ['cs'], Length : 1
11/21/2021 11:17:59 - INFO - __main__ -   Adapter Names ['cs/wiki@ukp'], Length : 1
11/21/2021 11:17:59 - INFO - __main__ -   Language = cs
11/21/2021 11:17:59 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/cs/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_cs_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/adapter_config.json
Adding adapter 'cs' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/head_config.json
11/21/2021 11:18:04 - INFO - __main__ -   Language adapter for ar not found, using cs instead
11/21/2021 11:18:04 - INFO - __main__ -   Set active language adapter to cs
11/21/2021 11:18:04 - INFO - __main__ -   Args Adapter Weight = None
11/21/2021 11:18:04 - INFO - __main__ -   Adapter Languages = ['cs']
11/21/2021 11:18:04 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_ar_bert-base-multilingual-cased_128
11/21/2021 11:18:05 - INFO - __main__ -   ***** Running evaluation  in ar *****
11/21/2021 11:18:05 - INFO - __main__ -     Num examples = 1784
11/21/2021 11:18:05 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/56 [00:00<?, ?it/s]11/21/2021 11:18:05 - INFO - __main__ -   Batch number = 1
Evaluating:   2%|▏         | 1/56 [00:00<00:08,  6.63it/s]11/21/2021 11:18:05 - INFO - __main__ -   Batch number = 2
Evaluating:   4%|▎         | 2/56 [00:00<00:07,  6.95it/s]11/21/2021 11:18:05 - INFO - __main__ -   Batch number = 3
Evaluating:   5%|▌         | 3/56 [00:00<00:07,  7.13it/s]11/21/2021 11:18:05 - INFO - __main__ -   Batch number = 4
Evaluating:   7%|▋         | 4/56 [00:00<00:07,  7.17it/s]11/21/2021 11:18:05 - INFO - __main__ -   Batch number = 5
Evaluating:   9%|▉         | 5/56 [00:00<00:07,  7.22it/s]11/21/2021 11:18:05 - INFO - __main__ -   Batch number = 6
Evaluating:  11%|█         | 6/56 [00:00<00:06,  7.25it/s]11/21/2021 11:18:05 - INFO - __main__ -   Batch number = 7
Evaluating:  12%|█▎        | 7/56 [00:00<00:06,  7.27it/s]11/21/2021 11:18:06 - INFO - __main__ -   Batch number = 8
Evaluating:  14%|█▍        | 8/56 [00:01<00:06,  7.29it/s]11/21/2021 11:18:06 - INFO - __main__ -   Batch number = 9
Evaluating:  16%|█▌        | 9/56 [00:01<00:06,  7.28it/s]11/21/2021 11:18:06 - INFO - __main__ -   Batch number = 10
Evaluating:  18%|█▊        | 10/56 [00:01<00:06,  7.28it/s]11/21/2021 11:18:06 - INFO - __main__ -   Batch number = 11
Evaluating:  20%|█▉        | 11/56 [00:01<00:06,  7.26it/s]11/21/2021 11:18:06 - INFO - __main__ -   Batch number = 12
Evaluating:  21%|██▏       | 12/56 [00:01<00:06,  7.24it/s]11/21/2021 11:18:06 - INFO - __main__ -   Batch number = 13
Evaluating:  23%|██▎       | 13/56 [00:01<00:05,  7.24it/s]11/21/2021 11:18:06 - INFO - __main__ -   Batch number = 14
Evaluating:  25%|██▌       | 14/56 [00:01<00:05,  7.23it/s]11/21/2021 11:18:07 - INFO - __main__ -   Batch number = 15
Evaluating:  27%|██▋       | 15/56 [00:02<00:05,  7.22it/s]11/21/2021 11:18:07 - INFO - __main__ -   Batch number = 16
Evaluating:  29%|██▊       | 16/56 [00:02<00:05,  7.23it/s]11/21/2021 11:18:07 - INFO - __main__ -   Batch number = 17
Evaluating:  30%|███       | 17/56 [00:02<00:05,  7.24it/s]11/21/2021 11:18:07 - INFO - __main__ -   Batch number = 18
Evaluating:  32%|███▏      | 18/56 [00:02<00:05,  7.23it/s]11/21/2021 11:18:07 - INFO - __main__ -   Batch number = 19
Evaluating:  34%|███▍      | 19/56 [00:02<00:05,  7.25it/s]11/21/2021 11:18:07 - INFO - __main__ -   Batch number = 20
Evaluating:  36%|███▌      | 20/56 [00:02<00:04,  7.25it/s]11/21/2021 11:18:07 - INFO - __main__ -   Batch number = 21
Evaluating:  38%|███▊      | 21/56 [00:02<00:04,  7.24it/s]11/21/2021 11:18:08 - INFO - __main__ -   Batch number = 22
Evaluating:  39%|███▉      | 22/56 [00:03<00:04,  7.22it/s]11/21/2021 11:18:08 - INFO - __main__ -   Batch number = 23
Evaluating:  41%|████      | 23/56 [00:03<00:04,  7.22it/s]11/21/2021 11:18:08 - INFO - __main__ -   Batch number = 24
Evaluating:  43%|████▎     | 24/56 [00:03<00:04,  7.19it/s]11/21/2021 11:18:08 - INFO - __main__ -   Batch number = 25
Evaluating:  45%|████▍     | 25/56 [00:03<00:04,  7.18it/s]11/21/2021 11:18:08 - INFO - __main__ -   Batch number = 26
Evaluating:  46%|████▋     | 26/56 [00:03<00:04,  7.18it/s]11/21/2021 11:18:08 - INFO - __main__ -   Batch number = 27
Evaluating:  48%|████▊     | 27/56 [00:03<00:04,  7.13it/s]11/21/2021 11:18:08 - INFO - __main__ -   Batch number = 28
Evaluating:  50%|█████     | 28/56 [00:03<00:03,  7.11it/s]11/21/2021 11:18:08 - INFO - __main__ -   Batch number = 29
Evaluating:  52%|█████▏    | 29/56 [00:04<00:03,  7.10it/s]11/21/2021 11:18:09 - INFO - __main__ -   Batch number = 30
Evaluating:  54%|█████▎    | 30/56 [00:04<00:03,  7.08it/s]11/21/2021 11:18:09 - INFO - __main__ -   Batch number = 31
Evaluating:  55%|█████▌    | 31/56 [00:04<00:03,  7.07it/s]11/21/2021 11:18:09 - INFO - __main__ -   Batch number = 32
Evaluating:  57%|█████▋    | 32/56 [00:04<00:03,  7.08it/s]11/21/2021 11:18:09 - INFO - __main__ -   Batch number = 33
Evaluating:  59%|█████▉    | 33/56 [00:04<00:03,  7.07it/s]11/21/2021 11:18:09 - INFO - __main__ -   Batch number = 34
Evaluating:  61%|██████    | 34/56 [00:04<00:03,  7.07it/s]11/21/2021 11:18:09 - INFO - __main__ -   Batch number = 35
Evaluating:  62%|██████▎   | 35/56 [00:04<00:02,  7.06it/s]11/21/2021 11:18:09 - INFO - __main__ -   Batch number = 36
Evaluating:  64%|██████▍   | 36/56 [00:05<00:02,  7.04it/s]11/21/2021 11:18:10 - INFO - __main__ -   Batch number = 37
Evaluating:  66%|██████▌   | 37/56 [00:05<00:02,  6.95it/s]11/21/2021 11:18:10 - INFO - __main__ -   Batch number = 38
Evaluating:  68%|██████▊   | 38/56 [00:05<00:02,  6.98it/s]11/21/2021 11:18:10 - INFO - __main__ -   Batch number = 39
Evaluating:  70%|██████▉   | 39/56 [00:05<00:02,  7.01it/s]11/21/2021 11:18:10 - INFO - __main__ -   Batch number = 40
Evaluating:  71%|███████▏  | 40/56 [00:05<00:02,  7.00it/s]11/21/2021 11:18:10 - INFO - __main__ -   Batch number = 41
Evaluating:  73%|███████▎  | 41/56 [00:05<00:03,  4.69it/s]11/21/2021 11:18:11 - INFO - __main__ -   Batch number = 42
Evaluating:  75%|███████▌  | 42/56 [00:06<00:02,  5.22it/s]11/21/2021 11:18:11 - INFO - __main__ -   Batch number = 43
Evaluating:  77%|███████▋  | 43/56 [00:06<00:02,  5.67it/s]11/21/2021 11:18:11 - INFO - __main__ -   Batch number = 44
Evaluating:  79%|███████▊  | 44/56 [00:06<00:01,  6.04it/s]11/21/2021 11:18:11 - INFO - __main__ -   Batch number = 45
Evaluating:  80%|████████  | 45/56 [00:06<00:01,  6.32it/s]11/21/2021 11:18:11 - INFO - __main__ -   Batch number = 46
Evaluating:  82%|████████▏ | 46/56 [00:06<00:01,  6.54it/s]11/21/2021 11:18:11 - INFO - __main__ -   Batch number = 47
Evaluating:  84%|████████▍ | 47/56 [00:06<00:01,  6.69it/s]11/21/2021 11:18:11 - INFO - __main__ -   Batch number = 48
Evaluating:  86%|████████▌ | 48/56 [00:06<00:01,  6.77it/s]11/21/2021 11:18:12 - INFO - __main__ -   Batch number = 49
Evaluating:  88%|████████▊ | 49/56 [00:07<00:01,  6.84it/s]11/21/2021 11:18:12 - INFO - __main__ -   Batch number = 50
Evaluating:  89%|████████▉ | 50/56 [00:07<00:00,  6.84it/s]11/21/2021 11:18:12 - INFO - __main__ -   Batch number = 51
Evaluating:  91%|█████████ | 51/56 [00:07<00:00,  6.82it/s]11/21/2021 11:18:12 - INFO - __main__ -   Batch number = 52
Evaluating:  93%|█████████▎| 52/56 [00:07<00:00,  6.84it/s]11/21/2021 11:18:12 - INFO - __main__ -   Batch number = 53
Evaluating:  95%|█████████▍| 53/56 [00:07<00:00,  6.88it/s]11/21/2021 11:18:12 - INFO - __main__ -   Batch number = 54
Evaluating:  96%|█████████▋| 54/56 [00:07<00:00,  6.89it/s]11/21/2021 11:18:12 - INFO - __main__ -   Batch number = 55
Evaluating:  98%|█████████▊| 55/56 [00:07<00:00,  6.88it/s]11/21/2021 11:18:13 - INFO - __main__ -   Batch number = 56
Evaluating: 100%|██████████| 56/56 [00:08<00:00,  7.36it/s]Evaluating: 100%|██████████| 56/56 [00:08<00:00,  6.92it/s]
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/21/2021 11:18:14 - INFO - __main__ -   ***** Evaluation result  in ar *****
11/21/2021 11:18:14 - INFO - __main__ -     f1 = 0.5663504974195956
11/21/2021 11:18:14 - INFO - __main__ -     loss = 1.7338247373700142
11/21/2021 11:18:14 - INFO - __main__ -     precision = 0.5537017464391766
11/21/2021 11:18:14 - INFO - __main__ -     recall = 0.5795906540578984
22.50user 10.50system 0:25.02elapsed 131%CPU (0avgtext+0avgdata 3870116maxresident)k
0inputs+512outputs (0major+1349850minor)pagefaults 0swaps
PyTorch version 1.9.0+cu102 available.
11/21/2021 11:18:16 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ar', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/21/2021 11:18:16 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/21/2021 11:18:16 - INFO - __main__ -   Seed = 3
11/21/2021 11:18:16 - INFO - root -   save model
11/21/2021 11:18:16 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ar', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/21/2021 11:18:16 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/21/2021 11:18:18 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/21/2021 11:18:24 - INFO - __main__ -   Using lang2id = None
11/21/2021 11:18:24 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/21/2021 11:18:24 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
11/21/2021 11:18:24 - INFO - root -   Trying to decide if add adapter
11/21/2021 11:18:24 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_model_head.bin
11/21/2021 11:18:24 - INFO - root -   loading lang adpater cs/wiki@ukp
11/21/2021 11:18:24 - INFO - __main__ -   Adapter Languages : ['cs'], Length : 1
11/21/2021 11:18:24 - INFO - __main__ -   Adapter Names ['cs/wiki@ukp'], Length : 1
11/21/2021 11:18:24 - INFO - __main__ -   Language = cs
11/21/2021 11:18:24 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/cs/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_cs_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/adapter_config.json
Adding adapter 'cs' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/head_config.json
11/21/2021 11:18:30 - INFO - __main__ -   Language adapter for ar not found, using cs instead
11/21/2021 11:18:30 - INFO - __main__ -   Set active language adapter to cs
11/21/2021 11:18:30 - INFO - __main__ -   Args Adapter Weight = None
11/21/2021 11:18:30 - INFO - __main__ -   Adapter Languages = ['cs']
11/21/2021 11:18:30 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_ar_bert-base-multilingual-cased_128
11/21/2021 11:18:30 - INFO - __main__ -   ***** Running evaluation  in ar *****
11/21/2021 11:18:30 - INFO - __main__ -     Num examples = 1784
11/21/2021 11:18:30 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/56 [00:00<?, ?it/s]11/21/2021 11:18:30 - INFO - __main__ -   Batch number = 1
Evaluating:   2%|▏         | 1/56 [00:00<00:07,  7.25it/s]11/21/2021 11:18:30 - INFO - __main__ -   Batch number = 2
Evaluating:   4%|▎         | 2/56 [00:00<00:07,  7.28it/s]11/21/2021 11:18:30 - INFO - __main__ -   Batch number = 3
Evaluating:   5%|▌         | 3/56 [00:00<00:07,  7.29it/s]11/21/2021 11:18:30 - INFO - __main__ -   Batch number = 4
Evaluating:   7%|▋         | 4/56 [00:00<00:08,  6.47it/s]11/21/2021 11:18:30 - INFO - __main__ -   Batch number = 5
Evaluating:   9%|▉         | 5/56 [00:00<00:07,  6.74it/s]11/21/2021 11:18:31 - INFO - __main__ -   Batch number = 6
Evaluating:  11%|█         | 6/56 [00:00<00:07,  6.92it/s]11/21/2021 11:18:31 - INFO - __main__ -   Batch number = 7
Evaluating:  12%|█▎        | 7/56 [00:01<00:06,  7.02it/s]11/21/2021 11:18:31 - INFO - __main__ -   Batch number = 8
Evaluating:  14%|█▍        | 8/56 [00:01<00:06,  7.11it/s]11/21/2021 11:18:31 - INFO - __main__ -   Batch number = 9
Evaluating:  16%|█▌        | 9/56 [00:01<00:06,  7.14it/s]11/21/2021 11:18:31 - INFO - __main__ -   Batch number = 10
Evaluating:  18%|█▊        | 10/56 [00:01<00:06,  7.16it/s]11/21/2021 11:18:31 - INFO - __main__ -   Batch number = 11
Evaluating:  20%|█▉        | 11/56 [00:01<00:06,  7.17it/s]11/21/2021 11:18:31 - INFO - __main__ -   Batch number = 12
Evaluating:  21%|██▏       | 12/56 [00:01<00:06,  7.19it/s]11/21/2021 11:18:32 - INFO - __main__ -   Batch number = 13
Evaluating:  23%|██▎       | 13/56 [00:01<00:05,  7.21it/s]11/21/2021 11:18:32 - INFO - __main__ -   Batch number = 14
Evaluating:  25%|██▌       | 14/56 [00:01<00:05,  7.21it/s]11/21/2021 11:18:32 - INFO - __main__ -   Batch number = 15
Evaluating:  27%|██▋       | 15/56 [00:02<00:05,  7.20it/s]11/21/2021 11:18:32 - INFO - __main__ -   Batch number = 16
Evaluating:  29%|██▊       | 16/56 [00:02<00:05,  7.21it/s]11/21/2021 11:18:32 - INFO - __main__ -   Batch number = 17
Evaluating:  30%|███       | 17/56 [00:02<00:05,  7.19it/s]11/21/2021 11:18:32 - INFO - __main__ -   Batch number = 18
Evaluating:  32%|███▏      | 18/56 [00:02<00:05,  7.18it/s]11/21/2021 11:18:32 - INFO - __main__ -   Batch number = 19
Evaluating:  34%|███▍      | 19/56 [00:02<00:05,  7.19it/s]11/21/2021 11:18:32 - INFO - __main__ -   Batch number = 20
Evaluating:  36%|███▌      | 20/56 [00:02<00:05,  7.18it/s]11/21/2021 11:18:33 - INFO - __main__ -   Batch number = 21
Evaluating:  38%|███▊      | 21/56 [00:02<00:04,  7.16it/s]11/21/2021 11:18:33 - INFO - __main__ -   Batch number = 22
Evaluating:  39%|███▉      | 22/56 [00:03<00:04,  7.15it/s]11/21/2021 11:18:33 - INFO - __main__ -   Batch number = 23
Evaluating:  41%|████      | 23/56 [00:03<00:04,  7.13it/s]11/21/2021 11:18:33 - INFO - __main__ -   Batch number = 24
Evaluating:  43%|████▎     | 24/56 [00:03<00:04,  7.08it/s]11/21/2021 11:18:33 - INFO - __main__ -   Batch number = 25
Evaluating:  45%|████▍     | 25/56 [00:03<00:04,  7.03it/s]11/21/2021 11:18:33 - INFO - __main__ -   Batch number = 26
Evaluating:  46%|████▋     | 26/56 [00:03<00:04,  7.02it/s]11/21/2021 11:18:33 - INFO - __main__ -   Batch number = 27
Evaluating:  48%|████▊     | 27/56 [00:03<00:04,  7.03it/s]11/21/2021 11:18:34 - INFO - __main__ -   Batch number = 28
Evaluating:  50%|█████     | 28/56 [00:03<00:04,  6.99it/s]11/21/2021 11:18:34 - INFO - __main__ -   Batch number = 29
Evaluating:  52%|█████▏    | 29/56 [00:04<00:03,  6.97it/s]11/21/2021 11:18:34 - INFO - __main__ -   Batch number = 30
Evaluating:  54%|█████▎    | 30/56 [00:04<00:03,  6.91it/s]11/21/2021 11:18:34 - INFO - __main__ -   Batch number = 31
Evaluating:  55%|█████▌    | 31/56 [00:04<00:03,  6.92it/s]11/21/2021 11:18:34 - INFO - __main__ -   Batch number = 32
Evaluating:  57%|█████▋    | 32/56 [00:04<00:03,  6.91it/s]11/21/2021 11:18:34 - INFO - __main__ -   Batch number = 33
Evaluating:  59%|█████▉    | 33/56 [00:04<00:03,  6.88it/s]11/21/2021 11:18:35 - INFO - __main__ -   Batch number = 34
Evaluating:  61%|██████    | 34/56 [00:04<00:03,  6.86it/s]11/21/2021 11:18:35 - INFO - __main__ -   Batch number = 35
Evaluating:  62%|██████▎   | 35/56 [00:04<00:03,  6.80it/s]11/21/2021 11:18:35 - INFO - __main__ -   Batch number = 36
Evaluating:  64%|██████▍   | 36/56 [00:05<00:02,  6.74it/s]11/21/2021 11:18:35 - INFO - __main__ -   Batch number = 37
Evaluating:  66%|██████▌   | 37/56 [00:05<00:02,  6.69it/s]11/21/2021 11:18:35 - INFO - __main__ -   Batch number = 38
Evaluating:  68%|██████▊   | 38/56 [00:05<00:02,  6.66it/s]11/21/2021 11:18:35 - INFO - __main__ -   Batch number = 39
Evaluating:  70%|██████▉   | 39/56 [00:05<00:02,  6.63it/s]11/21/2021 11:18:35 - INFO - __main__ -   Batch number = 40
Evaluating:  71%|███████▏  | 40/56 [00:05<00:02,  6.62it/s]11/21/2021 11:18:36 - INFO - __main__ -   Batch number = 41
Evaluating:  73%|███████▎  | 41/56 [00:05<00:02,  6.64it/s]11/21/2021 11:18:36 - INFO - __main__ -   Batch number = 42
Evaluating:  75%|███████▌  | 42/56 [00:06<00:02,  6.56it/s]11/21/2021 11:18:36 - INFO - __main__ -   Batch number = 43
Evaluating:  77%|███████▋  | 43/56 [00:06<00:01,  6.56it/s]11/21/2021 11:18:36 - INFO - __main__ -   Batch number = 44
Evaluating:  79%|███████▊  | 44/56 [00:06<00:01,  6.60it/s]11/21/2021 11:18:36 - INFO - __main__ -   Batch number = 45
Evaluating:  80%|████████  | 45/56 [00:06<00:01,  6.61it/s]11/21/2021 11:18:36 - INFO - __main__ -   Batch number = 46
Evaluating:  82%|████████▏ | 46/56 [00:06<00:01,  6.62it/s]11/21/2021 11:18:36 - INFO - __main__ -   Batch number = 47
Evaluating:  84%|████████▍ | 47/56 [00:06<00:01,  6.62it/s]11/21/2021 11:18:37 - INFO - __main__ -   Batch number = 48
Evaluating:  86%|████████▌ | 48/56 [00:06<00:01,  6.63it/s]11/21/2021 11:18:37 - INFO - __main__ -   Batch number = 49
Evaluating:  88%|████████▊ | 49/56 [00:07<00:01,  6.66it/s]11/21/2021 11:18:37 - INFO - __main__ -   Batch number = 50
Evaluating:  89%|████████▉ | 50/56 [00:07<00:00,  6.71it/s]11/21/2021 11:18:37 - INFO - __main__ -   Batch number = 51
Evaluating:  91%|█████████ | 51/56 [00:07<00:00,  6.73it/s]11/21/2021 11:18:37 - INFO - __main__ -   Batch number = 52
Evaluating:  93%|█████████▎| 52/56 [00:07<00:00,  6.72it/s]11/21/2021 11:18:37 - INFO - __main__ -   Batch number = 53
Evaluating:  95%|█████████▍| 53/56 [00:07<00:00,  6.75it/s]11/21/2021 11:18:38 - INFO - __main__ -   Batch number = 54
Evaluating:  96%|█████████▋| 54/56 [00:07<00:00,  6.67it/s]11/21/2021 11:18:38 - INFO - __main__ -   Batch number = 55
Evaluating:  98%|█████████▊| 55/56 [00:07<00:00,  6.62it/s]11/21/2021 11:18:38 - INFO - __main__ -   Batch number = 56
Evaluating: 100%|██████████| 56/56 [00:08<00:00,  7.06it/s]Evaluating: 100%|██████████| 56/56 [00:08<00:00,  6.91it/s]
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/21/2021 11:18:39 - INFO - __main__ -   ***** Evaluation result  in ar *****
11/21/2021 11:18:39 - INFO - __main__ -     f1 = 0.5724269130721745
11/21/2021 11:18:39 - INFO - __main__ -     loss = 1.4918592923453875
11/21/2021 11:18:39 - INFO - __main__ -     precision = 0.5626335605983515
11/21/2021 11:18:39 - INFO - __main__ -     recall = 0.5825672365197693
23.52user 10.35system 0:25.07elapsed 135%CPU (0avgtext+0avgdata 3867616maxresident)k
0inputs+512outputs (0major+1192780minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:09:05 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='cdo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:09:05 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:09:05 - INFO - __main__ -   Seed = 1
11/28/2021 01:09:05 - INFO - root -   save model
11/28/2021 01:09:05 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='cdo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:09:05 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:09:08 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:09:14 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:09:14 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:09:14 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
11/28/2021 01:09:14 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:09:14 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:09:14 - INFO - root -   loading lang adpater cs/wiki@ukp
11/28/2021 01:09:14 - INFO - __main__ -   Adapter Languages : ['cs'], Length : 1
11/28/2021 01:09:14 - INFO - __main__ -   Adapter Names ['cs/wiki@ukp'], Length : 1
11/28/2021 01:09:14 - INFO - __main__ -   Language = cs
11/28/2021 01:09:14 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/cs/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_cs_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/adapter_config.json
Adding adapter 'cs' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/head_config.json
11/28/2021 01:09:24 - INFO - __main__ -   Language cdo, split test does not exist
15.51user 6.49system 0:21.79elapsed 100%CPU (0avgtext+0avgdata 3987076maxresident)k
24inputs+48outputs (0major+1583227minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:09:27 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='cdo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:09:27 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:09:27 - INFO - __main__ -   Seed = 2
11/28/2021 01:09:27 - INFO - root -   save model
11/28/2021 01:09:27 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='cdo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:09:27 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:09:30 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:09:36 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:09:36 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:09:36 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
11/28/2021 01:09:36 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:09:36 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:09:36 - INFO - root -   loading lang adpater cs/wiki@ukp
11/28/2021 01:09:36 - INFO - __main__ -   Adapter Languages : ['cs'], Length : 1
11/28/2021 01:09:36 - INFO - __main__ -   Adapter Names ['cs/wiki@ukp'], Length : 1
11/28/2021 01:09:36 - INFO - __main__ -   Language = cs
11/28/2021 01:09:36 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/cs/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_cs_wiki_pfeiffer.zip.
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:09:38 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='gn', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:09:38 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:09:38 - INFO - __main__ -   Seed = 1
11/28/2021 01:09:38 - INFO - root -   save model
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/adapter_config.json
11/28/2021 01:09:38 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='gn', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:09:38 - INFO - __main__ -   Loading pretrained model and tokenizer
Adding adapter 'cs' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/head_config.json
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:09:40 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:09:46 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:09:46 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:09:46 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
11/28/2021 01:09:46 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:09:46 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:09:46 - INFO - root -   loading lang adpater cs/wiki@ukp
11/28/2021 01:09:46 - INFO - __main__ -   Adapter Languages : ['cs'], Length : 1
11/28/2021 01:09:46 - INFO - __main__ -   Adapter Names ['cs/wiki@ukp'], Length : 1
11/28/2021 01:09:46 - INFO - __main__ -   Language = cs
11/28/2021 01:09:46 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/cs/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_cs_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/adapter_config.json
Adding adapter 'cs' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/head_config.json
11/28/2021 01:09:49 - INFO - __main__ -   Language cdo, split test does not exist
19.23user 7.44system 0:24.38elapsed 109%CPU (0avgtext+0avgdata 3984128maxresident)k
0inputs+48outputs (0major+1602244minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:09:51 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='cdo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:09:51 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:09:51 - INFO - __main__ -   Seed = 3
11/28/2021 01:09:51 - INFO - root -   save model
11/28/2021 01:09:51 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='cdo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:09:51 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

11/28/2021 01:09:53 - INFO - __main__ -   Language gn, split test does not exist
loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:09:54 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
14.33user 6.44system 0:18.46elapsed 112%CPU (0avgtext+0avgdata 3986812maxresident)k
0inputs+72outputs (0major+1475242minor)pagefaults 0swaps
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:09:56 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='gn', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:09:56 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:09:56 - INFO - __main__ -   Seed = 2
11/28/2021 01:09:56 - INFO - root -   save model
11/28/2021 01:09:56 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='gn', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:09:56 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:09:59 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:10:00 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:10:00 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:10:00 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
11/28/2021 01:10:00 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:10:00 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:10:00 - INFO - root -   loading lang adpater cs/wiki@ukp
11/28/2021 01:10:00 - INFO - __main__ -   Adapter Languages : ['cs'], Length : 1
11/28/2021 01:10:00 - INFO - __main__ -   Adapter Names ['cs/wiki@ukp'], Length : 1
11/28/2021 01:10:00 - INFO - __main__ -   Language = cs
11/28/2021 01:10:00 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/cs/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_cs_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/adapter_config.json
Adding adapter 'cs' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/head_config.json
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:10:05 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:10:05 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:10:05 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
11/28/2021 01:10:05 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:10:05 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:10:05 - INFO - root -   loading lang adpater cs/wiki@ukp
11/28/2021 01:10:05 - INFO - __main__ -   Adapter Languages : ['cs'], Length : 1
11/28/2021 01:10:05 - INFO - __main__ -   Adapter Names ['cs/wiki@ukp'], Length : 1
11/28/2021 01:10:05 - INFO - __main__ -   Language = cs
11/28/2021 01:10:05 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/cs/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_cs_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/adapter_config.json
Adding adapter 'cs' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/head_config.json
11/28/2021 01:10:11 - INFO - __main__ -   Language cdo, split test does not exist
15.63user 6.91system 0:22.16elapsed 101%CPU (0avgtext+0avgdata 3981380maxresident)k
8inputs+40outputs (0major+1663482minor)pagefaults 0swaps
11/28/2021 01:10:14 - INFO - __main__ -   Language gn, split test does not exist
14.75user 5.80system 0:20.92elapsed 98%CPU (0avgtext+0avgdata 3983140maxresident)k
0inputs+32outputs (0major+1566227minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:10:17 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='gn', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:10:17 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:10:17 - INFO - __main__ -   Seed = 3
11/28/2021 01:10:17 - INFO - root -   save model
11/28/2021 01:10:17 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='gn', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:10:17 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:10:20 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:10:26 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:10:26 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:10:26 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
11/28/2021 01:10:26 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:10:26 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:10:26 - INFO - root -   loading lang adpater cs/wiki@ukp
11/28/2021 01:10:26 - INFO - __main__ -   Adapter Languages : ['cs'], Length : 1
11/28/2021 01:10:26 - INFO - __main__ -   Adapter Names ['cs/wiki@ukp'], Length : 1
11/28/2021 01:10:26 - INFO - __main__ -   Language = cs
11/28/2021 01:10:26 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/cs/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_cs_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/adapter_config.json
Adding adapter 'cs' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/head_config.json
11/28/2021 01:10:34 - INFO - __main__ -   Language gn, split test does not exist
15.22user 7.05system 0:20.34elapsed 109%CPU (0avgtext+0avgdata 3987432maxresident)k
0inputs+56outputs (0major+1534458minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:12:30 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:12:30 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:12:30 - INFO - __main__ -   Seed = 1
11/28/2021 01:12:30 - INFO - root -   save model
11/28/2021 01:12:30 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:12:30 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:12:33 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:12:39 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:12:39 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:12:39 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
11/28/2021 01:12:39 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:12:39 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:12:39 - INFO - root -   loading lang adpater cs/wiki@ukp
11/28/2021 01:12:39 - INFO - __main__ -   Adapter Languages : ['cs'], Length : 1
11/28/2021 01:12:39 - INFO - __main__ -   Adapter Names ['cs/wiki@ukp'], Length : 1
11/28/2021 01:12:39 - INFO - __main__ -   Language = cs
11/28/2021 01:12:39 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/cs/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_cs_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/adapter_config.json
Adding adapter 'cs' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/head_config.json
11/28/2021 01:12:47 - INFO - __main__ -   Language mi, split test does not exist
14.44user 5.80system 0:20.35elapsed 99%CPU (0avgtext+0avgdata 3988060maxresident)k
8inputs+64outputs (0major+1487929minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:12:50 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:12:50 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:12:50 - INFO - __main__ -   Seed = 2
11/28/2021 01:12:50 - INFO - root -   save model
11/28/2021 01:12:50 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:12:50 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:12:53 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:12:59 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='hi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:12:59 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:12:59 - INFO - __main__ -   Seed = 1
11/28/2021 01:12:59 - INFO - root -   save model
11/28/2021 01:12:59 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='hi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:12:59 - INFO - __main__ -   Loading pretrained model and tokenizer
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

11/28/2021 01:13:00 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:13:00 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:13:00 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
11/28/2021 01:13:00 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:13:00 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:13:00 - INFO - root -   loading lang adpater cs/wiki@ukp
11/28/2021 01:13:00 - INFO - __main__ -   Adapter Languages : ['cs'], Length : 1
11/28/2021 01:13:00 - INFO - __main__ -   Adapter Names ['cs/wiki@ukp'], Length : 1
11/28/2021 01:13:00 - INFO - __main__ -   Language = cs
11/28/2021 01:13:00 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/cs/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_cs_wiki_pfeiffer.zip.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:13:01 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/adapter_config.json
Adding adapter 'cs' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/head_config.json
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:13:08 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:13:08 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:13:08 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
11/28/2021 01:13:08 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:13:08 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:13:08 - INFO - root -   loading lang adpater cs/wiki@ukp
11/28/2021 01:13:08 - INFO - __main__ -   Adapter Languages : ['cs'], Length : 1
11/28/2021 01:13:08 - INFO - __main__ -   Adapter Names ['cs/wiki@ukp'], Length : 1
11/28/2021 01:13:08 - INFO - __main__ -   Language = cs
11/28/2021 01:13:08 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/cs/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_cs_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/adapter_config.json
Adding adapter 'cs' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/head_config.json
11/28/2021 01:13:11 - INFO - __main__ -   Language mi, split test does not exist
17.90user 7.15system 0:23.28elapsed 107%CPU (0avgtext+0avgdata 3983388maxresident)k
0inputs+48outputs (0major+1489609minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:13:14 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:13:14 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:13:14 - INFO - __main__ -   Seed = 3
11/28/2021 01:13:14 - INFO - root -   save model
11/28/2021 01:13:14 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:13:14 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:13:17 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
11/28/2021 01:13:20 - INFO - __main__ -   Language adapter for hi not found, using cs instead
11/28/2021 01:13:20 - INFO - __main__ -   Set active language adapter to cs
11/28/2021 01:13:20 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:13:20 - INFO - __main__ -   Adapter Languages = ['cs']
11/28/2021 01:13:20 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_hi_bert-base-multilingual-cased_128
11/28/2021 01:13:20 - INFO - __main__ -   ***** Running evaluation  in hi *****
11/28/2021 01:13:20 - INFO - __main__ -     Num examples = 2685
11/28/2021 01:13:20 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/84 [00:00<?, ?it/s]11/28/2021 01:13:20 - INFO - __main__ -   Batch number = 1
Evaluating:   1%|          | 1/84 [00:00<00:41,  1.99it/s]11/28/2021 01:13:21 - INFO - __main__ -   Batch number = 2
Evaluating:   2%|▏         | 2/84 [00:00<00:40,  2.01it/s]11/28/2021 01:13:21 - INFO - __main__ -   Batch number = 3
Evaluating:   4%|▎         | 3/84 [00:01<00:40,  2.01it/s]11/28/2021 01:13:22 - INFO - __main__ -   Batch number = 4
Evaluating:   5%|▍         | 4/84 [00:01<00:39,  2.03it/s]11/28/2021 01:13:22 - INFO - __main__ -   Batch number = 5
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:13:23 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:13:23 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:13:23 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
11/28/2021 01:13:23 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:13:23 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:13:23 - INFO - root -   loading lang adpater cs/wiki@ukp
11/28/2021 01:13:23 - INFO - __main__ -   Adapter Languages : ['cs'], Length : 1
11/28/2021 01:13:23 - INFO - __main__ -   Adapter Names ['cs/wiki@ukp'], Length : 1
11/28/2021 01:13:23 - INFO - __main__ -   Language = cs
11/28/2021 01:13:23 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/cs/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_cs_wiki_pfeiffer.zip.
Evaluating:   6%|▌         | 5/84 [00:02<00:38,  2.04it/s]11/28/2021 01:13:23 - INFO - __main__ -   Batch number = 6
Evaluating:   7%|▋         | 6/84 [00:02<00:38,  2.04it/s]11/28/2021 01:13:23 - INFO - __main__ -   Batch number = 7
Evaluating:   8%|▊         | 7/84 [00:03<00:37,  2.05it/s]11/28/2021 01:13:24 - INFO - __main__ -   Batch number = 8
Evaluating:  10%|▉         | 8/84 [00:03<00:37,  2.05it/s]11/28/2021 01:13:24 - INFO - __main__ -   Batch number = 9
Evaluating:  11%|█         | 9/84 [00:04<00:35,  2.14it/s]11/28/2021 01:13:25 - INFO - __main__ -   Batch number = 10
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/adapter_config.json
Adding adapter 'cs' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/head_config.json
Evaluating:  12%|█▏        | 10/84 [00:04<00:34,  2.12it/s]11/28/2021 01:13:25 - INFO - __main__ -   Batch number = 11
Evaluating:  13%|█▎        | 11/84 [00:05<00:34,  2.10it/s]11/28/2021 01:13:26 - INFO - __main__ -   Batch number = 12
Evaluating:  14%|█▍        | 12/84 [00:05<00:34,  2.08it/s]11/28/2021 01:13:26 - INFO - __main__ -   Batch number = 13
Evaluating:  15%|█▌        | 13/84 [00:06<00:34,  2.07it/s]11/28/2021 01:13:27 - INFO - __main__ -   Batch number = 14
Evaluating:  17%|█▋        | 14/84 [00:06<00:33,  2.06it/s]11/28/2021 01:13:27 - INFO - __main__ -   Batch number = 15
Evaluating:  18%|█▊        | 15/84 [00:07<00:33,  2.06it/s]11/28/2021 01:13:28 - INFO - __main__ -   Batch number = 16
Evaluating:  19%|█▉        | 16/84 [00:07<00:32,  2.06it/s]11/28/2021 01:13:28 - INFO - __main__ -   Batch number = 17
Evaluating:  20%|██        | 17/84 [00:08<00:32,  2.07it/s]11/28/2021 01:13:29 - INFO - __main__ -   Batch number = 18
Evaluating:  21%|██▏       | 18/84 [00:08<00:31,  2.07it/s]11/28/2021 01:13:29 - INFO - __main__ -   Batch number = 19
Evaluating:  23%|██▎       | 19/84 [00:09<00:31,  2.06it/s]11/28/2021 01:13:29 - INFO - __main__ -   Batch number = 20
Evaluating:  24%|██▍       | 20/84 [00:09<00:31,  2.05it/s]11/28/2021 01:13:30 - INFO - __main__ -   Batch number = 21
Evaluating:  25%|██▌       | 21/84 [00:10<00:30,  2.05it/s]11/28/2021 01:13:30 - INFO - __main__ -   Batch number = 22
Evaluating:  26%|██▌       | 22/84 [00:10<00:30,  2.05it/s]11/28/2021 01:13:31 - INFO - __main__ -   Batch number = 23
Evaluating:  27%|██▋       | 23/84 [00:11<00:29,  2.06it/s]11/28/2021 01:13:31 - INFO - __main__ -   Batch number = 24
Evaluating:  29%|██▊       | 24/84 [00:11<00:29,  2.05it/s]11/28/2021 01:13:32 - INFO - __main__ -   Batch number = 25
Evaluating:  30%|██▉       | 25/84 [00:12<00:28,  2.04it/s]11/28/2021 01:13:32 - INFO - __main__ -   Batch number = 26
Evaluating:  31%|███       | 26/84 [00:12<00:28,  2.04it/s]11/28/2021 01:13:33 - INFO - __main__ -   Batch number = 27
Evaluating:  32%|███▏      | 27/84 [00:13<00:27,  2.04it/s]11/28/2021 01:13:33 - INFO - __main__ -   Batch number = 28
Evaluating:  33%|███▎      | 28/84 [00:13<00:27,  2.05it/s]11/28/2021 01:13:34 - INFO - __main__ -   Batch number = 29
Evaluating:  35%|███▍      | 29/84 [00:14<00:26,  2.06it/s]11/28/2021 01:13:35 - INFO - __main__ -   Batch number = 30
Evaluating:  36%|███▌      | 30/84 [00:14<00:29,  1.82it/s]11/28/2021 01:13:35 - INFO - __main__ -   Batch number = 31
Evaluating:  37%|███▋      | 31/84 [00:15<00:27,  1.92it/s]11/28/2021 01:13:36 - INFO - __main__ -   Batch number = 32
11/28/2021 01:13:36 - INFO - __main__ -   Language mi, split test does not exist
Evaluating:  38%|███▊      | 32/84 [00:15<00:26,  1.98it/s]11/28/2021 01:13:36 - INFO - __main__ -   Batch number = 33
Evaluating:  39%|███▉      | 33/84 [00:16<00:25,  1.99it/s]11/28/2021 01:13:36 - INFO - __main__ -   Batch number = 34
19.06user 7.81system 0:24.85elapsed 108%CPU (0avgtext+0avgdata 3987732maxresident)k
0inputs+40outputs (0major+1488596minor)pagefaults 0swaps
Evaluating:  40%|████      | 34/84 [00:16<00:24,  2.02it/s]11/28/2021 01:13:37 - INFO - __main__ -   Batch number = 35
Evaluating:  42%|████▏     | 35/84 [00:17<00:22,  2.14it/s]11/28/2021 01:13:37 - INFO - __main__ -   Batch number = 36
Evaluating:  43%|████▎     | 36/84 [00:17<00:20,  2.38it/s]11/28/2021 01:13:38 - INFO - __main__ -   Batch number = 37
Evaluating:  44%|████▍     | 37/84 [00:17<00:18,  2.56it/s]11/28/2021 01:13:38 - INFO - __main__ -   Batch number = 38
Evaluating:  45%|████▌     | 38/84 [00:18<00:16,  2.72it/s]11/28/2021 01:13:38 - INFO - __main__ -   Batch number = 39
Evaluating:  46%|████▋     | 39/84 [00:18<00:15,  2.84it/s]11/28/2021 01:13:39 - INFO - __main__ -   Batch number = 40
Evaluating:  48%|████▊     | 40/84 [00:18<00:15,  2.93it/s]11/28/2021 01:13:39 - INFO - __main__ -   Batch number = 41
Evaluating:  49%|████▉     | 41/84 [00:18<00:14,  3.00it/s]11/28/2021 01:13:39 - INFO - __main__ -   Batch number = 42
Evaluating:  50%|█████     | 42/84 [00:19<00:13,  3.04it/s]11/28/2021 01:13:40 - INFO - __main__ -   Batch number = 43
Evaluating:  51%|█████     | 43/84 [00:19<00:13,  3.03it/s]11/28/2021 01:13:40 - INFO - __main__ -   Batch number = 44
Evaluating:  52%|█████▏    | 44/84 [00:19<00:13,  3.03it/s]11/28/2021 01:13:40 - INFO - __main__ -   Batch number = 45
Evaluating:  54%|█████▎    | 45/84 [00:20<00:12,  3.08it/s]11/28/2021 01:13:41 - INFO - __main__ -   Batch number = 46
Evaluating:  55%|█████▍    | 46/84 [00:20<00:12,  3.14it/s]11/28/2021 01:13:41 - INFO - __main__ -   Batch number = 47
Evaluating:  56%|█████▌    | 47/84 [00:20<00:11,  3.14it/s]11/28/2021 01:13:41 - INFO - __main__ -   Batch number = 48
Evaluating:  57%|█████▋    | 48/84 [00:21<00:11,  3.15it/s]11/28/2021 01:13:41 - INFO - __main__ -   Batch number = 49
Evaluating:  58%|█████▊    | 49/84 [00:21<00:11,  3.17it/s]11/28/2021 01:13:42 - INFO - __main__ -   Batch number = 50
Evaluating:  60%|█████▉    | 50/84 [00:21<00:10,  3.17it/s]11/28/2021 01:13:42 - INFO - __main__ -   Batch number = 51
Evaluating:  61%|██████    | 51/84 [00:22<00:10,  3.18it/s]11/28/2021 01:13:42 - INFO - __main__ -   Batch number = 52
Evaluating:  62%|██████▏   | 52/84 [00:22<00:10,  3.17it/s]11/28/2021 01:13:43 - INFO - __main__ -   Batch number = 53
Evaluating:  63%|██████▎   | 53/84 [00:22<00:09,  3.17it/s]11/28/2021 01:13:43 - INFO - __main__ -   Batch number = 54
Evaluating:  64%|██████▍   | 54/84 [00:23<00:09,  3.12it/s]11/28/2021 01:13:43 - INFO - __main__ -   Batch number = 55
Evaluating:  65%|██████▌   | 55/84 [00:23<00:09,  3.09it/s]11/28/2021 01:13:44 - INFO - __main__ -   Batch number = 56
Evaluating:  67%|██████▋   | 56/84 [00:23<00:08,  3.13it/s]11/28/2021 01:13:44 - INFO - __main__ -   Batch number = 57
Evaluating:  68%|██████▊   | 57/84 [00:24<00:08,  3.15it/s]11/28/2021 01:13:44 - INFO - __main__ -   Batch number = 58
Evaluating:  69%|██████▉   | 58/84 [00:24<00:08,  3.04it/s]11/28/2021 01:13:45 - INFO - __main__ -   Batch number = 59
Evaluating:  70%|███████   | 59/84 [00:24<00:08,  3.09it/s]11/28/2021 01:13:45 - INFO - __main__ -   Batch number = 60
Evaluating:  71%|███████▏  | 60/84 [00:25<00:07,  3.11it/s]11/28/2021 01:13:45 - INFO - __main__ -   Batch number = 61
Evaluating:  73%|███████▎  | 61/84 [00:25<00:07,  3.13it/s]11/28/2021 01:13:46 - INFO - __main__ -   Batch number = 62
Evaluating:  74%|███████▍  | 62/84 [00:25<00:06,  3.17it/s]11/28/2021 01:13:46 - INFO - __main__ -   Batch number = 63
Evaluating:  75%|███████▌  | 63/84 [00:25<00:06,  3.16it/s]11/28/2021 01:13:46 - INFO - __main__ -   Batch number = 64
Evaluating:  76%|███████▌  | 64/84 [00:26<00:06,  3.17it/s]11/28/2021 01:13:47 - INFO - __main__ -   Batch number = 65
Evaluating:  77%|███████▋  | 65/84 [00:26<00:05,  3.17it/s]11/28/2021 01:13:47 - INFO - __main__ -   Batch number = 66
Evaluating:  79%|███████▊  | 66/84 [00:26<00:05,  3.20it/s]11/28/2021 01:13:47 - INFO - __main__ -   Batch number = 67
Evaluating:  80%|███████▉  | 67/84 [00:27<00:05,  3.18it/s]11/28/2021 01:13:48 - INFO - __main__ -   Batch number = 68
Evaluating:  81%|████████  | 68/84 [00:27<00:05,  3.14it/s]11/28/2021 01:13:48 - INFO - __main__ -   Batch number = 69
Evaluating:  82%|████████▏ | 69/84 [00:27<00:04,  3.11it/s]11/28/2021 01:13:48 - INFO - __main__ -   Batch number = 70
Evaluating:  83%|████████▎ | 70/84 [00:28<00:04,  3.16it/s]11/28/2021 01:13:48 - INFO - __main__ -   Batch number = 71
Evaluating:  85%|████████▍ | 71/84 [00:28<00:04,  3.22it/s]11/28/2021 01:13:49 - INFO - __main__ -   Batch number = 72
Evaluating:  86%|████████▌ | 72/84 [00:28<00:03,  3.20it/s]11/28/2021 01:13:49 - INFO - __main__ -   Batch number = 73
Evaluating:  87%|████████▋ | 73/84 [00:29<00:03,  3.19it/s]11/28/2021 01:13:49 - INFO - __main__ -   Batch number = 74
Evaluating:  88%|████████▊ | 74/84 [00:29<00:03,  3.20it/s]11/28/2021 01:13:50 - INFO - __main__ -   Batch number = 75
Evaluating:  89%|████████▉ | 75/84 [00:29<00:02,  3.20it/s]11/28/2021 01:13:50 - INFO - __main__ -   Batch number = 76
Evaluating:  90%|█████████ | 76/84 [00:30<00:02,  3.19it/s]11/28/2021 01:13:50 - INFO - __main__ -   Batch number = 77
Evaluating:  92%|█████████▏| 77/84 [00:30<00:02,  3.19it/s]11/28/2021 01:13:51 - INFO - __main__ -   Batch number = 78
Evaluating:  93%|█████████▎| 78/84 [00:30<00:01,  3.13it/s]11/28/2021 01:13:51 - INFO - __main__ -   Batch number = 79
Evaluating:  94%|█████████▍| 79/84 [00:31<00:01,  3.09it/s]11/28/2021 01:13:51 - INFO - __main__ -   Batch number = 80
Evaluating:  95%|█████████▌| 80/84 [00:31<00:01,  3.14it/s]11/28/2021 01:13:52 - INFO - __main__ -   Batch number = 81
Evaluating:  96%|█████████▋| 81/84 [00:31<00:00,  3.16it/s]11/28/2021 01:13:52 - INFO - __main__ -   Batch number = 82
Evaluating:  98%|█████████▊| 82/84 [00:31<00:00,  3.16it/s]11/28/2021 01:13:52 - INFO - __main__ -   Batch number = 83
Evaluating:  99%|█████████▉| 83/84 [00:32<00:00,  3.17it/s]11/28/2021 01:13:53 - INFO - __main__ -   Batch number = 84
Evaluating: 100%|██████████| 84/84 [00:32<00:00,  3.23it/s]Evaluating: 100%|██████████| 84/84 [00:32<00:00,  2.58it/s]
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:13:54 - INFO - __main__ -   ***** Evaluation result  in hi *****
11/28/2021 01:13:54 - INFO - __main__ -     f1 = 0.6403148463022754
11/28/2021 01:13:54 - INFO - __main__ -     loss = 1.2433432922476815
11/28/2021 01:13:54 - INFO - __main__ -     precision = 0.6387163166520563
11/28/2021 01:13:54 - INFO - __main__ -     recall = 0.6419213973799127
44.05user 16.20system 0:58.93elapsed 102%CPU (0avgtext+0avgdata 3986056maxresident)k
0inputs+688outputs (0major+1607554minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:13:58 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='hi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:13:58 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:13:58 - INFO - __main__ -   Seed = 2
11/28/2021 01:13:58 - INFO - root -   save model
11/28/2021 01:13:58 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='hi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:13:58 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:14:00 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:14:07 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:14:07 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:14:07 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
11/28/2021 01:14:07 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:14:07 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:14:07 - INFO - root -   loading lang adpater cs/wiki@ukp
11/28/2021 01:14:07 - INFO - __main__ -   Adapter Languages : ['cs'], Length : 1
11/28/2021 01:14:07 - INFO - __main__ -   Adapter Names ['cs/wiki@ukp'], Length : 1
11/28/2021 01:14:07 - INFO - __main__ -   Language = cs
11/28/2021 01:14:07 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/cs/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_cs_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/adapter_config.json
Adding adapter 'cs' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/head_config.json
11/28/2021 01:14:18 - INFO - __main__ -   Language adapter for hi not found, using cs instead
11/28/2021 01:14:18 - INFO - __main__ -   Set active language adapter to cs
11/28/2021 01:14:18 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:14:18 - INFO - __main__ -   Adapter Languages = ['cs']
11/28/2021 01:14:18 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_hi_bert-base-multilingual-cased_128
11/28/2021 01:14:19 - INFO - __main__ -   ***** Running evaluation  in hi *****
11/28/2021 01:14:19 - INFO - __main__ -     Num examples = 2685
11/28/2021 01:14:19 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/84 [00:00<?, ?it/s]11/28/2021 01:14:19 - INFO - __main__ -   Batch number = 1
Evaluating:   1%|          | 1/84 [00:00<00:38,  2.14it/s]11/28/2021 01:14:19 - INFO - __main__ -   Batch number = 2
Evaluating:   2%|▏         | 2/84 [00:00<00:35,  2.32it/s]11/28/2021 01:14:20 - INFO - __main__ -   Batch number = 3
Evaluating:   4%|▎         | 3/84 [00:01<00:30,  2.62it/s]11/28/2021 01:14:20 - INFO - __main__ -   Batch number = 4
Evaluating:   5%|▍         | 4/84 [00:01<00:28,  2.81it/s]11/28/2021 01:14:20 - INFO - __main__ -   Batch number = 5
Evaluating:   6%|▌         | 5/84 [00:01<00:27,  2.89it/s]11/28/2021 01:14:21 - INFO - __main__ -   Batch number = 6
Evaluating:   7%|▋         | 6/84 [00:02<00:26,  2.94it/s]11/28/2021 01:14:21 - INFO - __main__ -   Batch number = 7
Evaluating:   8%|▊         | 7/84 [00:02<00:26,  2.96it/s]11/28/2021 01:14:21 - INFO - __main__ -   Batch number = 8
Evaluating:  10%|▉         | 8/84 [00:02<00:25,  2.98it/s]11/28/2021 01:14:22 - INFO - __main__ -   Batch number = 9
Evaluating:  11%|█         | 9/84 [00:03<00:25,  3.00it/s]11/28/2021 01:14:22 - INFO - __main__ -   Batch number = 10
Evaluating:  12%|█▏        | 10/84 [00:03<00:24,  3.02it/s]11/28/2021 01:14:22 - INFO - __main__ -   Batch number = 11
Evaluating:  13%|█▎        | 11/84 [00:03<00:24,  3.02it/s]11/28/2021 01:14:23 - INFO - __main__ -   Batch number = 12
Evaluating:  14%|█▍        | 12/84 [00:04<00:23,  3.12it/s]11/28/2021 01:14:23 - INFO - __main__ -   Batch number = 13
Evaluating:  15%|█▌        | 13/84 [00:04<00:22,  3.09it/s]11/28/2021 01:14:23 - INFO - __main__ -   Batch number = 14
Evaluating:  17%|█▋        | 14/84 [00:04<00:22,  3.08it/s]11/28/2021 01:14:24 - INFO - __main__ -   Batch number = 15
Evaluating:  18%|█▊        | 15/84 [00:05<00:22,  3.06it/s]11/28/2021 01:14:24 - INFO - __main__ -   Batch number = 16
Evaluating:  19%|█▉        | 16/84 [00:05<00:22,  3.05it/s]11/28/2021 01:14:24 - INFO - __main__ -   Batch number = 17
Evaluating:  20%|██        | 17/84 [00:05<00:23,  2.83it/s]11/28/2021 01:14:25 - INFO - __main__ -   Batch number = 18
Evaluating:  21%|██▏       | 18/84 [00:06<00:22,  2.90it/s]11/28/2021 01:14:25 - INFO - __main__ -   Batch number = 19
Evaluating:  23%|██▎       | 19/84 [00:06<00:22,  2.94it/s]11/28/2021 01:14:25 - INFO - __main__ -   Batch number = 20
Evaluating:  24%|██▍       | 20/84 [00:06<00:21,  2.97it/s]11/28/2021 01:14:26 - INFO - __main__ -   Batch number = 21
Evaluating:  25%|██▌       | 21/84 [00:07<00:21,  2.99it/s]11/28/2021 01:14:26 - INFO - __main__ -   Batch number = 22
Evaluating:  26%|██▌       | 22/84 [00:07<00:20,  3.01it/s]11/28/2021 01:14:26 - INFO - __main__ -   Batch number = 23
Evaluating:  27%|██▋       | 23/84 [00:07<00:20,  3.03it/s]11/28/2021 01:14:27 - INFO - __main__ -   Batch number = 24
Evaluating:  29%|██▊       | 24/84 [00:08<00:19,  3.03it/s]11/28/2021 01:14:27 - INFO - __main__ -   Batch number = 25
Evaluating:  30%|██▉       | 25/84 [00:08<00:19,  3.03it/s]11/28/2021 01:14:27 - INFO - __main__ -   Batch number = 26
Evaluating:  31%|███       | 26/84 [00:08<00:19,  3.05it/s]11/28/2021 01:14:28 - INFO - __main__ -   Batch number = 27
Evaluating:  32%|███▏      | 27/84 [00:09<00:18,  3.05it/s]11/28/2021 01:14:28 - INFO - __main__ -   Batch number = 28
Evaluating:  33%|███▎      | 28/84 [00:09<00:18,  3.07it/s]11/28/2021 01:14:28 - INFO - __main__ -   Batch number = 29
Evaluating:  35%|███▍      | 29/84 [00:09<00:17,  3.08it/s]11/28/2021 01:14:29 - INFO - __main__ -   Batch number = 30
Evaluating:  36%|███▌      | 30/84 [00:10<00:17,  3.08it/s]11/28/2021 01:14:29 - INFO - __main__ -   Batch number = 31
Evaluating:  37%|███▋      | 31/84 [00:10<00:17,  3.09it/s]11/28/2021 01:14:29 - INFO - __main__ -   Batch number = 32
Evaluating:  38%|███▊      | 32/84 [00:10<00:18,  2.82it/s]11/28/2021 01:14:30 - INFO - __main__ -   Batch number = 33
Evaluating:  39%|███▉      | 33/84 [00:11<00:17,  2.90it/s]11/28/2021 01:14:30 - INFO - __main__ -   Batch number = 34
Evaluating:  40%|████      | 34/84 [00:11<00:16,  2.95it/s]11/28/2021 01:14:30 - INFO - __main__ -   Batch number = 35
Evaluating:  42%|████▏     | 35/84 [00:11<00:16,  2.99it/s]11/28/2021 01:14:31 - INFO - __main__ -   Batch number = 36
Evaluating:  43%|████▎     | 36/84 [00:12<00:15,  3.01it/s]11/28/2021 01:14:31 - INFO - __main__ -   Batch number = 37
Evaluating:  44%|████▍     | 37/84 [00:12<00:15,  3.03it/s]11/28/2021 01:14:31 - INFO - __main__ -   Batch number = 38
Evaluating:  45%|████▌     | 38/84 [00:12<00:15,  3.04it/s]11/28/2021 01:14:32 - INFO - __main__ -   Batch number = 39
Evaluating:  46%|████▋     | 39/84 [00:13<00:14,  3.04it/s]11/28/2021 01:14:32 - INFO - __main__ -   Batch number = 40
Evaluating:  48%|████▊     | 40/84 [00:13<00:14,  3.04it/s]11/28/2021 01:14:32 - INFO - __main__ -   Batch number = 41
Evaluating:  49%|████▉     | 41/84 [00:13<00:14,  3.04it/s]11/28/2021 01:14:33 - INFO - __main__ -   Batch number = 42
Evaluating:  50%|█████     | 42/84 [00:14<00:13,  3.04it/s]11/28/2021 01:14:33 - INFO - __main__ -   Batch number = 43
Evaluating:  51%|█████     | 43/84 [00:14<00:13,  3.05it/s]11/28/2021 01:14:33 - INFO - __main__ -   Batch number = 44
Evaluating:  52%|█████▏    | 44/84 [00:14<00:12,  3.12it/s]11/28/2021 01:14:34 - INFO - __main__ -   Batch number = 45
Evaluating:  54%|█████▎    | 45/84 [00:14<00:10,  3.66it/s]11/28/2021 01:14:34 - INFO - __main__ -   Batch number = 46
Evaluating:  55%|█████▍    | 46/84 [00:15<00:09,  4.15it/s]11/28/2021 01:14:34 - INFO - __main__ -   Batch number = 47
Evaluating:  56%|█████▌    | 47/84 [00:15<00:07,  4.63it/s]11/28/2021 01:14:34 - INFO - __main__ -   Batch number = 48
Evaluating:  57%|█████▋    | 48/84 [00:15<00:07,  5.02it/s]11/28/2021 01:14:34 - INFO - __main__ -   Batch number = 49
Evaluating:  58%|█████▊    | 49/84 [00:15<00:06,  5.29it/s]11/28/2021 01:14:35 - INFO - __main__ -   Batch number = 50
Evaluating:  60%|█████▉    | 50/84 [00:15<00:07,  4.65it/s]11/28/2021 01:14:35 - INFO - __main__ -   Batch number = 51
Evaluating:  61%|██████    | 51/84 [00:15<00:06,  5.01it/s]11/28/2021 01:14:35 - INFO - __main__ -   Batch number = 52
Evaluating:  62%|██████▏   | 52/84 [00:16<00:06,  5.27it/s]11/28/2021 01:14:35 - INFO - __main__ -   Batch number = 53
Evaluating:  63%|██████▎   | 53/84 [00:16<00:05,  5.50it/s]11/28/2021 01:14:35 - INFO - __main__ -   Batch number = 54
Evaluating:  64%|██████▍   | 54/84 [00:16<00:05,  5.68it/s]11/28/2021 01:14:35 - INFO - __main__ -   Batch number = 55
Evaluating:  65%|██████▌   | 55/84 [00:16<00:05,  5.77it/s]11/28/2021 01:14:36 - INFO - __main__ -   Batch number = 56
Evaluating:  67%|██████▋   | 56/84 [00:16<00:04,  5.87it/s]11/28/2021 01:14:36 - INFO - __main__ -   Batch number = 57
Evaluating:  68%|██████▊   | 57/84 [00:16<00:04,  5.93it/s]11/28/2021 01:14:36 - INFO - __main__ -   Batch number = 58
Evaluating:  69%|██████▉   | 58/84 [00:17<00:04,  6.03it/s]11/28/2021 01:14:36 - INFO - __main__ -   Batch number = 59
Evaluating:  70%|███████   | 59/84 [00:17<00:04,  6.09it/s]11/28/2021 01:14:36 - INFO - __main__ -   Batch number = 60
Evaluating:  71%|███████▏  | 60/84 [00:17<00:03,  6.14it/s]11/28/2021 01:14:36 - INFO - __main__ -   Batch number = 61
Evaluating:  73%|███████▎  | 61/84 [00:17<00:03,  6.13it/s]11/28/2021 01:14:36 - INFO - __main__ -   Batch number = 62
Evaluating:  74%|███████▍  | 62/84 [00:17<00:03,  6.09it/s]11/28/2021 01:14:37 - INFO - __main__ -   Batch number = 63
Evaluating:  75%|███████▌  | 63/84 [00:17<00:03,  6.14it/s]11/28/2021 01:14:37 - INFO - __main__ -   Batch number = 64
Evaluating:  76%|███████▌  | 64/84 [00:18<00:03,  6.13it/s]11/28/2021 01:14:37 - INFO - __main__ -   Batch number = 65
Evaluating:  77%|███████▋  | 65/84 [00:18<00:03,  6.14it/s]11/28/2021 01:14:37 - INFO - __main__ -   Batch number = 66
Evaluating:  79%|███████▊  | 66/84 [00:18<00:02,  6.13it/s]11/28/2021 01:14:37 - INFO - __main__ -   Batch number = 67
Evaluating:  80%|███████▉  | 67/84 [00:18<00:02,  6.11it/s]11/28/2021 01:14:37 - INFO - __main__ -   Batch number = 68
Evaluating:  81%|████████  | 68/84 [00:18<00:02,  6.07it/s]11/28/2021 01:14:38 - INFO - __main__ -   Batch number = 69
Evaluating:  82%|████████▏ | 69/84 [00:18<00:02,  6.07it/s]11/28/2021 01:14:38 - INFO - __main__ -   Batch number = 70
Evaluating:  83%|████████▎ | 70/84 [00:19<00:02,  6.06it/s]11/28/2021 01:14:38 - INFO - __main__ -   Batch number = 71
Evaluating:  85%|████████▍ | 71/84 [00:19<00:02,  6.08it/s]11/28/2021 01:14:38 - INFO - __main__ -   Batch number = 72
Evaluating:  86%|████████▌ | 72/84 [00:19<00:01,  6.07it/s]11/28/2021 01:14:38 - INFO - __main__ -   Batch number = 73
Evaluating:  87%|████████▋ | 73/84 [00:19<00:01,  5.98it/s]11/28/2021 01:14:38 - INFO - __main__ -   Batch number = 74
Evaluating:  88%|████████▊ | 74/84 [00:19<00:01,  5.99it/s]11/28/2021 01:14:39 - INFO - __main__ -   Batch number = 75
Evaluating:  89%|████████▉ | 75/84 [00:19<00:01,  5.99it/s]11/28/2021 01:14:39 - INFO - __main__ -   Batch number = 76
Evaluating:  90%|█████████ | 76/84 [00:20<00:01,  5.94it/s]11/28/2021 01:14:39 - INFO - __main__ -   Batch number = 77
Evaluating:  92%|█████████▏| 77/84 [00:20<00:01,  5.94it/s]11/28/2021 01:14:39 - INFO - __main__ -   Batch number = 78
Evaluating:  93%|█████████▎| 78/84 [00:20<00:01,  5.88it/s]11/28/2021 01:14:39 - INFO - __main__ -   Batch number = 79
Evaluating:  94%|█████████▍| 79/84 [00:20<00:00,  5.33it/s]11/28/2021 01:14:40 - INFO - __main__ -   Batch number = 80
Evaluating:  95%|█████████▌| 80/84 [00:20<00:00,  5.54it/s]11/28/2021 01:14:40 - INFO - __main__ -   Batch number = 81
Evaluating:  96%|█████████▋| 81/84 [00:21<00:00,  5.67it/s]11/28/2021 01:14:40 - INFO - __main__ -   Batch number = 82
Evaluating:  98%|█████████▊| 82/84 [00:21<00:00,  5.81it/s]11/28/2021 01:14:40 - INFO - __main__ -   Batch number = 83
Evaluating:  99%|█████████▉| 83/84 [00:21<00:00,  5.89it/s]11/28/2021 01:14:40 - INFO - __main__ -   Batch number = 84
Evaluating: 100%|██████████| 84/84 [00:21<00:00,  6.10it/s]Evaluating: 100%|██████████| 84/84 [00:21<00:00,  3.91it/s]
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:14:42 - INFO - __main__ -   ***** Evaluation result  in hi *****
11/28/2021 01:14:42 - INFO - __main__ -     f1 = 0.6206346460627727
11/28/2021 01:14:42 - INFO - __main__ -     loss = 1.39368932445844
11/28/2021 01:14:42 - INFO - __main__ -     precision = 0.6228252902827605
11/28/2021 01:14:42 - INFO - __main__ -     recall = 0.6184593580019919
35.28user 11.52system 0:47.20elapsed 99%CPU (0avgtext+0avgdata 3988964maxresident)k
0inputs+696outputs (0major+1706782minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:14:45 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='hi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:14:45 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:14:45 - INFO - __main__ -   Seed = 3
11/28/2021 01:14:45 - INFO - root -   save model
11/28/2021 01:14:45 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='hi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:14:45 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:14:48 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:14:54 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:14:54 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:14:54 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
11/28/2021 01:14:54 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:14:54 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:14:54 - INFO - root -   loading lang adpater cs/wiki@ukp
11/28/2021 01:14:54 - INFO - __main__ -   Adapter Languages : ['cs'], Length : 1
11/28/2021 01:14:54 - INFO - __main__ -   Adapter Names ['cs/wiki@ukp'], Length : 1
11/28/2021 01:14:54 - INFO - __main__ -   Language = cs
11/28/2021 01:14:54 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/cs/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_cs_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/adapter_config.json
Adding adapter 'cs' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/head_config.json
11/28/2021 01:15:06 - INFO - __main__ -   Language adapter for hi not found, using cs instead
11/28/2021 01:15:06 - INFO - __main__ -   Set active language adapter to cs
11/28/2021 01:15:06 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:15:06 - INFO - __main__ -   Adapter Languages = ['cs']
11/28/2021 01:15:06 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_hi_bert-base-multilingual-cased_128
11/28/2021 01:15:07 - INFO - __main__ -   ***** Running evaluation  in hi *****
11/28/2021 01:15:07 - INFO - __main__ -     Num examples = 2685
11/28/2021 01:15:07 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/84 [00:00<?, ?it/s]11/28/2021 01:15:07 - INFO - __main__ -   Batch number = 1
Evaluating:   1%|          | 1/84 [00:00<00:40,  2.04it/s]11/28/2021 01:15:07 - INFO - __main__ -   Batch number = 2
Evaluating:   2%|▏         | 2/84 [00:00<00:37,  2.17it/s]11/28/2021 01:15:07 - INFO - __main__ -   Batch number = 3
Evaluating:   4%|▎         | 3/84 [00:01<00:38,  2.11it/s]11/28/2021 01:15:08 - INFO - __main__ -   Batch number = 4
Evaluating:   5%|▍         | 4/84 [00:01<00:38,  2.09it/s]11/28/2021 01:15:08 - INFO - __main__ -   Batch number = 5
Evaluating:   6%|▌         | 5/84 [00:02<00:37,  2.08it/s]11/28/2021 01:15:09 - INFO - __main__ -   Batch number = 6
Evaluating:   7%|▋         | 6/84 [00:02<00:37,  2.08it/s]11/28/2021 01:15:09 - INFO - __main__ -   Batch number = 7
Evaluating:   8%|▊         | 7/84 [00:03<00:37,  2.08it/s]11/28/2021 01:15:10 - INFO - __main__ -   Batch number = 8
Evaluating:  10%|▉         | 8/84 [00:03<00:36,  2.08it/s]11/28/2021 01:15:10 - INFO - __main__ -   Batch number = 9
Evaluating:  11%|█         | 9/84 [00:04<00:36,  2.07it/s]11/28/2021 01:15:11 - INFO - __main__ -   Batch number = 10
Evaluating:  12%|█▏        | 10/84 [00:04<00:35,  2.07it/s]11/28/2021 01:15:11 - INFO - __main__ -   Batch number = 11
Evaluating:  13%|█▎        | 11/84 [00:05<00:35,  2.07it/s]11/28/2021 01:15:12 - INFO - __main__ -   Batch number = 12
Evaluating:  14%|█▍        | 12/84 [00:05<00:34,  2.07it/s]11/28/2021 01:15:12 - INFO - __main__ -   Batch number = 13
Evaluating:  15%|█▌        | 13/84 [00:06<00:34,  2.07it/s]11/28/2021 01:15:13 - INFO - __main__ -   Batch number = 14
Evaluating:  17%|█▋        | 14/84 [00:06<00:34,  2.06it/s]11/28/2021 01:15:13 - INFO - __main__ -   Batch number = 15
Evaluating:  18%|█▊        | 15/84 [00:07<00:33,  2.05it/s]11/28/2021 01:15:14 - INFO - __main__ -   Batch number = 16
Evaluating:  19%|█▉        | 16/84 [00:07<00:33,  2.03it/s]11/28/2021 01:15:14 - INFO - __main__ -   Batch number = 17
Evaluating:  20%|██        | 17/84 [00:08<00:31,  2.11it/s]11/28/2021 01:15:15 - INFO - __main__ -   Batch number = 18
Evaluating:  21%|██▏       | 18/84 [00:08<00:31,  2.06it/s]11/28/2021 01:15:15 - INFO - __main__ -   Batch number = 19
Evaluating:  23%|██▎       | 19/84 [00:09<00:31,  2.05it/s]11/28/2021 01:15:16 - INFO - __main__ -   Batch number = 20
Evaluating:  24%|██▍       | 20/84 [00:09<00:31,  2.05it/s]11/28/2021 01:15:16 - INFO - __main__ -   Batch number = 21
Evaluating:  25%|██▌       | 21/84 [00:10<00:30,  2.05it/s]11/28/2021 01:15:17 - INFO - __main__ -   Batch number = 22
Evaluating:  26%|██▌       | 22/84 [00:10<00:30,  2.05it/s]11/28/2021 01:15:17 - INFO - __main__ -   Batch number = 23
Evaluating:  27%|██▋       | 23/84 [00:11<00:29,  2.05it/s]11/28/2021 01:15:18 - INFO - __main__ -   Batch number = 24
Evaluating:  29%|██▊       | 24/84 [00:11<00:29,  2.06it/s]11/28/2021 01:15:18 - INFO - __main__ -   Batch number = 25
Evaluating:  30%|██▉       | 25/84 [00:12<00:28,  2.06it/s]11/28/2021 01:15:19 - INFO - __main__ -   Batch number = 26
Evaluating:  31%|███       | 26/84 [00:12<00:28,  2.06it/s]11/28/2021 01:15:19 - INFO - __main__ -   Batch number = 27
Evaluating:  32%|███▏      | 27/84 [00:13<00:27,  2.05it/s]11/28/2021 01:15:20 - INFO - __main__ -   Batch number = 28
Evaluating:  33%|███▎      | 28/84 [00:13<00:30,  1.84it/s]11/28/2021 01:15:20 - INFO - __main__ -   Batch number = 29
Evaluating:  35%|███▍      | 29/84 [00:14<00:28,  1.91it/s]11/28/2021 01:15:21 - INFO - __main__ -   Batch number = 30
Evaluating:  36%|███▌      | 30/84 [00:14<00:27,  1.96it/s]11/28/2021 01:15:21 - INFO - __main__ -   Batch number = 31
Evaluating:  37%|███▋      | 31/84 [00:15<00:26,  1.99it/s]11/28/2021 01:15:22 - INFO - __main__ -   Batch number = 32
Evaluating:  38%|███▊      | 32/84 [00:15<00:25,  2.01it/s]11/28/2021 01:15:22 - INFO - __main__ -   Batch number = 33
Evaluating:  39%|███▉      | 33/84 [00:16<00:25,  2.01it/s]11/28/2021 01:15:23 - INFO - __main__ -   Batch number = 34
Evaluating:  40%|████      | 34/84 [00:16<00:23,  2.14it/s]11/28/2021 01:15:23 - INFO - __main__ -   Batch number = 35
Evaluating:  42%|████▏     | 35/84 [00:17<00:23,  2.12it/s]11/28/2021 01:15:24 - INFO - __main__ -   Batch number = 36
Evaluating:  43%|████▎     | 36/84 [00:17<00:21,  2.22it/s]11/28/2021 01:15:24 - INFO - __main__ -   Batch number = 37
Evaluating:  44%|████▍     | 37/84 [00:17<00:21,  2.20it/s]11/28/2021 01:15:24 - INFO - __main__ -   Batch number = 38
Evaluating:  45%|████▌     | 38/84 [00:18<00:20,  2.20it/s]11/28/2021 01:15:25 - INFO - __main__ -   Batch number = 39
Evaluating:  46%|████▋     | 39/84 [00:18<00:21,  2.06it/s]11/28/2021 01:15:25 - INFO - __main__ -   Batch number = 40
Evaluating:  48%|████▊     | 40/84 [00:19<00:20,  2.14it/s]11/28/2021 01:15:26 - INFO - __main__ -   Batch number = 41
Evaluating:  49%|████▉     | 41/84 [00:19<00:20,  2.15it/s]11/28/2021 01:15:26 - INFO - __main__ -   Batch number = 42
Evaluating:  50%|█████     | 42/84 [00:20<00:19,  2.12it/s]11/28/2021 01:15:27 - INFO - __main__ -   Batch number = 43
Evaluating:  51%|█████     | 43/84 [00:20<00:19,  2.10it/s]11/28/2021 01:15:27 - INFO - __main__ -   Batch number = 44
Evaluating:  52%|█████▏    | 44/84 [00:21<00:19,  2.09it/s]11/28/2021 01:15:28 - INFO - __main__ -   Batch number = 45
Evaluating:  54%|█████▎    | 45/84 [00:21<00:18,  2.07it/s]11/28/2021 01:15:28 - INFO - __main__ -   Batch number = 46
Evaluating:  55%|█████▍    | 46/84 [00:22<00:18,  2.07it/s]11/28/2021 01:15:29 - INFO - __main__ -   Batch number = 47
Evaluating:  56%|█████▌    | 47/84 [00:22<00:17,  2.06it/s]11/28/2021 01:15:29 - INFO - __main__ -   Batch number = 48
Evaluating:  57%|█████▋    | 48/84 [00:23<00:16,  2.12it/s]11/28/2021 01:15:30 - INFO - __main__ -   Batch number = 49
Evaluating:  58%|█████▊    | 49/84 [00:23<00:17,  1.98it/s]11/28/2021 01:15:30 - INFO - __main__ -   Batch number = 50
Evaluating:  60%|█████▉    | 50/84 [00:24<00:17,  1.99it/s]11/28/2021 01:15:31 - INFO - __main__ -   Batch number = 51
Evaluating:  61%|██████    | 51/84 [00:24<00:16,  2.00it/s]11/28/2021 01:15:31 - INFO - __main__ -   Batch number = 52
Evaluating:  62%|██████▏   | 52/84 [00:25<00:15,  2.02it/s]11/28/2021 01:15:32 - INFO - __main__ -   Batch number = 53
Evaluating:  63%|██████▎   | 53/84 [00:25<00:15,  2.02it/s]11/28/2021 01:15:32 - INFO - __main__ -   Batch number = 54
Evaluating:  64%|██████▍   | 54/84 [00:26<00:14,  2.03it/s]11/28/2021 01:15:33 - INFO - __main__ -   Batch number = 55
Evaluating:  65%|██████▌   | 55/84 [00:26<00:14,  2.02it/s]11/28/2021 01:15:33 - INFO - __main__ -   Batch number = 56
Evaluating:  67%|██████▋   | 56/84 [00:27<00:13,  2.03it/s]11/28/2021 01:15:34 - INFO - __main__ -   Batch number = 57
Evaluating:  68%|██████▊   | 57/84 [00:27<00:13,  2.02it/s]11/28/2021 01:15:34 - INFO - __main__ -   Batch number = 58
Evaluating:  69%|██████▉   | 58/84 [00:28<00:12,  2.03it/s]11/28/2021 01:15:35 - INFO - __main__ -   Batch number = 59
Evaluating:  70%|███████   | 59/84 [00:28<00:12,  2.02it/s]11/28/2021 01:15:35 - INFO - __main__ -   Batch number = 60
Evaluating:  71%|███████▏  | 60/84 [00:29<00:11,  2.02it/s]11/28/2021 01:15:36 - INFO - __main__ -   Batch number = 61
Evaluating:  73%|███████▎  | 61/84 [00:29<00:11,  2.03it/s]11/28/2021 01:15:36 - INFO - __main__ -   Batch number = 62
Evaluating:  74%|███████▍  | 62/84 [00:30<00:10,  2.04it/s]11/28/2021 01:15:37 - INFO - __main__ -   Batch number = 63
Evaluating:  75%|███████▌  | 63/84 [00:30<00:10,  2.05it/s]11/28/2021 01:15:37 - INFO - __main__ -   Batch number = 64
Evaluating:  76%|███████▌  | 64/84 [00:31<00:09,  2.04it/s]11/28/2021 01:15:38 - INFO - __main__ -   Batch number = 65
Evaluating:  77%|███████▋  | 65/84 [00:31<00:09,  2.04it/s]11/28/2021 01:15:38 - INFO - __main__ -   Batch number = 66
Evaluating:  79%|███████▊  | 66/84 [00:32<00:08,  2.04it/s]11/28/2021 01:15:39 - INFO - __main__ -   Batch number = 67
Evaluating:  80%|███████▉  | 67/84 [00:32<00:08,  2.04it/s]11/28/2021 01:15:39 - INFO - __main__ -   Batch number = 68
Evaluating:  81%|████████  | 68/84 [00:33<00:07,  2.03it/s]11/28/2021 01:15:40 - INFO - __main__ -   Batch number = 69
Evaluating:  82%|████████▏ | 69/84 [00:33<00:07,  1.92it/s]11/28/2021 01:15:40 - INFO - __main__ -   Batch number = 70
Evaluating:  83%|████████▎ | 70/84 [00:34<00:07,  1.96it/s]11/28/2021 01:15:41 - INFO - __main__ -   Batch number = 71
Evaluating:  85%|████████▍ | 71/84 [00:34<00:06,  1.99it/s]11/28/2021 01:15:41 - INFO - __main__ -   Batch number = 72
Evaluating:  86%|████████▌ | 72/84 [00:35<00:05,  2.01it/s]11/28/2021 01:15:42 - INFO - __main__ -   Batch number = 73
Evaluating:  87%|████████▋ | 73/84 [00:35<00:05,  2.05it/s]11/28/2021 01:15:42 - INFO - __main__ -   Batch number = 74
Evaluating:  88%|████████▊ | 74/84 [00:35<00:04,  2.26it/s]11/28/2021 01:15:42 - INFO - __main__ -   Batch number = 75
Evaluating:  89%|████████▉ | 75/84 [00:36<00:03,  2.48it/s]11/28/2021 01:15:43 - INFO - __main__ -   Batch number = 76
Evaluating:  90%|█████████ | 76/84 [00:36<00:03,  2.66it/s]11/28/2021 01:15:43 - INFO - __main__ -   Batch number = 77
Evaluating:  92%|█████████▏| 77/84 [00:36<00:02,  2.81it/s]11/28/2021 01:15:43 - INFO - __main__ -   Batch number = 78
Evaluating:  93%|█████████▎| 78/84 [00:37<00:02,  2.87it/s]11/28/2021 01:15:44 - INFO - __main__ -   Batch number = 79
Evaluating:  94%|█████████▍| 79/84 [00:37<00:01,  2.93it/s]11/28/2021 01:15:44 - INFO - __main__ -   Batch number = 80
Evaluating:  95%|█████████▌| 80/84 [00:37<00:01,  3.01it/s]11/28/2021 01:15:44 - INFO - __main__ -   Batch number = 81
Evaluating:  96%|█████████▋| 81/84 [00:38<00:00,  3.05it/s]11/28/2021 01:15:45 - INFO - __main__ -   Batch number = 82
Evaluating:  98%|█████████▊| 82/84 [00:38<00:00,  3.10it/s]11/28/2021 01:15:45 - INFO - __main__ -   Batch number = 83
Evaluating:  99%|█████████▉| 83/84 [00:38<00:00,  3.11it/s]11/28/2021 01:15:45 - INFO - __main__ -   Batch number = 84
Evaluating: 100%|██████████| 84/84 [00:39<00:00,  3.19it/s]Evaluating: 100%|██████████| 84/84 [00:39<00:00,  2.15it/s]
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:15:47 - INFO - __main__ -   ***** Evaluation result  in hi *****
11/28/2021 01:15:47 - INFO - __main__ -     f1 = 0.6556488081551315
11/28/2021 01:15:47 - INFO - __main__ -     loss = 1.245936672602381
11/28/2021 01:15:47 - INFO - __main__ -     precision = 0.6559504639214784
11/28/2021 01:15:47 - INFO - __main__ -     recall = 0.6553474297096453
47.63user 16.49system 1:05.44elapsed 97%CPU (0avgtext+0avgdata 3986408maxresident)k
0inputs+688outputs (0major+1482937minor)pagefaults 0swaps
Traceback (most recent call last):
  File "third_party/my_run_tag.py", line 21, in <module>
    import argparse
  File "/usr/lib/python3.7/argparse.py", line 91, in <module>
    from gettext import gettext as _, ngettext
  File "/usr/lib/python3.7/gettext.py", line 84, in <module>
    """, re.VERBOSE|re.DOTALL)
  File "/usr/lib/python3.7/re.py", line 236, in compile
    return _compile(pattern, flags)
  File "/usr/lib/python3.7/re.py", line 288, in _compile
    p = sre_compile.compile(pattern, flags)
  File "/usr/lib/python3.7/sre_compile.py", line 764, in compile
    p = sre_parse.parse(p, flags)
  File "/usr/lib/python3.7/sre_parse.py", line 924, in parse
    p = _parse_sub(source, pattern, flags & SRE_FLAG_VERBOSE, 0)
  File "/usr/lib/python3.7/sre_parse.py", line 420, in _parse_sub
    not nested and not items))
  File "/usr/lib/python3.7/sre_parse.py", line 489, in _parse
    if verbose:
KeyboardInterrupt
Command exited with non-zero status 1
0.05user 0.00system 0:00.05elapsed 98%CPU (0avgtext+0avgdata 13084maxresident)k
48inputs+0outputs (0major+2091minor)pagefaults 0swaps
Traceback (most recent call last):
  File "third_party/my_run_tag.py", line 32, in <module>
    import torch
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/torch/__init__.py", line 29, in <module>
    from .torch_version import __version__ as __version__
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/torch/torch_version.py", line 3, in <module>
    from pkg_resources import packaging  # type: ignore[attr-defined]
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/pkg_resources/__init__.py", line 78, in <module>
    __import__('pkg_resources.extern.packaging.requirements')
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/pkg_resources/_vendor/packaging/requirements.py", line 10, in <module>
    from pkg_resources.extern.pyparsing import (  # noqa
  File "<frozen importlib._bootstrap>", line 983, in _find_and_load
  File "<frozen importlib._bootstrap>", line 967, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 670, in _load_unlocked
  File "<frozen importlib._bootstrap>", line 583, in module_from_spec
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/pkg_resources/extern/__init__.py", line 52, in create_module
    return self.load_module(spec.name)
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/pkg_resources/extern/__init__.py", line 37, in load_module
    __import__(extant)
  File "<frozen importlib._bootstrap>", line 983, in _find_and_load
  File "<frozen importlib._bootstrap>", line 967, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 677, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 818, in get_code
  File "<frozen importlib._bootstrap_external>", line 917, in get_data
KeyboardInterrupt
Command exited with non-zero status 1
1.18user 1.79system 0:00.31elapsed 955%CPU (0avgtext+0avgdata 34644maxresident)k
24inputs+8outputs (0major+8719minor)pagefaults 0swaps
Traceback (most recent call last):
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/numpy/core/__init__.py", line 22, in <module>
    from . import multiarray
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/numpy/core/multiarray.py", line 12, in <module>
    from . import overrides
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/numpy/core/overrides.py", line 7, in <module>
    from numpy.core._multiarray_umath import (
ImportError: PyCapsule_Import could not import module "datetime"

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "third_party/my_run_tag.py", line 30, in <module>
    import numpy as np
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/numpy/__init__.py", line 150, in <module>
    from . import core
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/numpy/core/__init__.py", line 48, in <module>
    raise ImportError(msg)
ImportError: 

IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!

Importing the numpy C-extensions failed. This error can happen for
many reasons, often due to issues with your setup or how NumPy was
installed.

We have compiled some common reasons and troubleshooting tips at:

    https://numpy.org/devdocs/user/troubleshooting-importerror.html

Please note and check the following:

  * The Python version is: Python3.7 from "/home/abhijeet/rohan/venvs/cloud-emea/bin/python"
  * The NumPy version is: "1.21.4"

and make sure that they are the versions you expect.
Please carefully study the documentation linked above for further help.

Original error was: PyCapsule_Import could not import module "datetime"

Command exited with non-zero status 1
0.22user 0.32system 0:00.08elapsed 660%CPU (0avgtext+0avgdata 18616maxresident)k
0inputs+0outputs (0major+2910minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:16:24 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='wo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:16:24 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:16:24 - INFO - __main__ -   Seed = 1
11/28/2021 01:16:24 - INFO - root -   save model
11/28/2021 01:16:24 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='wo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:16:24 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:16:27 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:16:33 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:16:33 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:16:33 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
11/28/2021 01:16:33 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:16:33 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:16:33 - INFO - root -   loading lang adpater cs/wiki@ukp
11/28/2021 01:16:33 - INFO - __main__ -   Adapter Languages : ['cs'], Length : 1
11/28/2021 01:16:33 - INFO - __main__ -   Adapter Names ['cs/wiki@ukp'], Length : 1
11/28/2021 01:16:33 - INFO - __main__ -   Language = cs
11/28/2021 01:16:33 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/cs/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_cs_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/adapter_config.json
Adding adapter 'cs' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/head_config.json
11/28/2021 01:16:43 - INFO - __main__ -   Language wo, split test does not exist
15.15user 5.33system 0:21.12elapsed 96%CPU (0avgtext+0avgdata 3985844maxresident)k
0inputs+56outputs (0major+1511070minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:16:45 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='wo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:16:45 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:16:45 - INFO - __main__ -   Seed = 2
11/28/2021 01:16:45 - INFO - root -   save model
11/28/2021 01:16:45 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='wo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:16:45 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:16:48 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:16:55 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:16:55 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:16:55 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
11/28/2021 01:16:55 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:16:55 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:16:55 - INFO - root -   loading lang adpater cs/wiki@ukp
11/28/2021 01:16:55 - INFO - __main__ -   Adapter Languages : ['cs'], Length : 1
11/28/2021 01:16:55 - INFO - __main__ -   Adapter Names ['cs/wiki@ukp'], Length : 1
11/28/2021 01:16:55 - INFO - __main__ -   Language = cs
11/28/2021 01:16:55 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/cs/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_cs_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/adapter_config.json
Adding adapter 'cs' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/head_config.json
11/28/2021 01:17:04 - INFO - __main__ -   Language wo, split test does not exist
14.94user 6.57system 0:20.84elapsed 103%CPU (0avgtext+0avgdata 3988544maxresident)k
0inputs+56outputs (0major+1695842minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:17:06 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='wo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:17:06 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:17:06 - INFO - __main__ -   Seed = 3
11/28/2021 01:17:06 - INFO - root -   save model
11/28/2021 01:17:06 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='wo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:17:06 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:17:09 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:17:15 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:17:15 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:17:15 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
11/28/2021 01:17:15 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:17:15 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:17:15 - INFO - root -   loading lang adpater cs/wiki@ukp
11/28/2021 01:17:15 - INFO - __main__ -   Adapter Languages : ['cs'], Length : 1
11/28/2021 01:17:15 - INFO - __main__ -   Adapter Names ['cs/wiki@ukp'], Length : 1
11/28/2021 01:17:15 - INFO - __main__ -   Language = cs
11/28/2021 01:17:15 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/cs/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_cs_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/adapter_config.json
Adding adapter 'cs' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/head_config.json
11/28/2021 01:17:22 - INFO - __main__ -   Language wo, split test does not exist
14.43user 5.30system 0:18.37elapsed 107%CPU (0avgtext+0avgdata 3989940maxresident)k
0inputs+48outputs (0major+1244012minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:17:41 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='eu', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:17:41 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:17:41 - INFO - __main__ -   Seed = 1
11/28/2021 01:17:41 - INFO - root -   save model
11/28/2021 01:17:41 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='eu', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:17:41 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:17:43 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:17:50 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:17:50 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:17:50 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
11/28/2021 01:17:50 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:17:50 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:17:50 - INFO - root -   loading lang adpater cs/wiki@ukp
11/28/2021 01:17:50 - INFO - __main__ -   Adapter Languages : ['cs'], Length : 1
11/28/2021 01:17:50 - INFO - __main__ -   Adapter Names ['cs/wiki@ukp'], Length : 1
11/28/2021 01:17:50 - INFO - __main__ -   Language = cs
11/28/2021 01:17:50 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/cs/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_cs_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/adapter_config.json
Adding adapter 'cs' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/head_config.json
11/28/2021 01:17:58 - INFO - __main__ -   Language adapter for eu not found, using cs instead
11/28/2021 01:17:58 - INFO - __main__ -   Set active language adapter to cs
11/28/2021 01:17:58 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:17:58 - INFO - __main__ -   Adapter Languages = ['cs']
11/28/2021 01:17:58 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_eu_bert-base-multilingual-cased_128
11/28/2021 01:17:58 - INFO - __main__ -   ***** Running evaluation  in eu *****
11/28/2021 01:17:58 - INFO - __main__ -     Num examples = 1799
11/28/2021 01:17:58 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/57 [00:00<?, ?it/s]11/28/2021 01:17:58 - INFO - __main__ -   Batch number = 1
Evaluating:   2%|▏         | 1/57 [00:00<00:08,  6.69it/s]11/28/2021 01:17:58 - INFO - __main__ -   Batch number = 2
Evaluating:   4%|▎         | 2/57 [00:00<00:07,  6.91it/s]11/28/2021 01:17:58 - INFO - __main__ -   Batch number = 3
Evaluating:   5%|▌         | 3/57 [00:00<00:07,  7.01it/s]11/28/2021 01:17:59 - INFO - __main__ -   Batch number = 4
Evaluating:   7%|▋         | 4/57 [00:00<00:07,  7.01it/s]11/28/2021 01:17:59 - INFO - __main__ -   Batch number = 5
Evaluating:   9%|▉         | 5/57 [00:00<00:07,  7.05it/s]11/28/2021 01:17:59 - INFO - __main__ -   Batch number = 6
Evaluating:  11%|█         | 6/57 [00:00<00:07,  7.07it/s]11/28/2021 01:17:59 - INFO - __main__ -   Batch number = 7
Evaluating:  12%|█▏        | 7/57 [00:00<00:07,  7.08it/s]11/28/2021 01:17:59 - INFO - __main__ -   Batch number = 8
Evaluating:  14%|█▍        | 8/57 [00:01<00:06,  7.08it/s]11/28/2021 01:17:59 - INFO - __main__ -   Batch number = 9
Evaluating:  16%|█▌        | 9/57 [00:01<00:06,  7.07it/s]11/28/2021 01:17:59 - INFO - __main__ -   Batch number = 10
Evaluating:  18%|█▊        | 10/57 [00:01<00:06,  7.08it/s]11/28/2021 01:18:00 - INFO - __main__ -   Batch number = 11
Evaluating:  19%|█▉        | 11/57 [00:01<00:06,  7.08it/s]11/28/2021 01:18:00 - INFO - __main__ -   Batch number = 12
Evaluating:  21%|██        | 12/57 [00:01<00:06,  7.06it/s]11/28/2021 01:18:00 - INFO - __main__ -   Batch number = 13
Evaluating:  23%|██▎       | 13/57 [00:01<00:06,  7.06it/s]11/28/2021 01:18:00 - INFO - __main__ -   Batch number = 14
Evaluating:  25%|██▍       | 14/57 [00:01<00:06,  7.06it/s]11/28/2021 01:18:00 - INFO - __main__ -   Batch number = 15
Evaluating:  26%|██▋       | 15/57 [00:02<00:05,  7.06it/s]11/28/2021 01:18:00 - INFO - __main__ -   Batch number = 16
Evaluating:  28%|██▊       | 16/57 [00:02<00:05,  7.05it/s]11/28/2021 01:18:00 - INFO - __main__ -   Batch number = 17
Evaluating:  30%|██▉       | 17/57 [00:02<00:05,  7.05it/s]11/28/2021 01:18:01 - INFO - __main__ -   Batch number = 18
Evaluating:  32%|███▏      | 18/57 [00:02<00:05,  7.02it/s]11/28/2021 01:18:01 - INFO - __main__ -   Batch number = 19
Evaluating:  33%|███▎      | 19/57 [00:02<00:05,  7.02it/s]11/28/2021 01:18:01 - INFO - __main__ -   Batch number = 20
Evaluating:  35%|███▌      | 20/57 [00:02<00:05,  7.00it/s]11/28/2021 01:18:01 - INFO - __main__ -   Batch number = 21
Evaluating:  37%|███▋      | 21/57 [00:03<00:05,  6.26it/s]11/28/2021 01:18:01 - INFO - __main__ -   Batch number = 22
Evaluating:  39%|███▊      | 22/57 [00:03<00:05,  6.47it/s]11/28/2021 01:18:01 - INFO - __main__ -   Batch number = 23
Evaluating:  40%|████      | 23/57 [00:03<00:05,  6.63it/s]11/28/2021 01:18:01 - INFO - __main__ -   Batch number = 24
Evaluating:  42%|████▏     | 24/57 [00:03<00:04,  6.74it/s]11/28/2021 01:18:02 - INFO - __main__ -   Batch number = 25
Evaluating:  44%|████▍     | 25/57 [00:03<00:04,  6.81it/s]11/28/2021 01:18:02 - INFO - __main__ -   Batch number = 26
Evaluating:  46%|████▌     | 26/57 [00:03<00:04,  6.87it/s]11/28/2021 01:18:02 - INFO - __main__ -   Batch number = 27
Evaluating:  47%|████▋     | 27/57 [00:03<00:04,  6.90it/s]11/28/2021 01:18:02 - INFO - __main__ -   Batch number = 28
Evaluating:  49%|████▉     | 28/57 [00:04<00:04,  6.94it/s]11/28/2021 01:18:02 - INFO - __main__ -   Batch number = 29
Evaluating:  51%|█████     | 29/57 [00:04<00:04,  6.95it/s]11/28/2021 01:18:02 - INFO - __main__ -   Batch number = 30
Evaluating:  53%|█████▎    | 30/57 [00:04<00:03,  6.96it/s]11/28/2021 01:18:02 - INFO - __main__ -   Batch number = 31
Evaluating:  54%|█████▍    | 31/57 [00:04<00:03,  6.96it/s]11/28/2021 01:18:03 - INFO - __main__ -   Batch number = 32
Evaluating:  56%|█████▌    | 32/57 [00:04<00:03,  6.93it/s]11/28/2021 01:18:03 - INFO - __main__ -   Batch number = 33
Evaluating:  58%|█████▊    | 33/57 [00:04<00:03,  6.94it/s]11/28/2021 01:18:03 - INFO - __main__ -   Batch number = 34
Evaluating:  60%|█████▉    | 34/57 [00:04<00:03,  6.95it/s]11/28/2021 01:18:03 - INFO - __main__ -   Batch number = 35
Evaluating:  61%|██████▏   | 35/57 [00:05<00:03,  6.96it/s]11/28/2021 01:18:03 - INFO - __main__ -   Batch number = 36
Evaluating:  63%|██████▎   | 36/57 [00:05<00:03,  6.96it/s]11/28/2021 01:18:03 - INFO - __main__ -   Batch number = 37
Evaluating:  65%|██████▍   | 37/57 [00:05<00:02,  6.94it/s]11/28/2021 01:18:03 - INFO - __main__ -   Batch number = 38
Evaluating:  67%|██████▋   | 38/57 [00:05<00:02,  6.93it/s]11/28/2021 01:18:04 - INFO - __main__ -   Batch number = 39
Evaluating:  68%|██████▊   | 39/57 [00:05<00:02,  6.94it/s]11/28/2021 01:18:04 - INFO - __main__ -   Batch number = 40
Evaluating:  70%|███████   | 40/57 [00:05<00:02,  6.92it/s]11/28/2021 01:18:04 - INFO - __main__ -   Batch number = 41
Evaluating:  72%|███████▏  | 41/57 [00:05<00:02,  6.90it/s]11/28/2021 01:18:04 - INFO - __main__ -   Batch number = 42
Evaluating:  74%|███████▎  | 42/57 [00:06<00:02,  6.83it/s]11/28/2021 01:18:04 - INFO - __main__ -   Batch number = 43
Evaluating:  75%|███████▌  | 43/57 [00:06<00:02,  6.77it/s]11/28/2021 01:18:04 - INFO - __main__ -   Batch number = 44
Evaluating:  77%|███████▋  | 44/57 [00:06<00:01,  6.80it/s]11/28/2021 01:18:04 - INFO - __main__ -   Batch number = 45
Evaluating:  79%|███████▉  | 45/57 [00:06<00:01,  6.82it/s]11/28/2021 01:18:05 - INFO - __main__ -   Batch number = 46
Evaluating:  81%|████████  | 46/57 [00:06<00:01,  6.81it/s]11/28/2021 01:18:05 - INFO - __main__ -   Batch number = 47
Evaluating:  82%|████████▏ | 47/57 [00:06<00:01,  6.82it/s]11/28/2021 01:18:05 - INFO - __main__ -   Batch number = 48
Evaluating:  84%|████████▍ | 48/57 [00:06<00:01,  6.82it/s]11/28/2021 01:18:05 - INFO - __main__ -   Batch number = 49
Evaluating:  86%|████████▌ | 49/57 [00:07<00:01,  6.80it/s]11/28/2021 01:18:05 - INFO - __main__ -   Batch number = 50
Evaluating:  88%|████████▊ | 50/57 [00:07<00:01,  6.80it/s]11/28/2021 01:18:05 - INFO - __main__ -   Batch number = 51
Evaluating:  89%|████████▉ | 51/57 [00:07<00:00,  6.80it/s]11/28/2021 01:18:05 - INFO - __main__ -   Batch number = 52
Evaluating:  91%|█████████ | 52/57 [00:07<00:00,  6.76it/s]11/28/2021 01:18:06 - INFO - __main__ -   Batch number = 53
Evaluating:  93%|█████████▎| 53/57 [00:07<00:00,  6.77it/s]11/28/2021 01:18:06 - INFO - __main__ -   Batch number = 54
Evaluating:  95%|█████████▍| 54/57 [00:07<00:00,  6.78it/s]11/28/2021 01:18:06 - INFO - __main__ -   Batch number = 55
Evaluating:  96%|█████████▋| 55/57 [00:08<00:00,  5.59it/s]11/28/2021 01:18:06 - INFO - __main__ -   Batch number = 56
Evaluating:  98%|█████████▊| 56/57 [00:08<00:00,  5.94it/s]11/28/2021 01:18:06 - INFO - __main__ -   Batch number = 57
Evaluating: 100%|██████████| 57/57 [00:08<00:00,  6.89it/s]
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:18:07 - INFO - __main__ -   ***** Evaluation result  in eu *****
11/28/2021 01:18:07 - INFO - __main__ -     f1 = 0.6053864449943541
11/28/2021 01:18:07 - INFO - __main__ -     loss = 1.286177677020692
11/28/2021 01:18:07 - INFO - __main__ -     precision = 0.6376012145748988
11/28/2021 01:18:07 - INFO - __main__ -     recall = 0.5762704111969995
21.37user 7.10system 0:29.16elapsed 97%CPU (0avgtext+0avgdata 3998152maxresident)k
0inputs+336outputs (0major+1274406minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:18:10 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='eu', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:18:10 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:18:10 - INFO - __main__ -   Seed = 2
11/28/2021 01:18:10 - INFO - root -   save model
11/28/2021 01:18:10 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='eu', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:18:10 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:18:13 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:18:19 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:18:19 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:18:19 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
11/28/2021 01:18:19 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:18:19 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:18:19 - INFO - root -   loading lang adpater cs/wiki@ukp
11/28/2021 01:18:19 - INFO - __main__ -   Adapter Languages : ['cs'], Length : 1
11/28/2021 01:18:19 - INFO - __main__ -   Adapter Names ['cs/wiki@ukp'], Length : 1
11/28/2021 01:18:19 - INFO - __main__ -   Language = cs
11/28/2021 01:18:19 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/cs/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_cs_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/adapter_config.json
Adding adapter 'cs' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/head_config.json
11/28/2021 01:18:26 - INFO - __main__ -   Language adapter for eu not found, using cs instead
11/28/2021 01:18:26 - INFO - __main__ -   Set active language adapter to cs
11/28/2021 01:18:26 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:18:26 - INFO - __main__ -   Adapter Languages = ['cs']
11/28/2021 01:18:26 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_eu_bert-base-multilingual-cased_128
11/28/2021 01:18:27 - INFO - __main__ -   ***** Running evaluation  in eu *****
11/28/2021 01:18:27 - INFO - __main__ -     Num examples = 1799
11/28/2021 01:18:27 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/57 [00:00<?, ?it/s]11/28/2021 01:18:27 - INFO - __main__ -   Batch number = 1
Evaluating:   2%|▏         | 1/57 [00:00<00:08,  6.54it/s]11/28/2021 01:18:27 - INFO - __main__ -   Batch number = 2
Evaluating:   4%|▎         | 2/57 [00:00<00:08,  6.80it/s]11/28/2021 01:18:27 - INFO - __main__ -   Batch number = 3
Evaluating:   5%|▌         | 3/57 [00:00<00:07,  6.94it/s]11/28/2021 01:18:27 - INFO - __main__ -   Batch number = 4
Evaluating:   7%|▋         | 4/57 [00:00<00:07,  7.01it/s]11/28/2021 01:18:27 - INFO - __main__ -   Batch number = 5
Evaluating:   9%|▉         | 5/57 [00:00<00:07,  7.04it/s]11/28/2021 01:18:28 - INFO - __main__ -   Batch number = 6
Evaluating:  11%|█         | 6/57 [00:00<00:07,  7.01it/s]11/28/2021 01:18:28 - INFO - __main__ -   Batch number = 7
Evaluating:  12%|█▏        | 7/57 [00:01<00:07,  6.97it/s]11/28/2021 01:18:28 - INFO - __main__ -   Batch number = 8
Evaluating:  14%|█▍        | 8/57 [00:01<00:07,  6.94it/s]11/28/2021 01:18:28 - INFO - __main__ -   Batch number = 9
Evaluating:  16%|█▌        | 9/57 [00:01<00:06,  6.91it/s]11/28/2021 01:18:28 - INFO - __main__ -   Batch number = 10
Evaluating:  18%|█▊        | 10/57 [00:01<00:06,  6.83it/s]11/28/2021 01:18:28 - INFO - __main__ -   Batch number = 11
Evaluating:  19%|█▉        | 11/57 [00:01<00:06,  6.84it/s]11/28/2021 01:18:28 - INFO - __main__ -   Batch number = 12
Evaluating:  21%|██        | 12/57 [00:01<00:06,  6.82it/s]11/28/2021 01:18:29 - INFO - __main__ -   Batch number = 13
Evaluating:  23%|██▎       | 13/57 [00:01<00:06,  6.84it/s]11/28/2021 01:18:29 - INFO - __main__ -   Batch number = 14
Evaluating:  25%|██▍       | 14/57 [00:02<00:06,  6.85it/s]11/28/2021 01:18:29 - INFO - __main__ -   Batch number = 15
Evaluating:  26%|██▋       | 15/57 [00:02<00:06,  6.90it/s]11/28/2021 01:18:29 - INFO - __main__ -   Batch number = 16
Evaluating:  28%|██▊       | 16/57 [00:02<00:05,  6.93it/s]11/28/2021 01:18:29 - INFO - __main__ -   Batch number = 17
Evaluating:  30%|██▉       | 17/57 [00:02<00:05,  6.88it/s]11/28/2021 01:18:29 - INFO - __main__ -   Batch number = 18
Evaluating:  32%|███▏      | 18/57 [00:02<00:05,  6.86it/s]11/28/2021 01:18:29 - INFO - __main__ -   Batch number = 19
Evaluating:  33%|███▎      | 19/57 [00:02<00:05,  6.74it/s]11/28/2021 01:18:30 - INFO - __main__ -   Batch number = 20
Evaluating:  35%|███▌      | 20/57 [00:02<00:05,  6.75it/s]11/28/2021 01:18:30 - INFO - __main__ -   Batch number = 21
Evaluating:  37%|███▋      | 21/57 [00:03<00:05,  6.72it/s]11/28/2021 01:18:30 - INFO - __main__ -   Batch number = 22
Evaluating:  39%|███▊      | 22/57 [00:03<00:05,  6.69it/s]11/28/2021 01:18:30 - INFO - __main__ -   Batch number = 23
Evaluating:  40%|████      | 23/57 [00:03<00:05,  6.68it/s]11/28/2021 01:18:30 - INFO - __main__ -   Batch number = 24
Evaluating:  42%|████▏     | 24/57 [00:03<00:04,  6.63it/s]11/28/2021 01:18:30 - INFO - __main__ -   Batch number = 25
Evaluating:  44%|████▍     | 25/57 [00:03<00:04,  6.62it/s]11/28/2021 01:18:30 - INFO - __main__ -   Batch number = 26
Evaluating:  46%|████▌     | 26/57 [00:03<00:04,  6.62it/s]11/28/2021 01:18:31 - INFO - __main__ -   Batch number = 27
Evaluating:  47%|████▋     | 27/57 [00:03<00:04,  6.59it/s]11/28/2021 01:18:31 - INFO - __main__ -   Batch number = 28
Evaluating:  49%|████▉     | 28/57 [00:04<00:04,  6.60it/s]11/28/2021 01:18:31 - INFO - __main__ -   Batch number = 29
Evaluating:  51%|█████     | 29/57 [00:04<00:04,  6.58it/s]11/28/2021 01:18:31 - INFO - __main__ -   Batch number = 30
Evaluating:  53%|█████▎    | 30/57 [00:04<00:05,  4.71it/s]11/28/2021 01:18:31 - INFO - __main__ -   Batch number = 31
Evaluating:  54%|█████▍    | 31/57 [00:04<00:05,  5.15it/s]11/28/2021 01:18:32 - INFO - __main__ -   Batch number = 32
Evaluating:  56%|█████▌    | 32/57 [00:04<00:04,  5.49it/s]11/28/2021 01:18:32 - INFO - __main__ -   Batch number = 33
Evaluating:  58%|█████▊    | 33/57 [00:05<00:04,  5.81it/s]11/28/2021 01:18:32 - INFO - __main__ -   Batch number = 34
Evaluating:  60%|█████▉    | 34/57 [00:05<00:03,  5.99it/s]11/28/2021 01:18:32 - INFO - __main__ -   Batch number = 35
Evaluating:  61%|██████▏   | 35/57 [00:05<00:03,  6.15it/s]11/28/2021 01:18:32 - INFO - __main__ -   Batch number = 36
Evaluating:  63%|██████▎   | 36/57 [00:05<00:03,  6.25it/s]11/28/2021 01:18:32 - INFO - __main__ -   Batch number = 37
Evaluating:  65%|██████▍   | 37/57 [00:05<00:03,  6.33it/s]11/28/2021 01:18:33 - INFO - __main__ -   Batch number = 38
Evaluating:  67%|██████▋   | 38/57 [00:05<00:02,  6.38it/s]11/28/2021 01:18:33 - INFO - __main__ -   Batch number = 39
Evaluating:  68%|██████▊   | 39/57 [00:06<00:02,  6.43it/s]11/28/2021 01:18:33 - INFO - __main__ -   Batch number = 40
Evaluating:  70%|███████   | 40/57 [00:06<00:02,  6.33it/s]11/28/2021 01:18:33 - INFO - __main__ -   Batch number = 41
Evaluating:  72%|███████▏  | 41/57 [00:06<00:02,  6.28it/s]11/28/2021 01:18:33 - INFO - __main__ -   Batch number = 42
Evaluating:  74%|███████▎  | 42/57 [00:06<00:02,  6.37it/s]11/28/2021 01:18:33 - INFO - __main__ -   Batch number = 43
Evaluating:  75%|███████▌  | 43/57 [00:06<00:02,  6.43it/s]11/28/2021 01:18:33 - INFO - __main__ -   Batch number = 44
Evaluating:  77%|███████▋  | 44/57 [00:06<00:02,  6.40it/s]11/28/2021 01:18:34 - INFO - __main__ -   Batch number = 45
Evaluating:  79%|███████▉  | 45/57 [00:06<00:01,  6.42it/s]11/28/2021 01:18:34 - INFO - __main__ -   Batch number = 46
Evaluating:  81%|████████  | 46/57 [00:07<00:01,  6.50it/s]11/28/2021 01:18:34 - INFO - __main__ -   Batch number = 47
Evaluating:  82%|████████▏ | 47/57 [00:07<00:01,  6.48it/s]11/28/2021 01:18:34 - INFO - __main__ -   Batch number = 48
Evaluating:  84%|████████▍ | 48/57 [00:07<00:01,  6.54it/s]11/28/2021 01:18:34 - INFO - __main__ -   Batch number = 49
Evaluating:  86%|████████▌ | 49/57 [00:07<00:01,  6.58it/s]11/28/2021 01:18:34 - INFO - __main__ -   Batch number = 50
Evaluating:  88%|████████▊ | 50/57 [00:07<00:01,  6.56it/s]11/28/2021 01:18:35 - INFO - __main__ -   Batch number = 51
Evaluating:  89%|████████▉ | 51/57 [00:07<00:00,  6.58it/s]11/28/2021 01:18:35 - INFO - __main__ -   Batch number = 52
Evaluating:  91%|█████████ | 52/57 [00:08<00:00,  6.57it/s]11/28/2021 01:18:35 - INFO - __main__ -   Batch number = 53
Evaluating:  93%|█████████▎| 53/57 [00:08<00:00,  6.55it/s]11/28/2021 01:18:35 - INFO - __main__ -   Batch number = 54
Evaluating:  95%|█████████▍| 54/57 [00:08<00:00,  6.51it/s]11/28/2021 01:18:35 - INFO - __main__ -   Batch number = 55
Evaluating:  96%|█████████▋| 55/57 [00:08<00:00,  6.52it/s]11/28/2021 01:18:35 - INFO - __main__ -   Batch number = 56
Evaluating:  98%|█████████▊| 56/57 [00:08<00:00,  6.60it/s]11/28/2021 01:18:35 - INFO - __main__ -   Batch number = 57
Evaluating: 100%|██████████| 57/57 [00:08<00:00,  6.58it/s]
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:18:36 - INFO - __main__ -   ***** Evaluation result  in eu *****
11/28/2021 01:18:36 - INFO - __main__ -     f1 = 0.6196465151731696
11/28/2021 01:18:36 - INFO - __main__ -     loss = 1.2513225224980138
11/28/2021 01:18:36 - INFO - __main__ -     precision = 0.646503305333267
11/28/2021 01:18:36 - INFO - __main__ -     recall = 0.594932077025111
22.35user 8.52system 0:29.12elapsed 106%CPU (0avgtext+0avgdata 3995560maxresident)k
0inputs+336outputs (0major+1273735minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:18:38 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bxr', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:18:38 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:18:38 - INFO - __main__ -   Seed = 1
11/28/2021 01:18:38 - INFO - root -   save model
11/28/2021 01:18:38 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bxr', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:18:38 - INFO - __main__ -   Loading pretrained model and tokenizer
PyTorch version 1.10.0+cu102 available.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

11/28/2021 01:18:39 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='eu', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:18:39 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:18:39 - INFO - __main__ -   Seed = 3
11/28/2021 01:18:39 - INFO - root -   save model
11/28/2021 01:18:39 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='eu', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:18:39 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:18:41 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:18:42 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:18:46 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:18:46 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:18:46 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
11/28/2021 01:18:46 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:18:46 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:18:46 - INFO - root -   loading lang adpater cs/wiki@ukp
11/28/2021 01:18:46 - INFO - __main__ -   Adapter Languages : ['cs'], Length : 1
11/28/2021 01:18:46 - INFO - __main__ -   Adapter Names ['cs/wiki@ukp'], Length : 1
11/28/2021 01:18:46 - INFO - __main__ -   Language = cs
11/28/2021 01:18:46 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/cs/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_cs_wiki_pfeiffer.zip.
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:18:48 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:18:48 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:18:48 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
11/28/2021 01:18:48 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:18:48 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:18:48 - INFO - root -   loading lang adpater cs/wiki@ukp
11/28/2021 01:18:48 - INFO - __main__ -   Adapter Languages : ['cs'], Length : 1
11/28/2021 01:18:48 - INFO - __main__ -   Adapter Names ['cs/wiki@ukp'], Length : 1
11/28/2021 01:18:48 - INFO - __main__ -   Language = cs
11/28/2021 01:18:48 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/adapter_config.json
Adding adapter 'cs' of type 'text_lang'.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/cs/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_cs_wiki_pfeiffer.zip.
Loading module weights from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/head_config.json
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/adapter_config.json
Adding adapter 'cs' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/head_config.json
11/28/2021 01:18:53 - INFO - __main__ -   Language bxr, split test does not exist
13.25user 4.31system 0:17.38elapsed 101%CPU (0avgtext+0avgdata 3990264maxresident)k
0inputs+40outputs (0major+1243378minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:18:55 - INFO - __main__ -   Language adapter for eu not found, using cs instead
11/28/2021 01:18:55 - INFO - __main__ -   Set active language adapter to cs
11/28/2021 01:18:55 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:18:55 - INFO - __main__ -   Adapter Languages = ['cs']
11/28/2021 01:18:55 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_eu_bert-base-multilingual-cased_128
11/28/2021 01:18:55 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bxr', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:18:55 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:18:55 - INFO - __main__ -   Seed = 2
11/28/2021 01:18:55 - INFO - root -   save model
11/28/2021 01:18:55 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bxr', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:18:55 - INFO - __main__ -   Loading pretrained model and tokenizer
11/28/2021 01:18:55 - INFO - __main__ -   ***** Running evaluation  in eu *****
11/28/2021 01:18:55 - INFO - __main__ -     Num examples = 1799
11/28/2021 01:18:55 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/57 [00:00<?, ?it/s]11/28/2021 01:18:55 - INFO - __main__ -   Batch number = 1
Evaluating:   2%|▏         | 1/57 [00:00<00:09,  5.90it/s]11/28/2021 01:18:56 - INFO - __main__ -   Batch number = 2
Evaluating:   4%|▎         | 2/57 [00:00<00:08,  6.52it/s]11/28/2021 01:18:56 - INFO - __main__ -   Batch number = 3
Evaluating:   5%|▌         | 3/57 [00:00<00:07,  6.76it/s]11/28/2021 01:18:56 - INFO - __main__ -   Batch number = 4
Evaluating:   7%|▋         | 4/57 [00:00<00:07,  6.89it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
11/28/2021 01:18:56 - INFO - __main__ -   Batch number = 5
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:   9%|▉         | 5/57 [00:00<00:09,  5.71it/s]11/28/2021 01:18:56 - INFO - __main__ -   Batch number = 6
Evaluating:  11%|█         | 6/57 [00:00<00:08,  6.11it/s]11/28/2021 01:18:56 - INFO - __main__ -   Batch number = 7
Evaluating:  12%|█▏        | 7/57 [00:01<00:07,  6.40it/s]11/28/2021 01:18:57 - INFO - __main__ -   Batch number = 8
Evaluating:  14%|█▍        | 8/57 [00:01<00:07,  6.60it/s]11/28/2021 01:18:57 - INFO - __main__ -   Batch number = 9
Evaluating:  16%|█▌        | 9/57 [00:01<00:07,  6.74it/s]11/28/2021 01:18:57 - INFO - __main__ -   Batch number = 10
Evaluating:  18%|█▊        | 10/57 [00:01<00:06,  6.83it/s]11/28/2021 01:18:57 - INFO - __main__ -   Batch number = 11
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  19%|█▉        | 11/57 [00:01<00:06,  6.89it/s]11/28/2021 01:18:57 - INFO - __main__ -   Batch number = 12
Evaluating:  21%|██        | 12/57 [00:01<00:06,  6.94it/s]11/28/2021 01:18:57 - INFO - __main__ -   Batch number = 13
Evaluating:  23%|██▎       | 13/57 [00:01<00:06,  6.98it/s]11/28/2021 01:18:57 - INFO - __main__ -   Batch number = 14
Evaluating:  25%|██▍       | 14/57 [00:02<00:06,  6.98it/s]11/28/2021 01:18:58 - INFO - __main__ -   Batch number = 15
Evaluating:  26%|██▋       | 15/57 [00:02<00:06,  6.98it/s]11/28/2021 01:18:58 - INFO - __main__ -   Batch number = 16
Evaluating:  28%|██▊       | 16/57 [00:02<00:05,  7.00it/s]11/28/2021 01:18:58 - INFO - __main__ -   Batch number = 17
loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
Evaluating:  30%|██▉       | 17/57 [00:02<00:05,  7.01it/s]11/28/2021 01:18:58 - INFO - __main__ -   Batch number = 18
11/28/2021 01:18:58 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
Evaluating:  32%|███▏      | 18/57 [00:02<00:05,  6.99it/s]11/28/2021 01:18:58 - INFO - __main__ -   Batch number = 19
Evaluating:  33%|███▎      | 19/57 [00:02<00:05,  7.01it/s]11/28/2021 01:18:58 - INFO - __main__ -   Batch number = 20
Evaluating:  35%|███▌      | 20/57 [00:02<00:05,  7.01it/s]11/28/2021 01:18:58 - INFO - __main__ -   Batch number = 21
Evaluating:  37%|███▋      | 21/57 [00:03<00:05,  7.00it/s]11/28/2021 01:18:59 - INFO - __main__ -   Batch number = 22
Evaluating:  39%|███▊      | 22/57 [00:03<00:05,  6.98it/s]11/28/2021 01:18:59 - INFO - __main__ -   Batch number = 23
Evaluating:  40%|████      | 23/57 [00:03<00:04,  6.92it/s]11/28/2021 01:18:59 - INFO - __main__ -   Batch number = 24
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  42%|████▏     | 24/57 [00:03<00:04,  6.95it/s]11/28/2021 01:18:59 - INFO - __main__ -   Batch number = 25
Evaluating:  44%|████▍     | 25/57 [00:03<00:04,  6.96it/s]11/28/2021 01:18:59 - INFO - __main__ -   Batch number = 26
Evaluating:  46%|████▌     | 26/57 [00:03<00:04,  6.94it/s]11/28/2021 01:18:59 - INFO - __main__ -   Batch number = 27
Evaluating:  47%|████▋     | 27/57 [00:03<00:04,  6.92it/s]11/28/2021 01:18:59 - INFO - __main__ -   Batch number = 28
Evaluating:  49%|████▉     | 28/57 [00:04<00:04,  6.93it/s]11/28/2021 01:19:00 - INFO - __main__ -   Batch number = 29
Evaluating:  51%|█████     | 29/57 [00:04<00:04,  6.94it/s]11/28/2021 01:19:00 - INFO - __main__ -   Batch number = 30
Evaluating:  53%|█████▎    | 30/57 [00:04<00:03,  6.96it/s]11/28/2021 01:19:00 - INFO - __main__ -   Batch number = 31
Evaluating:  54%|█████▍    | 31/57 [00:04<00:03,  6.97it/s]11/28/2021 01:19:00 - INFO - __main__ -   Batch number = 32
Evaluating:  56%|█████▌    | 32/57 [00:04<00:03,  6.96it/s]11/28/2021 01:19:00 - INFO - __main__ -   Batch number = 33
Evaluating:  58%|█████▊    | 33/57 [00:04<00:03,  6.97it/s]11/28/2021 01:19:00 - INFO - __main__ -   Batch number = 34
Evaluating:  60%|█████▉    | 34/57 [00:04<00:03,  6.97it/s]11/28/2021 01:19:00 - INFO - __main__ -   Batch number = 35
Evaluating:  61%|██████▏   | 35/57 [00:05<00:03,  6.96it/s]11/28/2021 01:19:01 - INFO - __main__ -   Batch number = 36
Evaluating:  63%|██████▎   | 36/57 [00:05<00:03,  6.97it/s]11/28/2021 01:19:01 - INFO - __main__ -   Batch number = 37
Evaluating:  65%|██████▍   | 37/57 [00:05<00:02,  6.97it/s]11/28/2021 01:19:01 - INFO - __main__ -   Batch number = 38
Evaluating:  67%|██████▋   | 38/57 [00:05<00:02,  6.96it/s]11/28/2021 01:19:01 - INFO - __main__ -   Batch number = 39
Evaluating:  68%|██████▊   | 39/57 [00:05<00:02,  6.96it/s]11/28/2021 01:19:01 - INFO - __main__ -   Batch number = 40
Evaluating:  70%|███████   | 40/57 [00:05<00:02,  5.78it/s]11/28/2021 01:19:01 - INFO - __main__ -   Batch number = 41
Evaluating:  72%|███████▏  | 41/57 [00:06<00:02,  5.93it/s]11/28/2021 01:19:02 - INFO - __main__ -   Batch number = 42
Evaluating:  74%|███████▎  | 42/57 [00:06<00:02,  6.05it/s]11/28/2021 01:19:02 - INFO - __main__ -   Batch number = 43
Evaluating:  75%|███████▌  | 43/57 [00:06<00:02,  6.13it/s]11/28/2021 01:19:02 - INFO - __main__ -   Batch number = 44
Evaluating:  77%|███████▋  | 44/57 [00:06<00:02,  6.18it/s]11/28/2021 01:19:02 - INFO - __main__ -   Batch number = 45
Evaluating:  79%|███████▉  | 45/57 [00:06<00:01,  6.20it/s]11/28/2021 01:19:02 - INFO - __main__ -   Batch number = 46
Evaluating:  81%|████████  | 46/57 [00:06<00:01,  6.22it/s]11/28/2021 01:19:02 - INFO - __main__ -   Batch number = 47
Evaluating:  82%|████████▏ | 47/57 [00:07<00:01,  6.24it/s]11/28/2021 01:19:03 - INFO - __main__ -   Batch number = 48
Evaluating:  84%|████████▍ | 48/57 [00:07<00:01,  6.27it/s]11/28/2021 01:19:03 - INFO - __main__ -   Batch number = 49
Evaluating:  86%|████████▌ | 49/57 [00:07<00:01,  6.29it/s]11/28/2021 01:19:03 - INFO - __main__ -   Batch number = 50
Evaluating:  88%|████████▊ | 50/57 [00:07<00:01,  6.31it/s]11/28/2021 01:19:03 - INFO - __main__ -   Batch number = 51
Evaluating:  89%|████████▉ | 51/57 [00:07<00:00,  6.34it/s]11/28/2021 01:19:03 - INFO - __main__ -   Batch number = 52
Evaluating:  91%|█████████ | 52/57 [00:07<00:00,  6.36it/s]11/28/2021 01:19:03 - INFO - __main__ -   Batch number = 53
Evaluating:  93%|█████████▎| 53/57 [00:08<00:00,  6.12it/s]11/28/2021 01:19:03 - INFO - __main__ -   Batch number = 54
Evaluating:  95%|█████████▍| 54/57 [00:08<00:00,  6.17it/s]11/28/2021 01:19:04 - INFO - __main__ -   Batch number = 55
Evaluating:  96%|█████████▋| 55/57 [00:08<00:00,  6.20it/s]11/28/2021 01:19:04 - INFO - __main__ -   Batch number = 56
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:19:04 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:19:04 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:19:04 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
11/28/2021 01:19:04 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:19:04 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:19:04 - INFO - root -   loading lang adpater cs/wiki@ukp
11/28/2021 01:19:04 - INFO - __main__ -   Adapter Languages : ['cs'], Length : 1
11/28/2021 01:19:04 - INFO - __main__ -   Adapter Names ['cs/wiki@ukp'], Length : 1
11/28/2021 01:19:04 - INFO - __main__ -   Language = cs
11/28/2021 01:19:04 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
Evaluating:  98%|█████████▊| 56/57 [00:08<00:00,  6.22it/s]11/28/2021 01:19:04 - INFO - __main__ -   Batch number = 57
No exactly matching adapter config found for this specifier, falling back to default.
Evaluating: 100%|██████████| 57/57 [00:08<00:00,  6.69it/s]Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/cs/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_cs_wiki_pfeiffer.zip.

/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:19:05 - INFO - __main__ -   ***** Evaluation result  in eu *****
11/28/2021 01:19:05 - INFO - __main__ -     f1 = 0.5933784436926051
11/28/2021 01:19:05 - INFO - __main__ -     loss = 1.297282180242371
11/28/2021 01:19:05 - INFO - __main__ -     precision = 0.6290413485679152
11/28/2021 01:19:05 - INFO - __main__ -     recall = 0.5615423317934409
20.84user 8.37system 0:28.57elapsed 102%CPU (0avgtext+0avgdata 3984472maxresident)k
0inputs+352outputs (0major+1562890minor)pagefaults 0swaps
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/adapter_config.json
Adding adapter 'cs' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/head_config.json
11/28/2021 01:19:11 - INFO - __main__ -   Language bxr, split test does not exist
16.21user 6.19system 0:17.87elapsed 125%CPU (0avgtext+0avgdata 3995824maxresident)k
0inputs+32outputs (0major+1236440minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:19:13 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bxr', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:19:13 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:19:13 - INFO - __main__ -   Seed = 3
11/28/2021 01:19:13 - INFO - root -   save model
11/28/2021 01:19:13 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bxr', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:19:13 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:19:16 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:19:22 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:19:22 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:19:22 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
11/28/2021 01:19:22 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:19:22 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:19:22 - INFO - root -   loading lang adpater cs/wiki@ukp
11/28/2021 01:19:22 - INFO - __main__ -   Adapter Languages : ['cs'], Length : 1
11/28/2021 01:19:22 - INFO - __main__ -   Adapter Names ['cs/wiki@ukp'], Length : 1
11/28/2021 01:19:22 - INFO - __main__ -   Language = cs
11/28/2021 01:19:22 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/cs/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_cs_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/adapter_config.json
Adding adapter 'cs' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/head_config.json
11/28/2021 01:19:31 - INFO - __main__ -   Language bxr, split test does not exist
15.86user 6.49system 0:20.75elapsed 107%CPU (0avgtext+0avgdata 3989460maxresident)k
0inputs+56outputs (0major+1471262minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:25:46 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='es', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:25:46 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:25:46 - INFO - __main__ -   Seed = 1
11/28/2021 01:25:46 - INFO - root -   save model
11/28/2021 01:25:46 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='es', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:25:46 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:25:49 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:25:55 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:25:55 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:25:55 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
11/28/2021 01:25:55 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:25:55 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:25:55 - INFO - root -   loading lang adpater cs/wiki@ukp
11/28/2021 01:25:55 - INFO - __main__ -   Adapter Languages : ['cs'], Length : 1
11/28/2021 01:25:55 - INFO - __main__ -   Adapter Names ['cs/wiki@ukp'], Length : 1
11/28/2021 01:25:55 - INFO - __main__ -   Language = cs
11/28/2021 01:25:55 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/cs/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_cs_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/adapter_config.json
Adding adapter 'cs' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/head_config.json
11/28/2021 01:26:04 - INFO - __main__ -   Language adapter for es not found, using cs instead
11/28/2021 01:26:04 - INFO - __main__ -   Set active language adapter to cs
11/28/2021 01:26:04 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:26:04 - INFO - __main__ -   Adapter Languages = ['cs']
11/28/2021 01:26:04 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_es_bert-base-multilingual-cased_128
11/28/2021 01:26:05 - INFO - __main__ -   ***** Running evaluation  in es *****
11/28/2021 01:26:05 - INFO - __main__ -     Num examples = 3154
11/28/2021 01:26:05 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/99 [00:00<?, ?it/s]11/28/2021 01:26:05 - INFO - __main__ -   Batch number = 1
Evaluating:   1%|          | 1/99 [00:00<00:16,  5.86it/s]11/28/2021 01:26:05 - INFO - __main__ -   Batch number = 2
Evaluating:   2%|▏         | 2/99 [00:00<00:16,  6.05it/s]11/28/2021 01:26:05 - INFO - __main__ -   Batch number = 3
Evaluating:   3%|▎         | 3/99 [00:00<00:15,  6.11it/s]11/28/2021 01:26:05 - INFO - __main__ -   Batch number = 4
Evaluating:   4%|▍         | 4/99 [00:00<00:15,  6.22it/s]11/28/2021 01:26:05 - INFO - __main__ -   Batch number = 5
Evaluating:   5%|▌         | 5/99 [00:00<00:15,  6.21it/s]11/28/2021 01:26:06 - INFO - __main__ -   Batch number = 6
Evaluating:   6%|▌         | 6/99 [00:00<00:15,  6.14it/s]11/28/2021 01:26:06 - INFO - __main__ -   Batch number = 7
Evaluating:   7%|▋         | 7/99 [00:01<00:15,  6.06it/s]11/28/2021 01:26:06 - INFO - __main__ -   Batch number = 8
Evaluating:   8%|▊         | 8/99 [00:01<00:14,  6.07it/s]11/28/2021 01:26:06 - INFO - __main__ -   Batch number = 9
Evaluating:   9%|▉         | 9/99 [00:01<00:14,  6.03it/s]11/28/2021 01:26:06 - INFO - __main__ -   Batch number = 10
Evaluating:  10%|█         | 10/99 [00:01<00:14,  6.03it/s]11/28/2021 01:26:06 - INFO - __main__ -   Batch number = 11
Evaluating:  11%|█         | 11/99 [00:01<00:14,  6.02it/s]11/28/2021 01:26:07 - INFO - __main__ -   Batch number = 12
Evaluating:  12%|█▏        | 12/99 [00:01<00:14,  6.10it/s]11/28/2021 01:26:07 - INFO - __main__ -   Batch number = 13
Evaluating:  13%|█▎        | 13/99 [00:02<00:14,  6.04it/s]11/28/2021 01:26:07 - INFO - __main__ -   Batch number = 14
Evaluating:  14%|█▍        | 14/99 [00:02<00:14,  6.00it/s]11/28/2021 01:26:07 - INFO - __main__ -   Batch number = 15
Evaluating:  15%|█▌        | 15/99 [00:02<00:14,  5.97it/s]11/28/2021 01:26:07 - INFO - __main__ -   Batch number = 16
Evaluating:  16%|█▌        | 16/99 [00:02<00:13,  5.96it/s]11/28/2021 01:26:07 - INFO - __main__ -   Batch number = 17
Evaluating:  17%|█▋        | 17/99 [00:02<00:13,  6.04it/s]11/28/2021 01:26:08 - INFO - __main__ -   Batch number = 18
Evaluating:  18%|█▊        | 18/99 [00:02<00:13,  6.06it/s]11/28/2021 01:26:08 - INFO - __main__ -   Batch number = 19
Evaluating:  19%|█▉        | 19/99 [00:03<00:13,  6.04it/s]11/28/2021 01:26:08 - INFO - __main__ -   Batch number = 20
Evaluating:  20%|██        | 20/99 [00:03<00:12,  6.09it/s]11/28/2021 01:26:08 - INFO - __main__ -   Batch number = 21
Evaluating:  21%|██        | 21/99 [00:03<00:12,  6.14it/s]11/28/2021 01:26:08 - INFO - __main__ -   Batch number = 22
Evaluating:  22%|██▏       | 22/99 [00:03<00:12,  6.11it/s]11/28/2021 01:26:08 - INFO - __main__ -   Batch number = 23
Evaluating:  23%|██▎       | 23/99 [00:03<00:12,  6.09it/s]11/28/2021 01:26:09 - INFO - __main__ -   Batch number = 24
Evaluating:  24%|██▍       | 24/99 [00:03<00:12,  6.05it/s]11/28/2021 01:26:09 - INFO - __main__ -   Batch number = 25
Evaluating:  25%|██▌       | 25/99 [00:04<00:12,  5.99it/s]11/28/2021 01:26:09 - INFO - __main__ -   Batch number = 26
Evaluating:  26%|██▋       | 26/99 [00:04<00:13,  5.41it/s]11/28/2021 01:26:09 - INFO - __main__ -   Batch number = 27
Evaluating:  27%|██▋       | 27/99 [00:04<00:12,  5.67it/s]11/28/2021 01:26:09 - INFO - __main__ -   Batch number = 28
Evaluating:  28%|██▊       | 28/99 [00:04<00:12,  5.86it/s]11/28/2021 01:26:09 - INFO - __main__ -   Batch number = 29
Evaluating:  29%|██▉       | 29/99 [00:04<00:11,  5.92it/s]11/28/2021 01:26:10 - INFO - __main__ -   Batch number = 30
Evaluating:  30%|███       | 30/99 [00:04<00:11,  6.03it/s]11/28/2021 01:26:10 - INFO - __main__ -   Batch number = 31
Evaluating:  31%|███▏      | 31/99 [00:05<00:11,  6.10it/s]11/28/2021 01:26:10 - INFO - __main__ -   Batch number = 32
Evaluating:  32%|███▏      | 32/99 [00:05<00:11,  6.08it/s]11/28/2021 01:26:10 - INFO - __main__ -   Batch number = 33
Evaluating:  33%|███▎      | 33/99 [00:05<00:10,  6.07it/s]11/28/2021 01:26:10 - INFO - __main__ -   Batch number = 34
Evaluating:  34%|███▍      | 34/99 [00:05<00:10,  6.11it/s]11/28/2021 01:26:10 - INFO - __main__ -   Batch number = 35
Evaluating:  35%|███▌      | 35/99 [00:05<00:12,  5.03it/s]11/28/2021 01:26:11 - INFO - __main__ -   Batch number = 36
Evaluating:  36%|███▋      | 36/99 [00:06<00:15,  4.18it/s]11/28/2021 01:26:11 - INFO - __main__ -   Batch number = 37
Evaluating:  37%|███▋      | 37/99 [00:06<00:16,  3.73it/s]11/28/2021 01:26:11 - INFO - __main__ -   Batch number = 38
Evaluating:  38%|███▊      | 38/99 [00:06<00:17,  3.48it/s]11/28/2021 01:26:12 - INFO - __main__ -   Batch number = 39
Evaluating:  39%|███▉      | 39/99 [00:07<00:18,  3.30it/s]11/28/2021 01:26:12 - INFO - __main__ -   Batch number = 40
Evaluating:  40%|████      | 40/99 [00:07<00:18,  3.20it/s]11/28/2021 01:26:12 - INFO - __main__ -   Batch number = 41
Evaluating:  41%|████▏     | 41/99 [00:07<00:18,  3.13it/s]11/28/2021 01:26:13 - INFO - __main__ -   Batch number = 42
Evaluating:  42%|████▏     | 42/99 [00:08<00:18,  3.09it/s]11/28/2021 01:26:13 - INFO - __main__ -   Batch number = 43
Evaluating:  43%|████▎     | 43/99 [00:08<00:18,  3.06it/s]11/28/2021 01:26:13 - INFO - __main__ -   Batch number = 44
Evaluating:  44%|████▍     | 44/99 [00:08<00:18,  3.02it/s]11/28/2021 01:26:14 - INFO - __main__ -   Batch number = 45
Evaluating:  45%|████▌     | 45/99 [00:09<00:18,  3.00it/s]11/28/2021 01:26:14 - INFO - __main__ -   Batch number = 46
Evaluating:  46%|████▋     | 46/99 [00:09<00:19,  2.77it/s]11/28/2021 01:26:14 - INFO - __main__ -   Batch number = 47
Evaluating:  47%|████▋     | 47/99 [00:10<00:18,  2.83it/s]11/28/2021 01:26:15 - INFO - __main__ -   Batch number = 48
Evaluating:  48%|████▊     | 48/99 [00:10<00:17,  2.88it/s]11/28/2021 01:26:15 - INFO - __main__ -   Batch number = 49
Evaluating:  49%|████▉     | 49/99 [00:10<00:17,  2.90it/s]11/28/2021 01:26:15 - INFO - __main__ -   Batch number = 50
Evaluating:  51%|█████     | 50/99 [00:11<00:16,  2.92it/s]11/28/2021 01:26:16 - INFO - __main__ -   Batch number = 51
Evaluating:  52%|█████▏    | 51/99 [00:11<00:16,  2.95it/s]11/28/2021 01:26:16 - INFO - __main__ -   Batch number = 52
Evaluating:  53%|█████▎    | 52/99 [00:11<00:15,  2.97it/s]11/28/2021 01:26:16 - INFO - __main__ -   Batch number = 53
Evaluating:  54%|█████▎    | 53/99 [00:12<00:15,  2.99it/s]11/28/2021 01:26:17 - INFO - __main__ -   Batch number = 54
Evaluating:  55%|█████▍    | 54/99 [00:12<00:14,  3.00it/s]11/28/2021 01:26:17 - INFO - __main__ -   Batch number = 55
Evaluating:  56%|█████▌    | 55/99 [00:12<00:14,  3.00it/s]11/28/2021 01:26:17 - INFO - __main__ -   Batch number = 56
Evaluating:  57%|█████▋    | 56/99 [00:13<00:14,  3.00it/s]11/28/2021 01:26:18 - INFO - __main__ -   Batch number = 57
Evaluating:  58%|█████▊    | 57/99 [00:13<00:14,  3.00it/s]11/28/2021 01:26:18 - INFO - __main__ -   Batch number = 58
Evaluating:  59%|█████▊    | 58/99 [00:13<00:13,  3.00it/s]11/28/2021 01:26:18 - INFO - __main__ -   Batch number = 59
Evaluating:  60%|█████▉    | 59/99 [00:14<00:13,  2.99it/s]11/28/2021 01:26:19 - INFO - __main__ -   Batch number = 60
Evaluating:  61%|██████    | 60/99 [00:14<00:13,  2.98it/s]11/28/2021 01:26:19 - INFO - __main__ -   Batch number = 61
Evaluating:  62%|██████▏   | 61/99 [00:14<00:12,  2.97it/s]11/28/2021 01:26:19 - INFO - __main__ -   Batch number = 62
Evaluating:  63%|██████▎   | 62/99 [00:15<00:12,  2.97it/s]11/28/2021 01:26:20 - INFO - __main__ -   Batch number = 63
Evaluating:  64%|██████▎   | 63/99 [00:15<00:12,  2.81it/s]11/28/2021 01:26:20 - INFO - __main__ -   Batch number = 64
Evaluating:  65%|██████▍   | 64/99 [00:15<00:14,  2.49it/s]11/28/2021 01:26:21 - INFO - __main__ -   Batch number = 65
Evaluating:  66%|██████▌   | 65/99 [00:16<00:14,  2.29it/s]11/28/2021 01:26:21 - INFO - __main__ -   Batch number = 66
Evaluating:  67%|██████▋   | 66/99 [00:16<00:15,  2.19it/s]11/28/2021 01:26:22 - INFO - __main__ -   Batch number = 67
Evaluating:  68%|██████▊   | 67/99 [00:17<00:15,  2.12it/s]11/28/2021 01:26:22 - INFO - __main__ -   Batch number = 68
Evaluating:  69%|██████▊   | 68/99 [00:18<00:14,  2.08it/s]11/28/2021 01:26:23 - INFO - __main__ -   Batch number = 69
Evaluating:  70%|██████▉   | 69/99 [00:18<00:14,  2.04it/s]11/28/2021 01:26:23 - INFO - __main__ -   Batch number = 70
Evaluating:  71%|███████   | 70/99 [00:19<00:14,  2.02it/s]11/28/2021 01:26:24 - INFO - __main__ -   Batch number = 71
Evaluating:  72%|███████▏  | 71/99 [00:19<00:13,  2.01it/s]11/28/2021 01:26:24 - INFO - __main__ -   Batch number = 72
Evaluating:  73%|███████▎  | 72/99 [00:20<00:13,  2.01it/s]11/28/2021 01:26:25 - INFO - __main__ -   Batch number = 73
Evaluating:  74%|███████▎  | 73/99 [00:20<00:13,  1.99it/s]11/28/2021 01:26:25 - INFO - __main__ -   Batch number = 74
Evaluating:  75%|███████▍  | 74/99 [00:21<00:12,  1.98it/s]11/28/2021 01:26:26 - INFO - __main__ -   Batch number = 75
Evaluating:  76%|███████▌  | 75/99 [00:21<00:12,  1.98it/s]11/28/2021 01:26:26 - INFO - __main__ -   Batch number = 76
Evaluating:  77%|███████▋  | 76/99 [00:22<00:11,  1.97it/s]11/28/2021 01:26:27 - INFO - __main__ -   Batch number = 77
Evaluating:  78%|███████▊  | 77/99 [00:22<00:11,  1.98it/s]11/28/2021 01:26:27 - INFO - __main__ -   Batch number = 78
Evaluating:  79%|███████▉  | 78/99 [00:23<00:10,  1.98it/s]11/28/2021 01:26:28 - INFO - __main__ -   Batch number = 79
Evaluating:  80%|███████▉  | 79/99 [00:23<00:10,  1.97it/s]11/28/2021 01:26:28 - INFO - __main__ -   Batch number = 80
Evaluating:  81%|████████  | 80/99 [00:24<00:09,  1.97it/s]11/28/2021 01:26:29 - INFO - __main__ -   Batch number = 81
Evaluating:  82%|████████▏ | 81/99 [00:24<00:07,  2.27it/s]11/28/2021 01:26:29 - INFO - __main__ -   Batch number = 82
Evaluating:  83%|████████▎ | 82/99 [00:24<00:07,  2.14it/s]11/28/2021 01:26:30 - INFO - __main__ -   Batch number = 83
Evaluating:  84%|████████▍ | 83/99 [00:25<00:08,  1.95it/s]11/28/2021 01:26:30 - INFO - __main__ -   Batch number = 84
Evaluating:  85%|████████▍ | 84/99 [00:26<00:08,  1.78it/s]11/28/2021 01:26:31 - INFO - __main__ -   Batch number = 85
Evaluating:  86%|████████▌ | 85/99 [00:26<00:08,  1.68it/s]11/28/2021 01:26:32 - INFO - __main__ -   Batch number = 86
Evaluating:  87%|████████▋ | 86/99 [00:27<00:08,  1.62it/s]11/28/2021 01:26:32 - INFO - __main__ -   Batch number = 87
Evaluating:  88%|████████▊ | 87/99 [00:28<00:07,  1.58it/s]11/28/2021 01:26:33 - INFO - __main__ -   Batch number = 88
Evaluating:  89%|████████▉ | 88/99 [00:28<00:07,  1.55it/s]11/28/2021 01:26:34 - INFO - __main__ -   Batch number = 89
Evaluating:  90%|████████▉ | 89/99 [00:29<00:06,  1.53it/s]11/28/2021 01:26:34 - INFO - __main__ -   Batch number = 90
Evaluating:  91%|█████████ | 90/99 [00:30<00:05,  1.52it/s]11/28/2021 01:26:35 - INFO - __main__ -   Batch number = 91
Evaluating:  92%|█████████▏| 91/99 [00:30<00:05,  1.51it/s]11/28/2021 01:26:36 - INFO - __main__ -   Batch number = 92
Evaluating:  93%|█████████▎| 92/99 [00:31<00:04,  1.51it/s]11/28/2021 01:26:36 - INFO - __main__ -   Batch number = 93
Evaluating:  94%|█████████▍| 93/99 [00:32<00:03,  1.52it/s]11/28/2021 01:26:37 - INFO - __main__ -   Batch number = 94
Evaluating:  95%|█████████▍| 94/99 [00:32<00:03,  1.51it/s]11/28/2021 01:26:38 - INFO - __main__ -   Batch number = 95
Evaluating:  96%|█████████▌| 95/99 [00:33<00:02,  1.51it/s]11/28/2021 01:26:38 - INFO - __main__ -   Batch number = 96
Evaluating:  97%|█████████▋| 96/99 [00:34<00:01,  1.50it/s]11/28/2021 01:26:39 - INFO - __main__ -   Batch number = 97
Evaluating:  98%|█████████▊| 97/99 [00:34<00:01,  1.50it/s]11/28/2021 01:26:40 - INFO - __main__ -   Batch number = 98
Evaluating:  99%|█████████▉| 98/99 [00:35<00:00,  1.50it/s]11/28/2021 01:26:40 - INFO - __main__ -   Batch number = 99
Evaluating: 100%|██████████| 99/99 [00:35<00:00,  1.67it/s]Evaluating: 100%|██████████| 99/99 [00:35<00:00,  2.75it/s]
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:26:43 - INFO - __main__ -   ***** Evaluation result  in es *****
11/28/2021 01:26:43 - INFO - __main__ -     f1 = 0.8786496905135407
11/28/2021 01:26:43 - INFO - __main__ -     loss = 0.38824423997089114
11/28/2021 01:26:43 - INFO - __main__ -     precision = 0.8874341369950502
11/28/2021 01:26:43 - INFO - __main__ -     recall = 0.8700374486736426
45.31user 15.45system 1:00.06elapsed 101%CPU (0avgtext+0avgdata 3995640maxresident)k
0inputs+976outputs (0major+1330069minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:26:46 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='es', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:26:46 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:26:46 - INFO - __main__ -   Seed = 2
11/28/2021 01:26:46 - INFO - root -   save model
11/28/2021 01:26:46 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='es', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:26:46 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:26:49 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:26:55 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:26:55 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:26:55 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
11/28/2021 01:26:55 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:26:55 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:26:55 - INFO - root -   loading lang adpater cs/wiki@ukp
11/28/2021 01:26:55 - INFO - __main__ -   Adapter Languages : ['cs'], Length : 1
11/28/2021 01:26:55 - INFO - __main__ -   Adapter Names ['cs/wiki@ukp'], Length : 1
11/28/2021 01:26:55 - INFO - __main__ -   Language = cs
11/28/2021 01:26:55 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/cs/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_cs_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/adapter_config.json
Adding adapter 'cs' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/head_config.json
11/28/2021 01:27:09 - INFO - __main__ -   Language adapter for es not found, using cs instead
11/28/2021 01:27:09 - INFO - __main__ -   Set active language adapter to cs
11/28/2021 01:27:09 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:27:09 - INFO - __main__ -   Adapter Languages = ['cs']
11/28/2021 01:27:09 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_es_bert-base-multilingual-cased_128
11/28/2021 01:27:09 - INFO - __main__ -   ***** Running evaluation  in es *****
11/28/2021 01:27:09 - INFO - __main__ -     Num examples = 3154
11/28/2021 01:27:09 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/99 [00:00<?, ?it/s]11/28/2021 01:27:09 - INFO - __main__ -   Batch number = 1
Evaluating:   1%|          | 1/99 [00:00<01:04,  1.51it/s]11/28/2021 01:27:10 - INFO - __main__ -   Batch number = 2
Evaluating:   2%|▏         | 2/99 [00:01<01:04,  1.51it/s]11/28/2021 01:27:11 - INFO - __main__ -   Batch number = 3
Evaluating:   3%|▎         | 3/99 [00:01<01:03,  1.51it/s]11/28/2021 01:27:11 - INFO - __main__ -   Batch number = 4
Evaluating:   4%|▍         | 4/99 [00:02<01:02,  1.52it/s]11/28/2021 01:27:12 - INFO - __main__ -   Batch number = 5
Evaluating:   5%|▌         | 5/99 [00:03<01:01,  1.52it/s]11/28/2021 01:27:13 - INFO - __main__ -   Batch number = 6
Evaluating:   6%|▌         | 6/99 [00:03<01:01,  1.52it/s]11/28/2021 01:27:13 - INFO - __main__ -   Batch number = 7
Evaluating:   7%|▋         | 7/99 [00:04<01:00,  1.52it/s]11/28/2021 01:27:14 - INFO - __main__ -   Batch number = 8
Evaluating:   8%|▊         | 8/99 [00:05<00:59,  1.52it/s]11/28/2021 01:27:15 - INFO - __main__ -   Batch number = 9
Evaluating:   9%|▉         | 9/99 [00:05<00:56,  1.59it/s]11/28/2021 01:27:15 - INFO - __main__ -   Batch number = 10
Evaluating:  10%|█         | 10/99 [00:06<00:51,  1.72it/s]11/28/2021 01:27:16 - INFO - __main__ -   Batch number = 11
Evaluating:  11%|█         | 11/99 [00:06<00:44,  1.99it/s]11/28/2021 01:27:16 - INFO - __main__ -   Batch number = 12
Evaluating:  12%|█▏        | 12/99 [00:06<00:38,  2.23it/s]11/28/2021 01:27:16 - INFO - __main__ -   Batch number = 13
Evaluating:  13%|█▎        | 13/99 [00:07<00:35,  2.46it/s]11/28/2021 01:27:17 - INFO - __main__ -   Batch number = 14
Evaluating:  14%|█▍        | 14/99 [00:07<00:32,  2.63it/s]11/28/2021 01:27:17 - INFO - __main__ -   Batch number = 15
Evaluating:  15%|█▌        | 15/99 [00:07<00:30,  2.73it/s]11/28/2021 01:27:17 - INFO - __main__ -   Batch number = 16
Evaluating:  16%|█▌        | 16/99 [00:08<00:29,  2.86it/s]11/28/2021 01:27:18 - INFO - __main__ -   Batch number = 17
Evaluating:  17%|█▋        | 17/99 [00:08<00:27,  3.00it/s]11/28/2021 01:27:18 - INFO - __main__ -   Batch number = 18
Evaluating:  18%|█▊        | 18/99 [00:08<00:27,  3.00it/s]11/28/2021 01:27:18 - INFO - __main__ -   Batch number = 19
Evaluating:  19%|█▉        | 19/99 [00:09<00:26,  3.01it/s]11/28/2021 01:27:18 - INFO - __main__ -   Batch number = 20
Evaluating:  20%|██        | 20/99 [00:09<00:25,  3.07it/s]11/28/2021 01:27:19 - INFO - __main__ -   Batch number = 21
Evaluating:  21%|██        | 21/99 [00:09<00:25,  3.09it/s]11/28/2021 01:27:19 - INFO - __main__ -   Batch number = 22
Evaluating:  22%|██▏       | 22/99 [00:10<00:24,  3.16it/s]11/28/2021 01:27:19 - INFO - __main__ -   Batch number = 23
Evaluating:  23%|██▎       | 23/99 [00:10<00:24,  3.16it/s]11/28/2021 01:27:20 - INFO - __main__ -   Batch number = 24
Evaluating:  24%|██▍       | 24/99 [00:10<00:23,  3.15it/s]11/28/2021 01:27:20 - INFO - __main__ -   Batch number = 25
Evaluating:  25%|██▌       | 25/99 [00:11<00:23,  3.14it/s]11/28/2021 01:27:20 - INFO - __main__ -   Batch number = 26
Evaluating:  26%|██▋       | 26/99 [00:11<00:23,  3.13it/s]11/28/2021 01:27:21 - INFO - __main__ -   Batch number = 27
Evaluating:  27%|██▋       | 27/99 [00:11<00:23,  3.12it/s]11/28/2021 01:27:21 - INFO - __main__ -   Batch number = 28
Evaluating:  28%|██▊       | 28/99 [00:12<00:22,  3.14it/s]11/28/2021 01:27:21 - INFO - __main__ -   Batch number = 29
Evaluating:  29%|██▉       | 29/99 [00:12<00:22,  3.11it/s]11/28/2021 01:27:22 - INFO - __main__ -   Batch number = 30
Evaluating:  30%|███       | 30/99 [00:12<00:22,  3.11it/s]11/28/2021 01:27:22 - INFO - __main__ -   Batch number = 31
Evaluating:  31%|███▏      | 31/99 [00:13<00:21,  3.12it/s]11/28/2021 01:27:22 - INFO - __main__ -   Batch number = 32
Evaluating:  32%|███▏      | 32/99 [00:13<00:21,  3.14it/s]11/28/2021 01:27:23 - INFO - __main__ -   Batch number = 33
Evaluating:  33%|███▎      | 33/99 [00:13<00:20,  3.15it/s]11/28/2021 01:27:23 - INFO - __main__ -   Batch number = 34
Evaluating:  34%|███▍      | 34/99 [00:13<00:20,  3.15it/s]11/28/2021 01:27:23 - INFO - __main__ -   Batch number = 35
Evaluating:  35%|███▌      | 35/99 [00:14<00:20,  3.16it/s]11/28/2021 01:27:24 - INFO - __main__ -   Batch number = 36
Evaluating:  36%|███▋      | 36/99 [00:14<00:19,  3.17it/s]11/28/2021 01:27:24 - INFO - __main__ -   Batch number = 37
Evaluating:  37%|███▋      | 37/99 [00:14<00:19,  3.17it/s]11/28/2021 01:27:24 - INFO - __main__ -   Batch number = 38
Evaluating:  38%|███▊      | 38/99 [00:15<00:19,  3.05it/s]11/28/2021 01:27:25 - INFO - __main__ -   Batch number = 39
Evaluating:  39%|███▉      | 39/99 [00:15<00:19,  3.08it/s]11/28/2021 01:27:25 - INFO - __main__ -   Batch number = 40
Evaluating:  40%|████      | 40/99 [00:15<00:19,  3.10it/s]11/28/2021 01:27:25 - INFO - __main__ -   Batch number = 41
Evaluating:  41%|████▏     | 41/99 [00:16<00:18,  3.14it/s]11/28/2021 01:27:25 - INFO - __main__ -   Batch number = 42
Evaluating:  42%|████▏     | 42/99 [00:16<00:18,  3.16it/s]11/28/2021 01:27:26 - INFO - __main__ -   Batch number = 43
Evaluating:  43%|████▎     | 43/99 [00:16<00:17,  3.13it/s]11/28/2021 01:27:26 - INFO - __main__ -   Batch number = 44
Evaluating:  44%|████▍     | 44/99 [00:17<00:17,  3.12it/s]11/28/2021 01:27:26 - INFO - __main__ -   Batch number = 45
Evaluating:  45%|████▌     | 45/99 [00:17<00:17,  3.15it/s]11/28/2021 01:27:27 - INFO - __main__ -   Batch number = 46
Evaluating:  46%|████▋     | 46/99 [00:17<00:16,  3.17it/s]11/28/2021 01:27:27 - INFO - __main__ -   Batch number = 47
Evaluating:  47%|████▋     | 47/99 [00:18<00:16,  3.17it/s]11/28/2021 01:27:27 - INFO - __main__ -   Batch number = 48
Evaluating:  48%|████▊     | 48/99 [00:18<00:16,  3.18it/s]11/28/2021 01:27:28 - INFO - __main__ -   Batch number = 49
Evaluating:  49%|████▉     | 49/99 [00:18<00:15,  3.18it/s]11/28/2021 01:27:28 - INFO - __main__ -   Batch number = 50
Evaluating:  51%|█████     | 50/99 [00:19<00:15,  3.18it/s]11/28/2021 01:27:28 - INFO - __main__ -   Batch number = 51
Evaluating:  52%|█████▏    | 51/99 [00:19<00:15,  3.20it/s]11/28/2021 01:27:29 - INFO - __main__ -   Batch number = 52
Evaluating:  53%|█████▎    | 52/99 [00:19<00:14,  3.21it/s]11/28/2021 01:27:29 - INFO - __main__ -   Batch number = 53
Evaluating:  54%|█████▎    | 53/99 [00:19<00:14,  3.23it/s]11/28/2021 01:27:29 - INFO - __main__ -   Batch number = 54
Evaluating:  55%|█████▍    | 54/99 [00:20<00:14,  3.16it/s]11/28/2021 01:27:30 - INFO - __main__ -   Batch number = 55
Evaluating:  56%|█████▌    | 55/99 [00:20<00:14,  3.14it/s]11/28/2021 01:27:30 - INFO - __main__ -   Batch number = 56
Evaluating:  57%|█████▋    | 56/99 [00:20<00:13,  3.18it/s]11/28/2021 01:27:30 - INFO - __main__ -   Batch number = 57
Evaluating:  58%|█████▊    | 57/99 [00:21<00:13,  3.19it/s]11/28/2021 01:27:31 - INFO - __main__ -   Batch number = 58
Evaluating:  59%|█████▊    | 58/99 [00:21<00:12,  3.19it/s]11/28/2021 01:27:31 - INFO - __main__ -   Batch number = 59
Evaluating:  60%|█████▉    | 59/99 [00:21<00:12,  3.17it/s]11/28/2021 01:27:31 - INFO - __main__ -   Batch number = 60
Evaluating:  61%|██████    | 60/99 [00:22<00:12,  3.18it/s]11/28/2021 01:27:31 - INFO - __main__ -   Batch number = 61
Evaluating:  62%|██████▏   | 61/99 [00:22<00:12,  3.16it/s]11/28/2021 01:27:32 - INFO - __main__ -   Batch number = 62
Evaluating:  63%|██████▎   | 62/99 [00:22<00:11,  3.15it/s]11/28/2021 01:27:32 - INFO - __main__ -   Batch number = 63
Evaluating:  64%|██████▎   | 63/99 [00:23<00:11,  3.14it/s]11/28/2021 01:27:32 - INFO - __main__ -   Batch number = 64
Evaluating:  65%|██████▍   | 64/99 [00:23<00:11,  3.13it/s]11/28/2021 01:27:33 - INFO - __main__ -   Batch number = 65
Evaluating:  66%|██████▌   | 65/99 [00:23<00:11,  3.09it/s]11/28/2021 01:27:33 - INFO - __main__ -   Batch number = 66
Evaluating:  67%|██████▋   | 66/99 [00:24<00:10,  3.06it/s]11/28/2021 01:27:33 - INFO - __main__ -   Batch number = 67
Evaluating:  68%|██████▊   | 67/99 [00:24<00:10,  3.11it/s]11/28/2021 01:27:34 - INFO - __main__ -   Batch number = 68
Evaluating:  69%|██████▊   | 68/99 [00:24<00:09,  3.11it/s]11/28/2021 01:27:34 - INFO - __main__ -   Batch number = 69
Evaluating:  70%|██████▉   | 69/99 [00:25<00:09,  3.10it/s]11/28/2021 01:27:34 - INFO - __main__ -   Batch number = 70
Evaluating:  71%|███████   | 70/99 [00:25<00:09,  3.11it/s]11/28/2021 01:27:35 - INFO - __main__ -   Batch number = 71
Evaluating:  72%|███████▏  | 71/99 [00:25<00:08,  3.11it/s]11/28/2021 01:27:35 - INFO - __main__ -   Batch number = 72
Evaluating:  73%|███████▎  | 72/99 [00:26<00:08,  3.11it/s]11/28/2021 01:27:35 - INFO - __main__ -   Batch number = 73
Evaluating:  74%|███████▎  | 73/99 [00:26<00:08,  3.12it/s]11/28/2021 01:27:36 - INFO - __main__ -   Batch number = 74
Evaluating:  75%|███████▍  | 74/99 [00:26<00:08,  3.11it/s]11/28/2021 01:27:36 - INFO - __main__ -   Batch number = 75
Evaluating:  76%|███████▌  | 75/99 [00:27<00:07,  3.09it/s]11/28/2021 01:27:36 - INFO - __main__ -   Batch number = 76
Evaluating:  77%|███████▋  | 76/99 [00:27<00:07,  3.08it/s]11/28/2021 01:27:37 - INFO - __main__ -   Batch number = 77
Evaluating:  78%|███████▊  | 77/99 [00:27<00:07,  3.05it/s]11/28/2021 01:27:37 - INFO - __main__ -   Batch number = 78
Evaluating:  79%|███████▉  | 78/99 [00:28<00:06,  3.02it/s]11/28/2021 01:27:37 - INFO - __main__ -   Batch number = 79
Evaluating:  80%|███████▉  | 79/99 [00:28<00:06,  3.02it/s]11/28/2021 01:27:38 - INFO - __main__ -   Batch number = 80
Evaluating:  81%|████████  | 80/99 [00:28<00:06,  3.06it/s]11/28/2021 01:27:38 - INFO - __main__ -   Batch number = 81
Evaluating:  82%|████████▏ | 81/99 [00:28<00:05,  3.08it/s]11/28/2021 01:27:38 - INFO - __main__ -   Batch number = 82
Evaluating:  83%|████████▎ | 82/99 [00:29<00:05,  3.08it/s]11/28/2021 01:27:39 - INFO - __main__ -   Batch number = 83
Evaluating:  84%|████████▍ | 83/99 [00:29<00:05,  3.10it/s]11/28/2021 01:27:39 - INFO - __main__ -   Batch number = 84
Evaluating:  85%|████████▍ | 84/99 [00:29<00:04,  3.10it/s]11/28/2021 01:27:39 - INFO - __main__ -   Batch number = 85
Evaluating:  86%|████████▌ | 85/99 [00:30<00:04,  3.10it/s]11/28/2021 01:27:40 - INFO - __main__ -   Batch number = 86
Evaluating:  87%|████████▋ | 86/99 [00:30<00:04,  3.10it/s]11/28/2021 01:27:40 - INFO - __main__ -   Batch number = 87
Evaluating:  88%|████████▊ | 87/99 [00:30<00:03,  3.12it/s]11/28/2021 01:27:40 - INFO - __main__ -   Batch number = 88
Evaluating:  89%|████████▉ | 88/99 [00:31<00:04,  2.58it/s]11/28/2021 01:27:41 - INFO - __main__ -   Batch number = 89
Evaluating:  90%|████████▉ | 89/99 [00:32<00:04,  2.14it/s]11/28/2021 01:27:41 - INFO - __main__ -   Batch number = 90
Evaluating:  91%|█████████ | 90/99 [00:32<00:04,  1.89it/s]11/28/2021 01:27:42 - INFO - __main__ -   Batch number = 91
Evaluating:  92%|█████████▏| 91/99 [00:33<00:04,  1.75it/s]11/28/2021 01:27:43 - INFO - __main__ -   Batch number = 92
Evaluating:  93%|█████████▎| 92/99 [00:34<00:04,  1.68it/s]11/28/2021 01:27:43 - INFO - __main__ -   Batch number = 93
Evaluating:  94%|█████████▍| 93/99 [00:34<00:03,  1.62it/s]11/28/2021 01:27:44 - INFO - __main__ -   Batch number = 94
Evaluating:  95%|█████████▍| 94/99 [00:35<00:03,  1.58it/s]11/28/2021 01:27:45 - INFO - __main__ -   Batch number = 95
Evaluating:  96%|█████████▌| 95/99 [00:36<00:02,  1.57it/s]11/28/2021 01:27:45 - INFO - __main__ -   Batch number = 96
Evaluating:  97%|█████████▋| 96/99 [00:36<00:01,  1.55it/s]11/28/2021 01:27:46 - INFO - __main__ -   Batch number = 97
Evaluating:  98%|█████████▊| 97/99 [00:37<00:01,  1.53it/s]11/28/2021 01:27:47 - INFO - __main__ -   Batch number = 98
Evaluating:  99%|█████████▉| 98/99 [00:38<00:00,  1.52it/s]11/28/2021 01:27:47 - INFO - __main__ -   Batch number = 99
Evaluating: 100%|██████████| 99/99 [00:38<00:00,  1.69it/s]Evaluating: 100%|██████████| 99/99 [00:38<00:00,  2.57it/s]
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:27:50 - INFO - __main__ -   ***** Evaluation result  in es *****
11/28/2021 01:27:50 - INFO - __main__ -     f1 = 0.8614007853164095
11/28/2021 01:27:50 - INFO - __main__ -     loss = 0.4522297096372855
11/28/2021 01:27:50 - INFO - __main__ -     precision = 0.8711000024625083
11/28/2021 01:27:50 - INFO - __main__ -     recall = 0.8519151805604056
49.51user 17.39system 1:06.59elapsed 100%CPU (0avgtext+0avgdata 3995452maxresident)k
0inputs+960outputs (0major+1374919minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:27:53 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='es', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:27:53 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:27:53 - INFO - __main__ -   Seed = 3
11/28/2021 01:27:53 - INFO - root -   save model
11/28/2021 01:27:53 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='es', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:27:53 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:27:56 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:28:02 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:28:02 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:28:02 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
11/28/2021 01:28:02 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:28:02 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:28:03 - INFO - root -   loading lang adpater cs/wiki@ukp
11/28/2021 01:28:03 - INFO - __main__ -   Adapter Languages : ['cs'], Length : 1
11/28/2021 01:28:03 - INFO - __main__ -   Adapter Names ['cs/wiki@ukp'], Length : 1
11/28/2021 01:28:03 - INFO - __main__ -   Language = cs
11/28/2021 01:28:03 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/cs/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_cs_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/adapter_config.json
Adding adapter 'cs' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/head_config.json
11/28/2021 01:28:19 - INFO - __main__ -   Language adapter for es not found, using cs instead
11/28/2021 01:28:19 - INFO - __main__ -   Set active language adapter to cs
11/28/2021 01:28:19 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:28:19 - INFO - __main__ -   Adapter Languages = ['cs']
11/28/2021 01:28:19 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_es_bert-base-multilingual-cased_128
11/28/2021 01:28:19 - INFO - __main__ -   ***** Running evaluation  in es *****
11/28/2021 01:28:19 - INFO - __main__ -     Num examples = 3154
11/28/2021 01:28:19 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/99 [00:00<?, ?it/s]11/28/2021 01:28:19 - INFO - __main__ -   Batch number = 1
Evaluating:   1%|          | 1/99 [00:00<01:25,  1.15it/s]11/28/2021 01:28:20 - INFO - __main__ -   Batch number = 2
Evaluating:   2%|▏         | 2/99 [00:01<01:12,  1.34it/s]11/28/2021 01:28:21 - INFO - __main__ -   Batch number = 3
Evaluating:   3%|▎         | 3/99 [00:02<01:07,  1.43it/s]11/28/2021 01:28:22 - INFO - __main__ -   Batch number = 4
Evaluating:   4%|▍         | 4/99 [00:02<01:04,  1.48it/s]11/28/2021 01:28:22 - INFO - __main__ -   Batch number = 5
Evaluating:   5%|▌         | 5/99 [00:03<01:02,  1.51it/s]11/28/2021 01:28:23 - INFO - __main__ -   Batch number = 6
Evaluating:   6%|▌         | 6/99 [00:04<00:58,  1.58it/s]11/28/2021 01:28:23 - INFO - __main__ -   Batch number = 7
Evaluating:   7%|▋         | 7/99 [00:04<00:53,  1.73it/s]11/28/2021 01:28:24 - INFO - __main__ -   Batch number = 8
Evaluating:   8%|▊         | 8/99 [00:04<00:49,  1.82it/s]11/28/2021 01:28:24 - INFO - __main__ -   Batch number = 9
Evaluating:   9%|▉         | 9/99 [00:05<00:47,  1.91it/s]11/28/2021 01:28:25 - INFO - __main__ -   Batch number = 10
Evaluating:  10%|█         | 10/99 [00:05<00:45,  1.95it/s]11/28/2021 01:28:25 - INFO - __main__ -   Batch number = 11
Evaluating:  11%|█         | 11/99 [00:06<00:44,  1.99it/s]11/28/2021 01:28:26 - INFO - __main__ -   Batch number = 12
Evaluating:  12%|█▏        | 12/99 [00:06<00:40,  2.14it/s]11/28/2021 01:28:26 - INFO - __main__ -   Batch number = 13
Evaluating:  13%|█▎        | 13/99 [00:07<00:36,  2.34it/s]11/28/2021 01:28:27 - INFO - __main__ -   Batch number = 14
Evaluating:  14%|█▍        | 14/99 [00:07<00:33,  2.52it/s]11/28/2021 01:28:27 - INFO - __main__ -   Batch number = 15
Evaluating:  15%|█▌        | 15/99 [00:07<00:31,  2.66it/s]11/28/2021 01:28:27 - INFO - __main__ -   Batch number = 16
Evaluating:  16%|█▌        | 16/99 [00:08<00:29,  2.79it/s]11/28/2021 01:28:28 - INFO - __main__ -   Batch number = 17
Evaluating:  17%|█▋        | 17/99 [00:08<00:28,  2.88it/s]11/28/2021 01:28:28 - INFO - __main__ -   Batch number = 18
Evaluating:  18%|█▊        | 18/99 [00:08<00:27,  2.95it/s]11/28/2021 01:28:28 - INFO - __main__ -   Batch number = 19
Evaluating:  19%|█▉        | 19/99 [00:09<00:26,  3.01it/s]11/28/2021 01:28:28 - INFO - __main__ -   Batch number = 20
Evaluating:  20%|██        | 20/99 [00:09<00:25,  3.04it/s]11/28/2021 01:28:29 - INFO - __main__ -   Batch number = 21
Evaluating:  21%|██        | 21/99 [00:09<00:25,  3.07it/s]11/28/2021 01:28:29 - INFO - __main__ -   Batch number = 22
Evaluating:  22%|██▏       | 22/99 [00:10<00:24,  3.08it/s]11/28/2021 01:28:29 - INFO - __main__ -   Batch number = 23
Evaluating:  23%|██▎       | 23/99 [00:10<00:24,  3.10it/s]11/28/2021 01:28:30 - INFO - __main__ -   Batch number = 24
Evaluating:  24%|██▍       | 24/99 [00:10<00:23,  3.18it/s]11/28/2021 01:28:30 - INFO - __main__ -   Batch number = 25
Evaluating:  25%|██▌       | 25/99 [00:10<00:19,  3.71it/s]11/28/2021 01:28:30 - INFO - __main__ -   Batch number = 26
Evaluating:  26%|██▋       | 26/99 [00:10<00:17,  4.17it/s]11/28/2021 01:28:30 - INFO - __main__ -   Batch number = 27
Evaluating:  27%|██▋       | 27/99 [00:11<00:15,  4.64it/s]11/28/2021 01:28:31 - INFO - __main__ -   Batch number = 28
Evaluating:  28%|██▊       | 28/99 [00:11<00:14,  5.07it/s]11/28/2021 01:28:31 - INFO - __main__ -   Batch number = 29
Evaluating:  29%|██▉       | 29/99 [00:11<00:13,  5.28it/s]11/28/2021 01:28:31 - INFO - __main__ -   Batch number = 30
Evaluating:  30%|███       | 30/99 [00:11<00:12,  5.44it/s]11/28/2021 01:28:31 - INFO - __main__ -   Batch number = 31
Evaluating:  31%|███▏      | 31/99 [00:11<00:12,  5.59it/s]11/28/2021 01:28:31 - INFO - __main__ -   Batch number = 32
Evaluating:  32%|███▏      | 32/99 [00:11<00:11,  5.70it/s]11/28/2021 01:28:31 - INFO - __main__ -   Batch number = 33
Evaluating:  33%|███▎      | 33/99 [00:12<00:11,  5.83it/s]11/28/2021 01:28:32 - INFO - __main__ -   Batch number = 34
Evaluating:  34%|███▍      | 34/99 [00:12<00:10,  5.96it/s]11/28/2021 01:28:32 - INFO - __main__ -   Batch number = 35
Evaluating:  35%|███▌      | 35/99 [00:12<00:10,  6.08it/s]11/28/2021 01:28:32 - INFO - __main__ -   Batch number = 36
Evaluating:  36%|███▋      | 36/99 [00:12<00:10,  6.12it/s]11/28/2021 01:28:32 - INFO - __main__ -   Batch number = 37
Evaluating:  37%|███▋      | 37/99 [00:12<00:10,  6.12it/s]11/28/2021 01:28:32 - INFO - __main__ -   Batch number = 38
Evaluating:  38%|███▊      | 38/99 [00:12<00:09,  6.14it/s]11/28/2021 01:28:32 - INFO - __main__ -   Batch number = 39
Evaluating:  39%|███▉      | 39/99 [00:13<00:09,  6.16it/s]11/28/2021 01:28:33 - INFO - __main__ -   Batch number = 40
Evaluating:  40%|████      | 40/99 [00:13<00:09,  6.17it/s]11/28/2021 01:28:33 - INFO - __main__ -   Batch number = 41
Evaluating:  41%|████▏     | 41/99 [00:13<00:09,  6.17it/s]11/28/2021 01:28:33 - INFO - __main__ -   Batch number = 42
Evaluating:  42%|████▏     | 42/99 [00:13<00:09,  6.20it/s]11/28/2021 01:28:33 - INFO - __main__ -   Batch number = 43
Evaluating:  43%|████▎     | 43/99 [00:13<00:09,  6.22it/s]11/28/2021 01:28:33 - INFO - __main__ -   Batch number = 44
Evaluating:  44%|████▍     | 44/99 [00:13<00:08,  6.24it/s]11/28/2021 01:28:33 - INFO - __main__ -   Batch number = 45
Evaluating:  45%|████▌     | 45/99 [00:14<00:08,  6.26it/s]11/28/2021 01:28:33 - INFO - __main__ -   Batch number = 46
PyTorch version 1.10.0+cu102 available.
Evaluating:  46%|████▋     | 46/99 [00:14<00:08,  6.30it/s]11/28/2021 01:28:34 - INFO - __main__ -   Batch number = 47
Evaluating:  47%|████▋     | 47/99 [00:14<00:08,  6.32it/s]11/28/2021 01:28:34 - INFO - __main__ -   Batch number = 48
11/28/2021 01:28:34 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ru', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:28:34 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:28:34 - INFO - __main__ -   Seed = 1
11/28/2021 01:28:34 - INFO - root -   save model
11/28/2021 01:28:34 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ru', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:28:34 - INFO - __main__ -   Loading pretrained model and tokenizer
Evaluating:  48%|████▊     | 48/99 [00:14<00:08,  6.32it/s]11/28/2021 01:28:34 - INFO - __main__ -   Batch number = 49
Evaluating:  49%|████▉     | 49/99 [00:14<00:07,  6.31it/s]11/28/2021 01:28:34 - INFO - __main__ -   Batch number = 50
Evaluating:  51%|█████     | 50/99 [00:14<00:07,  6.26it/s]11/28/2021 01:28:34 - INFO - __main__ -   Batch number = 51
Evaluating:  52%|█████▏    | 51/99 [00:15<00:07,  6.21it/s]11/28/2021 01:28:34 - INFO - __main__ -   Batch number = 52
Evaluating:  53%|█████▎    | 52/99 [00:15<00:07,  6.21it/s]11/28/2021 01:28:35 - INFO - __main__ -   Batch number = 53
Evaluating:  54%|█████▎    | 53/99 [00:15<00:07,  6.18it/s]11/28/2021 01:28:35 - INFO - __main__ -   Batch number = 54
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  55%|█████▍    | 54/99 [00:15<00:07,  6.23it/s]11/28/2021 01:28:35 - INFO - __main__ -   Batch number = 55
Evaluating:  56%|█████▌    | 55/99 [00:15<00:07,  6.23it/s]11/28/2021 01:28:35 - INFO - __main__ -   Batch number = 56
Evaluating:  57%|█████▋    | 56/99 [00:15<00:06,  6.24it/s]11/28/2021 01:28:35 - INFO - __main__ -   Batch number = 57
Evaluating:  58%|█████▊    | 57/99 [00:15<00:06,  6.25it/s]11/28/2021 01:28:35 - INFO - __main__ -   Batch number = 58
Evaluating:  59%|█████▊    | 58/99 [00:16<00:06,  6.12it/s]11/28/2021 01:28:36 - INFO - __main__ -   Batch number = 59
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  60%|█████▉    | 59/99 [00:16<00:06,  5.95it/s]11/28/2021 01:28:36 - INFO - __main__ -   Batch number = 60
Evaluating:  61%|██████    | 60/99 [00:16<00:06,  5.92it/s]11/28/2021 01:28:36 - INFO - __main__ -   Batch number = 61
Evaluating:  62%|██████▏   | 61/99 [00:16<00:06,  5.92it/s]11/28/2021 01:28:36 - INFO - __main__ -   Batch number = 62
Evaluating:  63%|██████▎   | 62/99 [00:16<00:06,  5.91it/s]11/28/2021 01:28:36 - INFO - __main__ -   Batch number = 63
Evaluating:  64%|██████▎   | 63/99 [00:16<00:06,  5.96it/s]11/28/2021 01:28:36 - INFO - __main__ -   Batch number = 64
Evaluating:  65%|██████▍   | 64/99 [00:17<00:05,  6.00it/s]11/28/2021 01:28:37 - INFO - __main__ -   Batch number = 65
loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
Evaluating:  66%|██████▌   | 65/99 [00:17<00:05,  6.01it/s]11/28/2021 01:28:37 - INFO - __main__ -   Batch number = 66
11/28/2021 01:28:37 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
Evaluating:  67%|██████▋   | 66/99 [00:17<00:05,  5.97it/s]11/28/2021 01:28:37 - INFO - __main__ -   Batch number = 67
Evaluating:  68%|██████▊   | 67/99 [00:17<00:05,  5.96it/s]11/28/2021 01:28:37 - INFO - __main__ -   Batch number = 68
Evaluating:  69%|██████▊   | 68/99 [00:17<00:05,  6.04it/s]11/28/2021 01:28:37 - INFO - __main__ -   Batch number = 69
Evaluating:  70%|██████▉   | 69/99 [00:17<00:04,  6.01it/s]11/28/2021 01:28:37 - INFO - __main__ -   Batch number = 70
Evaluating:  71%|███████   | 70/99 [00:18<00:04,  6.03it/s]11/28/2021 01:28:38 - INFO - __main__ -   Batch number = 71
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  72%|███████▏  | 71/99 [00:18<00:04,  5.99it/s]11/28/2021 01:28:38 - INFO - __main__ -   Batch number = 72
Evaluating:  73%|███████▎  | 72/99 [00:18<00:04,  6.02it/s]11/28/2021 01:28:38 - INFO - __main__ -   Batch number = 73
Evaluating:  74%|███████▎  | 73/99 [00:18<00:04,  6.02it/s]11/28/2021 01:28:38 - INFO - __main__ -   Batch number = 74
Evaluating:  75%|███████▍  | 74/99 [00:18<00:04,  6.09it/s]11/28/2021 01:28:38 - INFO - __main__ -   Batch number = 75
Evaluating:  76%|███████▌  | 75/99 [00:18<00:03,  6.10it/s]11/28/2021 01:28:38 - INFO - __main__ -   Batch number = 76
Evaluating:  77%|███████▋  | 76/99 [00:19<00:03,  6.10it/s]11/28/2021 01:28:39 - INFO - __main__ -   Batch number = 77
Evaluating:  78%|███████▊  | 77/99 [00:19<00:03,  6.06it/s]11/28/2021 01:28:39 - INFO - __main__ -   Batch number = 78
Evaluating:  79%|███████▉  | 78/99 [00:19<00:03,  6.06it/s]11/28/2021 01:28:39 - INFO - __main__ -   Batch number = 79
Evaluating:  80%|███████▉  | 79/99 [00:19<00:03,  5.87it/s]11/28/2021 01:28:39 - INFO - __main__ -   Batch number = 80
Evaluating:  81%|████████  | 80/99 [00:19<00:03,  5.64it/s]11/28/2021 01:28:39 - INFO - __main__ -   Batch number = 81
Evaluating:  82%|████████▏ | 81/99 [00:20<00:03,  5.45it/s]11/28/2021 01:28:39 - INFO - __main__ -   Batch number = 82
Evaluating:  83%|████████▎ | 82/99 [00:20<00:03,  5.55it/s]11/28/2021 01:28:40 - INFO - __main__ -   Batch number = 83
Evaluating:  84%|████████▍ | 83/99 [00:20<00:02,  5.61it/s]11/28/2021 01:28:40 - INFO - __main__ -   Batch number = 84
Evaluating:  85%|████████▍ | 84/99 [00:20<00:02,  5.62it/s]11/28/2021 01:28:40 - INFO - __main__ -   Batch number = 85
Evaluating:  86%|████████▌ | 85/99 [00:20<00:02,  5.51it/s]11/28/2021 01:28:40 - INFO - __main__ -   Batch number = 86
Evaluating:  87%|████████▋ | 86/99 [00:20<00:02,  5.55it/s]11/28/2021 01:28:40 - INFO - __main__ -   Batch number = 87
Evaluating:  88%|████████▊ | 87/99 [00:21<00:02,  5.58it/s]11/28/2021 01:28:41 - INFO - __main__ -   Batch number = 88
Evaluating:  89%|████████▉ | 88/99 [00:21<00:01,  5.68it/s]11/28/2021 01:28:41 - INFO - __main__ -   Batch number = 89
Evaluating:  90%|████████▉ | 89/99 [00:21<00:01,  5.70it/s]11/28/2021 01:28:41 - INFO - __main__ -   Batch number = 90
Evaluating:  91%|█████████ | 90/99 [00:21<00:01,  5.77it/s]11/28/2021 01:28:41 - INFO - __main__ -   Batch number = 91
Evaluating:  92%|█████████▏| 91/99 [00:21<00:01,  5.76it/s]11/28/2021 01:28:41 - INFO - __main__ -   Batch number = 92
Evaluating:  93%|█████████▎| 92/99 [00:21<00:01,  5.77it/s]11/28/2021 01:28:41 - INFO - __main__ -   Batch number = 93
Evaluating:  94%|█████████▍| 93/99 [00:22<00:01,  5.77it/s]11/28/2021 01:28:42 - INFO - __main__ -   Batch number = 94
Evaluating:  95%|█████████▍| 94/99 [00:22<00:00,  5.68it/s]11/28/2021 01:28:42 - INFO - __main__ -   Batch number = 95
Evaluating:  96%|█████████▌| 95/99 [00:22<00:00,  5.75it/s]11/28/2021 01:28:42 - INFO - __main__ -   Batch number = 96
Evaluating:  97%|█████████▋| 96/99 [00:22<00:00,  5.76it/s]11/28/2021 01:28:42 - INFO - __main__ -   Batch number = 97
Evaluating:  98%|█████████▊| 97/99 [00:22<00:00,  5.84it/s]11/28/2021 01:28:42 - INFO - __main__ -   Batch number = 98
Evaluating:  99%|█████████▉| 98/99 [00:22<00:00,  5.95it/s]11/28/2021 01:28:42 - INFO - __main__ -   Batch number = 99
Evaluating: 100%|██████████| 99/99 [00:23<00:00,  4.29it/s]Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:28:43 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:28:43 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:28:43 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
11/28/2021 01:28:43 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:28:43 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:28:43 - INFO - root -   loading lang adpater cs/wiki@ukp
11/28/2021 01:28:43 - INFO - __main__ -   Adapter Languages : ['cs'], Length : 1
11/28/2021 01:28:43 - INFO - __main__ -   Adapter Names ['cs/wiki@ukp'], Length : 1
11/28/2021 01:28:43 - INFO - __main__ -   Language = cs
11/28/2021 01:28:43 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/cs/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_cs_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/adapter_config.json
Adding adapter 'cs' of type 'text_lang'.

/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:28:45 - INFO - __main__ -   ***** Evaluation result  in es *****
11/28/2021 01:28:45 - INFO - __main__ -     f1 = 0.877776089928604
11/28/2021 01:28:45 - INFO - __main__ -     loss = 0.38803482145974133
11/28/2021 01:28:45 - INFO - __main__ -     precision = 0.8859532921204984
11/28/2021 01:28:45 - INFO - __main__ -     recall = 0.8697484556937638
Loading module weights from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/head_config.json
39.36user 13.22system 0:55.16elapsed 95%CPU (0avgtext+0avgdata 3995872maxresident)k
0inputs+944outputs (0major+1906781minor)pagefaults 0swaps
11/28/2021 01:28:54 - INFO - __main__ -   Language adapter for ru not found, using cs instead
11/28/2021 01:28:54 - INFO - __main__ -   Set active language adapter to cs
11/28/2021 01:28:54 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:28:54 - INFO - __main__ -   Adapter Languages = ['cs']
11/28/2021 01:28:54 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_ru_bert-base-multilingual-cased_128
11/28/2021 01:28:55 - INFO - __main__ -   ***** Running evaluation  in ru *****
11/28/2021 01:28:55 - INFO - __main__ -     Num examples = 8995
11/28/2021 01:28:55 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/282 [00:00<?, ?it/s]11/28/2021 01:28:55 - INFO - __main__ -   Batch number = 1
Evaluating:   0%|          | 1/282 [00:00<02:18,  2.03it/s]11/28/2021 01:28:56 - INFO - __main__ -   Batch number = 2
Evaluating:   1%|          | 2/282 [00:00<02:18,  2.03it/s]11/28/2021 01:28:56 - INFO - __main__ -   Batch number = 3
Evaluating:   1%|          | 3/282 [00:01<02:18,  2.02it/s]11/28/2021 01:28:57 - INFO - __main__ -   Batch number = 4
Evaluating:   1%|▏         | 4/282 [00:01<02:18,  2.01it/s]11/28/2021 01:28:57 - INFO - __main__ -   Batch number = 5
Evaluating:   2%|▏         | 5/282 [00:02<02:17,  2.01it/s]11/28/2021 01:28:58 - INFO - __main__ -   Batch number = 6
Evaluating:   2%|▏         | 6/282 [00:02<02:16,  2.02it/s]11/28/2021 01:28:58 - INFO - __main__ -   Batch number = 7
Evaluating:   2%|▏         | 7/282 [00:03<02:16,  2.02it/s]11/28/2021 01:28:59 - INFO - __main__ -   Batch number = 8
Evaluating:   3%|▎         | 8/282 [00:03<02:15,  2.02it/s]11/28/2021 01:28:59 - INFO - __main__ -   Batch number = 9
Evaluating:   3%|▎         | 9/282 [00:04<02:21,  1.93it/s]11/28/2021 01:29:00 - INFO - __main__ -   Batch number = 10
Evaluating:   4%|▎         | 10/282 [00:05<02:18,  1.96it/s]11/28/2021 01:29:00 - INFO - __main__ -   Batch number = 11
Evaluating:   4%|▍         | 11/282 [00:05<02:16,  1.98it/s]11/28/2021 01:29:01 - INFO - __main__ -   Batch number = 12
Evaluating:   4%|▍         | 12/282 [00:06<02:15,  1.99it/s]11/28/2021 01:29:01 - INFO - __main__ -   Batch number = 13
Evaluating:   5%|▍         | 13/282 [00:06<02:14,  2.00it/s]11/28/2021 01:29:02 - INFO - __main__ -   Batch number = 14
Evaluating:   5%|▍         | 14/282 [00:07<02:13,  2.00it/s]11/28/2021 01:29:02 - INFO - __main__ -   Batch number = 15
Evaluating:   5%|▌         | 15/282 [00:07<02:13,  2.00it/s]11/28/2021 01:29:03 - INFO - __main__ -   Batch number = 16
Evaluating:   6%|▌         | 16/282 [00:08<02:13,  2.00it/s]11/28/2021 01:29:03 - INFO - __main__ -   Batch number = 17
Evaluating:   6%|▌         | 17/282 [00:08<02:12,  2.00it/s]11/28/2021 01:29:04 - INFO - __main__ -   Batch number = 18
Evaluating:   6%|▋         | 18/282 [00:09<02:11,  2.00it/s]11/28/2021 01:29:04 - INFO - __main__ -   Batch number = 19
Evaluating:   7%|▋         | 19/282 [00:09<02:10,  2.01it/s]11/28/2021 01:29:05 - INFO - __main__ -   Batch number = 20
Evaluating:   7%|▋         | 20/282 [00:10<02:10,  2.00it/s]11/28/2021 01:29:05 - INFO - __main__ -   Batch number = 21
Evaluating:   7%|▋         | 21/282 [00:10<02:10,  2.00it/s]11/28/2021 01:29:06 - INFO - __main__ -   Batch number = 22
Evaluating:   8%|▊         | 22/282 [00:11<02:10,  2.00it/s]11/28/2021 01:29:06 - INFO - __main__ -   Batch number = 23
Evaluating:   8%|▊         | 23/282 [00:11<02:09,  2.00it/s]11/28/2021 01:29:07 - INFO - __main__ -   Batch number = 24
Evaluating:   9%|▊         | 24/282 [00:12<02:09,  1.99it/s]11/28/2021 01:29:07 - INFO - __main__ -   Batch number = 25
Evaluating:   9%|▉         | 25/282 [00:12<02:08,  2.00it/s]11/28/2021 01:29:08 - INFO - __main__ -   Batch number = 26
Evaluating:   9%|▉         | 26/282 [00:13<02:07,  2.00it/s]11/28/2021 01:29:08 - INFO - __main__ -   Batch number = 27
Evaluating:  10%|▉         | 27/282 [00:13<02:07,  1.99it/s]11/28/2021 01:29:09 - INFO - __main__ -   Batch number = 28
Evaluating:  10%|▉         | 28/282 [00:14<02:07,  2.00it/s]11/28/2021 01:29:09 - INFO - __main__ -   Batch number = 29
Evaluating:  10%|█         | 29/282 [00:14<02:06,  2.00it/s]11/28/2021 01:29:10 - INFO - __main__ -   Batch number = 30
Evaluating:  11%|█         | 30/282 [00:14<02:03,  2.04it/s]11/28/2021 01:29:10 - INFO - __main__ -   Batch number = 31
Evaluating:  11%|█         | 31/282 [00:15<01:51,  2.25it/s]11/28/2021 01:29:11 - INFO - __main__ -   Batch number = 32
Evaluating:  11%|█▏        | 32/282 [00:15<01:42,  2.44it/s]11/28/2021 01:29:11 - INFO - __main__ -   Batch number = 33
Evaluating:  12%|█▏        | 33/282 [00:15<01:36,  2.58it/s]11/28/2021 01:29:11 - INFO - __main__ -   Batch number = 34
Evaluating:  12%|█▏        | 34/282 [00:16<01:40,  2.46it/s]11/28/2021 01:29:12 - INFO - __main__ -   Batch number = 35
Evaluating:  12%|█▏        | 35/282 [00:16<01:46,  2.32it/s]11/28/2021 01:29:12 - INFO - __main__ -   Batch number = 36
Evaluating:  13%|█▎        | 36/282 [00:17<01:50,  2.22it/s]11/28/2021 01:29:13 - INFO - __main__ -   Batch number = 37
Evaluating:  13%|█▎        | 37/282 [00:17<01:54,  2.15it/s]11/28/2021 01:29:13 - INFO - __main__ -   Batch number = 38
Evaluating:  13%|█▎        | 38/282 [00:18<01:56,  2.09it/s]11/28/2021 01:29:14 - INFO - __main__ -   Batch number = 39
Evaluating:  14%|█▍        | 39/282 [00:18<01:57,  2.07it/s]11/28/2021 01:29:14 - INFO - __main__ -   Batch number = 40
Evaluating:  14%|█▍        | 40/282 [00:19<02:13,  1.81it/s]11/28/2021 01:29:15 - INFO - __main__ -   Batch number = 41
Evaluating:  15%|█▍        | 41/282 [00:20<02:08,  1.88it/s]11/28/2021 01:29:15 - INFO - __main__ -   Batch number = 42
Evaluating:  15%|█▍        | 42/282 [00:20<02:05,  1.91it/s]11/28/2021 01:29:16 - INFO - __main__ -   Batch number = 43
Evaluating:  15%|█▌        | 43/282 [00:21<02:03,  1.94it/s]11/28/2021 01:29:16 - INFO - __main__ -   Batch number = 44
Evaluating:  16%|█▌        | 44/282 [00:21<02:01,  1.96it/s]11/28/2021 01:29:17 - INFO - __main__ -   Batch number = 45
Evaluating:  16%|█▌        | 45/282 [00:22<01:59,  1.98it/s]11/28/2021 01:29:17 - INFO - __main__ -   Batch number = 46
Evaluating:  16%|█▋        | 46/282 [00:22<01:59,  1.98it/s]11/28/2021 01:29:18 - INFO - __main__ -   Batch number = 47
Evaluating:  17%|█▋        | 47/282 [00:23<01:57,  1.99it/s]11/28/2021 01:29:18 - INFO - __main__ -   Batch number = 48
Evaluating:  17%|█▋        | 48/282 [00:23<01:56,  2.00it/s]11/28/2021 01:29:19 - INFO - __main__ -   Batch number = 49
Evaluating:  17%|█▋        | 49/282 [00:24<01:56,  2.00it/s]11/28/2021 01:29:19 - INFO - __main__ -   Batch number = 50
Evaluating:  18%|█▊        | 50/282 [00:24<01:55,  2.01it/s]11/28/2021 01:29:20 - INFO - __main__ -   Batch number = 51
Evaluating:  18%|█▊        | 51/282 [00:25<01:55,  2.01it/s]11/28/2021 01:29:20 - INFO - __main__ -   Batch number = 52
Evaluating:  18%|█▊        | 52/282 [00:25<01:54,  2.01it/s]11/28/2021 01:29:21 - INFO - __main__ -   Batch number = 53
Evaluating:  19%|█▉        | 53/282 [00:26<01:53,  2.01it/s]11/28/2021 01:29:21 - INFO - __main__ -   Batch number = 54
Evaluating:  19%|█▉        | 54/282 [00:26<01:53,  2.00it/s]11/28/2021 01:29:22 - INFO - __main__ -   Batch number = 55
Evaluating:  20%|█▉        | 55/282 [00:27<01:53,  2.01it/s]11/28/2021 01:29:22 - INFO - __main__ -   Batch number = 56
Evaluating:  20%|█▉        | 56/282 [00:27<01:51,  2.03it/s]11/28/2021 01:29:23 - INFO - __main__ -   Batch number = 57
Evaluating:  20%|██        | 57/282 [00:28<01:50,  2.04it/s]11/28/2021 01:29:23 - INFO - __main__ -   Batch number = 58
Evaluating:  21%|██        | 58/282 [00:28<01:50,  2.03it/s]11/28/2021 01:29:24 - INFO - __main__ -   Batch number = 59
Evaluating:  21%|██        | 59/282 [00:29<01:49,  2.03it/s]11/28/2021 01:29:24 - INFO - __main__ -   Batch number = 60
Evaluating:  21%|██▏       | 60/282 [00:29<01:51,  1.99it/s]11/28/2021 01:29:25 - INFO - __main__ -   Batch number = 61
Evaluating:  22%|██▏       | 61/282 [00:30<01:50,  2.00it/s]11/28/2021 01:29:25 - INFO - __main__ -   Batch number = 62
Evaluating:  22%|██▏       | 62/282 [00:30<01:49,  2.00it/s]11/28/2021 01:29:26 - INFO - __main__ -   Batch number = 63
Evaluating:  22%|██▏       | 63/282 [00:31<01:49,  2.00it/s]11/28/2021 01:29:26 - INFO - __main__ -   Batch number = 64
Evaluating:  23%|██▎       | 64/282 [00:31<01:49,  1.98it/s]11/28/2021 01:29:27 - INFO - __main__ -   Batch number = 65
Evaluating:  23%|██▎       | 65/282 [00:32<01:49,  1.98it/s]11/28/2021 01:29:27 - INFO - __main__ -   Batch number = 66
Evaluating:  23%|██▎       | 66/282 [00:32<01:49,  1.98it/s]11/28/2021 01:29:28 - INFO - __main__ -   Batch number = 67
Evaluating:  24%|██▍       | 67/282 [00:33<01:49,  1.96it/s]11/28/2021 01:29:28 - INFO - __main__ -   Batch number = 68
Evaluating:  24%|██▍       | 68/282 [00:33<01:48,  1.98it/s]11/28/2021 01:29:29 - INFO - __main__ -   Batch number = 69
Evaluating:  24%|██▍       | 69/282 [00:34<01:46,  2.00it/s]11/28/2021 01:29:29 - INFO - __main__ -   Batch number = 70
Evaluating:  25%|██▍       | 70/282 [00:34<01:49,  1.94it/s]11/28/2021 01:29:30 - INFO - __main__ -   Batch number = 71
Evaluating:  25%|██▌       | 71/282 [00:35<01:47,  1.97it/s]11/28/2021 01:29:30 - INFO - __main__ -   Batch number = 72
Evaluating:  26%|██▌       | 72/282 [00:35<01:46,  1.97it/s]11/28/2021 01:29:31 - INFO - __main__ -   Batch number = 73
Evaluating:  26%|██▌       | 73/282 [00:36<01:46,  1.97it/s]11/28/2021 01:29:31 - INFO - __main__ -   Batch number = 74
Evaluating:  26%|██▌       | 74/282 [00:36<01:46,  1.96it/s]11/28/2021 01:29:32 - INFO - __main__ -   Batch number = 75
Evaluating:  27%|██▋       | 75/282 [00:37<01:46,  1.95it/s]11/28/2021 01:29:32 - INFO - __main__ -   Batch number = 76
Evaluating:  27%|██▋       | 76/282 [00:37<01:45,  1.95it/s]11/28/2021 01:29:33 - INFO - __main__ -   Batch number = 77
Evaluating:  27%|██▋       | 77/282 [00:38<01:44,  1.95it/s]11/28/2021 01:29:33 - INFO - __main__ -   Batch number = 78
Evaluating:  28%|██▊       | 78/282 [00:38<01:43,  1.97it/s]11/28/2021 01:29:34 - INFO - __main__ -   Batch number = 79
Evaluating:  28%|██▊       | 79/282 [00:39<01:43,  1.97it/s]11/28/2021 01:29:34 - INFO - __main__ -   Batch number = 80
Evaluating:  28%|██▊       | 80/282 [00:39<01:42,  1.97it/s]11/28/2021 01:29:35 - INFO - __main__ -   Batch number = 81
Evaluating:  29%|██▊       | 81/282 [00:40<01:41,  1.97it/s]11/28/2021 01:29:36 - INFO - __main__ -   Batch number = 82
Evaluating:  29%|██▉       | 82/282 [00:40<01:40,  1.99it/s]11/28/2021 01:29:36 - INFO - __main__ -   Batch number = 83
Evaluating:  29%|██▉       | 83/282 [00:41<01:40,  1.99it/s]11/28/2021 01:29:37 - INFO - __main__ -   Batch number = 84
Evaluating:  30%|██▉       | 84/282 [00:41<01:39,  2.00it/s]11/28/2021 01:29:37 - INFO - __main__ -   Batch number = 85
Evaluating:  30%|███       | 85/282 [00:42<01:38,  2.00it/s]11/28/2021 01:29:38 - INFO - __main__ -   Batch number = 86
Evaluating:  30%|███       | 86/282 [00:42<01:47,  1.82it/s]11/28/2021 01:29:38 - INFO - __main__ -   Batch number = 87
Evaluating:  31%|███       | 87/282 [00:43<01:54,  1.71it/s]11/28/2021 01:29:39 - INFO - __main__ -   Batch number = 88
Evaluating:  31%|███       | 88/282 [00:44<01:53,  1.70it/s]11/28/2021 01:29:39 - INFO - __main__ -   Batch number = 89
Evaluating:  32%|███▏      | 89/282 [00:44<02:00,  1.60it/s]11/28/2021 01:29:40 - INFO - __main__ -   Batch number = 90
Evaluating:  32%|███▏      | 90/282 [00:45<02:03,  1.55it/s]11/28/2021 01:29:41 - INFO - __main__ -   Batch number = 91
Evaluating:  32%|███▏      | 91/282 [00:46<02:05,  1.53it/s]11/28/2021 01:29:42 - INFO - __main__ -   Batch number = 92
Evaluating:  33%|███▎      | 92/282 [00:46<02:04,  1.52it/s]11/28/2021 01:29:42 - INFO - __main__ -   Batch number = 93
Evaluating:  33%|███▎      | 93/282 [00:47<02:04,  1.52it/s]11/28/2021 01:29:43 - INFO - __main__ -   Batch number = 94
Evaluating:  33%|███▎      | 94/282 [00:48<02:04,  1.52it/s]11/28/2021 01:29:43 - INFO - __main__ -   Batch number = 95
Evaluating:  34%|███▎      | 95/282 [00:48<02:03,  1.51it/s]11/28/2021 01:29:44 - INFO - __main__ -   Batch number = 96
Evaluating:  34%|███▍      | 96/282 [00:49<02:02,  1.52it/s]11/28/2021 01:29:45 - INFO - __main__ -   Batch number = 97
Evaluating:  34%|███▍      | 97/282 [00:50<02:02,  1.51it/s]11/28/2021 01:29:45 - INFO - __main__ -   Batch number = 98
Evaluating:  35%|███▍      | 98/282 [00:50<02:01,  1.51it/s]11/28/2021 01:29:46 - INFO - __main__ -   Batch number = 99
Evaluating:  35%|███▌      | 99/282 [00:51<02:01,  1.51it/s]11/28/2021 01:29:47 - INFO - __main__ -   Batch number = 100
Evaluating:  35%|███▌      | 100/282 [00:52<01:55,  1.58it/s]11/28/2021 01:29:47 - INFO - __main__ -   Batch number = 101
Evaluating:  36%|███▌      | 101/282 [00:52<01:47,  1.68it/s]11/28/2021 01:29:48 - INFO - __main__ -   Batch number = 102
Evaluating:  36%|███▌      | 102/282 [00:53<01:42,  1.76it/s]11/28/2021 01:29:48 - INFO - __main__ -   Batch number = 103
Evaluating:  37%|███▋      | 103/282 [00:53<01:38,  1.81it/s]11/28/2021 01:29:49 - INFO - __main__ -   Batch number = 104
Evaluating:  37%|███▋      | 104/282 [00:54<01:35,  1.87it/s]11/28/2021 01:29:49 - INFO - __main__ -   Batch number = 105
Evaluating:  37%|███▋      | 105/282 [00:54<01:33,  1.90it/s]11/28/2021 01:29:50 - INFO - __main__ -   Batch number = 106
Evaluating:  38%|███▊      | 106/282 [00:55<01:31,  1.92it/s]11/28/2021 01:29:50 - INFO - __main__ -   Batch number = 107
Evaluating:  38%|███▊      | 107/282 [00:55<01:30,  1.92it/s]11/28/2021 01:29:51 - INFO - __main__ -   Batch number = 108
Evaluating:  38%|███▊      | 108/282 [00:56<01:30,  1.92it/s]11/28/2021 01:29:51 - INFO - __main__ -   Batch number = 109
Evaluating:  39%|███▊      | 109/282 [00:56<01:29,  1.93it/s]11/28/2021 01:29:52 - INFO - __main__ -   Batch number = 110
Evaluating:  39%|███▉      | 110/282 [00:57<01:28,  1.94it/s]11/28/2021 01:29:52 - INFO - __main__ -   Batch number = 111
Evaluating:  39%|███▉      | 111/282 [00:57<01:27,  1.95it/s]11/28/2021 01:29:53 - INFO - __main__ -   Batch number = 112
Evaluating:  40%|███▉      | 112/282 [00:58<01:27,  1.95it/s]11/28/2021 01:29:53 - INFO - __main__ -   Batch number = 113
Evaluating:  40%|████      | 113/282 [00:58<01:26,  1.96it/s]11/28/2021 01:29:54 - INFO - __main__ -   Batch number = 114
Evaluating:  40%|████      | 114/282 [00:59<01:24,  1.98it/s]11/28/2021 01:29:54 - INFO - __main__ -   Batch number = 115
Evaluating:  41%|████      | 115/282 [00:59<01:24,  1.98it/s]11/28/2021 01:29:55 - INFO - __main__ -   Batch number = 116
Evaluating:  41%|████      | 116/282 [01:00<01:23,  1.98it/s]11/28/2021 01:29:55 - INFO - __main__ -   Batch number = 117
Evaluating:  41%|████▏     | 117/282 [01:00<01:23,  1.98it/s]11/28/2021 01:29:56 - INFO - __main__ -   Batch number = 118
Evaluating:  42%|████▏     | 118/282 [01:01<01:23,  1.96it/s]11/28/2021 01:29:57 - INFO - __main__ -   Batch number = 119
Evaluating:  42%|████▏     | 119/282 [01:01<01:23,  1.96it/s]11/28/2021 01:29:57 - INFO - __main__ -   Batch number = 120
Evaluating:  43%|████▎     | 120/282 [01:02<01:22,  1.96it/s]11/28/2021 01:29:58 - INFO - __main__ -   Batch number = 121
Evaluating:  43%|████▎     | 121/282 [01:02<01:22,  1.96it/s]11/28/2021 01:29:58 - INFO - __main__ -   Batch number = 122
Evaluating:  43%|████▎     | 122/282 [01:03<01:21,  1.97it/s]11/28/2021 01:29:59 - INFO - __main__ -   Batch number = 123
Evaluating:  44%|████▎     | 123/282 [01:03<01:20,  1.97it/s]11/28/2021 01:29:59 - INFO - __main__ -   Batch number = 124
Evaluating:  44%|████▍     | 124/282 [01:04<01:18,  2.01it/s]11/28/2021 01:30:00 - INFO - __main__ -   Batch number = 125
Evaluating:  44%|████▍     | 125/282 [01:04<01:18,  2.00it/s]11/28/2021 01:30:00 - INFO - __main__ -   Batch number = 126
Evaluating:  45%|████▍     | 126/282 [01:05<01:19,  1.97it/s]11/28/2021 01:30:01 - INFO - __main__ -   Batch number = 127
Evaluating:  45%|████▌     | 127/282 [01:05<01:19,  1.95it/s]11/28/2021 01:30:01 - INFO - __main__ -   Batch number = 128
Evaluating:  45%|████▌     | 128/282 [01:06<01:18,  1.96it/s]11/28/2021 01:30:02 - INFO - __main__ -   Batch number = 129
Evaluating:  46%|████▌     | 129/282 [01:06<01:18,  1.96it/s]11/28/2021 01:30:02 - INFO - __main__ -   Batch number = 130
Evaluating:  46%|████▌     | 130/282 [01:07<01:17,  1.95it/s]11/28/2021 01:30:03 - INFO - __main__ -   Batch number = 131
Evaluating:  46%|████▋     | 131/282 [01:07<01:17,  1.96it/s]11/28/2021 01:30:03 - INFO - __main__ -   Batch number = 132
Evaluating:  47%|████▋     | 132/282 [01:08<01:13,  2.05it/s]11/28/2021 01:30:04 - INFO - __main__ -   Batch number = 133
Evaluating:  47%|████▋     | 133/282 [01:08<01:06,  2.24it/s]11/28/2021 01:30:04 - INFO - __main__ -   Batch number = 134
Evaluating:  48%|████▊     | 134/282 [01:08<01:01,  2.41it/s]11/28/2021 01:30:04 - INFO - __main__ -   Batch number = 135
Evaluating:  48%|████▊     | 135/282 [01:09<01:00,  2.43it/s]11/28/2021 01:30:05 - INFO - __main__ -   Batch number = 136
Evaluating:  48%|████▊     | 136/282 [01:09<00:57,  2.55it/s]11/28/2021 01:30:05 - INFO - __main__ -   Batch number = 137
Evaluating:  49%|████▊     | 137/282 [01:10<00:54,  2.65it/s]11/28/2021 01:30:05 - INFO - __main__ -   Batch number = 138
Evaluating:  49%|████▉     | 138/282 [01:10<00:52,  2.72it/s]11/28/2021 01:30:06 - INFO - __main__ -   Batch number = 139
Evaluating:  49%|████▉     | 139/282 [01:10<00:51,  2.76it/s]11/28/2021 01:30:06 - INFO - __main__ -   Batch number = 140
Evaluating:  50%|████▉     | 140/282 [01:11<00:50,  2.84it/s]11/28/2021 01:30:06 - INFO - __main__ -   Batch number = 141
Evaluating:  50%|█████     | 141/282 [01:11<00:49,  2.87it/s]11/28/2021 01:30:07 - INFO - __main__ -   Batch number = 142
Evaluating:  50%|█████     | 142/282 [01:11<00:48,  2.89it/s]11/28/2021 01:30:07 - INFO - __main__ -   Batch number = 143
Evaluating:  51%|█████     | 143/282 [01:12<00:47,  2.91it/s]11/28/2021 01:30:07 - INFO - __main__ -   Batch number = 144
Evaluating:  51%|█████     | 144/282 [01:12<00:47,  2.92it/s]11/28/2021 01:30:08 - INFO - __main__ -   Batch number = 145
Evaluating:  51%|█████▏    | 145/282 [01:12<00:47,  2.91it/s]11/28/2021 01:30:08 - INFO - __main__ -   Batch number = 146
Evaluating:  52%|█████▏    | 146/282 [01:13<00:46,  2.93it/s]11/28/2021 01:30:08 - INFO - __main__ -   Batch number = 147
Evaluating:  52%|█████▏    | 147/282 [01:13<00:45,  2.98it/s]11/28/2021 01:30:09 - INFO - __main__ -   Batch number = 148
Evaluating:  52%|█████▏    | 148/282 [01:13<00:45,  2.95it/s]11/28/2021 01:30:09 - INFO - __main__ -   Batch number = 149
Evaluating:  53%|█████▎    | 149/282 [01:14<00:45,  2.95it/s]11/28/2021 01:30:09 - INFO - __main__ -   Batch number = 150
Evaluating:  53%|█████▎    | 150/282 [01:14<00:44,  2.95it/s]11/28/2021 01:30:10 - INFO - __main__ -   Batch number = 151
Evaluating:  54%|█████▎    | 151/282 [01:14<00:44,  2.93it/s]11/28/2021 01:30:10 - INFO - __main__ -   Batch number = 152
Evaluating:  54%|█████▍    | 152/282 [01:15<00:44,  2.89it/s]11/28/2021 01:30:10 - INFO - __main__ -   Batch number = 153
Evaluating:  54%|█████▍    | 153/282 [01:15<00:44,  2.88it/s]11/28/2021 01:30:11 - INFO - __main__ -   Batch number = 154
Evaluating:  55%|█████▍    | 154/282 [01:15<00:44,  2.88it/s]11/28/2021 01:30:11 - INFO - __main__ -   Batch number = 155
Evaluating:  55%|█████▍    | 155/282 [01:16<00:45,  2.80it/s]11/28/2021 01:30:12 - INFO - __main__ -   Batch number = 156
Evaluating:  55%|█████▌    | 156/282 [01:16<00:50,  2.49it/s]11/28/2021 01:30:12 - INFO - __main__ -   Batch number = 157
Evaluating:  56%|█████▌    | 157/282 [01:17<00:54,  2.30it/s]11/28/2021 01:30:13 - INFO - __main__ -   Batch number = 158
Evaluating:  56%|█████▌    | 158/282 [01:17<00:56,  2.19it/s]11/28/2021 01:30:13 - INFO - __main__ -   Batch number = 159
Evaluating:  56%|█████▋    | 159/282 [01:18<00:58,  2.11it/s]11/28/2021 01:30:14 - INFO - __main__ -   Batch number = 160
Evaluating:  57%|█████▋    | 160/282 [01:18<00:59,  2.06it/s]11/28/2021 01:30:14 - INFO - __main__ -   Batch number = 161
Evaluating:  57%|█████▋    | 161/282 [01:19<00:58,  2.07it/s]11/28/2021 01:30:15 - INFO - __main__ -   Batch number = 162
Evaluating:  57%|█████▋    | 162/282 [01:19<00:59,  2.03it/s]11/28/2021 01:30:15 - INFO - __main__ -   Batch number = 163
Evaluating:  58%|█████▊    | 163/282 [01:20<00:59,  2.01it/s]11/28/2021 01:30:16 - INFO - __main__ -   Batch number = 164
Evaluating:  58%|█████▊    | 164/282 [01:20<00:59,  1.98it/s]11/28/2021 01:30:16 - INFO - __main__ -   Batch number = 165
Evaluating:  59%|█████▊    | 165/282 [01:21<00:58,  1.98it/s]11/28/2021 01:30:17 - INFO - __main__ -   Batch number = 166
Evaluating:  59%|█████▉    | 166/282 [01:21<00:56,  2.06it/s]11/28/2021 01:30:17 - INFO - __main__ -   Batch number = 167
Evaluating:  59%|█████▉    | 167/282 [01:22<00:50,  2.26it/s]11/28/2021 01:30:17 - INFO - __main__ -   Batch number = 168
Evaluating:  60%|█████▉    | 168/282 [01:22<00:47,  2.41it/s]11/28/2021 01:30:18 - INFO - __main__ -   Batch number = 169
Evaluating:  60%|█████▉    | 169/282 [01:22<00:44,  2.54it/s]11/28/2021 01:30:18 - INFO - __main__ -   Batch number = 170
Evaluating:  60%|██████    | 170/282 [01:23<00:42,  2.63it/s]11/28/2021 01:30:18 - INFO - __main__ -   Batch number = 171
Evaluating:  61%|██████    | 171/282 [01:23<00:40,  2.71it/s]11/28/2021 01:30:19 - INFO - __main__ -   Batch number = 172
Evaluating:  61%|██████    | 172/282 [01:23<00:40,  2.74it/s]11/28/2021 01:30:19 - INFO - __main__ -   Batch number = 173
Evaluating:  61%|██████▏   | 173/282 [01:24<00:36,  2.99it/s]11/28/2021 01:30:19 - INFO - __main__ -   Batch number = 174
Evaluating:  62%|██████▏   | 174/282 [01:24<00:36,  2.93it/s]11/28/2021 01:30:20 - INFO - __main__ -   Batch number = 175
Evaluating:  62%|██████▏   | 175/282 [01:24<00:36,  2.93it/s]11/28/2021 01:30:20 - INFO - __main__ -   Batch number = 176
Evaluating:  62%|██████▏   | 176/282 [01:25<00:36,  2.89it/s]11/28/2021 01:30:20 - INFO - __main__ -   Batch number = 177
Evaluating:  63%|██████▎   | 177/282 [01:25<00:36,  2.90it/s]11/28/2021 01:30:21 - INFO - __main__ -   Batch number = 178
Evaluating:  63%|██████▎   | 178/282 [01:25<00:36,  2.85it/s]11/28/2021 01:30:21 - INFO - __main__ -   Batch number = 179
Evaluating:  63%|██████▎   | 179/282 [01:26<00:36,  2.86it/s]11/28/2021 01:30:22 - INFO - __main__ -   Batch number = 180
Evaluating:  64%|██████▍   | 180/282 [01:26<00:36,  2.83it/s]11/28/2021 01:30:22 - INFO - __main__ -   Batch number = 181
Evaluating:  64%|██████▍   | 181/282 [01:26<00:35,  2.84it/s]11/28/2021 01:30:22 - INFO - __main__ -   Batch number = 182
Evaluating:  65%|██████▍   | 182/282 [01:27<00:35,  2.83it/s]11/28/2021 01:30:23 - INFO - __main__ -   Batch number = 183
Evaluating:  65%|██████▍   | 183/282 [01:27<00:34,  2.87it/s]11/28/2021 01:30:23 - INFO - __main__ -   Batch number = 184
Evaluating:  65%|██████▌   | 184/282 [01:27<00:34,  2.85it/s]11/28/2021 01:30:23 - INFO - __main__ -   Batch number = 185
Evaluating:  66%|██████▌   | 185/282 [01:28<00:33,  2.87it/s]11/28/2021 01:30:24 - INFO - __main__ -   Batch number = 186
Evaluating:  66%|██████▌   | 186/282 [01:28<00:33,  2.87it/s]11/28/2021 01:30:24 - INFO - __main__ -   Batch number = 187
Evaluating:  66%|██████▋   | 187/282 [01:29<00:33,  2.88it/s]11/28/2021 01:30:24 - INFO - __main__ -   Batch number = 188
Evaluating:  67%|██████▋   | 188/282 [01:29<00:35,  2.64it/s]11/28/2021 01:30:25 - INFO - __main__ -   Batch number = 189
Evaluating:  67%|██████▋   | 189/282 [01:29<00:34,  2.72it/s]11/28/2021 01:30:25 - INFO - __main__ -   Batch number = 190
Evaluating:  67%|██████▋   | 190/282 [01:30<00:33,  2.75it/s]11/28/2021 01:30:25 - INFO - __main__ -   Batch number = 191
Evaluating:  68%|██████▊   | 191/282 [01:30<00:32,  2.79it/s]11/28/2021 01:30:26 - INFO - __main__ -   Batch number = 192
Evaluating:  68%|██████▊   | 192/282 [01:30<00:32,  2.79it/s]11/28/2021 01:30:26 - INFO - __main__ -   Batch number = 193
Evaluating:  68%|██████▊   | 193/282 [01:31<00:31,  2.82it/s]11/28/2021 01:30:27 - INFO - __main__ -   Batch number = 194
Evaluating:  69%|██████▉   | 194/282 [01:31<00:31,  2.82it/s]11/28/2021 01:30:27 - INFO - __main__ -   Batch number = 195
Evaluating:  69%|██████▉   | 195/282 [01:31<00:30,  2.84it/s]11/28/2021 01:30:27 - INFO - __main__ -   Batch number = 196
Evaluating:  70%|██████▉   | 196/282 [01:32<00:30,  2.83it/s]11/28/2021 01:30:28 - INFO - __main__ -   Batch number = 197
Evaluating:  70%|██████▉   | 197/282 [01:32<00:29,  2.87it/s]11/28/2021 01:30:28 - INFO - __main__ -   Batch number = 198
Evaluating:  70%|███████   | 198/282 [01:33<00:30,  2.80it/s]11/28/2021 01:30:28 - INFO - __main__ -   Batch number = 199
Evaluating:  71%|███████   | 199/282 [01:33<00:29,  2.83it/s]11/28/2021 01:30:29 - INFO - __main__ -   Batch number = 200
Evaluating:  71%|███████   | 200/282 [01:33<00:29,  2.83it/s]11/28/2021 01:30:29 - INFO - __main__ -   Batch number = 201
Evaluating:  71%|███████▏  | 201/282 [01:34<00:28,  2.85it/s]11/28/2021 01:30:30 - INFO - __main__ -   Batch number = 202
Evaluating:  72%|███████▏  | 202/282 [01:34<00:32,  2.49it/s]11/28/2021 01:30:30 - INFO - __main__ -   Batch number = 203
Evaluating:  72%|███████▏  | 203/282 [01:35<00:34,  2.31it/s]11/28/2021 01:30:30 - INFO - __main__ -   Batch number = 204
Evaluating:  72%|███████▏  | 204/282 [01:35<00:35,  2.19it/s]11/28/2021 01:30:31 - INFO - __main__ -   Batch number = 205
Evaluating:  73%|███████▎  | 205/282 [01:36<00:36,  2.13it/s]11/28/2021 01:30:31 - INFO - __main__ -   Batch number = 206
Evaluating:  73%|███████▎  | 206/282 [01:36<00:36,  2.06it/s]11/28/2021 01:30:32 - INFO - __main__ -   Batch number = 207
Evaluating:  73%|███████▎  | 207/282 [01:37<00:36,  2.03it/s]11/28/2021 01:30:32 - INFO - __main__ -   Batch number = 208
Evaluating:  74%|███████▍  | 208/282 [01:37<00:37,  2.00it/s]11/28/2021 01:30:33 - INFO - __main__ -   Batch number = 209
Evaluating:  74%|███████▍  | 209/282 [01:38<00:36,  1.98it/s]11/28/2021 01:30:33 - INFO - __main__ -   Batch number = 210
Evaluating:  74%|███████▍  | 210/282 [01:38<00:36,  1.97it/s]11/28/2021 01:30:34 - INFO - __main__ -   Batch number = 211
Evaluating:  75%|███████▍  | 211/282 [01:39<00:36,  1.96it/s]11/28/2021 01:30:34 - INFO - __main__ -   Batch number = 212
Evaluating:  75%|███████▌  | 212/282 [01:39<00:36,  1.94it/s]11/28/2021 01:30:35 - INFO - __main__ -   Batch number = 213
Evaluating:  76%|███████▌  | 213/282 [01:40<00:35,  1.95it/s]11/28/2021 01:30:35 - INFO - __main__ -   Batch number = 214
Evaluating:  76%|███████▌  | 214/282 [01:40<00:34,  1.95it/s]11/28/2021 01:30:36 - INFO - __main__ -   Batch number = 215
Evaluating:  76%|███████▌  | 215/282 [01:41<00:34,  1.96it/s]11/28/2021 01:30:37 - INFO - __main__ -   Batch number = 216
Evaluating:  77%|███████▋  | 216/282 [01:41<00:34,  1.94it/s]11/28/2021 01:30:37 - INFO - __main__ -   Batch number = 217
Evaluating:  77%|███████▋  | 217/282 [01:42<00:33,  1.94it/s]11/28/2021 01:30:38 - INFO - __main__ -   Batch number = 218
Evaluating:  77%|███████▋  | 218/282 [01:42<00:33,  1.93it/s]11/28/2021 01:30:38 - INFO - __main__ -   Batch number = 219
Evaluating:  78%|███████▊  | 219/282 [01:43<00:32,  1.94it/s]11/28/2021 01:30:39 - INFO - __main__ -   Batch number = 220
Evaluating:  78%|███████▊  | 220/282 [01:43<00:32,  1.93it/s]11/28/2021 01:30:39 - INFO - __main__ -   Batch number = 221
Evaluating:  78%|███████▊  | 221/282 [01:44<00:31,  1.93it/s]11/28/2021 01:30:40 - INFO - __main__ -   Batch number = 222
Evaluating:  79%|███████▊  | 222/282 [01:44<00:31,  1.93it/s]11/28/2021 01:30:40 - INFO - __main__ -   Batch number = 223
Evaluating:  79%|███████▉  | 223/282 [01:45<00:30,  1.93it/s]11/28/2021 01:30:41 - INFO - __main__ -   Batch number = 224
Evaluating:  79%|███████▉  | 224/282 [01:45<00:30,  1.91it/s]11/28/2021 01:30:41 - INFO - __main__ -   Batch number = 225
Evaluating:  80%|███████▉  | 225/282 [01:46<00:29,  1.92it/s]11/28/2021 01:30:42 - INFO - __main__ -   Batch number = 226
Evaluating:  80%|████████  | 226/282 [01:46<00:29,  1.92it/s]11/28/2021 01:30:42 - INFO - __main__ -   Batch number = 227
Evaluating:  80%|████████  | 227/282 [01:47<00:28,  1.93it/s]11/28/2021 01:30:43 - INFO - __main__ -   Batch number = 228
Evaluating:  81%|████████  | 228/282 [01:48<00:28,  1.91it/s]11/28/2021 01:30:43 - INFO - __main__ -   Batch number = 229
Evaluating:  81%|████████  | 229/282 [01:48<00:27,  1.92it/s]11/28/2021 01:30:44 - INFO - __main__ -   Batch number = 230
Evaluating:  82%|████████▏ | 230/282 [01:49<00:27,  1.91it/s]11/28/2021 01:30:44 - INFO - __main__ -   Batch number = 231
Evaluating:  82%|████████▏ | 231/282 [01:49<00:26,  1.92it/s]11/28/2021 01:30:45 - INFO - __main__ -   Batch number = 232
Evaluating:  82%|████████▏ | 232/282 [01:50<00:26,  1.91it/s]11/28/2021 01:30:45 - INFO - __main__ -   Batch number = 233
Evaluating:  83%|████████▎ | 233/282 [01:50<00:27,  1.80it/s]11/28/2021 01:30:46 - INFO - __main__ -   Batch number = 234
Evaluating:  83%|████████▎ | 234/282 [01:51<00:29,  1.65it/s]11/28/2021 01:30:47 - INFO - __main__ -   Batch number = 235
Evaluating:  83%|████████▎ | 235/282 [01:52<00:29,  1.60it/s]11/28/2021 01:30:47 - INFO - __main__ -   Batch number = 236
Evaluating:  84%|████████▎ | 236/282 [01:52<00:29,  1.54it/s]11/28/2021 01:30:48 - INFO - __main__ -   Batch number = 237
Evaluating:  84%|████████▍ | 237/282 [01:53<00:29,  1.52it/s]11/28/2021 01:30:49 - INFO - __main__ -   Batch number = 238
Evaluating:  84%|████████▍ | 238/282 [01:54<00:29,  1.50it/s]11/28/2021 01:30:49 - INFO - __main__ -   Batch number = 239
Evaluating:  85%|████████▍ | 239/282 [01:54<00:28,  1.50it/s]11/28/2021 01:30:50 - INFO - __main__ -   Batch number = 240
Evaluating:  85%|████████▌ | 240/282 [01:55<00:28,  1.48it/s]11/28/2021 01:30:51 - INFO - __main__ -   Batch number = 241
Evaluating:  85%|████████▌ | 241/282 [01:56<00:27,  1.48it/s]11/28/2021 01:30:52 - INFO - __main__ -   Batch number = 242
Evaluating:  86%|████████▌ | 242/282 [01:56<00:27,  1.47it/s]11/28/2021 01:30:52 - INFO - __main__ -   Batch number = 243
Evaluating:  86%|████████▌ | 243/282 [01:57<00:26,  1.48it/s]11/28/2021 01:30:53 - INFO - __main__ -   Batch number = 244
Evaluating:  87%|████████▋ | 244/282 [01:58<00:25,  1.46it/s]11/28/2021 01:30:54 - INFO - __main__ -   Batch number = 245
Evaluating:  87%|████████▋ | 245/282 [01:58<00:25,  1.47it/s]11/28/2021 01:30:54 - INFO - __main__ -   Batch number = 246
Evaluating:  87%|████████▋ | 246/282 [01:59<00:24,  1.46it/s]11/28/2021 01:30:55 - INFO - __main__ -   Batch number = 247
Evaluating:  88%|████████▊ | 247/282 [02:00<00:23,  1.46it/s]11/28/2021 01:30:56 - INFO - __main__ -   Batch number = 248
Evaluating:  88%|████████▊ | 248/282 [02:01<00:23,  1.47it/s]11/28/2021 01:30:56 - INFO - __main__ -   Batch number = 249
Evaluating:  88%|████████▊ | 249/282 [02:01<00:22,  1.47it/s]11/28/2021 01:30:57 - INFO - __main__ -   Batch number = 250
Evaluating:  89%|████████▊ | 250/282 [02:02<00:21,  1.47it/s]11/28/2021 01:30:58 - INFO - __main__ -   Batch number = 251
Evaluating:  89%|████████▉ | 251/282 [02:03<00:21,  1.47it/s]11/28/2021 01:30:58 - INFO - __main__ -   Batch number = 252
Evaluating:  89%|████████▉ | 252/282 [02:03<00:20,  1.47it/s]11/28/2021 01:30:59 - INFO - __main__ -   Batch number = 253
Evaluating:  90%|████████▉ | 253/282 [02:04<00:19,  1.47it/s]11/28/2021 01:31:00 - INFO - __main__ -   Batch number = 254
Evaluating:  90%|█████████ | 254/282 [02:05<00:19,  1.47it/s]11/28/2021 01:31:00 - INFO - __main__ -   Batch number = 255
Evaluating:  90%|█████████ | 255/282 [02:05<00:18,  1.49it/s]11/28/2021 01:31:01 - INFO - __main__ -   Batch number = 256
Evaluating:  91%|█████████ | 256/282 [02:06<00:16,  1.60it/s]11/28/2021 01:31:02 - INFO - __main__ -   Batch number = 257
Evaluating:  91%|█████████ | 257/282 [02:06<00:14,  1.69it/s]11/28/2021 01:31:02 - INFO - __main__ -   Batch number = 258
Evaluating:  91%|█████████▏| 258/282 [02:07<00:13,  1.75it/s]11/28/2021 01:31:03 - INFO - __main__ -   Batch number = 259
Evaluating:  92%|█████████▏| 259/282 [02:07<00:12,  1.80it/s]11/28/2021 01:31:03 - INFO - __main__ -   Batch number = 260
Evaluating:  92%|█████████▏| 260/282 [02:08<00:11,  1.83it/s]11/28/2021 01:31:04 - INFO - __main__ -   Batch number = 261
Evaluating:  93%|█████████▎| 261/282 [02:08<00:11,  1.86it/s]11/28/2021 01:31:04 - INFO - __main__ -   Batch number = 262
Evaluating:  93%|█████████▎| 262/282 [02:09<00:10,  1.87it/s]11/28/2021 01:31:05 - INFO - __main__ -   Batch number = 263
Evaluating:  93%|█████████▎| 263/282 [02:09<00:10,  1.89it/s]11/28/2021 01:31:05 - INFO - __main__ -   Batch number = 264
Evaluating:  94%|█████████▎| 264/282 [02:10<00:09,  1.89it/s]11/28/2021 01:31:06 - INFO - __main__ -   Batch number = 265
Evaluating:  94%|█████████▍| 265/282 [02:10<00:08,  1.91it/s]11/28/2021 01:31:06 - INFO - __main__ -   Batch number = 266
Evaluating:  94%|█████████▍| 266/282 [02:11<00:08,  1.90it/s]11/28/2021 01:31:07 - INFO - __main__ -   Batch number = 267
Evaluating:  95%|█████████▍| 267/282 [02:11<00:07,  1.92it/s]11/28/2021 01:31:07 - INFO - __main__ -   Batch number = 268
Evaluating:  95%|█████████▌| 268/282 [02:12<00:07,  1.92it/s]11/28/2021 01:31:08 - INFO - __main__ -   Batch number = 269
Evaluating:  95%|█████████▌| 269/282 [02:13<00:06,  1.92it/s]11/28/2021 01:31:08 - INFO - __main__ -   Batch number = 270
Evaluating:  96%|█████████▌| 270/282 [02:13<00:06,  1.90it/s]11/28/2021 01:31:09 - INFO - __main__ -   Batch number = 271
Evaluating:  96%|█████████▌| 271/282 [02:14<00:05,  1.92it/s]11/28/2021 01:31:09 - INFO - __main__ -   Batch number = 272
Evaluating:  96%|█████████▋| 272/282 [02:14<00:05,  1.91it/s]11/28/2021 01:31:10 - INFO - __main__ -   Batch number = 273
Evaluating:  97%|█████████▋| 273/282 [02:15<00:04,  1.91it/s]11/28/2021 01:31:10 - INFO - __main__ -   Batch number = 274
Evaluating:  97%|█████████▋| 274/282 [02:15<00:04,  1.90it/s]11/28/2021 01:31:11 - INFO - __main__ -   Batch number = 275
Evaluating:  98%|█████████▊| 275/282 [02:16<00:03,  1.91it/s]11/28/2021 01:31:11 - INFO - __main__ -   Batch number = 276
Evaluating:  98%|█████████▊| 276/282 [02:16<00:03,  1.89it/s]11/28/2021 01:31:12 - INFO - __main__ -   Batch number = 277
Evaluating:  98%|█████████▊| 277/282 [02:17<00:02,  1.90it/s]11/28/2021 01:31:13 - INFO - __main__ -   Batch number = 278
Evaluating:  99%|█████████▊| 278/282 [02:17<00:02,  1.89it/s]11/28/2021 01:31:13 - INFO - __main__ -   Batch number = 279
Evaluating:  99%|█████████▉| 279/282 [02:18<00:01,  1.91it/s]11/28/2021 01:31:14 - INFO - __main__ -   Batch number = 280
Evaluating:  99%|█████████▉| 280/282 [02:18<00:01,  1.93it/s]11/28/2021 01:31:14 - INFO - __main__ -   Batch number = 281
Evaluating: 100%|█████████▉| 281/282 [02:19<00:00,  1.93it/s]11/28/2021 01:31:15 - INFO - __main__ -   Batch number = 282
Evaluating: 100%|██████████| 282/282 [02:19<00:00,  2.13it/s]Evaluating: 100%|██████████| 282/282 [02:19<00:00,  2.02it/s]
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:31:19 - INFO - __main__ -   ***** Evaluation result  in ru *****
11/28/2021 01:31:19 - INFO - __main__ -     f1 = 0.8644137298888004
11/28/2021 01:31:19 - INFO - __main__ -     loss = 0.51072252145473
11/28/2021 01:31:19 - INFO - __main__ -     precision = 0.8683906255409007
11/28/2021 01:31:19 - INFO - __main__ -     recall = 0.8604730934935924
123.16user 42.99system 2:48.72elapsed 98%CPU (0avgtext+0avgdata 3987252maxresident)k
0inputs+1816outputs (0major+1858980minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:31:23 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ru', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:31:23 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:31:23 - INFO - __main__ -   Seed = 2
11/28/2021 01:31:23 - INFO - root -   save model
11/28/2021 01:31:23 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ru', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:31:23 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:31:26 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:31:32 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:31:32 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:31:32 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
11/28/2021 01:31:32 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:31:32 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:31:32 - INFO - root -   loading lang adpater cs/wiki@ukp
11/28/2021 01:31:32 - INFO - __main__ -   Adapter Languages : ['cs'], Length : 1
11/28/2021 01:31:32 - INFO - __main__ -   Adapter Names ['cs/wiki@ukp'], Length : 1
11/28/2021 01:31:32 - INFO - __main__ -   Language = cs
11/28/2021 01:31:32 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/cs/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_cs_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/adapter_config.json
Adding adapter 'cs' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/head_config.json
11/28/2021 01:31:43 - INFO - __main__ -   Language adapter for ru not found, using cs instead
11/28/2021 01:31:43 - INFO - __main__ -   Set active language adapter to cs
11/28/2021 01:31:43 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:31:43 - INFO - __main__ -   Adapter Languages = ['cs']
11/28/2021 01:31:43 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_ru_bert-base-multilingual-cased_128
11/28/2021 01:31:44 - INFO - __main__ -   ***** Running evaluation  in ru *****
11/28/2021 01:31:44 - INFO - __main__ -     Num examples = 8995
11/28/2021 01:31:44 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/282 [00:00<?, ?it/s]11/28/2021 01:31:44 - INFO - __main__ -   Batch number = 1
Evaluating:   0%|          | 1/282 [00:00<02:18,  2.03it/s]11/28/2021 01:31:44 - INFO - __main__ -   Batch number = 2
Evaluating:   1%|          | 2/282 [00:00<02:00,  2.32it/s]11/28/2021 01:31:45 - INFO - __main__ -   Batch number = 3
Evaluating:   1%|          | 3/282 [00:01<01:48,  2.57it/s]11/28/2021 01:31:45 - INFO - __main__ -   Batch number = 4
Evaluating:   1%|▏         | 4/282 [00:01<01:41,  2.73it/s]11/28/2021 01:31:45 - INFO - __main__ -   Batch number = 5
Evaluating:   2%|▏         | 5/282 [00:01<01:47,  2.59it/s]11/28/2021 01:31:46 - INFO - __main__ -   Batch number = 6
Evaluating:   2%|▏         | 6/282 [00:02<01:57,  2.36it/s]11/28/2021 01:31:46 - INFO - __main__ -   Batch number = 7
Evaluating:   2%|▏         | 7/282 [00:02<02:03,  2.23it/s]11/28/2021 01:31:47 - INFO - __main__ -   Batch number = 8
Evaluating:   3%|▎         | 8/282 [00:03<02:07,  2.15it/s]11/28/2021 01:31:47 - INFO - __main__ -   Batch number = 9
Evaluating:   3%|▎         | 9/282 [00:03<02:09,  2.11it/s]11/28/2021 01:31:48 - INFO - __main__ -   Batch number = 10
Evaluating:   4%|▎         | 10/282 [00:04<02:10,  2.08it/s]11/28/2021 01:31:48 - INFO - __main__ -   Batch number = 11
Evaluating:   4%|▍         | 11/282 [00:04<02:11,  2.06it/s]11/28/2021 01:31:49 - INFO - __main__ -   Batch number = 12
Evaluating:   4%|▍         | 12/282 [00:05<02:12,  2.03it/s]11/28/2021 01:31:50 - INFO - __main__ -   Batch number = 13
Evaluating:   5%|▍         | 13/282 [00:06<02:28,  1.81it/s]11/28/2021 01:31:50 - INFO - __main__ -   Batch number = 14
Evaluating:   5%|▍         | 14/282 [00:06<02:22,  1.87it/s]11/28/2021 01:31:51 - INFO - __main__ -   Batch number = 15
Evaluating:   5%|▌         | 15/282 [00:07<02:19,  1.91it/s]11/28/2021 01:31:51 - INFO - __main__ -   Batch number = 16
Evaluating:   6%|▌         | 16/282 [00:07<02:16,  1.94it/s]11/28/2021 01:31:52 - INFO - __main__ -   Batch number = 17
Evaluating:   6%|▌         | 17/282 [00:08<02:15,  1.96it/s]11/28/2021 01:31:52 - INFO - __main__ -   Batch number = 18
Evaluating:   6%|▋         | 18/282 [00:08<02:14,  1.96it/s]11/28/2021 01:31:53 - INFO - __main__ -   Batch number = 19
Evaluating:   7%|▋         | 19/282 [00:09<02:13,  1.97it/s]11/28/2021 01:31:53 - INFO - __main__ -   Batch number = 20
Evaluating:   7%|▋         | 20/282 [00:09<02:12,  1.98it/s]11/28/2021 01:31:54 - INFO - __main__ -   Batch number = 21
Evaluating:   7%|▋         | 21/282 [00:10<02:11,  1.98it/s]11/28/2021 01:31:54 - INFO - __main__ -   Batch number = 22
Evaluating:   8%|▊         | 22/282 [00:10<02:11,  1.98it/s]11/28/2021 01:31:55 - INFO - __main__ -   Batch number = 23
Evaluating:   8%|▊         | 23/282 [00:11<02:10,  1.98it/s]11/28/2021 01:31:55 - INFO - __main__ -   Batch number = 24
Evaluating:   9%|▊         | 24/282 [00:11<02:10,  1.98it/s]11/28/2021 01:31:56 - INFO - __main__ -   Batch number = 25
Evaluating:   9%|▉         | 25/282 [00:12<02:10,  1.98it/s]11/28/2021 01:31:56 - INFO - __main__ -   Batch number = 26
Evaluating:   9%|▉         | 26/282 [00:12<02:09,  1.98it/s]11/28/2021 01:31:57 - INFO - __main__ -   Batch number = 27
Evaluating:  10%|▉         | 27/282 [00:13<02:08,  1.98it/s]11/28/2021 01:31:57 - INFO - __main__ -   Batch number = 28
Evaluating:  10%|▉         | 28/282 [00:13<02:08,  1.98it/s]11/28/2021 01:31:58 - INFO - __main__ -   Batch number = 29
Evaluating:  10%|█         | 29/282 [00:14<02:07,  1.98it/s]11/28/2021 01:31:58 - INFO - __main__ -   Batch number = 30
Evaluating:  11%|█         | 30/282 [00:14<02:07,  1.98it/s]11/28/2021 01:31:59 - INFO - __main__ -   Batch number = 31
Evaluating:  11%|█         | 31/282 [00:15<02:06,  1.98it/s]11/28/2021 01:31:59 - INFO - __main__ -   Batch number = 32
Evaluating:  11%|█▏        | 32/282 [00:15<02:06,  1.98it/s]11/28/2021 01:32:00 - INFO - __main__ -   Batch number = 33
Evaluating:  12%|█▏        | 33/282 [00:16<02:05,  1.98it/s]11/28/2021 01:32:00 - INFO - __main__ -   Batch number = 34
Evaluating:  12%|█▏        | 34/282 [00:16<02:05,  1.97it/s]11/28/2021 01:32:01 - INFO - __main__ -   Batch number = 35
Evaluating:  12%|█▏        | 35/282 [00:17<02:05,  1.98it/s]11/28/2021 01:32:01 - INFO - __main__ -   Batch number = 36
Evaluating:  13%|█▎        | 36/282 [00:17<02:04,  1.98it/s]11/28/2021 01:32:02 - INFO - __main__ -   Batch number = 37
Evaluating:  13%|█▎        | 37/282 [00:18<02:04,  1.98it/s]11/28/2021 01:32:02 - INFO - __main__ -   Batch number = 38
Evaluating:  13%|█▎        | 38/282 [00:18<02:03,  1.98it/s]11/28/2021 01:32:03 - INFO - __main__ -   Batch number = 39
Evaluating:  14%|█▍        | 39/282 [00:19<02:02,  1.98it/s]11/28/2021 01:32:03 - INFO - __main__ -   Batch number = 40
Evaluating:  14%|█▍        | 40/282 [00:19<02:02,  1.98it/s]11/28/2021 01:32:04 - INFO - __main__ -   Batch number = 41
Evaluating:  15%|█▍        | 41/282 [00:20<02:02,  1.98it/s]11/28/2021 01:32:04 - INFO - __main__ -   Batch number = 42
Evaluating:  15%|█▍        | 42/282 [00:20<02:01,  1.98it/s]11/28/2021 01:32:05 - INFO - __main__ -   Batch number = 43
Evaluating:  15%|█▌        | 43/282 [00:21<02:00,  1.98it/s]11/28/2021 01:32:05 - INFO - __main__ -   Batch number = 44
Evaluating:  16%|█▌        | 44/282 [00:21<01:59,  1.98it/s]11/28/2021 01:32:06 - INFO - __main__ -   Batch number = 45
Evaluating:  16%|█▌        | 45/282 [00:22<01:58,  1.99it/s]11/28/2021 01:32:06 - INFO - __main__ -   Batch number = 46
Evaluating:  16%|█▋        | 46/282 [00:22<01:57,  2.00it/s]11/28/2021 01:32:07 - INFO - __main__ -   Batch number = 47
Evaluating:  17%|█▋        | 47/282 [00:23<01:57,  2.00it/s]11/28/2021 01:32:07 - INFO - __main__ -   Batch number = 48
Evaluating:  17%|█▋        | 48/282 [00:23<01:56,  2.00it/s]11/28/2021 01:32:08 - INFO - __main__ -   Batch number = 49
Evaluating:  17%|█▋        | 49/282 [00:24<01:57,  1.99it/s]11/28/2021 01:32:08 - INFO - __main__ -   Batch number = 50
Evaluating:  18%|█▊        | 50/282 [00:24<01:56,  1.98it/s]11/28/2021 01:32:09 - INFO - __main__ -   Batch number = 51
Evaluating:  18%|█▊        | 51/282 [00:25<01:56,  1.99it/s]11/28/2021 01:32:09 - INFO - __main__ -   Batch number = 52
Evaluating:  18%|█▊        | 52/282 [00:25<01:55,  1.99it/s]11/28/2021 01:32:10 - INFO - __main__ -   Batch number = 53
Evaluating:  19%|█▉        | 53/282 [00:26<01:54,  1.99it/s]11/28/2021 01:32:10 - INFO - __main__ -   Batch number = 54
Evaluating:  19%|█▉        | 54/282 [00:26<01:55,  1.97it/s]11/28/2021 01:32:11 - INFO - __main__ -   Batch number = 55
Evaluating:  20%|█▉        | 55/282 [00:27<01:54,  1.98it/s]11/28/2021 01:32:11 - INFO - __main__ -   Batch number = 56
Evaluating:  20%|█▉        | 56/282 [00:27<01:53,  1.99it/s]11/28/2021 01:32:12 - INFO - __main__ -   Batch number = 57
Evaluating:  20%|██        | 57/282 [00:28<01:59,  1.89it/s]11/28/2021 01:32:12 - INFO - __main__ -   Batch number = 58
Evaluating:  21%|██        | 58/282 [00:29<02:08,  1.75it/s]11/28/2021 01:32:13 - INFO - __main__ -   Batch number = 59
Evaluating:  21%|██        | 59/282 [00:29<02:13,  1.67it/s]11/28/2021 01:32:14 - INFO - __main__ -   Batch number = 60
Evaluating:  21%|██▏       | 60/282 [00:30<02:17,  1.61it/s]11/28/2021 01:32:14 - INFO - __main__ -   Batch number = 61
Evaluating:  22%|██▏       | 61/282 [00:31<02:25,  1.52it/s]11/28/2021 01:32:15 - INFO - __main__ -   Batch number = 62
Evaluating:  22%|██▏       | 62/282 [00:31<02:25,  1.51it/s]11/28/2021 01:32:16 - INFO - __main__ -   Batch number = 63
Evaluating:  22%|██▏       | 63/282 [00:32<02:25,  1.50it/s]11/28/2021 01:32:16 - INFO - __main__ -   Batch number = 64
Evaluating:  23%|██▎       | 64/282 [00:33<02:24,  1.51it/s]11/28/2021 01:32:17 - INFO - __main__ -   Batch number = 65
Evaluating:  23%|██▎       | 65/282 [00:33<02:24,  1.50it/s]11/28/2021 01:32:18 - INFO - __main__ -   Batch number = 66
Evaluating:  23%|██▎       | 66/282 [00:34<02:23,  1.50it/s]11/28/2021 01:32:18 - INFO - __main__ -   Batch number = 67
Evaluating:  24%|██▍       | 67/282 [00:35<02:23,  1.50it/s]11/28/2021 01:32:19 - INFO - __main__ -   Batch number = 68
Evaluating:  24%|██▍       | 68/282 [00:35<02:22,  1.50it/s]11/28/2021 01:32:20 - INFO - __main__ -   Batch number = 69
Evaluating:  24%|██▍       | 69/282 [00:36<02:21,  1.50it/s]11/28/2021 01:32:20 - INFO - __main__ -   Batch number = 70
Evaluating:  25%|██▍       | 70/282 [00:37<02:21,  1.50it/s]11/28/2021 01:32:21 - INFO - __main__ -   Batch number = 71
Evaluating:  25%|██▌       | 71/282 [00:37<02:20,  1.50it/s]11/28/2021 01:32:22 - INFO - __main__ -   Batch number = 72
Evaluating:  26%|██▌       | 72/282 [00:38<02:20,  1.49it/s]11/28/2021 01:32:22 - INFO - __main__ -   Batch number = 73
Evaluating:  26%|██▌       | 73/282 [00:39<02:19,  1.49it/s]11/28/2021 01:32:23 - INFO - __main__ -   Batch number = 74
Evaluating:  26%|██▌       | 74/282 [00:39<02:19,  1.49it/s]11/28/2021 01:32:24 - INFO - __main__ -   Batch number = 75
Evaluating:  27%|██▋       | 75/282 [00:40<02:18,  1.49it/s]11/28/2021 01:32:24 - INFO - __main__ -   Batch number = 76
Evaluating:  27%|██▋       | 76/282 [00:41<02:18,  1.49it/s]11/28/2021 01:32:25 - INFO - __main__ -   Batch number = 77
Evaluating:  27%|██▋       | 77/282 [00:41<02:17,  1.49it/s]11/28/2021 01:32:26 - INFO - __main__ -   Batch number = 78
Evaluating:  28%|██▊       | 78/282 [00:42<02:17,  1.48it/s]11/28/2021 01:32:26 - INFO - __main__ -   Batch number = 79
Evaluating:  28%|██▊       | 79/282 [00:43<02:16,  1.49it/s]11/28/2021 01:32:27 - INFO - __main__ -   Batch number = 80
Evaluating:  28%|██▊       | 80/282 [00:43<02:16,  1.48it/s]11/28/2021 01:32:28 - INFO - __main__ -   Batch number = 81
Evaluating:  29%|██▊       | 81/282 [00:44<02:15,  1.48it/s]11/28/2021 01:32:28 - INFO - __main__ -   Batch number = 82
Evaluating:  29%|██▉       | 82/282 [00:45<02:14,  1.48it/s]11/28/2021 01:32:29 - INFO - __main__ -   Batch number = 83
Evaluating:  29%|██▉       | 83/282 [00:45<02:12,  1.51it/s]11/28/2021 01:32:30 - INFO - __main__ -   Batch number = 84
Evaluating:  30%|██▉       | 84/282 [00:46<02:11,  1.50it/s]11/28/2021 01:32:30 - INFO - __main__ -   Batch number = 85
Evaluating:  30%|███       | 85/282 [00:47<02:11,  1.50it/s]11/28/2021 01:32:31 - INFO - __main__ -   Batch number = 86
Evaluating:  30%|███       | 86/282 [00:47<02:10,  1.50it/s]11/28/2021 01:32:32 - INFO - __main__ -   Batch number = 87
Evaluating:  31%|███       | 87/282 [00:48<02:09,  1.51it/s]11/28/2021 01:32:32 - INFO - __main__ -   Batch number = 88
Evaluating:  31%|███       | 88/282 [00:49<01:59,  1.63it/s]11/28/2021 01:32:33 - INFO - __main__ -   Batch number = 89
Evaluating:  32%|███▏      | 89/282 [00:49<01:51,  1.72it/s]11/28/2021 01:32:33 - INFO - __main__ -   Batch number = 90
Evaluating:  32%|███▏      | 90/282 [00:50<01:46,  1.80it/s]11/28/2021 01:32:34 - INFO - __main__ -   Batch number = 91
Evaluating:  32%|███▏      | 91/282 [00:50<01:42,  1.86it/s]11/28/2021 01:32:34 - INFO - __main__ -   Batch number = 92
Evaluating:  33%|███▎      | 92/282 [00:51<01:39,  1.91it/s]11/28/2021 01:32:35 - INFO - __main__ -   Batch number = 93
Evaluating:  33%|███▎      | 93/282 [00:51<01:37,  1.94it/s]11/28/2021 01:32:35 - INFO - __main__ -   Batch number = 94
Evaluating:  33%|███▎      | 94/282 [00:52<01:35,  1.97it/s]11/28/2021 01:32:36 - INFO - __main__ -   Batch number = 95
Evaluating:  34%|███▎      | 95/282 [00:52<01:34,  1.98it/s]11/28/2021 01:32:36 - INFO - __main__ -   Batch number = 96
Evaluating:  34%|███▍      | 96/282 [00:53<01:33,  1.99it/s]11/28/2021 01:32:37 - INFO - __main__ -   Batch number = 97
Evaluating:  34%|███▍      | 97/282 [00:53<01:30,  2.04it/s]11/28/2021 01:32:37 - INFO - __main__ -   Batch number = 98
Evaluating:  35%|███▍      | 98/282 [00:53<01:21,  2.25it/s]11/28/2021 01:32:38 - INFO - __main__ -   Batch number = 99
Evaluating:  35%|███▌      | 99/282 [00:54<01:15,  2.43it/s]11/28/2021 01:32:38 - INFO - __main__ -   Batch number = 100
Evaluating:  35%|███▌      | 100/282 [00:54<01:10,  2.58it/s]11/28/2021 01:32:38 - INFO - __main__ -   Batch number = 101
Evaluating:  36%|███▌      | 101/282 [00:54<01:07,  2.69it/s]11/28/2021 01:32:39 - INFO - __main__ -   Batch number = 102
Evaluating:  36%|███▌      | 102/282 [00:55<01:04,  2.77it/s]11/28/2021 01:32:39 - INFO - __main__ -   Batch number = 103
Evaluating:  37%|███▋      | 103/282 [00:55<01:03,  2.82it/s]11/28/2021 01:32:39 - INFO - __main__ -   Batch number = 104
Evaluating:  37%|███▋      | 104/282 [00:55<01:02,  2.83it/s]11/28/2021 01:32:40 - INFO - __main__ -   Batch number = 105
Evaluating:  37%|███▋      | 105/282 [00:56<01:01,  2.87it/s]11/28/2021 01:32:40 - INFO - __main__ -   Batch number = 106
Evaluating:  38%|███▊      | 106/282 [00:56<01:00,  2.91it/s]11/28/2021 01:32:40 - INFO - __main__ -   Batch number = 107
Evaluating:  38%|███▊      | 107/282 [00:56<00:59,  2.93it/s]11/28/2021 01:32:41 - INFO - __main__ -   Batch number = 108
Evaluating:  38%|███▊      | 108/282 [00:57<00:57,  3.01it/s]11/28/2021 01:32:41 - INFO - __main__ -   Batch number = 109
Evaluating:  39%|███▊      | 109/282 [00:57<00:57,  3.01it/s]11/28/2021 01:32:41 - INFO - __main__ -   Batch number = 110
Evaluating:  39%|███▉      | 110/282 [00:57<00:57,  3.00it/s]11/28/2021 01:32:42 - INFO - __main__ -   Batch number = 111
Evaluating:  39%|███▉      | 111/282 [00:58<00:57,  2.99it/s]11/28/2021 01:32:42 - INFO - __main__ -   Batch number = 112
Evaluating:  40%|███▉      | 112/282 [00:58<00:57,  2.96it/s]11/28/2021 01:32:42 - INFO - __main__ -   Batch number = 113
Evaluating:  40%|████      | 113/282 [00:58<00:57,  2.96it/s]11/28/2021 01:32:43 - INFO - __main__ -   Batch number = 114
Evaluating:  40%|████      | 114/282 [00:59<00:56,  2.96it/s]11/28/2021 01:32:43 - INFO - __main__ -   Batch number = 115
Evaluating:  41%|████      | 115/282 [00:59<00:48,  3.43it/s]11/28/2021 01:32:43 - INFO - __main__ -   Batch number = 116
Evaluating:  41%|████      | 116/282 [00:59<00:49,  3.35it/s]11/28/2021 01:32:44 - INFO - __main__ -   Batch number = 117
Evaluating:  41%|████▏     | 117/282 [00:59<00:50,  3.28it/s]11/28/2021 01:32:44 - INFO - __main__ -   Batch number = 118
Evaluating:  42%|████▏     | 118/282 [01:00<00:51,  3.19it/s]11/28/2021 01:32:44 - INFO - __main__ -   Batch number = 119
Evaluating:  42%|████▏     | 119/282 [01:00<00:52,  3.13it/s]11/28/2021 01:32:45 - INFO - __main__ -   Batch number = 120
Evaluating:  43%|████▎     | 120/282 [01:00<00:53,  3.06it/s]11/28/2021 01:32:45 - INFO - __main__ -   Batch number = 121
Evaluating:  43%|████▎     | 121/282 [01:01<00:52,  3.05it/s]11/28/2021 01:32:45 - INFO - __main__ -   Batch number = 122
Evaluating:  43%|████▎     | 122/282 [01:01<00:52,  3.04it/s]11/28/2021 01:32:46 - INFO - __main__ -   Batch number = 123
Evaluating:  44%|████▎     | 123/282 [01:02<00:52,  3.01it/s]11/28/2021 01:32:46 - INFO - __main__ -   Batch number = 124
Evaluating:  44%|████▍     | 124/282 [01:02<00:52,  2.99it/s]11/28/2021 01:32:46 - INFO - __main__ -   Batch number = 125
Evaluating:  44%|████▍     | 125/282 [01:02<00:52,  2.98it/s]11/28/2021 01:32:47 - INFO - __main__ -   Batch number = 126
Evaluating:  45%|████▍     | 126/282 [01:03<00:52,  2.97it/s]11/28/2021 01:32:47 - INFO - __main__ -   Batch number = 127
Evaluating:  45%|████▌     | 127/282 [01:03<00:52,  2.98it/s]11/28/2021 01:32:47 - INFO - __main__ -   Batch number = 128
Evaluating:  45%|████▌     | 128/282 [01:03<00:51,  2.98it/s]11/28/2021 01:32:48 - INFO - __main__ -   Batch number = 129
Evaluating:  46%|████▌     | 129/282 [01:04<00:51,  2.95it/s]11/28/2021 01:32:48 - INFO - __main__ -   Batch number = 130
Evaluating:  46%|████▌     | 130/282 [01:04<00:51,  2.93it/s]11/28/2021 01:32:48 - INFO - __main__ -   Batch number = 131
Evaluating:  46%|████▋     | 131/282 [01:04<00:51,  2.93it/s]11/28/2021 01:32:49 - INFO - __main__ -   Batch number = 132
Evaluating:  47%|████▋     | 132/282 [01:05<00:51,  2.92it/s]11/28/2021 01:32:49 - INFO - __main__ -   Batch number = 133
Evaluating:  47%|████▋     | 133/282 [01:05<00:51,  2.91it/s]11/28/2021 01:32:49 - INFO - __main__ -   Batch number = 134
Evaluating:  48%|████▊     | 134/282 [01:05<00:50,  2.92it/s]11/28/2021 01:32:50 - INFO - __main__ -   Batch number = 135
Evaluating:  48%|████▊     | 135/282 [01:06<00:50,  2.93it/s]11/28/2021 01:32:50 - INFO - __main__ -   Batch number = 136
Evaluating:  48%|████▊     | 136/282 [01:06<00:49,  2.92it/s]11/28/2021 01:32:50 - INFO - __main__ -   Batch number = 137
Evaluating:  49%|████▊     | 137/282 [01:06<00:49,  2.95it/s]11/28/2021 01:32:51 - INFO - __main__ -   Batch number = 138
Evaluating:  49%|████▉     | 138/282 [01:07<00:48,  2.96it/s]11/28/2021 01:32:51 - INFO - __main__ -   Batch number = 139
Evaluating:  49%|████▉     | 139/282 [01:07<00:48,  2.97it/s]11/28/2021 01:32:51 - INFO - __main__ -   Batch number = 140
Evaluating:  50%|████▉     | 140/282 [01:07<00:47,  2.97it/s]11/28/2021 01:32:52 - INFO - __main__ -   Batch number = 141
Evaluating:  50%|█████     | 141/282 [01:08<00:47,  2.96it/s]11/28/2021 01:32:52 - INFO - __main__ -   Batch number = 142
Evaluating:  50%|█████     | 142/282 [01:08<00:47,  2.93it/s]11/28/2021 01:32:52 - INFO - __main__ -   Batch number = 143
Evaluating:  51%|█████     | 143/282 [01:08<00:47,  2.94it/s]11/28/2021 01:32:53 - INFO - __main__ -   Batch number = 144
Evaluating:  51%|█████     | 144/282 [01:09<00:46,  2.94it/s]11/28/2021 01:32:53 - INFO - __main__ -   Batch number = 145
Evaluating:  51%|█████▏    | 145/282 [01:09<00:47,  2.91it/s]11/28/2021 01:32:53 - INFO - __main__ -   Batch number = 146
Evaluating:  52%|█████▏    | 146/282 [01:09<00:46,  2.91it/s]11/28/2021 01:32:54 - INFO - __main__ -   Batch number = 147
Evaluating:  52%|█████▏    | 147/282 [01:10<00:46,  2.91it/s]11/28/2021 01:32:54 - INFO - __main__ -   Batch number = 148
Evaluating:  52%|█████▏    | 148/282 [01:10<00:46,  2.90it/s]11/28/2021 01:32:54 - INFO - __main__ -   Batch number = 149
Evaluating:  53%|█████▎    | 149/282 [01:10<00:45,  2.90it/s]11/28/2021 01:32:55 - INFO - __main__ -   Batch number = 150
Evaluating:  53%|█████▎    | 150/282 [01:11<00:45,  2.91it/s]11/28/2021 01:32:55 - INFO - __main__ -   Batch number = 151
Evaluating:  54%|█████▎    | 151/282 [01:11<00:45,  2.89it/s]11/28/2021 01:32:55 - INFO - __main__ -   Batch number = 152
Evaluating:  54%|█████▍    | 152/282 [01:11<00:45,  2.89it/s]11/28/2021 01:32:56 - INFO - __main__ -   Batch number = 153
Evaluating:  54%|█████▍    | 153/282 [01:12<00:44,  2.91it/s]11/28/2021 01:32:56 - INFO - __main__ -   Batch number = 154
Evaluating:  55%|█████▍    | 154/282 [01:12<00:44,  2.88it/s]11/28/2021 01:32:57 - INFO - __main__ -   Batch number = 155
Evaluating:  55%|█████▍    | 155/282 [01:12<00:43,  2.89it/s]11/28/2021 01:32:57 - INFO - __main__ -   Batch number = 156
Evaluating:  55%|█████▌    | 156/282 [01:13<00:42,  2.96it/s]11/28/2021 01:32:57 - INFO - __main__ -   Batch number = 157
Evaluating:  56%|█████▌    | 157/282 [01:13<00:41,  3.01it/s]11/28/2021 01:32:58 - INFO - __main__ -   Batch number = 158
Evaluating:  56%|█████▌    | 158/282 [01:13<00:43,  2.87it/s]11/28/2021 01:32:58 - INFO - __main__ -   Batch number = 159
Evaluating:  56%|█████▋    | 159/282 [01:14<00:48,  2.54it/s]11/28/2021 01:32:58 - INFO - __main__ -   Batch number = 160
Evaluating:  57%|█████▋    | 160/282 [01:14<00:52,  2.33it/s]11/28/2021 01:32:59 - INFO - __main__ -   Batch number = 161
Evaluating:  57%|█████▋    | 161/282 [01:15<00:52,  2.31it/s]11/28/2021 01:32:59 - INFO - __main__ -   Batch number = 162
Evaluating:  57%|█████▋    | 162/282 [01:15<00:55,  2.16it/s]11/28/2021 01:33:00 - INFO - __main__ -   Batch number = 163
Evaluating:  58%|█████▊    | 163/282 [01:16<00:52,  2.25it/s]11/28/2021 01:33:00 - INFO - __main__ -   Batch number = 164
Evaluating:  58%|█████▊    | 164/282 [01:16<00:50,  2.31it/s]11/28/2021 01:33:01 - INFO - __main__ -   Batch number = 165
Evaluating:  59%|█████▊    | 165/282 [01:17<00:47,  2.45it/s]11/28/2021 01:33:01 - INFO - __main__ -   Batch number = 166
Evaluating:  59%|█████▉    | 166/282 [01:17<00:45,  2.55it/s]11/28/2021 01:33:01 - INFO - __main__ -   Batch number = 167
Evaluating:  59%|█████▉    | 167/282 [01:17<00:43,  2.65it/s]11/28/2021 01:33:02 - INFO - __main__ -   Batch number = 168
Evaluating:  60%|█████▉    | 168/282 [01:18<00:42,  2.68it/s]11/28/2021 01:33:02 - INFO - __main__ -   Batch number = 169
Evaluating:  60%|█████▉    | 169/282 [01:18<00:41,  2.74it/s]11/28/2021 01:33:02 - INFO - __main__ -   Batch number = 170
Evaluating:  60%|██████    | 170/282 [01:18<00:40,  2.76it/s]11/28/2021 01:33:03 - INFO - __main__ -   Batch number = 171
Evaluating:  61%|██████    | 171/282 [01:19<00:39,  2.79it/s]11/28/2021 01:33:03 - INFO - __main__ -   Batch number = 172
Evaluating:  61%|██████    | 172/282 [01:19<00:40,  2.75it/s]11/28/2021 01:33:04 - INFO - __main__ -   Batch number = 173
Evaluating:  61%|██████▏   | 173/282 [01:20<00:44,  2.44it/s]11/28/2021 01:33:04 - INFO - __main__ -   Batch number = 174
Evaluating:  62%|██████▏   | 174/282 [01:20<00:47,  2.29it/s]11/28/2021 01:33:05 - INFO - __main__ -   Batch number = 175
Evaluating:  62%|██████▏   | 175/282 [01:21<00:49,  2.18it/s]11/28/2021 01:33:05 - INFO - __main__ -   Batch number = 176
Evaluating:  62%|██████▏   | 176/282 [01:21<00:50,  2.09it/s]11/28/2021 01:33:06 - INFO - __main__ -   Batch number = 177
Evaluating:  63%|██████▎   | 177/282 [01:22<00:51,  2.05it/s]11/28/2021 01:33:06 - INFO - __main__ -   Batch number = 178
Evaluating:  63%|██████▎   | 178/282 [01:22<00:51,  2.01it/s]11/28/2021 01:33:07 - INFO - __main__ -   Batch number = 179
Evaluating:  63%|██████▎   | 179/282 [01:23<00:51,  2.00it/s]11/28/2021 01:33:07 - INFO - __main__ -   Batch number = 180
Evaluating:  64%|██████▍   | 180/282 [01:23<00:51,  1.98it/s]11/28/2021 01:33:08 - INFO - __main__ -   Batch number = 181
Evaluating:  64%|██████▍   | 181/282 [01:24<00:51,  1.97it/s]11/28/2021 01:33:08 - INFO - __main__ -   Batch number = 182
Evaluating:  65%|██████▍   | 182/282 [01:24<00:51,  1.95it/s]11/28/2021 01:33:09 - INFO - __main__ -   Batch number = 183
Evaluating:  65%|██████▍   | 183/282 [01:25<00:50,  1.95it/s]11/28/2021 01:33:09 - INFO - __main__ -   Batch number = 184
Evaluating:  65%|██████▌   | 184/282 [01:25<00:50,  1.93it/s]11/28/2021 01:33:10 - INFO - __main__ -   Batch number = 185
Evaluating:  66%|██████▌   | 185/282 [01:26<00:50,  1.93it/s]11/28/2021 01:33:10 - INFO - __main__ -   Batch number = 186
Evaluating:  66%|██████▌   | 186/282 [01:26<00:49,  1.92it/s]11/28/2021 01:33:11 - INFO - __main__ -   Batch number = 187
Evaluating:  66%|██████▋   | 187/282 [01:27<00:49,  1.93it/s]11/28/2021 01:33:11 - INFO - __main__ -   Batch number = 188
Evaluating:  67%|██████▋   | 188/282 [01:27<00:48,  1.93it/s]11/28/2021 01:33:12 - INFO - __main__ -   Batch number = 189
Evaluating:  67%|██████▋   | 189/282 [01:28<00:48,  1.93it/s]11/28/2021 01:33:12 - INFO - __main__ -   Batch number = 190
Evaluating:  67%|██████▋   | 190/282 [01:28<00:47,  1.92it/s]11/28/2021 01:33:13 - INFO - __main__ -   Batch number = 191
Evaluating:  68%|██████▊   | 191/282 [01:29<00:47,  1.92it/s]11/28/2021 01:33:13 - INFO - __main__ -   Batch number = 192
Evaluating:  68%|██████▊   | 192/282 [01:29<00:47,  1.91it/s]11/28/2021 01:33:14 - INFO - __main__ -   Batch number = 193
Evaluating:  68%|██████▊   | 193/282 [01:30<00:46,  1.92it/s]11/28/2021 01:33:15 - INFO - __main__ -   Batch number = 194
Evaluating:  69%|██████▉   | 194/282 [01:31<00:49,  1.78it/s]11/28/2021 01:33:15 - INFO - __main__ -   Batch number = 195
Evaluating:  69%|██████▉   | 195/282 [01:31<00:47,  1.83it/s]11/28/2021 01:33:16 - INFO - __main__ -   Batch number = 196
Evaluating:  70%|██████▉   | 196/282 [01:32<00:46,  1.84it/s]11/28/2021 01:33:16 - INFO - __main__ -   Batch number = 197
Evaluating:  70%|██████▉   | 197/282 [01:32<00:45,  1.87it/s]11/28/2021 01:33:17 - INFO - __main__ -   Batch number = 198
Evaluating:  70%|███████   | 198/282 [01:33<00:44,  1.88it/s]11/28/2021 01:33:17 - INFO - __main__ -   Batch number = 199
Evaluating:  71%|███████   | 199/282 [01:33<00:43,  1.90it/s]11/28/2021 01:33:18 - INFO - __main__ -   Batch number = 200
Evaluating:  71%|███████   | 200/282 [01:34<00:43,  1.90it/s]11/28/2021 01:33:18 - INFO - __main__ -   Batch number = 201
Evaluating:  71%|███████▏  | 201/282 [01:34<00:42,  1.91it/s]11/28/2021 01:33:19 - INFO - __main__ -   Batch number = 202
Evaluating:  72%|███████▏  | 202/282 [01:35<00:41,  1.92it/s]11/28/2021 01:33:19 - INFO - __main__ -   Batch number = 203
Evaluating:  72%|███████▏  | 203/282 [01:35<00:36,  2.18it/s]11/28/2021 01:33:20 - INFO - __main__ -   Batch number = 204
Evaluating:  72%|███████▏  | 204/282 [01:36<00:36,  2.12it/s]11/28/2021 01:33:20 - INFO - __main__ -   Batch number = 205
Evaluating:  73%|███████▎  | 205/282 [01:36<00:37,  2.06it/s]11/28/2021 01:33:21 - INFO - __main__ -   Batch number = 206
Evaluating:  73%|███████▎  | 206/282 [01:37<00:37,  2.01it/s]11/28/2021 01:33:21 - INFO - __main__ -   Batch number = 207
Evaluating:  73%|███████▎  | 207/282 [01:37<00:37,  1.98it/s]11/28/2021 01:33:22 - INFO - __main__ -   Batch number = 208
Evaluating:  74%|███████▍  | 208/282 [01:38<00:37,  1.96it/s]11/28/2021 01:33:22 - INFO - __main__ -   Batch number = 209
Evaluating:  74%|███████▍  | 209/282 [01:38<00:37,  1.96it/s]11/28/2021 01:33:23 - INFO - __main__ -   Batch number = 210
Evaluating:  74%|███████▍  | 210/282 [01:39<00:37,  1.94it/s]11/28/2021 01:33:23 - INFO - __main__ -   Batch number = 211
Evaluating:  75%|███████▍  | 211/282 [01:39<00:36,  1.95it/s]11/28/2021 01:33:24 - INFO - __main__ -   Batch number = 212
Evaluating:  75%|███████▌  | 212/282 [01:40<00:36,  1.94it/s]11/28/2021 01:33:24 - INFO - __main__ -   Batch number = 213
Evaluating:  76%|███████▌  | 213/282 [01:40<00:31,  2.16it/s]11/28/2021 01:33:25 - INFO - __main__ -   Batch number = 214
Evaluating:  76%|███████▌  | 214/282 [01:41<00:32,  2.09it/s]11/28/2021 01:33:25 - INFO - __main__ -   Batch number = 215
Evaluating:  76%|███████▌  | 215/282 [01:41<00:32,  2.05it/s]11/28/2021 01:33:26 - INFO - __main__ -   Batch number = 216
Evaluating:  77%|███████▋  | 216/282 [01:42<00:32,  2.01it/s]11/28/2021 01:33:26 - INFO - __main__ -   Batch number = 217
Evaluating:  77%|███████▋  | 217/282 [01:42<00:32,  1.99it/s]11/28/2021 01:33:27 - INFO - __main__ -   Batch number = 218
Evaluating:  77%|███████▋  | 218/282 [01:43<00:32,  1.98it/s]11/28/2021 01:33:27 - INFO - __main__ -   Batch number = 219
Evaluating:  78%|███████▊  | 219/282 [01:43<00:31,  1.98it/s]11/28/2021 01:33:28 - INFO - __main__ -   Batch number = 220
Evaluating:  78%|███████▊  | 220/282 [01:44<00:31,  1.96it/s]11/28/2021 01:33:28 - INFO - __main__ -   Batch number = 221
Evaluating:  78%|███████▊  | 221/282 [01:44<00:33,  1.80it/s]11/28/2021 01:33:29 - INFO - __main__ -   Batch number = 222
Evaluating:  79%|███████▊  | 222/282 [01:45<00:35,  1.68it/s]11/28/2021 01:33:29 - INFO - __main__ -   Batch number = 223
Evaluating:  79%|███████▉  | 223/282 [01:46<00:36,  1.61it/s]11/28/2021 01:33:30 - INFO - __main__ -   Batch number = 224
Evaluating:  79%|███████▉  | 224/282 [01:46<00:37,  1.56it/s]11/28/2021 01:33:31 - INFO - __main__ -   Batch number = 225
Evaluating:  80%|███████▉  | 225/282 [01:47<00:37,  1.53it/s]11/28/2021 01:33:32 - INFO - __main__ -   Batch number = 226
Evaluating:  80%|████████  | 226/282 [01:48<00:37,  1.50it/s]11/28/2021 01:33:32 - INFO - __main__ -   Batch number = 227
Evaluating:  80%|████████  | 227/282 [01:48<00:36,  1.49it/s]11/28/2021 01:33:33 - INFO - __main__ -   Batch number = 228
Evaluating:  81%|████████  | 228/282 [01:49<00:36,  1.47it/s]11/28/2021 01:33:34 - INFO - __main__ -   Batch number = 229
Evaluating:  81%|████████  | 229/282 [01:50<00:36,  1.47it/s]11/28/2021 01:33:34 - INFO - __main__ -   Batch number = 230
Evaluating:  82%|████████▏ | 230/282 [01:51<00:35,  1.46it/s]11/28/2021 01:33:35 - INFO - __main__ -   Batch number = 231
Evaluating:  82%|████████▏ | 231/282 [01:51<00:34,  1.47it/s]11/28/2021 01:33:36 - INFO - __main__ -   Batch number = 232
Evaluating:  82%|████████▏ | 232/282 [01:52<00:34,  1.46it/s]11/28/2021 01:33:36 - INFO - __main__ -   Batch number = 233
Evaluating:  83%|████████▎ | 233/282 [01:53<00:33,  1.46it/s]11/28/2021 01:33:37 - INFO - __main__ -   Batch number = 234
Evaluating:  83%|████████▎ | 234/282 [01:53<00:32,  1.45it/s]11/28/2021 01:33:38 - INFO - __main__ -   Batch number = 235
Evaluating:  83%|████████▎ | 235/282 [01:54<00:32,  1.46it/s]11/28/2021 01:33:38 - INFO - __main__ -   Batch number = 236
Evaluating:  84%|████████▎ | 236/282 [01:55<00:31,  1.45it/s]11/28/2021 01:33:39 - INFO - __main__ -   Batch number = 237
Evaluating:  84%|████████▍ | 237/282 [01:55<00:30,  1.46it/s]11/28/2021 01:33:40 - INFO - __main__ -   Batch number = 238
Evaluating:  84%|████████▍ | 238/282 [01:56<00:28,  1.57it/s]11/28/2021 01:33:40 - INFO - __main__ -   Batch number = 239
Evaluating:  85%|████████▍ | 239/282 [01:56<00:25,  1.66it/s]11/28/2021 01:33:41 - INFO - __main__ -   Batch number = 240
Evaluating:  85%|████████▌ | 240/282 [01:57<00:24,  1.73it/s]11/28/2021 01:33:41 - INFO - __main__ -   Batch number = 241
Evaluating:  85%|████████▌ | 241/282 [01:57<00:22,  1.78it/s]11/28/2021 01:33:42 - INFO - __main__ -   Batch number = 242
Evaluating:  86%|████████▌ | 242/282 [01:58<00:21,  1.82it/s]11/28/2021 01:33:42 - INFO - __main__ -   Batch number = 243
Evaluating:  86%|████████▌ | 243/282 [01:58<00:20,  1.86it/s]11/28/2021 01:33:43 - INFO - __main__ -   Batch number = 244
Evaluating:  87%|████████▋ | 244/282 [01:59<00:20,  1.87it/s]11/28/2021 01:33:43 - INFO - __main__ -   Batch number = 245
Evaluating:  87%|████████▋ | 245/282 [02:00<00:19,  1.90it/s]11/28/2021 01:33:44 - INFO - __main__ -   Batch number = 246
Evaluating:  87%|████████▋ | 246/282 [02:00<00:18,  1.90it/s]11/28/2021 01:33:44 - INFO - __main__ -   Batch number = 247
Evaluating:  88%|████████▊ | 247/282 [02:01<00:18,  1.91it/s]11/28/2021 01:33:45 - INFO - __main__ -   Batch number = 248
Evaluating:  88%|████████▊ | 248/282 [02:01<00:17,  1.91it/s]11/28/2021 01:33:46 - INFO - __main__ -   Batch number = 249
Evaluating:  88%|████████▊ | 249/282 [02:02<00:17,  1.92it/s]11/28/2021 01:33:46 - INFO - __main__ -   Batch number = 250
Evaluating:  89%|████████▊ | 250/282 [02:02<00:16,  1.92it/s]11/28/2021 01:33:47 - INFO - __main__ -   Batch number = 251
Evaluating:  89%|████████▉ | 251/282 [02:03<00:16,  1.93it/s]11/28/2021 01:33:47 - INFO - __main__ -   Batch number = 252
Evaluating:  89%|████████▉ | 252/282 [02:03<00:15,  1.91it/s]11/28/2021 01:33:48 - INFO - __main__ -   Batch number = 253
Evaluating:  90%|████████▉ | 253/282 [02:04<00:15,  1.93it/s]11/28/2021 01:33:48 - INFO - __main__ -   Batch number = 254
Evaluating:  90%|█████████ | 254/282 [02:04<00:14,  1.93it/s]11/28/2021 01:33:49 - INFO - __main__ -   Batch number = 255
Evaluating:  90%|█████████ | 255/282 [02:05<00:13,  1.94it/s]11/28/2021 01:33:49 - INFO - __main__ -   Batch number = 256
Evaluating:  91%|█████████ | 256/282 [02:05<00:13,  1.94it/s]11/28/2021 01:33:50 - INFO - __main__ -   Batch number = 257
Evaluating:  91%|█████████ | 257/282 [02:06<00:12,  1.95it/s]11/28/2021 01:33:50 - INFO - __main__ -   Batch number = 258
Evaluating:  91%|█████████▏| 258/282 [02:06<00:12,  1.93it/s]11/28/2021 01:33:51 - INFO - __main__ -   Batch number = 259
Evaluating:  92%|█████████▏| 259/282 [02:07<00:11,  1.93it/s]11/28/2021 01:33:51 - INFO - __main__ -   Batch number = 260
Evaluating:  92%|█████████▏| 260/282 [02:07<00:11,  1.92it/s]11/28/2021 01:33:52 - INFO - __main__ -   Batch number = 261
Evaluating:  93%|█████████▎| 261/282 [02:08<00:10,  1.92it/s]11/28/2021 01:33:52 - INFO - __main__ -   Batch number = 262
Evaluating:  93%|█████████▎| 262/282 [02:08<00:10,  1.92it/s]11/28/2021 01:33:53 - INFO - __main__ -   Batch number = 263
Evaluating:  93%|█████████▎| 263/282 [02:09<00:09,  1.92it/s]11/28/2021 01:33:53 - INFO - __main__ -   Batch number = 264
Evaluating:  94%|█████████▎| 264/282 [02:09<00:09,  1.92it/s]11/28/2021 01:33:54 - INFO - __main__ -   Batch number = 265
Evaluating:  94%|█████████▍| 265/282 [02:10<00:08,  1.93it/s]11/28/2021 01:33:54 - INFO - __main__ -   Batch number = 266
Evaluating:  94%|█████████▍| 266/282 [02:10<00:07,  2.11it/s]11/28/2021 01:33:55 - INFO - __main__ -   Batch number = 267
Evaluating:  95%|█████████▍| 267/282 [02:11<00:06,  2.21it/s]11/28/2021 01:33:55 - INFO - __main__ -   Batch number = 268
Evaluating:  95%|█████████▌| 268/282 [02:11<00:05,  2.34it/s]11/28/2021 01:33:55 - INFO - __main__ -   Batch number = 269
Evaluating:  95%|█████████▌| 269/282 [02:11<00:05,  2.48it/s]11/28/2021 01:33:56 - INFO - __main__ -   Batch number = 270
Evaluating:  96%|█████████▌| 270/282 [02:12<00:04,  2.55it/s]11/28/2021 01:33:56 - INFO - __main__ -   Batch number = 271
Evaluating:  96%|█████████▌| 271/282 [02:12<00:04,  2.63it/s]11/28/2021 01:33:57 - INFO - __main__ -   Batch number = 272
Evaluating:  96%|█████████▋| 272/282 [02:12<00:03,  2.67it/s]11/28/2021 01:33:57 - INFO - __main__ -   Batch number = 273
Evaluating:  97%|█████████▋| 273/282 [02:13<00:03,  2.73it/s]11/28/2021 01:33:57 - INFO - __main__ -   Batch number = 274
Evaluating:  97%|█████████▋| 274/282 [02:13<00:02,  2.72it/s]11/28/2021 01:33:58 - INFO - __main__ -   Batch number = 275
Evaluating:  98%|█████████▊| 275/282 [02:14<00:02,  2.75it/s]11/28/2021 01:33:58 - INFO - __main__ -   Batch number = 276
Evaluating:  98%|█████████▊| 276/282 [02:14<00:02,  2.76it/s]11/28/2021 01:33:58 - INFO - __main__ -   Batch number = 277
Evaluating:  98%|█████████▊| 277/282 [02:14<00:01,  2.79it/s]11/28/2021 01:33:59 - INFO - __main__ -   Batch number = 278
Evaluating:  99%|█████████▊| 278/282 [02:15<00:01,  2.74it/s]11/28/2021 01:33:59 - INFO - __main__ -   Batch number = 279
Evaluating:  99%|█████████▉| 279/282 [02:15<00:01,  2.75it/s]11/28/2021 01:33:59 - INFO - __main__ -   Batch number = 280
Evaluating:  99%|█████████▉| 280/282 [02:15<00:00,  2.58it/s]11/28/2021 01:34:00 - INFO - __main__ -   Batch number = 281
Evaluating: 100%|█████████▉| 281/282 [02:16<00:00,  2.67it/s]11/28/2021 01:34:00 - INFO - __main__ -   Batch number = 282
Evaluating: 100%|██████████| 282/282 [02:16<00:00,  3.21it/s]Evaluating: 100%|██████████| 282/282 [02:16<00:00,  2.07it/s]
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:34:04 - INFO - __main__ -   ***** Evaluation result  in ru *****
11/28/2021 01:34:05 - INFO - __main__ -     f1 = 0.8576613751867429
11/28/2021 01:34:05 - INFO - __main__ -     loss = 0.5609101448075992
11/28/2021 01:34:05 - INFO - __main__ -     precision = 0.8626946062065758
11/28/2021 01:34:05 - INFO - __main__ -     recall = 0.8526865343980681
123.70user 42.23system 2:44.80elapsed 100%CPU (0avgtext+0avgdata 3989212maxresident)k
0inputs+1824outputs (0major+1797601minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:34:07 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ru', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:34:07 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:34:07 - INFO - __main__ -   Seed = 3
11/28/2021 01:34:07 - INFO - root -   save model
11/28/2021 01:34:07 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ru', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:34:07 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:34:10 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:34:16 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:34:16 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:34:16 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
11/28/2021 01:34:16 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:34:16 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:34:16 - INFO - root -   loading lang adpater cs/wiki@ukp
11/28/2021 01:34:16 - INFO - __main__ -   Adapter Languages : ['cs'], Length : 1
11/28/2021 01:34:16 - INFO - __main__ -   Adapter Names ['cs/wiki@ukp'], Length : 1
11/28/2021 01:34:16 - INFO - __main__ -   Language = cs
11/28/2021 01:34:16 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/cs/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_cs_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/adapter_config.json
Adding adapter 'cs' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/head_config.json
11/28/2021 01:34:27 - INFO - __main__ -   Language adapter for ru not found, using cs instead
11/28/2021 01:34:27 - INFO - __main__ -   Set active language adapter to cs
11/28/2021 01:34:27 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:34:27 - INFO - __main__ -   Adapter Languages = ['cs']
11/28/2021 01:34:27 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_ru_bert-base-multilingual-cased_128
11/28/2021 01:34:29 - INFO - __main__ -   ***** Running evaluation  in ru *****
11/28/2021 01:34:29 - INFO - __main__ -     Num examples = 8995
11/28/2021 01:34:29 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/282 [00:00<?, ?it/s]11/28/2021 01:34:29 - INFO - __main__ -   Batch number = 1
Evaluating:   0%|          | 1/282 [00:00<03:08,  1.49it/s]11/28/2021 01:34:29 - INFO - __main__ -   Batch number = 2
Evaluating:   1%|          | 2/282 [00:00<02:11,  2.13it/s]11/28/2021 01:34:30 - INFO - __main__ -   Batch number = 3
Evaluating:   1%|          | 3/282 [00:01<01:53,  2.45it/s]11/28/2021 01:34:30 - INFO - __main__ -   Batch number = 4
Evaluating:   1%|▏         | 4/282 [00:01<01:44,  2.66it/s]11/28/2021 01:34:30 - INFO - __main__ -   Batch number = 5
Evaluating:   2%|▏         | 5/282 [00:01<01:39,  2.79it/s]11/28/2021 01:34:31 - INFO - __main__ -   Batch number = 6
Evaluating:   2%|▏         | 6/282 [00:02<01:36,  2.85it/s]11/28/2021 01:34:31 - INFO - __main__ -   Batch number = 7
Evaluating:   2%|▏         | 7/282 [00:02<01:34,  2.90it/s]11/28/2021 01:34:31 - INFO - __main__ -   Batch number = 8
Evaluating:   3%|▎         | 8/282 [00:02<01:32,  2.95it/s]11/28/2021 01:34:32 - INFO - __main__ -   Batch number = 9
Evaluating:   3%|▎         | 9/282 [00:03<01:32,  2.96it/s]11/28/2021 01:34:32 - INFO - __main__ -   Batch number = 10
Evaluating:   4%|▎         | 10/282 [00:03<01:31,  2.97it/s]11/28/2021 01:34:32 - INFO - __main__ -   Batch number = 11
Evaluating:   4%|▍         | 11/282 [00:03<01:31,  2.97it/s]11/28/2021 01:34:33 - INFO - __main__ -   Batch number = 12
Evaluating:   4%|▍         | 12/282 [00:04<01:30,  2.97it/s]11/28/2021 01:34:33 - INFO - __main__ -   Batch number = 13
Evaluating:   5%|▍         | 13/282 [00:04<01:30,  2.97it/s]11/28/2021 01:34:33 - INFO - __main__ -   Batch number = 14
Evaluating:   5%|▍         | 14/282 [00:04<01:29,  2.98it/s]11/28/2021 01:34:34 - INFO - __main__ -   Batch number = 15
Evaluating:   5%|▌         | 15/282 [00:05<01:29,  2.98it/s]11/28/2021 01:34:34 - INFO - __main__ -   Batch number = 16
Evaluating:   6%|▌         | 16/282 [00:05<01:29,  2.98it/s]11/28/2021 01:34:34 - INFO - __main__ -   Batch number = 17
Evaluating:   6%|▌         | 17/282 [00:06<01:28,  2.98it/s]11/28/2021 01:34:35 - INFO - __main__ -   Batch number = 18
Evaluating:   6%|▋         | 18/282 [00:06<01:28,  2.99it/s]11/28/2021 01:34:35 - INFO - __main__ -   Batch number = 19
Evaluating:   7%|▋         | 19/282 [00:06<01:27,  2.99it/s]11/28/2021 01:34:35 - INFO - __main__ -   Batch number = 20
Evaluating:   7%|▋         | 20/282 [00:07<01:27,  2.98it/s]11/28/2021 01:34:36 - INFO - __main__ -   Batch number = 21
Evaluating:   7%|▋         | 21/282 [00:07<01:27,  2.98it/s]11/28/2021 01:34:36 - INFO - __main__ -   Batch number = 22
Evaluating:   8%|▊         | 22/282 [00:07<01:27,  2.98it/s]11/28/2021 01:34:36 - INFO - __main__ -   Batch number = 23
Evaluating:   8%|▊         | 23/282 [00:08<01:27,  2.97it/s]11/28/2021 01:34:37 - INFO - __main__ -   Batch number = 24
Evaluating:   9%|▊         | 24/282 [00:08<01:26,  2.98it/s]11/28/2021 01:34:37 - INFO - __main__ -   Batch number = 25
Evaluating:   9%|▉         | 25/282 [00:08<01:25,  3.02it/s]11/28/2021 01:34:37 - INFO - __main__ -   Batch number = 26
Evaluating:   9%|▉         | 26/282 [00:09<01:25,  3.01it/s]11/28/2021 01:34:38 - INFO - __main__ -   Batch number = 27
Evaluating:  10%|▉         | 27/282 [00:09<01:23,  3.04it/s]11/28/2021 01:34:38 - INFO - __main__ -   Batch number = 28
Evaluating:  10%|▉         | 28/282 [00:09<01:24,  3.02it/s]11/28/2021 01:34:38 - INFO - __main__ -   Batch number = 29
Evaluating:  10%|█         | 29/282 [00:09<01:23,  3.02it/s]11/28/2021 01:34:39 - INFO - __main__ -   Batch number = 30
Evaluating:  11%|█         | 30/282 [00:10<01:29,  2.81it/s]11/28/2021 01:34:39 - INFO - __main__ -   Batch number = 31
Evaluating:  11%|█         | 31/282 [00:10<01:40,  2.50it/s]11/28/2021 01:34:40 - INFO - __main__ -   Batch number = 32
Evaluating:  11%|█▏        | 32/282 [00:11<01:47,  2.33it/s]11/28/2021 01:34:40 - INFO - __main__ -   Batch number = 33
Evaluating:  12%|█▏        | 33/282 [00:11<01:52,  2.21it/s]11/28/2021 01:34:41 - INFO - __main__ -   Batch number = 34
Evaluating:  12%|█▏        | 34/282 [00:12<01:55,  2.14it/s]11/28/2021 01:34:41 - INFO - __main__ -   Batch number = 35
Evaluating:  12%|█▏        | 35/282 [00:12<01:57,  2.10it/s]11/28/2021 01:34:42 - INFO - __main__ -   Batch number = 36
Evaluating:  13%|█▎        | 36/282 [00:13<01:58,  2.07it/s]11/28/2021 01:34:42 - INFO - __main__ -   Batch number = 37
Evaluating:  13%|█▎        | 37/282 [00:13<01:59,  2.04it/s]11/28/2021 01:34:43 - INFO - __main__ -   Batch number = 38
Evaluating:  13%|█▎        | 38/282 [00:14<02:00,  2.02it/s]11/28/2021 01:34:43 - INFO - __main__ -   Batch number = 39
Evaluating:  14%|█▍        | 39/282 [00:14<02:00,  2.01it/s]11/28/2021 01:34:44 - INFO - __main__ -   Batch number = 40
Evaluating:  14%|█▍        | 40/282 [00:15<02:00,  2.00it/s]11/28/2021 01:34:44 - INFO - __main__ -   Batch number = 41
Evaluating:  15%|█▍        | 41/282 [00:15<02:00,  2.00it/s]11/28/2021 01:34:45 - INFO - __main__ -   Batch number = 42
Evaluating:  15%|█▍        | 42/282 [00:16<02:00,  1.99it/s]11/28/2021 01:34:45 - INFO - __main__ -   Batch number = 43
Evaluating:  15%|█▌        | 43/282 [00:16<01:59,  1.99it/s]11/28/2021 01:34:46 - INFO - __main__ -   Batch number = 44
Evaluating:  16%|█▌        | 44/282 [00:17<01:59,  1.99it/s]11/28/2021 01:34:46 - INFO - __main__ -   Batch number = 45
Evaluating:  16%|█▌        | 45/282 [00:17<01:58,  1.99it/s]11/28/2021 01:34:47 - INFO - __main__ -   Batch number = 46
Evaluating:  16%|█▋        | 46/282 [00:18<01:59,  1.98it/s]11/28/2021 01:34:47 - INFO - __main__ -   Batch number = 47
Evaluating:  17%|█▋        | 47/282 [00:18<01:58,  1.98it/s]11/28/2021 01:34:48 - INFO - __main__ -   Batch number = 48
Evaluating:  17%|█▋        | 48/282 [00:19<01:58,  1.98it/s]11/28/2021 01:34:48 - INFO - __main__ -   Batch number = 49
Evaluating:  17%|█▋        | 49/282 [00:19<01:57,  1.98it/s]11/28/2021 01:34:49 - INFO - __main__ -   Batch number = 50
Evaluating:  18%|█▊        | 50/282 [00:20<01:57,  1.98it/s]11/28/2021 01:34:49 - INFO - __main__ -   Batch number = 51
Evaluating:  18%|█▊        | 51/282 [00:20<01:56,  1.98it/s]11/28/2021 01:34:50 - INFO - __main__ -   Batch number = 52
Evaluating:  18%|█▊        | 52/282 [00:21<01:58,  1.95it/s]11/28/2021 01:34:50 - INFO - __main__ -   Batch number = 53
Evaluating:  19%|█▉        | 53/282 [00:21<01:51,  2.06it/s]11/28/2021 01:34:51 - INFO - __main__ -   Batch number = 54
Evaluating:  19%|█▉        | 54/282 [00:22<01:40,  2.27it/s]11/28/2021 01:34:51 - INFO - __main__ -   Batch number = 55
Evaluating:  20%|█▉        | 55/282 [00:22<01:33,  2.44it/s]11/28/2021 01:34:51 - INFO - __main__ -   Batch number = 56
Evaluating:  20%|█▉        | 56/282 [00:22<01:28,  2.56it/s]11/28/2021 01:34:52 - INFO - __main__ -   Batch number = 57
Evaluating:  20%|██        | 57/282 [00:23<01:24,  2.65it/s]11/28/2021 01:34:52 - INFO - __main__ -   Batch number = 58
Evaluating:  21%|██        | 58/282 [00:23<01:22,  2.73it/s]11/28/2021 01:34:52 - INFO - __main__ -   Batch number = 59
Evaluating:  21%|██        | 59/282 [00:23<01:19,  2.79it/s]11/28/2021 01:34:53 - INFO - __main__ -   Batch number = 60
Evaluating:  21%|██▏       | 60/282 [00:24<01:19,  2.81it/s]11/28/2021 01:34:53 - INFO - __main__ -   Batch number = 61
Evaluating:  22%|██▏       | 61/282 [00:24<01:18,  2.83it/s]11/28/2021 01:34:53 - INFO - __main__ -   Batch number = 62
Evaluating:  22%|██▏       | 62/282 [00:25<01:16,  2.86it/s]11/28/2021 01:34:54 - INFO - __main__ -   Batch number = 63
Evaluating:  22%|██▏       | 63/282 [00:25<01:16,  2.88it/s]11/28/2021 01:34:54 - INFO - __main__ -   Batch number = 64
Evaluating:  23%|██▎       | 64/282 [00:25<01:16,  2.84it/s]11/28/2021 01:34:54 - INFO - __main__ -   Batch number = 65
Evaluating:  23%|██▎       | 65/282 [00:26<01:24,  2.58it/s]11/28/2021 01:34:55 - INFO - __main__ -   Batch number = 66
Evaluating:  23%|██▎       | 66/282 [00:26<01:31,  2.37it/s]11/28/2021 01:34:55 - INFO - __main__ -   Batch number = 67
Evaluating:  24%|██▍       | 67/282 [00:27<01:36,  2.24it/s]11/28/2021 01:34:56 - INFO - __main__ -   Batch number = 68
Evaluating:  24%|██▍       | 68/282 [00:27<01:39,  2.15it/s]11/28/2021 01:34:56 - INFO - __main__ -   Batch number = 69
Evaluating:  24%|██▍       | 69/282 [00:28<01:41,  2.09it/s]11/28/2021 01:34:57 - INFO - __main__ -   Batch number = 70
Evaluating:  25%|██▍       | 70/282 [00:28<01:43,  2.06it/s]11/28/2021 01:34:57 - INFO - __main__ -   Batch number = 71
Evaluating:  25%|██▌       | 71/282 [00:29<01:43,  2.03it/s]11/28/2021 01:34:58 - INFO - __main__ -   Batch number = 72
Evaluating:  26%|██▌       | 72/282 [00:29<01:44,  2.01it/s]11/28/2021 01:34:58 - INFO - __main__ -   Batch number = 73
Evaluating:  26%|██▌       | 73/282 [00:30<01:44,  2.00it/s]11/28/2021 01:34:59 - INFO - __main__ -   Batch number = 74
Evaluating:  26%|██▌       | 74/282 [00:30<01:43,  2.00it/s]11/28/2021 01:34:59 - INFO - __main__ -   Batch number = 75
Evaluating:  27%|██▋       | 75/282 [00:31<01:43,  1.99it/s]11/28/2021 01:35:00 - INFO - __main__ -   Batch number = 76
Evaluating:  27%|██▋       | 76/282 [00:31<01:43,  1.99it/s]11/28/2021 01:35:00 - INFO - __main__ -   Batch number = 77
Evaluating:  27%|██▋       | 77/282 [00:32<01:43,  1.98it/s]11/28/2021 01:35:01 - INFO - __main__ -   Batch number = 78
Evaluating:  28%|██▊       | 78/282 [00:32<01:42,  1.99it/s]11/28/2021 01:35:01 - INFO - __main__ -   Batch number = 79
Evaluating:  28%|██▊       | 79/282 [00:33<01:42,  1.99it/s]11/28/2021 01:35:02 - INFO - __main__ -   Batch number = 80
Evaluating:  28%|██▊       | 80/282 [00:33<01:41,  1.99it/s]11/28/2021 01:35:02 - INFO - __main__ -   Batch number = 81
Evaluating:  29%|██▊       | 81/282 [00:34<01:40,  1.99it/s]11/28/2021 01:35:03 - INFO - __main__ -   Batch number = 82
Evaluating:  29%|██▉       | 82/282 [00:34<01:40,  1.99it/s]11/28/2021 01:35:03 - INFO - __main__ -   Batch number = 83
Evaluating:  29%|██▉       | 83/282 [00:35<01:40,  1.99it/s]11/28/2021 01:35:04 - INFO - __main__ -   Batch number = 84
Evaluating:  30%|██▉       | 84/282 [00:35<01:39,  1.99it/s]11/28/2021 01:35:04 - INFO - __main__ -   Batch number = 85
Evaluating:  30%|███       | 85/282 [00:36<01:37,  2.02it/s]11/28/2021 01:35:05 - INFO - __main__ -   Batch number = 86
Evaluating:  30%|███       | 86/282 [00:36<01:37,  2.01it/s]11/28/2021 01:35:05 - INFO - __main__ -   Batch number = 87
Evaluating:  31%|███       | 87/282 [00:37<01:36,  2.01it/s]11/28/2021 01:35:06 - INFO - __main__ -   Batch number = 88
Evaluating:  31%|███       | 88/282 [00:37<01:36,  2.00it/s]11/28/2021 01:35:06 - INFO - __main__ -   Batch number = 89
Evaluating:  32%|███▏      | 89/282 [00:38<01:37,  1.99it/s]11/28/2021 01:35:07 - INFO - __main__ -   Batch number = 90
Evaluating:  32%|███▏      | 90/282 [00:38<01:36,  1.99it/s]11/28/2021 01:35:07 - INFO - __main__ -   Batch number = 91
Evaluating:  32%|███▏      | 91/282 [00:39<01:37,  1.97it/s]11/28/2021 01:35:08 - INFO - __main__ -   Batch number = 92
Evaluating:  33%|███▎      | 92/282 [00:39<01:36,  1.96it/s]11/28/2021 01:35:08 - INFO - __main__ -   Batch number = 93
Evaluating:  33%|███▎      | 93/282 [00:40<01:35,  1.98it/s]11/28/2021 01:35:09 - INFO - __main__ -   Batch number = 94
Evaluating:  33%|███▎      | 94/282 [00:40<01:35,  1.98it/s]11/28/2021 01:35:09 - INFO - __main__ -   Batch number = 95
Evaluating:  34%|███▎      | 95/282 [00:41<01:32,  2.02it/s]11/28/2021 01:35:10 - INFO - __main__ -   Batch number = 96
Evaluating:  34%|███▍      | 96/282 [00:41<01:32,  2.02it/s]11/28/2021 01:35:10 - INFO - __main__ -   Batch number = 97
Evaluating:  34%|███▍      | 97/282 [00:42<01:33,  1.97it/s]11/28/2021 01:35:11 - INFO - __main__ -   Batch number = 98
Evaluating:  35%|███▍      | 98/282 [00:42<01:33,  1.97it/s]11/28/2021 01:35:11 - INFO - __main__ -   Batch number = 99
Evaluating:  35%|███▌      | 99/282 [00:43<01:32,  1.98it/s]11/28/2021 01:35:12 - INFO - __main__ -   Batch number = 100
Evaluating:  35%|███▌      | 100/282 [00:43<01:32,  1.98it/s]11/28/2021 01:35:12 - INFO - __main__ -   Batch number = 101
Evaluating:  36%|███▌      | 101/282 [00:44<01:31,  1.98it/s]11/28/2021 01:35:13 - INFO - __main__ -   Batch number = 102
Evaluating:  36%|███▌      | 102/282 [00:44<01:30,  1.98it/s]11/28/2021 01:35:13 - INFO - __main__ -   Batch number = 103
Evaluating:  37%|███▋      | 103/282 [00:45<01:30,  1.98it/s]11/28/2021 01:35:14 - INFO - __main__ -   Batch number = 104
Evaluating:  37%|███▋      | 104/282 [00:45<01:29,  1.98it/s]11/28/2021 01:35:15 - INFO - __main__ -   Batch number = 105
Evaluating:  37%|███▋      | 105/282 [00:46<01:47,  1.65it/s]11/28/2021 01:35:15 - INFO - __main__ -   Batch number = 106
Evaluating:  38%|███▊      | 106/282 [00:47<01:41,  1.74it/s]11/28/2021 01:35:16 - INFO - __main__ -   Batch number = 107
Evaluating:  38%|███▊      | 107/282 [00:47<01:36,  1.81it/s]11/28/2021 01:35:16 - INFO - __main__ -   Batch number = 108
Evaluating:  38%|███▊      | 108/282 [00:48<01:33,  1.86it/s]11/28/2021 01:35:17 - INFO - __main__ -   Batch number = 109
Evaluating:  39%|███▊      | 109/282 [00:48<01:31,  1.89it/s]11/28/2021 01:35:17 - INFO - __main__ -   Batch number = 110
Evaluating:  39%|███▉      | 110/282 [00:49<01:30,  1.90it/s]11/28/2021 01:35:18 - INFO - __main__ -   Batch number = 111
Evaluating:  39%|███▉      | 111/282 [00:49<01:34,  1.81it/s]11/28/2021 01:35:18 - INFO - __main__ -   Batch number = 112
Evaluating:  40%|███▉      | 112/282 [00:50<01:40,  1.70it/s]11/28/2021 01:35:19 - INFO - __main__ -   Batch number = 113
Evaluating:  40%|████      | 113/282 [00:51<02:04,  1.36it/s]11/28/2021 01:35:20 - INFO - __main__ -   Batch number = 114
Evaluating:  40%|████      | 114/282 [00:52<02:00,  1.39it/s]11/28/2021 01:35:21 - INFO - __main__ -   Batch number = 115
Evaluating:  41%|████      | 115/282 [00:52<01:57,  1.42it/s]11/28/2021 01:35:22 - INFO - __main__ -   Batch number = 116
Evaluating:  41%|████      | 116/282 [00:53<01:58,  1.40it/s]11/28/2021 01:35:22 - INFO - __main__ -   Batch number = 117
Evaluating:  41%|████▏     | 117/282 [00:54<01:55,  1.43it/s]11/28/2021 01:35:23 - INFO - __main__ -   Batch number = 118
Evaluating:  42%|████▏     | 118/282 [00:55<01:53,  1.45it/s]11/28/2021 01:35:24 - INFO - __main__ -   Batch number = 119
Evaluating:  42%|████▏     | 119/282 [00:55<02:03,  1.32it/s]11/28/2021 01:35:25 - INFO - __main__ -   Batch number = 120
Evaluating:  43%|████▎     | 120/282 [00:56<02:02,  1.32it/s]11/28/2021 01:35:25 - INFO - __main__ -   Batch number = 121
Evaluating:  43%|████▎     | 121/282 [00:57<01:57,  1.37it/s]11/28/2021 01:35:26 - INFO - __main__ -   Batch number = 122
Evaluating:  43%|████▎     | 122/282 [00:58<01:57,  1.36it/s]11/28/2021 01:35:27 - INFO - __main__ -   Batch number = 123
Evaluating:  44%|████▎     | 123/282 [00:58<01:53,  1.40it/s]11/28/2021 01:35:27 - INFO - __main__ -   Batch number = 124
Evaluating:  44%|████▍     | 124/282 [00:59<01:50,  1.43it/s]11/28/2021 01:35:28 - INFO - __main__ -   Batch number = 125
Evaluating:  44%|████▍     | 125/282 [01:00<01:48,  1.45it/s]11/28/2021 01:35:29 - INFO - __main__ -   Batch number = 126
Evaluating:  45%|████▍     | 126/282 [01:00<01:46,  1.46it/s]11/28/2021 01:35:29 - INFO - __main__ -   Batch number = 127
Evaluating:  45%|████▌     | 127/282 [01:01<01:44,  1.48it/s]11/28/2021 01:35:30 - INFO - __main__ -   Batch number = 128
Evaluating:  45%|████▌     | 128/282 [01:02<01:43,  1.48it/s]11/28/2021 01:35:31 - INFO - __main__ -   Batch number = 129
Evaluating:  46%|████▌     | 129/282 [01:02<01:43,  1.48it/s]11/28/2021 01:35:31 - INFO - __main__ -   Batch number = 130
Evaluating:  46%|████▌     | 130/282 [01:03<01:42,  1.48it/s]11/28/2021 01:35:32 - INFO - __main__ -   Batch number = 131
Evaluating:  46%|████▋     | 131/282 [01:04<01:41,  1.49it/s]11/28/2021 01:35:33 - INFO - __main__ -   Batch number = 132
Evaluating:  47%|████▋     | 132/282 [01:04<01:40,  1.49it/s]11/28/2021 01:35:33 - INFO - __main__ -   Batch number = 133
Evaluating:  47%|████▋     | 133/282 [01:05<01:51,  1.34it/s]11/28/2021 01:35:34 - INFO - __main__ -   Batch number = 134
Evaluating:  48%|████▊     | 134/282 [01:06<01:44,  1.41it/s]11/28/2021 01:35:35 - INFO - __main__ -   Batch number = 135
Evaluating:  48%|████▊     | 135/282 [01:06<01:36,  1.53it/s]11/28/2021 01:35:35 - INFO - __main__ -   Batch number = 136
Evaluating:  48%|████▊     | 136/282 [01:07<01:28,  1.64it/s]11/28/2021 01:35:36 - INFO - __main__ -   Batch number = 137
Evaluating:  49%|████▊     | 137/282 [01:07<01:23,  1.73it/s]11/28/2021 01:35:36 - INFO - __main__ -   Batch number = 138
Evaluating:  49%|████▉     | 138/282 [01:08<01:30,  1.59it/s]11/28/2021 01:35:37 - INFO - __main__ -   Batch number = 139
Evaluating:  49%|████▉     | 139/282 [01:09<01:24,  1.69it/s]11/28/2021 01:35:38 - INFO - __main__ -   Batch number = 140
Evaluating:  50%|████▉     | 140/282 [01:09<01:21,  1.74it/s]11/28/2021 01:35:38 - INFO - __main__ -   Batch number = 141
Evaluating:  50%|█████     | 141/282 [01:10<01:17,  1.82it/s]11/28/2021 01:35:39 - INFO - __main__ -   Batch number = 142
Evaluating:  50%|█████     | 142/282 [01:11<01:45,  1.32it/s]11/28/2021 01:35:40 - INFO - __main__ -   Batch number = 143
Evaluating:  51%|█████     | 143/282 [01:11<01:27,  1.58it/s]11/28/2021 01:35:40 - INFO - __main__ -   Batch number = 144
Evaluating:  51%|█████     | 144/282 [01:12<01:27,  1.57it/s]11/28/2021 01:35:41 - INFO - __main__ -   Batch number = 145
Evaluating:  51%|█████▏    | 145/282 [01:12<01:15,  1.82it/s]11/28/2021 01:35:41 - INFO - __main__ -   Batch number = 146
Evaluating:  52%|█████▏    | 146/282 [01:13<01:09,  1.96it/s]11/28/2021 01:35:42 - INFO - __main__ -   Batch number = 147
Evaluating:  52%|█████▏    | 147/282 [01:13<01:01,  2.20it/s]11/28/2021 01:35:42 - INFO - __main__ -   Batch number = 148
Evaluating:  52%|█████▏    | 148/282 [01:14<01:18,  1.71it/s]11/28/2021 01:35:43 - INFO - __main__ -   Batch number = 149
Evaluating:  53%|█████▎    | 149/282 [01:14<01:08,  1.96it/s]11/28/2021 01:35:43 - INFO - __main__ -   Batch number = 150
Evaluating:  53%|█████▎    | 150/282 [01:15<01:09,  1.90it/s]11/28/2021 01:35:44 - INFO - __main__ -   Batch number = 151
Evaluating:  54%|█████▎    | 151/282 [01:15<01:01,  2.14it/s]11/28/2021 01:35:44 - INFO - __main__ -   Batch number = 152
Evaluating:  54%|█████▍    | 152/282 [01:15<00:58,  2.21it/s]11/28/2021 01:35:45 - INFO - __main__ -   Batch number = 153
Evaluating:  54%|█████▍    | 153/282 [01:16<00:54,  2.38it/s]11/28/2021 01:35:45 - INFO - __main__ -   Batch number = 154
Evaluating:  55%|█████▍    | 154/282 [01:16<00:58,  2.17it/s]11/28/2021 01:35:46 - INFO - __main__ -   Batch number = 155
Evaluating:  55%|█████▍    | 155/282 [01:17<00:53,  2.36it/s]11/28/2021 01:35:46 - INFO - __main__ -   Batch number = 156
Evaluating:  55%|█████▌    | 156/282 [01:17<00:57,  2.21it/s]11/28/2021 01:35:46 - INFO - __main__ -   Batch number = 157
Evaluating:  56%|█████▌    | 157/282 [01:18<00:52,  2.38it/s]11/28/2021 01:35:47 - INFO - __main__ -   Batch number = 158
Evaluating:  56%|█████▌    | 158/282 [01:18<00:57,  2.14it/s]11/28/2021 01:35:47 - INFO - __main__ -   Batch number = 159
Evaluating:  56%|█████▋    | 159/282 [01:18<00:52,  2.33it/s]11/28/2021 01:35:48 - INFO - __main__ -   Batch number = 160
Evaluating:  57%|█████▋    | 160/282 [01:19<00:56,  2.17it/s]11/28/2021 01:35:48 - INFO - __main__ -   Batch number = 161
Evaluating:  57%|█████▋    | 161/282 [01:19<00:51,  2.36it/s]11/28/2021 01:35:49 - INFO - __main__ -   Batch number = 162
Evaluating:  57%|█████▋    | 162/282 [01:20<01:03,  1.89it/s]11/28/2021 01:35:49 - INFO - __main__ -   Batch number = 163
Evaluating:  58%|█████▊    | 163/282 [01:20<00:55,  2.16it/s]11/28/2021 01:35:50 - INFO - __main__ -   Batch number = 164
Evaluating:  58%|█████▊    | 164/282 [01:21<00:55,  2.11it/s]11/28/2021 01:35:50 - INFO - __main__ -   Batch number = 165
Evaluating:  59%|█████▊    | 165/282 [01:21<00:50,  2.32it/s]11/28/2021 01:35:50 - INFO - __main__ -   Batch number = 166
Evaluating:  59%|█████▉    | 166/282 [01:22<01:01,  1.89it/s]11/28/2021 01:35:51 - INFO - __main__ -   Batch number = 167
Evaluating:  59%|█████▉    | 167/282 [01:22<00:54,  2.12it/s]11/28/2021 01:35:52 - INFO - __main__ -   Batch number = 168
Evaluating:  60%|█████▉    | 168/282 [01:23<00:54,  2.11it/s]11/28/2021 01:35:52 - INFO - __main__ -   Batch number = 169
Evaluating:  60%|█████▉    | 169/282 [01:23<00:48,  2.31it/s]11/28/2021 01:35:52 - INFO - __main__ -   Batch number = 170
Evaluating:  60%|██████    | 170/282 [01:24<00:51,  2.16it/s]11/28/2021 01:35:53 - INFO - __main__ -   Batch number = 171
Evaluating:  61%|██████    | 171/282 [01:24<00:47,  2.35it/s]11/28/2021 01:35:53 - INFO - __main__ -   Batch number = 172
Evaluating:  61%|██████    | 172/282 [01:25<00:50,  2.20it/s]11/28/2021 01:35:54 - INFO - __main__ -   Batch number = 173
Evaluating:  61%|██████▏   | 173/282 [01:25<00:46,  2.36it/s]11/28/2021 01:35:54 - INFO - __main__ -   Batch number = 174
Evaluating:  62%|██████▏   | 174/282 [01:26<00:52,  2.05it/s]11/28/2021 01:35:55 - INFO - __main__ -   Batch number = 175
Evaluating:  62%|██████▏   | 175/282 [01:26<00:47,  2.24it/s]11/28/2021 01:35:55 - INFO - __main__ -   Batch number = 176
Evaluating:  62%|██████▏   | 176/282 [01:27<01:10,  1.51it/s]11/28/2021 01:35:56 - INFO - __main__ -   Batch number = 177
Evaluating:  63%|██████▎   | 177/282 [01:27<00:54,  1.93it/s]11/28/2021 01:35:56 - INFO - __main__ -   Batch number = 178
Evaluating:  63%|██████▎   | 178/282 [01:28<00:53,  1.93it/s]11/28/2021 01:35:57 - INFO - __main__ -   Batch number = 179
Evaluating:  63%|██████▎   | 179/282 [01:28<00:43,  2.39it/s]11/28/2021 01:35:57 - INFO - __main__ -   Batch number = 180
Evaluating:  64%|██████▍   | 180/282 [01:28<00:44,  2.31it/s]11/28/2021 01:35:58 - INFO - __main__ -   Batch number = 181
Evaluating:  64%|██████▍   | 181/282 [01:29<00:36,  2.80it/s]11/28/2021 01:35:58 - INFO - __main__ -   Batch number = 182
Evaluating:  65%|██████▍   | 182/282 [01:29<00:48,  2.08it/s]11/28/2021 01:35:59 - INFO - __main__ -   Batch number = 183
Evaluating:  65%|██████▍   | 183/282 [01:30<00:38,  2.56it/s]11/28/2021 01:35:59 - INFO - __main__ -   Batch number = 184
Evaluating:  65%|██████▌   | 184/282 [01:30<00:44,  2.20it/s]11/28/2021 01:35:59 - INFO - __main__ -   Batch number = 185
Evaluating:  66%|██████▌   | 185/282 [01:30<00:40,  2.42it/s]11/28/2021 01:36:00 - INFO - __main__ -   Batch number = 186
Evaluating:  66%|██████▌   | 186/282 [01:31<00:40,  2.36it/s]11/28/2021 01:36:00 - INFO - __main__ -   Batch number = 187
Evaluating:  66%|██████▋   | 187/282 [01:31<00:37,  2.53it/s]11/28/2021 01:36:00 - INFO - __main__ -   Batch number = 188
Evaluating:  67%|██████▋   | 188/282 [01:32<00:48,  1.92it/s]11/28/2021 01:36:01 - INFO - __main__ -   Batch number = 189
Evaluating:  67%|██████▋   | 189/282 [01:33<00:46,  2.01it/s]11/28/2021 01:36:02 - INFO - __main__ -   Batch number = 190
Evaluating:  67%|██████▋   | 190/282 [01:33<00:46,  1.96it/s]11/28/2021 01:36:02 - INFO - __main__ -   Batch number = 191
Evaluating:  68%|██████▊   | 191/282 [01:34<00:46,  1.96it/s]11/28/2021 01:36:03 - INFO - __main__ -   Batch number = 192
Evaluating:  68%|██████▊   | 192/282 [01:35<01:03,  1.42it/s]11/28/2021 01:36:04 - INFO - __main__ -   Batch number = 193
Evaluating:  68%|██████▊   | 193/282 [01:35<00:57,  1.54it/s]11/28/2021 01:36:04 - INFO - __main__ -   Batch number = 194
Evaluating:  69%|██████▉   | 194/282 [01:36<00:58,  1.51it/s]11/28/2021 01:36:05 - INFO - __main__ -   Batch number = 195
Evaluating:  69%|██████▉   | 195/282 [01:36<00:53,  1.61it/s]11/28/2021 01:36:06 - INFO - __main__ -   Batch number = 196
Evaluating:  70%|██████▉   | 196/282 [01:37<00:55,  1.55it/s]11/28/2021 01:36:06 - INFO - __main__ -   Batch number = 197
Evaluating:  70%|██████▉   | 197/282 [01:38<00:51,  1.65it/s]11/28/2021 01:36:07 - INFO - __main__ -   Batch number = 198
Evaluating:  70%|███████   | 198/282 [01:39<00:57,  1.47it/s]11/28/2021 01:36:08 - INFO - __main__ -   Batch number = 199
Evaluating:  71%|███████   | 199/282 [01:39<00:52,  1.58it/s]11/28/2021 01:36:08 - INFO - __main__ -   Batch number = 200
Evaluating:  71%|███████   | 200/282 [01:41<01:12,  1.14it/s]11/28/2021 01:36:10 - INFO - __main__ -   Batch number = 201
Evaluating:  71%|███████▏  | 201/282 [01:41<01:05,  1.24it/s]11/28/2021 01:36:10 - INFO - __main__ -   Batch number = 202
Evaluating:  72%|███████▏  | 202/282 [01:42<00:58,  1.38it/s]11/28/2021 01:36:11 - INFO - __main__ -   Batch number = 203
Evaluating:  72%|███████▏  | 203/282 [01:42<00:52,  1.51it/s]11/28/2021 01:36:11 - INFO - __main__ -   Batch number = 204
Evaluating:  72%|███████▏  | 204/282 [01:43<00:53,  1.47it/s]11/28/2021 01:36:12 - INFO - __main__ -   Batch number = 205
Evaluating:  73%|███████▎  | 205/282 [01:43<00:48,  1.58it/s]11/28/2021 01:36:13 - INFO - __main__ -   Batch number = 206
Evaluating:  73%|███████▎  | 206/282 [01:44<00:48,  1.57it/s]11/28/2021 01:36:13 - INFO - __main__ -   Batch number = 207
Evaluating:  73%|███████▎  | 207/282 [01:45<00:45,  1.66it/s]11/28/2021 01:36:14 - INFO - __main__ -   Batch number = 208
Evaluating:  74%|███████▍  | 208/282 [01:45<00:49,  1.50it/s]11/28/2021 01:36:15 - INFO - __main__ -   Batch number = 209
Evaluating:  74%|███████▍  | 209/282 [01:46<00:45,  1.61it/s]11/28/2021 01:36:15 - INFO - __main__ -   Batch number = 210
Evaluating:  74%|███████▍  | 210/282 [01:47<00:52,  1.37it/s]11/28/2021 01:36:16 - INFO - __main__ -   Batch number = 211
Evaluating:  75%|███████▍  | 211/282 [01:47<00:47,  1.50it/s]11/28/2021 01:36:17 - INFO - __main__ -   Batch number = 212
Evaluating:  75%|███████▌  | 212/282 [01:48<00:52,  1.34it/s]11/28/2021 01:36:18 - INFO - __main__ -   Batch number = 213
Evaluating:  76%|███████▌  | 213/282 [01:49<00:46,  1.47it/s]11/28/2021 01:36:18 - INFO - __main__ -   Batch number = 214
Evaluating:  76%|███████▌  | 214/282 [01:50<00:53,  1.27it/s]11/28/2021 01:36:19 - INFO - __main__ -   Batch number = 215
Evaluating:  76%|███████▌  | 215/282 [01:50<00:47,  1.41it/s]11/28/2021 01:36:20 - INFO - __main__ -   Batch number = 216
Evaluating:  77%|███████▋  | 216/282 [01:51<00:48,  1.37it/s]11/28/2021 01:36:20 - INFO - __main__ -   Batch number = 217
Evaluating:  77%|███████▋  | 217/282 [01:52<00:43,  1.49it/s]11/28/2021 01:36:21 - INFO - __main__ -   Batch number = 218
Evaluating:  77%|███████▋  | 218/282 [01:53<00:49,  1.29it/s]11/28/2021 01:36:22 - INFO - __main__ -   Batch number = 219
Evaluating:  78%|███████▊  | 219/282 [01:53<00:44,  1.43it/s]11/28/2021 01:36:22 - INFO - __main__ -   Batch number = 220
Evaluating:  78%|███████▊  | 220/282 [01:54<00:50,  1.22it/s]11/28/2021 01:36:24 - INFO - __main__ -   Batch number = 221
Evaluating:  78%|███████▊  | 221/282 [01:55<00:45,  1.33it/s]11/28/2021 01:36:24 - INFO - __main__ -   Batch number = 222
Evaluating:  79%|███████▊  | 222/282 [01:56<00:53,  1.13it/s]11/28/2021 01:36:25 - INFO - __main__ -   Batch number = 223
Evaluating:  79%|███████▉  | 223/282 [01:57<00:48,  1.21it/s]11/28/2021 01:36:26 - INFO - __main__ -   Batch number = 224
Evaluating:  79%|███████▉  | 224/282 [01:58<00:52,  1.09it/s]11/28/2021 01:36:27 - INFO - __main__ -   Batch number = 225
Evaluating:  80%|███████▉  | 225/282 [01:59<00:48,  1.18it/s]11/28/2021 01:36:28 - INFO - __main__ -   Batch number = 226
Evaluating:  80%|████████  | 226/282 [02:00<00:49,  1.13it/s]11/28/2021 01:36:29 - INFO - __main__ -   Batch number = 227
Evaluating:  80%|████████  | 227/282 [02:00<00:45,  1.22it/s]11/28/2021 01:36:29 - INFO - __main__ -   Batch number = 228
Evaluating:  81%|████████  | 228/282 [02:01<00:45,  1.17it/s]11/28/2021 01:36:30 - INFO - __main__ -   Batch number = 229
Evaluating:  81%|████████  | 229/282 [02:02<00:42,  1.25it/s]11/28/2021 01:36:31 - INFO - __main__ -   Batch number = 230
Evaluating:  82%|████████▏ | 230/282 [02:03<00:43,  1.19it/s]11/28/2021 01:36:32 - INFO - __main__ -   Batch number = 231
Evaluating:  82%|████████▏ | 231/282 [02:04<00:40,  1.26it/s]11/28/2021 01:36:33 - INFO - __main__ -   Batch number = 232
Evaluating:  82%|████████▏ | 232/282 [02:04<00:41,  1.20it/s]11/28/2021 01:36:34 - INFO - __main__ -   Batch number = 233
Evaluating:  83%|████████▎ | 233/282 [02:05<00:38,  1.27it/s]11/28/2021 01:36:34 - INFO - __main__ -   Batch number = 234
Evaluating:  83%|████████▎ | 234/282 [02:07<00:50,  1.05s/it]11/28/2021 01:36:36 - INFO - __main__ -   Batch number = 235
Evaluating:  83%|████████▎ | 235/282 [02:08<00:44,  1.06it/s]11/28/2021 01:36:37 - INFO - __main__ -   Batch number = 236
Evaluating:  84%|████████▎ | 236/282 [02:08<00:43,  1.07it/s]11/28/2021 01:36:38 - INFO - __main__ -   Batch number = 237
Evaluating:  84%|████████▍ | 237/282 [02:09<00:38,  1.16it/s]11/28/2021 01:36:38 - INFO - __main__ -   Batch number = 238
Evaluating:  84%|████████▍ | 238/282 [02:10<00:41,  1.07it/s]11/28/2021 01:36:39 - INFO - __main__ -   Batch number = 239
Evaluating:  85%|████████▍ | 239/282 [02:11<00:36,  1.16it/s]11/28/2021 01:36:40 - INFO - __main__ -   Batch number = 240
Evaluating:  85%|████████▌ | 240/282 [02:12<00:42,  1.00s/it]11/28/2021 01:36:41 - INFO - __main__ -   Batch number = 241
Evaluating:  85%|████████▌ | 241/282 [02:13<00:37,  1.10it/s]11/28/2021 01:36:42 - INFO - __main__ -   Batch number = 242
Evaluating:  86%|████████▌ | 242/282 [02:15<00:45,  1.14s/it]11/28/2021 01:36:44 - INFO - __main__ -   Batch number = 243
Evaluating:  86%|████████▌ | 243/282 [02:15<00:37,  1.05it/s]11/28/2021 01:36:44 - INFO - __main__ -   Batch number = 244
Evaluating:  87%|████████▋ | 244/282 [02:16<00:39,  1.04s/it]11/28/2021 01:36:46 - INFO - __main__ -   Batch number = 245
Evaluating:  87%|████████▋ | 245/282 [02:17<00:32,  1.13it/s]11/28/2021 01:36:46 - INFO - __main__ -   Batch number = 246
Evaluating:  87%|████████▋ | 246/282 [02:18<00:31,  1.15it/s]11/28/2021 01:36:47 - INFO - __main__ -   Batch number = 247
Evaluating:  88%|████████▊ | 247/282 [02:18<00:26,  1.31it/s]11/28/2021 01:36:47 - INFO - __main__ -   Batch number = 248
Evaluating:  88%|████████▊ | 248/282 [02:19<00:26,  1.27it/s]11/28/2021 01:36:48 - INFO - __main__ -   Batch number = 249
Evaluating:  88%|████████▊ | 249/282 [02:20<00:23,  1.42it/s]11/28/2021 01:36:49 - INFO - __main__ -   Batch number = 250
Evaluating:  89%|████████▊ | 250/282 [02:20<00:21,  1.49it/s]11/28/2021 01:36:49 - INFO - __main__ -   Batch number = 251
Evaluating:  89%|████████▉ | 251/282 [02:21<00:19,  1.60it/s]11/28/2021 01:36:50 - INFO - __main__ -   Batch number = 252
Evaluating:  89%|████████▉ | 252/282 [02:22<00:27,  1.10it/s]11/28/2021 01:36:51 - INFO - __main__ -   Batch number = 253
Evaluating:  90%|████████▉ | 253/282 [02:23<00:21,  1.35it/s]11/28/2021 01:36:52 - INFO - __main__ -   Batch number = 254
Evaluating:  90%|█████████ | 254/282 [02:24<00:23,  1.17it/s]11/28/2021 01:36:53 - INFO - __main__ -   Batch number = 255
Evaluating:  90%|█████████ | 255/282 [02:24<00:19,  1.42it/s]11/28/2021 01:36:53 - INFO - __main__ -   Batch number = 256
Evaluating:  91%|█████████ | 256/282 [02:24<00:15,  1.67it/s]11/28/2021 01:36:54 - INFO - __main__ -   Batch number = 257
Evaluating:  91%|█████████ | 257/282 [02:25<00:13,  1.91it/s]11/28/2021 01:36:54 - INFO - __main__ -   Batch number = 258
Evaluating:  91%|█████████▏| 258/282 [02:27<00:22,  1.08it/s]11/28/2021 01:36:56 - INFO - __main__ -   Batch number = 259
Evaluating:  92%|█████████▏| 259/282 [02:27<00:17,  1.30it/s]11/28/2021 01:36:56 - INFO - __main__ -   Batch number = 260
Evaluating:  92%|█████████▏| 260/282 [02:28<00:17,  1.22it/s]11/28/2021 01:36:57 - INFO - __main__ -   Batch number = 261
Evaluating:  93%|█████████▎| 261/282 [02:29<00:20,  1.00it/s]11/28/2021 01:36:59 - INFO - __main__ -   Batch number = 262
Evaluating:  93%|█████████▎| 262/282 [02:30<00:19,  1.04it/s]11/28/2021 01:36:59 - INFO - __main__ -   Batch number = 263
Evaluating:  93%|█████████▎| 263/282 [02:32<00:22,  1.20s/it]11/28/2021 01:37:01 - INFO - __main__ -   Batch number = 264
Evaluating:  94%|█████████▎| 264/282 [02:33<00:18,  1.02s/it]11/28/2021 01:37:02 - INFO - __main__ -   Batch number = 265
Evaluating:  94%|█████████▍| 265/282 [02:35<00:21,  1.28s/it]11/28/2021 01:37:04 - INFO - __main__ -   Batch number = 266
Evaluating:  94%|█████████▍| 266/282 [02:36<00:22,  1.43s/it]11/28/2021 01:37:05 - INFO - __main__ -   Batch number = 267
Evaluating:  95%|█████████▍| 267/282 [02:38<00:20,  1.35s/it]11/28/2021 01:37:07 - INFO - __main__ -   Batch number = 268
Evaluating:  95%|█████████▌| 268/282 [02:38<00:15,  1.09s/it]11/28/2021 01:37:07 - INFO - __main__ -   Batch number = 269
Evaluating:  95%|█████████▌| 269/282 [02:39<00:13,  1.00s/it]11/28/2021 01:37:08 - INFO - __main__ -   Batch number = 270
Evaluating:  96%|█████████▌| 270/282 [02:40<00:11,  1.00it/s]11/28/2021 01:37:09 - INFO - __main__ -   Batch number = 271
Evaluating:  96%|█████████▌| 271/282 [02:41<00:11,  1.03s/it]11/28/2021 01:37:10 - INFO - __main__ -   Batch number = 272
Evaluating:  96%|█████████▋| 272/282 [02:42<00:09,  1.09it/s]11/28/2021 01:37:11 - INFO - __main__ -   Batch number = 273
Evaluating:  97%|█████████▋| 273/282 [02:43<00:08,  1.04it/s]11/28/2021 01:37:12 - INFO - __main__ -   Batch number = 274
Evaluating:  97%|█████████▋| 274/282 [02:44<00:08,  1.02s/it]11/28/2021 01:37:13 - INFO - __main__ -   Batch number = 275
Evaluating:  98%|█████████▊| 275/282 [02:44<00:06,  1.09it/s]11/28/2021 01:37:14 - INFO - __main__ -   Batch number = 276
Evaluating:  98%|█████████▊| 276/282 [02:45<00:05,  1.11it/s]11/28/2021 01:37:14 - INFO - __main__ -   Batch number = 277
Evaluating:  98%|█████████▊| 277/282 [02:46<00:04,  1.21it/s]11/28/2021 01:37:15 - INFO - __main__ -   Batch number = 278
Evaluating:  99%|█████████▊| 278/282 [02:47<00:03,  1.28it/s]11/28/2021 01:37:16 - INFO - __main__ -   Batch number = 279
Evaluating:  99%|█████████▉| 279/282 [02:47<00:02,  1.25it/s]11/28/2021 01:37:17 - INFO - __main__ -   Batch number = 280
Evaluating:  99%|█████████▉| 280/282 [02:51<00:03,  1.53s/it]11/28/2021 01:37:20 - INFO - __main__ -   Batch number = 281
Evaluating: 100%|█████████▉| 281/282 [02:52<00:01,  1.32s/it]11/28/2021 01:37:21 - INFO - __main__ -   Batch number = 282
Evaluating: 100%|██████████| 282/282 [02:52<00:00,  1.18s/it]Evaluating: 100%|██████████| 282/282 [02:52<00:00,  1.63it/s]
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:37:26 - INFO - __main__ -   ***** Evaluation result  in ru *****
11/28/2021 01:37:26 - INFO - __main__ -     f1 = 0.8662336855618517
11/28/2021 01:37:26 - INFO - __main__ -     loss = 0.4942243995924368
11/28/2021 01:37:26 - INFO - __main__ -     precision = 0.8715089229914589
11/28/2021 01:37:26 - INFO - __main__ -     recall = 0.8610219258527483
118.47user 81.56system 3:21.25elapsed 99%CPU (0avgtext+0avgdata 3984472maxresident)k
8inputs+1840outputs (0major+3131064minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:44:00 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='pt', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:44:00 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:44:00 - INFO - __main__ -   Seed = 1
11/28/2021 01:44:00 - INFO - root -   save model
11/28/2021 01:44:00 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='pt', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:44:00 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:44:03 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:44:09 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:44:09 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:44:09 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
11/28/2021 01:44:09 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:44:09 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:44:10 - INFO - root -   loading lang adpater cs/wiki@ukp
11/28/2021 01:44:10 - INFO - __main__ -   Adapter Languages : ['cs'], Length : 1
11/28/2021 01:44:10 - INFO - __main__ -   Adapter Names ['cs/wiki@ukp'], Length : 1
11/28/2021 01:44:10 - INFO - __main__ -   Language = cs
11/28/2021 01:44:10 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/cs/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_cs_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/adapter_config.json
Adding adapter 'cs' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/head_config.json
11/28/2021 01:44:22 - INFO - __main__ -   Language adapter for pt not found, using cs instead
11/28/2021 01:44:22 - INFO - __main__ -   Set active language adapter to cs
11/28/2021 01:44:22 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:44:22 - INFO - __main__ -   Adapter Languages = ['cs']
11/28/2021 01:44:22 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_pt_bert-base-multilingual-cased_128
11/28/2021 01:44:23 - INFO - __main__ -   ***** Running evaluation  in pt *****
11/28/2021 01:44:23 - INFO - __main__ -     Num examples = 2682
11/28/2021 01:44:23 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/84 [00:00<?, ?it/s]11/28/2021 01:44:23 - INFO - __main__ -   Batch number = 1
Evaluating:   1%|          | 1/84 [00:00<00:49,  1.67it/s]11/28/2021 01:44:23 - INFO - __main__ -   Batch number = 2
Evaluating:   2%|▏         | 2/84 [00:01<00:43,  1.87it/s]11/28/2021 01:44:24 - INFO - __main__ -   Batch number = 3
Evaluating:   4%|▎         | 3/84 [00:01<00:41,  1.94it/s]11/28/2021 01:44:24 - INFO - __main__ -   Batch number = 4
Evaluating:   5%|▍         | 4/84 [00:02<00:40,  1.98it/s]11/28/2021 01:44:25 - INFO - __main__ -   Batch number = 5
Evaluating:   6%|▌         | 5/84 [00:02<00:39,  1.99it/s]11/28/2021 01:44:25 - INFO - __main__ -   Batch number = 6
Evaluating:   7%|▋         | 6/84 [00:03<00:38,  2.01it/s]11/28/2021 01:44:26 - INFO - __main__ -   Batch number = 7
Evaluating:   8%|▊         | 7/84 [00:03<00:36,  2.12it/s]11/28/2021 01:44:26 - INFO - __main__ -   Batch number = 8
Evaluating:  10%|▉         | 8/84 [00:03<00:32,  2.33it/s]11/28/2021 01:44:26 - INFO - __main__ -   Batch number = 9
Evaluating:  11%|█         | 9/84 [00:04<00:30,  2.50it/s]11/28/2021 01:44:27 - INFO - __main__ -   Batch number = 10
Evaluating:  12%|█▏        | 10/84 [00:04<00:28,  2.63it/s]11/28/2021 01:44:27 - INFO - __main__ -   Batch number = 11
Evaluating:  13%|█▎        | 11/84 [00:04<00:26,  2.73it/s]11/28/2021 01:44:27 - INFO - __main__ -   Batch number = 12
Evaluating:  14%|█▍        | 12/84 [00:05<00:25,  2.82it/s]11/28/2021 01:44:28 - INFO - __main__ -   Batch number = 13
Evaluating:  15%|█▌        | 13/84 [00:05<00:24,  2.88it/s]11/28/2021 01:44:28 - INFO - __main__ -   Batch number = 14
Evaluating:  17%|█▋        | 14/84 [00:05<00:24,  2.91it/s]11/28/2021 01:44:28 - INFO - __main__ -   Batch number = 15
Evaluating:  18%|█▊        | 15/84 [00:06<00:23,  2.93it/s]11/28/2021 01:44:29 - INFO - __main__ -   Batch number = 16
Evaluating:  19%|█▉        | 16/84 [00:06<00:23,  2.95it/s]11/28/2021 01:44:29 - INFO - __main__ -   Batch number = 17
Evaluating:  20%|██        | 17/84 [00:06<00:22,  2.96it/s]11/28/2021 01:44:29 - INFO - __main__ -   Batch number = 18
Evaluating:  21%|██▏       | 18/84 [00:07<00:22,  2.97it/s]11/28/2021 01:44:30 - INFO - __main__ -   Batch number = 19
Evaluating:  23%|██▎       | 19/84 [00:07<00:21,  2.98it/s]11/28/2021 01:44:30 - INFO - __main__ -   Batch number = 20
Evaluating:  24%|██▍       | 20/84 [00:07<00:21,  2.99it/s]11/28/2021 01:44:30 - INFO - __main__ -   Batch number = 21
Evaluating:  25%|██▌       | 21/84 [00:08<00:23,  2.63it/s]11/28/2021 01:44:31 - INFO - __main__ -   Batch number = 22
Evaluating:  26%|██▌       | 22/84 [00:08<00:22,  2.75it/s]11/28/2021 01:44:31 - INFO - __main__ -   Batch number = 23
Evaluating:  27%|██▋       | 23/84 [00:08<00:21,  2.82it/s]11/28/2021 01:44:31 - INFO - __main__ -   Batch number = 24
Evaluating:  29%|██▊       | 24/84 [00:09<00:20,  2.88it/s]11/28/2021 01:44:32 - INFO - __main__ -   Batch number = 25
Evaluating:  30%|██▉       | 25/84 [00:09<00:20,  2.91it/s]11/28/2021 01:44:32 - INFO - __main__ -   Batch number = 26
Evaluating:  31%|███       | 26/84 [00:09<00:19,  2.95it/s]11/28/2021 01:44:32 - INFO - __main__ -   Batch number = 27
Evaluating:  32%|███▏      | 27/84 [00:10<00:19,  2.98it/s]11/28/2021 01:44:33 - INFO - __main__ -   Batch number = 28
Evaluating:  33%|███▎      | 28/84 [00:10<00:18,  3.00it/s]11/28/2021 01:44:33 - INFO - __main__ -   Batch number = 29
Evaluating:  35%|███▍      | 29/84 [00:10<00:18,  3.01it/s]11/28/2021 01:44:33 - INFO - __main__ -   Batch number = 30
Evaluating:  36%|███▌      | 30/84 [00:11<00:17,  3.02it/s]11/28/2021 01:44:34 - INFO - __main__ -   Batch number = 31
Evaluating:  37%|███▋      | 31/84 [00:11<00:17,  3.01it/s]11/28/2021 01:44:34 - INFO - __main__ -   Batch number = 32
Evaluating:  38%|███▊      | 32/84 [00:11<00:17,  3.01it/s]11/28/2021 01:44:34 - INFO - __main__ -   Batch number = 33
Evaluating:  39%|███▉      | 33/84 [00:12<00:16,  3.01it/s]11/28/2021 01:44:35 - INFO - __main__ -   Batch number = 34
Evaluating:  40%|████      | 34/84 [00:12<00:16,  3.01it/s]11/28/2021 01:44:35 - INFO - __main__ -   Batch number = 35
Evaluating:  42%|████▏     | 35/84 [00:12<00:14,  3.35it/s]11/28/2021 01:44:35 - INFO - __main__ -   Batch number = 36
Evaluating:  43%|████▎     | 36/84 [00:13<00:14,  3.23it/s]11/28/2021 01:44:36 - INFO - __main__ -   Batch number = 37
Evaluating:  44%|████▍     | 37/84 [00:13<00:14,  3.17it/s]11/28/2021 01:44:36 - INFO - __main__ -   Batch number = 38
Evaluating:  45%|████▌     | 38/84 [00:13<00:14,  3.10it/s]11/28/2021 01:44:36 - INFO - __main__ -   Batch number = 39
Evaluating:  46%|████▋     | 39/84 [00:14<00:14,  3.06it/s]11/28/2021 01:44:37 - INFO - __main__ -   Batch number = 40
Evaluating:  48%|████▊     | 40/84 [00:14<00:14,  3.04it/s]11/28/2021 01:44:37 - INFO - __main__ -   Batch number = 41
Evaluating:  49%|████▉     | 41/84 [00:14<00:14,  3.02it/s]11/28/2021 01:44:37 - INFO - __main__ -   Batch number = 42
Evaluating:  50%|█████     | 42/84 [00:15<00:14,  2.99it/s]11/28/2021 01:44:38 - INFO - __main__ -   Batch number = 43
Evaluating:  51%|█████     | 43/84 [00:15<00:13,  2.99it/s]11/28/2021 01:44:38 - INFO - __main__ -   Batch number = 44
Evaluating:  52%|█████▏    | 44/84 [00:15<00:13,  2.98it/s]11/28/2021 01:44:38 - INFO - __main__ -   Batch number = 45
Evaluating:  54%|█████▎    | 45/84 [00:16<00:13,  2.99it/s]11/28/2021 01:44:39 - INFO - __main__ -   Batch number = 46
Evaluating:  55%|█████▍    | 46/84 [00:16<00:12,  2.98it/s]11/28/2021 01:44:39 - INFO - __main__ -   Batch number = 47
Evaluating:  56%|█████▌    | 47/84 [00:16<00:12,  2.98it/s]11/28/2021 01:44:39 - INFO - __main__ -   Batch number = 48
Evaluating:  57%|█████▋    | 48/84 [00:17<00:12,  2.98it/s]11/28/2021 01:44:40 - INFO - __main__ -   Batch number = 49
Evaluating:  58%|█████▊    | 49/84 [00:17<00:11,  2.98it/s]11/28/2021 01:44:40 - INFO - __main__ -   Batch number = 50
Evaluating:  60%|█████▉    | 50/84 [00:17<00:11,  2.92it/s]11/28/2021 01:44:40 - INFO - __main__ -   Batch number = 51
Evaluating:  61%|██████    | 51/84 [00:18<00:11,  2.94it/s]11/28/2021 01:44:41 - INFO - __main__ -   Batch number = 52
Evaluating:  62%|██████▏   | 52/84 [00:18<00:10,  3.01it/s]11/28/2021 01:44:41 - INFO - __main__ -   Batch number = 53
Evaluating:  63%|██████▎   | 53/84 [00:18<00:10,  3.02it/s]11/28/2021 01:44:41 - INFO - __main__ -   Batch number = 54
Evaluating:  64%|██████▍   | 54/84 [00:19<00:09,  3.00it/s]11/28/2021 01:44:42 - INFO - __main__ -   Batch number = 55
Evaluating:  65%|██████▌   | 55/84 [00:19<00:09,  3.02it/s]11/28/2021 01:44:42 - INFO - __main__ -   Batch number = 56
Evaluating:  67%|██████▋   | 56/84 [00:19<00:09,  3.02it/s]11/28/2021 01:44:42 - INFO - __main__ -   Batch number = 57
Evaluating:  68%|██████▊   | 57/84 [00:20<00:08,  3.03it/s]11/28/2021 01:44:43 - INFO - __main__ -   Batch number = 58
Evaluating:  69%|██████▉   | 58/84 [00:20<00:08,  3.05it/s]11/28/2021 01:44:43 - INFO - __main__ -   Batch number = 59
Evaluating:  70%|███████   | 59/84 [00:20<00:08,  3.04it/s]11/28/2021 01:44:43 - INFO - __main__ -   Batch number = 60
Evaluating:  71%|███████▏  | 60/84 [00:21<00:07,  3.03it/s]11/28/2021 01:44:44 - INFO - __main__ -   Batch number = 61
Evaluating:  73%|███████▎  | 61/84 [00:21<00:07,  3.05it/s]11/28/2021 01:44:44 - INFO - __main__ -   Batch number = 62
Evaluating:  74%|███████▍  | 62/84 [00:21<00:07,  3.04it/s]11/28/2021 01:44:44 - INFO - __main__ -   Batch number = 63
Evaluating:  75%|███████▌  | 63/84 [00:22<00:06,  3.03it/s]11/28/2021 01:44:45 - INFO - __main__ -   Batch number = 64
Evaluating:  76%|███████▌  | 64/84 [00:22<00:06,  3.04it/s]11/28/2021 01:44:45 - INFO - __main__ -   Batch number = 65
Evaluating:  77%|███████▋  | 65/84 [00:22<00:05,  3.21it/s]11/28/2021 01:44:45 - INFO - __main__ -   Batch number = 66
Evaluating:  79%|███████▊  | 66/84 [00:23<00:05,  3.14it/s]11/28/2021 01:44:46 - INFO - __main__ -   Batch number = 67
Evaluating:  80%|███████▉  | 67/84 [00:23<00:05,  3.08it/s]11/28/2021 01:44:46 - INFO - __main__ -   Batch number = 68
Evaluating:  81%|████████  | 68/84 [00:23<00:05,  3.03it/s]11/28/2021 01:44:46 - INFO - __main__ -   Batch number = 69
Evaluating:  82%|████████▏ | 69/84 [00:24<00:05,  2.62it/s]11/28/2021 01:44:47 - INFO - __main__ -   Batch number = 70
Evaluating:  83%|████████▎ | 70/84 [00:24<00:05,  2.41it/s]11/28/2021 01:44:47 - INFO - __main__ -   Batch number = 71
Evaluating:  85%|████████▍ | 71/84 [00:25<00:05,  2.25it/s]11/28/2021 01:44:48 - INFO - __main__ -   Batch number = 72
Evaluating:  86%|████████▌ | 72/84 [00:25<00:05,  2.16it/s]11/28/2021 01:44:48 - INFO - __main__ -   Batch number = 73
Evaluating:  87%|████████▋ | 73/84 [00:26<00:05,  2.10it/s]11/28/2021 01:44:49 - INFO - __main__ -   Batch number = 74
Evaluating:  88%|████████▊ | 74/84 [00:26<00:04,  2.06it/s]11/28/2021 01:44:49 - INFO - __main__ -   Batch number = 75
Evaluating:  89%|████████▉ | 75/84 [00:27<00:04,  2.03it/s]11/28/2021 01:44:50 - INFO - __main__ -   Batch number = 76
Evaluating:  90%|█████████ | 76/84 [00:27<00:04,  1.90it/s]11/28/2021 01:44:50 - INFO - __main__ -   Batch number = 77
Evaluating:  92%|█████████▏| 77/84 [00:28<00:03,  1.77it/s]11/28/2021 01:44:51 - INFO - __main__ -   Batch number = 78
Evaluating:  93%|█████████▎| 78/84 [00:29<00:03,  1.67it/s]11/28/2021 01:44:52 - INFO - __main__ -   Batch number = 79
Evaluating:  94%|█████████▍| 79/84 [00:29<00:03,  1.61it/s]11/28/2021 01:44:52 - INFO - __main__ -   Batch number = 80
Evaluating:  95%|█████████▌| 80/84 [00:30<00:02,  1.57it/s]11/28/2021 01:44:53 - INFO - __main__ -   Batch number = 81
Evaluating:  96%|█████████▋| 81/84 [00:31<00:01,  1.55it/s]11/28/2021 01:44:54 - INFO - __main__ -   Batch number = 82
Evaluating:  98%|█████████▊| 82/84 [00:31<00:01,  1.54it/s]11/28/2021 01:44:54 - INFO - __main__ -   Batch number = 83
Evaluating:  99%|█████████▉| 83/84 [00:32<00:00,  1.53it/s]11/28/2021 01:44:55 - INFO - __main__ -   Batch number = 84
Evaluating: 100%|██████████| 84/84 [00:33<00:00,  1.59it/s]Evaluating: 100%|██████████| 84/84 [00:33<00:00,  2.53it/s]
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:44:57 - INFO - __main__ -   ***** Evaluation result  in pt *****
11/28/2021 01:44:57 - INFO - __main__ -     f1 = 0.8829247491493268
11/28/2021 01:44:57 - INFO - __main__ -     loss = 0.40794978184359415
11/28/2021 01:44:57 - INFO - __main__ -     precision = 0.8893719976156247
11/28/2021 01:44:57 - INFO - __main__ -     recall = 0.8765703029151043
44.61user 15.87system 1:00.32elapsed 100%CPU (0avgtext+0avgdata 3981796maxresident)k
16inputs+712outputs (0major+1891948minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:45:00 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='pt', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:45:00 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:45:00 - INFO - __main__ -   Seed = 2
11/28/2021 01:45:00 - INFO - root -   save model
11/28/2021 01:45:00 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='pt', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:45:00 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:45:03 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:45:09 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:45:09 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:45:09 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
11/28/2021 01:45:09 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:45:09 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:45:09 - INFO - root -   loading lang adpater cs/wiki@ukp
11/28/2021 01:45:09 - INFO - __main__ -   Adapter Languages : ['cs'], Length : 1
11/28/2021 01:45:09 - INFO - __main__ -   Adapter Names ['cs/wiki@ukp'], Length : 1
11/28/2021 01:45:09 - INFO - __main__ -   Language = cs
11/28/2021 01:45:09 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/cs/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_cs_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/adapter_config.json
Adding adapter 'cs' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/head_config.json
11/28/2021 01:45:21 - INFO - __main__ -   Language adapter for pt not found, using cs instead
11/28/2021 01:45:21 - INFO - __main__ -   Set active language adapter to cs
11/28/2021 01:45:21 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:45:21 - INFO - __main__ -   Adapter Languages = ['cs']
11/28/2021 01:45:21 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_pt_bert-base-multilingual-cased_128
11/28/2021 01:45:22 - INFO - __main__ -   ***** Running evaluation  in pt *****
11/28/2021 01:45:22 - INFO - __main__ -     Num examples = 2682
11/28/2021 01:45:22 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/84 [00:00<?, ?it/s]11/28/2021 01:45:22 - INFO - __main__ -   Batch number = 1
Evaluating:   1%|          | 1/84 [00:00<00:41,  2.02it/s]11/28/2021 01:45:22 - INFO - __main__ -   Batch number = 2
Evaluating:   2%|▏         | 2/84 [00:00<00:39,  2.06it/s]11/28/2021 01:45:23 - INFO - __main__ -   Batch number = 3
Evaluating:   4%|▎         | 3/84 [00:01<00:39,  2.05it/s]11/28/2021 01:45:23 - INFO - __main__ -   Batch number = 4
Evaluating:   5%|▍         | 4/84 [00:01<00:39,  2.04it/s]11/28/2021 01:45:24 - INFO - __main__ -   Batch number = 5
Evaluating:   6%|▌         | 5/84 [00:02<00:34,  2.31it/s]11/28/2021 01:45:24 - INFO - __main__ -   Batch number = 6
Evaluating:   7%|▋         | 6/84 [00:02<00:30,  2.52it/s]11/28/2021 01:45:25 - INFO - __main__ -   Batch number = 7
Evaluating:   8%|▊         | 7/84 [00:03<00:32,  2.38it/s]11/28/2021 01:45:25 - INFO - __main__ -   Batch number = 8
Evaluating:  10%|▉         | 8/84 [00:03<00:33,  2.26it/s]11/28/2021 01:45:26 - INFO - __main__ -   Batch number = 9
Evaluating:  11%|█         | 9/84 [00:04<00:34,  2.20it/s]11/28/2021 01:45:26 - INFO - __main__ -   Batch number = 10
Evaluating:  12%|█▏        | 10/84 [00:04<00:34,  2.15it/s]11/28/2021 01:45:27 - INFO - __main__ -   Batch number = 11
Evaluating:  13%|█▎        | 11/84 [00:05<00:34,  2.14it/s]11/28/2021 01:45:27 - INFO - __main__ -   Batch number = 12
Evaluating:  14%|█▍        | 12/84 [00:05<00:31,  2.31it/s]11/28/2021 01:45:27 - INFO - __main__ -   Batch number = 13
Evaluating:  15%|█▌        | 13/84 [00:05<00:28,  2.50it/s]11/28/2021 01:45:28 - INFO - __main__ -   Batch number = 14
Evaluating:  17%|█▋        | 14/84 [00:06<00:26,  2.64it/s]11/28/2021 01:45:28 - INFO - __main__ -   Batch number = 15
Evaluating:  18%|█▊        | 15/84 [00:06<00:25,  2.76it/s]11/28/2021 01:45:28 - INFO - __main__ -   Batch number = 16
Evaluating:  19%|█▉        | 16/84 [00:06<00:23,  2.85it/s]11/28/2021 01:45:29 - INFO - __main__ -   Batch number = 17
Evaluating:  20%|██        | 17/84 [00:07<00:23,  2.91it/s]11/28/2021 01:45:29 - INFO - __main__ -   Batch number = 18
Evaluating:  21%|██▏       | 18/84 [00:07<00:22,  2.94it/s]11/28/2021 01:45:29 - INFO - __main__ -   Batch number = 19
Evaluating:  23%|██▎       | 19/84 [00:07<00:21,  2.97it/s]11/28/2021 01:45:30 - INFO - __main__ -   Batch number = 20
Evaluating:  24%|██▍       | 20/84 [00:07<00:21,  3.03it/s]11/28/2021 01:45:30 - INFO - __main__ -   Batch number = 21
Evaluating:  25%|██▌       | 21/84 [00:08<00:20,  3.04it/s]11/28/2021 01:45:30 - INFO - __main__ -   Batch number = 22
Evaluating:  26%|██▌       | 22/84 [00:08<00:21,  2.88it/s]11/28/2021 01:45:31 - INFO - __main__ -   Batch number = 23
Evaluating:  27%|██▋       | 23/84 [00:08<00:20,  3.01it/s]11/28/2021 01:45:31 - INFO - __main__ -   Batch number = 24
Evaluating:  29%|██▊       | 24/84 [00:09<00:19,  3.10it/s]11/28/2021 01:45:31 - INFO - __main__ -   Batch number = 25
Evaluating:  30%|██▉       | 25/84 [00:09<00:18,  3.15it/s]11/28/2021 01:45:32 - INFO - __main__ -   Batch number = 26
Evaluating:  31%|███       | 26/84 [00:09<00:18,  3.16it/s]11/28/2021 01:45:32 - INFO - __main__ -   Batch number = 27
Evaluating:  32%|███▏      | 27/84 [00:10<00:18,  3.14it/s]11/28/2021 01:45:32 - INFO - __main__ -   Batch number = 28
Evaluating:  33%|███▎      | 28/84 [00:10<00:17,  3.12it/s]11/28/2021 01:45:33 - INFO - __main__ -   Batch number = 29
Evaluating:  35%|███▍      | 29/84 [00:10<00:17,  3.11it/s]11/28/2021 01:45:33 - INFO - __main__ -   Batch number = 30
Evaluating:  36%|███▌      | 30/84 [00:11<00:17,  3.12it/s]11/28/2021 01:45:33 - INFO - __main__ -   Batch number = 31
Evaluating:  37%|███▋      | 31/84 [00:11<00:16,  3.14it/s]11/28/2021 01:45:33 - INFO - __main__ -   Batch number = 32
Evaluating:  38%|███▊      | 32/84 [00:11<00:16,  3.19it/s]11/28/2021 01:45:34 - INFO - __main__ -   Batch number = 33
Evaluating:  39%|███▉      | 33/84 [00:12<00:16,  3.13it/s]11/28/2021 01:45:34 - INFO - __main__ -   Batch number = 34
Evaluating:  40%|████      | 34/84 [00:12<00:16,  3.10it/s]11/28/2021 01:45:34 - INFO - __main__ -   Batch number = 35
Evaluating:  42%|████▏     | 35/84 [00:12<00:15,  3.06it/s]11/28/2021 01:45:35 - INFO - __main__ -   Batch number = 36
Evaluating:  43%|████▎     | 36/84 [00:13<00:15,  3.02it/s]11/28/2021 01:45:35 - INFO - __main__ -   Batch number = 37
Evaluating:  44%|████▍     | 37/84 [00:13<00:15,  3.04it/s]11/28/2021 01:45:35 - INFO - __main__ -   Batch number = 38
Evaluating:  45%|████▌     | 38/84 [00:13<00:15,  3.05it/s]11/28/2021 01:45:36 - INFO - __main__ -   Batch number = 39
Evaluating:  46%|████▋     | 39/84 [00:14<00:14,  3.08it/s]11/28/2021 01:45:36 - INFO - __main__ -   Batch number = 40
Evaluating:  48%|████▊     | 40/84 [00:14<00:14,  3.08it/s]11/28/2021 01:45:36 - INFO - __main__ -   Batch number = 41
Evaluating:  49%|████▉     | 41/84 [00:14<00:14,  3.07it/s]11/28/2021 01:45:37 - INFO - __main__ -   Batch number = 42
Evaluating:  50%|█████     | 42/84 [00:15<00:13,  3.07it/s]11/28/2021 01:45:37 - INFO - __main__ -   Batch number = 43
Evaluating:  51%|█████     | 43/84 [00:15<00:13,  3.06it/s]11/28/2021 01:45:37 - INFO - __main__ -   Batch number = 44
Evaluating:  52%|█████▏    | 44/84 [00:15<00:13,  3.05it/s]11/28/2021 01:45:38 - INFO - __main__ -   Batch number = 45
Evaluating:  54%|█████▎    | 45/84 [00:16<00:12,  3.06it/s]11/28/2021 01:45:38 - INFO - __main__ -   Batch number = 46
Evaluating:  55%|█████▍    | 46/84 [00:16<00:12,  3.05it/s]11/28/2021 01:45:38 - INFO - __main__ -   Batch number = 47
Evaluating:  56%|█████▌    | 47/84 [00:16<00:12,  3.05it/s]11/28/2021 01:45:39 - INFO - __main__ -   Batch number = 48
Evaluating:  57%|█████▋    | 48/84 [00:17<00:11,  3.07it/s]11/28/2021 01:45:39 - INFO - __main__ -   Batch number = 49
Evaluating:  58%|█████▊    | 49/84 [00:17<00:11,  3.08it/s]11/28/2021 01:45:39 - INFO - __main__ -   Batch number = 50
Evaluating:  60%|█████▉    | 50/84 [00:17<00:11,  3.07it/s]11/28/2021 01:45:40 - INFO - __main__ -   Batch number = 51
Evaluating:  61%|██████    | 51/84 [00:18<00:10,  3.07it/s]11/28/2021 01:45:40 - INFO - __main__ -   Batch number = 52
Evaluating:  62%|██████▏   | 52/84 [00:18<00:10,  3.08it/s]11/28/2021 01:45:40 - INFO - __main__ -   Batch number = 53
Evaluating:  63%|██████▎   | 53/84 [00:18<00:10,  3.05it/s]11/28/2021 01:45:41 - INFO - __main__ -   Batch number = 54
Evaluating:  64%|██████▍   | 54/84 [00:19<00:09,  3.02it/s]11/28/2021 01:45:41 - INFO - __main__ -   Batch number = 55
Evaluating:  65%|██████▌   | 55/84 [00:19<00:09,  3.00it/s]11/28/2021 01:45:41 - INFO - __main__ -   Batch number = 56
Evaluating:  67%|██████▋   | 56/84 [00:19<00:10,  2.77it/s]11/28/2021 01:45:42 - INFO - __main__ -   Batch number = 57
Evaluating:  68%|██████▊   | 57/84 [00:20<00:10,  2.49it/s]11/28/2021 01:45:42 - INFO - __main__ -   Batch number = 58
Evaluating:  69%|██████▉   | 58/84 [00:20<00:11,  2.32it/s]11/28/2021 01:45:43 - INFO - __main__ -   Batch number = 59
Evaluating:  70%|███████   | 59/84 [00:21<00:11,  2.22it/s]11/28/2021 01:45:43 - INFO - __main__ -   Batch number = 60
Evaluating:  71%|███████▏  | 60/84 [00:21<00:11,  2.15it/s]11/28/2021 01:45:44 - INFO - __main__ -   Batch number = 61
Evaluating:  73%|███████▎  | 61/84 [00:22<00:10,  2.14it/s]11/28/2021 01:45:44 - INFO - __main__ -   Batch number = 62
Evaluating:  74%|███████▍  | 62/84 [00:22<00:10,  2.12it/s]11/28/2021 01:45:45 - INFO - __main__ -   Batch number = 63
Evaluating:  75%|███████▌  | 63/84 [00:23<00:10,  2.09it/s]11/28/2021 01:45:45 - INFO - __main__ -   Batch number = 64
Evaluating:  76%|███████▌  | 64/84 [00:23<00:09,  2.07it/s]11/28/2021 01:45:46 - INFO - __main__ -   Batch number = 65
Evaluating:  77%|███████▋  | 65/84 [00:24<00:09,  2.06it/s]11/28/2021 01:45:46 - INFO - __main__ -   Batch number = 66
Evaluating:  79%|███████▊  | 66/84 [00:24<00:08,  2.04it/s]11/28/2021 01:45:47 - INFO - __main__ -   Batch number = 67
Evaluating:  80%|███████▉  | 67/84 [00:25<00:08,  2.03it/s]11/28/2021 01:45:47 - INFO - __main__ -   Batch number = 68
Evaluating:  81%|████████  | 68/84 [00:25<00:07,  2.02it/s]11/28/2021 01:45:48 - INFO - __main__ -   Batch number = 69
Evaluating:  82%|████████▏ | 69/84 [00:26<00:07,  2.03it/s]11/28/2021 01:45:48 - INFO - __main__ -   Batch number = 70
Evaluating:  83%|████████▎ | 70/84 [00:26<00:06,  2.01it/s]11/28/2021 01:45:49 - INFO - __main__ -   Batch number = 71
Evaluating:  85%|████████▍ | 71/84 [00:27<00:06,  2.01it/s]11/28/2021 01:45:49 - INFO - __main__ -   Batch number = 72
Evaluating:  86%|████████▌ | 72/84 [00:27<00:05,  2.01it/s]11/28/2021 01:45:50 - INFO - __main__ -   Batch number = 73
Evaluating:  87%|████████▋ | 73/84 [00:28<00:05,  2.00it/s]11/28/2021 01:45:50 - INFO - __main__ -   Batch number = 74
Evaluating:  88%|████████▊ | 74/84 [00:28<00:04,  2.17it/s]11/28/2021 01:45:51 - INFO - __main__ -   Batch number = 75
Evaluating:  89%|████████▉ | 75/84 [00:29<00:04,  2.13it/s]11/28/2021 01:45:51 - INFO - __main__ -   Batch number = 76
Evaluating:  90%|█████████ | 76/84 [00:29<00:03,  2.08it/s]11/28/2021 01:45:52 - INFO - __main__ -   Batch number = 77
Evaluating:  92%|█████████▏| 77/84 [00:30<00:03,  2.04it/s]11/28/2021 01:45:52 - INFO - __main__ -   Batch number = 78
Evaluating:  93%|█████████▎| 78/84 [00:30<00:02,  2.01it/s]11/28/2021 01:45:53 - INFO - __main__ -   Batch number = 79
Evaluating:  94%|█████████▍| 79/84 [00:31<00:02,  2.00it/s]11/28/2021 01:45:53 - INFO - __main__ -   Batch number = 80
Evaluating:  95%|█████████▌| 80/84 [00:31<00:02,  2.00it/s]11/28/2021 01:45:54 - INFO - __main__ -   Batch number = 81
Evaluating:  96%|█████████▋| 81/84 [00:32<00:01,  2.01it/s]11/28/2021 01:45:54 - INFO - __main__ -   Batch number = 82
Evaluating:  98%|█████████▊| 82/84 [00:32<00:00,  2.01it/s]11/28/2021 01:45:55 - INFO - __main__ -   Batch number = 83
Evaluating:  99%|█████████▉| 83/84 [00:33<00:00,  2.01it/s]11/28/2021 01:45:55 - INFO - __main__ -   Batch number = 84
Evaluating: 100%|██████████| 84/84 [00:33<00:00,  2.13it/s]Evaluating: 100%|██████████| 84/84 [00:33<00:00,  2.51it/s]
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:45:57 - INFO - __main__ -   ***** Evaluation result  in pt *****
11/28/2021 01:45:57 - INFO - __main__ -     f1 = 0.8785703082794555
11/28/2021 01:45:57 - INFO - __main__ -     loss = 0.4400281913223721
11/28/2021 01:45:57 - INFO - __main__ -     precision = 0.8861059847086739
11/28/2021 01:45:57 - INFO - __main__ -     recall = 0.8711617217604672
43.97user 15.86system 0:59.88elapsed 99%CPU (0avgtext+0avgdata 3997108maxresident)k
0inputs+720outputs (0major+1542485minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:46:00 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='pt', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:46:00 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:46:00 - INFO - __main__ -   Seed = 3
11/28/2021 01:46:00 - INFO - root -   save model
11/28/2021 01:46:00 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='pt', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:46:00 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:46:03 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:46:09 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:46:09 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:46:09 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
11/28/2021 01:46:09 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:46:09 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:46:09 - INFO - root -   loading lang adpater cs/wiki@ukp
11/28/2021 01:46:09 - INFO - __main__ -   Adapter Languages : ['cs'], Length : 1
11/28/2021 01:46:09 - INFO - __main__ -   Adapter Names ['cs/wiki@ukp'], Length : 1
11/28/2021 01:46:09 - INFO - __main__ -   Language = cs
11/28/2021 01:46:09 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/cs/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_cs_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/adapter_config.json
Adding adapter 'cs' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/head_config.json
11/28/2021 01:46:20 - INFO - __main__ -   Language adapter for pt not found, using cs instead
11/28/2021 01:46:20 - INFO - __main__ -   Set active language adapter to cs
11/28/2021 01:46:20 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:46:20 - INFO - __main__ -   Adapter Languages = ['cs']
11/28/2021 01:46:20 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_pt_bert-base-multilingual-cased_128
11/28/2021 01:46:21 - INFO - __main__ -   ***** Running evaluation  in pt *****
11/28/2021 01:46:21 - INFO - __main__ -     Num examples = 2682
11/28/2021 01:46:21 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/84 [00:00<?, ?it/s]11/28/2021 01:46:21 - INFO - __main__ -   Batch number = 1
Evaluating:   1%|          | 1/84 [00:00<00:31,  2.68it/s]11/28/2021 01:46:21 - INFO - __main__ -   Batch number = 2
Evaluating:   2%|▏         | 2/84 [00:00<00:28,  2.88it/s]11/28/2021 01:46:21 - INFO - __main__ -   Batch number = 3
Evaluating:   4%|▎         | 3/84 [00:01<00:27,  2.96it/s]11/28/2021 01:46:22 - INFO - __main__ -   Batch number = 4
Evaluating:   5%|▍         | 4/84 [00:01<00:26,  3.00it/s]11/28/2021 01:46:22 - INFO - __main__ -   Batch number = 5
Evaluating:   6%|▌         | 5/84 [00:01<00:26,  3.02it/s]11/28/2021 01:46:22 - INFO - __main__ -   Batch number = 6
Evaluating:   7%|▋         | 6/84 [00:02<00:25,  3.04it/s]11/28/2021 01:46:23 - INFO - __main__ -   Batch number = 7
Evaluating:   8%|▊         | 7/84 [00:02<00:25,  3.02it/s]11/28/2021 01:46:23 - INFO - __main__ -   Batch number = 8
Evaluating:  10%|▉         | 8/84 [00:02<00:25,  3.01it/s]11/28/2021 01:46:23 - INFO - __main__ -   Batch number = 9
Evaluating:  11%|█         | 9/84 [00:03<00:24,  3.02it/s]11/28/2021 01:46:24 - INFO - __main__ -   Batch number = 10
Evaluating:  12%|█▏        | 10/84 [00:03<00:24,  3.01it/s]11/28/2021 01:46:24 - INFO - __main__ -   Batch number = 11
Evaluating:  13%|█▎        | 11/84 [00:03<00:24,  2.99it/s]11/28/2021 01:46:24 - INFO - __main__ -   Batch number = 12
Evaluating:  14%|█▍        | 12/84 [00:04<00:24,  2.98it/s]11/28/2021 01:46:25 - INFO - __main__ -   Batch number = 13
Evaluating:  15%|█▌        | 13/84 [00:04<00:23,  2.98it/s]11/28/2021 01:46:25 - INFO - __main__ -   Batch number = 14
Evaluating:  17%|█▋        | 14/84 [00:04<00:23,  2.98it/s]11/28/2021 01:46:26 - INFO - __main__ -   Batch number = 15
Evaluating:  18%|█▊        | 15/84 [00:05<00:25,  2.72it/s]11/28/2021 01:46:26 - INFO - __main__ -   Batch number = 16
Evaluating:  19%|█▉        | 16/84 [00:05<00:23,  2.87it/s]11/28/2021 01:46:26 - INFO - __main__ -   Batch number = 17
Evaluating:  20%|██        | 17/84 [00:05<00:22,  2.95it/s]11/28/2021 01:46:26 - INFO - __main__ -   Batch number = 18
Evaluating:  21%|██▏       | 18/84 [00:06<00:21,  3.00it/s]11/28/2021 01:46:27 - INFO - __main__ -   Batch number = 19
Evaluating:  23%|██▎       | 19/84 [00:06<00:21,  3.03it/s]11/28/2021 01:46:27 - INFO - __main__ -   Batch number = 20
Evaluating:  24%|██▍       | 20/84 [00:06<00:21,  3.01it/s]11/28/2021 01:46:27 - INFO - __main__ -   Batch number = 21
Evaluating:  25%|██▌       | 21/84 [00:07<00:21,  2.99it/s]11/28/2021 01:46:28 - INFO - __main__ -   Batch number = 22
Evaluating:  26%|██▌       | 22/84 [00:07<00:20,  2.99it/s]11/28/2021 01:46:28 - INFO - __main__ -   Batch number = 23
Evaluating:  27%|██▋       | 23/84 [00:07<00:20,  2.98it/s]11/28/2021 01:46:28 - INFO - __main__ -   Batch number = 24
Evaluating:  29%|██▊       | 24/84 [00:08<00:20,  2.97it/s]11/28/2021 01:46:29 - INFO - __main__ -   Batch number = 25
Evaluating:  30%|██▉       | 25/84 [00:08<00:19,  2.97it/s]11/28/2021 01:46:29 - INFO - __main__ -   Batch number = 26
Evaluating:  31%|███       | 26/84 [00:08<00:19,  2.98it/s]11/28/2021 01:46:29 - INFO - __main__ -   Batch number = 27
Evaluating:  32%|███▏      | 27/84 [00:09<00:19,  2.99it/s]11/28/2021 01:46:30 - INFO - __main__ -   Batch number = 28
Evaluating:  33%|███▎      | 28/84 [00:09<00:18,  3.00it/s]11/28/2021 01:46:30 - INFO - __main__ -   Batch number = 29
Evaluating:  35%|███▍      | 29/84 [00:09<00:18,  3.00it/s]11/28/2021 01:46:31 - INFO - __main__ -   Batch number = 30
Evaluating:  36%|███▌      | 30/84 [00:10<00:18,  2.93it/s]11/28/2021 01:46:31 - INFO - __main__ -   Batch number = 31
Evaluating:  37%|███▋      | 31/84 [00:10<00:17,  2.98it/s]11/28/2021 01:46:31 - INFO - __main__ -   Batch number = 32
Evaluating:  38%|███▊      | 32/84 [00:10<00:17,  3.01it/s]11/28/2021 01:46:31 - INFO - __main__ -   Batch number = 33
Evaluating:  39%|███▉      | 33/84 [00:11<00:17,  2.87it/s]11/28/2021 01:46:32 - INFO - __main__ -   Batch number = 34
Evaluating:  40%|████      | 34/84 [00:11<00:19,  2.55it/s]11/28/2021 01:46:32 - INFO - __main__ -   Batch number = 35
Evaluating:  42%|████▏     | 35/84 [00:12<00:20,  2.35it/s]11/28/2021 01:46:33 - INFO - __main__ -   Batch number = 36
Evaluating:  43%|████▎     | 36/84 [00:12<00:21,  2.23it/s]11/28/2021 01:46:33 - INFO - __main__ -   Batch number = 37
Evaluating:  44%|████▍     | 37/84 [00:13<00:21,  2.16it/s]11/28/2021 01:46:34 - INFO - __main__ -   Batch number = 38
Evaluating:  45%|████▌     | 38/84 [00:13<00:21,  2.12it/s]11/28/2021 01:46:34 - INFO - __main__ -   Batch number = 39
Evaluating:  46%|████▋     | 39/84 [00:14<00:21,  2.09it/s]11/28/2021 01:46:35 - INFO - __main__ -   Batch number = 40
Evaluating:  48%|████▊     | 40/84 [00:14<00:21,  2.05it/s]11/28/2021 01:46:35 - INFO - __main__ -   Batch number = 41
Evaluating:  49%|████▉     | 41/84 [00:15<00:20,  2.09it/s]11/28/2021 01:46:36 - INFO - __main__ -   Batch number = 42
Evaluating:  50%|█████     | 42/84 [00:15<00:20,  2.05it/s]11/28/2021 01:46:36 - INFO - __main__ -   Batch number = 43
Evaluating:  51%|█████     | 43/84 [00:16<00:20,  2.03it/s]11/28/2021 01:46:37 - INFO - __main__ -   Batch number = 44
Evaluating:  52%|█████▏    | 44/84 [00:16<00:18,  2.11it/s]11/28/2021 01:46:37 - INFO - __main__ -   Batch number = 45
Evaluating:  54%|█████▎    | 45/84 [00:17<00:18,  2.08it/s]11/28/2021 01:46:38 - INFO - __main__ -   Batch number = 46
Evaluating:  55%|█████▍    | 46/84 [00:17<00:18,  2.05it/s]11/28/2021 01:46:38 - INFO - __main__ -   Batch number = 47
Evaluating:  56%|█████▌    | 47/84 [00:18<00:18,  2.05it/s]11/28/2021 01:46:39 - INFO - __main__ -   Batch number = 48
Evaluating:  57%|█████▋    | 48/84 [00:18<00:17,  2.02it/s]11/28/2021 01:46:39 - INFO - __main__ -   Batch number = 49
Evaluating:  58%|█████▊    | 49/84 [00:19<00:17,  2.01it/s]11/28/2021 01:46:40 - INFO - __main__ -   Batch number = 50
Evaluating:  60%|█████▉    | 50/84 [00:19<00:17,  2.00it/s]11/28/2021 01:46:40 - INFO - __main__ -   Batch number = 51
Evaluating:  61%|██████    | 51/84 [00:20<00:16,  1.99it/s]11/28/2021 01:46:41 - INFO - __main__ -   Batch number = 52
Evaluating:  62%|██████▏   | 52/84 [00:20<00:16,  1.98it/s]11/28/2021 01:46:41 - INFO - __main__ -   Batch number = 53
Evaluating:  63%|██████▎   | 53/84 [00:21<00:15,  2.00it/s]11/28/2021 01:46:42 - INFO - __main__ -   Batch number = 54
Evaluating:  64%|██████▍   | 54/84 [00:21<00:15,  2.00it/s]11/28/2021 01:46:42 - INFO - __main__ -   Batch number = 55
Evaluating:  65%|██████▌   | 55/84 [00:22<00:14,  1.98it/s]11/28/2021 01:46:43 - INFO - __main__ -   Batch number = 56
Evaluating:  67%|██████▋   | 56/84 [00:22<00:14,  1.98it/s]11/28/2021 01:46:43 - INFO - __main__ -   Batch number = 57
Evaluating:  68%|██████▊   | 57/84 [00:23<00:13,  1.96it/s]11/28/2021 01:46:44 - INFO - __main__ -   Batch number = 58
Evaluating:  69%|██████▉   | 58/84 [00:23<00:14,  1.79it/s]11/28/2021 01:46:45 - INFO - __main__ -   Batch number = 59
Evaluating:  70%|███████   | 59/84 [00:24<00:14,  1.68it/s]11/28/2021 01:46:45 - INFO - __main__ -   Batch number = 60
Evaluating:  71%|███████▏  | 60/84 [00:25<00:14,  1.65it/s]11/28/2021 01:46:46 - INFO - __main__ -   Batch number = 61
Evaluating:  73%|███████▎  | 61/84 [00:25<00:14,  1.60it/s]11/28/2021 01:46:46 - INFO - __main__ -   Batch number = 62
Evaluating:  74%|███████▍  | 62/84 [00:26<00:14,  1.56it/s]11/28/2021 01:46:47 - INFO - __main__ -   Batch number = 63
Evaluating:  75%|███████▌  | 63/84 [00:27<00:13,  1.54it/s]11/28/2021 01:46:48 - INFO - __main__ -   Batch number = 64
Evaluating:  76%|███████▌  | 64/84 [00:27<00:13,  1.53it/s]11/28/2021 01:46:48 - INFO - __main__ -   Batch number = 65
Evaluating:  77%|███████▋  | 65/84 [00:28<00:12,  1.52it/s]11/28/2021 01:46:49 - INFO - __main__ -   Batch number = 66
Evaluating:  79%|███████▊  | 66/84 [00:29<00:11,  1.52it/s]11/28/2021 01:46:50 - INFO - __main__ -   Batch number = 67
Evaluating:  80%|███████▉  | 67/84 [00:29<00:11,  1.52it/s]11/28/2021 01:46:50 - INFO - __main__ -   Batch number = 68
Evaluating:  81%|████████  | 68/84 [00:30<00:10,  1.51it/s]11/28/2021 01:46:51 - INFO - __main__ -   Batch number = 69
Evaluating:  82%|████████▏ | 69/84 [00:31<00:10,  1.50it/s]11/28/2021 01:46:52 - INFO - __main__ -   Batch number = 70
Evaluating:  83%|████████▎ | 70/84 [00:31<00:09,  1.50it/s]11/28/2021 01:46:52 - INFO - __main__ -   Batch number = 71
Evaluating:  85%|████████▍ | 71/84 [00:32<00:08,  1.50it/s]11/28/2021 01:46:53 - INFO - __main__ -   Batch number = 72
Evaluating:  86%|████████▌ | 72/84 [00:33<00:08,  1.50it/s]11/28/2021 01:46:54 - INFO - __main__ -   Batch number = 73
Evaluating:  87%|████████▋ | 73/84 [00:33<00:07,  1.50it/s]11/28/2021 01:46:54 - INFO - __main__ -   Batch number = 74
Evaluating:  88%|████████▊ | 74/84 [00:34<00:06,  1.56it/s]11/28/2021 01:46:55 - INFO - __main__ -   Batch number = 75
Evaluating:  89%|████████▉ | 75/84 [00:35<00:05,  1.54it/s]11/28/2021 01:46:56 - INFO - __main__ -   Batch number = 76
Evaluating:  90%|█████████ | 76/84 [00:35<00:05,  1.52it/s]11/28/2021 01:46:56 - INFO - __main__ -   Batch number = 77
Evaluating:  92%|█████████▏| 77/84 [00:36<00:04,  1.52it/s]11/28/2021 01:46:57 - INFO - __main__ -   Batch number = 78
Evaluating:  93%|█████████▎| 78/84 [00:37<00:03,  1.52it/s]11/28/2021 01:46:58 - INFO - __main__ -   Batch number = 79
Evaluating:  94%|█████████▍| 79/84 [00:37<00:03,  1.52it/s]11/28/2021 01:46:58 - INFO - __main__ -   Batch number = 80
Evaluating:  95%|█████████▌| 80/84 [00:38<00:02,  1.52it/s]11/28/2021 01:46:59 - INFO - __main__ -   Batch number = 81
Evaluating:  96%|█████████▋| 81/84 [00:38<00:01,  1.52it/s]11/28/2021 01:47:00 - INFO - __main__ -   Batch number = 82
Evaluating:  98%|█████████▊| 82/84 [00:39<00:01,  1.52it/s]11/28/2021 01:47:00 - INFO - __main__ -   Batch number = 83
Evaluating:  99%|█████████▉| 83/84 [00:40<00:00,  1.56it/s]11/28/2021 01:47:01 - INFO - __main__ -   Batch number = 84
Evaluating: 100%|██████████| 84/84 [00:40<00:00,  1.64it/s]Evaluating: 100%|██████████| 84/84 [00:40<00:00,  2.06it/s]
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:47:03 - INFO - __main__ -   ***** Evaluation result  in pt *****
11/28/2021 01:47:03 - INFO - __main__ -     f1 = 0.882104053489344
11/28/2021 01:47:03 - INFO - __main__ -     loss = 0.4002897781985147
11/28/2021 01:47:03 - INFO - __main__ -     precision = 0.8888986366746794
11/28/2021 01:47:03 - INFO - __main__ -     recall = 0.8754125555113961
48.40user 17.49system 1:05.69elapsed 100%CPU (0avgtext+0avgdata 3986088maxresident)k
0inputs+704outputs (0major+1520431minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:48:22 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='zh', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:48:22 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:48:22 - INFO - __main__ -   Seed = 1
11/28/2021 01:48:22 - INFO - root -   save model
11/28/2021 01:48:22 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='zh', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:48:22 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:48:25 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:48:31 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:48:31 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:48:31 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
11/28/2021 01:48:31 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:48:31 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:48:31 - INFO - root -   loading lang adpater cs/wiki@ukp
11/28/2021 01:48:31 - INFO - __main__ -   Adapter Languages : ['cs'], Length : 1
11/28/2021 01:48:31 - INFO - __main__ -   Adapter Names ['cs/wiki@ukp'], Length : 1
11/28/2021 01:48:31 - INFO - __main__ -   Language = cs
11/28/2021 01:48:31 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/cs/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_cs_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/adapter_config.json
Adding adapter 'cs' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/head_config.json
11/28/2021 01:48:44 - INFO - __main__ -   Language adapter for zh not found, using cs instead
11/28/2021 01:48:44 - INFO - __main__ -   Set active language adapter to cs
11/28/2021 01:48:44 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:48:44 - INFO - __main__ -   Adapter Languages = ['cs']
11/28/2021 01:48:44 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_zh_bert-base-multilingual-cased_128
11/28/2021 01:48:45 - INFO - __main__ -   ***** Running evaluation  in zh *****
11/28/2021 01:48:45 - INFO - __main__ -     Num examples = 3458
11/28/2021 01:48:45 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/109 [00:00<?, ?it/s]11/28/2021 01:48:45 - INFO - __main__ -   Batch number = 1
Evaluating:   1%|          | 1/109 [00:00<01:11,  1.51it/s]11/28/2021 01:48:45 - INFO - __main__ -   Batch number = 2
Evaluating:   2%|▏         | 2/109 [00:01<01:10,  1.52it/s]11/28/2021 01:48:46 - INFO - __main__ -   Batch number = 3
Evaluating:   3%|▎         | 3/109 [00:01<01:10,  1.50it/s]11/28/2021 01:48:47 - INFO - __main__ -   Batch number = 4
Evaluating:   4%|▎         | 4/109 [00:02<01:09,  1.50it/s]11/28/2021 01:48:47 - INFO - __main__ -   Batch number = 5
Evaluating:   5%|▍         | 5/109 [00:03<01:08,  1.51it/s]11/28/2021 01:48:48 - INFO - __main__ -   Batch number = 6
Evaluating:   6%|▌         | 6/109 [00:03<01:08,  1.51it/s]11/28/2021 01:48:49 - INFO - __main__ -   Batch number = 7
Evaluating:   6%|▋         | 7/109 [00:04<01:07,  1.51it/s]11/28/2021 01:48:49 - INFO - __main__ -   Batch number = 8
Evaluating:   7%|▋         | 8/109 [00:05<01:06,  1.51it/s]11/28/2021 01:48:50 - INFO - __main__ -   Batch number = 9
Evaluating:   8%|▊         | 9/109 [00:05<01:05,  1.52it/s]11/28/2021 01:48:51 - INFO - __main__ -   Batch number = 10
Evaluating:   9%|▉         | 10/109 [00:06<01:05,  1.52it/s]11/28/2021 01:48:51 - INFO - __main__ -   Batch number = 11
Evaluating:  10%|█         | 11/109 [00:07<01:04,  1.52it/s]11/28/2021 01:48:52 - INFO - __main__ -   Batch number = 12
Evaluating:  11%|█         | 12/109 [00:07<01:03,  1.53it/s]11/28/2021 01:48:53 - INFO - __main__ -   Batch number = 13
Evaluating:  12%|█▏        | 13/109 [00:08<01:03,  1.52it/s]11/28/2021 01:48:53 - INFO - __main__ -   Batch number = 14
Evaluating:  13%|█▎        | 14/109 [00:09<01:02,  1.52it/s]11/28/2021 01:48:54 - INFO - __main__ -   Batch number = 15
Evaluating:  14%|█▍        | 15/109 [00:09<01:01,  1.52it/s]11/28/2021 01:48:54 - INFO - __main__ -   Batch number = 16
Evaluating:  15%|█▍        | 16/109 [00:10<01:01,  1.52it/s]11/28/2021 01:48:55 - INFO - __main__ -   Batch number = 17
Evaluating:  16%|█▌        | 17/109 [00:11<01:00,  1.52it/s]11/28/2021 01:48:56 - INFO - __main__ -   Batch number = 18
Evaluating:  17%|█▋        | 18/109 [00:11<00:53,  1.69it/s]11/28/2021 01:48:56 - INFO - __main__ -   Batch number = 19
Evaluating:  17%|█▋        | 19/109 [00:12<00:54,  1.64it/s]11/28/2021 01:48:57 - INFO - __main__ -   Batch number = 20
Evaluating:  18%|█▊        | 20/109 [00:12<00:53,  1.66it/s]11/28/2021 01:48:57 - INFO - __main__ -   Batch number = 21
Evaluating:  19%|█▉        | 21/109 [00:13<00:50,  1.74it/s]11/28/2021 01:48:58 - INFO - __main__ -   Batch number = 22
Evaluating:  20%|██        | 22/109 [00:13<00:48,  1.81it/s]11/28/2021 01:48:58 - INFO - __main__ -   Batch number = 23
Evaluating:  21%|██        | 23/109 [00:14<00:46,  1.87it/s]11/28/2021 01:48:59 - INFO - __main__ -   Batch number = 24
Evaluating:  22%|██▏       | 24/109 [00:14<00:44,  1.91it/s]11/28/2021 01:48:59 - INFO - __main__ -   Batch number = 25
Evaluating:  23%|██▎       | 25/109 [00:15<00:43,  1.94it/s]11/28/2021 01:49:00 - INFO - __main__ -   Batch number = 26
Evaluating:  24%|██▍       | 26/109 [00:15<00:42,  1.97it/s]11/28/2021 01:49:00 - INFO - __main__ -   Batch number = 27
Evaluating:  25%|██▍       | 27/109 [00:16<00:41,  2.00it/s]11/28/2021 01:49:01 - INFO - __main__ -   Batch number = 28
Evaluating:  26%|██▌       | 28/109 [00:17<00:46,  1.73it/s]11/28/2021 01:49:02 - INFO - __main__ -   Batch number = 29
Evaluating:  27%|██▋       | 29/109 [00:17<00:43,  1.82it/s]11/28/2021 01:49:02 - INFO - __main__ -   Batch number = 30
Evaluating:  28%|██▊       | 30/109 [00:18<00:41,  1.88it/s]11/28/2021 01:49:03 - INFO - __main__ -   Batch number = 31
Evaluating:  28%|██▊       | 31/109 [00:18<00:40,  1.93it/s]11/28/2021 01:49:03 - INFO - __main__ -   Batch number = 32
Evaluating:  29%|██▉       | 32/109 [00:19<00:39,  1.95it/s]11/28/2021 01:49:04 - INFO - __main__ -   Batch number = 33
Evaluating:  30%|███       | 33/109 [00:19<00:38,  1.99it/s]11/28/2021 01:49:04 - INFO - __main__ -   Batch number = 34
Evaluating:  31%|███       | 34/109 [00:20<00:37,  2.01it/s]11/28/2021 01:49:05 - INFO - __main__ -   Batch number = 35
Evaluating:  32%|███▏      | 35/109 [00:20<00:36,  2.01it/s]11/28/2021 01:49:05 - INFO - __main__ -   Batch number = 36
Evaluating:  33%|███▎      | 36/109 [00:21<00:36,  2.00it/s]11/28/2021 01:49:06 - INFO - __main__ -   Batch number = 37
Evaluating:  34%|███▍      | 37/109 [00:21<00:35,  2.01it/s]11/28/2021 01:49:06 - INFO - __main__ -   Batch number = 38
Evaluating:  35%|███▍      | 38/109 [00:22<00:35,  2.01it/s]11/28/2021 01:49:07 - INFO - __main__ -   Batch number = 39
Evaluating:  36%|███▌      | 39/109 [00:22<00:34,  2.01it/s]11/28/2021 01:49:07 - INFO - __main__ -   Batch number = 40
Evaluating:  37%|███▋      | 40/109 [00:23<00:34,  2.02it/s]11/28/2021 01:49:08 - INFO - __main__ -   Batch number = 41
Evaluating:  38%|███▊      | 41/109 [00:23<00:33,  2.02it/s]11/28/2021 01:49:08 - INFO - __main__ -   Batch number = 42
Evaluating:  39%|███▊      | 42/109 [00:23<00:33,  2.03it/s]11/28/2021 01:49:09 - INFO - __main__ -   Batch number = 43
Evaluating:  39%|███▉      | 43/109 [00:24<00:32,  2.03it/s]11/28/2021 01:49:09 - INFO - __main__ -   Batch number = 44
Evaluating:  40%|████      | 44/109 [00:24<00:31,  2.04it/s]11/28/2021 01:49:10 - INFO - __main__ -   Batch number = 45
Evaluating:  41%|████▏     | 45/109 [00:25<00:31,  2.03it/s]11/28/2021 01:49:10 - INFO - __main__ -   Batch number = 46
Evaluating:  42%|████▏     | 46/109 [00:25<00:31,  2.03it/s]11/28/2021 01:49:11 - INFO - __main__ -   Batch number = 47
Evaluating:  43%|████▎     | 47/109 [00:26<00:30,  2.02it/s]11/28/2021 01:49:11 - INFO - __main__ -   Batch number = 48
Evaluating:  44%|████▍     | 48/109 [00:27<00:32,  1.88it/s]11/28/2021 01:49:12 - INFO - __main__ -   Batch number = 49
Evaluating:  45%|████▍     | 49/109 [00:27<00:31,  1.92it/s]11/28/2021 01:49:12 - INFO - __main__ -   Batch number = 50
Evaluating:  46%|████▌     | 50/109 [00:28<00:30,  1.95it/s]11/28/2021 01:49:13 - INFO - __main__ -   Batch number = 51
Evaluating:  47%|████▋     | 51/109 [00:28<00:29,  1.98it/s]11/28/2021 01:49:13 - INFO - __main__ -   Batch number = 52
Evaluating:  48%|████▊     | 52/109 [00:29<00:28,  1.98it/s]11/28/2021 01:49:14 - INFO - __main__ -   Batch number = 53
Evaluating:  49%|████▊     | 53/109 [00:29<00:28,  1.98it/s]11/28/2021 01:49:14 - INFO - __main__ -   Batch number = 54
Evaluating:  50%|████▉     | 54/109 [00:30<00:27,  1.99it/s]11/28/2021 01:49:15 - INFO - __main__ -   Batch number = 55
Evaluating:  50%|█████     | 55/109 [00:30<00:27,  1.99it/s]11/28/2021 01:49:15 - INFO - __main__ -   Batch number = 56
Evaluating:  51%|█████▏    | 56/109 [00:31<00:26,  2.00it/s]11/28/2021 01:49:16 - INFO - __main__ -   Batch number = 57
Evaluating:  52%|█████▏    | 57/109 [00:31<00:25,  2.01it/s]11/28/2021 01:49:16 - INFO - __main__ -   Batch number = 58
Evaluating:  53%|█████▎    | 58/109 [00:32<00:25,  2.01it/s]11/28/2021 01:49:17 - INFO - __main__ -   Batch number = 59
Evaluating:  54%|█████▍    | 59/109 [00:32<00:25,  1.99it/s]11/28/2021 01:49:17 - INFO - __main__ -   Batch number = 60
Evaluating:  55%|█████▌    | 60/109 [00:33<00:24,  1.99it/s]11/28/2021 01:49:18 - INFO - __main__ -   Batch number = 61
Evaluating:  56%|█████▌    | 61/109 [00:33<00:24,  2.00it/s]11/28/2021 01:49:18 - INFO - __main__ -   Batch number = 62
Evaluating:  57%|█████▋    | 62/109 [00:33<00:21,  2.24it/s]11/28/2021 01:49:18 - INFO - __main__ -   Batch number = 63
Evaluating:  58%|█████▊    | 63/109 [00:34<00:18,  2.44it/s]11/28/2021 01:49:19 - INFO - __main__ -   Batch number = 64
Evaluating:  59%|█████▊    | 64/109 [00:34<00:17,  2.60it/s]11/28/2021 01:49:19 - INFO - __main__ -   Batch number = 65
Evaluating:  60%|█████▉    | 65/109 [00:34<00:15,  2.77it/s]11/28/2021 01:49:19 - INFO - __main__ -   Batch number = 66
Evaluating:  61%|██████    | 66/109 [00:35<00:15,  2.86it/s]11/28/2021 01:49:20 - INFO - __main__ -   Batch number = 67
Evaluating:  61%|██████▏   | 67/109 [00:35<00:14,  2.93it/s]11/28/2021 01:49:20 - INFO - __main__ -   Batch number = 68
Evaluating:  62%|██████▏   | 68/109 [00:35<00:13,  2.99it/s]11/28/2021 01:49:20 - INFO - __main__ -   Batch number = 69
Evaluating:  63%|██████▎   | 69/109 [00:36<00:13,  3.04it/s]11/28/2021 01:49:21 - INFO - __main__ -   Batch number = 70
Evaluating:  64%|██████▍   | 70/109 [00:36<00:12,  3.04it/s]11/28/2021 01:49:21 - INFO - __main__ -   Batch number = 71
Evaluating:  65%|██████▌   | 71/109 [00:36<00:12,  3.06it/s]11/28/2021 01:49:21 - INFO - __main__ -   Batch number = 72
Evaluating:  66%|██████▌   | 72/109 [00:37<00:12,  3.05it/s]11/28/2021 01:49:22 - INFO - __main__ -   Batch number = 73
Evaluating:  67%|██████▋   | 73/109 [00:37<00:11,  3.01it/s]11/28/2021 01:49:22 - INFO - __main__ -   Batch number = 74
Evaluating:  68%|██████▊   | 74/109 [00:37<00:11,  3.00it/s]11/28/2021 01:49:22 - INFO - __main__ -   Batch number = 75
Evaluating:  69%|██████▉   | 75/109 [00:38<00:11,  3.03it/s]11/28/2021 01:49:23 - INFO - __main__ -   Batch number = 76
Evaluating:  70%|██████▉   | 76/109 [00:38<00:10,  3.02it/s]11/28/2021 01:49:23 - INFO - __main__ -   Batch number = 77
Evaluating:  71%|███████   | 77/109 [00:38<00:10,  3.01it/s]11/28/2021 01:49:23 - INFO - __main__ -   Batch number = 78
Evaluating:  72%|███████▏  | 78/109 [00:39<00:10,  2.99it/s]11/28/2021 01:49:24 - INFO - __main__ -   Batch number = 79
Evaluating:  72%|███████▏  | 79/109 [00:39<00:09,  3.01it/s]11/28/2021 01:49:24 - INFO - __main__ -   Batch number = 80
Evaluating:  73%|███████▎  | 80/109 [00:39<00:09,  3.06it/s]11/28/2021 01:49:24 - INFO - __main__ -   Batch number = 81
Evaluating:  74%|███████▍  | 81/109 [00:40<00:09,  3.04it/s]11/28/2021 01:49:25 - INFO - __main__ -   Batch number = 82
Evaluating:  75%|███████▌  | 82/109 [00:40<00:10,  2.64it/s]11/28/2021 01:49:25 - INFO - __main__ -   Batch number = 83
Evaluating:  76%|███████▌  | 83/109 [00:41<00:10,  2.41it/s]11/28/2021 01:49:26 - INFO - __main__ -   Batch number = 84
Evaluating:  77%|███████▋  | 84/109 [00:41<00:10,  2.28it/s]11/28/2021 01:49:26 - INFO - __main__ -   Batch number = 85
Evaluating:  78%|███████▊  | 85/109 [00:42<00:10,  2.18it/s]11/28/2021 01:49:27 - INFO - __main__ -   Batch number = 86
Evaluating:  79%|███████▉  | 86/109 [00:42<00:10,  2.13it/s]11/28/2021 01:49:27 - INFO - __main__ -   Batch number = 87
Evaluating:  80%|███████▉  | 87/109 [00:43<00:10,  2.09it/s]11/28/2021 01:49:28 - INFO - __main__ -   Batch number = 88
Evaluating:  81%|████████  | 88/109 [00:43<00:10,  2.07it/s]11/28/2021 01:49:28 - INFO - __main__ -   Batch number = 89
Evaluating:  82%|████████▏ | 89/109 [00:44<00:09,  2.05it/s]11/28/2021 01:49:29 - INFO - __main__ -   Batch number = 90
Evaluating:  83%|████████▎ | 90/109 [00:44<00:09,  2.04it/s]11/28/2021 01:49:29 - INFO - __main__ -   Batch number = 91
Evaluating:  83%|████████▎ | 91/109 [00:45<00:08,  2.02it/s]11/28/2021 01:49:30 - INFO - __main__ -   Batch number = 92
Evaluating:  84%|████████▍ | 92/109 [00:45<00:08,  2.02it/s]11/28/2021 01:49:30 - INFO - __main__ -   Batch number = 93
Evaluating:  85%|████████▌ | 93/109 [00:46<00:07,  2.02it/s]11/28/2021 01:49:31 - INFO - __main__ -   Batch number = 94
Evaluating:  86%|████████▌ | 94/109 [00:46<00:07,  2.03it/s]11/28/2021 01:49:31 - INFO - __main__ -   Batch number = 95
Evaluating:  87%|████████▋ | 95/109 [00:47<00:07,  1.89it/s]11/28/2021 01:49:32 - INFO - __main__ -   Batch number = 96
Evaluating:  88%|████████▊ | 96/109 [00:47<00:06,  1.93it/s]11/28/2021 01:49:32 - INFO - __main__ -   Batch number = 97
Evaluating:  89%|████████▉ | 97/109 [00:48<00:06,  1.95it/s]11/28/2021 01:49:33 - INFO - __main__ -   Batch number = 98
Evaluating:  90%|████████▉ | 98/109 [00:48<00:05,  1.96it/s]11/28/2021 01:49:33 - INFO - __main__ -   Batch number = 99
Evaluating:  91%|█████████ | 99/109 [00:49<00:05,  1.97it/s]11/28/2021 01:49:34 - INFO - __main__ -   Batch number = 100
Evaluating:  92%|█████████▏| 100/109 [00:49<00:04,  1.98it/s]11/28/2021 01:49:34 - INFO - __main__ -   Batch number = 101
Evaluating:  93%|█████████▎| 101/109 [00:50<00:04,  1.99it/s]11/28/2021 01:49:35 - INFO - __main__ -   Batch number = 102
Evaluating:  94%|█████████▎| 102/109 [00:50<00:03,  1.99it/s]11/28/2021 01:49:35 - INFO - __main__ -   Batch number = 103
Evaluating:  94%|█████████▍| 103/109 [00:51<00:03,  1.99it/s]11/28/2021 01:49:36 - INFO - __main__ -   Batch number = 104
Evaluating:  95%|█████████▌| 104/109 [00:51<00:02,  2.00it/s]11/28/2021 01:49:36 - INFO - __main__ -   Batch number = 105
Evaluating:  96%|█████████▋| 105/109 [00:52<00:02,  2.00it/s]11/28/2021 01:49:37 - INFO - __main__ -   Batch number = 106
Evaluating:  97%|█████████▋| 106/109 [00:52<00:01,  2.00it/s]11/28/2021 01:49:37 - INFO - __main__ -   Batch number = 107
Evaluating:  98%|█████████▊| 107/109 [00:53<00:01,  2.00it/s]11/28/2021 01:49:38 - INFO - __main__ -   Batch number = 108
Evaluating:  99%|█████████▉| 108/109 [00:53<00:00,  2.00it/s]11/28/2021 01:49:38 - INFO - __main__ -   Batch number = 109
Evaluating: 100%|██████████| 109/109 [00:53<00:00,  2.61it/s]Evaluating: 100%|██████████| 109/109 [00:53<00:00,  2.03it/s]
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:49:40 - INFO - __main__ -   ***** Evaluation result  in zh *****
11/28/2021 01:49:40 - INFO - __main__ -     f1 = 0.6074717699226253
11/28/2021 01:49:40 - INFO - __main__ -     loss = 1.457914115638908
11/28/2021 01:49:40 - INFO - __main__ -     precision = 0.6141866627944966
11/28/2021 01:49:40 - INFO - __main__ -     recall = 0.6009021167779515
60.05user 19.90system 1:20.35elapsed 99%CPU (0avgtext+0avgdata 3989464maxresident)k
0inputs+784outputs (0major+1435778minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:49:43 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='zh', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:49:43 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:49:43 - INFO - __main__ -   Seed = 2
11/28/2021 01:49:43 - INFO - root -   save model
11/28/2021 01:49:43 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='zh', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:49:43 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:49:45 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:49:51 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:49:51 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:49:51 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
11/28/2021 01:49:51 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:49:51 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:49:51 - INFO - root -   loading lang adpater cs/wiki@ukp
11/28/2021 01:49:51 - INFO - __main__ -   Adapter Languages : ['cs'], Length : 1
11/28/2021 01:49:51 - INFO - __main__ -   Adapter Names ['cs/wiki@ukp'], Length : 1
11/28/2021 01:49:51 - INFO - __main__ -   Language = cs
11/28/2021 01:49:51 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/cs/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_cs_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/adapter_config.json
Adding adapter 'cs' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/head_config.json
11/28/2021 01:50:05 - INFO - __main__ -   Language adapter for zh not found, using cs instead
11/28/2021 01:50:05 - INFO - __main__ -   Set active language adapter to cs
11/28/2021 01:50:05 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:50:05 - INFO - __main__ -   Adapter Languages = ['cs']
11/28/2021 01:50:05 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_zh_bert-base-multilingual-cased_128
11/28/2021 01:50:06 - INFO - __main__ -   ***** Running evaluation  in zh *****
11/28/2021 01:50:06 - INFO - __main__ -     Num examples = 3458
11/28/2021 01:50:06 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/109 [00:00<?, ?it/s]11/28/2021 01:50:06 - INFO - __main__ -   Batch number = 1
Evaluating:   1%|          | 1/109 [00:00<01:11,  1.52it/s]11/28/2021 01:50:07 - INFO - __main__ -   Batch number = 2
Evaluating:   2%|▏         | 2/109 [00:01<01:09,  1.53it/s]11/28/2021 01:50:07 - INFO - __main__ -   Batch number = 3
Evaluating:   3%|▎         | 3/109 [00:01<01:09,  1.53it/s]11/28/2021 01:50:08 - INFO - __main__ -   Batch number = 4
Evaluating:   4%|▎         | 4/109 [00:02<01:08,  1.54it/s]11/28/2021 01:50:09 - INFO - __main__ -   Batch number = 5
Evaluating:   5%|▍         | 5/109 [00:03<01:07,  1.54it/s]11/28/2021 01:50:09 - INFO - __main__ -   Batch number = 6
Evaluating:   6%|▌         | 6/109 [00:03<01:06,  1.54it/s]11/28/2021 01:50:10 - INFO - __main__ -   Batch number = 7
Evaluating:   6%|▋         | 7/109 [00:04<01:06,  1.53it/s]11/28/2021 01:50:10 - INFO - __main__ -   Batch number = 8
Evaluating:   7%|▋         | 8/109 [00:05<01:05,  1.54it/s]11/28/2021 01:50:11 - INFO - __main__ -   Batch number = 9
Evaluating:   8%|▊         | 9/109 [00:05<01:05,  1.54it/s]11/28/2021 01:50:12 - INFO - __main__ -   Batch number = 10
Evaluating:   9%|▉         | 10/109 [00:06<01:01,  1.61it/s]11/28/2021 01:50:12 - INFO - __main__ -   Batch number = 11
Evaluating:  10%|█         | 11/109 [00:06<00:56,  1.73it/s]11/28/2021 01:50:13 - INFO - __main__ -   Batch number = 12
Evaluating:  11%|█         | 12/109 [00:07<00:53,  1.83it/s]11/28/2021 01:50:13 - INFO - __main__ -   Batch number = 13
Evaluating:  12%|█▏        | 13/109 [00:07<00:50,  1.89it/s]11/28/2021 01:50:14 - INFO - __main__ -   Batch number = 14
Evaluating:  13%|█▎        | 14/109 [00:08<00:48,  1.94it/s]11/28/2021 01:50:14 - INFO - __main__ -   Batch number = 15
Evaluating:  14%|█▍        | 15/109 [00:08<00:47,  1.97it/s]11/28/2021 01:50:15 - INFO - __main__ -   Batch number = 16
Evaluating:  15%|█▍        | 16/109 [00:09<00:46,  2.02it/s]11/28/2021 01:50:15 - INFO - __main__ -   Batch number = 17
Evaluating:  16%|█▌        | 17/109 [00:09<00:45,  2.01it/s]11/28/2021 01:50:16 - INFO - __main__ -   Batch number = 18
Evaluating:  17%|█▋        | 18/109 [00:10<00:44,  2.02it/s]11/28/2021 01:50:16 - INFO - __main__ -   Batch number = 19
Evaluating:  17%|█▋        | 19/109 [00:10<00:44,  2.04it/s]11/28/2021 01:50:17 - INFO - __main__ -   Batch number = 20
Evaluating:  18%|█▊        | 20/109 [00:11<00:43,  2.05it/s]11/28/2021 01:50:17 - INFO - __main__ -   Batch number = 21
Evaluating:  19%|█▉        | 21/109 [00:11<00:42,  2.05it/s]11/28/2021 01:50:18 - INFO - __main__ -   Batch number = 22
Evaluating:  20%|██        | 22/109 [00:12<00:42,  2.05it/s]11/28/2021 01:50:18 - INFO - __main__ -   Batch number = 23
Evaluating:  21%|██        | 23/109 [00:12<00:42,  2.05it/s]11/28/2021 01:50:19 - INFO - __main__ -   Batch number = 24
Evaluating:  22%|██▏       | 24/109 [00:13<00:41,  2.04it/s]11/28/2021 01:50:19 - INFO - __main__ -   Batch number = 25
Evaluating:  23%|██▎       | 25/109 [00:13<00:40,  2.06it/s]11/28/2021 01:50:20 - INFO - __main__ -   Batch number = 26
Evaluating:  24%|██▍       | 26/109 [00:14<00:40,  2.06it/s]11/28/2021 01:50:20 - INFO - __main__ -   Batch number = 27
Evaluating:  25%|██▍       | 27/109 [00:14<00:39,  2.08it/s]11/28/2021 01:50:21 - INFO - __main__ -   Batch number = 28
Evaluating:  26%|██▌       | 28/109 [00:15<00:38,  2.11it/s]11/28/2021 01:50:21 - INFO - __main__ -   Batch number = 29
Evaluating:  27%|██▋       | 29/109 [00:15<00:34,  2.29it/s]11/28/2021 01:50:21 - INFO - __main__ -   Batch number = 30
Evaluating:  28%|██▊       | 30/109 [00:15<00:33,  2.39it/s]11/28/2021 01:50:22 - INFO - __main__ -   Batch number = 31
Evaluating:  28%|██▊       | 31/109 [00:16<00:30,  2.57it/s]11/28/2021 01:50:22 - INFO - __main__ -   Batch number = 32
Evaluating:  29%|██▉       | 32/109 [00:16<00:28,  2.69it/s]11/28/2021 01:50:22 - INFO - __main__ -   Batch number = 33
Evaluating:  30%|███       | 33/109 [00:16<00:27,  2.80it/s]11/28/2021 01:50:23 - INFO - __main__ -   Batch number = 34
Evaluating:  31%|███       | 34/109 [00:17<00:26,  2.87it/s]11/28/2021 01:50:23 - INFO - __main__ -   Batch number = 35
Evaluating:  32%|███▏      | 35/109 [00:17<00:25,  2.93it/s]11/28/2021 01:50:23 - INFO - __main__ -   Batch number = 36
Evaluating:  33%|███▎      | 36/109 [00:17<00:24,  2.96it/s]11/28/2021 01:50:24 - INFO - __main__ -   Batch number = 37
Evaluating:  34%|███▍      | 37/109 [00:18<00:24,  2.98it/s]11/28/2021 01:50:24 - INFO - __main__ -   Batch number = 38
Evaluating:  35%|███▍      | 38/109 [00:18<00:23,  3.00it/s]11/28/2021 01:50:24 - INFO - __main__ -   Batch number = 39
Evaluating:  36%|███▌      | 39/109 [00:18<00:20,  3.50it/s]11/28/2021 01:50:25 - INFO - __main__ -   Batch number = 40
Evaluating:  37%|███▋      | 40/109 [00:18<00:17,  4.03it/s]11/28/2021 01:50:25 - INFO - __main__ -   Batch number = 41
Evaluating:  38%|███▊      | 41/109 [00:18<00:15,  4.49it/s]11/28/2021 01:50:25 - INFO - __main__ -   Batch number = 42
Evaluating:  39%|███▊      | 42/109 [00:19<00:13,  4.86it/s]11/28/2021 01:50:25 - INFO - __main__ -   Batch number = 43
Evaluating:  39%|███▉      | 43/109 [00:19<00:12,  5.16it/s]11/28/2021 01:50:25 - INFO - __main__ -   Batch number = 44
Evaluating:  40%|████      | 44/109 [00:19<00:12,  5.40it/s]11/28/2021 01:50:25 - INFO - __main__ -   Batch number = 45
Evaluating:  41%|████▏     | 45/109 [00:19<00:11,  5.63it/s]11/28/2021 01:50:26 - INFO - __main__ -   Batch number = 46
Evaluating:  42%|████▏     | 46/109 [00:19<00:10,  5.77it/s]11/28/2021 01:50:26 - INFO - __main__ -   Batch number = 47
Evaluating:  43%|████▎     | 47/109 [00:19<00:10,  5.86it/s]11/28/2021 01:50:26 - INFO - __main__ -   Batch number = 48
Evaluating:  44%|████▍     | 48/109 [00:20<00:10,  5.94it/s]11/28/2021 01:50:26 - INFO - __main__ -   Batch number = 49
Evaluating:  45%|████▍     | 49/109 [00:20<00:09,  6.06it/s]11/28/2021 01:50:26 - INFO - __main__ -   Batch number = 50
Evaluating:  46%|████▌     | 50/109 [00:20<00:09,  6.08it/s]11/28/2021 01:50:26 - INFO - __main__ -   Batch number = 51
Evaluating:  47%|████▋     | 51/109 [00:20<00:12,  4.78it/s]11/28/2021 01:50:27 - INFO - __main__ -   Batch number = 52
Evaluating:  48%|████▊     | 52/109 [00:20<00:11,  5.09it/s]11/28/2021 01:50:27 - INFO - __main__ -   Batch number = 53
Evaluating:  49%|████▊     | 53/109 [00:21<00:10,  5.36it/s]11/28/2021 01:50:27 - INFO - __main__ -   Batch number = 54
Evaluating:  50%|████▉     | 54/109 [00:21<00:09,  5.62it/s]11/28/2021 01:50:27 - INFO - __main__ -   Batch number = 55
Evaluating:  50%|█████     | 55/109 [00:21<00:09,  5.81it/s]11/28/2021 01:50:27 - INFO - __main__ -   Batch number = 56
Evaluating:  51%|█████▏    | 56/109 [00:21<00:08,  5.97it/s]11/28/2021 01:50:27 - INFO - __main__ -   Batch number = 57
Evaluating:  52%|█████▏    | 57/109 [00:21<00:08,  5.98it/s]11/28/2021 01:50:28 - INFO - __main__ -   Batch number = 58
Evaluating:  53%|█████▎    | 58/109 [00:21<00:08,  6.05it/s]11/28/2021 01:50:28 - INFO - __main__ -   Batch number = 59
Evaluating:  54%|█████▍    | 59/109 [00:22<00:08,  6.12it/s]11/28/2021 01:50:28 - INFO - __main__ -   Batch number = 60
Evaluating:  55%|█████▌    | 60/109 [00:22<00:07,  6.16it/s]11/28/2021 01:50:28 - INFO - __main__ -   Batch number = 61
Evaluating:  56%|█████▌    | 61/109 [00:22<00:07,  6.21it/s]11/28/2021 01:50:28 - INFO - __main__ -   Batch number = 62
Evaluating:  57%|█████▋    | 62/109 [00:22<00:07,  6.25it/s]11/28/2021 01:50:28 - INFO - __main__ -   Batch number = 63
Evaluating:  58%|█████▊    | 63/109 [00:22<00:07,  6.30it/s]11/28/2021 01:50:29 - INFO - __main__ -   Batch number = 64
Evaluating:  59%|█████▊    | 64/109 [00:22<00:07,  6.33it/s]11/28/2021 01:50:29 - INFO - __main__ -   Batch number = 65
Evaluating:  60%|█████▉    | 65/109 [00:22<00:06,  6.32it/s]11/28/2021 01:50:29 - INFO - __main__ -   Batch number = 66
Evaluating:  61%|██████    | 66/109 [00:23<00:06,  6.28it/s]11/28/2021 01:50:29 - INFO - __main__ -   Batch number = 67
Evaluating:  61%|██████▏   | 67/109 [00:23<00:06,  6.24it/s]11/28/2021 01:50:29 - INFO - __main__ -   Batch number = 68
Evaluating:  62%|██████▏   | 68/109 [00:23<00:06,  6.24it/s]11/28/2021 01:50:29 - INFO - __main__ -   Batch number = 69
Evaluating:  63%|██████▎   | 69/109 [00:23<00:06,  6.22it/s]11/28/2021 01:50:30 - INFO - __main__ -   Batch number = 70
Evaluating:  64%|██████▍   | 70/109 [00:23<00:06,  6.25it/s]11/28/2021 01:50:30 - INFO - __main__ -   Batch number = 71
Evaluating:  65%|██████▌   | 71/109 [00:23<00:06,  6.27it/s]11/28/2021 01:50:30 - INFO - __main__ -   Batch number = 72
Evaluating:  66%|██████▌   | 72/109 [00:24<00:05,  6.22it/s]11/28/2021 01:50:30 - INFO - __main__ -   Batch number = 73
Evaluating:  67%|██████▋   | 73/109 [00:24<00:05,  6.17it/s]11/28/2021 01:50:30 - INFO - __main__ -   Batch number = 74
Evaluating:  68%|██████▊   | 74/109 [00:24<00:05,  6.17it/s]11/28/2021 01:50:30 - INFO - __main__ -   Batch number = 75
Evaluating:  69%|██████▉   | 75/109 [00:24<00:05,  6.16it/s]11/28/2021 01:50:30 - INFO - __main__ -   Batch number = 76
Evaluating:  70%|██████▉   | 76/109 [00:24<00:05,  6.21it/s]11/28/2021 01:50:31 - INFO - __main__ -   Batch number = 77
Evaluating:  71%|███████   | 77/109 [00:24<00:05,  6.23it/s]11/28/2021 01:50:31 - INFO - __main__ -   Batch number = 78
Evaluating:  72%|███████▏  | 78/109 [00:25<00:04,  6.21it/s]11/28/2021 01:50:31 - INFO - __main__ -   Batch number = 79
Evaluating:  72%|███████▏  | 79/109 [00:25<00:04,  6.09it/s]11/28/2021 01:50:31 - INFO - __main__ -   Batch number = 80
Evaluating:  73%|███████▎  | 80/109 [00:25<00:04,  6.04it/s]11/28/2021 01:50:32 - INFO - __main__ -   Batch number = 81
Evaluating:  74%|███████▍  | 81/109 [00:25<00:06,  4.22it/s]11/28/2021 01:50:32 - INFO - __main__ -   Batch number = 82
Evaluating:  75%|███████▌  | 82/109 [00:25<00:05,  4.66it/s]11/28/2021 01:50:32 - INFO - __main__ -   Batch number = 83
Evaluating:  76%|███████▌  | 83/109 [00:26<00:05,  5.02it/s]11/28/2021 01:50:32 - INFO - __main__ -   Batch number = 84
Evaluating:  77%|███████▋  | 84/109 [00:26<00:04,  5.31it/s]11/28/2021 01:50:32 - INFO - __main__ -   Batch number = 85
Evaluating:  78%|███████▊  | 85/109 [00:26<00:04,  5.49it/s]11/28/2021 01:50:32 - INFO - __main__ -   Batch number = 86
Evaluating:  79%|███████▉  | 86/109 [00:26<00:04,  5.70it/s]11/28/2021 01:50:33 - INFO - __main__ -   Batch number = 87
Evaluating:  80%|███████▉  | 87/109 [00:26<00:03,  5.77it/s]11/28/2021 01:50:33 - INFO - __main__ -   Batch number = 88
Evaluating:  81%|████████  | 88/109 [00:26<00:03,  5.88it/s]11/28/2021 01:50:33 - INFO - __main__ -   Batch number = 89
Evaluating:  82%|████████▏ | 89/109 [00:27<00:03,  5.88it/s]11/28/2021 01:50:33 - INFO - __main__ -   Batch number = 90
Evaluating:  83%|████████▎ | 90/109 [00:27<00:03,  5.93it/s]11/28/2021 01:50:33 - INFO - __main__ -   Batch number = 91
Evaluating:  83%|████████▎ | 91/109 [00:27<00:03,  5.94it/s]11/28/2021 01:50:33 - INFO - __main__ -   Batch number = 92
Evaluating:  84%|████████▍ | 92/109 [00:27<00:02,  6.03it/s]11/28/2021 01:50:34 - INFO - __main__ -   Batch number = 93
Evaluating:  85%|████████▌ | 93/109 [00:27<00:02,  5.98it/s]11/28/2021 01:50:34 - INFO - __main__ -   Batch number = 94
Evaluating:  86%|████████▌ | 94/109 [00:27<00:02,  5.82it/s]11/28/2021 01:50:34 - INFO - __main__ -   Batch number = 95
Evaluating:  87%|████████▋ | 95/109 [00:28<00:02,  5.93it/s]11/28/2021 01:50:34 - INFO - __main__ -   Batch number = 96
Evaluating:  88%|████████▊ | 96/109 [00:28<00:02,  5.95it/s]11/28/2021 01:50:34 - INFO - __main__ -   Batch number = 97
Evaluating:  89%|████████▉ | 97/109 [00:28<00:02,  6.00it/s]11/28/2021 01:50:34 - INFO - __main__ -   Batch number = 98
Evaluating:  90%|████████▉ | 98/109 [00:28<00:01,  6.11it/s]11/28/2021 01:50:35 - INFO - __main__ -   Batch number = 99
Evaluating:  91%|█████████ | 99/109 [00:28<00:01,  6.12it/s]11/28/2021 01:50:35 - INFO - __main__ -   Batch number = 100
Evaluating:  92%|█████████▏| 100/109 [00:28<00:01,  5.64it/s]11/28/2021 01:50:35 - INFO - __main__ -   Batch number = 101
Evaluating:  93%|█████████▎| 101/109 [00:29<00:01,  5.08it/s]11/28/2021 01:50:35 - INFO - __main__ -   Batch number = 102
Evaluating:  94%|█████████▎| 102/109 [00:29<00:01,  4.90it/s]11/28/2021 01:50:35 - INFO - __main__ -   Batch number = 103
Evaluating:  94%|█████████▍| 103/109 [00:29<00:01,  4.91it/s]11/28/2021 01:50:36 - INFO - __main__ -   Batch number = 104
Evaluating:  95%|█████████▌| 104/109 [00:29<00:01,  4.96it/s]11/28/2021 01:50:36 - INFO - __main__ -   Batch number = 105
Evaluating:  96%|█████████▋| 105/109 [00:30<00:00,  5.11it/s]11/28/2021 01:50:36 - INFO - __main__ -   Batch number = 106
Evaluating:  97%|█████████▋| 106/109 [00:30<00:00,  5.28it/s]11/28/2021 01:50:36 - INFO - __main__ -   Batch number = 107
Evaluating:  98%|█████████▊| 107/109 [00:30<00:00,  5.41it/s]11/28/2021 01:50:36 - INFO - __main__ -   Batch number = 108
Evaluating:  99%|█████████▉| 108/109 [00:30<00:00,  3.33it/s]11/28/2021 01:50:37 - INFO - __main__ -   Batch number = 109
Evaluating: 100%|██████████| 109/109 [00:30<00:00,  3.52it/s]
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:50:38 - INFO - __main__ -   ***** Evaluation result  in zh *****
11/28/2021 01:50:38 - INFO - __main__ -     f1 = 0.6144789782086246
11/28/2021 01:50:38 - INFO - __main__ -     loss = 1.4079616999407427
11/28/2021 01:50:38 - INFO - __main__ -     precision = 0.6230310110062606
11/28/2021 01:50:38 - INFO - __main__ -     recall = 0.6061585452479046
43.99user 16.05system 0:58.74elapsed 102%CPU (0avgtext+0avgdata 3991696maxresident)k
0inputs+752outputs (0major+1630645minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:50:41 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='zh', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:50:41 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:50:41 - INFO - __main__ -   Seed = 3
11/28/2021 01:50:41 - INFO - root -   save model
11/28/2021 01:50:41 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='zh', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:50:41 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:50:44 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:50:50 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:50:50 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:50:50 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
11/28/2021 01:50:50 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:50:50 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:50:50 - INFO - root -   loading lang adpater cs/wiki@ukp
11/28/2021 01:50:50 - INFO - __main__ -   Adapter Languages : ['cs'], Length : 1
11/28/2021 01:50:50 - INFO - __main__ -   Adapter Names ['cs/wiki@ukp'], Length : 1
11/28/2021 01:50:50 - INFO - __main__ -   Language = cs
11/28/2021 01:50:50 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/cs/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_cs_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/adapter_config.json
Adding adapter 'cs' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/head_config.json
11/28/2021 01:51:03 - INFO - __main__ -   Language adapter for zh not found, using cs instead
11/28/2021 01:51:03 - INFO - __main__ -   Set active language adapter to cs
11/28/2021 01:51:03 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:51:03 - INFO - __main__ -   Adapter Languages = ['cs']
11/28/2021 01:51:03 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_zh_bert-base-multilingual-cased_128
11/28/2021 01:51:03 - INFO - __main__ -   ***** Running evaluation  in zh *****
11/28/2021 01:51:03 - INFO - __main__ -     Num examples = 3458
11/28/2021 01:51:03 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/109 [00:00<?, ?it/s]11/28/2021 01:51:03 - INFO - __main__ -   Batch number = 1
Evaluating:   1%|          | 1/109 [00:00<01:11,  1.50it/s]11/28/2021 01:51:04 - INFO - __main__ -   Batch number = 2
Evaluating:   2%|▏         | 2/109 [00:01<01:10,  1.52it/s]11/28/2021 01:51:05 - INFO - __main__ -   Batch number = 3
Evaluating:   3%|▎         | 3/109 [00:01<01:09,  1.51it/s]11/28/2021 01:51:05 - INFO - __main__ -   Batch number = 4
Evaluating:   4%|▎         | 4/109 [00:02<01:09,  1.52it/s]11/28/2021 01:51:06 - INFO - __main__ -   Batch number = 5
Evaluating:   5%|▍         | 5/109 [00:03<01:08,  1.52it/s]11/28/2021 01:51:07 - INFO - __main__ -   Batch number = 6
Evaluating:   6%|▌         | 6/109 [00:03<01:07,  1.52it/s]11/28/2021 01:51:07 - INFO - __main__ -   Batch number = 7
Evaluating:   6%|▋         | 7/109 [00:04<01:07,  1.52it/s]11/28/2021 01:51:08 - INFO - __main__ -   Batch number = 8
Evaluating:   7%|▋         | 8/109 [00:05<01:03,  1.59it/s]11/28/2021 01:51:09 - INFO - __main__ -   Batch number = 9
Evaluating:   8%|▊         | 9/109 [00:05<00:58,  1.71it/s]11/28/2021 01:51:09 - INFO - __main__ -   Batch number = 10
Evaluating:   9%|▉         | 10/109 [00:06<00:54,  1.80it/s]11/28/2021 01:51:10 - INFO - __main__ -   Batch number = 11
Evaluating:  10%|█         | 11/109 [00:06<00:52,  1.86it/s]11/28/2021 01:51:10 - INFO - __main__ -   Batch number = 12
Evaluating:  11%|█         | 12/109 [00:07<00:50,  1.90it/s]11/28/2021 01:51:11 - INFO - __main__ -   Batch number = 13
Evaluating:  12%|█▏        | 13/109 [00:07<00:49,  1.93it/s]11/28/2021 01:51:11 - INFO - __main__ -   Batch number = 14
Evaluating:  13%|█▎        | 14/109 [00:08<00:48,  1.97it/s]11/28/2021 01:51:12 - INFO - __main__ -   Batch number = 15
Evaluating:  14%|█▍        | 15/109 [00:08<00:47,  1.99it/s]11/28/2021 01:51:12 - INFO - __main__ -   Batch number = 16
Evaluating:  15%|█▍        | 16/109 [00:09<00:46,  2.00it/s]11/28/2021 01:51:12 - INFO - __main__ -   Batch number = 17
Evaluating:  16%|█▌        | 17/109 [00:09<00:45,  2.01it/s]11/28/2021 01:51:13 - INFO - __main__ -   Batch number = 18
Evaluating:  17%|█▋        | 18/109 [00:10<00:45,  2.02it/s]11/28/2021 01:51:13 - INFO - __main__ -   Batch number = 19
Evaluating:  17%|█▋        | 19/109 [00:10<00:44,  2.02it/s]11/28/2021 01:51:14 - INFO - __main__ -   Batch number = 20
Evaluating:  18%|█▊        | 20/109 [00:11<00:44,  2.02it/s]11/28/2021 01:51:14 - INFO - __main__ -   Batch number = 21
Evaluating:  19%|█▉        | 21/109 [00:11<00:43,  2.02it/s]11/28/2021 01:51:15 - INFO - __main__ -   Batch number = 22
Evaluating:  20%|██        | 22/109 [00:12<00:43,  2.01it/s]11/28/2021 01:51:15 - INFO - __main__ -   Batch number = 23
Evaluating:  21%|██        | 23/109 [00:12<00:42,  2.02it/s]11/28/2021 01:51:16 - INFO - __main__ -   Batch number = 24
Evaluating:  22%|██▏       | 24/109 [00:13<00:42,  2.02it/s]11/28/2021 01:51:16 - INFO - __main__ -   Batch number = 25
Evaluating:  23%|██▎       | 25/109 [00:13<00:41,  2.03it/s]11/28/2021 01:51:17 - INFO - __main__ -   Batch number = 26
Evaluating:  24%|██▍       | 26/109 [00:14<00:40,  2.04it/s]11/28/2021 01:51:17 - INFO - __main__ -   Batch number = 27
Evaluating:  25%|██▍       | 27/109 [00:14<00:40,  2.05it/s]11/28/2021 01:51:18 - INFO - __main__ -   Batch number = 28
Evaluating:  26%|██▌       | 28/109 [00:15<00:39,  2.05it/s]11/28/2021 01:51:18 - INFO - __main__ -   Batch number = 29
Evaluating:  27%|██▋       | 29/109 [00:15<00:39,  2.05it/s]11/28/2021 01:51:19 - INFO - __main__ -   Batch number = 30
Evaluating:  28%|██▊       | 30/109 [00:15<00:38,  2.05it/s]11/28/2021 01:51:19 - INFO - __main__ -   Batch number = 31
Evaluating:  28%|██▊       | 31/109 [00:16<00:38,  2.05it/s]11/28/2021 01:51:20 - INFO - __main__ -   Batch number = 32
Evaluating:  29%|██▉       | 32/109 [00:16<00:37,  2.05it/s]11/28/2021 01:51:20 - INFO - __main__ -   Batch number = 33
Evaluating:  30%|███       | 33/109 [00:17<00:37,  2.05it/s]11/28/2021 01:51:21 - INFO - __main__ -   Batch number = 34
Evaluating:  31%|███       | 34/109 [00:17<00:36,  2.04it/s]11/28/2021 01:51:21 - INFO - __main__ -   Batch number = 35
Evaluating:  32%|███▏      | 35/109 [00:18<00:36,  2.04it/s]11/28/2021 01:51:22 - INFO - __main__ -   Batch number = 36
Evaluating:  33%|███▎      | 36/109 [00:18<00:35,  2.04it/s]11/28/2021 01:51:22 - INFO - __main__ -   Batch number = 37
Evaluating:  34%|███▍      | 37/109 [00:19<00:35,  2.05it/s]11/28/2021 01:51:23 - INFO - __main__ -   Batch number = 38
Evaluating:  35%|███▍      | 38/109 [00:19<00:34,  2.05it/s]11/28/2021 01:51:23 - INFO - __main__ -   Batch number = 39
Evaluating:  36%|███▌      | 39/109 [00:20<00:34,  2.04it/s]11/28/2021 01:51:24 - INFO - __main__ -   Batch number = 40
Evaluating:  37%|███▋      | 40/109 [00:20<00:33,  2.05it/s]11/28/2021 01:51:24 - INFO - __main__ -   Batch number = 41
Evaluating:  38%|███▊      | 41/109 [00:21<00:33,  2.04it/s]11/28/2021 01:51:25 - INFO - __main__ -   Batch number = 42
Evaluating:  39%|███▊      | 42/109 [00:21<00:32,  2.04it/s]11/28/2021 01:51:25 - INFO - __main__ -   Batch number = 43
Evaluating:  39%|███▉      | 43/109 [00:22<00:32,  2.04it/s]11/28/2021 01:51:26 - INFO - __main__ -   Batch number = 44
Evaluating:  40%|████      | 44/109 [00:22<00:31,  2.05it/s]11/28/2021 01:51:26 - INFO - __main__ -   Batch number = 45
Evaluating:  41%|████▏     | 45/109 [00:23<00:31,  2.03it/s]11/28/2021 01:51:27 - INFO - __main__ -   Batch number = 46
Evaluating:  42%|████▏     | 46/109 [00:23<00:31,  2.03it/s]11/28/2021 01:51:27 - INFO - __main__ -   Batch number = 47
Evaluating:  43%|████▎     | 47/109 [00:24<00:30,  2.02it/s]11/28/2021 01:51:28 - INFO - __main__ -   Batch number = 48
Evaluating:  44%|████▍     | 48/109 [00:24<00:30,  2.02it/s]11/28/2021 01:51:28 - INFO - __main__ -   Batch number = 49
Evaluating:  45%|████▍     | 49/109 [00:25<00:29,  2.01it/s]11/28/2021 01:51:29 - INFO - __main__ -   Batch number = 50
Evaluating:  46%|████▌     | 50/109 [00:25<00:29,  2.01it/s]11/28/2021 01:51:29 - INFO - __main__ -   Batch number = 51
Evaluating:  47%|████▋     | 51/109 [00:26<00:28,  2.04it/s]11/28/2021 01:51:30 - INFO - __main__ -   Batch number = 52
Evaluating:  48%|████▊     | 52/109 [00:26<00:25,  2.25it/s]11/28/2021 01:51:30 - INFO - __main__ -   Batch number = 53
Evaluating:  49%|████▊     | 53/109 [00:26<00:23,  2.42it/s]11/28/2021 01:51:30 - INFO - __main__ -   Batch number = 54
Evaluating:  50%|████▉     | 54/109 [00:27<00:21,  2.57it/s]11/28/2021 01:51:31 - INFO - __main__ -   Batch number = 55
Evaluating:  50%|█████     | 55/109 [00:27<00:20,  2.68it/s]11/28/2021 01:51:31 - INFO - __main__ -   Batch number = 56
Evaluating:  51%|█████▏    | 56/109 [00:27<00:19,  2.76it/s]11/28/2021 01:51:31 - INFO - __main__ -   Batch number = 57
Evaluating:  52%|█████▏    | 57/109 [00:28<00:19,  2.65it/s]11/28/2021 01:51:32 - INFO - __main__ -   Batch number = 58
Evaluating:  53%|█████▎    | 58/109 [00:28<00:18,  2.75it/s]11/28/2021 01:51:32 - INFO - __main__ -   Batch number = 59
Evaluating:  54%|█████▍    | 59/109 [00:29<00:17,  2.83it/s]11/28/2021 01:51:32 - INFO - __main__ -   Batch number = 60
Evaluating:  55%|█████▌    | 60/109 [00:29<00:17,  2.87it/s]11/28/2021 01:51:33 - INFO - __main__ -   Batch number = 61
Evaluating:  56%|█████▌    | 61/109 [00:29<00:16,  2.90it/s]11/28/2021 01:51:33 - INFO - __main__ -   Batch number = 62
Evaluating:  57%|█████▋    | 62/109 [00:30<00:16,  2.93it/s]11/28/2021 01:51:33 - INFO - __main__ -   Batch number = 63
Evaluating:  58%|█████▊    | 63/109 [00:30<00:15,  2.98it/s]11/28/2021 01:51:34 - INFO - __main__ -   Batch number = 64
Evaluating:  59%|█████▊    | 64/109 [00:30<00:15,  2.99it/s]11/28/2021 01:51:34 - INFO - __main__ -   Batch number = 65
Evaluating:  60%|█████▉    | 65/109 [00:31<00:14,  3.00it/s]11/28/2021 01:51:34 - INFO - __main__ -   Batch number = 66
Evaluating:  61%|██████    | 66/109 [00:31<00:14,  3.03it/s]11/28/2021 01:51:35 - INFO - __main__ -   Batch number = 67
Evaluating:  61%|██████▏   | 67/109 [00:31<00:13,  3.03it/s]11/28/2021 01:51:35 - INFO - __main__ -   Batch number = 68
Evaluating:  62%|██████▏   | 68/109 [00:32<00:14,  2.91it/s]11/28/2021 01:51:35 - INFO - __main__ -   Batch number = 69
Evaluating:  63%|██████▎   | 69/109 [00:32<00:15,  2.57it/s]11/28/2021 01:51:36 - INFO - __main__ -   Batch number = 70
Evaluating:  64%|██████▍   | 70/109 [00:33<00:16,  2.37it/s]11/28/2021 01:51:36 - INFO - __main__ -   Batch number = 71
Evaluating:  65%|██████▌   | 71/109 [00:33<00:17,  2.19it/s]11/28/2021 01:51:37 - INFO - __main__ -   Batch number = 72
Evaluating:  66%|██████▌   | 72/109 [00:34<00:17,  2.13it/s]11/28/2021 01:51:37 - INFO - __main__ -   Batch number = 73
Evaluating:  67%|██████▋   | 73/109 [00:34<00:17,  2.09it/s]11/28/2021 01:51:38 - INFO - __main__ -   Batch number = 74
Evaluating:  68%|██████▊   | 74/109 [00:35<00:16,  2.07it/s]11/28/2021 01:51:38 - INFO - __main__ -   Batch number = 75
Evaluating:  69%|██████▉   | 75/109 [00:35<00:16,  2.05it/s]11/28/2021 01:51:39 - INFO - __main__ -   Batch number = 76
Evaluating:  70%|██████▉   | 76/109 [00:36<00:16,  2.04it/s]11/28/2021 01:51:39 - INFO - __main__ -   Batch number = 77
Evaluating:  71%|███████   | 77/109 [00:36<00:15,  2.07it/s]11/28/2021 01:51:40 - INFO - __main__ -   Batch number = 78
Evaluating:  72%|███████▏  | 78/109 [00:37<00:15,  2.06it/s]11/28/2021 01:51:40 - INFO - __main__ -   Batch number = 79
Evaluating:  72%|███████▏  | 79/109 [00:37<00:14,  2.04it/s]11/28/2021 01:51:41 - INFO - __main__ -   Batch number = 80
Evaluating:  73%|███████▎  | 80/109 [00:38<00:14,  2.04it/s]11/28/2021 01:51:41 - INFO - __main__ -   Batch number = 81
Evaluating:  74%|███████▍  | 81/109 [00:38<00:14,  2.00it/s]11/28/2021 01:51:42 - INFO - __main__ -   Batch number = 82
Evaluating:  75%|███████▌  | 82/109 [00:39<00:13,  2.01it/s]11/28/2021 01:51:42 - INFO - __main__ -   Batch number = 83
Evaluating:  76%|███████▌  | 83/109 [00:39<00:12,  2.01it/s]11/28/2021 01:51:43 - INFO - __main__ -   Batch number = 84
Evaluating:  77%|███████▋  | 84/109 [00:40<00:12,  2.02it/s]11/28/2021 01:51:43 - INFO - __main__ -   Batch number = 85
Evaluating:  78%|███████▊  | 85/109 [00:40<00:11,  2.02it/s]11/28/2021 01:51:44 - INFO - __main__ -   Batch number = 86
Evaluating:  79%|███████▉  | 86/109 [00:41<00:11,  2.03it/s]11/28/2021 01:51:44 - INFO - __main__ -   Batch number = 87
Evaluating:  80%|███████▉  | 87/109 [00:41<00:10,  2.03it/s]11/28/2021 01:51:45 - INFO - __main__ -   Batch number = 88
Evaluating:  81%|████████  | 88/109 [00:42<00:10,  2.03it/s]11/28/2021 01:51:45 - INFO - __main__ -   Batch number = 89
Evaluating:  82%|████████▏ | 89/109 [00:42<00:09,  2.02it/s]11/28/2021 01:51:46 - INFO - __main__ -   Batch number = 90
Evaluating:  83%|████████▎ | 90/109 [00:43<00:09,  2.01it/s]11/28/2021 01:51:46 - INFO - __main__ -   Batch number = 91
Evaluating:  83%|████████▎ | 91/109 [00:43<00:09,  1.92it/s]11/28/2021 01:51:47 - INFO - __main__ -   Batch number = 92
Evaluating:  84%|████████▍ | 92/109 [00:44<00:08,  1.94it/s]11/28/2021 01:51:47 - INFO - __main__ -   Batch number = 93
Evaluating:  85%|████████▌ | 93/109 [00:44<00:08,  1.96it/s]11/28/2021 01:51:48 - INFO - __main__ -   Batch number = 94
Evaluating:  86%|████████▌ | 94/109 [00:45<00:07,  1.97it/s]11/28/2021 01:51:48 - INFO - __main__ -   Batch number = 95
Evaluating:  87%|████████▋ | 95/109 [00:45<00:07,  1.98it/s]11/28/2021 01:51:49 - INFO - __main__ -   Batch number = 96
Evaluating:  88%|████████▊ | 96/109 [00:46<00:06,  1.98it/s]11/28/2021 01:51:49 - INFO - __main__ -   Batch number = 97
Evaluating:  89%|████████▉ | 97/109 [00:46<00:06,  1.98it/s]11/28/2021 01:51:50 - INFO - __main__ -   Batch number = 98
Evaluating:  90%|████████▉ | 98/109 [00:47<00:05,  1.99it/s]11/28/2021 01:51:50 - INFO - __main__ -   Batch number = 99
Evaluating:  91%|█████████ | 99/109 [00:47<00:05,  1.99it/s]11/28/2021 01:51:51 - INFO - __main__ -   Batch number = 100
Evaluating:  92%|█████████▏| 100/109 [00:48<00:04,  2.00it/s]11/28/2021 01:51:51 - INFO - __main__ -   Batch number = 101
Evaluating:  93%|█████████▎| 101/109 [00:48<00:04,  1.99it/s]11/28/2021 01:51:52 - INFO - __main__ -   Batch number = 102
Evaluating:  94%|█████████▎| 102/109 [00:49<00:03,  1.99it/s]11/28/2021 01:51:52 - INFO - __main__ -   Batch number = 103
Evaluating:  94%|█████████▍| 103/109 [00:49<00:03,  1.98it/s]11/28/2021 01:51:53 - INFO - __main__ -   Batch number = 104
Evaluating:  95%|█████████▌| 104/109 [00:50<00:02,  2.00it/s]11/28/2021 01:51:53 - INFO - __main__ -   Batch number = 105
Evaluating:  96%|█████████▋| 105/109 [00:50<00:02,  2.00it/s]11/28/2021 01:51:54 - INFO - __main__ -   Batch number = 106
Evaluating:  97%|█████████▋| 106/109 [00:51<00:01,  2.00it/s]11/28/2021 01:51:54 - INFO - __main__ -   Batch number = 107
Evaluating:  98%|█████████▊| 107/109 [00:51<00:00,  2.15it/s]11/28/2021 01:51:55 - INFO - __main__ -   Batch number = 108
Evaluating:  99%|█████████▉| 108/109 [00:51<00:00,  2.31it/s]11/28/2021 01:51:55 - INFO - __main__ -   Batch number = 109
Evaluating: 100%|██████████| 109/109 [00:51<00:00,  2.97it/s]Evaluating: 100%|██████████| 109/109 [00:51<00:00,  2.10it/s]
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:51:57 - INFO - __main__ -   ***** Evaluation result  in zh *****
11/28/2021 01:51:57 - INFO - __main__ -     f1 = 0.6215680114885788
11/28/2021 01:51:57 - INFO - __main__ -     loss = 1.4507080710262334
11/28/2021 01:51:57 - INFO - __main__ -     precision = 0.6283868391920585
11/28/2021 01:51:57 - INFO - __main__ -     recall = 0.6148955817587726
57.55user 20.18system 1:18.33elapsed 99%CPU (0avgtext+0avgdata 3986928maxresident)k
0inputs+760outputs (0major+1603692minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:52:02 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='is', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:52:02 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:52:02 - INFO - __main__ -   Seed = 1
11/28/2021 01:52:02 - INFO - root -   save model
11/28/2021 01:52:02 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='is', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:52:02 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:52:05 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:52:11 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:52:11 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:52:11 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
11/28/2021 01:52:11 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:52:11 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:52:11 - INFO - root -   loading lang adpater cs/wiki@ukp
11/28/2021 01:52:11 - INFO - __main__ -   Adapter Languages : ['cs'], Length : 1
11/28/2021 01:52:11 - INFO - __main__ -   Adapter Names ['cs/wiki@ukp'], Length : 1
11/28/2021 01:52:11 - INFO - __main__ -   Language = cs
11/28/2021 01:52:11 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/cs/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_cs_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/adapter_config.json
Adding adapter 'cs' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/head_config.json
11/28/2021 01:52:21 - INFO - __main__ -   Language adapter for is not found, using cs instead
11/28/2021 01:52:21 - INFO - __main__ -   Set active language adapter to cs
11/28/2021 01:52:21 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:52:21 - INFO - __main__ -   Adapter Languages = ['cs']
11/28/2021 01:52:21 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_is_bert-base-multilingual-cased_128
11/28/2021 01:52:22 - INFO - __main__ -   ***** Running evaluation  in is *****
11/28/2021 01:52:22 - INFO - __main__ -     Num examples = 6401
11/28/2021 01:52:22 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/201 [00:00<?, ?it/s]11/28/2021 01:52:22 - INFO - __main__ -   Batch number = 1
Evaluating:   0%|          | 1/201 [00:00<01:43,  1.94it/s]11/28/2021 01:52:23 - INFO - __main__ -   Batch number = 2
Evaluating:   1%|          | 2/201 [00:01<01:40,  1.99it/s]11/28/2021 01:52:23 - INFO - __main__ -   Batch number = 3
Evaluating:   1%|▏         | 3/201 [00:01<01:31,  2.16it/s]11/28/2021 01:52:24 - INFO - __main__ -   Batch number = 4
Evaluating:   2%|▏         | 4/201 [00:01<01:20,  2.45it/s]11/28/2021 01:52:24 - INFO - __main__ -   Batch number = 5
Evaluating:   2%|▏         | 5/201 [00:02<01:19,  2.46it/s]11/28/2021 01:52:25 - INFO - __main__ -   Batch number = 6
Evaluating:   3%|▎         | 6/201 [00:02<01:24,  2.30it/s]11/28/2021 01:52:25 - INFO - __main__ -   Batch number = 7
Evaluating:   3%|▎         | 7/201 [00:03<01:24,  2.31it/s]11/28/2021 01:52:26 - INFO - __main__ -   Batch number = 8
Evaluating:   4%|▍         | 8/201 [00:03<01:06,  2.89it/s]11/28/2021 01:52:26 - INFO - __main__ -   Batch number = 9
Evaluating:   4%|▍         | 9/201 [00:03<00:55,  3.45it/s]11/28/2021 01:52:26 - INFO - __main__ -   Batch number = 10
Evaluating:   5%|▍         | 10/201 [00:03<00:54,  3.53it/s]11/28/2021 01:52:26 - INFO - __main__ -   Batch number = 11
Evaluating:   5%|▌         | 11/201 [00:03<00:55,  3.39it/s]11/28/2021 01:52:27 - INFO - __main__ -   Batch number = 12
Evaluating:   6%|▌         | 12/201 [00:04<01:02,  3.04it/s]11/28/2021 01:52:27 - INFO - __main__ -   Batch number = 13
Evaluating:   6%|▋         | 13/201 [00:04<01:01,  3.07it/s]11/28/2021 01:52:27 - INFO - __main__ -   Batch number = 14
Evaluating:   7%|▋         | 14/201 [00:05<00:59,  3.12it/s]11/28/2021 01:52:28 - INFO - __main__ -   Batch number = 15
Evaluating:   7%|▋         | 15/201 [00:05<00:59,  3.12it/s]11/28/2021 01:52:28 - INFO - __main__ -   Batch number = 16
Evaluating:   8%|▊         | 16/201 [00:05<00:58,  3.16it/s]11/28/2021 01:52:28 - INFO - __main__ -   Batch number = 17
Evaluating:   8%|▊         | 17/201 [00:05<00:58,  3.16it/s]11/28/2021 01:52:28 - INFO - __main__ -   Batch number = 18
Evaluating:   9%|▉         | 18/201 [00:06<00:58,  3.14it/s]11/28/2021 01:52:29 - INFO - __main__ -   Batch number = 19
Evaluating:   9%|▉         | 19/201 [00:06<00:52,  3.44it/s]11/28/2021 01:52:29 - INFO - __main__ -   Batch number = 20
Evaluating:  10%|▉         | 20/201 [00:06<00:45,  3.97it/s]11/28/2021 01:52:29 - INFO - __main__ -   Batch number = 21
Evaluating:  10%|█         | 21/201 [00:06<00:40,  4.43it/s]11/28/2021 01:52:29 - INFO - __main__ -   Batch number = 22
Evaluating:  11%|█         | 22/201 [00:07<00:37,  4.82it/s]11/28/2021 01:52:29 - INFO - __main__ -   Batch number = 23
Evaluating:  11%|█▏        | 23/201 [00:07<00:36,  4.83it/s]11/28/2021 01:52:30 - INFO - __main__ -   Batch number = 24
Evaluating:  12%|█▏        | 24/201 [00:07<00:41,  4.23it/s]11/28/2021 01:52:30 - INFO - __main__ -   Batch number = 25
Evaluating:  12%|█▏        | 25/201 [00:07<00:46,  3.82it/s]11/28/2021 01:52:30 - INFO - __main__ -   Batch number = 26
Evaluating:  13%|█▎        | 26/201 [00:08<00:48,  3.58it/s]11/28/2021 01:52:31 - INFO - __main__ -   Batch number = 27
Evaluating:  13%|█▎        | 27/201 [00:08<00:50,  3.46it/s]11/28/2021 01:52:31 - INFO - __main__ -   Batch number = 28
Evaluating:  14%|█▍        | 28/201 [00:08<00:51,  3.36it/s]11/28/2021 01:52:31 - INFO - __main__ -   Batch number = 29
Evaluating:  14%|█▍        | 29/201 [00:09<00:52,  3.28it/s]11/28/2021 01:52:32 - INFO - __main__ -   Batch number = 30
Evaluating:  15%|█▍        | 30/201 [00:09<00:52,  3.26it/s]11/28/2021 01:52:32 - INFO - __main__ -   Batch number = 31
Evaluating:  15%|█▌        | 31/201 [00:09<00:52,  3.23it/s]11/28/2021 01:52:32 - INFO - __main__ -   Batch number = 32
Evaluating:  16%|█▌        | 32/201 [00:10<00:52,  3.19it/s]11/28/2021 01:52:33 - INFO - __main__ -   Batch number = 33
Evaluating:  16%|█▋        | 33/201 [00:10<00:52,  3.19it/s]11/28/2021 01:52:33 - INFO - __main__ -   Batch number = 34
Evaluating:  17%|█▋        | 34/201 [00:10<00:53,  3.15it/s]11/28/2021 01:52:33 - INFO - __main__ -   Batch number = 35
Evaluating:  17%|█▋        | 35/201 [00:11<00:53,  3.08it/s]11/28/2021 01:52:34 - INFO - __main__ -   Batch number = 36
Evaluating:  18%|█▊        | 36/201 [00:11<00:53,  3.07it/s]11/28/2021 01:52:34 - INFO - __main__ -   Batch number = 37
Evaluating:  18%|█▊        | 37/201 [00:11<00:52,  3.10it/s]11/28/2021 01:52:34 - INFO - __main__ -   Batch number = 38
Evaluating:  19%|█▉        | 38/201 [00:12<00:52,  3.11it/s]11/28/2021 01:52:34 - INFO - __main__ -   Batch number = 39
Evaluating:  19%|█▉        | 39/201 [00:12<00:52,  3.11it/s]11/28/2021 01:52:35 - INFO - __main__ -   Batch number = 40
Evaluating:  20%|█▉        | 40/201 [00:12<00:52,  3.09it/s]11/28/2021 01:52:35 - INFO - __main__ -   Batch number = 41
Evaluating:  20%|██        | 41/201 [00:12<00:51,  3.08it/s]11/28/2021 01:52:35 - INFO - __main__ -   Batch number = 42
Evaluating:  21%|██        | 42/201 [00:13<00:51,  3.09it/s]11/28/2021 01:52:36 - INFO - __main__ -   Batch number = 43
Evaluating:  21%|██▏       | 43/201 [00:13<00:50,  3.11it/s]11/28/2021 01:52:36 - INFO - __main__ -   Batch number = 44
Evaluating:  22%|██▏       | 44/201 [00:13<00:50,  3.13it/s]11/28/2021 01:52:36 - INFO - __main__ -   Batch number = 45
Evaluating:  22%|██▏       | 45/201 [00:14<01:01,  2.53it/s]11/28/2021 01:52:37 - INFO - __main__ -   Batch number = 46
Evaluating:  23%|██▎       | 46/201 [00:14<00:56,  2.74it/s]11/28/2021 01:52:37 - INFO - __main__ -   Batch number = 47
Evaluating:  23%|██▎       | 47/201 [00:15<00:52,  2.91it/s]11/28/2021 01:52:38 - INFO - __main__ -   Batch number = 48
Evaluating:  24%|██▍       | 48/201 [00:15<00:50,  3.01it/s]11/28/2021 01:52:38 - INFO - __main__ -   Batch number = 49
Evaluating:  24%|██▍       | 49/201 [00:15<00:57,  2.63it/s]11/28/2021 01:52:38 - INFO - __main__ -   Batch number = 50
Evaluating:  25%|██▍       | 50/201 [00:16<01:02,  2.41it/s]11/28/2021 01:52:39 - INFO - __main__ -   Batch number = 51
Evaluating:  25%|██▌       | 51/201 [00:16<01:06,  2.26it/s]11/28/2021 01:52:39 - INFO - __main__ -   Batch number = 52
Evaluating:  26%|██▌       | 52/201 [00:17<01:08,  2.18it/s]11/28/2021 01:52:40 - INFO - __main__ -   Batch number = 53
Evaluating:  26%|██▋       | 53/201 [00:17<01:09,  2.13it/s]11/28/2021 01:52:40 - INFO - __main__ -   Batch number = 54
Evaluating:  27%|██▋       | 54/201 [00:18<01:10,  2.09it/s]11/28/2021 01:52:41 - INFO - __main__ -   Batch number = 55
Evaluating:  27%|██▋       | 55/201 [00:18<01:10,  2.06it/s]11/28/2021 01:52:41 - INFO - __main__ -   Batch number = 56
Evaluating:  28%|██▊       | 56/201 [00:19<01:10,  2.05it/s]11/28/2021 01:52:42 - INFO - __main__ -   Batch number = 57
Evaluating:  28%|██▊       | 57/201 [00:19<01:10,  2.05it/s]11/28/2021 01:52:42 - INFO - __main__ -   Batch number = 58
Evaluating:  29%|██▉       | 58/201 [00:20<01:10,  2.03it/s]11/28/2021 01:52:43 - INFO - __main__ -   Batch number = 59
Evaluating:  29%|██▉       | 59/201 [00:20<01:10,  2.00it/s]11/28/2021 01:52:43 - INFO - __main__ -   Batch number = 60
Evaluating:  30%|██▉       | 60/201 [00:21<01:10,  2.01it/s]11/28/2021 01:52:44 - INFO - __main__ -   Batch number = 61
Evaluating:  30%|███       | 61/201 [00:21<01:06,  2.11it/s]11/28/2021 01:52:44 - INFO - __main__ -   Batch number = 62
Evaluating:  31%|███       | 62/201 [00:22<01:05,  2.14it/s]11/28/2021 01:52:45 - INFO - __main__ -   Batch number = 63
Evaluating:  31%|███▏      | 63/201 [00:22<01:04,  2.13it/s]11/28/2021 01:52:45 - INFO - __main__ -   Batch number = 64
Evaluating:  32%|███▏      | 64/201 [00:23<01:04,  2.14it/s]11/28/2021 01:52:46 - INFO - __main__ -   Batch number = 65
Evaluating:  32%|███▏      | 65/201 [00:23<01:04,  2.10it/s]11/28/2021 01:52:46 - INFO - __main__ -   Batch number = 66
Evaluating:  33%|███▎      | 66/201 [00:24<01:00,  2.24it/s]11/28/2021 01:52:47 - INFO - __main__ -   Batch number = 67
Evaluating:  33%|███▎      | 67/201 [00:24<01:06,  2.01it/s]11/28/2021 01:52:47 - INFO - __main__ -   Batch number = 68
Evaluating:  34%|███▍      | 68/201 [00:25<01:03,  2.09it/s]11/28/2021 01:52:48 - INFO - __main__ -   Batch number = 69
Evaluating:  34%|███▍      | 69/201 [00:25<01:02,  2.10it/s]11/28/2021 01:52:48 - INFO - __main__ -   Batch number = 70
Evaluating:  35%|███▍      | 70/201 [00:26<01:03,  2.07it/s]11/28/2021 01:52:49 - INFO - __main__ -   Batch number = 71
Evaluating:  35%|███▌      | 71/201 [00:26<01:03,  2.06it/s]11/28/2021 01:52:49 - INFO - __main__ -   Batch number = 72
Evaluating:  36%|███▌      | 72/201 [00:27<01:03,  2.04it/s]11/28/2021 01:52:50 - INFO - __main__ -   Batch number = 73
Evaluating:  36%|███▋      | 73/201 [00:27<01:03,  2.02it/s]11/28/2021 01:52:50 - INFO - __main__ -   Batch number = 74
Evaluating:  37%|███▋      | 74/201 [00:28<01:03,  2.01it/s]11/28/2021 01:52:51 - INFO - __main__ -   Batch number = 75
Evaluating:  37%|███▋      | 75/201 [00:28<01:02,  2.02it/s]11/28/2021 01:52:51 - INFO - __main__ -   Batch number = 76
Evaluating:  38%|███▊      | 76/201 [00:29<01:05,  1.92it/s]11/28/2021 01:52:52 - INFO - __main__ -   Batch number = 77
Evaluating:  38%|███▊      | 77/201 [00:29<01:10,  1.77it/s]11/28/2021 01:52:52 - INFO - __main__ -   Batch number = 78
Evaluating:  39%|███▉      | 78/201 [00:30<01:13,  1.67it/s]11/28/2021 01:52:53 - INFO - __main__ -   Batch number = 79
Evaluating:  39%|███▉      | 79/201 [00:31<01:15,  1.62it/s]11/28/2021 01:52:54 - INFO - __main__ -   Batch number = 80
Evaluating:  40%|███▉      | 80/201 [00:31<01:16,  1.59it/s]11/28/2021 01:52:54 - INFO - __main__ -   Batch number = 81
Evaluating:  40%|████      | 81/201 [00:32<01:16,  1.58it/s]11/28/2021 01:52:55 - INFO - __main__ -   Batch number = 82
Evaluating:  41%|████      | 82/201 [00:33<01:16,  1.56it/s]11/28/2021 01:52:56 - INFO - __main__ -   Batch number = 83
Evaluating:  41%|████▏     | 83/201 [00:33<01:16,  1.54it/s]11/28/2021 01:52:56 - INFO - __main__ -   Batch number = 84
Evaluating:  42%|████▏     | 84/201 [00:34<01:17,  1.52it/s]11/28/2021 01:52:57 - INFO - __main__ -   Batch number = 85
Evaluating:  42%|████▏     | 85/201 [00:35<01:17,  1.50it/s]11/28/2021 01:52:58 - INFO - __main__ -   Batch number = 86
Evaluating:  43%|████▎     | 86/201 [00:35<01:17,  1.49it/s]11/28/2021 01:52:58 - INFO - __main__ -   Batch number = 87
Evaluating:  43%|████▎     | 87/201 [00:36<01:10,  1.61it/s]11/28/2021 01:52:59 - INFO - __main__ -   Batch number = 88
Evaluating:  44%|████▍     | 88/201 [00:36<01:07,  1.68it/s]11/28/2021 01:52:59 - INFO - __main__ -   Batch number = 89
Evaluating:  44%|████▍     | 89/201 [00:37<01:06,  1.68it/s]11/28/2021 01:53:00 - INFO - __main__ -   Batch number = 90
Evaluating:  45%|████▍     | 90/201 [00:37<01:03,  1.75it/s]11/28/2021 01:53:00 - INFO - __main__ -   Batch number = 91
Evaluating:  45%|████▌     | 91/201 [00:38<01:00,  1.82it/s]11/28/2021 01:53:01 - INFO - __main__ -   Batch number = 92
Evaluating:  46%|████▌     | 92/201 [00:38<00:58,  1.87it/s]11/28/2021 01:53:02 - INFO - __main__ -   Batch number = 93
Evaluating:  46%|████▋     | 93/201 [00:39<00:57,  1.87it/s]11/28/2021 01:53:02 - INFO - __main__ -   Batch number = 94
Evaluating:  47%|████▋     | 94/201 [00:40<00:55,  1.92it/s]11/28/2021 01:53:02 - INFO - __main__ -   Batch number = 95
Evaluating:  47%|████▋     | 95/201 [00:40<00:54,  1.94it/s]11/28/2021 01:53:03 - INFO - __main__ -   Batch number = 96
Evaluating:  48%|████▊     | 96/201 [00:41<00:53,  1.95it/s]11/28/2021 01:53:04 - INFO - __main__ -   Batch number = 97
Evaluating:  48%|████▊     | 97/201 [00:41<00:53,  1.96it/s]11/28/2021 01:53:04 - INFO - __main__ -   Batch number = 98
Evaluating:  49%|████▉     | 98/201 [00:42<00:52,  1.97it/s]11/28/2021 01:53:05 - INFO - __main__ -   Batch number = 99
Evaluating:  49%|████▉     | 99/201 [00:42<00:51,  1.98it/s]11/28/2021 01:53:05 - INFO - __main__ -   Batch number = 100
Evaluating:  50%|████▉     | 100/201 [00:43<00:51,  1.96it/s]11/28/2021 01:53:06 - INFO - __main__ -   Batch number = 101
Evaluating:  50%|█████     | 101/201 [00:43<00:50,  1.96it/s]11/28/2021 01:53:06 - INFO - __main__ -   Batch number = 102
Evaluating:  51%|█████     | 102/201 [00:44<00:49,  1.99it/s]11/28/2021 01:53:07 - INFO - __main__ -   Batch number = 103
Evaluating:  51%|█████     | 103/201 [00:44<00:49,  1.99it/s]11/28/2021 01:53:07 - INFO - __main__ -   Batch number = 104
Evaluating:  52%|█████▏    | 104/201 [00:45<00:48,  2.01it/s]11/28/2021 01:53:08 - INFO - __main__ -   Batch number = 105
Evaluating:  52%|█████▏    | 105/201 [00:45<00:44,  2.16it/s]11/28/2021 01:53:08 - INFO - __main__ -   Batch number = 106
Evaluating:  53%|█████▎    | 106/201 [00:45<00:40,  2.34it/s]11/28/2021 01:53:08 - INFO - __main__ -   Batch number = 107
Evaluating:  53%|█████▎    | 107/201 [00:46<00:37,  2.50it/s]11/28/2021 01:53:09 - INFO - __main__ -   Batch number = 108
Evaluating:  54%|█████▎    | 108/201 [00:46<00:35,  2.61it/s]11/28/2021 01:53:09 - INFO - __main__ -   Batch number = 109
Evaluating:  54%|█████▍    | 109/201 [00:46<00:33,  2.71it/s]11/28/2021 01:53:09 - INFO - __main__ -   Batch number = 110
Evaluating:  55%|█████▍    | 110/201 [00:47<00:32,  2.78it/s]11/28/2021 01:53:10 - INFO - __main__ -   Batch number = 111
Evaluating:  55%|█████▌    | 111/201 [00:47<00:31,  2.83it/s]11/28/2021 01:53:10 - INFO - __main__ -   Batch number = 112
Evaluating:  56%|█████▌    | 112/201 [00:47<00:31,  2.86it/s]11/28/2021 01:53:10 - INFO - __main__ -   Batch number = 113
Evaluating:  56%|█████▌    | 113/201 [00:48<00:30,  2.89it/s]11/28/2021 01:53:11 - INFO - __main__ -   Batch number = 114
Evaluating:  57%|█████▋    | 114/201 [00:48<00:29,  2.90it/s]11/28/2021 01:53:11 - INFO - __main__ -   Batch number = 115
Evaluating:  57%|█████▋    | 115/201 [00:48<00:29,  2.91it/s]11/28/2021 01:53:11 - INFO - __main__ -   Batch number = 116
Evaluating:  58%|█████▊    | 116/201 [00:49<00:29,  2.93it/s]11/28/2021 01:53:12 - INFO - __main__ -   Batch number = 117
Evaluating:  58%|█████▊    | 117/201 [00:49<00:28,  2.90it/s]11/28/2021 01:53:12 - INFO - __main__ -   Batch number = 118
Evaluating:  59%|█████▊    | 118/201 [00:49<00:28,  2.91it/s]11/28/2021 01:53:12 - INFO - __main__ -   Batch number = 119
Evaluating:  59%|█████▉    | 119/201 [00:50<00:28,  2.92it/s]11/28/2021 01:53:13 - INFO - __main__ -   Batch number = 120
Evaluating:  60%|█████▉    | 120/201 [00:50<00:27,  2.92it/s]11/28/2021 01:53:13 - INFO - __main__ -   Batch number = 121
Evaluating:  60%|██████    | 121/201 [00:50<00:27,  2.93it/s]11/28/2021 01:53:13 - INFO - __main__ -   Batch number = 122
Evaluating:  61%|██████    | 122/201 [00:51<00:26,  2.94it/s]11/28/2021 01:53:14 - INFO - __main__ -   Batch number = 123
Evaluating:  61%|██████    | 123/201 [00:51<00:26,  2.94it/s]11/28/2021 01:53:14 - INFO - __main__ -   Batch number = 124
Evaluating:  62%|██████▏   | 124/201 [00:51<00:26,  2.95it/s]11/28/2021 01:53:14 - INFO - __main__ -   Batch number = 125
Evaluating:  62%|██████▏   | 125/201 [00:52<00:25,  2.96it/s]11/28/2021 01:53:15 - INFO - __main__ -   Batch number = 126
Evaluating:  63%|██████▎   | 126/201 [00:52<00:25,  2.95it/s]11/28/2021 01:53:15 - INFO - __main__ -   Batch number = 127
Evaluating:  63%|██████▎   | 127/201 [00:52<00:25,  2.95it/s]11/28/2021 01:53:15 - INFO - __main__ -   Batch number = 128
Evaluating:  64%|██████▎   | 128/201 [00:53<00:25,  2.91it/s]11/28/2021 01:53:16 - INFO - __main__ -   Batch number = 129
Evaluating:  64%|██████▍   | 129/201 [00:53<00:24,  2.91it/s]11/28/2021 01:53:16 - INFO - __main__ -   Batch number = 130
Evaluating:  65%|██████▍   | 130/201 [00:53<00:24,  2.91it/s]11/28/2021 01:53:16 - INFO - __main__ -   Batch number = 131
Evaluating:  65%|██████▌   | 131/201 [00:54<00:24,  2.92it/s]11/28/2021 01:53:17 - INFO - __main__ -   Batch number = 132
Evaluating:  66%|██████▌   | 132/201 [00:54<00:23,  2.95it/s]11/28/2021 01:53:17 - INFO - __main__ -   Batch number = 133
Evaluating:  66%|██████▌   | 133/201 [00:54<00:22,  2.96it/s]11/28/2021 01:53:17 - INFO - __main__ -   Batch number = 134
Evaluating:  67%|██████▋   | 134/201 [00:55<00:22,  2.96it/s]11/28/2021 01:53:18 - INFO - __main__ -   Batch number = 135
Evaluating:  67%|██████▋   | 135/201 [00:55<00:22,  2.97it/s]11/28/2021 01:53:18 - INFO - __main__ -   Batch number = 136
Evaluating:  68%|██████▊   | 136/201 [00:55<00:21,  2.97it/s]11/28/2021 01:53:18 - INFO - __main__ -   Batch number = 137
Evaluating:  68%|██████▊   | 137/201 [00:56<00:21,  2.97it/s]11/28/2021 01:53:19 - INFO - __main__ -   Batch number = 138
Evaluating:  69%|██████▊   | 138/201 [00:56<00:21,  2.97it/s]11/28/2021 01:53:19 - INFO - __main__ -   Batch number = 139
Evaluating:  69%|██████▉   | 139/201 [00:56<00:20,  2.98it/s]11/28/2021 01:53:19 - INFO - __main__ -   Batch number = 140
Evaluating:  70%|██████▉   | 140/201 [00:57<00:20,  2.97it/s]11/28/2021 01:53:20 - INFO - __main__ -   Batch number = 141
Evaluating:  70%|███████   | 141/201 [00:57<00:20,  2.98it/s]11/28/2021 01:53:20 - INFO - __main__ -   Batch number = 142
Evaluating:  71%|███████   | 142/201 [00:57<00:19,  2.99it/s]11/28/2021 01:53:20 - INFO - __main__ -   Batch number = 143
Evaluating:  71%|███████   | 143/201 [00:58<00:19,  2.97it/s]11/28/2021 01:53:21 - INFO - __main__ -   Batch number = 144
Evaluating:  72%|███████▏  | 144/201 [00:58<00:19,  3.00it/s]11/28/2021 01:53:21 - INFO - __main__ -   Batch number = 145
Evaluating:  72%|███████▏  | 145/201 [00:58<00:18,  2.99it/s]11/28/2021 01:53:21 - INFO - __main__ -   Batch number = 146
Evaluating:  73%|███████▎  | 146/201 [00:59<00:18,  2.98it/s]11/28/2021 01:53:22 - INFO - __main__ -   Batch number = 147
Evaluating:  73%|███████▎  | 147/201 [00:59<00:18,  2.97it/s]11/28/2021 01:53:22 - INFO - __main__ -   Batch number = 148
Evaluating:  74%|███████▎  | 148/201 [00:59<00:17,  3.00it/s]11/28/2021 01:53:22 - INFO - __main__ -   Batch number = 149
Evaluating:  74%|███████▍  | 149/201 [01:00<00:17,  3.00it/s]11/28/2021 01:53:23 - INFO - __main__ -   Batch number = 150
Evaluating:  75%|███████▍  | 150/201 [01:00<00:17,  2.99it/s]11/28/2021 01:53:23 - INFO - __main__ -   Batch number = 151
Evaluating:  75%|███████▌  | 151/201 [01:00<00:16,  2.96it/s]11/28/2021 01:53:23 - INFO - __main__ -   Batch number = 152
Evaluating:  76%|███████▌  | 152/201 [01:01<00:16,  2.92it/s]11/28/2021 01:53:24 - INFO - __main__ -   Batch number = 153
Evaluating:  76%|███████▌  | 153/201 [01:01<00:16,  2.92it/s]11/28/2021 01:53:24 - INFO - __main__ -   Batch number = 154
Evaluating:  77%|███████▋  | 154/201 [01:02<00:16,  2.90it/s]11/28/2021 01:53:25 - INFO - __main__ -   Batch number = 155
Evaluating:  77%|███████▋  | 155/201 [01:02<00:15,  2.90it/s]11/28/2021 01:53:25 - INFO - __main__ -   Batch number = 156
Evaluating:  78%|███████▊  | 156/201 [01:02<00:15,  2.92it/s]11/28/2021 01:53:25 - INFO - __main__ -   Batch number = 157
Evaluating:  78%|███████▊  | 157/201 [01:03<00:15,  2.93it/s]11/28/2021 01:53:26 - INFO - __main__ -   Batch number = 158
Evaluating:  79%|███████▊  | 158/201 [01:03<00:14,  3.07it/s]11/28/2021 01:53:26 - INFO - __main__ -   Batch number = 159
Evaluating:  79%|███████▉  | 159/201 [01:03<00:11,  3.54it/s]11/28/2021 01:53:26 - INFO - __main__ -   Batch number = 160
Evaluating:  80%|███████▉  | 160/201 [01:03<00:12,  3.33it/s]11/28/2021 01:53:26 - INFO - __main__ -   Batch number = 161
Evaluating:  80%|████████  | 161/201 [01:04<00:12,  3.19it/s]11/28/2021 01:53:27 - INFO - __main__ -   Batch number = 162
Evaluating:  81%|████████  | 162/201 [01:04<00:12,  3.06it/s]11/28/2021 01:53:27 - INFO - __main__ -   Batch number = 163
Evaluating:  81%|████████  | 163/201 [01:04<00:12,  2.97it/s]11/28/2021 01:53:27 - INFO - __main__ -   Batch number = 164
Evaluating:  82%|████████▏ | 164/201 [01:05<00:12,  2.96it/s]11/28/2021 01:53:28 - INFO - __main__ -   Batch number = 165
Evaluating:  82%|████████▏ | 165/201 [01:05<00:12,  2.92it/s]11/28/2021 01:53:28 - INFO - __main__ -   Batch number = 166
Evaluating:  83%|████████▎ | 166/201 [01:05<00:12,  2.91it/s]11/28/2021 01:53:28 - INFO - __main__ -   Batch number = 167
Evaluating:  83%|████████▎ | 167/201 [01:06<00:11,  2.90it/s]11/28/2021 01:53:29 - INFO - __main__ -   Batch number = 168
Evaluating:  84%|████████▎ | 168/201 [01:06<00:11,  2.86it/s]11/28/2021 01:53:29 - INFO - __main__ -   Batch number = 169
Evaluating:  84%|████████▍ | 169/201 [01:07<00:11,  2.84it/s]11/28/2021 01:53:30 - INFO - __main__ -   Batch number = 170
Evaluating:  85%|████████▍ | 170/201 [01:07<00:10,  2.86it/s]11/28/2021 01:53:30 - INFO - __main__ -   Batch number = 171
Evaluating:  85%|████████▌ | 171/201 [01:07<00:10,  2.86it/s]11/28/2021 01:53:30 - INFO - __main__ -   Batch number = 172
Evaluating:  86%|████████▌ | 172/201 [01:08<00:10,  2.88it/s]11/28/2021 01:53:31 - INFO - __main__ -   Batch number = 173
Evaluating:  86%|████████▌ | 173/201 [01:08<00:09,  2.87it/s]11/28/2021 01:53:31 - INFO - __main__ -   Batch number = 174
Evaluating:  87%|████████▋ | 174/201 [01:08<00:09,  2.87it/s]11/28/2021 01:53:31 - INFO - __main__ -   Batch number = 175
Evaluating:  87%|████████▋ | 175/201 [01:09<00:08,  2.98it/s]11/28/2021 01:53:32 - INFO - __main__ -   Batch number = 176
Evaluating:  88%|████████▊ | 176/201 [01:09<00:08,  2.86it/s]11/28/2021 01:53:32 - INFO - __main__ -   Batch number = 177
Evaluating:  88%|████████▊ | 177/201 [01:09<00:08,  2.88it/s]11/28/2021 01:53:32 - INFO - __main__ -   Batch number = 178
Evaluating:  89%|████████▊ | 178/201 [01:10<00:08,  2.68it/s]11/28/2021 01:53:33 - INFO - __main__ -   Batch number = 179
Evaluating:  89%|████████▉ | 179/201 [01:10<00:09,  2.42it/s]11/28/2021 01:53:33 - INFO - __main__ -   Batch number = 180
Evaluating:  90%|████████▉ | 180/201 [01:11<00:09,  2.24it/s]11/28/2021 01:53:34 - INFO - __main__ -   Batch number = 181
Evaluating:  90%|█████████ | 181/201 [01:11<00:09,  2.15it/s]11/28/2021 01:53:34 - INFO - __main__ -   Batch number = 182
Evaluating:  91%|█████████ | 182/201 [01:12<00:09,  2.08it/s]11/28/2021 01:53:35 - INFO - __main__ -   Batch number = 183
Evaluating:  91%|█████████ | 183/201 [01:12<00:08,  2.04it/s]11/28/2021 01:53:35 - INFO - __main__ -   Batch number = 184
Evaluating:  92%|█████████▏| 184/201 [01:13<00:08,  2.01it/s]11/28/2021 01:53:36 - INFO - __main__ -   Batch number = 185
Evaluating:  92%|█████████▏| 185/201 [01:13<00:08,  1.99it/s]11/28/2021 01:53:36 - INFO - __main__ -   Batch number = 186
Evaluating:  93%|█████████▎| 186/201 [01:14<00:07,  1.97it/s]11/28/2021 01:53:37 - INFO - __main__ -   Batch number = 187
Evaluating:  93%|█████████▎| 187/201 [01:14<00:07,  1.97it/s]11/28/2021 01:53:37 - INFO - __main__ -   Batch number = 188
Evaluating:  94%|█████████▎| 188/201 [01:15<00:06,  1.95it/s]11/28/2021 01:53:38 - INFO - __main__ -   Batch number = 189
Evaluating:  94%|█████████▍| 189/201 [01:15<00:06,  1.95it/s]11/28/2021 01:53:38 - INFO - __main__ -   Batch number = 190
Evaluating:  95%|█████████▍| 190/201 [01:16<00:05,  1.94it/s]11/28/2021 01:53:39 - INFO - __main__ -   Batch number = 191
Evaluating:  95%|█████████▌| 191/201 [01:16<00:05,  1.95it/s]11/28/2021 01:53:39 - INFO - __main__ -   Batch number = 192
Evaluating:  96%|█████████▌| 192/201 [01:17<00:04,  1.94it/s]11/28/2021 01:53:40 - INFO - __main__ -   Batch number = 193
Evaluating:  96%|█████████▌| 193/201 [01:17<00:04,  1.94it/s]11/28/2021 01:53:40 - INFO - __main__ -   Batch number = 194
Evaluating:  97%|█████████▋| 194/201 [01:18<00:03,  1.94it/s]11/28/2021 01:53:41 - INFO - __main__ -   Batch number = 195
Evaluating:  97%|█████████▋| 195/201 [01:18<00:03,  1.94it/s]11/28/2021 01:53:41 - INFO - __main__ -   Batch number = 196
Evaluating:  98%|█████████▊| 196/201 [01:19<00:02,  1.97it/s]11/28/2021 01:53:42 - INFO - __main__ -   Batch number = 197
Evaluating:  98%|█████████▊| 197/201 [01:19<00:02,  1.96it/s]11/28/2021 01:53:42 - INFO - __main__ -   Batch number = 198
Evaluating:  99%|█████████▊| 198/201 [01:20<00:01,  1.94it/s]11/28/2021 01:53:43 - INFO - __main__ -   Batch number = 199
Evaluating:  99%|█████████▉| 199/201 [01:21<00:01,  1.95it/s]11/28/2021 01:53:44 - INFO - __main__ -   Batch number = 200
Evaluating: 100%|█████████▉| 200/201 [01:21<00:00,  1.92it/s]11/28/2021 01:53:44 - INFO - __main__ -   Batch number = 201
Evaluating: 100%|██████████| 201/201 [01:21<00:00,  2.53it/s]Evaluating: 100%|██████████| 201/201 [01:21<00:00,  2.46it/s]
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:53:48 - INFO - __main__ -   ***** Evaluation result  in is *****
11/28/2021 01:53:48 - INFO - __main__ -     f1 = 0.7525356927675751
11/28/2021 01:53:48 - INFO - __main__ -     loss = 0.8266584167433022
11/28/2021 01:53:48 - INFO - __main__ -     precision = 0.757424482028465
11/28/2021 01:53:48 - INFO - __main__ -     recall = 0.7477096080701174
81.34user 27.33system 1:48.55elapsed 100%CPU (0avgtext+0avgdata 3994488maxresident)k
0inputs+1752outputs (0major+2006669minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:53:51 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='is', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:53:51 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:53:51 - INFO - __main__ -   Seed = 2
11/28/2021 01:53:51 - INFO - root -   save model
11/28/2021 01:53:51 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='is', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:53:51 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:53:53 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:54:00 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:54:00 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:54:00 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
11/28/2021 01:54:00 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:54:00 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:54:00 - INFO - root -   loading lang adpater cs/wiki@ukp
11/28/2021 01:54:00 - INFO - __main__ -   Adapter Languages : ['cs'], Length : 1
11/28/2021 01:54:00 - INFO - __main__ -   Adapter Names ['cs/wiki@ukp'], Length : 1
11/28/2021 01:54:00 - INFO - __main__ -   Language = cs
11/28/2021 01:54:00 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/cs/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_cs_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/adapter_config.json
Adding adapter 'cs' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/head_config.json
11/28/2021 01:54:10 - INFO - __main__ -   Language adapter for is not found, using cs instead
11/28/2021 01:54:10 - INFO - __main__ -   Set active language adapter to cs
11/28/2021 01:54:10 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:54:10 - INFO - __main__ -   Adapter Languages = ['cs']
11/28/2021 01:54:10 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_is_bert-base-multilingual-cased_128
11/28/2021 01:54:12 - INFO - __main__ -   ***** Running evaluation  in is *****
11/28/2021 01:54:12 - INFO - __main__ -     Num examples = 6401
11/28/2021 01:54:12 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/201 [00:00<?, ?it/s]11/28/2021 01:54:12 - INFO - __main__ -   Batch number = 1
Evaluating:   0%|          | 1/201 [00:00<01:28,  2.25it/s]11/28/2021 01:54:12 - INFO - __main__ -   Batch number = 2
Evaluating:   1%|          | 2/201 [00:00<01:28,  2.25it/s]11/28/2021 01:54:12 - INFO - __main__ -   Batch number = 3
Evaluating:   1%|▏         | 3/201 [00:01<01:29,  2.21it/s]11/28/2021 01:54:13 - INFO - __main__ -   Batch number = 4
Evaluating:   2%|▏         | 4/201 [00:01<01:30,  2.17it/s]11/28/2021 01:54:13 - INFO - __main__ -   Batch number = 5
Evaluating:   2%|▏         | 5/201 [00:02<01:32,  2.11it/s]11/28/2021 01:54:14 - INFO - __main__ -   Batch number = 6
Evaluating:   3%|▎         | 6/201 [00:02<01:33,  2.08it/s]11/28/2021 01:54:14 - INFO - __main__ -   Batch number = 7
Evaluating:   3%|▎         | 7/201 [00:03<01:34,  2.06it/s]11/28/2021 01:54:15 - INFO - __main__ -   Batch number = 8
Evaluating:   4%|▍         | 8/201 [00:03<01:34,  2.05it/s]11/28/2021 01:54:15 - INFO - __main__ -   Batch number = 9
Evaluating:   4%|▍         | 9/201 [00:04<01:34,  2.04it/s]11/28/2021 01:54:16 - INFO - __main__ -   Batch number = 10
Evaluating:   5%|▍         | 10/201 [00:04<01:34,  2.02it/s]11/28/2021 01:54:16 - INFO - __main__ -   Batch number = 11
Evaluating:   5%|▌         | 11/201 [00:05<01:33,  2.04it/s]11/28/2021 01:54:17 - INFO - __main__ -   Batch number = 12
Evaluating:   6%|▌         | 12/201 [00:05<01:32,  2.04it/s]11/28/2021 01:54:17 - INFO - __main__ -   Batch number = 13
Evaluating:   6%|▋         | 13/201 [00:06<01:32,  2.04it/s]11/28/2021 01:54:18 - INFO - __main__ -   Batch number = 14
Evaluating:   7%|▋         | 14/201 [00:06<01:31,  2.04it/s]11/28/2021 01:54:18 - INFO - __main__ -   Batch number = 15
Evaluating:   7%|▋         | 15/201 [00:07<01:31,  2.04it/s]11/28/2021 01:54:19 - INFO - __main__ -   Batch number = 16
Evaluating:   8%|▊         | 16/201 [00:07<01:30,  2.04it/s]11/28/2021 01:54:19 - INFO - __main__ -   Batch number = 17
Evaluating:   8%|▊         | 17/201 [00:08<01:30,  2.03it/s]11/28/2021 01:54:20 - INFO - __main__ -   Batch number = 18
Evaluating:   9%|▉         | 18/201 [00:08<01:30,  2.03it/s]11/28/2021 01:54:20 - INFO - __main__ -   Batch number = 19
Evaluating:   9%|▉         | 19/201 [00:09<01:29,  2.04it/s]11/28/2021 01:54:21 - INFO - __main__ -   Batch number = 20
Evaluating:  10%|▉         | 20/201 [00:09<01:26,  2.09it/s]11/28/2021 01:54:21 - INFO - __main__ -   Batch number = 21
Evaluating:  10%|█         | 21/201 [00:10<01:23,  2.15it/s]11/28/2021 01:54:22 - INFO - __main__ -   Batch number = 22
Evaluating:  11%|█         | 22/201 [00:10<01:34,  1.90it/s]11/28/2021 01:54:22 - INFO - __main__ -   Batch number = 23
Evaluating:  11%|█▏        | 23/201 [00:11<01:31,  1.94it/s]11/28/2021 01:54:23 - INFO - __main__ -   Batch number = 24
Evaluating:  12%|█▏        | 24/201 [00:11<01:22,  2.15it/s]11/28/2021 01:54:23 - INFO - __main__ -   Batch number = 25
Evaluating:  12%|█▏        | 25/201 [00:11<01:17,  2.26it/s]11/28/2021 01:54:24 - INFO - __main__ -   Batch number = 26
Evaluating:  13%|█▎        | 26/201 [00:12<01:19,  2.19it/s]11/28/2021 01:54:24 - INFO - __main__ -   Batch number = 27
Evaluating:  13%|█▎        | 27/201 [00:12<01:21,  2.13it/s]11/28/2021 01:54:24 - INFO - __main__ -   Batch number = 28
Evaluating:  14%|█▍        | 28/201 [00:13<01:22,  2.09it/s]11/28/2021 01:54:25 - INFO - __main__ -   Batch number = 29
Evaluating:  14%|█▍        | 29/201 [00:13<01:23,  2.06it/s]11/28/2021 01:54:25 - INFO - __main__ -   Batch number = 30
Evaluating:  15%|█▍        | 30/201 [00:14<01:16,  2.24it/s]11/28/2021 01:54:26 - INFO - __main__ -   Batch number = 31
Evaluating:  15%|█▌        | 31/201 [00:14<01:18,  2.17it/s]11/28/2021 01:54:26 - INFO - __main__ -   Batch number = 32
Evaluating:  16%|█▌        | 32/201 [00:15<01:17,  2.17it/s]11/28/2021 01:54:27 - INFO - __main__ -   Batch number = 33
Evaluating:  16%|█▋        | 33/201 [00:15<01:17,  2.17it/s]11/28/2021 01:54:27 - INFO - __main__ -   Batch number = 34
Evaluating:  17%|█▋        | 34/201 [00:16<01:18,  2.12it/s]11/28/2021 01:54:28 - INFO - __main__ -   Batch number = 35
Evaluating:  17%|█▋        | 35/201 [00:16<01:18,  2.11it/s]11/28/2021 01:54:28 - INFO - __main__ -   Batch number = 36
Evaluating:  18%|█▊        | 36/201 [00:17<01:17,  2.12it/s]11/28/2021 01:54:29 - INFO - __main__ -   Batch number = 37
Evaluating:  18%|█▊        | 37/201 [00:17<01:18,  2.10it/s]11/28/2021 01:54:29 - INFO - __main__ -   Batch number = 38
Evaluating:  19%|█▉        | 38/201 [00:18<01:17,  2.12it/s]11/28/2021 01:54:30 - INFO - __main__ -   Batch number = 39
Evaluating:  19%|█▉        | 39/201 [00:18<01:09,  2.32it/s]11/28/2021 01:54:30 - INFO - __main__ -   Batch number = 40
Evaluating:  20%|█▉        | 40/201 [00:18<01:05,  2.48it/s]11/28/2021 01:54:30 - INFO - __main__ -   Batch number = 41
Evaluating:  20%|██        | 41/201 [00:19<01:01,  2.61it/s]11/28/2021 01:54:31 - INFO - __main__ -   Batch number = 42
Evaluating:  21%|██        | 42/201 [00:19<00:58,  2.71it/s]11/28/2021 01:54:31 - INFO - __main__ -   Batch number = 43
Evaluating:  21%|██▏       | 43/201 [00:19<01:01,  2.57it/s]11/28/2021 01:54:31 - INFO - __main__ -   Batch number = 44
Evaluating:  22%|██▏       | 44/201 [00:20<00:57,  2.72it/s]11/28/2021 01:54:32 - INFO - __main__ -   Batch number = 45
Evaluating:  22%|██▏       | 45/201 [00:20<00:55,  2.80it/s]11/28/2021 01:54:32 - INFO - __main__ -   Batch number = 46
Evaluating:  23%|██▎       | 46/201 [00:21<00:59,  2.60it/s]11/28/2021 01:54:33 - INFO - __main__ -   Batch number = 47
Evaluating:  23%|██▎       | 47/201 [00:21<01:02,  2.47it/s]11/28/2021 01:54:33 - INFO - __main__ -   Batch number = 48
Evaluating:  24%|██▍       | 48/201 [00:21<01:04,  2.39it/s]11/28/2021 01:54:33 - INFO - __main__ -   Batch number = 49
Evaluating:  24%|██▍       | 49/201 [00:22<01:07,  2.26it/s]11/28/2021 01:54:34 - INFO - __main__ -   Batch number = 50
Evaluating:  25%|██▍       | 50/201 [00:22<01:09,  2.16it/s]11/28/2021 01:54:34 - INFO - __main__ -   Batch number = 51
Evaluating:  25%|██▌       | 51/201 [00:23<01:03,  2.36it/s]11/28/2021 01:54:35 - INFO - __main__ -   Batch number = 52
Evaluating:  26%|██▌       | 52/201 [00:23<00:59,  2.52it/s]11/28/2021 01:54:35 - INFO - __main__ -   Batch number = 53
Evaluating:  26%|██▋       | 53/201 [00:24<01:01,  2.41it/s]11/28/2021 01:54:36 - INFO - __main__ -   Batch number = 54
Evaluating:  27%|██▋       | 54/201 [00:24<01:04,  2.28it/s]11/28/2021 01:54:36 - INFO - __main__ -   Batch number = 55
Evaluating:  27%|██▋       | 55/201 [00:25<01:05,  2.21it/s]11/28/2021 01:54:37 - INFO - __main__ -   Batch number = 56
Evaluating:  28%|██▊       | 56/201 [00:25<01:11,  2.03it/s]11/28/2021 01:54:37 - INFO - __main__ -   Batch number = 57
Evaluating:  28%|██▊       | 57/201 [00:26<01:08,  2.10it/s]11/28/2021 01:54:38 - INFO - __main__ -   Batch number = 58
Evaluating:  29%|██▉       | 58/201 [00:26<01:07,  2.13it/s]11/28/2021 01:54:38 - INFO - __main__ -   Batch number = 59
Evaluating:  29%|██▉       | 59/201 [00:26<01:02,  2.26it/s]11/28/2021 01:54:38 - INFO - __main__ -   Batch number = 60
Evaluating:  30%|██▉       | 60/201 [00:27<01:04,  2.20it/s]11/28/2021 01:54:39 - INFO - __main__ -   Batch number = 61
Evaluating:  30%|███       | 61/201 [00:27<01:04,  2.16it/s]11/28/2021 01:54:39 - INFO - __main__ -   Batch number = 62
Evaluating:  31%|███       | 62/201 [00:28<01:05,  2.14it/s]11/28/2021 01:54:40 - INFO - __main__ -   Batch number = 63
Evaluating:  31%|███▏      | 63/201 [00:28<01:02,  2.22it/s]11/28/2021 01:54:40 - INFO - __main__ -   Batch number = 64
Evaluating:  32%|███▏      | 64/201 [00:29<01:03,  2.17it/s]11/28/2021 01:54:41 - INFO - __main__ -   Batch number = 65
Evaluating:  32%|███▏      | 65/201 [00:29<01:04,  2.12it/s]11/28/2021 01:54:41 - INFO - __main__ -   Batch number = 66
Evaluating:  33%|███▎      | 66/201 [00:30<01:02,  2.16it/s]11/28/2021 01:54:42 - INFO - __main__ -   Batch number = 67
Evaluating:  33%|███▎      | 67/201 [00:30<01:01,  2.20it/s]11/28/2021 01:54:42 - INFO - __main__ -   Batch number = 68
Evaluating:  34%|███▍      | 68/201 [00:31<01:01,  2.17it/s]11/28/2021 01:54:43 - INFO - __main__ -   Batch number = 69
Evaluating:  34%|███▍      | 69/201 [00:31<01:01,  2.14it/s]11/28/2021 01:54:43 - INFO - __main__ -   Batch number = 70
Evaluating:  35%|███▍      | 70/201 [00:32<01:00,  2.16it/s]11/28/2021 01:54:44 - INFO - __main__ -   Batch number = 71
Evaluating:  35%|███▌      | 71/201 [00:32<01:01,  2.11it/s]11/28/2021 01:54:44 - INFO - __main__ -   Batch number = 72
Evaluating:  36%|███▌      | 72/201 [00:33<01:02,  2.08it/s]11/28/2021 01:54:45 - INFO - __main__ -   Batch number = 73
Evaluating:  36%|███▋      | 73/201 [00:33<01:02,  2.05it/s]11/28/2021 01:54:45 - INFO - __main__ -   Batch number = 74
Evaluating:  37%|███▋      | 74/201 [00:34<01:01,  2.06it/s]11/28/2021 01:54:46 - INFO - __main__ -   Batch number = 75
Evaluating:  37%|███▋      | 75/201 [00:34<01:01,  2.05it/s]11/28/2021 01:54:46 - INFO - __main__ -   Batch number = 76
Evaluating:  38%|███▊      | 76/201 [00:34<01:01,  2.04it/s]11/28/2021 01:54:47 - INFO - __main__ -   Batch number = 77
Evaluating:  38%|███▊      | 77/201 [00:35<01:00,  2.07it/s]11/28/2021 01:54:47 - INFO - __main__ -   Batch number = 78
Evaluating:  39%|███▉      | 78/201 [00:35<00:59,  2.05it/s]11/28/2021 01:54:47 - INFO - __main__ -   Batch number = 79
Evaluating:  39%|███▉      | 79/201 [00:36<00:59,  2.05it/s]11/28/2021 01:54:48 - INFO - __main__ -   Batch number = 80
Evaluating:  40%|███▉      | 80/201 [00:36<00:57,  2.11it/s]11/28/2021 01:54:48 - INFO - __main__ -   Batch number = 81
Evaluating:  40%|████      | 81/201 [00:37<00:58,  2.07it/s]11/28/2021 01:54:49 - INFO - __main__ -   Batch number = 82
Evaluating:  41%|████      | 82/201 [00:37<00:53,  2.24it/s]11/28/2021 01:54:49 - INFO - __main__ -   Batch number = 83
Evaluating:  41%|████▏     | 83/201 [00:37<00:42,  2.75it/s]11/28/2021 01:54:49 - INFO - __main__ -   Batch number = 84
Evaluating:  42%|████▏     | 84/201 [00:38<00:36,  3.24it/s]11/28/2021 01:54:50 - INFO - __main__ -   Batch number = 85
Evaluating:  42%|████▏     | 85/201 [00:38<00:41,  2.79it/s]11/28/2021 01:54:50 - INFO - __main__ -   Batch number = 86
Evaluating:  43%|████▎     | 86/201 [00:39<00:46,  2.49it/s]11/28/2021 01:54:51 - INFO - __main__ -   Batch number = 87
Evaluating:  43%|████▎     | 87/201 [00:39<00:49,  2.31it/s]11/28/2021 01:54:51 - INFO - __main__ -   Batch number = 88
Evaluating:  44%|████▍     | 88/201 [00:40<00:51,  2.19it/s]11/28/2021 01:54:52 - INFO - __main__ -   Batch number = 89
Evaluating:  44%|████▍     | 89/201 [00:40<00:54,  2.06it/s]11/28/2021 01:54:52 - INFO - __main__ -   Batch number = 90
Evaluating:  45%|████▍     | 90/201 [00:41<00:54,  2.05it/s]11/28/2021 01:54:53 - INFO - __main__ -   Batch number = 91
Evaluating:  45%|████▌     | 91/201 [00:41<00:54,  2.04it/s]11/28/2021 01:54:53 - INFO - __main__ -   Batch number = 92
Evaluating:  46%|████▌     | 92/201 [00:42<00:53,  2.03it/s]11/28/2021 01:54:54 - INFO - __main__ -   Batch number = 93
Evaluating:  46%|████▋     | 93/201 [00:42<00:53,  2.03it/s]11/28/2021 01:54:54 - INFO - __main__ -   Batch number = 94
Evaluating:  47%|████▋     | 94/201 [00:43<00:52,  2.03it/s]11/28/2021 01:54:55 - INFO - __main__ -   Batch number = 95
Evaluating:  47%|████▋     | 95/201 [00:43<00:52,  2.03it/s]11/28/2021 01:54:55 - INFO - __main__ -   Batch number = 96
Evaluating:  48%|████▊     | 96/201 [00:44<00:51,  2.02it/s]11/28/2021 01:54:56 - INFO - __main__ -   Batch number = 97
Evaluating:  48%|████▊     | 97/201 [00:44<00:51,  2.02it/s]11/28/2021 01:54:56 - INFO - __main__ -   Batch number = 98
Evaluating:  49%|████▉     | 98/201 [00:45<00:50,  2.02it/s]11/28/2021 01:54:57 - INFO - __main__ -   Batch number = 99
Evaluating:  49%|████▉     | 99/201 [00:45<00:53,  1.89it/s]11/28/2021 01:54:57 - INFO - __main__ -   Batch number = 100
Evaluating:  50%|████▉     | 100/201 [00:46<00:52,  1.92it/s]11/28/2021 01:54:58 - INFO - __main__ -   Batch number = 101
Evaluating:  50%|█████     | 101/201 [00:46<00:51,  1.95it/s]11/28/2021 01:54:58 - INFO - __main__ -   Batch number = 102
Evaluating:  51%|█████     | 102/201 [00:47<00:49,  1.99it/s]11/28/2021 01:54:59 - INFO - __main__ -   Batch number = 103
Evaluating:  51%|█████     | 103/201 [00:47<00:49,  1.99it/s]11/28/2021 01:54:59 - INFO - __main__ -   Batch number = 104
Evaluating:  52%|█████▏    | 104/201 [00:48<00:48,  2.00it/s]11/28/2021 01:55:00 - INFO - __main__ -   Batch number = 105
Evaluating:  52%|█████▏    | 105/201 [00:48<00:47,  2.01it/s]11/28/2021 01:55:00 - INFO - __main__ -   Batch number = 106
Evaluating:  53%|█████▎    | 106/201 [00:49<00:47,  2.02it/s]11/28/2021 01:55:01 - INFO - __main__ -   Batch number = 107
Evaluating:  53%|█████▎    | 107/201 [00:49<00:46,  2.02it/s]11/28/2021 01:55:01 - INFO - __main__ -   Batch number = 108
Evaluating:  54%|█████▎    | 108/201 [00:50<00:45,  2.02it/s]11/28/2021 01:55:02 - INFO - __main__ -   Batch number = 109
Evaluating:  54%|█████▍    | 109/201 [00:50<00:45,  2.02it/s]11/28/2021 01:55:02 - INFO - __main__ -   Batch number = 110
Evaluating:  55%|█████▍    | 110/201 [00:51<00:42,  2.12it/s]11/28/2021 01:55:03 - INFO - __main__ -   Batch number = 111
Evaluating:  55%|█████▌    | 111/201 [00:51<00:38,  2.33it/s]11/28/2021 01:55:03 - INFO - __main__ -   Batch number = 112
Evaluating:  56%|█████▌    | 112/201 [00:51<00:35,  2.49it/s]11/28/2021 01:55:03 - INFO - __main__ -   Batch number = 113
Evaluating:  56%|█████▌    | 113/201 [00:52<00:33,  2.60it/s]11/28/2021 01:55:04 - INFO - __main__ -   Batch number = 114
Evaluating:  57%|█████▋    | 114/201 [00:52<00:32,  2.70it/s]11/28/2021 01:55:04 - INFO - __main__ -   Batch number = 115
Evaluating:  57%|█████▋    | 115/201 [00:52<00:31,  2.77it/s]11/28/2021 01:55:04 - INFO - __main__ -   Batch number = 116
Evaluating:  58%|█████▊    | 116/201 [00:53<00:30,  2.81it/s]11/28/2021 01:55:05 - INFO - __main__ -   Batch number = 117
Evaluating:  58%|█████▊    | 117/201 [00:53<00:29,  2.84it/s]11/28/2021 01:55:05 - INFO - __main__ -   Batch number = 118
Evaluating:  59%|█████▊    | 118/201 [00:53<00:28,  2.90it/s]11/28/2021 01:55:05 - INFO - __main__ -   Batch number = 119
Evaluating:  59%|█████▉    | 119/201 [00:54<00:27,  2.95it/s]11/28/2021 01:55:06 - INFO - __main__ -   Batch number = 120
Evaluating:  60%|█████▉    | 120/201 [00:54<00:27,  2.95it/s]11/28/2021 01:55:06 - INFO - __main__ -   Batch number = 121
Evaluating:  60%|██████    | 121/201 [00:54<00:27,  2.92it/s]11/28/2021 01:55:06 - INFO - __main__ -   Batch number = 122
Evaluating:  61%|██████    | 122/201 [00:55<00:27,  2.92it/s]11/28/2021 01:55:07 - INFO - __main__ -   Batch number = 123
Evaluating:  61%|██████    | 123/201 [00:55<00:27,  2.87it/s]11/28/2021 01:55:07 - INFO - __main__ -   Batch number = 124
Evaluating:  62%|██████▏   | 124/201 [00:55<00:26,  2.89it/s]11/28/2021 01:55:07 - INFO - __main__ -   Batch number = 125
Evaluating:  62%|██████▏   | 125/201 [00:56<00:26,  2.90it/s]11/28/2021 01:55:08 - INFO - __main__ -   Batch number = 126
Evaluating:  63%|██████▎   | 126/201 [00:56<00:25,  2.90it/s]11/28/2021 01:55:08 - INFO - __main__ -   Batch number = 127
Evaluating:  63%|██████▎   | 127/201 [00:56<00:25,  2.90it/s]11/28/2021 01:55:08 - INFO - __main__ -   Batch number = 128
Evaluating:  64%|██████▎   | 128/201 [00:57<00:25,  2.91it/s]11/28/2021 01:55:09 - INFO - __main__ -   Batch number = 129
Evaluating:  64%|██████▍   | 129/201 [00:57<00:24,  2.91it/s]11/28/2021 01:55:09 - INFO - __main__ -   Batch number = 130
Evaluating:  65%|██████▍   | 130/201 [00:57<00:24,  2.87it/s]11/28/2021 01:55:09 - INFO - __main__ -   Batch number = 131
Evaluating:  65%|██████▌   | 131/201 [00:58<00:24,  2.88it/s]11/28/2021 01:55:10 - INFO - __main__ -   Batch number = 132
Evaluating:  66%|██████▌   | 132/201 [00:58<00:23,  2.90it/s]11/28/2021 01:55:10 - INFO - __main__ -   Batch number = 133
Evaluating:  66%|██████▌   | 133/201 [00:58<00:23,  2.89it/s]11/28/2021 01:55:10 - INFO - __main__ -   Batch number = 134
Evaluating:  67%|██████▋   | 134/201 [00:59<00:23,  2.88it/s]11/28/2021 01:55:11 - INFO - __main__ -   Batch number = 135
Evaluating:  67%|██████▋   | 135/201 [00:59<00:22,  2.88it/s]11/28/2021 01:55:11 - INFO - __main__ -   Batch number = 136
Evaluating:  68%|██████▊   | 136/201 [00:59<00:22,  2.89it/s]11/28/2021 01:55:11 - INFO - __main__ -   Batch number = 137
Evaluating:  68%|██████▊   | 137/201 [01:00<00:22,  2.89it/s]11/28/2021 01:55:12 - INFO - __main__ -   Batch number = 138
Evaluating:  69%|██████▊   | 138/201 [01:00<00:21,  2.89it/s]11/28/2021 01:55:12 - INFO - __main__ -   Batch number = 139
Evaluating:  69%|██████▉   | 139/201 [01:01<00:21,  2.91it/s]11/28/2021 01:55:13 - INFO - __main__ -   Batch number = 140
Evaluating:  70%|██████▉   | 140/201 [01:01<00:20,  2.91it/s]11/28/2021 01:55:13 - INFO - __main__ -   Batch number = 141
Evaluating:  70%|███████   | 141/201 [01:01<00:20,  2.90it/s]11/28/2021 01:55:13 - INFO - __main__ -   Batch number = 142
Evaluating:  71%|███████   | 142/201 [01:02<00:21,  2.73it/s]11/28/2021 01:55:14 - INFO - __main__ -   Batch number = 143
Evaluating:  71%|███████   | 143/201 [01:02<00:23,  2.44it/s]11/28/2021 01:55:14 - INFO - __main__ -   Batch number = 144
Evaluating:  72%|███████▏  | 144/201 [01:03<00:25,  2.27it/s]11/28/2021 01:55:15 - INFO - __main__ -   Batch number = 145
Evaluating:  72%|███████▏  | 145/201 [01:03<00:25,  2.16it/s]11/28/2021 01:55:15 - INFO - __main__ -   Batch number = 146
Evaluating:  73%|███████▎  | 146/201 [01:04<00:26,  2.10it/s]11/28/2021 01:55:16 - INFO - __main__ -   Batch number = 147
Evaluating:  73%|███████▎  | 147/201 [01:04<00:26,  2.05it/s]11/28/2021 01:55:16 - INFO - __main__ -   Batch number = 148
Evaluating:  74%|███████▎  | 148/201 [01:05<00:26,  2.02it/s]11/28/2021 01:55:17 - INFO - __main__ -   Batch number = 149
Evaluating:  74%|███████▍  | 149/201 [01:05<00:25,  2.00it/s]11/28/2021 01:55:17 - INFO - __main__ -   Batch number = 150
Evaluating:  75%|███████▍  | 150/201 [01:06<00:25,  1.98it/s]11/28/2021 01:55:18 - INFO - __main__ -   Batch number = 151
Evaluating:  75%|███████▌  | 151/201 [01:06<00:25,  1.97it/s]11/28/2021 01:55:18 - INFO - __main__ -   Batch number = 152
Evaluating:  76%|███████▌  | 152/201 [01:07<00:24,  1.96it/s]11/28/2021 01:55:19 - INFO - __main__ -   Batch number = 153
Evaluating:  76%|███████▌  | 153/201 [01:07<00:24,  1.96it/s]11/28/2021 01:55:19 - INFO - __main__ -   Batch number = 154
Evaluating:  77%|███████▋  | 154/201 [01:08<00:24,  1.96it/s]11/28/2021 01:55:20 - INFO - __main__ -   Batch number = 155
Evaluating:  77%|███████▋  | 155/201 [01:08<00:23,  1.96it/s]11/28/2021 01:55:20 - INFO - __main__ -   Batch number = 156
Evaluating:  78%|███████▊  | 156/201 [01:09<00:22,  1.96it/s]11/28/2021 01:55:21 - INFO - __main__ -   Batch number = 157
Evaluating:  78%|███████▊  | 157/201 [01:09<00:22,  1.96it/s]11/28/2021 01:55:21 - INFO - __main__ -   Batch number = 158
Evaluating:  79%|███████▊  | 158/201 [01:10<00:22,  1.95it/s]11/28/2021 01:55:22 - INFO - __main__ -   Batch number = 159
Evaluating:  79%|███████▉  | 159/201 [01:10<00:21,  1.95it/s]11/28/2021 01:55:22 - INFO - __main__ -   Batch number = 160
Evaluating:  80%|███████▉  | 160/201 [01:11<00:21,  1.92it/s]11/28/2021 01:55:23 - INFO - __main__ -   Batch number = 161
Evaluating:  80%|████████  | 161/201 [01:11<00:20,  1.93it/s]11/28/2021 01:55:23 - INFO - __main__ -   Batch number = 162
Evaluating:  81%|████████  | 162/201 [01:12<00:20,  1.93it/s]11/28/2021 01:55:24 - INFO - __main__ -   Batch number = 163
Evaluating:  81%|████████  | 163/201 [01:12<00:19,  1.91it/s]11/28/2021 01:55:24 - INFO - __main__ -   Batch number = 164
Evaluating:  82%|████████▏ | 164/201 [01:13<00:18,  1.98it/s]11/28/2021 01:55:25 - INFO - __main__ -   Batch number = 165
Evaluating:  82%|████████▏ | 165/201 [01:13<00:16,  2.19it/s]11/28/2021 01:55:25 - INFO - __main__ -   Batch number = 166
Evaluating:  83%|████████▎ | 166/201 [01:14<00:14,  2.37it/s]11/28/2021 01:55:26 - INFO - __main__ -   Batch number = 167
Evaluating:  83%|████████▎ | 167/201 [01:14<00:13,  2.49it/s]11/28/2021 01:55:26 - INFO - __main__ -   Batch number = 168
Evaluating:  84%|████████▎ | 168/201 [01:14<00:12,  2.59it/s]11/28/2021 01:55:26 - INFO - __main__ -   Batch number = 169
Evaluating:  84%|████████▍ | 169/201 [01:15<00:12,  2.66it/s]11/28/2021 01:55:27 - INFO - __main__ -   Batch number = 170
Evaluating:  85%|████████▍ | 170/201 [01:15<00:11,  2.70it/s]11/28/2021 01:55:27 - INFO - __main__ -   Batch number = 171
Evaluating:  85%|████████▌ | 171/201 [01:15<00:10,  2.74it/s]11/28/2021 01:55:27 - INFO - __main__ -   Batch number = 172
Evaluating:  86%|████████▌ | 172/201 [01:16<00:10,  2.79it/s]11/28/2021 01:55:28 - INFO - __main__ -   Batch number = 173
Evaluating:  86%|████████▌ | 173/201 [01:16<00:10,  2.57it/s]11/28/2021 01:55:28 - INFO - __main__ -   Batch number = 174
Evaluating:  87%|████████▋ | 174/201 [01:17<00:11,  2.34it/s]11/28/2021 01:55:29 - INFO - __main__ -   Batch number = 175
Evaluating:  87%|████████▋ | 175/201 [01:17<00:11,  2.20it/s]11/28/2021 01:55:29 - INFO - __main__ -   Batch number = 176
Evaluating:  88%|████████▊ | 176/201 [01:18<00:11,  2.12it/s]11/28/2021 01:55:30 - INFO - __main__ -   Batch number = 177
Evaluating:  88%|████████▊ | 177/201 [01:18<00:11,  2.06it/s]11/28/2021 01:55:30 - INFO - __main__ -   Batch number = 178
Evaluating:  89%|████████▊ | 178/201 [01:19<00:11,  2.03it/s]11/28/2021 01:55:31 - INFO - __main__ -   Batch number = 179
Evaluating:  89%|████████▉ | 179/201 [01:19<00:11,  1.99it/s]11/28/2021 01:55:31 - INFO - __main__ -   Batch number = 180
Evaluating:  90%|████████▉ | 180/201 [01:20<00:10,  1.99it/s]11/28/2021 01:55:32 - INFO - __main__ -   Batch number = 181
Evaluating:  90%|█████████ | 181/201 [01:20<00:10,  1.96it/s]11/28/2021 01:55:32 - INFO - __main__ -   Batch number = 182
Evaluating:  91%|█████████ | 182/201 [01:21<00:09,  1.96it/s]11/28/2021 01:55:33 - INFO - __main__ -   Batch number = 183
Evaluating:  91%|█████████ | 183/201 [01:21<00:09,  1.95it/s]11/28/2021 01:55:33 - INFO - __main__ -   Batch number = 184
Evaluating:  92%|█████████▏| 184/201 [01:22<00:08,  1.94it/s]11/28/2021 01:55:34 - INFO - __main__ -   Batch number = 185
Evaluating:  92%|█████████▏| 185/201 [01:22<00:08,  1.94it/s]11/28/2021 01:55:34 - INFO - __main__ -   Batch number = 186
Evaluating:  93%|█████████▎| 186/201 [01:23<00:07,  1.95it/s]11/28/2021 01:55:35 - INFO - __main__ -   Batch number = 187
Evaluating:  93%|█████████▎| 187/201 [01:23<00:07,  1.94it/s]11/28/2021 01:55:35 - INFO - __main__ -   Batch number = 188
Evaluating:  94%|█████████▎| 188/201 [01:24<00:06,  1.94it/s]11/28/2021 01:55:36 - INFO - __main__ -   Batch number = 189
Evaluating:  94%|█████████▍| 189/201 [01:24<00:06,  1.93it/s]11/28/2021 01:55:36 - INFO - __main__ -   Batch number = 190
Evaluating:  95%|█████████▍| 190/201 [01:25<00:05,  1.95it/s]11/28/2021 01:55:37 - INFO - __main__ -   Batch number = 191
Evaluating:  95%|█████████▌| 191/201 [01:25<00:05,  1.94it/s]11/28/2021 01:55:37 - INFO - __main__ -   Batch number = 192
Evaluating:  96%|█████████▌| 192/201 [01:26<00:04,  1.95it/s]11/28/2021 01:55:38 - INFO - __main__ -   Batch number = 193
Evaluating:  96%|█████████▌| 193/201 [01:26<00:04,  1.92it/s]11/28/2021 01:55:38 - INFO - __main__ -   Batch number = 194
Evaluating:  97%|█████████▋| 194/201 [01:27<00:03,  1.98it/s]11/28/2021 01:55:39 - INFO - __main__ -   Batch number = 195
Evaluating:  97%|█████████▋| 195/201 [01:27<00:02,  2.13it/s]11/28/2021 01:55:39 - INFO - __main__ -   Batch number = 196
Evaluating:  98%|█████████▊| 196/201 [01:28<00:02,  2.30it/s]11/28/2021 01:55:40 - INFO - __main__ -   Batch number = 197
Evaluating:  98%|█████████▊| 197/201 [01:28<00:01,  2.44it/s]11/28/2021 01:55:40 - INFO - __main__ -   Batch number = 198
Evaluating:  99%|█████████▊| 198/201 [01:28<00:01,  2.57it/s]11/28/2021 01:55:40 - INFO - __main__ -   Batch number = 199
Evaluating:  99%|█████████▉| 199/201 [01:29<00:00,  2.62it/s]11/28/2021 01:55:41 - INFO - __main__ -   Batch number = 200
Evaluating: 100%|█████████▉| 200/201 [01:29<00:00,  2.69it/s]11/28/2021 01:55:41 - INFO - __main__ -   Batch number = 201
Evaluating: 100%|██████████| 201/201 [01:29<00:00,  2.24it/s]
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:55:45 - INFO - __main__ -   ***** Evaluation result  in is *****
11/28/2021 01:55:45 - INFO - __main__ -     f1 = 0.7345261718554587
11/28/2021 01:55:45 - INFO - __main__ -     loss = 0.9226992127314135
11/28/2021 01:55:45 - INFO - __main__ -     precision = 0.7407143337593327
11/28/2021 01:55:45 - INFO - __main__ -     recall = 0.7284405490325782
88.18user 30.12system 1:57.10elapsed 101%CPU (0avgtext+0avgdata 3985588maxresident)k
0inputs+1728outputs (0major+1824830minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:55:48 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='is', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:55:48 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:55:48 - INFO - __main__ -   Seed = 3
11/28/2021 01:55:48 - INFO - root -   save model
11/28/2021 01:55:48 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='is', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:55:48 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:55:50 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:55:57 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:55:57 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:55:57 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
11/28/2021 01:55:57 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:55:57 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:55:57 - INFO - root -   loading lang adpater cs/wiki@ukp
11/28/2021 01:55:57 - INFO - __main__ -   Adapter Languages : ['cs'], Length : 1
11/28/2021 01:55:57 - INFO - __main__ -   Adapter Names ['cs/wiki@ukp'], Length : 1
11/28/2021 01:55:57 - INFO - __main__ -   Language = cs
11/28/2021 01:55:57 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/cs/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_cs_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/adapter_config.json
Adding adapter 'cs' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/head_config.json
11/28/2021 01:56:10 - INFO - __main__ -   Language adapter for is not found, using cs instead
11/28/2021 01:56:10 - INFO - __main__ -   Set active language adapter to cs
11/28/2021 01:56:10 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:56:10 - INFO - __main__ -   Adapter Languages = ['cs']
11/28/2021 01:56:10 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_is_bert-base-multilingual-cased_128
11/28/2021 01:56:11 - INFO - __main__ -   ***** Running evaluation  in is *****
11/28/2021 01:56:11 - INFO - __main__ -     Num examples = 6401
11/28/2021 01:56:11 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/201 [00:00<?, ?it/s]11/28/2021 01:56:11 - INFO - __main__ -   Batch number = 1
Evaluating:   0%|          | 1/201 [00:00<01:53,  1.76it/s]11/28/2021 01:56:12 - INFO - __main__ -   Batch number = 2
Evaluating:   1%|          | 2/201 [00:01<02:01,  1.64it/s]11/28/2021 01:56:13 - INFO - __main__ -   Batch number = 3
Evaluating:   1%|▏         | 3/201 [00:01<02:05,  1.58it/s]11/28/2021 01:56:13 - INFO - __main__ -   Batch number = 4
Evaluating:   2%|▏         | 4/201 [00:02<02:06,  1.55it/s]11/28/2021 01:56:14 - INFO - __main__ -   Batch number = 5
Evaluating:   2%|▏         | 5/201 [00:03<02:07,  1.54it/s]11/28/2021 01:56:15 - INFO - __main__ -   Batch number = 6
Evaluating:   3%|▎         | 6/201 [00:03<02:07,  1.54it/s]11/28/2021 01:56:15 - INFO - __main__ -   Batch number = 7
Evaluating:   3%|▎         | 7/201 [00:04<02:06,  1.53it/s]11/28/2021 01:56:16 - INFO - __main__ -   Batch number = 8
Evaluating:   4%|▍         | 8/201 [00:05<02:06,  1.53it/s]11/28/2021 01:56:17 - INFO - __main__ -   Batch number = 9
Evaluating:   4%|▍         | 9/201 [00:05<02:06,  1.52it/s]11/28/2021 01:56:17 - INFO - __main__ -   Batch number = 10
Evaluating:   5%|▍         | 10/201 [00:06<02:05,  1.53it/s]11/28/2021 01:56:18 - INFO - __main__ -   Batch number = 11
Evaluating:   5%|▌         | 11/201 [00:07<02:04,  1.53it/s]11/28/2021 01:56:19 - INFO - __main__ -   Batch number = 12
Evaluating:   6%|▌         | 12/201 [00:07<02:04,  1.52it/s]11/28/2021 01:56:19 - INFO - __main__ -   Batch number = 13
Evaluating:   6%|▋         | 13/201 [00:08<02:03,  1.52it/s]11/28/2021 01:56:20 - INFO - __main__ -   Batch number = 14
Evaluating:   7%|▋         | 14/201 [00:09<02:03,  1.51it/s]11/28/2021 01:56:20 - INFO - __main__ -   Batch number = 15
Evaluating:   7%|▋         | 15/201 [00:09<02:02,  1.52it/s]11/28/2021 01:56:21 - INFO - __main__ -   Batch number = 16
Evaluating:   8%|▊         | 16/201 [00:10<02:00,  1.53it/s]11/28/2021 01:56:22 - INFO - __main__ -   Batch number = 17
Evaluating:   8%|▊         | 17/201 [00:11<02:04,  1.48it/s]11/28/2021 01:56:23 - INFO - __main__ -   Batch number = 18
Evaluating:   9%|▉         | 18/201 [00:11<02:03,  1.49it/s]11/28/2021 01:56:23 - INFO - __main__ -   Batch number = 19
Evaluating:   9%|▉         | 19/201 [00:12<01:59,  1.52it/s]11/28/2021 01:56:24 - INFO - __main__ -   Batch number = 20
Evaluating:  10%|▉         | 20/201 [00:12<01:50,  1.64it/s]11/28/2021 01:56:24 - INFO - __main__ -   Batch number = 21
Evaluating:  10%|█         | 21/201 [00:13<01:43,  1.74it/s]11/28/2021 01:56:25 - INFO - __main__ -   Batch number = 22
Evaluating:  11%|█         | 22/201 [00:13<01:38,  1.82it/s]11/28/2021 01:56:25 - INFO - __main__ -   Batch number = 23
Evaluating:  11%|█▏        | 23/201 [00:14<01:34,  1.88it/s]11/28/2021 01:56:26 - INFO - __main__ -   Batch number = 24
Evaluating:  12%|█▏        | 24/201 [00:14<01:32,  1.92it/s]11/28/2021 01:56:26 - INFO - __main__ -   Batch number = 25
Evaluating:  12%|█▏        | 25/201 [00:15<01:29,  1.97it/s]11/28/2021 01:56:27 - INFO - __main__ -   Batch number = 26
Evaluating:  13%|█▎        | 26/201 [00:15<01:30,  1.93it/s]11/28/2021 01:56:27 - INFO - __main__ -   Batch number = 27
Evaluating:  13%|█▎        | 27/201 [00:16<01:29,  1.95it/s]11/28/2021 01:56:28 - INFO - __main__ -   Batch number = 28
Evaluating:  14%|█▍        | 28/201 [00:16<01:27,  1.97it/s]11/28/2021 01:56:28 - INFO - __main__ -   Batch number = 29
Evaluating:  14%|█▍        | 29/201 [00:17<01:26,  2.00it/s]11/28/2021 01:56:29 - INFO - __main__ -   Batch number = 30
Evaluating:  15%|█▍        | 30/201 [00:17<01:24,  2.02it/s]11/28/2021 01:56:29 - INFO - __main__ -   Batch number = 31
Evaluating:  15%|█▌        | 31/201 [00:18<01:24,  2.02it/s]11/28/2021 01:56:30 - INFO - __main__ -   Batch number = 32
Evaluating:  16%|█▌        | 32/201 [00:18<01:23,  2.03it/s]11/28/2021 01:56:30 - INFO - __main__ -   Batch number = 33
Evaluating:  16%|█▋        | 33/201 [00:19<01:22,  2.04it/s]11/28/2021 01:56:31 - INFO - __main__ -   Batch number = 34
Evaluating:  17%|█▋        | 34/201 [00:19<01:21,  2.05it/s]11/28/2021 01:56:31 - INFO - __main__ -   Batch number = 35
Evaluating:  17%|█▋        | 35/201 [00:20<01:20,  2.05it/s]11/28/2021 01:56:32 - INFO - __main__ -   Batch number = 36
Evaluating:  18%|█▊        | 36/201 [00:20<01:23,  1.99it/s]11/28/2021 01:56:32 - INFO - __main__ -   Batch number = 37
Evaluating:  18%|█▊        | 37/201 [00:21<01:21,  2.00it/s]11/28/2021 01:56:33 - INFO - __main__ -   Batch number = 38
Evaluating:  19%|█▉        | 38/201 [00:21<01:20,  2.01it/s]11/28/2021 01:56:33 - INFO - __main__ -   Batch number = 39
Evaluating:  19%|█▉        | 39/201 [00:22<01:19,  2.03it/s]11/28/2021 01:56:34 - INFO - __main__ -   Batch number = 40
Evaluating:  20%|█▉        | 40/201 [00:22<01:13,  2.20it/s]11/28/2021 01:56:34 - INFO - __main__ -   Batch number = 41
Evaluating:  20%|██        | 41/201 [00:22<01:05,  2.43it/s]11/28/2021 01:56:34 - INFO - __main__ -   Batch number = 42
Evaluating:  21%|██        | 42/201 [00:23<01:00,  2.62it/s]11/28/2021 01:56:35 - INFO - __main__ -   Batch number = 43
Evaluating:  21%|██▏       | 43/201 [00:23<00:57,  2.76it/s]11/28/2021 01:56:35 - INFO - __main__ -   Batch number = 44
Evaluating:  22%|██▏       | 44/201 [00:23<00:55,  2.83it/s]11/28/2021 01:56:35 - INFO - __main__ -   Batch number = 45
Evaluating:  22%|██▏       | 45/201 [00:24<00:54,  2.87it/s]11/28/2021 01:56:36 - INFO - __main__ -   Batch number = 46
Evaluating:  23%|██▎       | 46/201 [00:24<00:52,  2.95it/s]11/28/2021 01:56:36 - INFO - __main__ -   Batch number = 47
Evaluating:  23%|██▎       | 47/201 [00:24<00:50,  3.03it/s]11/28/2021 01:56:36 - INFO - __main__ -   Batch number = 48
Evaluating:  24%|██▍       | 48/201 [00:25<00:49,  3.09it/s]11/28/2021 01:56:37 - INFO - __main__ -   Batch number = 49
Evaluating:  24%|██▍       | 49/201 [00:25<00:45,  3.34it/s]11/28/2021 01:56:37 - INFO - __main__ -   Batch number = 50
Evaluating:  25%|██▍       | 50/201 [00:25<00:45,  3.31it/s]11/28/2021 01:56:37 - INFO - __main__ -   Batch number = 51
Evaluating:  25%|██▌       | 51/201 [00:26<00:46,  3.23it/s]11/28/2021 01:56:37 - INFO - __main__ -   Batch number = 52
Evaluating:  26%|██▌       | 52/201 [00:26<00:46,  3.18it/s]11/28/2021 01:56:38 - INFO - __main__ -   Batch number = 53
Evaluating:  26%|██▋       | 53/201 [00:26<00:46,  3.20it/s]11/28/2021 01:56:38 - INFO - __main__ -   Batch number = 54
Evaluating:  27%|██▋       | 54/201 [00:27<00:46,  3.17it/s]11/28/2021 01:56:38 - INFO - __main__ -   Batch number = 55
Evaluating:  27%|██▋       | 55/201 [00:27<00:46,  3.16it/s]11/28/2021 01:56:39 - INFO - __main__ -   Batch number = 56
Evaluating:  28%|██▊       | 56/201 [00:27<00:45,  3.17it/s]11/28/2021 01:56:39 - INFO - __main__ -   Batch number = 57
Evaluating:  28%|██▊       | 57/201 [00:28<00:45,  3.15it/s]11/28/2021 01:56:39 - INFO - __main__ -   Batch number = 58
Evaluating:  29%|██▉       | 58/201 [00:28<00:45,  3.14it/s]11/28/2021 01:56:40 - INFO - __main__ -   Batch number = 59
Evaluating:  29%|██▉       | 59/201 [00:28<00:45,  3.14it/s]11/28/2021 01:56:40 - INFO - __main__ -   Batch number = 60
Evaluating:  30%|██▉       | 60/201 [00:28<00:44,  3.13it/s]11/28/2021 01:56:40 - INFO - __main__ -   Batch number = 61
Evaluating:  30%|███       | 61/201 [00:29<00:44,  3.13it/s]11/28/2021 01:56:41 - INFO - __main__ -   Batch number = 62
Evaluating:  31%|███       | 62/201 [00:29<00:44,  3.12it/s]11/28/2021 01:56:41 - INFO - __main__ -   Batch number = 63
Evaluating:  31%|███▏      | 63/201 [00:29<00:44,  3.07it/s]11/28/2021 01:56:41 - INFO - __main__ -   Batch number = 64
Evaluating:  32%|███▏      | 64/201 [00:30<00:44,  3.07it/s]11/28/2021 01:56:42 - INFO - __main__ -   Batch number = 65
Evaluating:  32%|███▏      | 65/201 [00:30<00:47,  2.85it/s]11/28/2021 01:56:42 - INFO - __main__ -   Batch number = 66
Evaluating:  33%|███▎      | 66/201 [00:30<00:44,  3.01it/s]11/28/2021 01:56:42 - INFO - __main__ -   Batch number = 67
Evaluating:  33%|███▎      | 67/201 [00:31<00:42,  3.15it/s]11/28/2021 01:56:43 - INFO - __main__ -   Batch number = 68
Evaluating:  34%|███▍      | 68/201 [00:31<00:41,  3.23it/s]11/28/2021 01:56:43 - INFO - __main__ -   Batch number = 69
Evaluating:  34%|███▍      | 69/201 [00:31<00:40,  3.26it/s]11/28/2021 01:56:43 - INFO - __main__ -   Batch number = 70
Evaluating:  35%|███▍      | 70/201 [00:32<00:40,  3.27it/s]11/28/2021 01:56:44 - INFO - __main__ -   Batch number = 71
Evaluating:  35%|███▌      | 71/201 [00:32<00:39,  3.27it/s]11/28/2021 01:56:44 - INFO - __main__ -   Batch number = 72
Evaluating:  36%|███▌      | 72/201 [00:32<00:39,  3.23it/s]11/28/2021 01:56:44 - INFO - __main__ -   Batch number = 73
Evaluating:  36%|███▋      | 73/201 [00:33<00:40,  3.19it/s]11/28/2021 01:56:44 - INFO - __main__ -   Batch number = 74
Evaluating:  37%|███▋      | 74/201 [00:33<00:40,  3.15it/s]11/28/2021 01:56:45 - INFO - __main__ -   Batch number = 75
Evaluating:  37%|███▋      | 75/201 [00:33<00:40,  3.09it/s]11/28/2021 01:56:45 - INFO - __main__ -   Batch number = 76
Evaluating:  38%|███▊      | 76/201 [00:34<00:40,  3.08it/s]11/28/2021 01:56:45 - INFO - __main__ -   Batch number = 77
Evaluating:  38%|███▊      | 77/201 [00:34<00:39,  3.11it/s]11/28/2021 01:56:46 - INFO - __main__ -   Batch number = 78
Evaluating:  39%|███▉      | 78/201 [00:34<00:39,  3.10it/s]11/28/2021 01:56:46 - INFO - __main__ -   Batch number = 79
Evaluating:  39%|███▉      | 79/201 [00:35<00:39,  3.10it/s]11/28/2021 01:56:46 - INFO - __main__ -   Batch number = 80
Evaluating:  40%|███▉      | 80/201 [00:35<00:38,  3.11it/s]11/28/2021 01:56:47 - INFO - __main__ -   Batch number = 81
Evaluating:  40%|████      | 81/201 [00:35<00:47,  2.52it/s]11/28/2021 01:56:47 - INFO - __main__ -   Batch number = 82
Evaluating:  41%|████      | 82/201 [00:36<00:44,  2.67it/s]11/28/2021 01:56:48 - INFO - __main__ -   Batch number = 83
Evaluating:  41%|████▏     | 83/201 [00:36<00:42,  2.76it/s]11/28/2021 01:56:48 - INFO - __main__ -   Batch number = 84
Evaluating:  42%|████▏     | 84/201 [00:36<00:41,  2.81it/s]11/28/2021 01:56:48 - INFO - __main__ -   Batch number = 85
Evaluating:  42%|████▏     | 85/201 [00:37<00:40,  2.86it/s]11/28/2021 01:56:49 - INFO - __main__ -   Batch number = 86
Evaluating:  43%|████▎     | 86/201 [00:37<00:39,  2.88it/s]11/28/2021 01:56:49 - INFO - __main__ -   Batch number = 87
Evaluating:  43%|████▎     | 87/201 [00:37<00:39,  2.92it/s]11/28/2021 01:56:49 - INFO - __main__ -   Batch number = 88
Evaluating:  44%|████▍     | 88/201 [00:38<00:38,  2.93it/s]11/28/2021 01:56:50 - INFO - __main__ -   Batch number = 89
Evaluating:  44%|████▍     | 89/201 [00:38<00:38,  2.93it/s]11/28/2021 01:56:50 - INFO - __main__ -   Batch number = 90
Evaluating:  45%|████▍     | 90/201 [00:38<00:37,  2.95it/s]11/28/2021 01:56:50 - INFO - __main__ -   Batch number = 91
Evaluating:  45%|████▌     | 91/201 [00:39<00:37,  2.96it/s]11/28/2021 01:56:51 - INFO - __main__ -   Batch number = 92
Evaluating:  46%|████▌     | 92/201 [00:39<00:36,  2.97it/s]11/28/2021 01:56:51 - INFO - __main__ -   Batch number = 93
Evaluating:  46%|████▋     | 93/201 [00:39<00:36,  2.97it/s]11/28/2021 01:56:51 - INFO - __main__ -   Batch number = 94
Evaluating:  47%|████▋     | 94/201 [00:40<00:36,  2.96it/s]11/28/2021 01:56:52 - INFO - __main__ -   Batch number = 95
Evaluating:  47%|████▋     | 95/201 [00:40<00:37,  2.80it/s]11/28/2021 01:56:52 - INFO - __main__ -   Batch number = 96
Evaluating:  48%|████▊     | 96/201 [00:41<00:35,  2.92it/s]11/28/2021 01:56:52 - INFO - __main__ -   Batch number = 97
Evaluating:  48%|████▊     | 97/201 [00:41<00:34,  3.00it/s]11/28/2021 01:56:53 - INFO - __main__ -   Batch number = 98
Evaluating:  49%|████▉     | 98/201 [00:41<00:33,  3.04it/s]11/28/2021 01:56:53 - INFO - __main__ -   Batch number = 99
Evaluating:  49%|████▉     | 99/201 [00:41<00:33,  3.06it/s]11/28/2021 01:56:53 - INFO - __main__ -   Batch number = 100
Evaluating:  50%|████▉     | 100/201 [00:42<00:33,  3.06it/s]11/28/2021 01:56:54 - INFO - __main__ -   Batch number = 101
Evaluating:  50%|█████     | 101/201 [00:42<00:33,  3.03it/s]11/28/2021 01:56:54 - INFO - __main__ -   Batch number = 102
Evaluating:  51%|█████     | 102/201 [00:42<00:33,  3.00it/s]11/28/2021 01:56:54 - INFO - __main__ -   Batch number = 103
Evaluating:  51%|█████     | 103/201 [00:43<00:32,  2.99it/s]11/28/2021 01:56:55 - INFO - __main__ -   Batch number = 104
Evaluating:  52%|█████▏    | 104/201 [00:43<00:32,  2.97it/s]11/28/2021 01:56:55 - INFO - __main__ -   Batch number = 105
Evaluating:  52%|█████▏    | 105/201 [00:43<00:32,  2.97it/s]11/28/2021 01:56:55 - INFO - __main__ -   Batch number = 106
Evaluating:  53%|█████▎    | 106/201 [00:44<00:32,  2.96it/s]11/28/2021 01:56:56 - INFO - __main__ -   Batch number = 107
Evaluating:  53%|█████▎    | 107/201 [00:44<00:31,  2.96it/s]11/28/2021 01:56:56 - INFO - __main__ -   Batch number = 108
Evaluating:  54%|█████▎    | 108/201 [00:45<00:31,  2.96it/s]11/28/2021 01:56:56 - INFO - __main__ -   Batch number = 109
Evaluating:  54%|█████▍    | 109/201 [00:45<00:31,  2.96it/s]11/28/2021 01:56:57 - INFO - __main__ -   Batch number = 110
Evaluating:  55%|█████▍    | 110/201 [00:45<00:30,  2.96it/s]11/28/2021 01:56:57 - INFO - __main__ -   Batch number = 111
Evaluating:  55%|█████▌    | 111/201 [00:46<00:30,  2.96it/s]11/28/2021 01:56:57 - INFO - __main__ -   Batch number = 112
Evaluating:  56%|█████▌    | 112/201 [00:46<00:34,  2.58it/s]11/28/2021 01:56:58 - INFO - __main__ -   Batch number = 113
Evaluating:  56%|█████▌    | 113/201 [00:47<00:37,  2.37it/s]11/28/2021 01:56:58 - INFO - __main__ -   Batch number = 114
Evaluating:  57%|█████▋    | 114/201 [00:47<00:38,  2.24it/s]11/28/2021 01:56:59 - INFO - __main__ -   Batch number = 115
Evaluating:  57%|█████▋    | 115/201 [00:48<00:39,  2.16it/s]11/28/2021 01:56:59 - INFO - __main__ -   Batch number = 116
Evaluating:  58%|█████▊    | 116/201 [00:48<00:40,  2.11it/s]11/28/2021 01:57:00 - INFO - __main__ -   Batch number = 117
Evaluating:  58%|█████▊    | 117/201 [00:49<00:40,  2.07it/s]11/28/2021 01:57:00 - INFO - __main__ -   Batch number = 118
Evaluating:  59%|█████▊    | 118/201 [00:49<00:40,  2.05it/s]11/28/2021 01:57:01 - INFO - __main__ -   Batch number = 119
Evaluating:  59%|█████▉    | 119/201 [00:50<00:40,  2.04it/s]11/28/2021 01:57:01 - INFO - __main__ -   Batch number = 120
Evaluating:  60%|█████▉    | 120/201 [00:50<00:39,  2.07it/s]11/28/2021 01:57:02 - INFO - __main__ -   Batch number = 121
Evaluating:  60%|██████    | 121/201 [00:51<00:39,  2.04it/s]11/28/2021 01:57:02 - INFO - __main__ -   Batch number = 122
Evaluating:  61%|██████    | 122/201 [00:51<00:39,  2.02it/s]11/28/2021 01:57:03 - INFO - __main__ -   Batch number = 123
Evaluating:  61%|██████    | 123/201 [00:52<00:38,  2.00it/s]11/28/2021 01:57:03 - INFO - __main__ -   Batch number = 124
Evaluating:  62%|██████▏   | 124/201 [00:52<00:38,  1.99it/s]11/28/2021 01:57:04 - INFO - __main__ -   Batch number = 125
Evaluating:  62%|██████▏   | 125/201 [00:53<00:38,  1.98it/s]11/28/2021 01:57:04 - INFO - __main__ -   Batch number = 126
Evaluating:  63%|██████▎   | 126/201 [00:53<00:37,  1.98it/s]11/28/2021 01:57:05 - INFO - __main__ -   Batch number = 127
Evaluating:  63%|██████▎   | 127/201 [00:54<00:37,  1.98it/s]11/28/2021 01:57:05 - INFO - __main__ -   Batch number = 128
Evaluating:  64%|██████▎   | 128/201 [00:54<00:36,  1.98it/s]11/28/2021 01:57:06 - INFO - __main__ -   Batch number = 129
Evaluating:  64%|██████▍   | 129/201 [00:55<00:36,  1.97it/s]11/28/2021 01:57:06 - INFO - __main__ -   Batch number = 130
Evaluating:  65%|██████▍   | 130/201 [00:55<00:36,  1.97it/s]11/28/2021 01:57:07 - INFO - __main__ -   Batch number = 131
Evaluating:  65%|██████▌   | 131/201 [00:56<00:35,  1.97it/s]11/28/2021 01:57:07 - INFO - __main__ -   Batch number = 132
Evaluating:  66%|██████▌   | 132/201 [00:56<00:34,  1.98it/s]11/28/2021 01:57:08 - INFO - __main__ -   Batch number = 133
Evaluating:  66%|██████▌   | 133/201 [00:57<00:34,  1.98it/s]11/28/2021 01:57:08 - INFO - __main__ -   Batch number = 134
Evaluating:  67%|██████▋   | 134/201 [00:57<00:33,  1.98it/s]11/28/2021 01:57:09 - INFO - __main__ -   Batch number = 135
Evaluating:  67%|██████▋   | 135/201 [00:58<00:33,  1.97it/s]11/28/2021 01:57:10 - INFO - __main__ -   Batch number = 136
Evaluating:  68%|██████▊   | 136/201 [00:58<00:33,  1.96it/s]11/28/2021 01:57:10 - INFO - __main__ -   Batch number = 137
Evaluating:  68%|██████▊   | 137/201 [00:59<00:32,  1.95it/s]11/28/2021 01:57:11 - INFO - __main__ -   Batch number = 138
Evaluating:  69%|██████▊   | 138/201 [00:59<00:32,  1.94it/s]11/28/2021 01:57:11 - INFO - __main__ -   Batch number = 139
Evaluating:  69%|██████▉   | 139/201 [01:00<00:32,  1.92it/s]11/28/2021 01:57:12 - INFO - __main__ -   Batch number = 140
Evaluating:  70%|██████▉   | 140/201 [01:00<00:30,  2.00it/s]11/28/2021 01:57:12 - INFO - __main__ -   Batch number = 141
Evaluating:  70%|███████   | 141/201 [01:01<00:30,  1.98it/s]11/28/2021 01:57:13 - INFO - __main__ -   Batch number = 142
Evaluating:  71%|███████   | 142/201 [01:01<00:29,  1.98it/s]11/28/2021 01:57:13 - INFO - __main__ -   Batch number = 143
Evaluating:  71%|███████   | 143/201 [01:02<00:29,  1.98it/s]11/28/2021 01:57:14 - INFO - __main__ -   Batch number = 144
Evaluating:  72%|███████▏  | 144/201 [01:02<00:31,  1.79it/s]11/28/2021 01:57:14 - INFO - __main__ -   Batch number = 145
Evaluating:  72%|███████▏  | 145/201 [01:03<00:33,  1.68it/s]11/28/2021 01:57:15 - INFO - __main__ -   Batch number = 146
Evaluating:  73%|███████▎  | 146/201 [01:04<00:33,  1.62it/s]11/28/2021 01:57:16 - INFO - __main__ -   Batch number = 147
Evaluating:  73%|███████▎  | 147/201 [01:04<00:34,  1.58it/s]11/28/2021 01:57:16 - INFO - __main__ -   Batch number = 148
Evaluating:  74%|███████▎  | 148/201 [01:05<00:34,  1.55it/s]11/28/2021 01:57:17 - INFO - __main__ -   Batch number = 149
Evaluating:  74%|███████▍  | 149/201 [01:06<00:34,  1.53it/s]11/28/2021 01:57:18 - INFO - __main__ -   Batch number = 150
Evaluating:  75%|███████▍  | 150/201 [01:06<00:33,  1.51it/s]11/28/2021 01:57:18 - INFO - __main__ -   Batch number = 151
Evaluating:  75%|███████▌  | 151/201 [01:07<00:33,  1.51it/s]11/28/2021 01:57:19 - INFO - __main__ -   Batch number = 152
Evaluating:  76%|███████▌  | 152/201 [01:08<00:32,  1.51it/s]11/28/2021 01:57:20 - INFO - __main__ -   Batch number = 153
Evaluating:  76%|███████▌  | 153/201 [01:08<00:32,  1.50it/s]11/28/2021 01:57:20 - INFO - __main__ -   Batch number = 154
Evaluating:  77%|███████▋  | 154/201 [01:09<00:31,  1.50it/s]11/28/2021 01:57:21 - INFO - __main__ -   Batch number = 155
Evaluating:  77%|███████▋  | 155/201 [01:10<00:30,  1.51it/s]11/28/2021 01:57:22 - INFO - __main__ -   Batch number = 156
Evaluating:  78%|███████▊  | 156/201 [01:10<00:29,  1.52it/s]11/28/2021 01:57:22 - INFO - __main__ -   Batch number = 157
Evaluating:  78%|███████▊  | 157/201 [01:11<00:29,  1.51it/s]11/28/2021 01:57:23 - INFO - __main__ -   Batch number = 158
Evaluating:  79%|███████▊  | 158/201 [01:12<00:28,  1.51it/s]11/28/2021 01:57:24 - INFO - __main__ -   Batch number = 159
Evaluating:  79%|███████▉  | 159/201 [01:12<00:27,  1.51it/s]11/28/2021 01:57:24 - INFO - __main__ -   Batch number = 160
Evaluating:  80%|███████▉  | 160/201 [01:13<00:27,  1.51it/s]11/28/2021 01:57:25 - INFO - __main__ -   Batch number = 161
Evaluating:  80%|████████  | 161/201 [01:14<00:26,  1.51it/s]11/28/2021 01:57:26 - INFO - __main__ -   Batch number = 162
Evaluating:  81%|████████  | 162/201 [01:14<00:25,  1.51it/s]11/28/2021 01:57:26 - INFO - __main__ -   Batch number = 163
Evaluating:  81%|████████  | 163/201 [01:15<00:25,  1.51it/s]11/28/2021 01:57:27 - INFO - __main__ -   Batch number = 164
Evaluating:  82%|████████▏ | 164/201 [01:16<00:24,  1.51it/s]11/28/2021 01:57:28 - INFO - __main__ -   Batch number = 165
Evaluating:  82%|████████▏ | 165/201 [01:16<00:22,  1.58it/s]11/28/2021 01:57:28 - INFO - __main__ -   Batch number = 166
Evaluating:  83%|████████▎ | 166/201 [01:17<00:19,  1.78it/s]11/28/2021 01:57:29 - INFO - __main__ -   Batch number = 167
Evaluating:  83%|████████▎ | 167/201 [01:17<00:16,  2.02it/s]11/28/2021 01:57:29 - INFO - __main__ -   Batch number = 168
Evaluating:  84%|████████▎ | 168/201 [01:17<00:14,  2.22it/s]11/28/2021 01:57:29 - INFO - __main__ -   Batch number = 169
Evaluating:  84%|████████▍ | 169/201 [01:18<00:13,  2.37it/s]11/28/2021 01:57:30 - INFO - __main__ -   Batch number = 170
Evaluating:  85%|████████▍ | 170/201 [01:18<00:12,  2.52it/s]11/28/2021 01:57:30 - INFO - __main__ -   Batch number = 171
Evaluating:  85%|████████▌ | 171/201 [01:18<00:11,  2.57it/s]11/28/2021 01:57:30 - INFO - __main__ -   Batch number = 172
Evaluating:  86%|████████▌ | 172/201 [01:19<00:10,  2.65it/s]11/28/2021 01:57:31 - INFO - __main__ -   Batch number = 173
Evaluating:  86%|████████▌ | 173/201 [01:19<00:10,  2.69it/s]11/28/2021 01:57:31 - INFO - __main__ -   Batch number = 174
Evaluating:  87%|████████▋ | 174/201 [01:19<00:09,  2.75it/s]11/28/2021 01:57:31 - INFO - __main__ -   Batch number = 175
Evaluating:  87%|████████▋ | 175/201 [01:20<00:09,  2.79it/s]11/28/2021 01:57:32 - INFO - __main__ -   Batch number = 176
Evaluating:  88%|████████▊ | 176/201 [01:20<00:08,  2.91it/s]11/28/2021 01:57:32 - INFO - __main__ -   Batch number = 177
Evaluating:  88%|████████▊ | 177/201 [01:20<00:08,  2.92it/s]11/28/2021 01:57:32 - INFO - __main__ -   Batch number = 178
Evaluating:  89%|████████▊ | 178/201 [01:21<00:07,  2.90it/s]11/28/2021 01:57:33 - INFO - __main__ -   Batch number = 179
Evaluating:  89%|████████▉ | 179/201 [01:21<00:07,  2.91it/s]11/28/2021 01:57:33 - INFO - __main__ -   Batch number = 180
Evaluating:  90%|████████▉ | 180/201 [01:22<00:07,  2.87it/s]11/28/2021 01:57:33 - INFO - __main__ -   Batch number = 181
Evaluating:  90%|█████████ | 181/201 [01:22<00:06,  2.89it/s]11/28/2021 01:57:34 - INFO - __main__ -   Batch number = 182
Evaluating:  91%|█████████ | 182/201 [01:22<00:06,  2.86it/s]11/28/2021 01:57:34 - INFO - __main__ -   Batch number = 183
Evaluating:  91%|█████████ | 183/201 [01:23<00:06,  2.83it/s]11/28/2021 01:57:34 - INFO - __main__ -   Batch number = 184
Evaluating:  92%|█████████▏| 184/201 [01:23<00:05,  2.83it/s]11/28/2021 01:57:35 - INFO - __main__ -   Batch number = 185
Evaluating:  92%|█████████▏| 185/201 [01:23<00:05,  2.85it/s]11/28/2021 01:57:35 - INFO - __main__ -   Batch number = 186
Evaluating:  93%|█████████▎| 186/201 [01:24<00:05,  2.84it/s]11/28/2021 01:57:36 - INFO - __main__ -   Batch number = 187
Evaluating:  93%|█████████▎| 187/201 [01:24<00:04,  2.86it/s]11/28/2021 01:57:36 - INFO - __main__ -   Batch number = 188
Evaluating:  94%|█████████▎| 188/201 [01:24<00:04,  2.84it/s]11/28/2021 01:57:36 - INFO - __main__ -   Batch number = 189
Evaluating:  94%|█████████▍| 189/201 [01:25<00:04,  2.87it/s]11/28/2021 01:57:37 - INFO - __main__ -   Batch number = 190
Evaluating:  95%|█████████▍| 190/201 [01:25<00:03,  3.07it/s]11/28/2021 01:57:37 - INFO - __main__ -   Batch number = 191
Evaluating:  95%|█████████▌| 191/201 [01:25<00:03,  3.03it/s]11/28/2021 01:57:37 - INFO - __main__ -   Batch number = 192
Evaluating:  96%|█████████▌| 192/201 [01:26<00:03,  3.00it/s]11/28/2021 01:57:38 - INFO - __main__ -   Batch number = 193
Evaluating:  96%|█████████▌| 193/201 [01:26<00:02,  2.99it/s]11/28/2021 01:57:38 - INFO - __main__ -   Batch number = 194
Evaluating:  97%|█████████▋| 194/201 [01:26<00:02,  2.94it/s]11/28/2021 01:57:38 - INFO - __main__ -   Batch number = 195
Evaluating:  97%|█████████▋| 195/201 [01:27<00:02,  2.94it/s]11/28/2021 01:57:39 - INFO - __main__ -   Batch number = 196
Evaluating:  98%|█████████▊| 196/201 [01:27<00:01,  2.92it/s]11/28/2021 01:57:39 - INFO - __main__ -   Batch number = 197
Evaluating:  98%|█████████▊| 197/201 [01:27<00:01,  2.92it/s]11/28/2021 01:57:39 - INFO - __main__ -   Batch number = 198
Evaluating:  99%|█████████▊| 198/201 [01:28<00:01,  2.89it/s]11/28/2021 01:57:40 - INFO - __main__ -   Batch number = 199
Evaluating:  99%|█████████▉| 199/201 [01:28<00:00,  2.89it/s]11/28/2021 01:57:40 - INFO - __main__ -   Batch number = 200
Evaluating: 100%|█████████▉| 200/201 [01:28<00:00,  2.90it/s]11/28/2021 01:57:40 - INFO - __main__ -   Batch number = 201
Evaluating: 100%|██████████| 201/201 [01:28<00:00,  2.26it/s]
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 01:57:44 - INFO - __main__ -   ***** Evaluation result  in is *****
11/28/2021 01:57:44 - INFO - __main__ -     f1 = 0.7535517888766095
11/28/2021 01:57:44 - INFO - __main__ -     loss = 0.816551954443775
11/28/2021 01:57:44 - INFO - __main__ -     precision = 0.758444621642086
11/28/2021 01:57:44 - INFO - __main__ -     recall = 0.7487216801719861
88.17user 30.22system 1:59.56elapsed 99%CPU (0avgtext+0avgdata 3990244maxresident)k
0inputs+1760outputs (0major+1732225minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 01:58:41 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:58:41 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 01:58:41 - INFO - __main__ -   Seed = 1
11/28/2021 01:58:41 - INFO - root -   save model
11/28/2021 01:58:41 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 01:58:41 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 01:58:44 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 01:58:50 - INFO - __main__ -   Using lang2id = None
11/28/2021 01:58:50 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 01:58:50 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
11/28/2021 01:58:50 - INFO - root -   Trying to decide if add adapter
11/28/2021 01:58:50 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 01:58:50 - INFO - root -   loading lang adpater cs/wiki@ukp
11/28/2021 01:58:50 - INFO - __main__ -   Adapter Languages : ['cs'], Length : 1
11/28/2021 01:58:50 - INFO - __main__ -   Adapter Names ['cs/wiki@ukp'], Length : 1
11/28/2021 01:58:50 - INFO - __main__ -   Language = cs
11/28/2021 01:58:50 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/cs/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_cs_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/adapter_config.json
Adding adapter 'cs' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/head_config.json
11/28/2021 01:59:01 - INFO - __main__ -   Language adapter for fi not found, using cs instead
11/28/2021 01:59:01 - INFO - __main__ -   Set active language adapter to cs
11/28/2021 01:59:01 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 01:59:01 - INFO - __main__ -   Adapter Languages = ['cs']
11/28/2021 01:59:01 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_fi_bert-base-multilingual-cased_128
11/28/2021 01:59:02 - INFO - __main__ -   ***** Running evaluation  in fi *****
11/28/2021 01:59:02 - INFO - __main__ -     Num examples = 6550
11/28/2021 01:59:02 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/205 [00:00<?, ?it/s]11/28/2021 01:59:02 - INFO - __main__ -   Batch number = 1
Evaluating:   0%|          | 1/205 [00:00<01:42,  2.00it/s]11/28/2021 01:59:03 - INFO - __main__ -   Batch number = 2
Evaluating:   1%|          | 2/205 [00:00<01:39,  2.03it/s]11/28/2021 01:59:03 - INFO - __main__ -   Batch number = 3
Evaluating:   1%|▏         | 3/205 [00:01<01:38,  2.05it/s]11/28/2021 01:59:04 - INFO - __main__ -   Batch number = 4
Evaluating:   2%|▏         | 4/205 [00:01<01:38,  2.05it/s]11/28/2021 01:59:04 - INFO - __main__ -   Batch number = 5
Evaluating:   2%|▏         | 5/205 [00:02<01:37,  2.05it/s]11/28/2021 01:59:05 - INFO - __main__ -   Batch number = 6
Evaluating:   3%|▎         | 6/205 [00:02<01:37,  2.04it/s]11/28/2021 01:59:05 - INFO - __main__ -   Batch number = 7
Evaluating:   3%|▎         | 7/205 [00:03<01:37,  2.04it/s]11/28/2021 01:59:06 - INFO - __main__ -   Batch number = 8
Evaluating:   4%|▍         | 8/205 [00:03<01:37,  2.02it/s]11/28/2021 01:59:06 - INFO - __main__ -   Batch number = 9
Evaluating:   4%|▍         | 9/205 [00:04<01:36,  2.02it/s]11/28/2021 01:59:07 - INFO - __main__ -   Batch number = 10
Evaluating:   5%|▍         | 10/205 [00:04<01:35,  2.05it/s]11/28/2021 01:59:07 - INFO - __main__ -   Batch number = 11
Evaluating:   5%|▌         | 11/205 [00:05<01:34,  2.04it/s]11/28/2021 01:59:08 - INFO - __main__ -   Batch number = 12
Evaluating:   6%|▌         | 12/205 [00:05<01:34,  2.04it/s]11/28/2021 01:59:08 - INFO - __main__ -   Batch number = 13
Evaluating:   6%|▋         | 13/205 [00:06<01:34,  2.04it/s]11/28/2021 01:59:09 - INFO - __main__ -   Batch number = 14
Evaluating:   7%|▋         | 14/205 [00:06<01:33,  2.03it/s]11/28/2021 01:59:09 - INFO - __main__ -   Batch number = 15
Evaluating:   7%|▋         | 15/205 [00:07<01:33,  2.03it/s]11/28/2021 01:59:10 - INFO - __main__ -   Batch number = 16
Evaluating:   8%|▊         | 16/205 [00:07<01:33,  2.03it/s]11/28/2021 01:59:10 - INFO - __main__ -   Batch number = 17
Evaluating:   8%|▊         | 17/205 [00:08<01:33,  2.02it/s]11/28/2021 01:59:11 - INFO - __main__ -   Batch number = 18
Evaluating:   9%|▉         | 18/205 [00:08<01:33,  2.01it/s]11/28/2021 01:59:11 - INFO - __main__ -   Batch number = 19
Evaluating:   9%|▉         | 19/205 [00:09<01:32,  2.01it/s]11/28/2021 01:59:12 - INFO - __main__ -   Batch number = 20
Evaluating:  10%|▉         | 20/205 [00:09<01:28,  2.09it/s]11/28/2021 01:59:12 - INFO - __main__ -   Batch number = 21
Evaluating:  10%|█         | 21/205 [00:10<01:28,  2.07it/s]11/28/2021 01:59:12 - INFO - __main__ -   Batch number = 22
Evaluating:  11%|█         | 22/205 [00:10<01:28,  2.06it/s]11/28/2021 01:59:13 - INFO - __main__ -   Batch number = 23
Evaluating:  11%|█         | 23/205 [00:11<01:28,  2.05it/s]11/28/2021 01:59:13 - INFO - __main__ -   Batch number = 24
Evaluating:  12%|█▏        | 24/205 [00:11<01:28,  2.04it/s]11/28/2021 01:59:14 - INFO - __main__ -   Batch number = 25
Evaluating:  12%|█▏        | 25/205 [00:12<01:28,  2.03it/s]11/28/2021 01:59:14 - INFO - __main__ -   Batch number = 26
Evaluating:  13%|█▎        | 26/205 [00:12<01:28,  2.02it/s]11/28/2021 01:59:15 - INFO - __main__ -   Batch number = 27
Evaluating:  13%|█▎        | 27/205 [00:13<01:28,  2.02it/s]11/28/2021 01:59:15 - INFO - __main__ -   Batch number = 28
Evaluating:  14%|█▎        | 28/205 [00:13<01:28,  2.01it/s]11/28/2021 01:59:16 - INFO - __main__ -   Batch number = 29
Evaluating:  14%|█▍        | 29/205 [00:14<01:27,  2.02it/s]11/28/2021 01:59:16 - INFO - __main__ -   Batch number = 30
Evaluating:  15%|█▍        | 30/205 [00:14<01:25,  2.04it/s]11/28/2021 01:59:17 - INFO - __main__ -   Batch number = 31
Evaluating:  15%|█▌        | 31/205 [00:15<01:25,  2.04it/s]11/28/2021 01:59:17 - INFO - __main__ -   Batch number = 32
Evaluating:  16%|█▌        | 32/205 [00:15<01:24,  2.05it/s]11/28/2021 01:59:18 - INFO - __main__ -   Batch number = 33
Evaluating:  16%|█▌        | 33/205 [00:16<01:23,  2.05it/s]11/28/2021 01:59:18 - INFO - __main__ -   Batch number = 34
Evaluating:  17%|█▋        | 34/205 [00:16<01:23,  2.05it/s]11/28/2021 01:59:19 - INFO - __main__ -   Batch number = 35
Evaluating:  17%|█▋        | 35/205 [00:17<01:23,  2.04it/s]11/28/2021 01:59:19 - INFO - __main__ -   Batch number = 36
Evaluating:  18%|█▊        | 36/205 [00:17<01:23,  2.03it/s]11/28/2021 01:59:20 - INFO - __main__ -   Batch number = 37
Evaluating:  18%|█▊        | 37/205 [00:18<01:22,  2.03it/s]11/28/2021 01:59:20 - INFO - __main__ -   Batch number = 38
Evaluating:  19%|█▊        | 38/205 [00:18<01:22,  2.02it/s]11/28/2021 01:59:21 - INFO - __main__ -   Batch number = 39
Evaluating:  19%|█▉        | 39/205 [00:19<01:22,  2.02it/s]11/28/2021 01:59:21 - INFO - __main__ -   Batch number = 40
Evaluating:  20%|█▉        | 40/205 [00:19<01:21,  2.02it/s]11/28/2021 01:59:22 - INFO - __main__ -   Batch number = 41
Evaluating:  20%|██        | 41/205 [00:20<01:22,  1.99it/s]11/28/2021 01:59:22 - INFO - __main__ -   Batch number = 42
Evaluating:  20%|██        | 42/205 [00:20<01:21,  2.01it/s]11/28/2021 01:59:23 - INFO - __main__ -   Batch number = 43
Evaluating:  21%|██        | 43/205 [00:21<01:20,  2.01it/s]11/28/2021 01:59:23 - INFO - __main__ -   Batch number = 44
Evaluating:  21%|██▏       | 44/205 [00:21<01:20,  2.01it/s]11/28/2021 01:59:24 - INFO - __main__ -   Batch number = 45
Evaluating:  22%|██▏       | 45/205 [00:22<01:18,  2.03it/s]11/28/2021 01:59:24 - INFO - __main__ -   Batch number = 46
Evaluating:  22%|██▏       | 46/205 [00:22<01:17,  2.04it/s]11/28/2021 01:59:25 - INFO - __main__ -   Batch number = 47
Evaluating:  23%|██▎       | 47/205 [00:23<01:17,  2.04it/s]11/28/2021 01:59:25 - INFO - __main__ -   Batch number = 48
Evaluating:  23%|██▎       | 48/205 [00:23<01:17,  2.04it/s]11/28/2021 01:59:26 - INFO - __main__ -   Batch number = 49
Evaluating:  24%|██▍       | 49/205 [00:24<01:16,  2.04it/s]11/28/2021 01:59:26 - INFO - __main__ -   Batch number = 50
Evaluating:  24%|██▍       | 50/205 [00:24<01:16,  2.03it/s]11/28/2021 01:59:27 - INFO - __main__ -   Batch number = 51
Evaluating:  25%|██▍       | 51/205 [00:25<01:15,  2.04it/s]11/28/2021 01:59:27 - INFO - __main__ -   Batch number = 52
Evaluating:  25%|██▌       | 52/205 [00:25<01:22,  1.86it/s]11/28/2021 01:59:28 - INFO - __main__ -   Batch number = 53
Evaluating:  26%|██▌       | 53/205 [00:26<01:26,  1.75it/s]11/28/2021 01:59:29 - INFO - __main__ -   Batch number = 54
Evaluating:  26%|██▋       | 54/205 [00:27<01:30,  1.68it/s]11/28/2021 01:59:29 - INFO - __main__ -   Batch number = 55
Evaluating:  27%|██▋       | 55/205 [00:27<01:31,  1.64it/s]11/28/2021 01:59:30 - INFO - __main__ -   Batch number = 56
Evaluating:  27%|██▋       | 56/205 [00:28<01:32,  1.60it/s]11/28/2021 01:59:31 - INFO - __main__ -   Batch number = 57
Evaluating:  28%|██▊       | 57/205 [00:28<01:33,  1.58it/s]11/28/2021 01:59:31 - INFO - __main__ -   Batch number = 58
Evaluating:  28%|██▊       | 58/205 [00:29<01:34,  1.56it/s]11/28/2021 01:59:32 - INFO - __main__ -   Batch number = 59
Evaluating:  29%|██▉       | 59/205 [00:30<01:38,  1.48it/s]11/28/2021 01:59:33 - INFO - __main__ -   Batch number = 60
Evaluating:  29%|██▉       | 60/205 [00:31<01:37,  1.49it/s]11/28/2021 01:59:33 - INFO - __main__ -   Batch number = 61
Evaluating:  30%|██▉       | 61/205 [00:31<01:36,  1.49it/s]11/28/2021 01:59:34 - INFO - __main__ -   Batch number = 62
Evaluating:  30%|███       | 62/205 [00:32<01:29,  1.60it/s]11/28/2021 01:59:34 - INFO - __main__ -   Batch number = 63
Evaluating:  31%|███       | 63/205 [00:32<01:23,  1.70it/s]11/28/2021 01:59:35 - INFO - __main__ -   Batch number = 64
Evaluating:  31%|███       | 64/205 [00:33<01:19,  1.78it/s]11/28/2021 01:59:35 - INFO - __main__ -   Batch number = 65
Evaluating:  32%|███▏      | 65/205 [00:33<01:16,  1.84it/s]11/28/2021 01:59:36 - INFO - __main__ -   Batch number = 66
Evaluating:  32%|███▏      | 66/205 [00:34<01:13,  1.88it/s]11/28/2021 01:59:36 - INFO - __main__ -   Batch number = 67
Evaluating:  33%|███▎      | 67/205 [00:34<01:12,  1.91it/s]11/28/2021 01:59:37 - INFO - __main__ -   Batch number = 68
Evaluating:  33%|███▎      | 68/205 [00:35<01:10,  1.94it/s]11/28/2021 01:59:37 - INFO - __main__ -   Batch number = 69
Evaluating:  34%|███▎      | 69/205 [00:35<01:09,  1.95it/s]11/28/2021 01:59:38 - INFO - __main__ -   Batch number = 70
Evaluating:  34%|███▍      | 70/205 [00:36<01:08,  1.96it/s]11/28/2021 01:59:38 - INFO - __main__ -   Batch number = 71
Evaluating:  35%|███▍      | 71/205 [00:36<01:07,  2.00it/s]11/28/2021 01:59:39 - INFO - __main__ -   Batch number = 72
Evaluating:  35%|███▌      | 72/205 [00:37<01:06,  2.00it/s]11/28/2021 01:59:39 - INFO - __main__ -   Batch number = 73
Evaluating:  36%|███▌      | 73/205 [00:37<01:06,  1.99it/s]11/28/2021 01:59:40 - INFO - __main__ -   Batch number = 74
Evaluating:  36%|███▌      | 74/205 [00:38<01:05,  1.99it/s]11/28/2021 01:59:40 - INFO - __main__ -   Batch number = 75
Evaluating:  37%|███▋      | 75/205 [00:38<01:05,  2.00it/s]11/28/2021 01:59:41 - INFO - __main__ -   Batch number = 76
Evaluating:  37%|███▋      | 76/205 [00:39<01:04,  2.00it/s]11/28/2021 01:59:41 - INFO - __main__ -   Batch number = 77
Evaluating:  38%|███▊      | 77/205 [00:39<01:03,  2.01it/s]11/28/2021 01:59:42 - INFO - __main__ -   Batch number = 78
Evaluating:  38%|███▊      | 78/205 [00:40<01:02,  2.04it/s]11/28/2021 01:59:42 - INFO - __main__ -   Batch number = 79
Evaluating:  39%|███▊      | 79/205 [00:40<00:59,  2.12it/s]11/28/2021 01:59:43 - INFO - __main__ -   Batch number = 80
Evaluating:  39%|███▉      | 80/205 [00:40<00:53,  2.33it/s]11/28/2021 01:59:43 - INFO - __main__ -   Batch number = 81
Evaluating:  40%|███▉      | 81/205 [00:41<00:49,  2.50it/s]11/28/2021 01:59:44 - INFO - __main__ -   Batch number = 82
Evaluating:  40%|████      | 82/205 [00:41<00:46,  2.63it/s]11/28/2021 01:59:44 - INFO - __main__ -   Batch number = 83
Evaluating:  40%|████      | 83/205 [00:41<00:44,  2.75it/s]11/28/2021 01:59:44 - INFO - __main__ -   Batch number = 84
Evaluating:  41%|████      | 84/205 [00:42<00:43,  2.81it/s]11/28/2021 01:59:45 - INFO - __main__ -   Batch number = 85
Evaluating:  41%|████▏     | 85/205 [00:42<00:42,  2.85it/s]11/28/2021 01:59:45 - INFO - __main__ -   Batch number = 86
Evaluating:  42%|████▏     | 86/205 [00:42<00:41,  2.88it/s]11/28/2021 01:59:45 - INFO - __main__ -   Batch number = 87
Evaluating:  42%|████▏     | 87/205 [00:43<00:40,  2.92it/s]11/28/2021 01:59:46 - INFO - __main__ -   Batch number = 88
Evaluating:  43%|████▎     | 88/205 [00:43<00:39,  2.95it/s]11/28/2021 01:59:46 - INFO - __main__ -   Batch number = 89
Evaluating:  43%|████▎     | 89/205 [00:43<00:39,  2.97it/s]11/28/2021 01:59:46 - INFO - __main__ -   Batch number = 90
Evaluating:  44%|████▍     | 90/205 [00:44<00:38,  2.98it/s]11/28/2021 01:59:47 - INFO - __main__ -   Batch number = 91
Evaluating:  44%|████▍     | 91/205 [00:44<00:38,  2.99it/s]11/28/2021 01:59:47 - INFO - __main__ -   Batch number = 92
Evaluating:  45%|████▍     | 92/205 [00:44<00:37,  2.99it/s]11/28/2021 01:59:47 - INFO - __main__ -   Batch number = 93
Evaluating:  45%|████▌     | 93/205 [00:45<00:37,  2.99it/s]11/28/2021 01:59:48 - INFO - __main__ -   Batch number = 94
Evaluating:  46%|████▌     | 94/205 [00:45<00:36,  3.00it/s]11/28/2021 01:59:48 - INFO - __main__ -   Batch number = 95
Evaluating:  46%|████▋     | 95/205 [00:45<00:36,  3.00it/s]11/28/2021 01:59:48 - INFO - __main__ -   Batch number = 96
Evaluating:  47%|████▋     | 96/205 [00:46<00:36,  3.00it/s]11/28/2021 01:59:49 - INFO - __main__ -   Batch number = 97
Evaluating:  47%|████▋     | 97/205 [00:46<00:36,  3.00it/s]11/28/2021 01:59:49 - INFO - __main__ -   Batch number = 98
Evaluating:  48%|████▊     | 98/205 [00:46<00:35,  2.97it/s]11/28/2021 01:59:49 - INFO - __main__ -   Batch number = 99
Evaluating:  48%|████▊     | 99/205 [00:47<00:35,  2.99it/s]11/28/2021 01:59:50 - INFO - __main__ -   Batch number = 100
Evaluating:  49%|████▉     | 100/205 [00:47<00:35,  3.00it/s]11/28/2021 01:59:50 - INFO - __main__ -   Batch number = 101
Evaluating:  49%|████▉     | 101/205 [00:47<00:34,  3.00it/s]11/28/2021 01:59:50 - INFO - __main__ -   Batch number = 102
Evaluating:  50%|████▉     | 102/205 [00:48<00:34,  3.00it/s]11/28/2021 01:59:51 - INFO - __main__ -   Batch number = 103
Evaluating:  50%|█████     | 103/205 [00:48<00:34,  3.00it/s]11/28/2021 01:59:51 - INFO - __main__ -   Batch number = 104
Evaluating:  51%|█████     | 104/205 [00:48<00:33,  2.99it/s]11/28/2021 01:59:51 - INFO - __main__ -   Batch number = 105
Evaluating:  51%|█████     | 105/205 [00:49<00:33,  2.99it/s]11/28/2021 01:59:52 - INFO - __main__ -   Batch number = 106
Evaluating:  52%|█████▏    | 106/205 [00:49<00:33,  3.00it/s]11/28/2021 01:59:52 - INFO - __main__ -   Batch number = 107
Evaluating:  52%|█████▏    | 107/205 [00:50<00:34,  2.82it/s]11/28/2021 01:59:52 - INFO - __main__ -   Batch number = 108
Evaluating:  53%|█████▎    | 108/205 [00:50<00:28,  3.38it/s]11/28/2021 01:59:52 - INFO - __main__ -   Batch number = 109
Evaluating:  53%|█████▎    | 109/205 [00:50<00:24,  3.92it/s]11/28/2021 01:59:53 - INFO - __main__ -   Batch number = 110
Evaluating:  54%|█████▎    | 110/205 [00:50<00:21,  4.45it/s]11/28/2021 01:59:53 - INFO - __main__ -   Batch number = 111
Evaluating:  54%|█████▍    | 111/205 [00:50<00:19,  4.90it/s]11/28/2021 01:59:53 - INFO - __main__ -   Batch number = 112
Evaluating:  55%|█████▍    | 112/205 [00:50<00:17,  5.28it/s]11/28/2021 01:59:53 - INFO - __main__ -   Batch number = 113
Evaluating:  55%|█████▌    | 113/205 [00:50<00:16,  5.60it/s]11/28/2021 01:59:53 - INFO - __main__ -   Batch number = 114
Evaluating:  56%|█████▌    | 114/205 [00:51<00:15,  5.82it/s]11/28/2021 01:59:53 - INFO - __main__ -   Batch number = 115
Evaluating:  56%|█████▌    | 115/205 [00:51<00:14,  6.00it/s]11/28/2021 01:59:54 - INFO - __main__ -   Batch number = 116
Evaluating:  57%|█████▋    | 116/205 [00:51<00:14,  6.16it/s]11/28/2021 01:59:54 - INFO - __main__ -   Batch number = 117
Evaluating:  57%|█████▋    | 117/205 [00:51<00:14,  6.27it/s]11/28/2021 01:59:54 - INFO - __main__ -   Batch number = 118
Evaluating:  58%|█████▊    | 118/205 [00:51<00:13,  6.32it/s]11/28/2021 01:59:54 - INFO - __main__ -   Batch number = 119
Evaluating:  58%|█████▊    | 119/205 [00:51<00:13,  6.38it/s]11/28/2021 01:59:54 - INFO - __main__ -   Batch number = 120
Evaluating:  59%|█████▊    | 120/205 [00:52<00:13,  6.38it/s]11/28/2021 01:59:54 - INFO - __main__ -   Batch number = 121
Evaluating:  59%|█████▉    | 121/205 [00:52<00:13,  6.40it/s]11/28/2021 01:59:54 - INFO - __main__ -   Batch number = 122
Evaluating:  60%|█████▉    | 122/205 [00:52<00:12,  6.39it/s]11/28/2021 01:59:55 - INFO - __main__ -   Batch number = 123
Evaluating:  60%|██████    | 123/205 [00:52<00:12,  6.39it/s]11/28/2021 01:59:55 - INFO - __main__ -   Batch number = 124
Evaluating:  60%|██████    | 124/205 [00:52<00:12,  6.38it/s]11/28/2021 01:59:55 - INFO - __main__ -   Batch number = 125
Evaluating:  61%|██████    | 125/205 [00:52<00:12,  6.22it/s]11/28/2021 01:59:55 - INFO - __main__ -   Batch number = 126
Evaluating:  61%|██████▏   | 126/205 [00:53<00:12,  6.24it/s]11/28/2021 01:59:55 - INFO - __main__ -   Batch number = 127
Evaluating:  62%|██████▏   | 127/205 [00:53<00:12,  6.28it/s]11/28/2021 01:59:55 - INFO - __main__ -   Batch number = 128
Evaluating:  62%|██████▏   | 128/205 [00:53<00:12,  6.15it/s]11/28/2021 01:59:56 - INFO - __main__ -   Batch number = 129
Evaluating:  63%|██████▎   | 129/205 [00:53<00:12,  6.19it/s]11/28/2021 01:59:56 - INFO - __main__ -   Batch number = 130
Evaluating:  63%|██████▎   | 130/205 [00:53<00:12,  6.21it/s]11/28/2021 01:59:56 - INFO - __main__ -   Batch number = 131
Evaluating:  64%|██████▍   | 131/205 [00:53<00:12,  6.09it/s]11/28/2021 01:59:56 - INFO - __main__ -   Batch number = 132
Evaluating:  64%|██████▍   | 132/205 [00:54<00:11,  6.13it/s]11/28/2021 01:59:56 - INFO - __main__ -   Batch number = 133
Evaluating:  65%|██████▍   | 133/205 [00:54<00:11,  6.19it/s]11/28/2021 01:59:56 - INFO - __main__ -   Batch number = 134
Evaluating:  65%|██████▌   | 134/205 [00:54<00:11,  6.08it/s]11/28/2021 01:59:57 - INFO - __main__ -   Batch number = 135
Evaluating:  66%|██████▌   | 135/205 [00:54<00:11,  6.13it/s]11/28/2021 01:59:57 - INFO - __main__ -   Batch number = 136
Evaluating:  66%|██████▋   | 136/205 [00:54<00:11,  6.13it/s]11/28/2021 01:59:57 - INFO - __main__ -   Batch number = 137
Evaluating:  67%|██████▋   | 137/205 [00:54<00:12,  5.43it/s]11/28/2021 01:59:57 - INFO - __main__ -   Batch number = 138
Evaluating:  67%|██████▋   | 138/205 [00:55<00:11,  5.65it/s]11/28/2021 01:59:57 - INFO - __main__ -   Batch number = 139
Evaluating:  68%|██████▊   | 139/205 [00:55<00:11,  5.81it/s]11/28/2021 01:59:57 - INFO - __main__ -   Batch number = 140
Evaluating:  68%|██████▊   | 140/205 [00:55<00:11,  5.80it/s]11/28/2021 01:59:58 - INFO - __main__ -   Batch number = 141
Evaluating:  69%|██████▉   | 141/205 [00:55<00:11,  5.75it/s]11/28/2021 01:59:58 - INFO - __main__ -   Batch number = 142
Evaluating:  69%|██████▉   | 142/205 [00:55<00:11,  5.70it/s]11/28/2021 01:59:58 - INFO - __main__ -   Batch number = 143
Evaluating:  70%|██████▉   | 143/205 [00:55<00:11,  5.54it/s]11/28/2021 01:59:58 - INFO - __main__ -   Batch number = 144
Evaluating:  70%|███████   | 144/205 [00:56<00:10,  5.56it/s]11/28/2021 01:59:58 - INFO - __main__ -   Batch number = 145
Evaluating:  71%|███████   | 145/205 [00:56<00:10,  5.61it/s]11/28/2021 01:59:58 - INFO - __main__ -   Batch number = 146
Evaluating:  71%|███████   | 146/205 [00:56<00:10,  5.67it/s]11/28/2021 01:59:59 - INFO - __main__ -   Batch number = 147
Evaluating:  72%|███████▏  | 147/205 [00:56<00:10,  5.65it/s]11/28/2021 01:59:59 - INFO - __main__ -   Batch number = 148
Evaluating:  72%|███████▏  | 148/205 [00:56<00:10,  5.57it/s]11/28/2021 01:59:59 - INFO - __main__ -   Batch number = 149
Evaluating:  73%|███████▎  | 149/205 [00:56<00:09,  5.61it/s]11/28/2021 01:59:59 - INFO - __main__ -   Batch number = 150
Evaluating:  73%|███████▎  | 150/205 [00:57<00:12,  4.50it/s]11/28/2021 02:00:00 - INFO - __main__ -   Batch number = 151
Evaluating:  74%|███████▎  | 151/205 [00:57<00:14,  3.81it/s]11/28/2021 02:00:00 - INFO - __main__ -   Batch number = 152
Evaluating:  74%|███████▍  | 152/205 [00:58<00:15,  3.50it/s]11/28/2021 02:00:00 - INFO - __main__ -   Batch number = 153
Evaluating:  75%|███████▍  | 153/205 [00:58<00:15,  3.29it/s]11/28/2021 02:00:01 - INFO - __main__ -   Batch number = 154
Evaluating:  75%|███████▌  | 154/205 [00:58<00:16,  3.14it/s]11/28/2021 02:00:01 - INFO - __main__ -   Batch number = 155
Evaluating:  76%|███████▌  | 155/205 [00:59<00:16,  3.05it/s]11/28/2021 02:00:01 - INFO - __main__ -   Batch number = 156
Evaluating:  76%|███████▌  | 156/205 [00:59<00:16,  2.99it/s]11/28/2021 02:00:02 - INFO - __main__ -   Batch number = 157
Evaluating:  77%|███████▋  | 157/205 [00:59<00:15,  3.01it/s]11/28/2021 02:00:02 - INFO - __main__ -   Batch number = 158
Evaluating:  77%|███████▋  | 158/205 [01:00<00:15,  2.94it/s]11/28/2021 02:00:02 - INFO - __main__ -   Batch number = 159
Evaluating:  78%|███████▊  | 159/205 [01:00<00:15,  2.94it/s]11/28/2021 02:00:03 - INFO - __main__ -   Batch number = 160
Evaluating:  78%|███████▊  | 160/205 [01:00<00:15,  2.92it/s]11/28/2021 02:00:03 - INFO - __main__ -   Batch number = 161
Evaluating:  79%|███████▊  | 161/205 [01:01<00:15,  2.91it/s]11/28/2021 02:00:03 - INFO - __main__ -   Batch number = 162
Evaluating:  79%|███████▉  | 162/205 [01:01<00:14,  2.93it/s]11/28/2021 02:00:04 - INFO - __main__ -   Batch number = 163
Evaluating:  80%|███████▉  | 163/205 [01:01<00:14,  2.92it/s]11/28/2021 02:00:04 - INFO - __main__ -   Batch number = 164
Evaluating:  80%|████████  | 164/205 [01:02<00:14,  2.93it/s]11/28/2021 02:00:04 - INFO - __main__ -   Batch number = 165
Evaluating:  80%|████████  | 165/205 [01:02<00:13,  2.93it/s]11/28/2021 02:00:05 - INFO - __main__ -   Batch number = 166
Evaluating:  81%|████████  | 166/205 [01:02<00:13,  2.93it/s]11/28/2021 02:00:05 - INFO - __main__ -   Batch number = 167
Evaluating:  81%|████████▏ | 167/205 [01:03<00:12,  2.93it/s]11/28/2021 02:00:05 - INFO - __main__ -   Batch number = 168
Evaluating:  82%|████████▏ | 168/205 [01:03<00:12,  2.91it/s]11/28/2021 02:00:06 - INFO - __main__ -   Batch number = 169
Evaluating:  82%|████████▏ | 169/205 [01:03<00:12,  2.91it/s]11/28/2021 02:00:06 - INFO - __main__ -   Batch number = 170
Evaluating:  83%|████████▎ | 170/205 [01:04<00:12,  2.90it/s]11/28/2021 02:00:06 - INFO - __main__ -   Batch number = 171
Evaluating:  83%|████████▎ | 171/205 [01:04<00:13,  2.56it/s]11/28/2021 02:00:07 - INFO - __main__ -   Batch number = 172
Evaluating:  84%|████████▍ | 172/205 [01:05<00:14,  2.31it/s]11/28/2021 02:00:07 - INFO - __main__ -   Batch number = 173
Evaluating:  84%|████████▍ | 173/205 [01:05<00:14,  2.18it/s]11/28/2021 02:00:08 - INFO - __main__ -   Batch number = 174
Evaluating:  85%|████████▍ | 174/205 [01:06<00:14,  2.12it/s]11/28/2021 02:00:08 - INFO - __main__ -   Batch number = 175
Evaluating:  85%|████████▌ | 175/205 [01:06<00:14,  2.06it/s]11/28/2021 02:00:09 - INFO - __main__ -   Batch number = 176
Evaluating:  86%|████████▌ | 176/205 [01:07<00:14,  2.04it/s]11/28/2021 02:00:09 - INFO - __main__ -   Batch number = 177
Evaluating:  86%|████████▋ | 177/205 [01:07<00:13,  2.01it/s]11/28/2021 02:00:10 - INFO - __main__ -   Batch number = 178
Evaluating:  87%|████████▋ | 178/205 [01:08<00:13,  2.00it/s]11/28/2021 02:00:11 - INFO - __main__ -   Batch number = 179
Evaluating:  87%|████████▋ | 179/205 [01:08<00:13,  1.98it/s]11/28/2021 02:00:11 - INFO - __main__ -   Batch number = 180
Evaluating:  88%|████████▊ | 180/205 [01:09<00:12,  1.97it/s]11/28/2021 02:00:12 - INFO - __main__ -   Batch number = 181
Evaluating:  88%|████████▊ | 181/205 [01:09<00:12,  1.96it/s]11/28/2021 02:00:12 - INFO - __main__ -   Batch number = 182
Evaluating:  89%|████████▉ | 182/205 [01:10<00:11,  1.96it/s]11/28/2021 02:00:13 - INFO - __main__ -   Batch number = 183
Evaluating:  89%|████████▉ | 183/205 [01:10<00:11,  1.95it/s]11/28/2021 02:00:13 - INFO - __main__ -   Batch number = 184
Evaluating:  90%|████████▉ | 184/205 [01:11<00:10,  1.95it/s]11/28/2021 02:00:14 - INFO - __main__ -   Batch number = 185
Evaluating:  90%|█████████ | 185/205 [01:11<00:10,  1.95it/s]11/28/2021 02:00:14 - INFO - __main__ -   Batch number = 186
Evaluating:  91%|█████████ | 186/205 [01:12<00:09,  1.95it/s]11/28/2021 02:00:15 - INFO - __main__ -   Batch number = 187
Evaluating:  91%|█████████ | 187/205 [01:12<00:09,  1.94it/s]11/28/2021 02:00:15 - INFO - __main__ -   Batch number = 188
Evaluating:  92%|█████████▏| 188/205 [01:13<00:08,  1.94it/s]11/28/2021 02:00:16 - INFO - __main__ -   Batch number = 189
Evaluating:  92%|█████████▏| 189/205 [01:13<00:08,  1.94it/s]11/28/2021 02:00:16 - INFO - __main__ -   Batch number = 190
Evaluating:  93%|█████████▎| 190/205 [01:14<00:07,  1.95it/s]11/28/2021 02:00:17 - INFO - __main__ -   Batch number = 191
Evaluating:  93%|█████████▎| 191/205 [01:14<00:06,  2.00it/s]11/28/2021 02:00:17 - INFO - __main__ -   Batch number = 192
Evaluating:  94%|█████████▎| 192/205 [01:15<00:06,  1.93it/s]11/28/2021 02:00:18 - INFO - __main__ -   Batch number = 193
Evaluating:  94%|█████████▍| 193/205 [01:16<00:06,  1.75it/s]11/28/2021 02:00:18 - INFO - __main__ -   Batch number = 194
Evaluating:  95%|█████████▍| 194/205 [01:16<00:06,  1.66it/s]11/28/2021 02:00:19 - INFO - __main__ -   Batch number = 195
Evaluating:  95%|█████████▌| 195/205 [01:17<00:06,  1.58it/s]11/28/2021 02:00:20 - INFO - __main__ -   Batch number = 196
Evaluating:  96%|█████████▌| 196/205 [01:18<00:05,  1.55it/s]11/28/2021 02:00:20 - INFO - __main__ -   Batch number = 197
Evaluating:  96%|█████████▌| 197/205 [01:18<00:05,  1.52it/s]11/28/2021 02:00:21 - INFO - __main__ -   Batch number = 198
Evaluating:  97%|█████████▋| 198/205 [01:19<00:04,  1.51it/s]11/28/2021 02:00:22 - INFO - __main__ -   Batch number = 199
Evaluating:  97%|█████████▋| 199/205 [01:20<00:04,  1.49it/s]11/28/2021 02:00:23 - INFO - __main__ -   Batch number = 200
Evaluating:  98%|█████████▊| 200/205 [01:20<00:03,  1.48it/s]11/28/2021 02:00:23 - INFO - __main__ -   Batch number = 201
Evaluating:  98%|█████████▊| 201/205 [01:21<00:02,  1.47it/s]11/28/2021 02:00:24 - INFO - __main__ -   Batch number = 202
Evaluating:  99%|█████████▊| 202/205 [01:22<00:02,  1.47it/s]11/28/2021 02:00:25 - INFO - __main__ -   Batch number = 203
Evaluating:  99%|█████████▉| 203/205 [01:23<00:01,  1.47it/s]11/28/2021 02:00:25 - INFO - __main__ -   Batch number = 204
Evaluating: 100%|█████████▉| 204/205 [01:23<00:00,  1.47it/s]11/28/2021 02:00:26 - INFO - __main__ -   Batch number = 205
Evaluating: 100%|██████████| 205/205 [01:24<00:00,  1.36it/s]Evaluating: 100%|██████████| 205/205 [01:24<00:00,  2.42it/s]
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 02:00:29 - INFO - __main__ -   ***** Evaluation result  in fi *****
11/28/2021 02:00:29 - INFO - __main__ -     f1 = 0.7600646601493342
11/28/2021 02:00:29 - INFO - __main__ -     loss = 0.8751982454846545
11/28/2021 02:00:29 - INFO - __main__ -     precision = 0.7625888168056842
11/28/2021 02:00:29 - INFO - __main__ -     recall = 0.7575571582016265
82.55user 28.22system 1:50.50elapsed 100%CPU (0avgtext+0avgdata 3990824maxresident)k
0inputs+928outputs (0major+1732242minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 02:00:32 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 02:00:32 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 02:00:32 - INFO - __main__ -   Seed = 2
11/28/2021 02:00:32 - INFO - root -   save model
11/28/2021 02:00:32 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 02:00:32 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 02:00:34 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 02:00:40 - INFO - __main__ -   Using lang2id = None
11/28/2021 02:00:40 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 02:00:40 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
11/28/2021 02:00:40 - INFO - root -   Trying to decide if add adapter
11/28/2021 02:00:40 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 02:00:40 - INFO - root -   loading lang adpater cs/wiki@ukp
11/28/2021 02:00:40 - INFO - __main__ -   Adapter Languages : ['cs'], Length : 1
11/28/2021 02:00:40 - INFO - __main__ -   Adapter Names ['cs/wiki@ukp'], Length : 1
11/28/2021 02:00:40 - INFO - __main__ -   Language = cs
11/28/2021 02:00:40 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/cs/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_cs_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/adapter_config.json
Adding adapter 'cs' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/head_config.json
11/28/2021 02:00:52 - INFO - __main__ -   Language adapter for fi not found, using cs instead
11/28/2021 02:00:52 - INFO - __main__ -   Set active language adapter to cs
11/28/2021 02:00:52 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 02:00:52 - INFO - __main__ -   Adapter Languages = ['cs']
11/28/2021 02:00:52 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_fi_bert-base-multilingual-cased_128
11/28/2021 02:00:53 - INFO - __main__ -   ***** Running evaluation  in fi *****
11/28/2021 02:00:53 - INFO - __main__ -     Num examples = 6550
11/28/2021 02:00:53 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/205 [00:00<?, ?it/s]11/28/2021 02:00:53 - INFO - __main__ -   Batch number = 1
Evaluating:   0%|          | 1/205 [00:00<01:04,  3.16it/s]11/28/2021 02:00:53 - INFO - __main__ -   Batch number = 2
Evaluating:   1%|          | 2/205 [00:00<01:03,  3.18it/s]11/28/2021 02:00:54 - INFO - __main__ -   Batch number = 3
Evaluating:   1%|▏         | 3/205 [00:00<01:04,  3.12it/s]11/28/2021 02:00:54 - INFO - __main__ -   Batch number = 4
Evaluating:   2%|▏         | 4/205 [00:01<01:04,  3.13it/s]11/28/2021 02:00:54 - INFO - __main__ -   Batch number = 5
Evaluating:   2%|▏         | 5/205 [00:01<01:03,  3.15it/s]11/28/2021 02:00:55 - INFO - __main__ -   Batch number = 6
Evaluating:   3%|▎         | 6/205 [00:01<01:02,  3.18it/s]11/28/2021 02:00:55 - INFO - __main__ -   Batch number = 7
Evaluating:   3%|▎         | 7/205 [00:02<01:02,  3.18it/s]11/28/2021 02:00:55 - INFO - __main__ -   Batch number = 8
Evaluating:   4%|▍         | 8/205 [00:02<01:02,  3.18it/s]11/28/2021 02:00:56 - INFO - __main__ -   Batch number = 9
Evaluating:   4%|▍         | 9/205 [00:02<01:00,  3.23it/s]11/28/2021 02:00:56 - INFO - __main__ -   Batch number = 10
Evaluating:   5%|▍         | 10/205 [00:03<01:00,  3.22it/s]11/28/2021 02:00:56 - INFO - __main__ -   Batch number = 11
Evaluating:   5%|▌         | 11/205 [00:03<01:00,  3.21it/s]11/28/2021 02:00:56 - INFO - __main__ -   Batch number = 12
Evaluating:   6%|▌         | 12/205 [00:03<01:00,  3.19it/s]11/28/2021 02:00:57 - INFO - __main__ -   Batch number = 13
Evaluating:   6%|▋         | 13/205 [00:04<01:01,  3.13it/s]11/28/2021 02:00:57 - INFO - __main__ -   Batch number = 14
Evaluating:   7%|▋         | 14/205 [00:04<01:01,  3.12it/s]11/28/2021 02:00:57 - INFO - __main__ -   Batch number = 15
Evaluating:   7%|▋         | 15/205 [00:04<01:00,  3.15it/s]11/28/2021 02:00:58 - INFO - __main__ -   Batch number = 16
Evaluating:   8%|▊         | 16/205 [00:05<00:59,  3.16it/s]11/28/2021 02:00:58 - INFO - __main__ -   Batch number = 17
Evaluating:   8%|▊         | 17/205 [00:05<00:54,  3.47it/s]11/28/2021 02:00:58 - INFO - __main__ -   Batch number = 18
Evaluating:   9%|▉         | 18/205 [00:05<00:55,  3.37it/s]11/28/2021 02:00:59 - INFO - __main__ -   Batch number = 19
Evaluating:   9%|▉         | 19/205 [00:05<00:57,  3.26it/s]11/28/2021 02:00:59 - INFO - __main__ -   Batch number = 20
Evaluating:  10%|▉         | 20/205 [00:06<00:57,  3.21it/s]11/28/2021 02:00:59 - INFO - __main__ -   Batch number = 21
Evaluating:  10%|█         | 21/205 [00:06<00:57,  3.21it/s]11/28/2021 02:01:00 - INFO - __main__ -   Batch number = 22
Evaluating:  11%|█         | 22/205 [00:06<00:57,  3.20it/s]11/28/2021 02:01:00 - INFO - __main__ -   Batch number = 23
Evaluating:  11%|█         | 23/205 [00:07<00:56,  3.20it/s]11/28/2021 02:01:00 - INFO - __main__ -   Batch number = 24
Evaluating:  12%|█▏        | 24/205 [00:07<00:56,  3.20it/s]11/28/2021 02:01:01 - INFO - __main__ -   Batch number = 25
Evaluating:  12%|█▏        | 25/205 [00:07<00:54,  3.29it/s]11/28/2021 02:01:01 - INFO - __main__ -   Batch number = 26
Evaluating:  13%|█▎        | 26/205 [00:08<00:54,  3.27it/s]11/28/2021 02:01:01 - INFO - __main__ -   Batch number = 27
Evaluating:  13%|█▎        | 27/205 [00:08<00:55,  3.23it/s]11/28/2021 02:01:01 - INFO - __main__ -   Batch number = 28
Evaluating:  14%|█▎        | 28/205 [00:08<00:56,  3.15it/s]11/28/2021 02:01:02 - INFO - __main__ -   Batch number = 29
Evaluating:  14%|█▍        | 29/205 [00:09<00:56,  3.13it/s]11/28/2021 02:01:02 - INFO - __main__ -   Batch number = 30
Evaluating:  15%|█▍        | 30/205 [00:09<01:01,  2.84it/s]11/28/2021 02:01:03 - INFO - __main__ -   Batch number = 31
Evaluating:  15%|█▌        | 31/205 [00:09<00:56,  3.07it/s]11/28/2021 02:01:03 - INFO - __main__ -   Batch number = 32
Evaluating:  16%|█▌        | 32/205 [00:10<00:55,  3.10it/s]11/28/2021 02:01:03 - INFO - __main__ -   Batch number = 33
Evaluating:  16%|█▌        | 33/205 [00:10<00:54,  3.13it/s]11/28/2021 02:01:03 - INFO - __main__ -   Batch number = 34
Evaluating:  17%|█▋        | 34/205 [00:10<00:52,  3.25it/s]11/28/2021 02:01:04 - INFO - __main__ -   Batch number = 35
Evaluating:  17%|█▋        | 35/205 [00:10<00:49,  3.46it/s]11/28/2021 02:01:04 - INFO - __main__ -   Batch number = 36
Evaluating:  18%|█▊        | 36/205 [00:11<00:49,  3.38it/s]11/28/2021 02:01:04 - INFO - __main__ -   Batch number = 37
Evaluating:  18%|█▊        | 37/205 [00:11<00:50,  3.32it/s]11/28/2021 02:01:05 - INFO - __main__ -   Batch number = 38
Evaluating:  19%|█▊        | 38/205 [00:11<00:50,  3.32it/s]11/28/2021 02:01:05 - INFO - __main__ -   Batch number = 39
Evaluating:  19%|█▉        | 39/205 [00:12<00:49,  3.36it/s]11/28/2021 02:01:05 - INFO - __main__ -   Batch number = 40
Evaluating:  20%|█▉        | 40/205 [00:12<00:50,  3.28it/s]11/28/2021 02:01:05 - INFO - __main__ -   Batch number = 41
Evaluating:  20%|██        | 41/205 [00:12<00:50,  3.23it/s]11/28/2021 02:01:06 - INFO - __main__ -   Batch number = 42
Evaluating:  20%|██        | 42/205 [00:13<00:48,  3.39it/s]11/28/2021 02:01:06 - INFO - __main__ -   Batch number = 43
Evaluating:  21%|██        | 43/205 [00:13<00:41,  3.86it/s]11/28/2021 02:01:06 - INFO - __main__ -   Batch number = 44
Evaluating:  21%|██▏       | 44/205 [00:13<00:37,  4.28it/s]11/28/2021 02:01:06 - INFO - __main__ -   Batch number = 45
Evaluating:  22%|██▏       | 45/205 [00:13<00:34,  4.61it/s]11/28/2021 02:01:07 - INFO - __main__ -   Batch number = 46
Evaluating:  22%|██▏       | 46/205 [00:13<00:32,  4.88it/s]11/28/2021 02:01:07 - INFO - __main__ -   Batch number = 47
Evaluating:  23%|██▎       | 47/205 [00:13<00:31,  5.09it/s]11/28/2021 02:01:07 - INFO - __main__ -   Batch number = 48
Evaluating:  23%|██▎       | 48/205 [00:14<00:38,  4.04it/s]11/28/2021 02:01:07 - INFO - __main__ -   Batch number = 49
Evaluating:  24%|██▍       | 49/205 [00:14<00:40,  3.81it/s]11/28/2021 02:01:08 - INFO - __main__ -   Batch number = 50
Evaluating:  24%|██▍       | 50/205 [00:14<00:43,  3.59it/s]11/28/2021 02:01:08 - INFO - __main__ -   Batch number = 51
Evaluating:  25%|██▍       | 51/205 [00:15<00:45,  3.40it/s]11/28/2021 02:01:08 - INFO - __main__ -   Batch number = 52
Evaluating:  25%|██▌       | 52/205 [00:15<00:46,  3.30it/s]11/28/2021 02:01:09 - INFO - __main__ -   Batch number = 53
Evaluating:  26%|██▌       | 53/205 [00:15<00:47,  3.20it/s]11/28/2021 02:01:09 - INFO - __main__ -   Batch number = 54
Evaluating:  26%|██▋       | 54/205 [00:16<00:47,  3.15it/s]11/28/2021 02:01:09 - INFO - __main__ -   Batch number = 55
Evaluating:  27%|██▋       | 55/205 [00:16<00:47,  3.14it/s]11/28/2021 02:01:10 - INFO - __main__ -   Batch number = 56
Evaluating:  27%|██▋       | 56/205 [00:17<00:55,  2.67it/s]11/28/2021 02:01:10 - INFO - __main__ -   Batch number = 57
Evaluating:  28%|██▊       | 57/205 [00:17<01:00,  2.43it/s]11/28/2021 02:01:11 - INFO - __main__ -   Batch number = 58
Evaluating:  28%|██▊       | 58/205 [00:18<01:04,  2.30it/s]11/28/2021 02:01:11 - INFO - __main__ -   Batch number = 59
Evaluating:  29%|██▉       | 59/205 [00:18<01:06,  2.19it/s]11/28/2021 02:01:12 - INFO - __main__ -   Batch number = 60
Evaluating:  29%|██▉       | 60/205 [00:19<01:08,  2.11it/s]11/28/2021 02:01:12 - INFO - __main__ -   Batch number = 61
Evaluating:  30%|██▉       | 61/205 [00:19<01:09,  2.08it/s]11/28/2021 02:01:13 - INFO - __main__ -   Batch number = 62
Evaluating:  30%|███       | 62/205 [00:20<01:09,  2.05it/s]11/28/2021 02:01:13 - INFO - __main__ -   Batch number = 63
Evaluating:  31%|███       | 63/205 [00:20<01:15,  1.88it/s]11/28/2021 02:01:14 - INFO - __main__ -   Batch number = 64
Evaluating:  31%|███       | 64/205 [00:21<01:20,  1.74it/s]11/28/2021 02:01:14 - INFO - __main__ -   Batch number = 65
Evaluating:  32%|███▏      | 65/205 [00:22<01:24,  1.66it/s]11/28/2021 02:01:15 - INFO - __main__ -   Batch number = 66
Evaluating:  32%|███▏      | 66/205 [00:22<01:26,  1.61it/s]11/28/2021 02:01:16 - INFO - __main__ -   Batch number = 67
Evaluating:  33%|███▎      | 67/205 [00:23<01:27,  1.58it/s]11/28/2021 02:01:16 - INFO - __main__ -   Batch number = 68
Evaluating:  33%|███▎      | 68/205 [00:24<01:27,  1.56it/s]11/28/2021 02:01:17 - INFO - __main__ -   Batch number = 69
Evaluating:  34%|███▎      | 69/205 [00:24<01:28,  1.53it/s]11/28/2021 02:01:18 - INFO - __main__ -   Batch number = 70
Evaluating:  34%|███▍      | 70/205 [00:25<01:28,  1.53it/s]11/28/2021 02:01:18 - INFO - __main__ -   Batch number = 71
Evaluating:  35%|███▍      | 71/205 [00:26<01:27,  1.53it/s]11/28/2021 02:01:19 - INFO - __main__ -   Batch number = 72
Evaluating:  35%|███▌      | 72/205 [00:26<01:27,  1.53it/s]11/28/2021 02:01:20 - INFO - __main__ -   Batch number = 73
Evaluating:  36%|███▌      | 73/205 [00:27<01:26,  1.52it/s]11/28/2021 02:01:20 - INFO - __main__ -   Batch number = 74
Evaluating:  36%|███▌      | 74/205 [00:27<01:26,  1.52it/s]11/28/2021 02:01:21 - INFO - __main__ -   Batch number = 75
Evaluating:  37%|███▋      | 75/205 [00:28<01:25,  1.52it/s]11/28/2021 02:01:22 - INFO - __main__ -   Batch number = 76
Evaluating:  37%|███▋      | 76/205 [00:29<01:24,  1.52it/s]11/28/2021 02:01:22 - INFO - __main__ -   Batch number = 77
Evaluating:  38%|███▊      | 77/205 [00:29<01:24,  1.52it/s]11/28/2021 02:01:23 - INFO - __main__ -   Batch number = 78
Evaluating:  38%|███▊      | 78/205 [00:30<01:23,  1.52it/s]11/28/2021 02:01:24 - INFO - __main__ -   Batch number = 79
Evaluating:  39%|███▊      | 79/205 [00:31<01:22,  1.53it/s]11/28/2021 02:01:24 - INFO - __main__ -   Batch number = 80
Evaluating:  39%|███▉      | 80/205 [00:31<01:21,  1.53it/s]11/28/2021 02:01:25 - INFO - __main__ -   Batch number = 81
Evaluating:  40%|███▉      | 81/205 [00:32<01:21,  1.51it/s]11/28/2021 02:01:26 - INFO - __main__ -   Batch number = 82
Evaluating:  40%|████      | 82/205 [00:33<01:21,  1.51it/s]11/28/2021 02:01:26 - INFO - __main__ -   Batch number = 83
Evaluating:  40%|████      | 83/205 [00:33<01:20,  1.51it/s]11/28/2021 02:01:27 - INFO - __main__ -   Batch number = 84
Evaluating:  41%|████      | 84/205 [00:34<01:27,  1.38it/s]11/28/2021 02:01:28 - INFO - __main__ -   Batch number = 85
Evaluating:  41%|████▏     | 85/205 [00:35<01:24,  1.42it/s]11/28/2021 02:01:28 - INFO - __main__ -   Batch number = 86
Evaluating:  42%|████▏     | 86/205 [00:36<01:22,  1.44it/s]11/28/2021 02:01:29 - INFO - __main__ -   Batch number = 87
Evaluating:  42%|████▏     | 87/205 [00:36<01:20,  1.46it/s]11/28/2021 02:01:30 - INFO - __main__ -   Batch number = 88
Evaluating:  43%|████▎     | 88/205 [00:37<01:19,  1.48it/s]11/28/2021 02:01:30 - INFO - __main__ -   Batch number = 89
Evaluating:  43%|████▎     | 89/205 [00:38<01:18,  1.49it/s]11/28/2021 02:01:31 - INFO - __main__ -   Batch number = 90
Evaluating:  44%|████▍     | 90/205 [00:38<01:16,  1.49it/s]11/28/2021 02:01:32 - INFO - __main__ -   Batch number = 91
Evaluating:  44%|████▍     | 91/205 [00:39<01:16,  1.50it/s]11/28/2021 02:01:32 - INFO - __main__ -   Batch number = 92
Evaluating:  45%|████▍     | 92/205 [00:40<01:13,  1.53it/s]11/28/2021 02:01:33 - INFO - __main__ -   Batch number = 93
Evaluating:  45%|████▌     | 93/205 [00:40<01:08,  1.64it/s]11/28/2021 02:01:34 - INFO - __main__ -   Batch number = 94
Evaluating:  46%|████▌     | 94/205 [00:41<01:04,  1.73it/s]11/28/2021 02:01:34 - INFO - __main__ -   Batch number = 95
Evaluating:  46%|████▋     | 95/205 [00:41<01:01,  1.78it/s]11/28/2021 02:01:35 - INFO - __main__ -   Batch number = 96
Evaluating:  47%|████▋     | 96/205 [00:42<00:59,  1.83it/s]11/28/2021 02:01:35 - INFO - __main__ -   Batch number = 97
Evaluating:  47%|████▋     | 97/205 [00:42<00:57,  1.87it/s]11/28/2021 02:01:36 - INFO - __main__ -   Batch number = 98
Evaluating:  48%|████▊     | 98/205 [00:43<00:56,  1.89it/s]11/28/2021 02:01:36 - INFO - __main__ -   Batch number = 99
Evaluating:  48%|████▊     | 99/205 [00:43<00:55,  1.92it/s]11/28/2021 02:01:37 - INFO - __main__ -   Batch number = 100
Evaluating:  49%|████▉     | 100/205 [00:44<00:53,  1.97it/s]11/28/2021 02:01:37 - INFO - __main__ -   Batch number = 101
Evaluating:  49%|████▉     | 101/205 [00:44<00:52,  1.97it/s]11/28/2021 02:01:38 - INFO - __main__ -   Batch number = 102
Evaluating:  50%|████▉     | 102/205 [00:45<00:51,  1.99it/s]11/28/2021 02:01:38 - INFO - __main__ -   Batch number = 103
Evaluating:  50%|█████     | 103/205 [00:45<00:51,  1.99it/s]11/28/2021 02:01:39 - INFO - __main__ -   Batch number = 104
Evaluating:  51%|█████     | 104/205 [00:46<00:50,  2.00it/s]11/28/2021 02:01:39 - INFO - __main__ -   Batch number = 105
Evaluating:  51%|█████     | 105/205 [00:46<00:50,  1.99it/s]11/28/2021 02:01:40 - INFO - __main__ -   Batch number = 106
Evaluating:  52%|█████▏    | 106/205 [00:47<00:49,  1.99it/s]11/28/2021 02:01:40 - INFO - __main__ -   Batch number = 107
Evaluating:  52%|█████▏    | 107/205 [00:47<00:49,  1.99it/s]11/28/2021 02:01:41 - INFO - __main__ -   Batch number = 108
Evaluating:  53%|█████▎    | 108/205 [00:48<00:49,  1.97it/s]11/28/2021 02:01:41 - INFO - __main__ -   Batch number = 109
Evaluating:  53%|█████▎    | 109/205 [00:48<00:48,  1.97it/s]11/28/2021 02:01:42 - INFO - __main__ -   Batch number = 110
Evaluating:  54%|█████▎    | 110/205 [00:49<00:48,  1.98it/s]11/28/2021 02:01:42 - INFO - __main__ -   Batch number = 111
Evaluating:  54%|█████▍    | 111/205 [00:49<00:43,  2.14it/s]11/28/2021 02:01:43 - INFO - __main__ -   Batch number = 112
Evaluating:  55%|█████▍    | 112/205 [00:49<00:40,  2.32it/s]11/28/2021 02:01:43 - INFO - __main__ -   Batch number = 113
Evaluating:  55%|█████▌    | 113/205 [00:50<00:37,  2.47it/s]11/28/2021 02:01:43 - INFO - __main__ -   Batch number = 114
Evaluating:  56%|█████▌    | 114/205 [00:50<00:35,  2.60it/s]11/28/2021 02:01:44 - INFO - __main__ -   Batch number = 115
Evaluating:  56%|█████▌    | 115/205 [00:50<00:32,  2.73it/s]11/28/2021 02:01:44 - INFO - __main__ -   Batch number = 116
Evaluating:  57%|█████▋    | 116/205 [00:51<00:31,  2.78it/s]11/28/2021 02:01:44 - INFO - __main__ -   Batch number = 117
Evaluating:  57%|█████▋    | 117/205 [00:51<00:31,  2.81it/s]11/28/2021 02:01:45 - INFO - __main__ -   Batch number = 118
Evaluating:  58%|█████▊    | 118/205 [00:51<00:30,  2.85it/s]11/28/2021 02:01:45 - INFO - __main__ -   Batch number = 119
Evaluating:  58%|█████▊    | 119/205 [00:52<00:29,  2.87it/s]11/28/2021 02:01:45 - INFO - __main__ -   Batch number = 120
Evaluating:  59%|█████▊    | 120/205 [00:52<00:29,  2.89it/s]11/28/2021 02:01:46 - INFO - __main__ -   Batch number = 121
Evaluating:  59%|█████▉    | 121/205 [00:52<00:28,  2.91it/s]11/28/2021 02:01:46 - INFO - __main__ -   Batch number = 122
Evaluating:  60%|█████▉    | 122/205 [00:53<00:28,  2.91it/s]11/28/2021 02:01:46 - INFO - __main__ -   Batch number = 123
Evaluating:  60%|██████    | 123/205 [00:53<00:27,  2.93it/s]11/28/2021 02:01:47 - INFO - __main__ -   Batch number = 124
Evaluating:  60%|██████    | 124/205 [00:53<00:27,  2.94it/s]11/28/2021 02:01:47 - INFO - __main__ -   Batch number = 125
Evaluating:  61%|██████    | 125/205 [00:54<00:29,  2.70it/s]11/28/2021 02:01:47 - INFO - __main__ -   Batch number = 126
Evaluating:  61%|██████▏   | 126/205 [00:54<00:28,  2.77it/s]11/28/2021 02:01:48 - INFO - __main__ -   Batch number = 127
Evaluating:  62%|██████▏   | 127/205 [00:55<00:28,  2.78it/s]11/28/2021 02:01:48 - INFO - __main__ -   Batch number = 128
Evaluating:  62%|██████▏   | 128/205 [00:55<00:27,  2.81it/s]11/28/2021 02:01:48 - INFO - __main__ -   Batch number = 129
Evaluating:  63%|██████▎   | 129/205 [00:55<00:26,  2.85it/s]11/28/2021 02:01:49 - INFO - __main__ -   Batch number = 130
Evaluating:  63%|██████▎   | 130/205 [00:56<00:26,  2.87it/s]11/28/2021 02:01:49 - INFO - __main__ -   Batch number = 131
Evaluating:  64%|██████▍   | 131/205 [00:56<00:25,  2.88it/s]11/28/2021 02:01:49 - INFO - __main__ -   Batch number = 132
Evaluating:  64%|██████▍   | 132/205 [00:56<00:25,  2.89it/s]11/28/2021 02:01:50 - INFO - __main__ -   Batch number = 133
Evaluating:  65%|██████▍   | 133/205 [00:57<00:24,  2.89it/s]11/28/2021 02:01:50 - INFO - __main__ -   Batch number = 134
Evaluating:  65%|██████▌   | 134/205 [00:57<00:24,  2.89it/s]11/28/2021 02:01:51 - INFO - __main__ -   Batch number = 135
Evaluating:  66%|██████▌   | 135/205 [00:57<00:24,  2.91it/s]11/28/2021 02:01:51 - INFO - __main__ -   Batch number = 136
Evaluating:  66%|██████▋   | 136/205 [00:58<00:23,  2.92it/s]11/28/2021 02:01:51 - INFO - __main__ -   Batch number = 137
Evaluating:  67%|██████▋   | 137/205 [00:58<00:23,  2.92it/s]11/28/2021 02:01:52 - INFO - __main__ -   Batch number = 138
Evaluating:  67%|██████▋   | 138/205 [00:58<00:22,  2.92it/s]11/28/2021 02:01:52 - INFO - __main__ -   Batch number = 139
Evaluating:  68%|██████▊   | 139/205 [00:59<00:22,  2.90it/s]11/28/2021 02:01:52 - INFO - __main__ -   Batch number = 140
Evaluating:  68%|██████▊   | 140/205 [00:59<00:22,  2.89it/s]11/28/2021 02:01:53 - INFO - __main__ -   Batch number = 141
Evaluating:  69%|██████▉   | 141/205 [00:59<00:22,  2.88it/s]11/28/2021 02:01:53 - INFO - __main__ -   Batch number = 142
Evaluating:  69%|██████▉   | 142/205 [01:00<00:21,  2.89it/s]11/28/2021 02:01:53 - INFO - __main__ -   Batch number = 143
Evaluating:  70%|██████▉   | 143/205 [01:00<00:21,  2.90it/s]11/28/2021 02:01:54 - INFO - __main__ -   Batch number = 144
Evaluating:  70%|███████   | 144/205 [01:00<00:19,  3.10it/s]11/28/2021 02:01:54 - INFO - __main__ -   Batch number = 145
Evaluating:  71%|███████   | 145/205 [01:01<00:16,  3.55it/s]11/28/2021 02:01:54 - INFO - __main__ -   Batch number = 146
Evaluating:  71%|███████   | 146/205 [01:01<00:14,  4.00it/s]11/28/2021 02:01:54 - INFO - __main__ -   Batch number = 147
Evaluating:  72%|███████▏  | 147/205 [01:01<00:13,  4.44it/s]11/28/2021 02:01:54 - INFO - __main__ -   Batch number = 148
Evaluating:  72%|███████▏  | 148/205 [01:01<00:11,  4.79it/s]11/28/2021 02:01:55 - INFO - __main__ -   Batch number = 149
Evaluating:  73%|███████▎  | 149/205 [01:01<00:10,  5.10it/s]11/28/2021 02:01:55 - INFO - __main__ -   Batch number = 150
Evaluating:  73%|███████▎  | 150/205 [01:01<00:10,  5.21it/s]11/28/2021 02:01:55 - INFO - __main__ -   Batch number = 151
Evaluating:  74%|███████▎  | 151/205 [01:02<00:10,  5.27it/s]11/28/2021 02:01:55 - INFO - __main__ -   Batch number = 152
Evaluating:  74%|███████▍  | 152/205 [01:02<00:09,  5.35it/s]11/28/2021 02:01:55 - INFO - __main__ -   Batch number = 153
Evaluating:  75%|███████▍  | 153/205 [01:02<00:09,  5.52it/s]11/28/2021 02:01:55 - INFO - __main__ -   Batch number = 154
Evaluating:  75%|███████▌  | 154/205 [01:02<00:09,  5.61it/s]11/28/2021 02:01:56 - INFO - __main__ -   Batch number = 155
Evaluating:  76%|███████▌  | 155/205 [01:02<00:08,  5.69it/s]11/28/2021 02:01:56 - INFO - __main__ -   Batch number = 156
Evaluating:  76%|███████▌  | 156/205 [01:02<00:08,  5.77it/s]11/28/2021 02:01:56 - INFO - __main__ -   Batch number = 157
Evaluating:  77%|███████▋  | 157/205 [01:03<00:08,  5.80it/s]11/28/2021 02:01:56 - INFO - __main__ -   Batch number = 158
Evaluating:  77%|███████▋  | 158/205 [01:03<00:08,  5.82it/s]11/28/2021 02:01:56 - INFO - __main__ -   Batch number = 159
Evaluating:  78%|███████▊  | 159/205 [01:03<00:07,  5.91it/s]11/28/2021 02:01:56 - INFO - __main__ -   Batch number = 160
Evaluating:  78%|███████▊  | 160/205 [01:03<00:07,  5.87it/s]11/28/2021 02:01:57 - INFO - __main__ -   Batch number = 161
Evaluating:  79%|███████▊  | 161/205 [01:03<00:07,  5.92it/s]11/28/2021 02:01:57 - INFO - __main__ -   Batch number = 162
Evaluating:  79%|███████▉  | 162/205 [01:03<00:07,  5.84it/s]11/28/2021 02:01:57 - INFO - __main__ -   Batch number = 163
Evaluating:  80%|███████▉  | 163/205 [01:04<00:08,  5.24it/s]11/28/2021 02:01:57 - INFO - __main__ -   Batch number = 164
Evaluating:  80%|████████  | 164/205 [01:04<00:07,  5.21it/s]11/28/2021 02:01:57 - INFO - __main__ -   Batch number = 165
Evaluating:  80%|████████  | 165/205 [01:04<00:07,  5.25it/s]11/28/2021 02:01:58 - INFO - __main__ -   Batch number = 166
Evaluating:  81%|████████  | 166/205 [01:04<00:07,  5.30it/s]11/28/2021 02:01:58 - INFO - __main__ -   Batch number = 167
Evaluating:  81%|████████▏ | 167/205 [01:04<00:07,  5.36it/s]11/28/2021 02:01:58 - INFO - __main__ -   Batch number = 168
Evaluating:  82%|████████▏ | 168/205 [01:05<00:06,  5.46it/s]11/28/2021 02:01:58 - INFO - __main__ -   Batch number = 169
Evaluating:  82%|████████▏ | 169/205 [01:05<00:06,  5.54it/s]11/28/2021 02:01:58 - INFO - __main__ -   Batch number = 170
Evaluating:  83%|████████▎ | 170/205 [01:05<00:06,  5.59it/s]11/28/2021 02:01:58 - INFO - __main__ -   Batch number = 171
Evaluating:  83%|████████▎ | 171/205 [01:05<00:07,  4.47it/s]11/28/2021 02:01:59 - INFO - __main__ -   Batch number = 172
Evaluating:  84%|████████▍ | 172/205 [01:06<00:08,  3.91it/s]11/28/2021 02:01:59 - INFO - __main__ -   Batch number = 173
Evaluating:  84%|████████▍ | 173/205 [01:06<00:08,  3.59it/s]11/28/2021 02:01:59 - INFO - __main__ -   Batch number = 174
Evaluating:  85%|████████▍ | 174/205 [01:06<00:09,  3.37it/s]11/28/2021 02:02:00 - INFO - __main__ -   Batch number = 175
Evaluating:  85%|████████▌ | 175/205 [01:07<00:09,  3.23it/s]11/28/2021 02:02:00 - INFO - __main__ -   Batch number = 176
Evaluating:  86%|████████▌ | 176/205 [01:07<00:09,  3.13it/s]11/28/2021 02:02:01 - INFO - __main__ -   Batch number = 177
Evaluating:  86%|████████▋ | 177/205 [01:07<00:09,  3.01it/s]11/28/2021 02:02:01 - INFO - __main__ -   Batch number = 178
Evaluating:  87%|████████▋ | 178/205 [01:08<00:09,  2.96it/s]11/28/2021 02:02:01 - INFO - __main__ -   Batch number = 179
Evaluating:  87%|████████▋ | 179/205 [01:08<00:08,  2.91it/s]11/28/2021 02:02:02 - INFO - __main__ -   Batch number = 180
Evaluating:  88%|████████▊ | 180/205 [01:08<00:08,  2.87it/s]11/28/2021 02:02:02 - INFO - __main__ -   Batch number = 181
Evaluating:  88%|████████▊ | 181/205 [01:09<00:08,  2.84it/s]11/28/2021 02:02:02 - INFO - __main__ -   Batch number = 182
Evaluating:  89%|████████▉ | 182/205 [01:09<00:08,  2.83it/s]11/28/2021 02:02:03 - INFO - __main__ -   Batch number = 183
Evaluating:  89%|████████▉ | 183/205 [01:09<00:07,  2.82it/s]11/28/2021 02:02:03 - INFO - __main__ -   Batch number = 184
Evaluating:  90%|████████▉ | 184/205 [01:10<00:07,  2.81it/s]11/28/2021 02:02:03 - INFO - __main__ -   Batch number = 185
Evaluating:  90%|█████████ | 185/205 [01:10<00:07,  2.81it/s]11/28/2021 02:02:04 - INFO - __main__ -   Batch number = 186
Evaluating:  91%|█████████ | 186/205 [01:11<00:06,  2.80it/s]11/28/2021 02:02:04 - INFO - __main__ -   Batch number = 187
Evaluating:  91%|█████████ | 187/205 [01:11<00:06,  2.79it/s]11/28/2021 02:02:04 - INFO - __main__ -   Batch number = 188
Evaluating:  92%|█████████▏| 188/205 [01:11<00:06,  2.79it/s]11/28/2021 02:02:05 - INFO - __main__ -   Batch number = 189
Evaluating:  92%|█████████▏| 189/205 [01:12<00:06,  2.48it/s]11/28/2021 02:02:05 - INFO - __main__ -   Batch number = 190
Evaluating:  93%|█████████▎| 190/205 [01:12<00:06,  2.28it/s]11/28/2021 02:02:06 - INFO - __main__ -   Batch number = 191
Evaluating:  93%|█████████▎| 191/205 [01:13<00:06,  2.16it/s]11/28/2021 02:02:06 - INFO - __main__ -   Batch number = 192
Evaluating:  94%|█████████▎| 192/205 [01:13<00:06,  2.09it/s]11/28/2021 02:02:07 - INFO - __main__ -   Batch number = 193
Evaluating:  94%|█████████▍| 193/205 [01:14<00:05,  2.03it/s]11/28/2021 02:02:07 - INFO - __main__ -   Batch number = 194
Evaluating:  95%|█████████▍| 194/205 [01:14<00:05,  2.01it/s]11/28/2021 02:02:08 - INFO - __main__ -   Batch number = 195
Evaluating:  95%|█████████▌| 195/205 [01:15<00:05,  1.98it/s]11/28/2021 02:02:08 - INFO - __main__ -   Batch number = 196
Evaluating:  96%|█████████▌| 196/205 [01:15<00:04,  1.96it/s]11/28/2021 02:02:09 - INFO - __main__ -   Batch number = 197
Evaluating:  96%|█████████▌| 197/205 [01:16<00:04,  1.95it/s]11/28/2021 02:02:09 - INFO - __main__ -   Batch number = 198
Evaluating:  97%|█████████▋| 198/205 [01:16<00:03,  1.93it/s]11/28/2021 02:02:10 - INFO - __main__ -   Batch number = 199
Evaluating:  97%|█████████▋| 199/205 [01:17<00:03,  1.92it/s]11/28/2021 02:02:11 - INFO - __main__ -   Batch number = 200
Evaluating:  98%|█████████▊| 200/205 [01:18<00:02,  1.92it/s]11/28/2021 02:02:11 - INFO - __main__ -   Batch number = 201
Evaluating:  98%|█████████▊| 201/205 [01:18<00:02,  1.91it/s]11/28/2021 02:02:12 - INFO - __main__ -   Batch number = 202
Evaluating:  99%|█████████▊| 202/205 [01:19<00:01,  1.93it/s]11/28/2021 02:02:12 - INFO - __main__ -   Batch number = 203
Evaluating:  99%|█████████▉| 203/205 [01:19<00:01,  1.92it/s]11/28/2021 02:02:13 - INFO - __main__ -   Batch number = 204
Evaluating: 100%|█████████▉| 204/205 [01:20<00:00,  1.92it/s]11/28/2021 02:02:13 - INFO - __main__ -   Batch number = 205
Evaluating: 100%|██████████| 205/205 [01:20<00:00,  2.05it/s]Evaluating: 100%|██████████| 205/205 [01:20<00:00,  2.55it/s]
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 02:02:15 - INFO - __main__ -   ***** Evaluation result  in fi *****
11/28/2021 02:02:15 - INFO - __main__ -     f1 = 0.7601786264243919
11/28/2021 02:02:15 - INFO - __main__ -     loss = 0.9041281466077014
11/28/2021 02:02:15 - INFO - __main__ -     precision = 0.7628805439653841
11/28/2021 02:02:15 - INFO - __main__ -     recall = 0.757495780266994
79.14user 28.99system 1:46.50elapsed 101%CPU (0avgtext+0avgdata 3985768maxresident)k
8inputs+920outputs (0major+1692137minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 02:02:18 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 02:02:18 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 02:02:18 - INFO - __main__ -   Seed = 3
11/28/2021 02:02:18 - INFO - root -   save model
11/28/2021 02:02:18 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fi', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 02:02:18 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 02:02:21 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 02:02:27 - INFO - __main__ -   Using lang2id = None
11/28/2021 02:02:27 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 02:02:27 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
11/28/2021 02:02:27 - INFO - root -   Trying to decide if add adapter
11/28/2021 02:02:27 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 02:02:27 - INFO - root -   loading lang adpater cs/wiki@ukp
11/28/2021 02:02:27 - INFO - __main__ -   Adapter Languages : ['cs'], Length : 1
11/28/2021 02:02:27 - INFO - __main__ -   Adapter Names ['cs/wiki@ukp'], Length : 1
11/28/2021 02:02:27 - INFO - __main__ -   Language = cs
11/28/2021 02:02:27 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/cs/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_cs_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/adapter_config.json
Adding adapter 'cs' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/head_config.json
11/28/2021 02:02:39 - INFO - __main__ -   Language adapter for fi not found, using cs instead
11/28/2021 02:02:39 - INFO - __main__ -   Set active language adapter to cs
11/28/2021 02:02:39 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 02:02:39 - INFO - __main__ -   Adapter Languages = ['cs']
11/28/2021 02:02:39 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_fi_bert-base-multilingual-cased_128
11/28/2021 02:02:40 - INFO - __main__ -   ***** Running evaluation  in fi *****
11/28/2021 02:02:40 - INFO - __main__ -     Num examples = 6550
11/28/2021 02:02:40 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/205 [00:00<?, ?it/s]11/28/2021 02:02:40 - INFO - __main__ -   Batch number = 1
Evaluating:   0%|          | 1/205 [00:00<01:43,  1.98it/s]11/28/2021 02:02:41 - INFO - __main__ -   Batch number = 2
Evaluating:   1%|          | 2/205 [00:00<01:41,  2.01it/s]11/28/2021 02:02:41 - INFO - __main__ -   Batch number = 3
Evaluating:   1%|▏         | 3/205 [00:01<01:40,  2.01it/s]11/28/2021 02:02:41 - INFO - __main__ -   Batch number = 4
Evaluating:   2%|▏         | 4/205 [00:01<01:39,  2.01it/s]11/28/2021 02:02:42 - INFO - __main__ -   Batch number = 5
Evaluating:   2%|▏         | 5/205 [00:02<01:39,  2.01it/s]11/28/2021 02:02:42 - INFO - __main__ -   Batch number = 6
Evaluating:   3%|▎         | 6/205 [00:02<01:39,  2.00it/s]11/28/2021 02:02:43 - INFO - __main__ -   Batch number = 7
Evaluating:   3%|▎         | 7/205 [00:03<01:38,  2.01it/s]11/28/2021 02:02:43 - INFO - __main__ -   Batch number = 8
Evaluating:   4%|▍         | 8/205 [00:03<01:38,  2.01it/s]11/28/2021 02:02:44 - INFO - __main__ -   Batch number = 9
Evaluating:   4%|▍         | 9/205 [00:04<01:39,  1.98it/s]11/28/2021 02:02:45 - INFO - __main__ -   Batch number = 10
Evaluating:   5%|▍         | 10/205 [00:05<01:38,  1.98it/s]11/28/2021 02:02:45 - INFO - __main__ -   Batch number = 11
Evaluating:   5%|▌         | 11/205 [00:05<01:37,  1.99it/s]11/28/2021 02:02:46 - INFO - __main__ -   Batch number = 12
Evaluating:   6%|▌         | 12/205 [00:06<01:36,  2.00it/s]11/28/2021 02:02:46 - INFO - __main__ -   Batch number = 13
Evaluating:   6%|▋         | 13/205 [00:06<01:35,  2.01it/s]11/28/2021 02:02:46 - INFO - __main__ -   Batch number = 14
Evaluating:   7%|▋         | 14/205 [00:06<01:34,  2.02it/s]11/28/2021 02:02:47 - INFO - __main__ -   Batch number = 15
Evaluating:   7%|▋         | 15/205 [00:07<01:34,  2.00it/s]11/28/2021 02:02:47 - INFO - __main__ -   Batch number = 16
Evaluating:   8%|▊         | 16/205 [00:07<01:34,  2.00it/s]11/28/2021 02:02:48 - INFO - __main__ -   Batch number = 17
Evaluating:   8%|▊         | 17/205 [00:08<01:33,  2.01it/s]11/28/2021 02:02:48 - INFO - __main__ -   Batch number = 18
Evaluating:   9%|▉         | 18/205 [00:08<01:32,  2.01it/s]11/28/2021 02:02:49 - INFO - __main__ -   Batch number = 19
Evaluating:   9%|▉         | 19/205 [00:09<01:32,  2.02it/s]11/28/2021 02:02:49 - INFO - __main__ -   Batch number = 20
Evaluating:  10%|▉         | 20/205 [00:09<01:31,  2.03it/s]11/28/2021 02:02:50 - INFO - __main__ -   Batch number = 21
Evaluating:  10%|█         | 21/205 [00:10<01:30,  2.03it/s]11/28/2021 02:02:50 - INFO - __main__ -   Batch number = 22
Evaluating:  11%|█         | 22/205 [00:10<01:30,  2.02it/s]11/28/2021 02:02:51 - INFO - __main__ -   Batch number = 23
Evaluating:  11%|█         | 23/205 [00:11<01:30,  2.01it/s]11/28/2021 02:02:51 - INFO - __main__ -   Batch number = 24
Evaluating:  12%|█▏        | 24/205 [00:11<01:30,  2.01it/s]11/28/2021 02:02:52 - INFO - __main__ -   Batch number = 25
Evaluating:  12%|█▏        | 25/205 [00:12<01:28,  2.03it/s]11/28/2021 02:02:52 - INFO - __main__ -   Batch number = 26
Evaluating:  13%|█▎        | 26/205 [00:12<01:28,  2.02it/s]11/28/2021 02:02:53 - INFO - __main__ -   Batch number = 27
Evaluating:  13%|█▎        | 27/205 [00:13<01:28,  2.02it/s]11/28/2021 02:02:53 - INFO - __main__ -   Batch number = 28
Evaluating:  14%|█▎        | 28/205 [00:13<01:27,  2.02it/s]11/28/2021 02:02:54 - INFO - __main__ -   Batch number = 29
Evaluating:  14%|█▍        | 29/205 [00:14<01:27,  2.02it/s]11/28/2021 02:02:54 - INFO - __main__ -   Batch number = 30
Evaluating:  15%|█▍        | 30/205 [00:14<01:25,  2.05it/s]11/28/2021 02:02:55 - INFO - __main__ -   Batch number = 31
Evaluating:  15%|█▌        | 31/205 [00:15<01:16,  2.28it/s]11/28/2021 02:02:55 - INFO - __main__ -   Batch number = 32
Evaluating:  16%|█▌        | 32/205 [00:15<01:10,  2.46it/s]11/28/2021 02:02:56 - INFO - __main__ -   Batch number = 33
Evaluating:  16%|█▌        | 33/205 [00:15<01:05,  2.63it/s]11/28/2021 02:02:56 - INFO - __main__ -   Batch number = 34
Evaluating:  17%|█▋        | 34/205 [00:16<01:02,  2.74it/s]11/28/2021 02:02:56 - INFO - __main__ -   Batch number = 35
Evaluating:  17%|█▋        | 35/205 [00:16<01:00,  2.81it/s]11/28/2021 02:02:57 - INFO - __main__ -   Batch number = 36
Evaluating:  18%|█▊        | 36/205 [00:16<00:58,  2.89it/s]11/28/2021 02:02:57 - INFO - __main__ -   Batch number = 37
Evaluating:  18%|█▊        | 37/205 [00:17<00:56,  2.98it/s]11/28/2021 02:02:57 - INFO - __main__ -   Batch number = 38
Evaluating:  19%|█▊        | 38/205 [00:17<00:55,  3.01it/s]11/28/2021 02:02:57 - INFO - __main__ -   Batch number = 39
Evaluating:  19%|█▉        | 39/205 [00:17<00:55,  3.02it/s]11/28/2021 02:02:58 - INFO - __main__ -   Batch number = 40
Evaluating:  20%|█▉        | 40/205 [00:18<00:54,  3.00it/s]11/28/2021 02:02:58 - INFO - __main__ -   Batch number = 41
Evaluating:  20%|██        | 41/205 [00:18<00:54,  3.00it/s]11/28/2021 02:02:58 - INFO - __main__ -   Batch number = 42
Evaluating:  20%|██        | 42/205 [00:18<00:54,  2.99it/s]11/28/2021 02:02:59 - INFO - __main__ -   Batch number = 43
Evaluating:  21%|██        | 43/205 [00:19<00:54,  2.98it/s]11/28/2021 02:02:59 - INFO - __main__ -   Batch number = 44
Evaluating:  21%|██▏       | 44/205 [00:19<00:53,  2.99it/s]11/28/2021 02:02:59 - INFO - __main__ -   Batch number = 45
Evaluating:  22%|██▏       | 45/205 [00:19<00:53,  2.98it/s]11/28/2021 02:03:00 - INFO - __main__ -   Batch number = 46
Evaluating:  22%|██▏       | 46/205 [00:20<00:53,  2.97it/s]11/28/2021 02:03:00 - INFO - __main__ -   Batch number = 47
Evaluating:  23%|██▎       | 47/205 [00:20<00:53,  2.98it/s]11/28/2021 02:03:01 - INFO - __main__ -   Batch number = 48
Evaluating:  23%|██▎       | 48/205 [00:20<00:54,  2.88it/s]11/28/2021 02:03:01 - INFO - __main__ -   Batch number = 49
Evaluating:  24%|██▍       | 49/205 [00:21<01:01,  2.53it/s]11/28/2021 02:03:01 - INFO - __main__ -   Batch number = 50
Evaluating:  24%|██▍       | 50/205 [00:21<01:06,  2.34it/s]11/28/2021 02:03:02 - INFO - __main__ -   Batch number = 51
Evaluating:  25%|██▍       | 51/205 [00:22<01:10,  2.20it/s]11/28/2021 02:03:02 - INFO - __main__ -   Batch number = 52
Evaluating:  25%|██▌       | 52/205 [00:22<01:12,  2.12it/s]11/28/2021 02:03:03 - INFO - __main__ -   Batch number = 53
Evaluating:  26%|██▌       | 53/205 [00:23<01:13,  2.07it/s]11/28/2021 02:03:03 - INFO - __main__ -   Batch number = 54
Evaluating:  26%|██▋       | 54/205 [00:23<01:14,  2.04it/s]11/28/2021 02:03:04 - INFO - __main__ -   Batch number = 55
Evaluating:  27%|██▋       | 55/205 [00:24<01:13,  2.03it/s]11/28/2021 02:03:04 - INFO - __main__ -   Batch number = 56
Evaluating:  27%|██▋       | 56/205 [00:24<01:14,  2.01it/s]11/28/2021 02:03:05 - INFO - __main__ -   Batch number = 57
Evaluating:  28%|██▊       | 57/205 [00:25<01:13,  2.00it/s]11/28/2021 02:03:05 - INFO - __main__ -   Batch number = 58
Evaluating:  28%|██▊       | 58/205 [00:25<01:13,  1.99it/s]11/28/2021 02:03:06 - INFO - __main__ -   Batch number = 59
Evaluating:  29%|██▉       | 59/205 [00:26<01:13,  1.99it/s]11/28/2021 02:03:06 - INFO - __main__ -   Batch number = 60
Evaluating:  29%|██▉       | 60/205 [00:26<01:12,  2.00it/s]11/28/2021 02:03:07 - INFO - __main__ -   Batch number = 61
Evaluating:  30%|██▉       | 61/205 [00:27<01:08,  2.09it/s]11/28/2021 02:03:07 - INFO - __main__ -   Batch number = 62
Evaluating:  30%|███       | 62/205 [00:27<01:09,  2.07it/s]11/28/2021 02:03:08 - INFO - __main__ -   Batch number = 63
Evaluating:  31%|███       | 63/205 [00:28<01:09,  2.04it/s]11/28/2021 02:03:08 - INFO - __main__ -   Batch number = 64
Evaluating:  31%|███       | 64/205 [00:28<01:09,  2.03it/s]11/28/2021 02:03:09 - INFO - __main__ -   Batch number = 65
Evaluating:  32%|███▏      | 65/205 [00:29<01:09,  2.00it/s]11/28/2021 02:03:09 - INFO - __main__ -   Batch number = 66
Evaluating:  32%|███▏      | 66/205 [00:29<01:09,  2.00it/s]11/28/2021 02:03:10 - INFO - __main__ -   Batch number = 67
Evaluating:  33%|███▎      | 67/205 [00:30<01:09,  2.00it/s]11/28/2021 02:03:10 - INFO - __main__ -   Batch number = 68
Evaluating:  33%|███▎      | 68/205 [00:30<01:08,  1.99it/s]11/28/2021 02:03:11 - INFO - __main__ -   Batch number = 69
Evaluating:  34%|███▎      | 69/205 [00:31<01:08,  1.99it/s]11/28/2021 02:03:11 - INFO - __main__ -   Batch number = 70
Evaluating:  34%|███▍      | 70/205 [00:31<01:07,  1.99it/s]11/28/2021 02:03:12 - INFO - __main__ -   Batch number = 71
Evaluating:  35%|███▍      | 71/205 [00:32<01:07,  1.99it/s]11/28/2021 02:03:12 - INFO - __main__ -   Batch number = 72
Evaluating:  35%|███▌      | 72/205 [00:32<01:06,  1.99it/s]11/28/2021 02:03:13 - INFO - __main__ -   Batch number = 73
Evaluating:  36%|███▌      | 73/205 [00:33<01:06,  1.97it/s]11/28/2021 02:03:13 - INFO - __main__ -   Batch number = 74
Evaluating:  36%|███▌      | 74/205 [00:33<01:06,  1.98it/s]11/28/2021 02:03:14 - INFO - __main__ -   Batch number = 75
Evaluating:  37%|███▋      | 75/205 [00:34<01:05,  1.98it/s]11/28/2021 02:03:14 - INFO - __main__ -   Batch number = 76
Evaluating:  37%|███▋      | 76/205 [00:34<01:05,  1.97it/s]11/28/2021 02:03:15 - INFO - __main__ -   Batch number = 77
Evaluating:  38%|███▊      | 77/205 [00:35<01:05,  1.97it/s]11/28/2021 02:03:15 - INFO - __main__ -   Batch number = 78
Evaluating:  38%|███▊      | 78/205 [00:35<01:04,  1.97it/s]11/28/2021 02:03:16 - INFO - __main__ -   Batch number = 79
Evaluating:  39%|███▊      | 79/205 [00:36<01:03,  1.97it/s]11/28/2021 02:03:16 - INFO - __main__ -   Batch number = 80
Evaluating:  39%|███▉      | 80/205 [00:36<01:03,  1.98it/s]11/28/2021 02:03:17 - INFO - __main__ -   Batch number = 81
Evaluating:  40%|███▉      | 81/205 [00:37<01:00,  2.03it/s]11/28/2021 02:03:17 - INFO - __main__ -   Batch number = 82
Evaluating:  40%|████      | 82/205 [00:37<01:01,  2.01it/s]11/28/2021 02:03:18 - INFO - __main__ -   Batch number = 83
Evaluating:  40%|████      | 83/205 [00:38<01:00,  2.02it/s]11/28/2021 02:03:18 - INFO - __main__ -   Batch number = 84
Evaluating:  41%|████      | 84/205 [00:38<01:00,  2.01it/s]11/28/2021 02:03:19 - INFO - __main__ -   Batch number = 85
Evaluating:  41%|████▏     | 85/205 [00:39<00:59,  2.02it/s]11/28/2021 02:03:19 - INFO - __main__ -   Batch number = 86
Evaluating:  42%|████▏     | 86/205 [00:39<00:59,  2.01it/s]11/28/2021 02:03:20 - INFO - __main__ -   Batch number = 87
Evaluating:  42%|████▏     | 87/205 [00:40<00:58,  2.03it/s]11/28/2021 02:03:20 - INFO - __main__ -   Batch number = 88
Evaluating:  43%|████▎     | 88/205 [00:40<00:58,  2.02it/s]11/28/2021 02:03:21 - INFO - __main__ -   Batch number = 89
Evaluating:  43%|████▎     | 89/205 [00:41<00:57,  2.02it/s]11/28/2021 02:03:21 - INFO - __main__ -   Batch number = 90
Evaluating:  44%|████▍     | 90/205 [00:41<00:57,  2.02it/s]11/28/2021 02:03:22 - INFO - __main__ -   Batch number = 91
Evaluating:  44%|████▍     | 91/205 [00:42<00:59,  1.93it/s]11/28/2021 02:03:22 - INFO - __main__ -   Batch number = 92
Evaluating:  45%|████▍     | 92/205 [00:43<01:03,  1.77it/s]11/28/2021 02:03:23 - INFO - __main__ -   Batch number = 93
Evaluating:  45%|████▌     | 93/205 [00:43<01:06,  1.68it/s]11/28/2021 02:03:24 - INFO - __main__ -   Batch number = 94
Evaluating:  46%|████▌     | 94/205 [00:44<01:08,  1.62it/s]11/28/2021 02:03:24 - INFO - __main__ -   Batch number = 95
Evaluating:  46%|████▋     | 95/205 [00:45<01:09,  1.58it/s]11/28/2021 02:03:25 - INFO - __main__ -   Batch number = 96
Evaluating:  47%|████▋     | 96/205 [00:45<01:09,  1.56it/s]11/28/2021 02:03:26 - INFO - __main__ -   Batch number = 97
Evaluating:  47%|████▋     | 97/205 [00:46<01:10,  1.54it/s]11/28/2021 02:03:27 - INFO - __main__ -   Batch number = 98
Evaluating:  48%|████▊     | 98/205 [00:47<01:09,  1.53it/s]11/28/2021 02:03:27 - INFO - __main__ -   Batch number = 99
Evaluating:  48%|████▊     | 99/205 [00:47<01:10,  1.51it/s]11/28/2021 02:03:28 - INFO - __main__ -   Batch number = 100
Evaluating:  49%|████▉     | 100/205 [00:48<01:08,  1.54it/s]11/28/2021 02:03:28 - INFO - __main__ -   Batch number = 101
Evaluating:  49%|████▉     | 101/205 [00:48<01:03,  1.65it/s]11/28/2021 02:03:29 - INFO - __main__ -   Batch number = 102
Evaluating:  50%|████▉     | 102/205 [00:49<00:59,  1.73it/s]11/28/2021 02:03:29 - INFO - __main__ -   Batch number = 103
Evaluating:  50%|█████     | 103/205 [00:49<00:52,  1.93it/s]11/28/2021 02:03:30 - INFO - __main__ -   Batch number = 104
Evaluating:  51%|█████     | 104/205 [00:50<00:47,  2.14it/s]11/28/2021 02:03:30 - INFO - __main__ -   Batch number = 105
Evaluating:  51%|█████     | 105/205 [00:50<00:42,  2.33it/s]11/28/2021 02:03:31 - INFO - __main__ -   Batch number = 106
Evaluating:  52%|█████▏    | 106/205 [00:50<00:40,  2.47it/s]11/28/2021 02:03:31 - INFO - __main__ -   Batch number = 107
Evaluating:  52%|█████▏    | 107/205 [00:51<00:37,  2.59it/s]11/28/2021 02:03:31 - INFO - __main__ -   Batch number = 108
Evaluating:  53%|█████▎    | 108/205 [00:51<00:36,  2.69it/s]11/28/2021 02:03:32 - INFO - __main__ -   Batch number = 109
Evaluating:  53%|█████▎    | 109/205 [00:51<00:34,  2.76it/s]11/28/2021 02:03:32 - INFO - __main__ -   Batch number = 110
Evaluating:  54%|█████▎    | 110/205 [00:52<00:33,  2.80it/s]11/28/2021 02:03:32 - INFO - __main__ -   Batch number = 111
Evaluating:  54%|█████▍    | 111/205 [00:52<00:33,  2.83it/s]11/28/2021 02:03:33 - INFO - __main__ -   Batch number = 112
Evaluating:  55%|█████▍    | 112/205 [00:52<00:32,  2.84it/s]11/28/2021 02:03:33 - INFO - __main__ -   Batch number = 113
Evaluating:  55%|█████▌    | 113/205 [00:53<00:32,  2.86it/s]11/28/2021 02:03:33 - INFO - __main__ -   Batch number = 114
Evaluating:  56%|█████▌    | 114/205 [00:53<00:31,  2.88it/s]11/28/2021 02:03:34 - INFO - __main__ -   Batch number = 115
Evaluating:  56%|█████▌    | 115/205 [00:53<00:31,  2.89it/s]11/28/2021 02:03:34 - INFO - __main__ -   Batch number = 116
Evaluating:  57%|█████▋    | 116/205 [00:54<00:30,  2.90it/s]11/28/2021 02:03:34 - INFO - __main__ -   Batch number = 117
Evaluating:  57%|█████▋    | 117/205 [00:54<00:30,  2.91it/s]11/28/2021 02:03:35 - INFO - __main__ -   Batch number = 118
Evaluating:  58%|█████▊    | 118/205 [00:55<00:29,  2.91it/s]11/28/2021 02:03:35 - INFO - __main__ -   Batch number = 119
Evaluating:  58%|█████▊    | 119/205 [00:55<00:29,  2.92it/s]11/28/2021 02:03:35 - INFO - __main__ -   Batch number = 120
Evaluating:  59%|█████▊    | 120/205 [00:55<00:29,  2.92it/s]11/28/2021 02:03:36 - INFO - __main__ -   Batch number = 121
Evaluating:  59%|█████▉    | 121/205 [00:56<00:28,  2.92it/s]11/28/2021 02:03:36 - INFO - __main__ -   Batch number = 122
Evaluating:  60%|█████▉    | 122/205 [00:56<00:28,  2.92it/s]11/28/2021 02:03:36 - INFO - __main__ -   Batch number = 123
Evaluating:  60%|██████    | 123/205 [00:56<00:27,  2.93it/s]11/28/2021 02:03:37 - INFO - __main__ -   Batch number = 124
Evaluating:  60%|██████    | 124/205 [00:57<00:27,  2.92it/s]11/28/2021 02:03:37 - INFO - __main__ -   Batch number = 125
Evaluating:  61%|██████    | 125/205 [00:57<00:24,  3.24it/s]11/28/2021 02:03:37 - INFO - __main__ -   Batch number = 126
Evaluating:  61%|██████▏   | 126/205 [00:57<00:25,  3.15it/s]11/28/2021 02:03:38 - INFO - __main__ -   Batch number = 127
Evaluating:  62%|██████▏   | 127/205 [00:57<00:25,  3.08it/s]11/28/2021 02:03:38 - INFO - __main__ -   Batch number = 128
Evaluating:  62%|██████▏   | 128/205 [00:58<00:25,  3.03it/s]11/28/2021 02:03:38 - INFO - __main__ -   Batch number = 129
Evaluating:  63%|██████▎   | 129/205 [00:58<00:25,  3.01it/s]11/28/2021 02:03:39 - INFO - __main__ -   Batch number = 130
Evaluating:  63%|██████▎   | 130/205 [00:58<00:25,  2.98it/s]11/28/2021 02:03:39 - INFO - __main__ -   Batch number = 131
Evaluating:  64%|██████▍   | 131/205 [00:59<00:24,  2.97it/s]11/28/2021 02:03:39 - INFO - __main__ -   Batch number = 132
Evaluating:  64%|██████▍   | 132/205 [00:59<00:24,  2.94it/s]11/28/2021 02:03:40 - INFO - __main__ -   Batch number = 133
Evaluating:  65%|██████▍   | 133/205 [01:00<00:24,  2.94it/s]11/28/2021 02:03:40 - INFO - __main__ -   Batch number = 134
Evaluating:  65%|██████▌   | 134/205 [01:00<00:24,  2.94it/s]11/28/2021 02:03:40 - INFO - __main__ -   Batch number = 135
Evaluating:  66%|██████▌   | 135/205 [01:00<00:23,  2.95it/s]11/28/2021 02:03:41 - INFO - __main__ -   Batch number = 136
Evaluating:  66%|██████▋   | 136/205 [01:01<00:23,  2.95it/s]11/28/2021 02:03:41 - INFO - __main__ -   Batch number = 137
Evaluating:  67%|██████▋   | 137/205 [01:01<00:23,  2.93it/s]11/28/2021 02:03:41 - INFO - __main__ -   Batch number = 138
Evaluating:  67%|██████▋   | 138/205 [01:01<00:22,  2.94it/s]11/28/2021 02:03:42 - INFO - __main__ -   Batch number = 139
Evaluating:  68%|██████▊   | 139/205 [01:02<00:22,  2.95it/s]11/28/2021 02:03:42 - INFO - __main__ -   Batch number = 140
Evaluating:  68%|██████▊   | 140/205 [01:02<00:21,  2.96it/s]11/28/2021 02:03:42 - INFO - __main__ -   Batch number = 141
Evaluating:  69%|██████▉   | 141/205 [01:02<00:21,  2.96it/s]11/28/2021 02:03:43 - INFO - __main__ -   Batch number = 142
Evaluating:  69%|██████▉   | 142/205 [01:03<00:21,  2.97it/s]11/28/2021 02:03:43 - INFO - __main__ -   Batch number = 143
Evaluating:  70%|██████▉   | 143/205 [01:03<00:20,  2.96it/s]11/28/2021 02:03:43 - INFO - __main__ -   Batch number = 144
Evaluating:  70%|███████   | 144/205 [01:03<00:20,  2.92it/s]11/28/2021 02:03:44 - INFO - __main__ -   Batch number = 145
Evaluating:  71%|███████   | 145/205 [01:04<00:20,  2.93it/s]11/28/2021 02:03:44 - INFO - __main__ -   Batch number = 146
Evaluating:  71%|███████   | 146/205 [01:04<00:20,  2.93it/s]11/28/2021 02:03:44 - INFO - __main__ -   Batch number = 147
Evaluating:  72%|███████▏  | 147/205 [01:04<00:19,  2.93it/s]11/28/2021 02:03:45 - INFO - __main__ -   Batch number = 148
Evaluating:  72%|███████▏  | 148/205 [01:05<00:19,  2.93it/s]11/28/2021 02:03:45 - INFO - __main__ -   Batch number = 149
Evaluating:  73%|███████▎  | 149/205 [01:05<00:19,  2.92it/s]11/28/2021 02:03:45 - INFO - __main__ -   Batch number = 150
Evaluating:  73%|███████▎  | 150/205 [01:05<00:18,  2.91it/s]11/28/2021 02:03:46 - INFO - __main__ -   Batch number = 151
Evaluating:  74%|███████▎  | 151/205 [01:06<00:18,  2.89it/s]11/28/2021 02:03:46 - INFO - __main__ -   Batch number = 152
Evaluating:  74%|███████▍  | 152/205 [01:06<00:18,  2.89it/s]11/28/2021 02:03:47 - INFO - __main__ -   Batch number = 153
Evaluating:  75%|███████▍  | 153/205 [01:06<00:17,  2.90it/s]11/28/2021 02:03:47 - INFO - __main__ -   Batch number = 154
Evaluating:  75%|███████▌  | 154/205 [01:07<00:17,  2.90it/s]11/28/2021 02:03:47 - INFO - __main__ -   Batch number = 155
Evaluating:  76%|███████▌  | 155/205 [01:07<00:17,  2.89it/s]11/28/2021 02:03:48 - INFO - __main__ -   Batch number = 156
Evaluating:  76%|███████▌  | 156/205 [01:07<00:16,  2.89it/s]11/28/2021 02:03:48 - INFO - __main__ -   Batch number = 157
Evaluating:  77%|███████▋  | 157/205 [01:08<00:16,  2.90it/s]11/28/2021 02:03:48 - INFO - __main__ -   Batch number = 158
Evaluating:  77%|███████▋  | 158/205 [01:08<00:16,  2.89it/s]11/28/2021 02:03:49 - INFO - __main__ -   Batch number = 159
Evaluating:  78%|███████▊  | 159/205 [01:08<00:15,  2.89it/s]11/28/2021 02:03:49 - INFO - __main__ -   Batch number = 160
Evaluating:  78%|███████▊  | 160/205 [01:09<00:15,  2.87it/s]11/28/2021 02:03:49 - INFO - __main__ -   Batch number = 161
Evaluating:  79%|███████▊  | 161/205 [01:09<00:15,  2.90it/s]11/28/2021 02:03:50 - INFO - __main__ -   Batch number = 162
Evaluating:  79%|███████▉  | 162/205 [01:09<00:14,  2.90it/s]11/28/2021 02:03:50 - INFO - __main__ -   Batch number = 163
Evaluating:  80%|███████▉  | 163/205 [01:10<00:14,  2.86it/s]11/28/2021 02:03:50 - INFO - __main__ -   Batch number = 164
Evaluating:  80%|████████  | 164/205 [01:10<00:14,  2.86it/s]11/28/2021 02:03:51 - INFO - __main__ -   Batch number = 165
Evaluating:  80%|████████  | 165/205 [01:11<00:14,  2.84it/s]11/28/2021 02:03:51 - INFO - __main__ -   Batch number = 166
Evaluating:  81%|████████  | 166/205 [01:11<00:13,  2.86it/s]11/28/2021 02:03:51 - INFO - __main__ -   Batch number = 167
Evaluating:  81%|████████▏ | 167/205 [01:11<00:13,  2.85it/s]11/28/2021 02:03:52 - INFO - __main__ -   Batch number = 168
Evaluating:  82%|████████▏ | 168/205 [01:12<00:12,  2.86it/s]11/28/2021 02:03:52 - INFO - __main__ -   Batch number = 169
Evaluating:  82%|████████▏ | 169/205 [01:12<00:12,  2.85it/s]11/28/2021 02:03:52 - INFO - __main__ -   Batch number = 170
Evaluating:  83%|████████▎ | 170/205 [01:12<00:13,  2.54it/s]11/28/2021 02:03:53 - INFO - __main__ -   Batch number = 171
Evaluating:  83%|████████▎ | 171/205 [01:13<00:14,  2.34it/s]11/28/2021 02:03:53 - INFO - __main__ -   Batch number = 172
Evaluating:  84%|████████▍ | 172/205 [01:13<00:14,  2.27it/s]11/28/2021 02:03:54 - INFO - __main__ -   Batch number = 173
Evaluating:  84%|████████▍ | 173/205 [01:14<00:13,  2.40it/s]11/28/2021 02:03:54 - INFO - __main__ -   Batch number = 174
Evaluating:  85%|████████▍ | 174/205 [01:14<00:12,  2.52it/s]11/28/2021 02:03:55 - INFO - __main__ -   Batch number = 175
Evaluating:  85%|████████▌ | 175/205 [01:14<00:11,  2.59it/s]11/28/2021 02:03:55 - INFO - __main__ -   Batch number = 176
Evaluating:  86%|████████▌ | 176/205 [01:15<00:10,  2.67it/s]11/28/2021 02:03:55 - INFO - __main__ -   Batch number = 177
Evaluating:  86%|████████▋ | 177/205 [01:15<00:11,  2.41it/s]11/28/2021 02:03:56 - INFO - __main__ -   Batch number = 178
Evaluating:  87%|████████▋ | 178/205 [01:16<00:12,  2.24it/s]11/28/2021 02:03:56 - INFO - __main__ -   Batch number = 179
Evaluating:  87%|████████▋ | 179/205 [01:16<00:12,  2.12it/s]11/28/2021 02:03:57 - INFO - __main__ -   Batch number = 180
Evaluating:  88%|████████▊ | 180/205 [01:17<00:12,  2.07it/s]11/28/2021 02:03:57 - INFO - __main__ -   Batch number = 181
Evaluating:  88%|████████▊ | 181/205 [01:17<00:11,  2.02it/s]11/28/2021 02:03:58 - INFO - __main__ -   Batch number = 182
Evaluating:  89%|████████▉ | 182/205 [01:18<00:11,  2.00it/s]11/28/2021 02:03:58 - INFO - __main__ -   Batch number = 183
Evaluating:  89%|████████▉ | 183/205 [01:18<00:11,  1.98it/s]11/28/2021 02:03:59 - INFO - __main__ -   Batch number = 184
Evaluating:  90%|████████▉ | 184/205 [01:19<00:10,  1.97it/s]11/28/2021 02:03:59 - INFO - __main__ -   Batch number = 185
Evaluating:  90%|█████████ | 185/205 [01:19<00:10,  1.96it/s]11/28/2021 02:04:00 - INFO - __main__ -   Batch number = 186
Evaluating:  91%|█████████ | 186/205 [01:20<00:09,  1.95it/s]11/28/2021 02:04:00 - INFO - __main__ -   Batch number = 187
Evaluating:  91%|█████████ | 187/205 [01:21<00:09,  1.94it/s]11/28/2021 02:04:01 - INFO - __main__ -   Batch number = 188
Evaluating:  92%|█████████▏| 188/205 [01:21<00:08,  1.94it/s]11/28/2021 02:04:02 - INFO - __main__ -   Batch number = 189
Evaluating:  92%|█████████▏| 189/205 [01:22<00:08,  1.94it/s]11/28/2021 02:04:02 - INFO - __main__ -   Batch number = 190
Evaluating:  93%|█████████▎| 190/205 [01:22<00:07,  1.94it/s]11/28/2021 02:04:03 - INFO - __main__ -   Batch number = 191
Evaluating:  93%|█████████▎| 191/205 [01:23<00:07,  1.94it/s]11/28/2021 02:04:03 - INFO - __main__ -   Batch number = 192
Evaluating:  94%|█████████▎| 192/205 [01:23<00:06,  1.94it/s]11/28/2021 02:04:04 - INFO - __main__ -   Batch number = 193
Evaluating:  94%|█████████▍| 193/205 [01:24<00:06,  1.94it/s]11/28/2021 02:04:04 - INFO - __main__ -   Batch number = 194
Evaluating:  95%|█████████▍| 194/205 [01:24<00:05,  1.94it/s]11/28/2021 02:04:05 - INFO - __main__ -   Batch number = 195
Evaluating:  95%|█████████▌| 195/205 [01:25<00:05,  1.94it/s]11/28/2021 02:04:05 - INFO - __main__ -   Batch number = 196
Evaluating:  96%|█████████▌| 196/205 [01:25<00:04,  1.94it/s]11/28/2021 02:04:06 - INFO - __main__ -   Batch number = 197
Evaluating:  96%|█████████▌| 197/205 [01:26<00:04,  1.93it/s]11/28/2021 02:04:06 - INFO - __main__ -   Batch number = 198
Evaluating:  97%|█████████▋| 198/205 [01:26<00:03,  1.92it/s]11/28/2021 02:04:07 - INFO - __main__ -   Batch number = 199
Evaluating:  97%|█████████▋| 199/205 [01:27<00:03,  1.92it/s]11/28/2021 02:04:07 - INFO - __main__ -   Batch number = 200
Evaluating:  98%|█████████▊| 200/205 [01:27<00:02,  1.86it/s]11/28/2021 02:04:08 - INFO - __main__ -   Batch number = 201
Evaluating:  98%|█████████▊| 201/205 [01:28<00:02,  1.88it/s]11/28/2021 02:04:08 - INFO - __main__ -   Batch number = 202
Evaluating:  99%|█████████▊| 202/205 [01:28<00:01,  1.89it/s]11/28/2021 02:04:09 - INFO - __main__ -   Batch number = 203
Evaluating:  99%|█████████▉| 203/205 [01:29<00:01,  1.89it/s]11/28/2021 02:04:09 - INFO - __main__ -   Batch number = 204
Evaluating: 100%|█████████▉| 204/205 [01:29<00:00,  1.91it/s]11/28/2021 02:04:10 - INFO - __main__ -   Batch number = 205
Evaluating: 100%|██████████| 205/205 [01:30<00:00,  2.05it/s]Evaluating: 100%|██████████| 205/205 [01:30<00:00,  2.27it/s]
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 02:04:12 - INFO - __main__ -   ***** Evaluation result  in fi *****
11/28/2021 02:04:12 - INFO - __main__ -     f1 = 0.7694920750113876
11/28/2021 02:04:12 - INFO - __main__ -     loss = 0.8321925060051244
11/28/2021 02:04:12 - INFO - __main__ -     precision = 0.7743524402939851
11/28/2021 02:04:12 - INFO - __main__ -     recall = 0.7646923431026545
86.84user 30.49system 1:56.86elapsed 100%CPU (0avgtext+0avgdata 3981864maxresident)k
0inputs+928outputs (0major+1693373minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 16:34:21 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='zh', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 16:34:21 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 16:34:21 - INFO - __main__ -   Seed = 1
11/28/2021 16:34:21 - INFO - root -   save model
11/28/2021 16:34:21 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='zh', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 16:34:21 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 16:34:23 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 16:34:29 - INFO - __main__ -   Using lang2id = None
11/28/2021 16:34:29 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 16:34:29 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
11/28/2021 16:34:29 - INFO - root -   Trying to decide if add adapter
11/28/2021 16:34:29 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 16:34:29 - INFO - root -   loading lang adpater cs/wiki@ukp
11/28/2021 16:34:29 - INFO - __main__ -   Adapter Languages : ['cs'], Length : 1
11/28/2021 16:34:29 - INFO - __main__ -   Adapter Names ['cs/wiki@ukp'], Length : 1
11/28/2021 16:34:29 - INFO - __main__ -   Language = cs
11/28/2021 16:34:29 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/cs/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_cs_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/adapter_config.json
Adding adapter 'cs' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/head_config.json
11/28/2021 16:34:34 - INFO - __main__ -   Language adapter for zh not found, using cs instead
11/28/2021 16:34:34 - INFO - __main__ -   Set active language adapter to cs
11/28/2021 16:34:34 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 16:34:34 - INFO - __main__ -   Adapter Languages = ['cs']
11/28/2021 16:34:34 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_zh_bert-base-multilingual-cased_128
11/28/2021 16:34:35 - INFO - __main__ -   ***** Running evaluation  in zh *****
11/28/2021 16:34:35 - INFO - __main__ -     Num examples = 3458
11/28/2021 16:34:35 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/109 [00:00<?, ?it/s]11/28/2021 16:34:35 - INFO - __main__ -   Batch number = 1
Evaluating:   1%|          | 1/109 [00:00<00:17,  6.20it/s]11/28/2021 16:34:35 - INFO - __main__ -   Batch number = 2
Evaluating:   2%|▏         | 2/109 [00:00<00:16,  6.63it/s]11/28/2021 16:34:35 - INFO - __main__ -   Batch number = 3
Evaluating:   3%|▎         | 3/109 [00:00<00:15,  6.84it/s]11/28/2021 16:34:35 - INFO - __main__ -   Batch number = 4
Evaluating:   4%|▎         | 4/109 [00:00<00:15,  6.94it/s]11/28/2021 16:34:35 - INFO - __main__ -   Batch number = 5
Evaluating:   5%|▍         | 5/109 [00:00<00:14,  7.00it/s]11/28/2021 16:34:36 - INFO - __main__ -   Batch number = 6
Evaluating:   6%|▌         | 6/109 [00:00<00:14,  7.02it/s]11/28/2021 16:34:36 - INFO - __main__ -   Batch number = 7
Evaluating:   6%|▋         | 7/109 [00:01<00:14,  7.04it/s]11/28/2021 16:34:36 - INFO - __main__ -   Batch number = 8
Evaluating:   7%|▋         | 8/109 [00:01<00:14,  7.06it/s]11/28/2021 16:34:36 - INFO - __main__ -   Batch number = 9
Evaluating:   8%|▊         | 9/109 [00:01<00:14,  7.04it/s]11/28/2021 16:34:36 - INFO - __main__ -   Batch number = 10
Evaluating:   9%|▉         | 10/109 [00:01<00:14,  7.04it/s]11/28/2021 16:34:36 - INFO - __main__ -   Batch number = 11
Evaluating:  10%|█         | 11/109 [00:01<00:13,  7.03it/s]11/28/2021 16:34:36 - INFO - __main__ -   Batch number = 12
Evaluating:  11%|█         | 12/109 [00:01<00:13,  7.01it/s]11/28/2021 16:34:37 - INFO - __main__ -   Batch number = 13
Evaluating:  12%|█▏        | 13/109 [00:01<00:13,  7.03it/s]11/28/2021 16:34:37 - INFO - __main__ -   Batch number = 14
Evaluating:  13%|█▎        | 14/109 [00:02<00:13,  7.04it/s]11/28/2021 16:34:37 - INFO - __main__ -   Batch number = 15
Evaluating:  14%|█▍        | 15/109 [00:02<00:13,  7.04it/s]11/28/2021 16:34:37 - INFO - __main__ -   Batch number = 16
Evaluating:  15%|█▍        | 16/109 [00:02<00:13,  7.04it/s]11/28/2021 16:34:37 - INFO - __main__ -   Batch number = 17
Evaluating:  16%|█▌        | 17/109 [00:02<00:13,  7.04it/s]11/28/2021 16:34:37 - INFO - __main__ -   Batch number = 18
Evaluating:  17%|█▋        | 18/109 [00:02<00:12,  7.03it/s]11/28/2021 16:34:37 - INFO - __main__ -   Batch number = 19
Evaluating:  17%|█▋        | 19/109 [00:02<00:12,  7.03it/s]11/28/2021 16:34:38 - INFO - __main__ -   Batch number = 20
Evaluating:  18%|█▊        | 20/109 [00:02<00:12,  7.03it/s]11/28/2021 16:34:38 - INFO - __main__ -   Batch number = 21
Evaluating:  19%|█▉        | 21/109 [00:03<00:12,  7.01it/s]11/28/2021 16:34:38 - INFO - __main__ -   Batch number = 22
Evaluating:  20%|██        | 22/109 [00:03<00:12,  7.01it/s]11/28/2021 16:34:38 - INFO - __main__ -   Batch number = 23
Evaluating:  21%|██        | 23/109 [00:03<00:12,  7.01it/s]11/28/2021 16:34:38 - INFO - __main__ -   Batch number = 24
Evaluating:  22%|██▏       | 24/109 [00:03<00:12,  7.00it/s]11/28/2021 16:34:38 - INFO - __main__ -   Batch number = 25
Evaluating:  23%|██▎       | 25/109 [00:03<00:12,  6.99it/s]11/28/2021 16:34:38 - INFO - __main__ -   Batch number = 26
Evaluating:  24%|██▍       | 26/109 [00:03<00:11,  6.97it/s]11/28/2021 16:34:39 - INFO - __main__ -   Batch number = 27
Evaluating:  25%|██▍       | 27/109 [00:03<00:11,  6.95it/s]11/28/2021 16:34:39 - INFO - __main__ -   Batch number = 28
Evaluating:  26%|██▌       | 28/109 [00:04<00:11,  6.96it/s]11/28/2021 16:34:39 - INFO - __main__ -   Batch number = 29
Evaluating:  27%|██▋       | 29/109 [00:04<00:11,  6.96it/s]11/28/2021 16:34:39 - INFO - __main__ -   Batch number = 30
Evaluating:  28%|██▊       | 30/109 [00:04<00:11,  6.95it/s]11/28/2021 16:34:39 - INFO - __main__ -   Batch number = 31
Evaluating:  28%|██▊       | 31/109 [00:04<00:11,  6.96it/s]11/28/2021 16:34:39 - INFO - __main__ -   Batch number = 32
Evaluating:  29%|██▉       | 32/109 [00:04<00:11,  6.97it/s]11/28/2021 16:34:39 - INFO - __main__ -   Batch number = 33
Evaluating:  30%|███       | 33/109 [00:04<00:10,  6.95it/s]11/28/2021 16:34:40 - INFO - __main__ -   Batch number = 34
Evaluating:  31%|███       | 34/109 [00:04<00:10,  6.95it/s]11/28/2021 16:34:40 - INFO - __main__ -   Batch number = 35
Evaluating:  32%|███▏      | 35/109 [00:05<00:12,  6.06it/s]11/28/2021 16:34:40 - INFO - __main__ -   Batch number = 36
Evaluating:  33%|███▎      | 36/109 [00:05<00:11,  6.29it/s]11/28/2021 16:34:40 - INFO - __main__ -   Batch number = 37
Evaluating:  34%|███▍      | 37/109 [00:05<00:11,  6.47it/s]11/28/2021 16:34:40 - INFO - __main__ -   Batch number = 38
Evaluating:  35%|███▍      | 38/109 [00:05<00:10,  6.61it/s]11/28/2021 16:34:40 - INFO - __main__ -   Batch number = 39
Evaluating:  36%|███▌      | 39/109 [00:05<00:10,  6.70it/s]11/28/2021 16:34:41 - INFO - __main__ -   Batch number = 40
Evaluating:  37%|███▋      | 40/109 [00:05<00:10,  6.68it/s]11/28/2021 16:34:41 - INFO - __main__ -   Batch number = 41
Evaluating:  38%|███▊      | 41/109 [00:05<00:10,  6.71it/s]11/28/2021 16:34:41 - INFO - __main__ -   Batch number = 42
Evaluating:  39%|███▊      | 42/109 [00:06<00:09,  6.76it/s]11/28/2021 16:34:41 - INFO - __main__ -   Batch number = 43
Evaluating:  39%|███▉      | 43/109 [00:06<00:09,  6.77it/s]11/28/2021 16:34:41 - INFO - __main__ -   Batch number = 44
Evaluating:  40%|████      | 44/109 [00:06<00:09,  6.76it/s]11/28/2021 16:34:41 - INFO - __main__ -   Batch number = 45
Evaluating:  41%|████▏     | 45/109 [00:06<00:09,  6.75it/s]11/28/2021 16:34:41 - INFO - __main__ -   Batch number = 46
Evaluating:  42%|████▏     | 46/109 [00:06<00:09,  6.74it/s]11/28/2021 16:34:42 - INFO - __main__ -   Batch number = 47
Evaluating:  43%|████▎     | 47/109 [00:06<00:09,  6.74it/s]11/28/2021 16:34:42 - INFO - __main__ -   Batch number = 48
Evaluating:  44%|████▍     | 48/109 [00:06<00:09,  6.73it/s]11/28/2021 16:34:42 - INFO - __main__ -   Batch number = 49
Evaluating:  45%|████▍     | 49/109 [00:07<00:08,  6.76it/s]11/28/2021 16:34:42 - INFO - __main__ -   Batch number = 50
Evaluating:  46%|████▌     | 50/109 [00:07<00:08,  6.76it/s]11/28/2021 16:34:42 - INFO - __main__ -   Batch number = 51
Evaluating:  47%|████▋     | 51/109 [00:07<00:08,  6.74it/s]11/28/2021 16:34:42 - INFO - __main__ -   Batch number = 52
Evaluating:  48%|████▊     | 52/109 [00:07<00:08,  6.76it/s]11/28/2021 16:34:42 - INFO - __main__ -   Batch number = 53
Evaluating:  49%|████▊     | 53/109 [00:07<00:08,  6.77it/s]11/28/2021 16:34:43 - INFO - __main__ -   Batch number = 54
Evaluating:  50%|████▉     | 54/109 [00:07<00:08,  6.78it/s]11/28/2021 16:34:43 - INFO - __main__ -   Batch number = 55
Evaluating:  50%|█████     | 55/109 [00:08<00:07,  6.80it/s]11/28/2021 16:34:43 - INFO - __main__ -   Batch number = 56
Evaluating:  51%|█████▏    | 56/109 [00:08<00:07,  6.81it/s]11/28/2021 16:34:43 - INFO - __main__ -   Batch number = 57
Evaluating:  52%|█████▏    | 57/109 [00:08<00:07,  6.83it/s]11/28/2021 16:34:43 - INFO - __main__ -   Batch number = 58
Evaluating:  53%|█████▎    | 58/109 [00:08<00:07,  6.84it/s]11/28/2021 16:34:43 - INFO - __main__ -   Batch number = 59
Evaluating:  54%|█████▍    | 59/109 [00:08<00:07,  6.85it/s]11/28/2021 16:34:43 - INFO - __main__ -   Batch number = 60
Evaluating:  55%|█████▌    | 60/109 [00:08<00:07,  6.84it/s]11/28/2021 16:34:44 - INFO - __main__ -   Batch number = 61
Evaluating:  56%|█████▌    | 61/109 [00:08<00:07,  6.82it/s]11/28/2021 16:34:44 - INFO - __main__ -   Batch number = 62
Evaluating:  57%|█████▋    | 62/109 [00:09<00:06,  6.81it/s]11/28/2021 16:34:44 - INFO - __main__ -   Batch number = 63
Evaluating:  58%|█████▊    | 63/109 [00:09<00:06,  6.72it/s]11/28/2021 16:34:44 - INFO - __main__ -   Batch number = 64
Evaluating:  59%|█████▊    | 64/109 [00:09<00:06,  6.74it/s]11/28/2021 16:34:44 - INFO - __main__ -   Batch number = 65
Evaluating:  60%|█████▉    | 65/109 [00:09<00:06,  6.77it/s]11/28/2021 16:34:44 - INFO - __main__ -   Batch number = 66
Evaluating:  61%|██████    | 66/109 [00:09<00:06,  6.77it/s]11/28/2021 16:34:44 - INFO - __main__ -   Batch number = 67
Evaluating:  61%|██████▏   | 67/109 [00:09<00:06,  6.79it/s]11/28/2021 16:34:45 - INFO - __main__ -   Batch number = 68
Evaluating:  62%|██████▏   | 68/109 [00:09<00:06,  6.75it/s]11/28/2021 16:34:45 - INFO - __main__ -   Batch number = 69
Evaluating:  63%|██████▎   | 69/109 [00:10<00:06,  6.48it/s]11/28/2021 16:34:45 - INFO - __main__ -   Batch number = 70
Evaluating:  64%|██████▍   | 70/109 [00:10<00:05,  6.58it/s]11/28/2021 16:34:45 - INFO - __main__ -   Batch number = 71
Evaluating:  65%|██████▌   | 71/109 [00:10<00:05,  6.66it/s]11/28/2021 16:34:45 - INFO - __main__ -   Batch number = 72
Evaluating:  66%|██████▌   | 72/109 [00:10<00:05,  6.67it/s]11/28/2021 16:34:45 - INFO - __main__ -   Batch number = 73
Evaluating:  67%|██████▋   | 73/109 [00:10<00:05,  6.69it/s]11/28/2021 16:34:46 - INFO - __main__ -   Batch number = 74
Evaluating:  68%|██████▊   | 74/109 [00:10<00:05,  6.72it/s]11/28/2021 16:34:46 - INFO - __main__ -   Batch number = 75
Evaluating:  69%|██████▉   | 75/109 [00:10<00:05,  6.71it/s]11/28/2021 16:34:46 - INFO - __main__ -   Batch number = 76
Evaluating:  70%|██████▉   | 76/109 [00:11<00:04,  6.73it/s]11/28/2021 16:34:46 - INFO - __main__ -   Batch number = 77
Evaluating:  71%|███████   | 77/109 [00:11<00:04,  6.75it/s]11/28/2021 16:34:46 - INFO - __main__ -   Batch number = 78
Evaluating:  72%|███████▏  | 78/109 [00:11<00:04,  6.75it/s]11/28/2021 16:34:46 - INFO - __main__ -   Batch number = 79
Evaluating:  72%|███████▏  | 79/109 [00:11<00:04,  6.69it/s]11/28/2021 16:34:46 - INFO - __main__ -   Batch number = 80
Evaluating:  73%|███████▎  | 80/109 [00:11<00:04,  6.65it/s]11/28/2021 16:34:47 - INFO - __main__ -   Batch number = 81
Evaluating:  74%|███████▍  | 81/109 [00:11<00:04,  6.63it/s]11/28/2021 16:34:47 - INFO - __main__ -   Batch number = 82
Evaluating:  75%|███████▌  | 82/109 [00:12<00:04,  6.57it/s]11/28/2021 16:34:47 - INFO - __main__ -   Batch number = 83
Evaluating:  76%|███████▌  | 83/109 [00:12<00:03,  6.50it/s]11/28/2021 16:34:47 - INFO - __main__ -   Batch number = 84
Evaluating:  77%|███████▋  | 84/109 [00:12<00:03,  6.56it/s]11/28/2021 16:34:47 - INFO - __main__ -   Batch number = 85
Evaluating:  78%|███████▊  | 85/109 [00:12<00:03,  6.61it/s]11/28/2021 16:34:47 - INFO - __main__ -   Batch number = 86
Evaluating:  79%|███████▉  | 86/109 [00:12<00:03,  6.62it/s]11/28/2021 16:34:48 - INFO - __main__ -   Batch number = 87
Evaluating:  80%|███████▉  | 87/109 [00:12<00:03,  6.64it/s]11/28/2021 16:34:48 - INFO - __main__ -   Batch number = 88
Evaluating:  81%|████████  | 88/109 [00:12<00:03,  6.66it/s]11/28/2021 16:34:48 - INFO - __main__ -   Batch number = 89
Evaluating:  82%|████████▏ | 89/109 [00:13<00:02,  6.68it/s]11/28/2021 16:34:48 - INFO - __main__ -   Batch number = 90
Evaluating:  83%|████████▎ | 90/109 [00:13<00:02,  6.70it/s]11/28/2021 16:34:48 - INFO - __main__ -   Batch number = 91
Evaluating:  83%|████████▎ | 91/109 [00:13<00:02,  6.70it/s]11/28/2021 16:34:48 - INFO - __main__ -   Batch number = 92
Evaluating:  84%|████████▍ | 92/109 [00:13<00:02,  6.71it/s]11/28/2021 16:34:48 - INFO - __main__ -   Batch number = 93
Evaluating:  85%|████████▌ | 93/109 [00:13<00:02,  6.72it/s]11/28/2021 16:34:49 - INFO - __main__ -   Batch number = 94
Evaluating:  86%|████████▌ | 94/109 [00:13<00:02,  6.72it/s]11/28/2021 16:34:49 - INFO - __main__ -   Batch number = 95
Evaluating:  87%|████████▋ | 95/109 [00:13<00:02,  6.72it/s]11/28/2021 16:34:49 - INFO - __main__ -   Batch number = 96
Evaluating:  88%|████████▊ | 96/109 [00:14<00:01,  6.73it/s]11/28/2021 16:34:49 - INFO - __main__ -   Batch number = 97
Evaluating:  89%|████████▉ | 97/109 [00:14<00:01,  6.70it/s]11/28/2021 16:34:49 - INFO - __main__ -   Batch number = 98
Evaluating:  90%|████████▉ | 98/109 [00:14<00:01,  6.70it/s]11/28/2021 16:34:49 - INFO - __main__ -   Batch number = 99
Evaluating:  91%|█████████ | 99/109 [00:14<00:01,  6.70it/s]11/28/2021 16:34:49 - INFO - __main__ -   Batch number = 100
Evaluating:  92%|█████████▏| 100/109 [00:14<00:01,  6.70it/s]11/28/2021 16:34:50 - INFO - __main__ -   Batch number = 101
Evaluating:  93%|█████████▎| 101/109 [00:14<00:01,  6.70it/s]11/28/2021 16:34:50 - INFO - __main__ -   Batch number = 102
Evaluating:  94%|█████████▎| 102/109 [00:15<00:01,  6.68it/s]11/28/2021 16:34:50 - INFO - __main__ -   Batch number = 103
Evaluating:  94%|█████████▍| 103/109 [00:15<00:00,  6.66it/s]11/28/2021 16:34:50 - INFO - __main__ -   Batch number = 104
Evaluating:  95%|█████████▌| 104/109 [00:15<00:00,  6.69it/s]11/28/2021 16:34:50 - INFO - __main__ -   Batch number = 105
Evaluating:  96%|█████████▋| 105/109 [00:15<00:00,  6.68it/s]11/28/2021 16:34:50 - INFO - __main__ -   Batch number = 106
Evaluating:  97%|█████████▋| 106/109 [00:15<00:00,  6.66it/s]11/28/2021 16:34:50 - INFO - __main__ -   Batch number = 107
Evaluating:  98%|█████████▊| 107/109 [00:15<00:00,  6.66it/s]11/28/2021 16:34:51 - INFO - __main__ -   Batch number = 108
Evaluating:  99%|█████████▉| 108/109 [00:15<00:00,  6.66it/s]11/28/2021 16:34:51 - INFO - __main__ -   Batch number = 109
Evaluating: 100%|██████████| 109/109 [00:15<00:00,  6.83it/s]
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 16:34:52 - INFO - __main__ -   ***** Evaluation result  in zh *****
11/28/2021 16:34:52 - INFO - __main__ -     f1 = 0.6074717699226253
11/28/2021 16:34:52 - INFO - __main__ -     loss = 1.457914115638908
11/28/2021 16:34:52 - INFO - __main__ -     precision = 0.6141866627944966
11/28/2021 16:34:52 - INFO - __main__ -     recall = 0.6009021167779515
27.23user 9.02system 0:33.88elapsed 107%CPU (0avgtext+0avgdata 3988348maxresident)k
0inputs+744outputs (0major+1549252minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 16:34:55 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='zh', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 16:34:55 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 16:34:55 - INFO - __main__ -   Seed = 2
11/28/2021 16:34:55 - INFO - root -   save model
11/28/2021 16:34:55 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='zh', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 16:34:55 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 16:34:57 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 16:35:04 - INFO - __main__ -   Using lang2id = None
11/28/2021 16:35:04 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 16:35:04 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
11/28/2021 16:35:04 - INFO - root -   Trying to decide if add adapter
11/28/2021 16:35:04 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 16:35:04 - INFO - root -   loading lang adpater cs/wiki@ukp
11/28/2021 16:35:04 - INFO - __main__ -   Adapter Languages : ['cs'], Length : 1
11/28/2021 16:35:04 - INFO - __main__ -   Adapter Names ['cs/wiki@ukp'], Length : 1
11/28/2021 16:35:04 - INFO - __main__ -   Language = cs
11/28/2021 16:35:04 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/cs/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_cs_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/adapter_config.json
Adding adapter 'cs' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/head_config.json
11/28/2021 16:35:09 - INFO - __main__ -   Language adapter for zh not found, using cs instead
11/28/2021 16:35:09 - INFO - __main__ -   Set active language adapter to cs
11/28/2021 16:35:09 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 16:35:09 - INFO - __main__ -   Adapter Languages = ['cs']
11/28/2021 16:35:09 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_zh_bert-base-multilingual-cased_128
11/28/2021 16:35:10 - INFO - __main__ -   ***** Running evaluation  in zh *****
11/28/2021 16:35:10 - INFO - __main__ -     Num examples = 3458
11/28/2021 16:35:10 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/109 [00:00<?, ?it/s]11/28/2021 16:35:10 - INFO - __main__ -   Batch number = 1
Evaluating:   1%|          | 1/109 [00:00<00:18,  5.94it/s]11/28/2021 16:35:10 - INFO - __main__ -   Batch number = 2
Evaluating:   2%|▏         | 2/109 [00:00<00:16,  6.42it/s]11/28/2021 16:35:10 - INFO - __main__ -   Batch number = 3
Evaluating:   3%|▎         | 3/109 [00:00<00:15,  6.71it/s]11/28/2021 16:35:10 - INFO - __main__ -   Batch number = 4
Evaluating:   4%|▎         | 4/109 [00:00<00:15,  6.88it/s]11/28/2021 16:35:10 - INFO - __main__ -   Batch number = 5
Evaluating:   5%|▍         | 5/109 [00:00<00:14,  6.97it/s]11/28/2021 16:35:11 - INFO - __main__ -   Batch number = 6
Evaluating:   6%|▌         | 6/109 [00:00<00:14,  7.00it/s]11/28/2021 16:35:11 - INFO - __main__ -   Batch number = 7
Evaluating:   6%|▋         | 7/109 [00:01<00:14,  7.02it/s]11/28/2021 16:35:11 - INFO - __main__ -   Batch number = 8
Evaluating:   7%|▋         | 8/109 [00:01<00:14,  7.04it/s]11/28/2021 16:35:11 - INFO - __main__ -   Batch number = 9
Evaluating:   8%|▊         | 9/109 [00:01<00:14,  7.06it/s]11/28/2021 16:35:11 - INFO - __main__ -   Batch number = 10
Evaluating:   9%|▉         | 10/109 [00:01<00:14,  7.06it/s]11/28/2021 16:35:11 - INFO - __main__ -   Batch number = 11
Evaluating:  10%|█         | 11/109 [00:01<00:13,  7.07it/s]11/28/2021 16:35:11 - INFO - __main__ -   Batch number = 12
Evaluating:  11%|█         | 12/109 [00:01<00:13,  7.08it/s]11/28/2021 16:35:12 - INFO - __main__ -   Batch number = 13
Evaluating:  12%|█▏        | 13/109 [00:01<00:13,  7.06it/s]11/28/2021 16:35:12 - INFO - __main__ -   Batch number = 14
Evaluating:  13%|█▎        | 14/109 [00:02<00:13,  7.06it/s]11/28/2021 16:35:12 - INFO - __main__ -   Batch number = 15
Evaluating:  14%|█▍        | 15/109 [00:02<00:13,  7.06it/s]11/28/2021 16:35:12 - INFO - __main__ -   Batch number = 16
Evaluating:  15%|█▍        | 16/109 [00:02<00:13,  7.06it/s]11/28/2021 16:35:12 - INFO - __main__ -   Batch number = 17
Evaluating:  16%|█▌        | 17/109 [00:02<00:13,  7.04it/s]11/28/2021 16:35:12 - INFO - __main__ -   Batch number = 18
Evaluating:  17%|█▋        | 18/109 [00:02<00:12,  7.04it/s]11/28/2021 16:35:12 - INFO - __main__ -   Batch number = 19
Evaluating:  17%|█▋        | 19/109 [00:02<00:12,  7.05it/s]11/28/2021 16:35:13 - INFO - __main__ -   Batch number = 20
Evaluating:  18%|█▊        | 20/109 [00:02<00:12,  7.03it/s]11/28/2021 16:35:13 - INFO - __main__ -   Batch number = 21
Evaluating:  19%|█▉        | 21/109 [00:03<00:12,  7.03it/s]11/28/2021 16:35:13 - INFO - __main__ -   Batch number = 22
Evaluating:  20%|██        | 22/109 [00:03<00:12,  6.97it/s]11/28/2021 16:35:13 - INFO - __main__ -   Batch number = 23
Evaluating:  21%|██        | 23/109 [00:03<00:12,  6.96it/s]11/28/2021 16:35:13 - INFO - __main__ -   Batch number = 24
Evaluating:  22%|██▏       | 24/109 [00:03<00:12,  6.98it/s]11/28/2021 16:35:13 - INFO - __main__ -   Batch number = 25
Evaluating:  23%|██▎       | 25/109 [00:03<00:12,  6.99it/s]11/28/2021 16:35:13 - INFO - __main__ -   Batch number = 26
Evaluating:  24%|██▍       | 26/109 [00:03<00:11,  6.99it/s]11/28/2021 16:35:14 - INFO - __main__ -   Batch number = 27
Evaluating:  25%|██▍       | 27/109 [00:03<00:11,  7.00it/s]11/28/2021 16:35:14 - INFO - __main__ -   Batch number = 28
Evaluating:  26%|██▌       | 28/109 [00:04<00:11,  7.01it/s]11/28/2021 16:35:14 - INFO - __main__ -   Batch number = 29
Evaluating:  27%|██▋       | 29/109 [00:04<00:11,  7.00it/s]11/28/2021 16:35:14 - INFO - __main__ -   Batch number = 30
Evaluating:  28%|██▊       | 30/109 [00:04<00:11,  7.01it/s]11/28/2021 16:35:14 - INFO - __main__ -   Batch number = 31
Evaluating:  28%|██▊       | 31/109 [00:04<00:11,  7.00it/s]11/28/2021 16:35:14 - INFO - __main__ -   Batch number = 32
Evaluating:  29%|██▉       | 32/109 [00:04<00:11,  6.99it/s]11/28/2021 16:35:14 - INFO - __main__ -   Batch number = 33
Evaluating:  30%|███       | 33/109 [00:04<00:10,  6.99it/s]11/28/2021 16:35:15 - INFO - __main__ -   Batch number = 34
Evaluating:  31%|███       | 34/109 [00:04<00:10,  7.00it/s]11/28/2021 16:35:15 - INFO - __main__ -   Batch number = 35
Evaluating:  32%|███▏      | 35/109 [00:05<00:10,  6.99it/s]11/28/2021 16:35:15 - INFO - __main__ -   Batch number = 36
Evaluating:  33%|███▎      | 36/109 [00:05<00:10,  6.99it/s]11/28/2021 16:35:15 - INFO - __main__ -   Batch number = 37
Evaluating:  34%|███▍      | 37/109 [00:05<00:10,  6.98it/s]11/28/2021 16:35:15 - INFO - __main__ -   Batch number = 38
Evaluating:  35%|███▍      | 38/109 [00:05<00:10,  6.95it/s]11/28/2021 16:35:15 - INFO - __main__ -   Batch number = 39
Evaluating:  36%|███▌      | 39/109 [00:05<00:10,  6.96it/s]11/28/2021 16:35:15 - INFO - __main__ -   Batch number = 40
Evaluating:  37%|███▋      | 40/109 [00:05<00:09,  6.97it/s]11/28/2021 16:35:16 - INFO - __main__ -   Batch number = 41
Evaluating:  38%|███▊      | 41/109 [00:05<00:09,  6.97it/s]11/28/2021 16:35:16 - INFO - __main__ -   Batch number = 42
Evaluating:  39%|███▊      | 42/109 [00:06<00:09,  6.97it/s]11/28/2021 16:35:16 - INFO - __main__ -   Batch number = 43
Evaluating:  39%|███▉      | 43/109 [00:06<00:09,  6.96it/s]11/28/2021 16:35:16 - INFO - __main__ -   Batch number = 44
Evaluating:  40%|████      | 44/109 [00:06<00:09,  6.95it/s]11/28/2021 16:35:16 - INFO - __main__ -   Batch number = 45
Evaluating:  41%|████▏     | 45/109 [00:06<00:09,  6.96it/s]11/28/2021 16:35:16 - INFO - __main__ -   Batch number = 46
Evaluating:  42%|████▏     | 46/109 [00:06<00:09,  6.95it/s]11/28/2021 16:35:16 - INFO - __main__ -   Batch number = 47
Evaluating:  43%|████▎     | 47/109 [00:06<00:08,  6.93it/s]11/28/2021 16:35:17 - INFO - __main__ -   Batch number = 48
Evaluating:  44%|████▍     | 48/109 [00:06<00:08,  6.93it/s]11/28/2021 16:35:17 - INFO - __main__ -   Batch number = 49
Evaluating:  45%|████▍     | 49/109 [00:07<00:08,  6.93it/s]11/28/2021 16:35:17 - INFO - __main__ -   Batch number = 50
Evaluating:  46%|████▌     | 50/109 [00:07<00:08,  6.81it/s]11/28/2021 16:35:17 - INFO - __main__ -   Batch number = 51
Evaluating:  47%|████▋     | 51/109 [00:07<00:08,  6.79it/s]11/28/2021 16:35:17 - INFO - __main__ -   Batch number = 52
Evaluating:  48%|████▊     | 52/109 [00:07<00:08,  6.83it/s]11/28/2021 16:35:17 - INFO - __main__ -   Batch number = 53
Evaluating:  49%|████▊     | 53/109 [00:07<00:08,  6.84it/s]11/28/2021 16:35:17 - INFO - __main__ -   Batch number = 54
Evaluating:  50%|████▉     | 54/109 [00:07<00:08,  6.87it/s]11/28/2021 16:35:18 - INFO - __main__ -   Batch number = 55
Evaluating:  50%|█████     | 55/109 [00:07<00:07,  6.87it/s]11/28/2021 16:35:18 - INFO - __main__ -   Batch number = 56
Evaluating:  51%|█████▏    | 56/109 [00:08<00:07,  6.88it/s]11/28/2021 16:35:18 - INFO - __main__ -   Batch number = 57
Evaluating:  52%|█████▏    | 57/109 [00:08<00:07,  6.89it/s]11/28/2021 16:35:18 - INFO - __main__ -   Batch number = 58
Evaluating:  53%|█████▎    | 58/109 [00:08<00:07,  6.89it/s]11/28/2021 16:35:18 - INFO - __main__ -   Batch number = 59
Evaluating:  54%|█████▍    | 59/109 [00:08<00:07,  6.89it/s]11/28/2021 16:35:18 - INFO - __main__ -   Batch number = 60
Evaluating:  55%|█████▌    | 60/109 [00:08<00:07,  6.90it/s]11/28/2021 16:35:18 - INFO - __main__ -   Batch number = 61
Evaluating:  56%|█████▌    | 61/109 [00:08<00:06,  6.88it/s]11/28/2021 16:35:19 - INFO - __main__ -   Batch number = 62
Evaluating:  57%|█████▋    | 62/109 [00:08<00:06,  6.87it/s]11/28/2021 16:35:19 - INFO - __main__ -   Batch number = 63
Evaluating:  58%|█████▊    | 63/109 [00:09<00:06,  6.88it/s]11/28/2021 16:35:19 - INFO - __main__ -   Batch number = 64
Evaluating:  59%|█████▊    | 64/109 [00:09<00:06,  6.88it/s]11/28/2021 16:35:19 - INFO - __main__ -   Batch number = 65
Evaluating:  60%|█████▉    | 65/109 [00:09<00:06,  6.86it/s]11/28/2021 16:35:19 - INFO - __main__ -   Batch number = 66
Evaluating:  61%|██████    | 66/109 [00:09<00:06,  6.87it/s]11/28/2021 16:35:19 - INFO - __main__ -   Batch number = 67
Evaluating:  61%|██████▏   | 67/109 [00:09<00:06,  6.87it/s]11/28/2021 16:35:19 - INFO - __main__ -   Batch number = 68
Evaluating:  62%|██████▏   | 68/109 [00:09<00:06,  6.83it/s]11/28/2021 16:35:20 - INFO - __main__ -   Batch number = 69
Evaluating:  63%|██████▎   | 69/109 [00:09<00:05,  6.81it/s]11/28/2021 16:35:20 - INFO - __main__ -   Batch number = 70
Evaluating:  64%|██████▍   | 70/109 [00:10<00:05,  6.81it/s]11/28/2021 16:35:20 - INFO - __main__ -   Batch number = 71
Evaluating:  65%|██████▌   | 71/109 [00:10<00:06,  6.26it/s]11/28/2021 16:35:20 - INFO - __main__ -   Batch number = 72
Evaluating:  66%|██████▌   | 72/109 [00:10<00:05,  6.27it/s]11/28/2021 16:35:20 - INFO - __main__ -   Batch number = 73
Evaluating:  67%|██████▋   | 73/109 [00:10<00:05,  6.27it/s]11/28/2021 16:35:20 - INFO - __main__ -   Batch number = 74
Evaluating:  68%|██████▊   | 74/109 [00:10<00:05,  6.31it/s]11/28/2021 16:35:21 - INFO - __main__ -   Batch number = 75
Evaluating:  69%|██████▉   | 75/109 [00:10<00:05,  6.37it/s]11/28/2021 16:35:21 - INFO - __main__ -   Batch number = 76
Evaluating:  70%|██████▉   | 76/109 [00:11<00:05,  6.42it/s]11/28/2021 16:35:21 - INFO - __main__ -   Batch number = 77
Evaluating:  71%|███████   | 77/109 [00:11<00:04,  6.44it/s]11/28/2021 16:35:21 - INFO - __main__ -   Batch number = 78
Evaluating:  72%|███████▏  | 78/109 [00:11<00:04,  6.32it/s]11/28/2021 16:35:21 - INFO - __main__ -   Batch number = 79
Evaluating:  72%|███████▏  | 79/109 [00:11<00:04,  6.30it/s]11/28/2021 16:35:21 - INFO - __main__ -   Batch number = 80
Evaluating:  73%|███████▎  | 80/109 [00:11<00:04,  6.32it/s]11/28/2021 16:35:21 - INFO - __main__ -   Batch number = 81
Evaluating:  74%|███████▍  | 81/109 [00:11<00:04,  6.31it/s]11/28/2021 16:35:22 - INFO - __main__ -   Batch number = 82
Evaluating:  75%|███████▌  | 82/109 [00:12<00:04,  6.30it/s]11/28/2021 16:35:22 - INFO - __main__ -   Batch number = 83
Evaluating:  76%|███████▌  | 83/109 [00:12<00:04,  6.27it/s]11/28/2021 16:35:22 - INFO - __main__ -   Batch number = 84
Evaluating:  77%|███████▋  | 84/109 [00:12<00:03,  6.27it/s]11/28/2021 16:35:22 - INFO - __main__ -   Batch number = 85
Evaluating:  78%|███████▊  | 85/109 [00:12<00:03,  6.26it/s]11/28/2021 16:35:22 - INFO - __main__ -   Batch number = 86
Evaluating:  79%|███████▉  | 86/109 [00:12<00:03,  6.27it/s]11/28/2021 16:35:22 - INFO - __main__ -   Batch number = 87
Evaluating:  80%|███████▉  | 87/109 [00:12<00:03,  6.29it/s]11/28/2021 16:35:23 - INFO - __main__ -   Batch number = 88
Evaluating:  81%|████████  | 88/109 [00:12<00:03,  6.33it/s]11/28/2021 16:35:23 - INFO - __main__ -   Batch number = 89
Evaluating:  82%|████████▏ | 89/109 [00:13<00:03,  6.38it/s]11/28/2021 16:35:23 - INFO - __main__ -   Batch number = 90
Evaluating:  83%|████████▎ | 90/109 [00:13<00:02,  6.42it/s]11/28/2021 16:35:23 - INFO - __main__ -   Batch number = 91
Evaluating:  83%|████████▎ | 91/109 [00:13<00:02,  6.45it/s]11/28/2021 16:35:23 - INFO - __main__ -   Batch number = 92
Evaluating:  84%|████████▍ | 92/109 [00:13<00:02,  6.51it/s]11/28/2021 16:35:23 - INFO - __main__ -   Batch number = 93
Evaluating:  85%|████████▌ | 93/109 [00:13<00:02,  6.53it/s]11/28/2021 16:35:24 - INFO - __main__ -   Batch number = 94
Evaluating:  86%|████████▌ | 94/109 [00:13<00:02,  6.56it/s]11/28/2021 16:35:24 - INFO - __main__ -   Batch number = 95
Evaluating:  87%|████████▋ | 95/109 [00:14<00:02,  6.59it/s]11/28/2021 16:35:24 - INFO - __main__ -   Batch number = 96
Evaluating:  88%|████████▊ | 96/109 [00:14<00:01,  6.65it/s]11/28/2021 16:35:24 - INFO - __main__ -   Batch number = 97
Evaluating:  89%|████████▉ | 97/109 [00:14<00:01,  6.69it/s]11/28/2021 16:35:24 - INFO - __main__ -   Batch number = 98
Evaluating:  90%|████████▉ | 98/109 [00:14<00:01,  6.72it/s]11/28/2021 16:35:24 - INFO - __main__ -   Batch number = 99
Evaluating:  91%|█████████ | 99/109 [00:14<00:01,  6.74it/s]11/28/2021 16:35:24 - INFO - __main__ -   Batch number = 100
Evaluating:  92%|█████████▏| 100/109 [00:14<00:01,  6.76it/s]11/28/2021 16:35:25 - INFO - __main__ -   Batch number = 101
Evaluating:  93%|█████████▎| 101/109 [00:14<00:01,  6.72it/s]11/28/2021 16:35:25 - INFO - __main__ -   Batch number = 102
Evaluating:  94%|█████████▎| 102/109 [00:15<00:01,  6.73it/s]11/28/2021 16:35:25 - INFO - __main__ -   Batch number = 103
Evaluating:  94%|█████████▍| 103/109 [00:15<00:01,  5.86it/s]11/28/2021 16:35:25 - INFO - __main__ -   Batch number = 104
Evaluating:  95%|█████████▌| 104/109 [00:15<00:00,  6.10it/s]11/28/2021 16:35:25 - INFO - __main__ -   Batch number = 105
Evaluating:  96%|█████████▋| 105/109 [00:15<00:00,  6.28it/s]11/28/2021 16:35:25 - INFO - __main__ -   Batch number = 106
Evaluating:  97%|█████████▋| 106/109 [00:15<00:00,  6.42it/s]11/28/2021 16:35:26 - INFO - __main__ -   Batch number = 107
Evaluating:  98%|█████████▊| 107/109 [00:15<00:00,  6.50it/s]11/28/2021 16:35:26 - INFO - __main__ -   Batch number = 108
Evaluating:  99%|█████████▉| 108/109 [00:16<00:00,  6.58it/s]11/28/2021 16:35:26 - INFO - __main__ -   Batch number = 109
Evaluating: 100%|██████████| 109/109 [00:16<00:00,  6.79it/s]
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 16:35:27 - INFO - __main__ -   ***** Evaluation result  in zh *****
11/28/2021 16:35:27 - INFO - __main__ -     f1 = 0.6144789782086246
11/28/2021 16:35:27 - INFO - __main__ -     loss = 1.4079616999407427
11/28/2021 16:35:27 - INFO - __main__ -     precision = 0.6230310110062606
11/28/2021 16:35:27 - INFO - __main__ -     recall = 0.6061585452479046
28.54user 11.56system 0:35.09elapsed 114%CPU (0avgtext+0avgdata 3991128maxresident)k
0inputs+744outputs (0major+1712360minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
11/28/2021 16:35:30 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='zh', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 16:35:30 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/28/2021 16:35:30 - INFO - __main__ -   Seed = 3
11/28/2021 16:35:30 - INFO - root -   save model
11/28/2021 16:35:30 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='zh', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_cs//train_ar.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
11/28/2021 16:35:30 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
11/28/2021 16:35:32 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/28/2021 16:35:38 - INFO - __main__ -   Using lang2id = None
11/28/2021 16:35:38 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
11/28/2021 16:35:38 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
11/28/2021 16:35:38 - INFO - root -   Trying to decide if add adapter
11/28/2021 16:35:38 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_model_head.bin
11/28/2021 16:35:38 - INFO - root -   loading lang adpater cs/wiki@ukp
11/28/2021 16:35:38 - INFO - __main__ -   Adapter Languages : ['cs'], Length : 1
11/28/2021 16:35:38 - INFO - __main__ -   Adapter Names ['cs/wiki@ukp'], Length : 1
11/28/2021 16:35:38 - INFO - __main__ -   Language = cs
11/28/2021 16:35:38 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/cs/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_cs_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/adapter_config.json
Adding adapter 'cs' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/6a2960229867d3f920b674d6f4f23e22c511f9f66431f4c2acf0a6019df19734-875886ddc9f7d0a0874b9a5219b12097e915aeb85831d0b6fd4686917d5bd0c6-extracted/head_config.json
11/28/2021 16:35:43 - INFO - __main__ -   Language adapter for zh not found, using cs instead
11/28/2021 16:35:43 - INFO - __main__ -   Set active language adapter to cs
11/28/2021 16:35:43 - INFO - __main__ -   Args Adapter Weight = None
11/28/2021 16:35:43 - INFO - __main__ -   Adapter Languages = ['cs']
11/28/2021 16:35:43 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_zh_bert-base-multilingual-cased_128
11/28/2021 16:35:43 - INFO - __main__ -   ***** Running evaluation  in zh *****
11/28/2021 16:35:43 - INFO - __main__ -     Num examples = 3458
11/28/2021 16:35:43 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/109 [00:00<?, ?it/s]11/28/2021 16:35:43 - INFO - __main__ -   Batch number = 1
Evaluating:   1%|          | 1/109 [00:00<00:16,  6.50it/s]11/28/2021 16:35:43 - INFO - __main__ -   Batch number = 2
Evaluating:   2%|▏         | 2/109 [00:00<00:15,  6.88it/s]11/28/2021 16:35:44 - INFO - __main__ -   Batch number = 3
Evaluating:   3%|▎         | 3/109 [00:00<00:15,  6.98it/s]11/28/2021 16:35:44 - INFO - __main__ -   Batch number = 4
Evaluating:   4%|▎         | 4/109 [00:00<00:14,  7.05it/s]11/28/2021 16:35:44 - INFO - __main__ -   Batch number = 5
Evaluating:   5%|▍         | 5/109 [00:00<00:14,  7.08it/s]11/28/2021 16:35:44 - INFO - __main__ -   Batch number = 6
Evaluating:   6%|▌         | 6/109 [00:00<00:14,  7.09it/s]11/28/2021 16:35:44 - INFO - __main__ -   Batch number = 7
Evaluating:   6%|▋         | 7/109 [00:00<00:14,  7.10it/s]11/28/2021 16:35:44 - INFO - __main__ -   Batch number = 8
Evaluating:   7%|▋         | 8/109 [00:01<00:14,  7.10it/s]11/28/2021 16:35:44 - INFO - __main__ -   Batch number = 9
Evaluating:   8%|▊         | 9/109 [00:01<00:14,  7.09it/s]11/28/2021 16:35:45 - INFO - __main__ -   Batch number = 10
Evaluating:   9%|▉         | 10/109 [00:01<00:13,  7.08it/s]11/28/2021 16:35:45 - INFO - __main__ -   Batch number = 11
Evaluating:  10%|█         | 11/109 [00:01<00:13,  7.10it/s]11/28/2021 16:35:45 - INFO - __main__ -   Batch number = 12
Evaluating:  11%|█         | 12/109 [00:01<00:13,  7.09it/s]11/28/2021 16:35:45 - INFO - __main__ -   Batch number = 13
Evaluating:  12%|█▏        | 13/109 [00:01<00:17,  5.52it/s]11/28/2021 16:35:45 - INFO - __main__ -   Batch number = 14
Evaluating:  13%|█▎        | 14/109 [00:02<00:16,  5.89it/s]11/28/2021 16:35:45 - INFO - __main__ -   Batch number = 15
Evaluating:  14%|█▍        | 15/109 [00:02<00:15,  6.19it/s]11/28/2021 16:35:46 - INFO - __main__ -   Batch number = 16
Evaluating:  15%|█▍        | 16/109 [00:02<00:14,  6.45it/s]11/28/2021 16:35:46 - INFO - __main__ -   Batch number = 17
Evaluating:  16%|█▌        | 17/109 [00:02<00:13,  6.63it/s]11/28/2021 16:35:46 - INFO - __main__ -   Batch number = 18
Evaluating:  17%|█▋        | 18/109 [00:02<00:13,  6.74it/s]11/28/2021 16:35:46 - INFO - __main__ -   Batch number = 19
Evaluating:  17%|█▋        | 19/109 [00:02<00:13,  6.85it/s]11/28/2021 16:35:46 - INFO - __main__ -   Batch number = 20
Evaluating:  18%|█▊        | 20/109 [00:02<00:12,  6.92it/s]11/28/2021 16:35:46 - INFO - __main__ -   Batch number = 21
Evaluating:  19%|█▉        | 21/109 [00:03<00:12,  6.96it/s]11/28/2021 16:35:46 - INFO - __main__ -   Batch number = 22
Evaluating:  20%|██        | 22/109 [00:03<00:12,  6.99it/s]11/28/2021 16:35:47 - INFO - __main__ -   Batch number = 23
Evaluating:  21%|██        | 23/109 [00:03<00:12,  7.02it/s]11/28/2021 16:35:47 - INFO - __main__ -   Batch number = 24
Evaluating:  22%|██▏       | 24/109 [00:03<00:12,  7.01it/s]11/28/2021 16:35:47 - INFO - __main__ -   Batch number = 25
Evaluating:  23%|██▎       | 25/109 [00:03<00:11,  7.02it/s]11/28/2021 16:35:47 - INFO - __main__ -   Batch number = 26
Evaluating:  24%|██▍       | 26/109 [00:03<00:11,  7.04it/s]11/28/2021 16:35:47 - INFO - __main__ -   Batch number = 27
Evaluating:  25%|██▍       | 27/109 [00:03<00:11,  7.02it/s]11/28/2021 16:35:47 - INFO - __main__ -   Batch number = 28
Evaluating:  26%|██▌       | 28/109 [00:04<00:11,  7.03it/s]11/28/2021 16:35:47 - INFO - __main__ -   Batch number = 29
Evaluating:  27%|██▋       | 29/109 [00:04<00:11,  7.03it/s]11/28/2021 16:35:48 - INFO - __main__ -   Batch number = 30
Evaluating:  28%|██▊       | 30/109 [00:04<00:11,  7.01it/s]11/28/2021 16:35:48 - INFO - __main__ -   Batch number = 31
Evaluating:  28%|██▊       | 31/109 [00:04<00:11,  7.02it/s]11/28/2021 16:35:48 - INFO - __main__ -   Batch number = 32
Evaluating:  29%|██▉       | 32/109 [00:04<00:10,  7.02it/s]11/28/2021 16:35:48 - INFO - __main__ -   Batch number = 33
Evaluating:  30%|███       | 33/109 [00:04<00:10,  7.02it/s]11/28/2021 16:35:48 - INFO - __main__ -   Batch number = 34
Evaluating:  31%|███       | 34/109 [00:04<00:10,  7.02it/s]11/28/2021 16:35:48 - INFO - __main__ -   Batch number = 35
Evaluating:  32%|███▏      | 35/109 [00:05<00:10,  7.02it/s]11/28/2021 16:35:48 - INFO - __main__ -   Batch number = 36
Evaluating:  33%|███▎      | 36/109 [00:05<00:10,  6.99it/s]11/28/2021 16:35:49 - INFO - __main__ -   Batch number = 37
Evaluating:  34%|███▍      | 37/109 [00:05<00:10,  6.99it/s]11/28/2021 16:35:49 - INFO - __main__ -   Batch number = 38
Evaluating:  35%|███▍      | 38/109 [00:05<00:10,  7.00it/s]11/28/2021 16:35:49 - INFO - __main__ -   Batch number = 39
Evaluating:  36%|███▌      | 39/109 [00:05<00:10,  6.98it/s]11/28/2021 16:35:49 - INFO - __main__ -   Batch number = 40
Evaluating:  37%|███▋      | 40/109 [00:05<00:09,  6.99it/s]11/28/2021 16:35:49 - INFO - __main__ -   Batch number = 41
Evaluating:  38%|███▊      | 41/109 [00:05<00:09,  7.00it/s]11/28/2021 16:35:49 - INFO - __main__ -   Batch number = 42
Evaluating:  39%|███▊      | 42/109 [00:06<00:09,  6.98it/s]11/28/2021 16:35:49 - INFO - __main__ -   Batch number = 43
Evaluating:  39%|███▉      | 43/109 [00:06<00:09,  6.98it/s]11/28/2021 16:35:50 - INFO - __main__ -   Batch number = 44
Evaluating:  40%|████      | 44/109 [00:06<00:09,  6.99it/s]11/28/2021 16:35:50 - INFO - __main__ -   Batch number = 45
Evaluating:  41%|████▏     | 45/109 [00:06<00:09,  6.98it/s]11/28/2021 16:35:50 - INFO - __main__ -   Batch number = 46
Evaluating:  42%|████▏     | 46/109 [00:06<00:09,  6.98it/s]11/28/2021 16:35:50 - INFO - __main__ -   Batch number = 47
Evaluating:  43%|████▎     | 47/109 [00:06<00:08,  6.99it/s]11/28/2021 16:35:50 - INFO - __main__ -   Batch number = 48
Evaluating:  44%|████▍     | 48/109 [00:06<00:08,  6.97it/s]11/28/2021 16:35:50 - INFO - __main__ -   Batch number = 49
Evaluating:  45%|████▍     | 49/109 [00:07<00:08,  6.96it/s]11/28/2021 16:35:50 - INFO - __main__ -   Batch number = 50
Evaluating:  46%|████▌     | 50/109 [00:07<00:08,  6.96it/s]11/28/2021 16:35:51 - INFO - __main__ -   Batch number = 51
Evaluating:  47%|████▋     | 51/109 [00:07<00:08,  6.95it/s]11/28/2021 16:35:51 - INFO - __main__ -   Batch number = 52
Evaluating:  48%|████▊     | 52/109 [00:07<00:08,  6.95it/s]11/28/2021 16:35:51 - INFO - __main__ -   Batch number = 53
Evaluating:  49%|████▊     | 53/109 [00:07<00:08,  6.95it/s]11/28/2021 16:35:51 - INFO - __main__ -   Batch number = 54
Evaluating:  50%|████▉     | 54/109 [00:07<00:07,  6.93it/s]11/28/2021 16:35:51 - INFO - __main__ -   Batch number = 55
Evaluating:  50%|█████     | 55/109 [00:07<00:07,  6.92it/s]11/28/2021 16:35:51 - INFO - __main__ -   Batch number = 56
Evaluating:  51%|█████▏    | 56/109 [00:08<00:07,  6.92it/s]11/28/2021 16:35:51 - INFO - __main__ -   Batch number = 57
Evaluating:  52%|█████▏    | 57/109 [00:08<00:07,  6.92it/s]11/28/2021 16:35:52 - INFO - __main__ -   Batch number = 58
Evaluating:  53%|█████▎    | 58/109 [00:08<00:07,  6.93it/s]11/28/2021 16:35:52 - INFO - __main__ -   Batch number = 59
Evaluating:  54%|█████▍    | 59/109 [00:08<00:07,  6.92it/s]11/28/2021 16:35:52 - INFO - __main__ -   Batch number = 60
Evaluating:  55%|█████▌    | 60/109 [00:08<00:07,  6.87it/s]11/28/2021 16:35:52 - INFO - __main__ -   Batch number = 61
Evaluating:  56%|█████▌    | 61/109 [00:08<00:07,  6.85it/s]11/28/2021 16:35:52 - INFO - __main__ -   Batch number = 62
Evaluating:  57%|█████▋    | 62/109 [00:08<00:06,  6.86it/s]11/28/2021 16:35:52 - INFO - __main__ -   Batch number = 63
Evaluating:  58%|█████▊    | 63/109 [00:09<00:06,  6.84it/s]11/28/2021 16:35:52 - INFO - __main__ -   Batch number = 64
Evaluating:  59%|█████▊    | 64/109 [00:09<00:06,  6.85it/s]11/28/2021 16:35:53 - INFO - __main__ -   Batch number = 65
Evaluating:  60%|█████▉    | 65/109 [00:09<00:06,  6.86it/s]11/28/2021 16:35:53 - INFO - __main__ -   Batch number = 66
Evaluating:  61%|██████    | 66/109 [00:09<00:06,  6.86it/s]11/28/2021 16:35:53 - INFO - __main__ -   Batch number = 67
Evaluating:  61%|██████▏   | 67/109 [00:09<00:06,  6.87it/s]11/28/2021 16:35:53 - INFO - __main__ -   Batch number = 68
Evaluating:  62%|██████▏   | 68/109 [00:09<00:05,  6.87it/s]11/28/2021 16:35:53 - INFO - __main__ -   Batch number = 69
Evaluating:  63%|██████▎   | 69/109 [00:10<00:05,  6.87it/s]11/28/2021 16:35:53 - INFO - __main__ -   Batch number = 70
Evaluating:  64%|██████▍   | 70/109 [00:10<00:05,  6.87it/s]11/28/2021 16:35:53 - INFO - __main__ -   Batch number = 71
Evaluating:  65%|██████▌   | 71/109 [00:10<00:05,  6.75it/s]11/28/2021 16:35:54 - INFO - __main__ -   Batch number = 72
Evaluating:  66%|██████▌   | 72/109 [00:10<00:05,  6.75it/s]11/28/2021 16:35:54 - INFO - __main__ -   Batch number = 73
Evaluating:  67%|██████▋   | 73/109 [00:10<00:05,  6.78it/s]11/28/2021 16:35:54 - INFO - __main__ -   Batch number = 74
Evaluating:  68%|██████▊   | 74/109 [00:10<00:05,  6.80it/s]11/28/2021 16:35:54 - INFO - __main__ -   Batch number = 75
Evaluating:  69%|██████▉   | 75/109 [00:10<00:04,  6.81it/s]11/28/2021 16:35:54 - INFO - __main__ -   Batch number = 76
Evaluating:  70%|██████▉   | 76/109 [00:11<00:04,  6.83it/s]11/28/2021 16:35:54 - INFO - __main__ -   Batch number = 77
Evaluating:  71%|███████   | 77/109 [00:11<00:04,  6.82it/s]11/28/2021 16:35:54 - INFO - __main__ -   Batch number = 78
Evaluating:  72%|███████▏  | 78/109 [00:11<00:04,  6.80it/s]11/28/2021 16:35:55 - INFO - __main__ -   Batch number = 79
Evaluating:  72%|███████▏  | 79/109 [00:11<00:04,  6.80it/s]11/28/2021 16:35:55 - INFO - __main__ -   Batch number = 80
Evaluating:  73%|███████▎  | 80/109 [00:11<00:04,  6.79it/s]11/28/2021 16:35:55 - INFO - __main__ -   Batch number = 81
Evaluating:  74%|███████▍  | 81/109 [00:11<00:04,  5.92it/s]11/28/2021 16:35:55 - INFO - __main__ -   Batch number = 82
Evaluating:  75%|███████▌  | 82/109 [00:12<00:04,  6.02it/s]11/28/2021 16:35:55 - INFO - __main__ -   Batch number = 83
Evaluating:  76%|███████▌  | 83/109 [00:12<00:04,  6.10it/s]11/28/2021 16:35:55 - INFO - __main__ -   Batch number = 84
Evaluating:  77%|███████▋  | 84/109 [00:12<00:04,  6.08it/s]11/28/2021 16:35:56 - INFO - __main__ -   Batch number = 85
Evaluating:  78%|███████▊  | 85/109 [00:12<00:03,  6.14it/s]11/28/2021 16:35:56 - INFO - __main__ -   Batch number = 86
Evaluating:  79%|███████▉  | 86/109 [00:12<00:03,  6.24it/s]11/28/2021 16:35:56 - INFO - __main__ -   Batch number = 87
Evaluating:  80%|███████▉  | 87/109 [00:12<00:03,  6.25it/s]11/28/2021 16:35:56 - INFO - __main__ -   Batch number = 88
Evaluating:  81%|████████  | 88/109 [00:12<00:03,  6.35it/s]11/28/2021 16:35:56 - INFO - __main__ -   Batch number = 89
Evaluating:  82%|████████▏ | 89/109 [00:13<00:03,  6.44it/s]11/28/2021 16:35:56 - INFO - __main__ -   Batch number = 90
Evaluating:  83%|████████▎ | 90/109 [00:13<00:02,  6.48it/s]11/28/2021 16:35:57 - INFO - __main__ -   Batch number = 91
Evaluating:  83%|████████▎ | 91/109 [00:13<00:02,  6.44it/s]11/28/2021 16:35:57 - INFO - __main__ -   Batch number = 92
Evaluating:  84%|████████▍ | 92/109 [00:13<00:02,  6.42it/s]11/28/2021 16:35:57 - INFO - __main__ -   Batch number = 93
Evaluating:  85%|████████▌ | 93/109 [00:13<00:02,  6.42it/s]11/28/2021 16:35:57 - INFO - __main__ -   Batch number = 94
Evaluating:  86%|████████▌ | 94/109 [00:13<00:02,  6.47it/s]11/28/2021 16:35:57 - INFO - __main__ -   Batch number = 95
Evaluating:  87%|████████▋ | 95/109 [00:14<00:02,  6.50it/s]11/28/2021 16:35:57 - INFO - __main__ -   Batch number = 96
Evaluating:  88%|████████▊ | 96/109 [00:14<00:01,  6.53it/s]11/28/2021 16:35:57 - INFO - __main__ -   Batch number = 97
Evaluating:  89%|████████▉ | 97/109 [00:14<00:01,  6.51it/s]11/28/2021 16:35:58 - INFO - __main__ -   Batch number = 98
Evaluating:  90%|████████▉ | 98/109 [00:14<00:01,  6.51it/s]11/28/2021 16:35:58 - INFO - __main__ -   Batch number = 99
Evaluating:  91%|█████████ | 99/109 [00:14<00:01,  6.48it/s]11/28/2021 16:35:58 - INFO - __main__ -   Batch number = 100
Evaluating:  92%|█████████▏| 100/109 [00:14<00:01,  6.48it/s]11/28/2021 16:35:58 - INFO - __main__ -   Batch number = 101
Evaluating:  93%|█████████▎| 101/109 [00:14<00:01,  6.49it/s]11/28/2021 16:35:58 - INFO - __main__ -   Batch number = 102
Evaluating:  94%|█████████▎| 102/109 [00:15<00:01,  6.49it/s]11/28/2021 16:35:58 - INFO - __main__ -   Batch number = 103
Evaluating:  94%|█████████▍| 103/109 [00:15<00:00,  6.42it/s]11/28/2021 16:35:59 - INFO - __main__ -   Batch number = 104
Evaluating:  95%|█████████▌| 104/109 [00:15<00:00,  6.34it/s]11/28/2021 16:35:59 - INFO - __main__ -   Batch number = 105
Evaluating:  96%|█████████▋| 105/109 [00:15<00:00,  6.34it/s]11/28/2021 16:35:59 - INFO - __main__ -   Batch number = 106
Evaluating:  97%|█████████▋| 106/109 [00:15<00:00,  6.36it/s]11/28/2021 16:35:59 - INFO - __main__ -   Batch number = 107
Evaluating:  98%|█████████▊| 107/109 [00:15<00:00,  6.39it/s]11/28/2021 16:35:59 - INFO - __main__ -   Batch number = 108
Evaluating:  99%|█████████▉| 108/109 [00:16<00:00,  6.41it/s]11/28/2021 16:35:59 - INFO - __main__ -   Batch number = 109
Evaluating: 100%|██████████| 109/109 [00:16<00:00,  6.77it/s]
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
11/28/2021 16:36:01 - INFO - __main__ -   ***** Evaluation result  in zh *****
11/28/2021 16:36:01 - INFO - __main__ -     f1 = 0.6215680114885788
11/28/2021 16:36:01 - INFO - __main__ -     loss = 1.4507080710262334
11/28/2021 16:36:01 - INFO - __main__ -     precision = 0.6283868391920585
11/28/2021 16:36:01 - INFO - __main__ -     recall = 0.6148955817587726
25.89user 8.34system 0:33.33elapsed 102%CPU (0avgtext+0avgdata 3989180maxresident)k
0inputs+728outputs (0major+1328692minor)pagefaults 0swaps
