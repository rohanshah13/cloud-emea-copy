01/14/2022 16:47:41 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_en,hi,ar_ensemble_hi_task/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mr,bho,ta', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_en,hi,ar_ensemble_hi_task//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLanghi_hi_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:47:41 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/14/2022 16:47:41 - INFO - __main__ -   Seed = 1
01/14/2022 16:47:41 - INFO - root -   save model
01/14/2022 16:47:41 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_en,hi,ar_ensemble_hi_task/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mr,bho,ta', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_en,hi,ar_ensemble_hi_task//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLanghi_hi_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:47:41 - INFO - __main__ -   Loading pretrained model and tokenizer
01/14/2022 16:47:43 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/14/2022 16:47:49 - INFO - __main__ -   Using lang2id = None
01/14/2022 16:47:49 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/14/2022 16:47:49 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLanghi_hi_s1/checkpoint-best/udpos/
01/14/2022 16:47:49 - INFO - root -   Trying to decide if add adapter
01/14/2022 16:47:49 - INFO - root -   loading task adapter
01/14/2022 16:47:49 - INFO - root -   loading lang adpater en/wiki@ukp,hi/wiki@ukp,ar/wiki@ukp
01/14/2022 16:47:49 - INFO - __main__ -   Adapter Languages : ['en', 'hi', 'ar'], Length : 3
01/14/2022 16:47:49 - INFO - __main__ -   Adapter Names ['en/wiki@ukp', 'hi/wiki@ukp', 'ar/wiki@ukp'], Length : 3
01/14/2022 16:47:49 - INFO - __main__ -   Language = en
01/14/2022 16:47:49 - INFO - __main__ -   Adapter Name = en/wiki@ukp
01/14/2022 16:47:51 - INFO - __main__ -   Language = hi
01/14/2022 16:47:51 - INFO - __main__ -   Adapter Name = hi/wiki@ukp
01/14/2022 16:47:52 - INFO - __main__ -   Language = ar
01/14/2022 16:47:52 - INFO - __main__ -   Adapter Name = ar/wiki@ukp
01/14/2022 16:47:56 - INFO - __main__ -   Args Adapter Weight = 0.33,0.33,0.33
01/14/2022 16:47:56 - INFO - __main__ -   Adapter Languages = ['en', 'hi', 'ar']
01/14/2022 16:47:56 - INFO - __main__ -   Adapter Weights = [0.33, 0.33, 0.33]
01/14/2022 16:47:56 - INFO - __main__ -   Sum of Adapter Weights = 0.99
01/14/2022 16:47:56 - INFO - __main__ -   Length of Adapter Weights = 3
01/14/2022 16:47:56 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/cached_test_mr_bert-base-multilingual-cased_128
01/14/2022 16:47:56 - INFO - __main__ -   ***** Running evaluation  in mr *****
01/14/2022 16:47:56 - INFO - __main__ -     Num examples = 47
01/14/2022 16:47:56 - INFO - __main__ -     Batch size = 32
01/14/2022 16:47:56 - INFO - __main__ -   Batch number = 1
01/14/2022 16:47:56 - INFO - __main__ -   Batch number = 2
01/14/2022 16:47:56 - INFO - __main__ -   ***** Evaluation result  in mr *****
01/14/2022 16:47:56 - INFO - __main__ -     f1 = 0.6050156739811913
01/14/2022 16:47:56 - INFO - __main__ -     loss = 1.5978148579597473
01/14/2022 16:47:56 - INFO - __main__ -     precision = 0.6327868852459017
01/14/2022 16:47:56 - INFO - __main__ -     recall = 0.5795795795795796
01/14/2022 16:47:56 - INFO - __main__ -   Args Adapter Weight = 0.33,0.33,0.33
01/14/2022 16:47:56 - INFO - __main__ -   Adapter Languages = ['en', 'hi', 'ar']
01/14/2022 16:47:56 - INFO - __main__ -   Adapter Weights = [0.33, 0.33, 0.33]
01/14/2022 16:47:56 - INFO - __main__ -   Sum of Adapter Weights = 0.99
01/14/2022 16:47:56 - INFO - __main__ -   Length of Adapter Weights = 3
01/14/2022 16:47:56 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/cached_test_bho_bert-base-multilingual-cased_128
01/14/2022 16:47:56 - INFO - __main__ -   ***** Running evaluation  in bho *****
01/14/2022 16:47:56 - INFO - __main__ -     Num examples = 361
01/14/2022 16:47:56 - INFO - __main__ -     Batch size = 32
01/14/2022 16:47:56 - INFO - __main__ -   Batch number = 1
01/14/2022 16:47:56 - INFO - __main__ -   Batch number = 2
01/14/2022 16:47:57 - INFO - __main__ -   Batch number = 3
01/14/2022 16:47:57 - INFO - __main__ -   Batch number = 4
01/14/2022 16:47:57 - INFO - __main__ -   Batch number = 5
01/14/2022 16:47:57 - INFO - __main__ -   Batch number = 6
01/14/2022 16:47:57 - INFO - __main__ -   Batch number = 7
01/14/2022 16:47:57 - INFO - __main__ -   Batch number = 8
01/14/2022 16:47:58 - INFO - __main__ -   Batch number = 9
01/14/2022 16:47:58 - INFO - __main__ -   Batch number = 10
01/14/2022 16:47:58 - INFO - __main__ -   Batch number = 11
01/14/2022 16:47:58 - INFO - __main__ -   Batch number = 12
01/14/2022 16:47:58 - INFO - __main__ -   ***** Evaluation result  in bho *****
01/14/2022 16:47:58 - INFO - __main__ -     f1 = 0.5386275356063875
01/14/2022 16:47:58 - INFO - __main__ -     loss = 2.8721321523189545
01/14/2022 16:47:58 - INFO - __main__ -     precision = 0.5611510791366906
01/14/2022 16:47:58 - INFO - __main__ -     recall = 0.5178423236514523
01/14/2022 16:47:58 - INFO - __main__ -   Args Adapter Weight = 0.33,0.33,0.33
01/14/2022 16:47:58 - INFO - __main__ -   Adapter Languages = ['en', 'hi', 'ar']
01/14/2022 16:47:58 - INFO - __main__ -   Adapter Weights = [0.33, 0.33, 0.33]
01/14/2022 16:47:58 - INFO - __main__ -   Sum of Adapter Weights = 0.99
01/14/2022 16:47:58 - INFO - __main__ -   Length of Adapter Weights = 3
01/14/2022 16:47:58 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/cached_test_ta_bert-base-multilingual-cased_128
01/14/2022 16:47:58 - INFO - __main__ -   ***** Running evaluation  in ta *****
01/14/2022 16:47:58 - INFO - __main__ -     Num examples = 656
01/14/2022 16:47:58 - INFO - __main__ -     Batch size = 32
01/14/2022 16:47:58 - INFO - __main__ -   Batch number = 1
01/14/2022 16:47:59 - INFO - __main__ -   Batch number = 2
01/14/2022 16:47:59 - INFO - __main__ -   Batch number = 3
01/14/2022 16:47:59 - INFO - __main__ -   Batch number = 4
01/14/2022 16:47:59 - INFO - __main__ -   Batch number = 5
01/14/2022 16:47:59 - INFO - __main__ -   Batch number = 6
01/14/2022 16:47:59 - INFO - __main__ -   Batch number = 7
01/14/2022 16:48:00 - INFO - __main__ -   Batch number = 8
01/14/2022 16:48:00 - INFO - __main__ -   Batch number = 9
01/14/2022 16:48:00 - INFO - __main__ -   Batch number = 10
01/14/2022 16:48:00 - INFO - __main__ -   Batch number = 11
01/14/2022 16:48:00 - INFO - __main__ -   Batch number = 12
01/14/2022 16:48:00 - INFO - __main__ -   Batch number = 13
01/14/2022 16:48:01 - INFO - __main__ -   Batch number = 14
01/14/2022 16:48:01 - INFO - __main__ -   Batch number = 15
01/14/2022 16:48:01 - INFO - __main__ -   Batch number = 16
01/14/2022 16:48:01 - INFO - __main__ -   Batch number = 17
01/14/2022 16:48:01 - INFO - __main__ -   Batch number = 18
01/14/2022 16:48:01 - INFO - __main__ -   Batch number = 19
01/14/2022 16:48:02 - INFO - __main__ -   Batch number = 20
01/14/2022 16:48:02 - INFO - __main__ -   Batch number = 21
01/14/2022 16:48:02 - INFO - __main__ -   ***** Evaluation result  in ta *****
01/14/2022 16:48:02 - INFO - __main__ -     f1 = 0.6546714647474343
01/14/2022 16:48:02 - INFO - __main__ -     loss = 1.3508239729063851
01/14/2022 16:48:02 - INFO - __main__ -     precision = 0.6812760055478502
01/14/2022 16:48:02 - INFO - __main__ -     recall = 0.6300667008722421
01/14/2022 16:48:04 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_en,hi,ar_ensemble_hi_task/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mr,bho,ta', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_en,hi,ar_ensemble_hi_task//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLanghi_hi_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:48:04 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/14/2022 16:48:04 - INFO - __main__ -   Seed = 2
01/14/2022 16:48:04 - INFO - root -   save model
01/14/2022 16:48:04 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_en,hi,ar_ensemble_hi_task/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mr,bho,ta', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_en,hi,ar_ensemble_hi_task//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLanghi_hi_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:48:04 - INFO - __main__ -   Loading pretrained model and tokenizer
01/14/2022 16:48:07 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/14/2022 16:48:12 - INFO - __main__ -   Using lang2id = None
01/14/2022 16:48:12 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/14/2022 16:48:12 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLanghi_hi_s2/checkpoint-best/udpos/
01/14/2022 16:48:12 - INFO - root -   Trying to decide if add adapter
01/14/2022 16:48:12 - INFO - root -   loading task adapter
01/14/2022 16:48:12 - INFO - root -   loading lang adpater en/wiki@ukp,hi/wiki@ukp,ar/wiki@ukp
01/14/2022 16:48:12 - INFO - __main__ -   Adapter Languages : ['en', 'hi', 'ar'], Length : 3
01/14/2022 16:48:12 - INFO - __main__ -   Adapter Names ['en/wiki@ukp', 'hi/wiki@ukp', 'ar/wiki@ukp'], Length : 3
01/14/2022 16:48:12 - INFO - __main__ -   Language = en
01/14/2022 16:48:12 - INFO - __main__ -   Adapter Name = en/wiki@ukp
01/14/2022 16:48:13 - INFO - __main__ -   Language = hi
01/14/2022 16:48:13 - INFO - __main__ -   Adapter Name = hi/wiki@ukp
01/14/2022 16:48:14 - INFO - __main__ -   Language = ar
01/14/2022 16:48:14 - INFO - __main__ -   Adapter Name = ar/wiki@ukp
01/14/2022 16:48:18 - INFO - __main__ -   Args Adapter Weight = 0.33,0.33,0.33
01/14/2022 16:48:18 - INFO - __main__ -   Adapter Languages = ['en', 'hi', 'ar']
01/14/2022 16:48:18 - INFO - __main__ -   Adapter Weights = [0.33, 0.33, 0.33]
01/14/2022 16:48:18 - INFO - __main__ -   Sum of Adapter Weights = 0.99
01/14/2022 16:48:18 - INFO - __main__ -   Length of Adapter Weights = 3
01/14/2022 16:48:18 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/cached_test_mr_bert-base-multilingual-cased_128
01/14/2022 16:48:18 - INFO - __main__ -   ***** Running evaluation  in mr *****
01/14/2022 16:48:18 - INFO - __main__ -     Num examples = 47
01/14/2022 16:48:18 - INFO - __main__ -     Batch size = 32
01/14/2022 16:48:18 - INFO - __main__ -   Batch number = 1
01/14/2022 16:48:18 - INFO - __main__ -   Batch number = 2
01/14/2022 16:48:18 - INFO - __main__ -   ***** Evaluation result  in mr *****
01/14/2022 16:48:18 - INFO - __main__ -     f1 = 0.6420890937019968
01/14/2022 16:48:18 - INFO - __main__ -     loss = 1.3851885199546814
01/14/2022 16:48:18 - INFO - __main__ -     precision = 0.6572327044025157
01/14/2022 16:48:18 - INFO - __main__ -     recall = 0.6276276276276276
01/14/2022 16:48:18 - INFO - __main__ -   Args Adapter Weight = 0.33,0.33,0.33
01/14/2022 16:48:18 - INFO - __main__ -   Adapter Languages = ['en', 'hi', 'ar']
01/14/2022 16:48:18 - INFO - __main__ -   Adapter Weights = [0.33, 0.33, 0.33]
01/14/2022 16:48:18 - INFO - __main__ -   Sum of Adapter Weights = 0.99
01/14/2022 16:48:18 - INFO - __main__ -   Length of Adapter Weights = 3
01/14/2022 16:48:18 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/cached_test_bho_bert-base-multilingual-cased_128
01/14/2022 16:48:18 - INFO - __main__ -   ***** Running evaluation  in bho *****
01/14/2022 16:48:18 - INFO - __main__ -     Num examples = 361
01/14/2022 16:48:18 - INFO - __main__ -     Batch size = 32
01/14/2022 16:48:18 - INFO - __main__ -   Batch number = 1
01/14/2022 16:48:18 - INFO - __main__ -   Batch number = 2
01/14/2022 16:48:18 - INFO - __main__ -   Batch number = 3
01/14/2022 16:48:19 - INFO - __main__ -   Batch number = 4
01/14/2022 16:48:19 - INFO - __main__ -   Batch number = 5
01/14/2022 16:48:19 - INFO - __main__ -   Batch number = 6
01/14/2022 16:48:19 - INFO - __main__ -   Batch number = 7
01/14/2022 16:48:19 - INFO - __main__ -   Batch number = 8
01/14/2022 16:48:19 - INFO - __main__ -   Batch number = 9
01/14/2022 16:48:20 - INFO - __main__ -   Batch number = 10
01/14/2022 16:48:20 - INFO - __main__ -   Batch number = 11
01/14/2022 16:48:20 - INFO - __main__ -   Batch number = 12
01/14/2022 16:48:20 - INFO - __main__ -   ***** Evaluation result  in bho *****
01/14/2022 16:48:20 - INFO - __main__ -     f1 = 0.5387875118913777
01/14/2022 16:48:20 - INFO - __main__ -     loss = 2.585423002640406
01/14/2022 16:48:20 - INFO - __main__ -     precision = 0.5624774286746118
01/14/2022 16:48:20 - INFO - __main__ -     recall = 0.5170124481327801
01/14/2022 16:48:20 - INFO - __main__ -   Args Adapter Weight = 0.33,0.33,0.33
01/14/2022 16:48:20 - INFO - __main__ -   Adapter Languages = ['en', 'hi', 'ar']
01/14/2022 16:48:20 - INFO - __main__ -   Adapter Weights = [0.33, 0.33, 0.33]
01/14/2022 16:48:20 - INFO - __main__ -   Sum of Adapter Weights = 0.99
01/14/2022 16:48:20 - INFO - __main__ -   Length of Adapter Weights = 3
01/14/2022 16:48:20 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/cached_test_ta_bert-base-multilingual-cased_128
01/14/2022 16:48:20 - INFO - __main__ -   ***** Running evaluation  in ta *****
01/14/2022 16:48:20 - INFO - __main__ -     Num examples = 656
01/14/2022 16:48:20 - INFO - __main__ -     Batch size = 32
01/14/2022 16:48:20 - INFO - __main__ -   Batch number = 1
01/14/2022 16:48:20 - INFO - __main__ -   Batch number = 2
01/14/2022 16:48:21 - INFO - __main__ -   Batch number = 3
01/14/2022 16:48:21 - INFO - __main__ -   Batch number = 4
01/14/2022 16:48:21 - INFO - __main__ -   Batch number = 5
01/14/2022 16:48:21 - INFO - __main__ -   Batch number = 6
01/14/2022 16:48:21 - INFO - __main__ -   Batch number = 7
01/14/2022 16:48:21 - INFO - __main__ -   Batch number = 8
01/14/2022 16:48:22 - INFO - __main__ -   Batch number = 9
01/14/2022 16:48:22 - INFO - __main__ -   Batch number = 10
01/14/2022 16:48:22 - INFO - __main__ -   Batch number = 11
01/14/2022 16:48:22 - INFO - __main__ -   Batch number = 12
01/14/2022 16:48:22 - INFO - __main__ -   Batch number = 13
01/14/2022 16:48:22 - INFO - __main__ -   Batch number = 14
01/14/2022 16:48:23 - INFO - __main__ -   Batch number = 15
01/14/2022 16:48:23 - INFO - __main__ -   Batch number = 16
01/14/2022 16:48:23 - INFO - __main__ -   Batch number = 17
01/14/2022 16:48:23 - INFO - __main__ -   Batch number = 18
01/14/2022 16:48:23 - INFO - __main__ -   Batch number = 19
01/14/2022 16:48:23 - INFO - __main__ -   Batch number = 20
01/14/2022 16:48:24 - INFO - __main__ -   Batch number = 21
01/14/2022 16:48:24 - INFO - __main__ -   ***** Evaluation result  in ta *****
01/14/2022 16:48:24 - INFO - __main__ -     f1 = 0.6785478547854786
01/14/2022 16:48:24 - INFO - __main__ -     loss = 1.1059226975554513
01/14/2022 16:48:24 - INFO - __main__ -     precision = 0.6989393527332064
01/14/2022 16:48:24 - INFO - __main__ -     recall = 0.6593124679322729
01/14/2022 16:48:26 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_en,hi,ar_ensemble_hi_task/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mr,bho,ta', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_en,hi,ar_ensemble_hi_task//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLanghi_hi_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:48:26 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/14/2022 16:48:26 - INFO - __main__ -   Seed = 3
01/14/2022 16:48:26 - INFO - root -   save model
01/14/2022 16:48:26 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_en,hi,ar_ensemble_hi_task/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mr,bho,ta', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_en,hi,ar_ensemble_hi_task//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLanghi_hi_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/14/2022 16:48:26 - INFO - __main__ -   Loading pretrained model and tokenizer
01/14/2022 16:48:29 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/14/2022 16:48:34 - INFO - __main__ -   Using lang2id = None
01/14/2022 16:48:34 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/14/2022 16:48:34 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLanghi_hi_s3/checkpoint-best/udpos/
01/14/2022 16:48:34 - INFO - root -   Trying to decide if add adapter
01/14/2022 16:48:34 - INFO - root -   loading task adapter
01/14/2022 16:48:35 - INFO - root -   loading lang adpater en/wiki@ukp,hi/wiki@ukp,ar/wiki@ukp
01/14/2022 16:48:35 - INFO - __main__ -   Adapter Languages : ['en', 'hi', 'ar'], Length : 3
01/14/2022 16:48:35 - INFO - __main__ -   Adapter Names ['en/wiki@ukp', 'hi/wiki@ukp', 'ar/wiki@ukp'], Length : 3
01/14/2022 16:48:35 - INFO - __main__ -   Language = en
01/14/2022 16:48:35 - INFO - __main__ -   Adapter Name = en/wiki@ukp
01/14/2022 16:48:35 - INFO - __main__ -   Language = hi
01/14/2022 16:48:35 - INFO - __main__ -   Adapter Name = hi/wiki@ukp
01/14/2022 16:48:36 - INFO - __main__ -   Language = ar
01/14/2022 16:48:36 - INFO - __main__ -   Adapter Name = ar/wiki@ukp
01/14/2022 16:48:41 - INFO - __main__ -   Args Adapter Weight = 0.33,0.33,0.33
01/14/2022 16:48:41 - INFO - __main__ -   Adapter Languages = ['en', 'hi', 'ar']
01/14/2022 16:48:41 - INFO - __main__ -   Adapter Weights = [0.33, 0.33, 0.33]
01/14/2022 16:48:41 - INFO - __main__ -   Sum of Adapter Weights = 0.99
01/14/2022 16:48:41 - INFO - __main__ -   Length of Adapter Weights = 3
01/14/2022 16:48:41 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/cached_test_mr_bert-base-multilingual-cased_128
01/14/2022 16:48:41 - INFO - __main__ -   ***** Running evaluation  in mr *****
01/14/2022 16:48:41 - INFO - __main__ -     Num examples = 47
01/14/2022 16:48:41 - INFO - __main__ -     Batch size = 32
01/14/2022 16:48:41 - INFO - __main__ -   Batch number = 1
01/14/2022 16:48:41 - INFO - __main__ -   Batch number = 2
01/14/2022 16:48:41 - INFO - __main__ -   ***** Evaluation result  in mr *****
01/14/2022 16:48:41 - INFO - __main__ -     f1 = 0.662557781201849
01/14/2022 16:48:41 - INFO - __main__ -     loss = 1.460173785686493
01/14/2022 16:48:41 - INFO - __main__ -     precision = 0.680379746835443
01/14/2022 16:48:41 - INFO - __main__ -     recall = 0.6456456456456456
01/14/2022 16:48:41 - INFO - __main__ -   Args Adapter Weight = 0.33,0.33,0.33
01/14/2022 16:48:41 - INFO - __main__ -   Adapter Languages = ['en', 'hi', 'ar']
01/14/2022 16:48:41 - INFO - __main__ -   Adapter Weights = [0.33, 0.33, 0.33]
01/14/2022 16:48:41 - INFO - __main__ -   Sum of Adapter Weights = 0.99
01/14/2022 16:48:41 - INFO - __main__ -   Length of Adapter Weights = 3
01/14/2022 16:48:41 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/cached_test_bho_bert-base-multilingual-cased_128
01/14/2022 16:48:41 - INFO - __main__ -   ***** Running evaluation  in bho *****
01/14/2022 16:48:41 - INFO - __main__ -     Num examples = 361
01/14/2022 16:48:41 - INFO - __main__ -     Batch size = 32
01/14/2022 16:48:41 - INFO - __main__ -   Batch number = 1
01/14/2022 16:48:41 - INFO - __main__ -   Batch number = 2
01/14/2022 16:48:41 - INFO - __main__ -   Batch number = 3
01/14/2022 16:48:42 - INFO - __main__ -   Batch number = 4
01/14/2022 16:48:42 - INFO - __main__ -   Batch number = 5
01/14/2022 16:48:42 - INFO - __main__ -   Batch number = 6
01/14/2022 16:48:42 - INFO - __main__ -   Batch number = 7
01/14/2022 16:48:42 - INFO - __main__ -   Batch number = 8
01/14/2022 16:48:42 - INFO - __main__ -   Batch number = 9
01/14/2022 16:48:43 - INFO - __main__ -   Batch number = 10
01/14/2022 16:48:43 - INFO - __main__ -   Batch number = 11
01/14/2022 16:48:43 - INFO - __main__ -   Batch number = 12
01/14/2022 16:48:43 - INFO - __main__ -   ***** Evaluation result  in bho *****
01/14/2022 16:48:43 - INFO - __main__ -     f1 = 0.5357971637301248
01/14/2022 16:48:43 - INFO - __main__ -     loss = 2.749474287033081
01/14/2022 16:48:43 - INFO - __main__ -     precision = 0.5556149732620321
01/14/2022 16:48:43 - INFO - __main__ -     recall = 0.5173443983402489
01/14/2022 16:48:43 - INFO - __main__ -   Args Adapter Weight = 0.33,0.33,0.33
01/14/2022 16:48:43 - INFO - __main__ -   Adapter Languages = ['en', 'hi', 'ar']
01/14/2022 16:48:43 - INFO - __main__ -   Adapter Weights = [0.33, 0.33, 0.33]
01/14/2022 16:48:43 - INFO - __main__ -   Sum of Adapter Weights = 0.99
01/14/2022 16:48:43 - INFO - __main__ -   Length of Adapter Weights = 3
01/14/2022 16:48:43 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/cached_test_ta_bert-base-multilingual-cased_128
01/14/2022 16:48:43 - INFO - __main__ -   ***** Running evaluation  in ta *****
01/14/2022 16:48:43 - INFO - __main__ -     Num examples = 656
01/14/2022 16:48:43 - INFO - __main__ -     Batch size = 32
01/14/2022 16:48:43 - INFO - __main__ -   Batch number = 1
01/14/2022 16:48:43 - INFO - __main__ -   Batch number = 2
01/14/2022 16:48:44 - INFO - __main__ -   Batch number = 3
01/14/2022 16:48:44 - INFO - __main__ -   Batch number = 4
01/14/2022 16:48:44 - INFO - __main__ -   Batch number = 5
01/14/2022 16:48:44 - INFO - __main__ -   Batch number = 6
01/14/2022 16:48:44 - INFO - __main__ -   Batch number = 7
01/14/2022 16:48:44 - INFO - __main__ -   Batch number = 8
01/14/2022 16:48:45 - INFO - __main__ -   Batch number = 9
01/14/2022 16:48:45 - INFO - __main__ -   Batch number = 10
01/14/2022 16:48:45 - INFO - __main__ -   Batch number = 11
01/14/2022 16:48:45 - INFO - __main__ -   Batch number = 12
01/14/2022 16:48:45 - INFO - __main__ -   Batch number = 13
01/14/2022 16:48:45 - INFO - __main__ -   Batch number = 14
01/14/2022 16:48:45 - INFO - __main__ -   Batch number = 15
01/14/2022 16:48:46 - INFO - __main__ -   Batch number = 16
01/14/2022 16:48:46 - INFO - __main__ -   Batch number = 17
01/14/2022 16:48:46 - INFO - __main__ -   Batch number = 18
01/14/2022 16:48:46 - INFO - __main__ -   Batch number = 19
01/14/2022 16:48:46 - INFO - __main__ -   Batch number = 20
01/14/2022 16:48:47 - INFO - __main__ -   Batch number = 21
01/14/2022 16:48:47 - INFO - __main__ -   ***** Evaluation result  in ta *****
01/14/2022 16:48:47 - INFO - __main__ -     f1 = 0.7026604068857589
01/14/2022 16:48:47 - INFO - __main__ -     loss = 0.998492792958305
01/14/2022 16:48:47 - INFO - __main__ -     precision = 0.7145888594164457
01/14/2022 16:48:47 - INFO - __main__ -     recall = 0.6911236531554643
