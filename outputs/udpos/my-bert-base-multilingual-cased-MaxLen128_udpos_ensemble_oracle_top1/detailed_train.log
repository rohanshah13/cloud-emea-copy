PyTorch version 1.10.0+cu102 available.
usage: my_run_tag.py [-h] --model_name_or_path MODEL_NAME_OR_PATH
                     [--model_type MODEL_TYPE] [--config_name CONFIG_NAME]
                     [--tokenizer_name TOKENIZER_NAME] [--cache_dir CACHE_DIR]
                     [--labels LABELS] [--data_dir DATA_DIR]
                     [--output_dir OUTPUT_DIR]
                     [--max_seq_length MAX_SEQ_LENGTH] [--do_train]
                     [--do_eval] [--do_predict] [--do_adapter_predict]
                     [--do_predict_dev] [--do_predict_train]
                     [--init_checkpoint INIT_CHECKPOINT]
                     [--evaluate_during_training] [--do_lower_case]
                     [--few_shot FEW_SHOT]
                     [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]
                     [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]
                     [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]
                     [--learning_rate LEARNING_RATE]
                     [--weight_decay WEIGHT_DECAY]
                     [--adam_epsilon ADAM_EPSILON]
                     [--max_grad_norm MAX_GRAD_NORM]
                     [--num_train_epochs NUM_TRAIN_EPOCHS]
                     [--max_steps MAX_STEPS] [--save_steps SAVE_STEPS]
                     [--warmup_steps WARMUP_STEPS]
                     [--logging_steps LOGGING_STEPS]
                     [--save_only_best_checkpoint] [--eval_all_checkpoints]
                     [--no_cuda] [--overwrite_output_dir] [--overwrite_cache]
                     [--seed SEED] [--fp16] [--fp16_opt_level FP16_OPT_LEVEL]
                     [--local_rank LOCAL_RANK] [--server_ip SERVER_IP]
                     [--server_port SERVER_PORT]
                     [--predict_langs PREDICT_LANGS]
                     [--train_langs TRAIN_LANGS] [--log_file LOG_FILE]
                     [--eval_patience EVAL_PATIENCE]
                     [--bpe_dropout BPE_DROPOUT] [--do_save_adapter_fusions]
                     [--task_name TASK_NAME]
                     [--predict_task_adapter PREDICT_TASK_ADAPTER]
                     [--predict_lang_adapter PREDICT_LANG_ADAPTER]
                     [--test_adapter] [--adapter_weight ADAPTER_WEIGHT]
                     [--lang_to_vec LANG_TO_VEC]
                     [--calc_weight_step CALC_WEIGHT_STEP]
                     [--predict_save_prefix PREDICT_SAVE_PREFIX]
                     [--en_weight EN_WEIGHT] [--temperature TEMPERATURE]
                     [--get_attr] [--topk TOPK] [--task TASK]
                     [--train_adapter] [--load_adapter LOAD_ADAPTER]
                     [--adapter_config ADAPTER_CONFIG]
                     [--adapter_non_linearity ADAPTER_NON_LINEARITY]
                     [--adapter_reduction_factor ADAPTER_REDUCTION_FACTOR]
                     [--language LANGUAGE]
                     [--load_lang_adapter LOAD_LANG_ADAPTER]
                     [--lang_adapter_config LANG_ADAPTER_CONFIG]
                     [--lang_adapter_non_linearity LANG_ADAPTER_NON_LINEARITY]
                     [--lang_adapter_reduction_factor LANG_ADAPTER_REDUCTION_FACTOR]
my_run_tag.py: error: argument --load_lang_adapter: expected one argument
Command exited with non-zero status 2
2.70user 1.82system 0:02.13elapsed 212%CPU (0avgtext+0avgdata 326104maxresident)k
0inputs+16outputs (0major+79328minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
usage: my_run_tag.py [-h] --model_name_or_path MODEL_NAME_OR_PATH
                     [--model_type MODEL_TYPE] [--config_name CONFIG_NAME]
                     [--tokenizer_name TOKENIZER_NAME] [--cache_dir CACHE_DIR]
                     [--labels LABELS] [--data_dir DATA_DIR]
                     [--output_dir OUTPUT_DIR]
                     [--max_seq_length MAX_SEQ_LENGTH] [--do_train]
                     [--do_eval] [--do_predict] [--do_adapter_predict]
                     [--do_predict_dev] [--do_predict_train]
                     [--init_checkpoint INIT_CHECKPOINT]
                     [--evaluate_during_training] [--do_lower_case]
                     [--few_shot FEW_SHOT]
                     [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]
                     [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]
                     [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]
                     [--learning_rate LEARNING_RATE]
                     [--weight_decay WEIGHT_DECAY]
                     [--adam_epsilon ADAM_EPSILON]
                     [--max_grad_norm MAX_GRAD_NORM]
                     [--num_train_epochs NUM_TRAIN_EPOCHS]
                     [--max_steps MAX_STEPS] [--save_steps SAVE_STEPS]
                     [--warmup_steps WARMUP_STEPS]
                     [--logging_steps LOGGING_STEPS]
                     [--save_only_best_checkpoint] [--eval_all_checkpoints]
                     [--no_cuda] [--overwrite_output_dir] [--overwrite_cache]
                     [--seed SEED] [--fp16] [--fp16_opt_level FP16_OPT_LEVEL]
                     [--local_rank LOCAL_RANK] [--server_ip SERVER_IP]
                     [--server_port SERVER_PORT]
                     [--predict_langs PREDICT_LANGS]
                     [--train_langs TRAIN_LANGS] [--log_file LOG_FILE]
                     [--eval_patience EVAL_PATIENCE]
                     [--bpe_dropout BPE_DROPOUT] [--do_save_adapter_fusions]
                     [--task_name TASK_NAME]
                     [--predict_task_adapter PREDICT_TASK_ADAPTER]
                     [--predict_lang_adapter PREDICT_LANG_ADAPTER]
                     [--test_adapter] [--adapter_weight ADAPTER_WEIGHT]
                     [--lang_to_vec LANG_TO_VEC]
                     [--calc_weight_step CALC_WEIGHT_STEP]
                     [--predict_save_prefix PREDICT_SAVE_PREFIX]
                     [--en_weight EN_WEIGHT] [--temperature TEMPERATURE]
                     [--get_attr] [--topk TOPK] [--task TASK]
                     [--train_adapter] [--load_adapter LOAD_ADAPTER]
                     [--adapter_config ADAPTER_CONFIG]
                     [--adapter_non_linearity ADAPTER_NON_LINEARITY]
                     [--adapter_reduction_factor ADAPTER_REDUCTION_FACTOR]
                     [--language LANGUAGE]
                     [--load_lang_adapter LOAD_LANG_ADAPTER]
                     [--lang_adapter_config LANG_ADAPTER_CONFIG]
                     [--lang_adapter_non_linearity LANG_ADAPTER_NON_LINEARITY]
                     [--lang_adapter_reduction_factor LANG_ADAPTER_REDUCTION_FACTOR]
my_run_tag.py: error: argument --load_lang_adapter: expected one argument
Command exited with non-zero status 2
2.29user 1.64system 0:01.97elapsed 199%CPU (0avgtext+0avgdata 325300maxresident)k
0inputs+16outputs (0major+79339minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
usage: my_run_tag.py [-h] --model_name_or_path MODEL_NAME_OR_PATH
                     [--model_type MODEL_TYPE] [--config_name CONFIG_NAME]
                     [--tokenizer_name TOKENIZER_NAME] [--cache_dir CACHE_DIR]
                     [--labels LABELS] [--data_dir DATA_DIR]
                     [--output_dir OUTPUT_DIR]
                     [--max_seq_length MAX_SEQ_LENGTH] [--do_train]
                     [--do_eval] [--do_predict] [--do_adapter_predict]
                     [--do_predict_dev] [--do_predict_train]
                     [--init_checkpoint INIT_CHECKPOINT]
                     [--evaluate_during_training] [--do_lower_case]
                     [--few_shot FEW_SHOT]
                     [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]
                     [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]
                     [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]
                     [--learning_rate LEARNING_RATE]
                     [--weight_decay WEIGHT_DECAY]
                     [--adam_epsilon ADAM_EPSILON]
                     [--max_grad_norm MAX_GRAD_NORM]
                     [--num_train_epochs NUM_TRAIN_EPOCHS]
                     [--max_steps MAX_STEPS] [--save_steps SAVE_STEPS]
                     [--warmup_steps WARMUP_STEPS]
                     [--logging_steps LOGGING_STEPS]
                     [--save_only_best_checkpoint] [--eval_all_checkpoints]
                     [--no_cuda] [--overwrite_output_dir] [--overwrite_cache]
                     [--seed SEED] [--fp16] [--fp16_opt_level FP16_OPT_LEVEL]
                     [--local_rank LOCAL_RANK] [--server_ip SERVER_IP]
                     [--server_port SERVER_PORT]
                     [--predict_langs PREDICT_LANGS]
                     [--train_langs TRAIN_LANGS] [--log_file LOG_FILE]
                     [--eval_patience EVAL_PATIENCE]
                     [--bpe_dropout BPE_DROPOUT] [--do_save_adapter_fusions]
                     [--task_name TASK_NAME]
                     [--predict_task_adapter PREDICT_TASK_ADAPTER]
                     [--predict_lang_adapter PREDICT_LANG_ADAPTER]
                     [--test_adapter] [--adapter_weight ADAPTER_WEIGHT]
                     [--lang_to_vec LANG_TO_VEC]
                     [--calc_weight_step CALC_WEIGHT_STEP]
                     [--predict_save_prefix PREDICT_SAVE_PREFIX]
                     [--en_weight EN_WEIGHT] [--temperature TEMPERATURE]
                     [--get_attr] [--topk TOPK] [--task TASK]
                     [--train_adapter] [--load_adapter LOAD_ADAPTER]
                     [--adapter_config ADAPTER_CONFIG]
                     [--adapter_non_linearity ADAPTER_NON_LINEARITY]
                     [--adapter_reduction_factor ADAPTER_REDUCTION_FACTOR]
                     [--language LANGUAGE]
                     [--load_lang_adapter LOAD_LANG_ADAPTER]
                     [--lang_adapter_config LANG_ADAPTER_CONFIG]
                     [--lang_adapter_non_linearity LANG_ADAPTER_NON_LINEARITY]
                     [--lang_adapter_reduction_factor LANG_ADAPTER_REDUCTION_FACTOR]
my_run_tag.py: error: argument --load_lang_adapter: expected one argument
Command exited with non-zero status 2
3.11user 3.15system 0:01.96elapsed 319%CPU (0avgtext+0avgdata 326368maxresident)k
0inputs+16outputs (0major+79340minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
usage: my_run_tag.py [-h] --model_name_or_path MODEL_NAME_OR_PATH
                     [--model_type MODEL_TYPE] [--config_name CONFIG_NAME]
                     [--tokenizer_name TOKENIZER_NAME] [--cache_dir CACHE_DIR]
                     [--labels LABELS] [--data_dir DATA_DIR]
                     [--output_dir OUTPUT_DIR]
                     [--max_seq_length MAX_SEQ_LENGTH] [--do_train]
                     [--do_eval] [--do_predict] [--do_adapter_predict]
                     [--do_predict_dev] [--do_predict_train]
                     [--init_checkpoint INIT_CHECKPOINT]
                     [--evaluate_during_training] [--do_lower_case]
                     [--few_shot FEW_SHOT]
                     [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]
                     [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]
                     [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]
                     [--learning_rate LEARNING_RATE]
                     [--weight_decay WEIGHT_DECAY]
                     [--adam_epsilon ADAM_EPSILON]
                     [--max_grad_norm MAX_GRAD_NORM]
                     [--num_train_epochs NUM_TRAIN_EPOCHS]
                     [--max_steps MAX_STEPS] [--save_steps SAVE_STEPS]
                     [--warmup_steps WARMUP_STEPS]
                     [--logging_steps LOGGING_STEPS]
                     [--save_only_best_checkpoint] [--eval_all_checkpoints]
                     [--no_cuda] [--overwrite_output_dir] [--overwrite_cache]
                     [--seed SEED] [--fp16] [--fp16_opt_level FP16_OPT_LEVEL]
                     [--local_rank LOCAL_RANK] [--server_ip SERVER_IP]
                     [--server_port SERVER_PORT]
                     [--predict_langs PREDICT_LANGS]
                     [--train_langs TRAIN_LANGS] [--log_file LOG_FILE]
                     [--eval_patience EVAL_PATIENCE]
                     [--bpe_dropout BPE_DROPOUT] [--do_save_adapter_fusions]
                     [--task_name TASK_NAME]
                     [--predict_task_adapter PREDICT_TASK_ADAPTER]
                     [--predict_lang_adapter PREDICT_LANG_ADAPTER]
                     [--test_adapter] [--adapter_weight ADAPTER_WEIGHT]
                     [--lang_to_vec LANG_TO_VEC]
                     [--calc_weight_step CALC_WEIGHT_STEP]
                     [--predict_save_prefix PREDICT_SAVE_PREFIX]
                     [--en_weight EN_WEIGHT] [--temperature TEMPERATURE]
                     [--get_attr] [--topk TOPK] [--task TASK]
                     [--train_adapter] [--load_adapter LOAD_ADAPTER]
                     [--adapter_config ADAPTER_CONFIG]
                     [--adapter_non_linearity ADAPTER_NON_LINEARITY]
                     [--adapter_reduction_factor ADAPTER_REDUCTION_FACTOR]
                     [--language LANGUAGE]
                     [--load_lang_adapter LOAD_LANG_ADAPTER]
                     [--lang_adapter_config LANG_ADAPTER_CONFIG]
                     [--lang_adapter_non_linearity LANG_ADAPTER_NON_LINEARITY]
                     [--lang_adapter_reduction_factor LANG_ADAPTER_REDUCTION_FACTOR]
my_run_tag.py: error: argument --load_lang_adapter: expected one argument
Command exited with non-zero status 2
2.93user 2.64system 0:02.00elapsed 277%CPU (0avgtext+0avgdata 325696maxresident)k
0inputs+16outputs (0major+79384minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
usage: my_run_tag.py [-h] --model_name_or_path MODEL_NAME_OR_PATH
                     [--model_type MODEL_TYPE] [--config_name CONFIG_NAME]
                     [--tokenizer_name TOKENIZER_NAME] [--cache_dir CACHE_DIR]
                     [--labels LABELS] [--data_dir DATA_DIR]
                     [--output_dir OUTPUT_DIR]
                     [--max_seq_length MAX_SEQ_LENGTH] [--do_train]
                     [--do_eval] [--do_predict] [--do_adapter_predict]
                     [--do_predict_dev] [--do_predict_train]
                     [--init_checkpoint INIT_CHECKPOINT]
                     [--evaluate_during_training] [--do_lower_case]
                     [--few_shot FEW_SHOT]
                     [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]
                     [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]
                     [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]
                     [--learning_rate LEARNING_RATE]
                     [--weight_decay WEIGHT_DECAY]
                     [--adam_epsilon ADAM_EPSILON]
                     [--max_grad_norm MAX_GRAD_NORM]
                     [--num_train_epochs NUM_TRAIN_EPOCHS]
                     [--max_steps MAX_STEPS] [--save_steps SAVE_STEPS]
                     [--warmup_steps WARMUP_STEPS]
                     [--logging_steps LOGGING_STEPS]
                     [--save_only_best_checkpoint] [--eval_all_checkpoints]
                     [--no_cuda] [--overwrite_output_dir] [--overwrite_cache]
                     [--seed SEED] [--fp16] [--fp16_opt_level FP16_OPT_LEVEL]
                     [--local_rank LOCAL_RANK] [--server_ip SERVER_IP]
                     [--server_port SERVER_PORT]
                     [--predict_langs PREDICT_LANGS]
                     [--train_langs TRAIN_LANGS] [--log_file LOG_FILE]
                     [--eval_patience EVAL_PATIENCE]
                     [--bpe_dropout BPE_DROPOUT] [--do_save_adapter_fusions]
                     [--task_name TASK_NAME]
                     [--predict_task_adapter PREDICT_TASK_ADAPTER]
                     [--predict_lang_adapter PREDICT_LANG_ADAPTER]
                     [--test_adapter] [--adapter_weight ADAPTER_WEIGHT]
                     [--lang_to_vec LANG_TO_VEC]
                     [--calc_weight_step CALC_WEIGHT_STEP]
                     [--predict_save_prefix PREDICT_SAVE_PREFIX]
                     [--en_weight EN_WEIGHT] [--temperature TEMPERATURE]
                     [--get_attr] [--topk TOPK] [--task TASK]
                     [--train_adapter] [--load_adapter LOAD_ADAPTER]
                     [--adapter_config ADAPTER_CONFIG]
                     [--adapter_non_linearity ADAPTER_NON_LINEARITY]
                     [--adapter_reduction_factor ADAPTER_REDUCTION_FACTOR]
                     [--language LANGUAGE]
                     [--load_lang_adapter LOAD_LANG_ADAPTER]
                     [--lang_adapter_config LANG_ADAPTER_CONFIG]
                     [--lang_adapter_non_linearity LANG_ADAPTER_NON_LINEARITY]
                     [--lang_adapter_reduction_factor LANG_ADAPTER_REDUCTION_FACTOR]
my_run_tag.py: error: argument --load_lang_adapter: expected one argument
Command exited with non-zero status 2
3.46user 3.23system 0:01.93elapsed 345%CPU (0avgtext+0avgdata 325768maxresident)k
0inputs+16outputs (0major+79394minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
usage: my_run_tag.py [-h] --model_name_or_path MODEL_NAME_OR_PATH
                     [--model_type MODEL_TYPE] [--config_name CONFIG_NAME]
                     [--tokenizer_name TOKENIZER_NAME] [--cache_dir CACHE_DIR]
                     [--labels LABELS] [--data_dir DATA_DIR]
                     [--output_dir OUTPUT_DIR]
                     [--max_seq_length MAX_SEQ_LENGTH] [--do_train]
                     [--do_eval] [--do_predict] [--do_adapter_predict]
                     [--do_predict_dev] [--do_predict_train]
                     [--init_checkpoint INIT_CHECKPOINT]
                     [--evaluate_during_training] [--do_lower_case]
                     [--few_shot FEW_SHOT]
                     [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]
                     [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]
                     [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]
                     [--learning_rate LEARNING_RATE]
                     [--weight_decay WEIGHT_DECAY]
                     [--adam_epsilon ADAM_EPSILON]
                     [--max_grad_norm MAX_GRAD_NORM]
                     [--num_train_epochs NUM_TRAIN_EPOCHS]
                     [--max_steps MAX_STEPS] [--save_steps SAVE_STEPS]
                     [--warmup_steps WARMUP_STEPS]
                     [--logging_steps LOGGING_STEPS]
                     [--save_only_best_checkpoint] [--eval_all_checkpoints]
                     [--no_cuda] [--overwrite_output_dir] [--overwrite_cache]
                     [--seed SEED] [--fp16] [--fp16_opt_level FP16_OPT_LEVEL]
                     [--local_rank LOCAL_RANK] [--server_ip SERVER_IP]
                     [--server_port SERVER_PORT]
                     [--predict_langs PREDICT_LANGS]
                     [--train_langs TRAIN_LANGS] [--log_file LOG_FILE]
                     [--eval_patience EVAL_PATIENCE]
                     [--bpe_dropout BPE_DROPOUT] [--do_save_adapter_fusions]
                     [--task_name TASK_NAME]
                     [--predict_task_adapter PREDICT_TASK_ADAPTER]
                     [--predict_lang_adapter PREDICT_LANG_ADAPTER]
                     [--test_adapter] [--adapter_weight ADAPTER_WEIGHT]
                     [--lang_to_vec LANG_TO_VEC]
                     [--calc_weight_step CALC_WEIGHT_STEP]
                     [--predict_save_prefix PREDICT_SAVE_PREFIX]
                     [--en_weight EN_WEIGHT] [--temperature TEMPERATURE]
                     [--get_attr] [--topk TOPK] [--task TASK]
                     [--train_adapter] [--load_adapter LOAD_ADAPTER]
                     [--adapter_config ADAPTER_CONFIG]
                     [--adapter_non_linearity ADAPTER_NON_LINEARITY]
                     [--adapter_reduction_factor ADAPTER_REDUCTION_FACTOR]
                     [--language LANGUAGE]
                     [--load_lang_adapter LOAD_LANG_ADAPTER]
                     [--lang_adapter_config LANG_ADAPTER_CONFIG]
                     [--lang_adapter_non_linearity LANG_ADAPTER_NON_LINEARITY]
                     [--lang_adapter_reduction_factor LANG_ADAPTER_REDUCTION_FACTOR]
my_run_tag.py: error: argument --load_lang_adapter: expected one argument
Command exited with non-zero status 2
3.08user 2.73system 0:01.88elapsed 309%CPU (0avgtext+0avgdata 324812maxresident)k
0inputs+8outputs (0major+79348minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
usage: my_run_tag.py [-h] --model_name_or_path MODEL_NAME_OR_PATH
                     [--model_type MODEL_TYPE] [--config_name CONFIG_NAME]
                     [--tokenizer_name TOKENIZER_NAME] [--cache_dir CACHE_DIR]
                     [--labels LABELS] [--data_dir DATA_DIR]
                     [--output_dir OUTPUT_DIR]
                     [--max_seq_length MAX_SEQ_LENGTH] [--do_train]
                     [--do_eval] [--do_predict] [--do_adapter_predict]
                     [--do_predict_dev] [--do_predict_train]
                     [--init_checkpoint INIT_CHECKPOINT]
                     [--evaluate_during_training] [--do_lower_case]
                     [--few_shot FEW_SHOT]
                     [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]
                     [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]
                     [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]
                     [--learning_rate LEARNING_RATE]
                     [--weight_decay WEIGHT_DECAY]
                     [--adam_epsilon ADAM_EPSILON]
                     [--max_grad_norm MAX_GRAD_NORM]
                     [--num_train_epochs NUM_TRAIN_EPOCHS]
                     [--max_steps MAX_STEPS] [--save_steps SAVE_STEPS]
                     [--warmup_steps WARMUP_STEPS]
                     [--logging_steps LOGGING_STEPS]
                     [--save_only_best_checkpoint] [--eval_all_checkpoints]
                     [--no_cuda] [--overwrite_output_dir] [--overwrite_cache]
                     [--seed SEED] [--fp16] [--fp16_opt_level FP16_OPT_LEVEL]
                     [--local_rank LOCAL_RANK] [--server_ip SERVER_IP]
                     [--server_port SERVER_PORT]
                     [--predict_langs PREDICT_LANGS]
                     [--train_langs TRAIN_LANGS] [--log_file LOG_FILE]
                     [--eval_patience EVAL_PATIENCE]
                     [--bpe_dropout BPE_DROPOUT] [--do_save_adapter_fusions]
                     [--task_name TASK_NAME]
                     [--predict_task_adapter PREDICT_TASK_ADAPTER]
                     [--predict_lang_adapter PREDICT_LANG_ADAPTER]
                     [--test_adapter] [--adapter_weight ADAPTER_WEIGHT]
                     [--lang_to_vec LANG_TO_VEC]
                     [--calc_weight_step CALC_WEIGHT_STEP]
                     [--predict_save_prefix PREDICT_SAVE_PREFIX]
                     [--en_weight EN_WEIGHT] [--temperature TEMPERATURE]
                     [--get_attr] [--topk TOPK] [--task TASK]
                     [--train_adapter] [--load_adapter LOAD_ADAPTER]
                     [--adapter_config ADAPTER_CONFIG]
                     [--adapter_non_linearity ADAPTER_NON_LINEARITY]
                     [--adapter_reduction_factor ADAPTER_REDUCTION_FACTOR]
                     [--language LANGUAGE]
                     [--load_lang_adapter LOAD_LANG_ADAPTER]
                     [--lang_adapter_config LANG_ADAPTER_CONFIG]
                     [--lang_adapter_non_linearity LANG_ADAPTER_NON_LINEARITY]
                     [--lang_adapter_reduction_factor LANG_ADAPTER_REDUCTION_FACTOR]
my_run_tag.py: error: argument --load_lang_adapter: expected one argument
Command exited with non-zero status 2
3.96user 4.09system 0:02.02elapsed 398%CPU (0avgtext+0avgdata 325888maxresident)k
0inputs+8outputs (0major+79371minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
usage: my_run_tag.py [-h] --model_name_or_path MODEL_NAME_OR_PATH
                     [--model_type MODEL_TYPE] [--config_name CONFIG_NAME]
                     [--tokenizer_name TOKENIZER_NAME] [--cache_dir CACHE_DIR]
                     [--labels LABELS] [--data_dir DATA_DIR]
                     [--output_dir OUTPUT_DIR]
                     [--max_seq_length MAX_SEQ_LENGTH] [--do_train]
                     [--do_eval] [--do_predict] [--do_adapter_predict]
                     [--do_predict_dev] [--do_predict_train]
                     [--init_checkpoint INIT_CHECKPOINT]
                     [--evaluate_during_training] [--do_lower_case]
                     [--few_shot FEW_SHOT]
                     [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]
                     [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]
                     [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]
                     [--learning_rate LEARNING_RATE]
                     [--weight_decay WEIGHT_DECAY]
                     [--adam_epsilon ADAM_EPSILON]
                     [--max_grad_norm MAX_GRAD_NORM]
                     [--num_train_epochs NUM_TRAIN_EPOCHS]
                     [--max_steps MAX_STEPS] [--save_steps SAVE_STEPS]
                     [--warmup_steps WARMUP_STEPS]
                     [--logging_steps LOGGING_STEPS]
                     [--save_only_best_checkpoint] [--eval_all_checkpoints]
                     [--no_cuda] [--overwrite_output_dir] [--overwrite_cache]
                     [--seed SEED] [--fp16] [--fp16_opt_level FP16_OPT_LEVEL]
                     [--local_rank LOCAL_RANK] [--server_ip SERVER_IP]
                     [--server_port SERVER_PORT]
                     [--predict_langs PREDICT_LANGS]
                     [--train_langs TRAIN_LANGS] [--log_file LOG_FILE]
                     [--eval_patience EVAL_PATIENCE]
                     [--bpe_dropout BPE_DROPOUT] [--do_save_adapter_fusions]
                     [--task_name TASK_NAME]
                     [--predict_task_adapter PREDICT_TASK_ADAPTER]
                     [--predict_lang_adapter PREDICT_LANG_ADAPTER]
                     [--test_adapter] [--adapter_weight ADAPTER_WEIGHT]
                     [--lang_to_vec LANG_TO_VEC]
                     [--calc_weight_step CALC_WEIGHT_STEP]
                     [--predict_save_prefix PREDICT_SAVE_PREFIX]
                     [--en_weight EN_WEIGHT] [--temperature TEMPERATURE]
                     [--get_attr] [--topk TOPK] [--task TASK]
                     [--train_adapter] [--load_adapter LOAD_ADAPTER]
                     [--adapter_config ADAPTER_CONFIG]
                     [--adapter_non_linearity ADAPTER_NON_LINEARITY]
                     [--adapter_reduction_factor ADAPTER_REDUCTION_FACTOR]
                     [--language LANGUAGE]
                     [--load_lang_adapter LOAD_LANG_ADAPTER]
                     [--lang_adapter_config LANG_ADAPTER_CONFIG]
                     [--lang_adapter_non_linearity LANG_ADAPTER_NON_LINEARITY]
                     [--lang_adapter_reduction_factor LANG_ADAPTER_REDUCTION_FACTOR]
my_run_tag.py: error: argument --load_lang_adapter: expected one argument
Command exited with non-zero status 2
2.43user 1.99system 0:02.14elapsed 206%CPU (0avgtext+0avgdata 326016maxresident)k
0inputs+16outputs (0major+79390minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
usage: my_run_tag.py [-h] --model_name_or_path MODEL_NAME_OR_PATH
                     [--model_type MODEL_TYPE] [--config_name CONFIG_NAME]
                     [--tokenizer_name TOKENIZER_NAME] [--cache_dir CACHE_DIR]
                     [--labels LABELS] [--data_dir DATA_DIR]
                     [--output_dir OUTPUT_DIR]
                     [--max_seq_length MAX_SEQ_LENGTH] [--do_train]
                     [--do_eval] [--do_predict] [--do_adapter_predict]
                     [--do_predict_dev] [--do_predict_train]
                     [--init_checkpoint INIT_CHECKPOINT]
                     [--evaluate_during_training] [--do_lower_case]
                     [--few_shot FEW_SHOT]
                     [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]
                     [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]
                     [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]
                     [--learning_rate LEARNING_RATE]
                     [--weight_decay WEIGHT_DECAY]
                     [--adam_epsilon ADAM_EPSILON]
                     [--max_grad_norm MAX_GRAD_NORM]
                     [--num_train_epochs NUM_TRAIN_EPOCHS]
                     [--max_steps MAX_STEPS] [--save_steps SAVE_STEPS]
                     [--warmup_steps WARMUP_STEPS]
                     [--logging_steps LOGGING_STEPS]
                     [--save_only_best_checkpoint] [--eval_all_checkpoints]
                     [--no_cuda] [--overwrite_output_dir] [--overwrite_cache]
                     [--seed SEED] [--fp16] [--fp16_opt_level FP16_OPT_LEVEL]
                     [--local_rank LOCAL_RANK] [--server_ip SERVER_IP]
                     [--server_port SERVER_PORT]
                     [--predict_langs PREDICT_LANGS]
                     [--train_langs TRAIN_LANGS] [--log_file LOG_FILE]
                     [--eval_patience EVAL_PATIENCE]
                     [--bpe_dropout BPE_DROPOUT] [--do_save_adapter_fusions]
                     [--task_name TASK_NAME]
                     [--predict_task_adapter PREDICT_TASK_ADAPTER]
                     [--predict_lang_adapter PREDICT_LANG_ADAPTER]
                     [--test_adapter] [--adapter_weight ADAPTER_WEIGHT]
                     [--lang_to_vec LANG_TO_VEC]
                     [--calc_weight_step CALC_WEIGHT_STEP]
                     [--predict_save_prefix PREDICT_SAVE_PREFIX]
                     [--en_weight EN_WEIGHT] [--temperature TEMPERATURE]
                     [--get_attr] [--topk TOPK] [--task TASK]
                     [--train_adapter] [--load_adapter LOAD_ADAPTER]
                     [--adapter_config ADAPTER_CONFIG]
                     [--adapter_non_linearity ADAPTER_NON_LINEARITY]
                     [--adapter_reduction_factor ADAPTER_REDUCTION_FACTOR]
                     [--language LANGUAGE]
                     [--load_lang_adapter LOAD_LANG_ADAPTER]
                     [--lang_adapter_config LANG_ADAPTER_CONFIG]
                     [--lang_adapter_non_linearity LANG_ADAPTER_NON_LINEARITY]
                     [--lang_adapter_reduction_factor LANG_ADAPTER_REDUCTION_FACTOR]
my_run_tag.py: error: argument --load_lang_adapter: expected one argument
Command exited with non-zero status 2
3.49user 3.50system 0:02.03elapsed 343%CPU (0avgtext+0avgdata 326416maxresident)k
0inputs+16outputs (0major+79353minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
usage: my_run_tag.py [-h] --model_name_or_path MODEL_NAME_OR_PATH
                     [--model_type MODEL_TYPE] [--config_name CONFIG_NAME]
                     [--tokenizer_name TOKENIZER_NAME] [--cache_dir CACHE_DIR]
                     [--labels LABELS] [--data_dir DATA_DIR]
                     [--output_dir OUTPUT_DIR]
                     [--max_seq_length MAX_SEQ_LENGTH] [--do_train]
                     [--do_eval] [--do_predict] [--do_adapter_predict]
                     [--do_predict_dev] [--do_predict_train]
                     [--init_checkpoint INIT_CHECKPOINT]
                     [--evaluate_during_training] [--do_lower_case]
                     [--few_shot FEW_SHOT]
                     [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]
                     [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]
                     [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]
                     [--learning_rate LEARNING_RATE]
                     [--weight_decay WEIGHT_DECAY]
                     [--adam_epsilon ADAM_EPSILON]
                     [--max_grad_norm MAX_GRAD_NORM]
                     [--num_train_epochs NUM_TRAIN_EPOCHS]
                     [--max_steps MAX_STEPS] [--save_steps SAVE_STEPS]
                     [--warmup_steps WARMUP_STEPS]
                     [--logging_steps LOGGING_STEPS]
                     [--save_only_best_checkpoint] [--eval_all_checkpoints]
                     [--no_cuda] [--overwrite_output_dir] [--overwrite_cache]
                     [--seed SEED] [--fp16] [--fp16_opt_level FP16_OPT_LEVEL]
                     [--local_rank LOCAL_RANK] [--server_ip SERVER_IP]
                     [--server_port SERVER_PORT]
                     [--predict_langs PREDICT_LANGS]
                     [--train_langs TRAIN_LANGS] [--log_file LOG_FILE]
                     [--eval_patience EVAL_PATIENCE]
                     [--bpe_dropout BPE_DROPOUT] [--do_save_adapter_fusions]
                     [--task_name TASK_NAME]
                     [--predict_task_adapter PREDICT_TASK_ADAPTER]
                     [--predict_lang_adapter PREDICT_LANG_ADAPTER]
                     [--test_adapter] [--adapter_weight ADAPTER_WEIGHT]
                     [--lang_to_vec LANG_TO_VEC]
                     [--calc_weight_step CALC_WEIGHT_STEP]
                     [--predict_save_prefix PREDICT_SAVE_PREFIX]
                     [--en_weight EN_WEIGHT] [--temperature TEMPERATURE]
                     [--get_attr] [--topk TOPK] [--task TASK]
                     [--train_adapter] [--load_adapter LOAD_ADAPTER]
                     [--adapter_config ADAPTER_CONFIG]
                     [--adapter_non_linearity ADAPTER_NON_LINEARITY]
                     [--adapter_reduction_factor ADAPTER_REDUCTION_FACTOR]
                     [--language LANGUAGE]
                     [--load_lang_adapter LOAD_LANG_ADAPTER]
                     [--lang_adapter_config LANG_ADAPTER_CONFIG]
                     [--lang_adapter_non_linearity LANG_ADAPTER_NON_LINEARITY]
                     [--lang_adapter_reduction_factor LANG_ADAPTER_REDUCTION_FACTOR]
my_run_tag.py: error: argument --load_lang_adapter: expected one argument
Command exited with non-zero status 2
2.54user 2.04system 0:01.89elapsed 242%CPU (0avgtext+0avgdata 325964maxresident)k
0inputs+16outputs (0major+79355minor)pagefaults 0swaps
Traceback (most recent call last):
  File "third_party/my_run_tag.py", line 32, in <module>
    import torch
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/torch/__init__.py", line 643, in <module>
    from .functional import *  # noqa: F403
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/torch/functional.py", line 6, in <module>
    import torch.nn.functional as F
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/torch/nn/__init__.py", line 1, in <module>
    from .modules import *  # noqa: F403
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/torch/nn/modules/__init__.py", line 2, in <module>
    from .linear import Identity, Linear, Bilinear, LazyLinear
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/torch/nn/modules/linear.py", line 6, in <module>
    from .. import functional as F
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/torch/nn/functional.py", line 11, in <module>
    from .._jit_internal import boolean_dispatch, _overload, BroadcastingList1, BroadcastingList2, BroadcastingList3
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/torch/_jit_internal.py", line 24, in <module>
    import torch.distributed.rpc
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/torch/distributed/rpc/__init__.py", line 63, in <module>
    from . import api, backend_registry, functions
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/torch/distributed/rpc/backend_registry.py", line 32, in <module>
    BackendType = enum.Enum(value="BackendType", names=dict())  # type: ignore[misc]
  File "/usr/lib/python3.7/enum.py", line 317, in __call__
    return cls._create_(value, names, module=module, qualname=qualname, type=type, start=start)
  File "/usr/lib/python3.7/enum.py", line 429, in _create_
    enum_class = metacls.__new__(metacls, class_name, bases, classdict)
  File "/usr/lib/python3.7/enum.py", line 182, in __new__
    dynamic_attributes = {k for c in enum_class.mro()
  File "/usr/lib/python3.7/enum.py", line 183, in <setcomp>
    for k, v in c.__dict__.items()
KeyboardInterrupt
Command exited with non-zero status 1
2.11user 2.47system 0:00.72elapsed 634%CPU (0avgtext+0avgdata 218860maxresident)k
0inputs+0outputs (0major+24738minor)pagefaults 0swaps
Traceback (most recent call last):
  File "third_party/my_run_tag.py", line 32, in <module>
    import torch
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/torch/__init__.py", line 197, in <module>
    from torch._C import *  # noqa: F403
RuntimeError: KeyboardInterrupt: 
Command exited with non-zero status 1
2.36user 2.83system 0:00.58elapsed 884%CPU (0avgtext+0avgdata 211464maxresident)k
0inputs+8outputs (0major+22836minor)pagefaults 0swaps
Traceback (most recent call last):
  File "third_party/my_run_tag.py", line 32, in <module>
    import torch
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/torch/__init__.py", line 197, in <module>
    from torch._C import *  # noqa: F403
RuntimeError: KeyboardInterrupt: 
Command exited with non-zero status 1
2.35user 2.92system 0:00.57elapsed 914%CPU (0avgtext+0avgdata 211500maxresident)k
0inputs+0outputs (0major+22839minor)pagefaults 0swaps
Traceback (most recent call last):
  File "third_party/my_run_tag.py", line 30, in <module>
    import numpy as np
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/numpy/__init__.py", line 150, in <module>
    from . import core
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/numpy/core/__init__.py", line 70, in <module>
    from . import numerictypes as nt
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/numpy/core/numerictypes.py", line 82, in <module>
    import numbers
  File "<frozen importlib._bootstrap>", line 983, in _find_and_load
  File "<frozen importlib._bootstrap>", line 967, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 677, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 857, in get_code
  File "<frozen importlib._bootstrap_external>", line 525, in _compile_bytecode
KeyboardInterrupt
Command terminated by signal 2
0.61user 0.89system 0:00.09elapsed 1538%CPU (0avgtext+0avgdata 20856maxresident)k
0inputs+0outputs (0major+3294minor)pagefaults 0swaps
Traceback (most recent call last):
  File "third_party/my_run_tag.py", line 32, in <module>
    import torch
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/torch/__init__.py", line 23, in <module>
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/torch/_utils_internal.py", line 3, in <module>
    import tempfile
  File "/usr/lib/python3.7/tempfile.py", line 44, in <module>
    import shutil as _shutil
  File "<frozen importlib._bootstrap>", line 983, in _find_and_load
  File "<frozen importlib._bootstrap>", line 963, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 906, in _find_spec
  File "<frozen importlib._bootstrap_external>", line 1280, in find_spec
  File "<frozen importlib._bootstrap_external>", line 1246, in _get_spec
KeyboardInterrupt
Exception ignored in: <module 'threading' from '/usr/lib/python3.7/threading.py'>
Traceback (most recent call last):
  File "/usr/lib/python3.7/threading.py", line 1294, in _shutdown
KeyboardInterrupt
Command terminated by signal 2
1.38user 1.80system 0:00.19elapsed 1633%CPU (0avgtext+0avgdata 31608maxresident)k
104inputs+0outputs PyTorch version 1.10.0+cu102 available.
12/25/2021 22:23:48 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_ensemble_oracle_top1/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ar', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_ensemble_oracle_top1//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
12/25/2021 22:23:48 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
12/25/2021 22:23:48 - INFO - __main__ -   Seed = 1
12/25/2021 22:23:48 - INFO - root -   save model
12/25/2021 22:23:48 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_ensemble_oracle_top1/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ar', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_ensemble_oracle_top1//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
12/25/2021 22:23:48 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
12/25/2021 22:23:51 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
12/25/2021 22:23:56 - INFO - __main__ -   Using lang2id = None
12/25/2021 22:23:56 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
12/25/2021 22:23:56 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
12/25/2021 22:23:56 - INFO - root -   Trying to decide if add adapter
12/25/2021 22:23:56 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_model_head.bin
12/25/2021 22:23:57 - INFO - root -   loading lang adpater topk
12/25/2021 22:23:57 - INFO - __main__ -   Loading Adapter Languages from scripts/udpos/en/ar.json
12/25/2021 22:23:57 - INFO - __main__ -   Adapter Languages : ['{"adapter": "ka", "num_seeds": 3, "f1": 58.37731855040207}'], Length : 1
12/25/2021 22:23:57 - INFO - __main__ -   Adapter Names ['{"adapter": "ka", "num_seeds": 3, "f1": 58.37731855040207}/wiki@ukp'], Length : 1
12/25/2021 22:23:57 - INFO - __main__ -   Language = {"adapter": "ka", "num_seeds": 3, "f1": 58.37731855040207}
12/25/2021 22:23:57 - INFO - __main__ -   Adapter Name = {"adapter": "ka", "num_seeds": 3, "f1": 58.37731855040207}/wiki@ukp
Traceback (most recent call last):
  File "third_party/my_run_tag.py", line 1115, in <module>
    main()
  File "third_party/my_run_tag.py", line 1053, in main
    model, lang_adapter_names, task_name = setup_adapter(args, adapter_args, model, load_adapter=load_adapter, load_lang_adapter=load_lang_adapter)
  File "third_party/my_run_tag.py", line 740, in setup_adapter
    load_as=language,
  File "/home/abhijeet/rohan/cloud-emea/src/transformers/adapter_model_mixin.py", line 1180, in load_adapter
    **kwargs,
  File "/home/abhijeet/rohan/cloud-emea/src/transformers/adapter_model_mixin.py", line 999, in load_adapter
    load_dir, load_name = loader.load(adapter_name_or_path, config, version, model_name, load_as, **kwargs)
  File "/home/abhijeet/rohan/cloud-emea/src/transformers/adapter_model_mixin.py", line 391, in load
    **kwargs,
  File "/home/abhijeet/rohan/cloud-emea/src/transformers/adapter_utils.py", line 440, in resolve_adapter_path
    raise ValueError("Unable to identify {} as a valid module location.".format(adapter_name_or_path))
ValueError: Unable to identify {"adapter": "ka", "num_seeds": 3, "f1": 58.37731855040207}/wiki@ukp as a valid module location.
Command exited with non-zero status 1
10.00user 5.12system 0:10.66elapsed 141%CPU (0avgtext+0avgdata 1757188maxresident)k
5648inputs+56outputs (0major+754939minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
12/25/2021 22:23:59 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_ensemble_oracle_top1/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ar', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_ensemble_oracle_top1//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
12/25/2021 22:23:59 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
12/25/2021 22:23:59 - INFO - __main__ -   Seed = 2
12/25/2021 22:23:59 - INFO - root -   save model
12/25/2021 22:23:59 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_ensemble_oracle_top1/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ar', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_ensemble_oracle_top1//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
12/25/2021 22:23:59 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
12/25/2021 22:24:01 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
12/25/2021 22:24:07 - INFO - __main__ -   Using lang2id = None
12/25/2021 22:24:07 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
12/25/2021 22:24:07 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
12/25/2021 22:24:07 - INFO - root -   Trying to decide if add adapter
12/25/2021 22:24:07 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_model_head.bin
12/25/2021 22:24:07 - INFO - root -   loading lang adpater topk
12/25/2021 22:24:07 - INFO - __main__ -   Loading Adapter Languages from scripts/udpos/en/ar.json
12/25/2021 22:24:07 - INFO - __main__ -   Adapter Languages : ['{"adapter": "ka", "num_seeds": 3, "f1": 58.37731855040207}'], Length : 1
12/25/2021 22:24:07 - INFO - __main__ -   Adapter Names ['{"adapter": "ka", "num_seeds": 3, "f1": 58.37731855040207}/wiki@ukp'], Length : 1
12/25/2021 22:24:07 - INFO - __main__ -   Language = {"adapter": "ka", "num_seeds": 3, "f1": 58.37731855040207}
12/25/2021 22:24:07 - INFO - __main__ -   Adapter Name = {"adapter": "ka", "num_seeds": 3, "f1": 58.37731855040207}/wiki@ukp
Traceback (most recent call last):
  File "third_party/my_run_tag.py", line 1115, in <module>
    main()
  File "third_party/my_run_tag.py", line 1053, in main
    model, lang_adapter_names, task_name = setup_adapter(args, adapter_args, model, load_adapter=load_adapter, load_lang_adapter=load_lang_adapter)
  File "third_party/my_run_tag.py", line 740, in setup_adapter
    load_as=language,
  File "/home/abhijeet/rohan/cloud-emea/src/transformers/adapter_model_mixin.py", line 1180, in load_adapter
    **kwargs,
  File "/home/abhijeet/rohan/cloud-emea/src/transformers/adapter_model_mixin.py", line 999, in load_adapter
    load_dir, load_name = loader.load(adapter_name_or_path, config, version, model_name, load_as, **kwargs)
  File "/home/abhijeet/rohan/cloud-emea/src/transformers/adapter_model_mixin.py", line 391, in load
    **kwargs,
  File "/home/abhijeet/rohan/cloud-emea/src/transformers/adapter_utils.py", line 440, in resolve_adapter_path
    raise ValueError("Unable to identify {} as a valid module location.".format(adapter_name_or_path))
ValueError: Unable to identify {"adapter": "ka", "num_seeds": 3, "f1": 58.37731855040207}/wiki@ukp as a valid module location.
Command exited with non-zero status 1
10.04user 4.00system 0:10.38elapsed 135%CPU (0avgtext+0avgdata 1756412maxresident)k
6608inputs+40outputs (0major+741343minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
12/25/2021 22:24:09 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_ensemble_oracle_top1/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ar', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_ensemble_oracle_top1//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
12/25/2021 22:24:09 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
12/25/2021 22:24:09 - INFO - __main__ -   Seed = 3
12/25/2021 22:24:09 - INFO - root -   save model
12/25/2021 22:24:09 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_ensemble_oracle_top1/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ar', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_ensemble_oracle_top1//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
12/25/2021 22:24:09 - INFO - __main__ -   Loading pretrained model and tokenizer
Traceback (most recent call last):
  File "third_party/my_run_tag.py", line 1115, in <module>
    main()
  File "third_party/my_run_tag.py", line 1042, in main
    model, tokenizer, lang2id = load_model(args, num_labels)
  File "third_party/my_run_tag.py", line 777, in load_model
    cache_dir=args.cache_dir,
  File "/home/abhijeet/rohan/cloud-emea/src/transformers/configuration_auto.py", line 333, in from_pretrained
    config_dict, _ = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/abhijeet/rohan/cloud-emea/src/transformers/configuration_utils.py", line 389, in get_config_dict
    local_files_only=local_files_only,
  File "/home/abhijeet/rohan/cloud-emea/src/transformers/file_utils.py", line 955, in cached_path
    local_files_only=local_files_only,
  File "/home/abhijeet/rohan/cloud-emea/src/transformers/file_utils.py", line 1075, in get_from_cache
    r = requests.head(url, headers=headers, allow_redirects=False, proxies=proxies, timeout=etag_timeout)
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/requests/api.py", line 102, in head
    return request('head', url, **kwargs)
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/requests/api.py", line 61, in request
    return session.request(method=method, url=url, **kwargs)
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/requests/sessions.py", line 542, in request
    resp = self.send(prep, **send_kwargs)
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/requests/sessions.py", line 655, in send
    r = adapter.send(request, **kwargs)
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/requests/adapters.py", line 449, in send
    timeout=timeout
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/urllib3/connectionpool.py", line 706, in urlopen
    chunked=chunked,
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/urllib3/connectionpool.py", line 445, in _make_request
    six.raise_from(e, None)
  File "<string>", line 3, in raise_from
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/urllib3/connectionpool.py", line 440, in _make_request
    httplib_response = conn.getresponse()
  File "/usr/lib/python3.7/http/client.py", line 1369, in getresponse
    response.begin()
  File "/usr/lib/python3.7/http/client.py", line 310, in begin
    version, status, reason = self._read_status()
  File "/usr/lib/python3.7/http/client.py", line 271, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/usr/lib/python3.7/socket.py", line 589, in readinto
    return self._sock.recv_into(b)
  File "/usr/lib/python3.7/ssl.py", line 1071, in recv_into
    return self.read(nbytes, buffer)
  File "/usr/lib/python3.7/ssl.py", line 929, in read
    return self._sslobj.read(len, buffer)
KeyboardInterrupt
Command terminated by signal 2
2.84user 2.26system 0:02.66elapsed 191%CPU (0avgtext+0avgdata 318076maxresident)k
112inputs+32outputs (0major+67301minor)pagefaults 0swaps
Traceback (most recent call last):
  File "third_party/my_run_tag.py", line 30, in <module>
    import numpy as np
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/numpy/__init__.py", line 150, in <module>
    from . import core
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/numpy/core/__init__.py", line 76, in <module>
    from . import defchararray as char
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/numpy/core/defchararray.py", line 318, in <module>
    @array_function_dispatch(_multiply_dispatcher)
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/numpy/core/overrides.py", line 198, in decorator
    source, filename='<__array_function__ internals>', mode='exec')
KeyboardInterrupt
Command exited with non-zero status 1
1.58user 2.24system 0:00.15elapsed 2517%CPU (0avgtext+0avgdata 22324maxresident)k
0inputs+0outputs (0major+3567minor)pagefaults 0swaps
Traceback (most recent call last):
  File "third_party/my_run_tag.py", line 30, in <module>
    import numpy as np
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/numpy/__init__.py", line 150, in <module>
    from . import core
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/numpy/core/__init__.py", line 99, in <module>
    from . import _add_newdocs_scalars
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/numpy/core/_add_newdocs_scalars.py", line 9, in <module>
    import platform
  File "/usr/lib/python3.7/platform.py", line 1159, in <module>
    r'([\d.]+)\s*'
  File "/usr/lib/python3.7/re.py", line 236, in compile
    return _compile(pattern, flags)
  File "/usr/lib/python3.7/re.py", line 288, in _compile
    p = sre_compile.compile(pattern, flags)
  File "/usr/lib/python3.7/sre_compile.py", line 764, in compile
    p = sre_parse.parse(p, flags)
  File "/usr/lib/python3.7/sre_parse.py", line 924, in parse
    p = _parse_sub(source, pattern, flags & SRE_FLAG_VERBOSE, 0)
  File "/usr/lib/python3.7/sre_parse.py", line 420, in _parse_sub
    not nested and not items))
  File "/usr/lib/python3.7/sre_parse.py", line 810, in _parse
    p = _parse_sub(source, state, sub_verbose, nested + 1)
  File "/usr/lib/python3.7/sre_parse.py", line 420, in _parse_sub
    not nested and not items))
  File "/usr/lib/python3.7/sre_parse.py", line 508, in _parse
    here = source.tell() - 1
  File "/usr/lib/python3.7/sre_parse.py", line 287, in tell
    return self.index - len(self.next or '')
KeyboardInterrupt
Command exited with non-zero status 1
2.59user 3.76system 0:00.19elapsed 3307%CPU (0avgtext+0avgdata 23608maxresident)k
0inputs+0outputs (0major+3914minor)pagefaults 0swaps
Traceback (most recent call last):
  File "third_party/my_run_tag.py", line 31, in <module>
    import scipy
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/scipy/__init__.py", line 74, in <module>
    from ._lib.deprecation import _deprecated
  File "<frozen importlib._bootstrap>", line 983, in _find_and_load
  File "<frozen importlib._bootstrap>", line 967, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 677, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 807, in get_code
  File "<frozen importlib._bootstrap_external>", line 298, in cache_from_source
KeyboardInterrupt
Command terminated by signal 2
1.65user 2.32system 0:00.19elapsed 2062%CPU (0avgtext+0avgdata 30568maxresident)k
0inputs+8outputs (0major+7410minor)pagefaults 0swaps
Traceback (most recent call last):
  File "third_party/my_run_tag.py", line 30, in <module>
    import numpy as np
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/numpy/__init__.py", line 163, in <module>
    from . import ma
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/numpy/ma/__init__.py", line 42, in <module>
    from . import core
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/numpy/ma/core.py", line 2432, in <module>
    def _recursive_printoption(result, mask, printopt):
KeyboardInterrupt
Command exited with non-zero status 1
1.96user 2.72system 0:00.21elapsed 2168%CPU (0avgtext+0avgdata 30220maxresident)k
0inputs+0outputs (0major+7835minor)pagefaults 0swaps
Traceback (most recent call last):
  File "third_party/my_run_tag.py", line 32, in <module>
    import torch
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/torch/__init__.py", line 197, in <module>
    from torch._C import *  # noqa: F403
RuntimeError: KeyboardInterrupt: 
Command exited with non-zero status 1
2.79user 3.83system 0:00.63elapsed 1040%CPU (0avgtext+0avgdata 210476maxresident)k
0inputs+0outputs (0major+22799minor)pagefaults 0swaps
Traceback (most recent call last):
  File "third_party/my_run_tag.py", line 32, in <module>
    import torch
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/torch/__init__.py", line 197, in <module>
    from torch._C import *  # noqa: F403
RuntimeError: KeyboardInterrupt: 
Command terminated by signal 2
2.01user 2.65system 0:00.56elapsed 827%CPU (0avgtext+0avgdata 210244maxresident)k
0inputs+0outputs (0major+22772minor)pagefaults 0swaps
Command terminated by signal 2
0.00user 0.00system 0:00.00elapsed ?%CPU (0avgtext+0avgdata 480maxresident)k
0inputs+0outputs (0major+16minor)pagefaults 0swaps
Fatal Python error: initsite: Failed to import the site module
Traceback (most recent call last):
  File "/usr/lib/python3.7/site.py", line 596, in <module>
    main()
  File "/usr/lib/python3.7/site.py", line 579, in main
    known_paths = venv(known_paths)
  File "/usr/lib/python3.7/site.py", line 511, in venv
    addsitepackages(known_paths, [sys.prefix])
  File "/usr/lib/python3.7/site.py", line 366, in addsitepackages
    addsitedir(sitedir, known_paths)
  File "/usr/lib/python3.7/site.py", line 213, in addsitedir
    addpackage(sitedir, name, known_paths)
  File "/usr/lib/python3.7/site.py", line 174, in addpackage
    exec(line)
  File "<string>", line 1, in <module>
  File "/usr/lib/python3.7/importlib/util.py", line 14, in <module>
    from contextlib import contextmanager
  File "<frozen importlib._bootstrap>", line 983, in _find_and_load
  File "<frozen importlib._bootstrap>", line 967, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 677, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 857, in get_code
  File "<frozen importlib._bootstrap_external>", line 525, in _compile_bytecode
KeyboardInterrupt
Command exited with non-zero status 1
0.01user 0.00system 0:00.02elapsed 96%CPU (0avgtext+0avgdata 9136maxresident)k
0inputs+0outputs (0major+932minor)pagefaults 0swaps
Traceback (most recent call last):
  File "third_party/my_run_tag.py", line 30, in <module>
    import numpy as np
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/numpy/__init__.py", line 153, in <module>
    from . import lib
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/numpy/lib/__init__.py", line 34, in <module>
    from .polynomial import *
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/numpy/lib/polynomial.py", line 1040, in <module>
    _poly_mat = re.compile(r"\*\*([0-9]*)")
  File "/usr/lib/python3.7/re.py", line 236, in compile
    return _compile(pattern, flags)
  File "/usr/lib/python3.7/re.py", line 288, in _compile
    p = sre_compile.compile(pattern, flags)
  File "/usr/lib/python3.7/sre_compile.py", line 764, in compile
    p = sre_parse.parse(p, flags)
  File "/usr/lib/python3.7/sre_parse.py", line 924, in parse
    p = _parse_sub(source, pattern, flags & SRE_FLAG_VERBOSE, 0)
  File "/usr/lib/python3.7/sre_parse.py", line 420, in _parse_sub
    not nested and not items))
  File "/usr/lib/python3.7/sre_parse.py", line 662, in _parse
    start = source.tell() - 1
KeyboardInterrupt
Command terminated by signal 2
1.28user 2.10system 0:00.15elapsed 2153%CPU (0avgtext+0avgdata 25136maxresident)k
0inputs+8outputs (0major+5977minor)pagefaults 0swaps
Command terminated by signal 2
0.00user 0.00system 0:00.01elapsed 92%CPU (0avgtext+0avgdata 7656maxresident)k
0inputs+0outputs (0major+650minor)pagefaults 0swaps

Command terminated by signal 2
1.77user 2.57system 0:00.18elapsed 2399%CPU (0avgtext+0avgdata 26040maxresident)k
0inputs+0outputs (0major+6218minor)pagefaults 0swaps
Command terminated by signal 2
0.00user 0.00system 0:00.00elapsed 100%CPU (0avgtext+0avgdata 5796maxresident)k
0inputs+0outputs (0major+383minor)pagefaults 0swaps
Fatal Python error: initsite: Failed to import the site module
Traceback (most recent call last):
  File "/usr/lib/python3.7/site.py", line 79, in <module>
    import os
  File "<frozen importlib._bootstrap>", line 983, in _find_and_load
  File "<frozen importlib._bootstrap>", line 967, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 677, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 857, in get_code
  File "<frozen importlib._bootstrap_external>", line 525, in _compile_bytecode
KeyboardInterrupt
Command exited with non-zero status 1
0.00user 0.01system 0:00.01elapsed 94%CPU (0avgtext+0avgdata 8340maxresident)k
0inputs+0outputs (0major+747minor)pagefaults 0swaps
Traceback (most recent call last):
  File "third_party/my_run_tag.py", line 30, in <module>
    import numpy as np
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/numpy/__init__.py", line 228, in <module>
    core.getlimits._register_known_types()
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/numpy/core/getlimits.py", line 111, in _register_known_types
    tiny=f16(2 ** -14))
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/numpy/core/getlimits.py", line 35, in __init__
    def __init__(self,
KeyboardInterrupt
Command terminated by signal 2
1.09user 1.62system 0:00.17elapsed 1597%CPU (0avgtext+0avgdata 30356maxresident)k
0inputs+0outputs (0major+7161minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
12/25/2021 22:25:12 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_ensemble_oracle_top1/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ar', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_ensemble_oracle_top1//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
12/25/2021 22:25:12 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
12/25/2021 22:25:12 - INFO - __main__ -   Seed = 1
12/25/2021 22:25:12 - INFO - root -   save model
12/25/2021 22:25:12 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_ensemble_oracle_top1/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ar', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_ensemble_oracle_top1//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
12/25/2021 22:25:12 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
12/25/2021 22:25:14 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
12/25/2021 22:25:20 - INFO - __main__ -   Using lang2id = None
12/25/2021 22:25:20 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
12/25/2021 22:25:20 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
12/25/2021 22:25:20 - INFO - root -   Trying to decide if add adapter
12/25/2021 22:25:20 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_model_head.bin
12/25/2021 22:25:20 - INFO - root -   loading lang adpater topk
12/25/2021 22:25:20 - INFO - __main__ -   Loading Adapter Languages from scripts/udpos/en/ar.json
12/25/2021 22:25:20 - INFO - __main__ -   Adapter Languages : ['ka'], Length : 1
12/25/2021 22:25:20 - INFO - __main__ -   Adapter Names ['ka/wiki@ukp'], Length : 1
12/25/2021 22:25:20 - INFO - __main__ -   Language = ka
12/25/2021 22:25:20 - INFO - __main__ -   Adapter Name = ka/wiki@ukp
Found matching adapter at: adapters/ukp/bert-base-multilingual-cased_ka_wiki_pfeiffer.json
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/ka/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_ka_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/aa02653c11f947d33a6fbda9ca26df2e87b26edf522f6641754238580b6e29cc-9e071283e8b0e30d7c18f6466800264db846fa4270983732130add1d82f52794-extracted/adapter_config.json
Adding adapter 'ka' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/aa02653c11f947d33a6fbda9ca26df2e87b26edf522f6641754238580b6e29cc-9e071283e8b0e30d7c18f6466800264db846fa4270983732130add1d82f52794-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/aa02653c11f947d33a6fbda9ca26df2e87b26edf522f6641754238580b6e29cc-9e071283e8b0e30d7c18f6466800264db846fa4270983732130add1d82f52794-extracted/head_config.json
12/25/2021 22:25:27 - INFO - __main__ -   Args Adapter Weight = equal
12/25/2021 22:25:27 - INFO - __main__ -   Adapter Languages = ['ka']
12/25/2021 22:25:27 - INFO - __main__ -   Adapter Weights = [1.0]
12/25/2021 22:25:27 - INFO - __main__ -   Sum of Adapter Weights = 1.0
12/25/2021 22:25:27 - INFO - __main__ -   Length of Adapter Weights = 1
12/25/2021 22:25:27 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_ar_bert-base-multilingual-cased_128
12/25/2021 22:25:28 - INFO - __main__ -   ***** Running evaluation  in ar *****
12/25/2021 22:25:28 - INFO - __main__ -     Num examples = 1784
12/25/2021 22:25:28 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/56 [00:00<?, ?it/s]12/25/2021 22:25:28 - INFO - __main__ -   Batch number = 1
Evaluating:   2%|         | 1/56 [00:00<00:08,  6.80it/s]12/25/2021 22:25:28 - INFO - __main__ -   Batch number = 2
Evaluating:   4%|         | 2/56 [00:00<00:07,  7.05it/s]12/25/2021 22:25:28 - INFO - __main__ -   Batch number = 3
Evaluating:   5%|         | 3/56 [00:00<00:07,  7.28it/s]12/25/2021 22:25:28 - INFO - __main__ -   Batch number = 4
Evaluating:   7%|         | 4/56 [00:00<00:06,  7.50it/s]12/25/2021 22:25:28 - INFO - __main__ -   Batch number = 5
Evaluating:   9%|         | 5/56 [00:00<00:06,  7.61it/s]12/25/2021 22:25:29 - INFO - __main__ -   Batch number = 6
Evaluating:  11%|         | 6/56 [00:00<00:06,  7.69it/s]12/25/2021 22:25:29 - INFO - __main__ -   Batch number = 7
Evaluating:  12%|        | 7/56 [00:00<00:06,  7.73it/s]12/25/2021 22:25:29 - INFO - __main__ -   Batch number = 8
Evaluating:  14%|        | 8/56 [00:01<00:06,  7.77it/s]12/25/2021 22:25:29 - INFO - __main__ -   Batch number = 9
Evaluating:  16%|        | 9/56 [00:01<00:06,  7.80it/s]12/25/2021 22:25:29 - INFO - __main__ -   Batch number = 10
Evaluating:  18%|        | 10/56 [00:01<00:05,  7.79it/s]12/25/2021 22:25:29 - INFO - __main__ -   Batch number = 11
Evaluating:  20%|        | 11/56 [00:01<00:05,  7.74it/s]12/25/2021 22:25:29 - INFO - __main__ -   Batch number = 12
Evaluating:  21%|       | 12/56 [00:01<00:05,  7.75it/s]12/25/2021 22:25:30 - INFO - __main__ -   Batch number = 13
Evaluating:  23%|       | 13/56 [00:01<00:05,  7.76it/s]12/25/2021 22:25:30 - INFO - __main__ -   Batch number = 14
Evaluating:  25%|       | 14/56 [00:01<00:05,  7.74it/s]12/25/2021 22:25:30 - INFO - __main__ -   Batch number = 15
Evaluating:  27%|       | 15/56 [00:01<00:05,  7.75it/s]12/25/2021 22:25:30 - INFO - __main__ -   Batch number = 16
Evaluating:  29%|       | 16/56 [00:02<00:05,  7.75it/s]12/25/2021 22:25:30 - INFO - __main__ -   Batch number = 17
Evaluating:  30%|       | 17/56 [00:02<00:05,  7.75it/s]12/25/2021 22:25:30 - INFO - __main__ -   Batch number = 18
Evaluating:  32%|      | 18/56 [00:02<00:04,  7.76it/s]12/25/2021 22:25:30 - INFO - __main__ -   Batch number = 19
Evaluating:  34%|      | 19/56 [00:02<00:04,  7.76it/s]12/25/2021 22:25:30 - INFO - __main__ -   Batch number = 20
Evaluating:  36%|      | 20/56 [00:02<00:04,  7.74it/s]12/25/2021 22:25:31 - INFO - __main__ -   Batch number = 21
Evaluating:  38%|      | 21/56 [00:02<00:04,  7.76it/s]12/25/2021 22:25:31 - INFO - __main__ -   Batch number = 22
Evaluating:  39%|      | 22/56 [00:02<00:04,  7.76it/s]12/25/2021 22:25:31 - INFO - __main__ -   Batch number = 23
Evaluating:  41%|      | 23/56 [00:03<00:04,  7.03it/s]12/25/2021 22:25:31 - INFO - __main__ -   Batch number = 24
Evaluating:  43%|     | 24/56 [00:03<00:04,  7.23it/s]12/25/2021 22:25:31 - INFO - __main__ -   Batch number = 25
Evaluating:  45%|     | 25/56 [00:03<00:04,  7.37it/s]12/25/2021 22:25:31 - INFO - __main__ -   Batch number = 26
Evaluating:  46%|     | 26/56 [00:03<00:04,  7.45it/s]12/25/2021 22:25:31 - INFO - __main__ -   Batch number = 27
Evaluating:  48%|     | 27/56 [00:03<00:03,  7.55it/s]12/25/2021 22:25:31 - INFO - __main__ -   Batch number = 28
Evaluating:  50%|     | 28/56 [00:03<00:03,  7.60it/s]12/25/2021 22:25:32 - INFO - __main__ -   Batch number = 29
Evaluating:  52%|    | 29/56 [00:03<00:03,  7.59it/s]12/25/2021 22:25:32 - INFO - __main__ -   Batch number = 30
Evaluating:  54%|    | 30/56 [00:03<00:03,  7.63it/s]12/25/2021 22:25:32 - INFO - __main__ -   Batch number = 31
Evaluating:  55%|    | 31/56 [00:04<00:03,  7.66it/s]12/25/2021 22:25:32 - INFO - __main__ -   Batch number = 32
Evaluating:  57%|    | 32/56 [00:04<00:03,  7.63it/s]12/25/2021 22:25:32 - INFO - __main__ -   Batch number = 33
Evaluating:  59%|    | 33/56 [00:04<00:03,  7.65it/s]12/25/2021 22:25:32 - INFO - __main__ -   Batch number = 34
Evaluating:  61%|    | 34/56 [00:04<00:02,  7.66it/s]12/25/2021 22:25:32 - INFO - __main__ -   Batch number = 35
Evaluating:  62%|   | 35/56 [00:04<00:03,  6.59it/s]12/25/2021 22:25:33 - INFO - __main__ -   Batch number = 36
Evaluating:  64%|   | 36/56 [00:04<00:02,  6.87it/s]12/25/2021 22:25:33 - INFO - __main__ -   Batch number = 37
Evaluating:  66%|   | 37/56 [00:04<00:02,  7.10it/s]12/25/2021 22:25:33 - INFO - __main__ -   Batch number = 38
Evaluating:  68%|   | 38/56 [00:05<00:02,  7.25it/s]12/25/2021 22:25:33 - INFO - __main__ -   Batch number = 39
Evaluating:  70%|   | 39/56 [00:05<00:02,  7.38it/s]12/25/2021 22:25:33 - INFO - __main__ -   Batch number = 40
Evaluating:  71%|  | 40/56 [00:05<00:02,  7.45it/s]12/25/2021 22:25:33 - INFO - __main__ -   Batch number = 41
Evaluating:  73%|  | 41/56 [00:05<00:02,  7.49it/s]12/25/2021 22:25:33 - INFO - __main__ -   Batch number = 42
Evaluating:  75%|  | 42/56 [00:05<00:02,  6.97it/s]12/25/2021 22:25:34 - INFO - __main__ -   Batch number = 43
Evaluating:  77%|  | 43/56 [00:05<00:01,  7.16it/s]12/25/2021 22:25:34 - INFO - __main__ -   Batch number = 44
Evaluating:  79%|  | 44/56 [00:05<00:01,  7.29it/s]12/25/2021 22:25:34 - INFO - __main__ -   Batch number = 45
Evaluating:  80%|  | 45/56 [00:06<00:01,  7.40it/s]12/25/2021 22:25:34 - INFO - __main__ -   Batch number = 46
Evaluating:  82%| | 46/56 [00:06<00:01,  7.47it/s]12/25/2021 22:25:34 - INFO - __main__ -   Batch number = 47
Evaluating:  84%| | 47/56 [00:06<00:01,  6.92it/s]12/25/2021 22:25:34 - INFO - __main__ -   Batch number = 48
Evaluating:  86%| | 48/56 [00:06<00:01,  7.11it/s]12/25/2021 22:25:34 - INFO - __main__ -   Batch number = 49
Evaluating:  88%| | 49/56 [00:06<00:00,  7.27it/s]12/25/2021 22:25:35 - INFO - __main__ -   Batch number = 50
Evaluating:  89%| | 50/56 [00:06<00:00,  7.35it/s]12/25/2021 22:25:35 - INFO - __main__ -   Batch number = 51
Evaluating:  91%| | 51/56 [00:06<00:00,  7.42it/s]12/25/2021 22:25:35 - INFO - __main__ -   Batch number = 52
Evaluating:  93%|| 52/56 [00:06<00:00,  7.46it/s]12/25/2021 22:25:35 - INFO - __main__ -   Batch number = 53
Evaluating:  95%|| 53/56 [00:07<00:00,  7.48it/s]12/25/2021 22:25:35 - INFO - __main__ -   Batch number = 54
Evaluating:  96%|| 54/56 [00:07<00:00,  7.52it/s]12/25/2021 22:25:35 - INFO - __main__ -   Batch number = 55
Evaluating:  98%|| 55/56 [00:07<00:00,  7.54it/s]12/25/2021 22:25:35 - INFO - __main__ -   Batch number = 56
Evaluating: 100%|| 56/56 [00:07<00:00,  8.04it/s]Evaluating: 100%|| 56/56 [00:07<00:00,  7.50it/s]
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
12/25/2021 22:25:36 - INFO - __main__ -   ***** Evaluation result  in ar *****
12/25/2021 22:25:36 - INFO - __main__ -     f1 = 0.598398509760294
12/25/2021 22:25:36 - INFO - __main__ -     loss = 1.4291882035987717
12/25/2021 22:25:36 - INFO - __main__ -     precision = 0.5879257752923234
12/25/2021 22:25:36 - INFO - __main__ -     recall = 0.6092511129257435
21.51user 8.99system 0:26.94elapsed 113%CPU (0avgtext+0avgdata 3937320maxresident)k
3016inputs+536outputs (0major+1366701minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
12/25/2021 22:25:39 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_ensemble_oracle_top1/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ar', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_ensemble_oracle_top1//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
12/25/2021 22:25:39 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
12/25/2021 22:25:39 - INFO - __main__ -   Seed = 2
12/25/2021 22:25:39 - INFO - root -   save model
12/25/2021 22:25:39 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_ensemble_oracle_top1/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ar', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_ensemble_oracle_top1//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
12/25/2021 22:25:39 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
12/25/2021 22:25:41 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
12/25/2021 22:25:47 - INFO - __main__ -   Using lang2id = None
12/25/2021 22:25:47 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
12/25/2021 22:25:47 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
12/25/2021 22:25:47 - INFO - root -   Trying to decide if add adapter
12/25/2021 22:25:47 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_model_head.bin
12/25/2021 22:25:47 - INFO - root -   loading lang adpater topk
12/25/2021 22:25:47 - INFO - __main__ -   Loading Adapter Languages from scripts/udpos/en/ar.json
12/25/2021 22:25:47 - INFO - __main__ -   Adapter Languages : ['ka'], Length : 1
12/25/2021 22:25:47 - INFO - __main__ -   Adapter Names ['ka/wiki@ukp'], Length : 1
12/25/2021 22:25:47 - INFO - __main__ -   Language = ka
12/25/2021 22:25:47 - INFO - __main__ -   Adapter Name = ka/wiki@ukp
Found matching adapter at: adapters/ukp/bert-base-multilingual-cased_ka_wiki_pfeiffer.json
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/ka/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_ka_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/aa02653c11f947d33a6fbda9ca26df2e87b26edf522f6641754238580b6e29cc-9e071283e8b0e30d7c18f6466800264db846fa4270983732130add1d82f52794-extracted/adapter_config.json
Adding adapter 'ka' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/aa02653c11f947d33a6fbda9ca26df2e87b26edf522f6641754238580b6e29cc-9e071283e8b0e30d7c18f6466800264db846fa4270983732130add1d82f52794-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/aa02653c11f947d33a6fbda9ca26df2e87b26edf522f6641754238580b6e29cc-9e071283e8b0e30d7c18f6466800264db846fa4270983732130add1d82f52794-extracted/head_config.json
12/25/2021 22:25:53 - INFO - __main__ -   Args Adapter Weight = equal
12/25/2021 22:25:53 - INFO - __main__ -   Adapter Languages = ['ka']
12/25/2021 22:25:53 - INFO - __main__ -   Adapter Weights = [1.0]
12/25/2021 22:25:53 - INFO - __main__ -   Sum of Adapter Weights = 1.0
12/25/2021 22:25:53 - INFO - __main__ -   Length of Adapter Weights = 1
12/25/2021 22:25:53 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_ar_bert-base-multilingual-cased_128
12/25/2021 22:25:53 - INFO - __main__ -   ***** Running evaluation  in ar *****
12/25/2021 22:25:53 - INFO - __main__ -     Num examples = 1784
12/25/2021 22:25:53 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/56 [00:00<?, ?it/s]12/25/2021 22:25:53 - INFO - __main__ -   Batch number = 1
Evaluating:   2%|         | 1/56 [00:00<00:08,  6.85it/s]12/25/2021 22:25:53 - INFO - __main__ -   Batch number = 2
Evaluating:   4%|         | 2/56 [00:00<00:07,  6.83it/s]12/25/2021 22:25:54 - INFO - __main__ -   Batch number = 3
Evaluating:   5%|         | 3/56 [00:00<00:07,  7.17it/s]12/25/2021 22:25:54 - INFO - __main__ -   Batch number = 4
Evaluating:   7%|         | 4/56 [00:00<00:07,  7.40it/s]12/25/2021 22:25:54 - INFO - __main__ -   Batch number = 5
Evaluating:   9%|         | 5/56 [00:00<00:06,  7.54it/s]12/25/2021 22:25:54 - INFO - __main__ -   Batch number = 6
Evaluating:  11%|         | 6/56 [00:00<00:06,  7.62it/s]12/25/2021 22:25:54 - INFO - __main__ -   Batch number = 7
Evaluating:  12%|        | 7/56 [00:00<00:06,  7.67it/s]12/25/2021 22:25:54 - INFO - __main__ -   Batch number = 8
Evaluating:  14%|        | 8/56 [00:01<00:06,  7.69it/s]12/25/2021 22:25:54 - INFO - __main__ -   Batch number = 9
Evaluating:  16%|        | 9/56 [00:01<00:06,  7.71it/s]12/25/2021 22:25:54 - INFO - __main__ -   Batch number = 10
Evaluating:  18%|        | 10/56 [00:01<00:05,  7.73it/s]12/25/2021 22:25:55 - INFO - __main__ -   Batch number = 11
Evaluating:  20%|        | 11/56 [00:01<00:05,  7.72it/s]12/25/2021 22:25:55 - INFO - __main__ -   Batch number = 12
Evaluating:  21%|       | 12/56 [00:01<00:05,  7.71it/s]12/25/2021 22:25:55 - INFO - __main__ -   Batch number = 13
Evaluating:  23%|       | 13/56 [00:01<00:05,  7.71it/s]12/25/2021 22:25:55 - INFO - __main__ -   Batch number = 14
Evaluating:  25%|       | 14/56 [00:01<00:05,  7.73it/s]12/25/2021 22:25:55 - INFO - __main__ -   Batch number = 15
Evaluating:  27%|       | 15/56 [00:01<00:05,  7.72it/s]12/25/2021 22:25:55 - INFO - __main__ -   Batch number = 16
Evaluating:  29%|       | 16/56 [00:02<00:05,  7.73it/s]12/25/2021 22:25:55 - INFO - __main__ -   Batch number = 17
Evaluating:  30%|       | 17/56 [00:02<00:05,  7.74it/s]12/25/2021 22:25:56 - INFO - __main__ -   Batch number = 18
Evaluating:  32%|      | 18/56 [00:02<00:04,  7.71it/s]12/25/2021 22:25:56 - INFO - __main__ -   Batch number = 19
Evaluating:  34%|      | 19/56 [00:02<00:04,  7.71it/s]12/25/2021 22:25:56 - INFO - __main__ -   Batch number = 20
Evaluating:  36%|      | 20/56 [00:02<00:04,  7.71it/s]12/25/2021 22:25:56 - INFO - __main__ -   Batch number = 21
Evaluating:  38%|      | 21/56 [00:02<00:04,  7.69it/s]12/25/2021 22:25:56 - INFO - __main__ -   Batch number = 22
Evaluating:  39%|      | 22/56 [00:02<00:04,  7.63it/s]12/25/2021 22:25:56 - INFO - __main__ -   Batch number = 23
Evaluating:  41%|      | 23/56 [00:03<00:04,  7.55it/s]12/25/2021 22:25:56 - INFO - __main__ -   Batch number = 24
Evaluating:  43%|     | 24/56 [00:03<00:04,  6.97it/s]12/25/2021 22:25:56 - INFO - __main__ -   Batch number = 25
Evaluating:  45%|     | 25/56 [00:03<00:04,  7.16it/s]12/25/2021 22:25:57 - INFO - __main__ -   Batch number = 26
Evaluating:  46%|     | 26/56 [00:03<00:04,  7.30it/s]12/25/2021 22:25:57 - INFO - __main__ -   Batch number = 27
Evaluating:  48%|     | 27/56 [00:03<00:03,  7.38it/s]12/25/2021 22:25:57 - INFO - __main__ -   Batch number = 28
Evaluating:  50%|     | 28/56 [00:03<00:03,  7.46it/s]12/25/2021 22:25:57 - INFO - __main__ -   Batch number = 29
Evaluating:  52%|    | 29/56 [00:03<00:03,  7.52it/s]12/25/2021 22:25:57 - INFO - __main__ -   Batch number = 30
Evaluating:  54%|    | 30/56 [00:03<00:03,  7.54it/s]12/25/2021 22:25:57 - INFO - __main__ -   Batch number = 31
Evaluating:  55%|    | 31/56 [00:04<00:03,  7.56it/s]12/25/2021 22:25:57 - INFO - __main__ -   Batch number = 32
Evaluating:  57%|    | 32/56 [00:04<00:03,  7.59it/s]12/25/2021 22:25:58 - INFO - __main__ -   Batch number = 33
Evaluating:  59%|    | 33/56 [00:04<00:03,  7.29it/s]12/25/2021 22:25:58 - INFO - __main__ -   Batch number = 34
Evaluating:  61%|    | 34/56 [00:04<00:02,  7.40it/s]12/25/2021 22:25:58 - INFO - __main__ -   Batch number = 35
Evaluating:  62%|   | 35/56 [00:04<00:02,  7.42it/s]12/25/2021 22:25:58 - INFO - __main__ -   Batch number = 36
Evaluating:  64%|   | 36/56 [00:04<00:02,  7.47it/s]12/25/2021 22:25:58 - INFO - __main__ -   Batch number = 37
Evaluating:  66%|   | 37/56 [00:04<00:02,  7.53it/s]12/25/2021 22:25:58 - INFO - __main__ -   Batch number = 38
Evaluating:  68%|   | 38/56 [00:05<00:02,  7.56it/s]12/25/2021 22:25:58 - INFO - __main__ -   Batch number = 39
Evaluating:  70%|   | 39/56 [00:05<00:02,  7.48it/s]12/25/2021 22:25:58 - INFO - __main__ -   Batch number = 40
Evaluating:  71%|  | 40/56 [00:05<00:02,  7.44it/s]12/25/2021 22:25:59 - INFO - __main__ -   Batch number = 41
Evaluating:  73%|  | 41/56 [00:05<00:02,  7.48it/s]12/25/2021 22:25:59 - INFO - __main__ -   Batch number = 42
Evaluating:  75%|  | 42/56 [00:05<00:01,  7.49it/s]12/25/2021 22:25:59 - INFO - __main__ -   Batch number = 43
Evaluating:  77%|  | 43/56 [00:05<00:01,  7.52it/s]12/25/2021 22:25:59 - INFO - __main__ -   Batch number = 44
Evaluating:  79%|  | 44/56 [00:05<00:01,  7.53it/s]12/25/2021 22:25:59 - INFO - __main__ -   Batch number = 45
Evaluating:  80%|  | 45/56 [00:06<00:01,  7.03it/s]12/25/2021 22:25:59 - INFO - __main__ -   Batch number = 46
Evaluating:  82%| | 46/56 [00:06<00:01,  7.18it/s]12/25/2021 22:25:59 - INFO - __main__ -   Batch number = 47
Evaluating:  84%| | 47/56 [00:06<00:01,  7.29it/s]12/25/2021 22:26:00 - INFO - __main__ -   Batch number = 48
Evaluating:  86%| | 48/56 [00:06<00:01,  7.35it/s]12/25/2021 22:26:00 - INFO - __main__ -   Batch number = 49
Evaluating:  88%| | 49/56 [00:06<00:00,  7.42it/s]12/25/2021 22:26:00 - INFO - __main__ -   Batch number = 50
Evaluating:  89%| | 50/56 [00:06<00:00,  7.47it/s]12/25/2021 22:26:00 - INFO - __main__ -   Batch number = 51
Evaluating:  91%| | 51/56 [00:06<00:00,  7.49it/s]12/25/2021 22:26:00 - INFO - __main__ -   Batch number = 52
Evaluating:  93%|| 52/56 [00:06<00:00,  7.50it/s]12/25/2021 22:26:00 - INFO - __main__ -   Batch number = 53
Evaluating:  95%|| 53/56 [00:07<00:00,  7.52it/s]12/25/2021 22:26:00 - INFO - __main__ -   Batch number = 54
Evaluating:  96%|| 54/56 [00:07<00:00,  7.50it/s]12/25/2021 22:26:00 - INFO - __main__ -   Batch number = 55
Evaluating:  98%|| 55/56 [00:07<00:00,  7.51it/s]12/25/2021 22:26:01 - INFO - __main__ -   Batch number = 56
Evaluating: 100%|| 56/56 [00:07<00:00,  8.05it/s]Evaluating: 100%|| 56/56 [00:07<00:00,  7.52it/s]
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
12/25/2021 22:26:02 - INFO - __main__ -   ***** Evaluation result  in ar *****
12/25/2021 22:26:02 - INFO - __main__ -     f1 = 0.5692174965555427
12/25/2021 22:26:02 - INFO - __main__ -     loss = 1.7461669647267886
12/25/2021 22:26:02 - INFO - __main__ -     precision = 0.5567786790266512
12/25/2021 22:26:02 - INFO - __main__ -     recall = 0.5822247978294656
19.71user 7.54system 0:25.24elapsed 107%CPU (0avgtext+0avgdata 3940424maxresident)k
0inputs+528outputs (0major+1548066minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
12/25/2021 22:26:04 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_ensemble_oracle_top1/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ar', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_ensemble_oracle_top1//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
12/25/2021 22:26:04 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
12/25/2021 22:26:04 - INFO - __main__ -   Seed = 3
12/25/2021 22:26:04 - INFO - root -   save model
12/25/2021 22:26:04 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_ensemble_oracle_top1/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ar', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_ensemble_oracle_top1//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
12/25/2021 22:26:04 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
12/25/2021 22:26:07 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
12/25/2021 22:26:12 - INFO - __main__ -   Using lang2id = None
12/25/2021 22:26:12 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
12/25/2021 22:26:12 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
12/25/2021 22:26:12 - INFO - root -   Trying to decide if add adapter
12/25/2021 22:26:12 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_model_head.bin
12/25/2021 22:26:12 - INFO - root -   loading lang adpater topk
12/25/2021 22:26:12 - INFO - __main__ -   Loading Adapter Languages from scripts/udpos/en/ar.json
12/25/2021 22:26:12 - INFO - __main__ -   Adapter Languages : ['ka'], Length : 1
12/25/2021 22:26:12 - INFO - __main__ -   Adapter Names ['ka/wiki@ukp'], Length : 1
12/25/2021 22:26:12 - INFO - __main__ -   Language = ka
12/25/2021 22:26:12 - INFO - __main__ -   Adapter Name = ka/wiki@ukp
Found matching adapter at: adapters/ukp/bert-base-multilingual-cased_ka_wiki_pfeiffer.json
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/ka/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_ka_wiki_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/aa02653c11f947d33a6fbda9ca26df2e87b26edf522f6641754238580b6e29cc-9e071283e8b0e30d7c18f6466800264db846fa4270983732130add1d82f52794-extracted/adapter_config.json
Adding adapter 'ka' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/aa02653c11f947d33a6fbda9ca26df2e87b26edf522f6641754238580b6e29cc-9e071283e8b0e30d7c18f6466800264db846fa4270983732130add1d82f52794-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/aa02653c11f947d33a6fbda9ca26df2e87b26edf522f6641754238580b6e29cc-9e071283e8b0e30d7c18f6466800264db846fa4270983732130add1d82f52794-extracted/head_config.json
12/25/2021 22:26:18 - INFO - __main__ -   Args Adapter Weight = equal
12/25/2021 22:26:18 - INFO - __main__ -   Adapter Languages = ['ka']
12/25/2021 22:26:18 - INFO - __main__ -   Adapter Weights = [1.0]
12/25/2021 22:26:18 - INFO - __main__ -   Sum of Adapter Weights = 1.0
12/25/2021 22:26:18 - INFO - __main__ -   Length of Adapter Weights = 1
12/25/2021 22:26:18 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_ar_bert-base-multilingual-cased_128
12/25/2021 22:26:19 - INFO - __main__ -   ***** Running evaluation  in ar *****
12/25/2021 22:26:19 - INFO - __main__ -     Num examples = 1784
12/25/2021 22:26:19 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/56 [00:00<?, ?it/s]12/25/2021 22:26:19 - INFO - __main__ -   Batch number = 1
Evaluating:   2%|         | 1/56 [00:00<00:07,  6.88it/s]12/25/2021 22:26:19 - INFO - __main__ -   Batch number = 2
Evaluating:   4%|         | 2/56 [00:00<00:07,  7.38it/s]12/25/2021 22:26:19 - INFO - __main__ -   Batch number = 3
Evaluating:   5%|         | 3/56 [00:00<00:07,  7.56it/s]12/25/2021 22:26:19 - INFO - __main__ -   Batch number = 4
Evaluating:   7%|         | 4/56 [00:00<00:06,  7.65it/s]12/25/2021 22:26:19 - INFO - __main__ -   Batch number = 5
Evaluating:   9%|         | 5/56 [00:00<00:06,  7.67it/s]12/25/2021 22:26:19 - INFO - __main__ -   Batch number = 6
Evaluating:  11%|         | 6/56 [00:00<00:06,  7.67it/s]12/25/2021 22:26:19 - INFO - __main__ -   Batch number = 7
Evaluating:  12%|        | 7/56 [00:00<00:06,  7.68it/s]12/25/2021 22:26:20 - INFO - __main__ -   Batch number = 8
Evaluating:  14%|        | 8/56 [00:01<00:06,  7.68it/s]12/25/2021 22:26:20 - INFO - __main__ -   Batch number = 9
Evaluating:  16%|        | 9/56 [00:01<00:06,  7.67it/s]12/25/2021 22:26:20 - INFO - __main__ -   Batch number = 10
Evaluating:  18%|        | 10/56 [00:01<00:05,  7.67it/s]12/25/2021 22:26:20 - INFO - __main__ -   Batch number = 11
Evaluating:  20%|        | 11/56 [00:01<00:05,  7.66it/s]12/25/2021 22:26:20 - INFO - __main__ -   Batch number = 12
Evaluating:  21%|       | 12/56 [00:01<00:05,  7.66it/s]12/25/2021 22:26:20 - INFO - __main__ -   Batch number = 13
Evaluating:  23%|       | 13/56 [00:01<00:05,  7.68it/s]12/25/2021 22:26:20 - INFO - __main__ -   Batch number = 14
Evaluating:  25%|       | 14/56 [00:01<00:05,  7.69it/s]12/25/2021 22:26:20 - INFO - __main__ -   Batch number = 15
Evaluating:  27%|       | 15/56 [00:01<00:05,  7.66it/s]12/25/2021 22:26:21 - INFO - __main__ -   Batch number = 16
Evaluating:  29%|       | 16/56 [00:02<00:05,  7.68it/s]12/25/2021 22:26:21 - INFO - __main__ -   Batch number = 17
Evaluating:  30%|       | 17/56 [00:02<00:05,  7.67it/s]12/25/2021 22:26:21 - INFO - __main__ -   Batch number = 18
Evaluating:  32%|      | 18/56 [00:02<00:04,  7.65it/s]12/25/2021 22:26:21 - INFO - __main__ -   Batch number = 19
Evaluating:  34%|      | 19/56 [00:02<00:04,  7.66it/s]12/25/2021 22:26:21 - INFO - __main__ -   Batch number = 20
Evaluating:  36%|      | 20/56 [00:02<00:04,  7.65it/s]12/25/2021 22:26:21 - INFO - __main__ -   Batch number = 21
Evaluating:  38%|      | 21/56 [00:02<00:04,  7.64it/s]12/25/2021 22:26:21 - INFO - __main__ -   Batch number = 22
Evaluating:  39%|      | 22/56 [00:02<00:04,  7.64it/s]12/25/2021 22:26:21 - INFO - __main__ -   Batch number = 23
Evaluating:  41%|      | 23/56 [00:03<00:04,  7.65it/s]12/25/2021 22:26:22 - INFO - __main__ -   Batch number = 24
Evaluating:  43%|     | 24/56 [00:03<00:04,  7.61it/s]12/25/2021 22:26:22 - INFO - __main__ -   Batch number = 25
Evaluating:  45%|     | 25/56 [00:03<00:04,  7.61it/s]12/25/2021 22:26:22 - INFO - __main__ -   Batch number = 26
Evaluating:  46%|     | 26/56 [00:03<00:03,  7.61it/s]12/25/2021 22:26:22 - INFO - __main__ -   Batch number = 27
Evaluating:  48%|     | 27/56 [00:03<00:03,  7.60it/s]12/25/2021 22:26:22 - INFO - __main__ -   Batch number = 28
Evaluating:  50%|     | 28/56 [00:03<00:03,  7.60it/s]12/25/2021 22:26:22 - INFO - __main__ -   Batch number = 29
Evaluating:  52%|    | 29/56 [00:03<00:03,  7.59it/s]12/25/2021 22:26:22 - INFO - __main__ -   Batch number = 30
Evaluating:  54%|    | 30/56 [00:03<00:03,  7.50it/s]12/25/2021 22:26:23 - INFO - __main__ -   Batch number = 31
Evaluating:  55%|    | 31/56 [00:04<00:03,  7.48it/s]12/25/2021 22:26:23 - INFO - __main__ -   Batch number = 32
Evaluating:  57%|    | 32/56 [00:04<00:03,  7.52it/s]12/25/2021 22:26:23 - INFO - __main__ -   Batch number = 33
Evaluating:  59%|    | 33/56 [00:04<00:03,  7.52it/s]12/25/2021 22:26:23 - INFO - __main__ -   Batch number = 34
Evaluating:  61%|    | 34/56 [00:04<00:02,  7.54it/s]12/25/2021 22:26:23 - INFO - __main__ -   Batch number = 35
Evaluating:  62%|   | 35/56 [00:04<00:02,  7.55it/s]12/25/2021 22:26:23 - INFO - __main__ -   Batch number = 36
Evaluating:  64%|   | 36/56 [00:04<00:02,  7.36it/s]12/25/2021 22:26:23 - INFO - __main__ -   Batch number = 37
Evaluating:  66%|   | 37/56 [00:04<00:02,  7.43it/s]12/25/2021 22:26:24 - INFO - __main__ -   Batch number = 38
Evaluating:  68%|   | 38/56 [00:05<00:02,  7.02it/s]12/25/2021 22:26:24 - INFO - __main__ -   Batch number = 39
Evaluating:  70%|   | 39/56 [00:05<00:02,  7.15it/s]12/25/2021 22:26:24 - INFO - __main__ -   Batch number = 40
Evaluating:  71%|  | 40/56 [00:05<00:02,  7.26it/s]12/25/2021 22:26:24 - INFO - __main__ -   Batch number = 41
Evaluating:  73%|  | 41/56 [00:05<00:02,  7.34it/s]12/25/2021 22:26:24 - INFO - __main__ -   Batch number = 42
Evaluating:  75%|  | 42/56 [00:05<00:01,  7.39it/s]12/25/2021 22:26:24 - INFO - __main__ -   Batch number = 43
Evaluating:  77%|  | 43/56 [00:05<00:01,  7.44it/s]12/25/2021 22:26:24 - INFO - __main__ -   Batch number = 44
Evaluating:  79%|  | 44/56 [00:05<00:01,  7.46it/s]12/25/2021 22:26:24 - INFO - __main__ -   Batch number = 45
Evaluating:  80%|  | 45/56 [00:05<00:01,  7.43it/s]12/25/2021 22:26:25 - INFO - __main__ -   Batch number = 46
Evaluating:  82%| | 46/56 [00:06<00:01,  7.46it/s]12/25/2021 22:26:25 - INFO - __main__ -   Batch number = 47
Evaluating:  84%| | 47/56 [00:06<00:01,  7.43it/s]12/25/2021 22:26:25 - INFO - __main__ -   Batch number = 48
Evaluating:  86%| | 48/56 [00:06<00:01,  5.23it/s]12/25/2021 22:26:25 - INFO - __main__ -   Batch number = 49
Evaluating:  88%| | 49/56 [00:06<00:01,  5.72it/s]12/25/2021 22:26:25 - INFO - __main__ -   Batch number = 50
Evaluating:  89%| | 50/56 [00:06<00:00,  6.03it/s]12/25/2021 22:26:25 - INFO - __main__ -   Batch number = 51
Evaluating:  91%| | 51/56 [00:06<00:00,  6.23it/s]12/25/2021 22:26:26 - INFO - __main__ -   Batch number = 52
Evaluating:  93%|| 52/56 [00:07<00:00,  6.39it/s]12/25/2021 22:26:26 - INFO - __main__ -   Batch number = 53
Evaluating:  95%|| 53/56 [00:07<00:00,  6.50it/s]12/25/2021 22:26:26 - INFO - __main__ -   Batch number = 54
Evaluating:  96%|| 54/56 [00:07<00:00,  6.57it/s]12/25/2021 22:26:26 - INFO - __main__ -   Batch number = 55
Evaluating:  98%|| 55/56 [00:07<00:00,  6.64it/s]12/25/2021 22:26:26 - INFO - __main__ -   Batch number = 56
Evaluating: 100%|| 56/56 [00:07<00:00,  7.15it/s]Evaluating: 100%|| 56/56 [00:07<00:00,  7.27it/s]
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
12/25/2021 22:26:27 - INFO - __main__ -   ***** Evaluation result  in ar *****
12/25/2021 22:26:27 - INFO - __main__ -     f1 = 0.5837035501962257
12/25/2021 22:26:27 - INFO - __main__ -     loss = 1.4693767045225417
12/25/2021 22:26:27 - INFO - __main__ -     precision = 0.5741769442462542
12/25/2021 22:26:27 - INFO - __main__ -     recall = 0.5935516160472039
19.58user 7.56system 0:25.77elapsed 105%CPU (0avgtext+0avgdata 3936764maxresident)k
6608inputs+528outputs (0major+1508838minor)pagefaults 0swaps
PyTorch version 1.10.0+cu102 available.
12/25/2021 22:26:30 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_ensemble_oracle_top1/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='de', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_ensemble_oracle_top1//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
12/25/2021 22:26:30 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
12/25/2021 22:26:30 - INFO - __main__ -   Seed = 1
12/25/2021 22:26:30 - INFO - root -   save model
12/25/2021 22:26:30 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_ensemble_oracle_top1/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='de', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_ensemble_oracle_top1//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
12/25/2021 22:26:30 - INFO - __main__ -   Loading pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
12/25/2021 22:26:32 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
12/25/2021 22:26:38 - INFO - __main__ -   Using lang2id = None
12/25/2021 22:26:38 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
12/25/2021 22:26:38 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
12/25/2021 22:26:38 - INFO - root -   Trying to decide if add adapter
12/25/2021 22:26:38 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_model_head.bin
12/25/2021 22:26:38 - INFO - root -   loading lang adpater topk
12/25/2021 22:26:38 - INFO - __main__ -   Loading Adapter Languages from scripts/udpos/en/de.json
12/25/2021 22:26:38 - INFO - __main__ -   Adapter Languages : ['id'], Length : 1
12/25/2021 22:26:38 - INFO - __main__ -   Adapter Names ['id/wiki@ukp'], Length : 1
12/25/2021 22:26:38 - INFO - __main__ -   Language = id
12/25/2021 22:26:38 - INFO - __main__ -   Adapter Name = id/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/id/bert-base-multilingual-cased/pfeiffer/bert-base-multilingual-cased_wiki_id_pfeiffer.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/a35790907fed08fbe8567ddb42eaf99029e1b389458b3fa71f34d0dfcbde11ec-d5193b80938046db7118bf0fe97da3f8ae720f067ac22d0df16c9a965fa594d5-extracted/adapter_config.json
Adding adapter 'id' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/a35790907fed08fbe8567ddb42eaf99029e1b389458b3fa71f34d0dfcbde11ec-d5193b80938046db7118bf0fe97da3f8ae720f067ac22d0df16c9a965fa594d5-extracted/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/.cache/torch/adapters/a35790907fed08fbe8567ddb42eaf99029e1b389458b3fa71f34d0dfcbde11ec-d5193b80938046db7118bf0fe97da3f8ae720f067ac22d0df16c9a965fa594d5-extracted/head_config.json
12/25/2021 22:26:45 - INFO - __main__ -   Args Adapter Weight = equal
12/25/2021 22:26:45 - INFO - __main__ -   Adapter Languages = ['id']
12/25/2021 22:26:45 - INFO - __main__ -   Adapter Weights = [1.0]
12/25/2021 22:26:45 - INFO - __main__ -   Sum of Adapter Weights = 1.0
12/25/2021 22:26:45 - INFO - __main__ -   Length of Adapter Weights = 1
12/25/2021 22:26:45 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_de_bert-base-multilingual-cased_128
12/25/2021 22:26:49 - INFO - __main__ -   ***** Running evaluation  in de *****
12/25/2021 22:26:49 - INFO - __main__ -     Num examples = 22360
12/25/2021 22:26:49 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/699 [00:00<?, ?it/s]12/25/2021 22:26:49 - INFO - __main__ -   Batch number = 1
Evaluating:   0%|          | 1/699 [00:00<01:50,  6.33it/s]12/25/2021 22:26:49 - INFO - __main__ -   Batch number = 2
Evaluating:   0%|          | 2/699 [00:00<01:44,  6.68it/s]12/25/2021 22:26:49 - INFO - __main__ -   Batch number = 3
Evaluating:   0%|          | 3/699 [00:00<01:40,  6.89it/s]12/25/2021 22:26:49 - INFO - __main__ -   Batch number = 4
Evaluating:   1%|          | 4/699 [00:00<01:39,  7.02it/s]12/25/2021 22:26:49 - INFO - __main__ -   Batch number = 5
Evaluating:   1%|          | 5/699 [00:00<01:37,  7.09it/s]12/25/2021 22:26:49 - INFO - __main__ -   Batch number = 6
Evaluating:   1%|          | 6/699 [00:00<01:37,  7.10it/s]12/25/2021 22:26:49 - INFO - __main__ -   Batch number = 7
Evaluating:   1%|          | 7/699 [00:00<01:37,  7.12it/s]12/25/2021 22:26:50 - INFO - __main__ -   Batch number = 8
Evaluating:   1%|          | 8/699 [00:01<01:36,  7.13it/s]12/25/2021 22:26:50 - INFO - __main__ -   Batch number = 9
Evaluating:   1%|         | 9/699 [00:01<01:36,  7.13it/s]12/25/2021 22:26:50 - INFO - __main__ -   Batch number = 10
Evaluating:   1%|         | 10/699 [00:01<01:36,  7.13it/s]12/25/2021 22:26:50 - INFO - __main__ -   Batch number = 11
Evaluating:   2%|         | 11/699 [00:01<01:36,  7.13it/s]12/25/2021 22:26:50 - INFO - __main__ -   Batch number = 12
Evaluating:   2%|         | 12/699 [00:01<01:36,  7.13it/s]12/25/2021 22:26:50 - INFO - __main__ -   Batch number = 13
Evaluating:   2%|         | 13/699 [00:01<01:36,  7.11it/s]12/25/2021 22:26:50 - INFO - __main__ -   Batch number = 14
Evaluating:   2%|         | 14/699 [00:01<01:37,  7.06it/s]12/25/2021 22:26:51 - INFO - __main__ -   Batch number = 15
Evaluating:   2%|         | 15/699 [00:02<01:36,  7.07it/s]12/25/2021 22:26:51 - INFO - __main__ -   Batch number = 16
Evaluating:   2%|         | 16/699 [00:02<01:36,  7.06it/s]12/25/2021 22:26:51 - INFO - __main__ -   Batch number = 17
Evaluating:   2%|         | 17/699 [00:02<01:36,  7.05it/s]12/25/2021 22:26:51 - INFO - __main__ -   Batch number = 18
Evaluating:   3%|         | 18/699 [00:02<01:36,  7.07it/s]12/25/2021 22:26:51 - INFO - __main__ -   Batch number = 19
Evaluating:   3%|         | 19/699 [00:02<01:36,  7.02it/s]12/25/2021 22:26:51 - INFO - __main__ -   Batch number = 20
Evaluating:   3%|         | 20/699 [00:02<01:36,  7.05it/s]12/25/2021 22:26:51 - INFO - __main__ -   Batch number = 21
Evaluating:   3%|         | 21/699 [00:02<01:35,  7.08it/s]12/25/2021 22:26:52 - INFO - __main__ -   Batch number = 22
Evaluating:   3%|         | 22/699 [00:03<02:01,  5.57it/s]12/25/2021 22:26:52 - INFO - __main__ -   Batch number = 23
Evaluating:   3%|         | 23/699 [00:03<01:57,  5.76it/s]12/25/2021 22:26:52 - INFO - __main__ -   Batch number = 24
Evaluating:   3%|         | 24/699 [00:03<01:50,  6.09it/s]12/25/2021 22:26:52 - INFO - __main__ -   Batch number = 25
Evaluating:   4%|         | 25/699 [00:03<01:46,  6.35it/s]12/25/2021 22:26:52 - INFO - __main__ -   Batch number = 26
Evaluating:   4%|         | 26/699 [00:03<01:43,  6.53it/s]12/25/2021 22:26:52 - INFO - __main__ -   Batch number = 27
Evaluating:   4%|         | 27/699 [00:03<01:40,  6.69it/s]12/25/2021 22:26:53 - INFO - __main__ -   Batch number = 28
Evaluating:   4%|         | 28/699 [00:04<01:38,  6.81it/s]12/25/2021 22:26:53 - INFO - __main__ -   Batch number = 29
Evaluating:   4%|         | 29/699 [00:04<01:37,  6.86it/s]12/25/2021 22:26:53 - INFO - __main__ -   Batch number = 30
Evaluating:   4%|         | 30/699 [00:04<01:36,  6.93it/s]12/25/2021 22:26:53 - INFO - __main__ -   Batch number = 31
Evaluating:   4%|         | 31/699 [00:04<01:36,  6.95it/s]12/25/2021 22:26:53 - INFO - __main__ -   Batch number = 32
Evaluating:   5%|         | 32/699 [00:04<01:36,  6.94it/s]12/25/2021 22:26:53 - INFO - __main__ -   Batch number = 33
Evaluating:   5%|         | 33/699 [00:04<01:35,  6.97it/s]12/25/2021 22:26:53 - INFO - __main__ -   Batch number = 34
Evaluating:   5%|         | 34/699 [00:04<01:35,  6.99it/s]12/25/2021 22:26:54 - INFO - __main__ -   Batch number = 35
Evaluating:   5%|         | 35/699 [00:05<01:35,  6.99it/s]12/25/2021 22:26:54 - INFO - __main__ -   Batch number = 36
Evaluating:   5%|         | 36/699 [00:05<01:34,  7.00it/s]12/25/2021 22:26:54 - INFO - __main__ -   Batch number = 37
Evaluating:   5%|         | 37/699 [00:05<01:34,  7.00it/s]12/25/2021 22:26:54 - INFO - __main__ -   Batch number = 38
Evaluating:   5%|         | 38/699 [00:05<01:34,  6.98it/s]12/25/2021 22:26:54 - INFO - __main__ -   Batch number = 39
Evaluating:   6%|         | 39/699 [00:05<01:34,  6.99it/s]12/25/2021 22:26:54 - INFO - __main__ -   Batch number = 40
Evaluating:   6%|         | 40/699 [00:05<01:34,  6.99it/s]12/25/2021 22:26:54 - INFO - __main__ -   Batch number = 41
Evaluating:   6%|         | 41/699 [00:05<01:34,  6.96it/s]12/25/2021 22:26:55 - INFO - __main__ -   Batch number = 42
Evaluating:   6%|         | 42/699 [00:06<01:34,  6.95it/s]12/25/2021 22:26:55 - INFO - __main__ -   Batch number = 43
Evaluating:   6%|         | 43/699 [00:06<01:34,  6.91it/s]12/25/2021 22:26:55 - INFO - __main__ -   Batch number = 44
Evaluating:   6%|         | 44/699 [00:06<01:35,  6.85it/s]12/25/2021 22:26:55 - INFO - __main__ -   Batch number = 45
Evaluating:   6%|         | 45/699 [00:06<01:36,  6.78it/s]12/25/2021 22:26:55 - INFO - __main__ -   Batch number = 46
Evaluating:   7%|         | 46/699 [00:06<01:36,  6.79it/s]12/25/2021 22:26:55 - INFO - __main__ -   Batch number = 47
Evaluating:   7%|         | 47/699 [00:06<01:35,  6.83it/s]12/25/2021 22:26:55 - INFO - __main__ -   Batch number = 48
Evaluating:   7%|         | 48/699 [00:06<01:35,  6.84it/s]12/25/2021 22:26:56 - INFO - __main__ -   Batch number = 49
Evaluating:   7%|         | 49/699 [00:07<01:35,  6.82it/s]12/25/2021 22:26:56 - INFO - __main__ -   Batch number = 50
Evaluating:   7%|         | 50/699 [00:07<01:35,  6.81it/s]12/25/2021 22:26:56 - INFO - __main__ -   Batch number = 51
Evaluating:   7%|         | 51/699 [00:07<01:35,  6.80it/s]12/25/2021 22:26:56 - INFO - __main__ -   Batch number = 52
Evaluating:   7%|         | 52/699 [00:07<01:34,  6.84it/s]12/25/2021 22:26:56 - INFO - __main__ -   Batch number = 53
Evaluating:   8%|         | 53/699 [00:07<01:34,  6.85it/s]12/25/2021 22:26:56 - INFO - __main__ -   Batch number = 54
Evaluating:   8%|         | 54/699 [00:07<01:33,  6.88it/s]12/25/2021 22:26:56 - INFO - __main__ -   Batch number = 55
Evaluating:   8%|         | 55/699 [00:08<01:33,  6.89it/s]12/25/2021 22:26:57 - INFO - __main__ -   Batch number = 56
Evaluating:   8%|         | 56/699 [00:08<01:33,  6.86it/s]12/25/2021 22:26:57 - INFO - __main__ -   Batch number = 57
Evaluating:   8%|         | 57/699 [00:08<01:33,  6.87it/s]12/25/2021 22:26:57 - INFO - __main__ -   Batch number = 58
Evaluating:   8%|         | 58/699 [00:08<01:32,  6.89it/s]12/25/2021 22:26:57 - INFO - __main__ -   Batch number = 59
Evaluating:   8%|         | 59/699 [00:08<01:32,  6.90it/s]12/25/2021 22:26:57 - INFO - __main__ -   Batch number = 60
Evaluating:   9%|         | 60/699 [00:08<01:32,  6.90it/s]12/25/2021 22:26:57 - INFO - __main__ -   Batch number = 61
Evaluating:   9%|         | 61/699 [00:08<01:32,  6.90it/s]12/25/2021 22:26:57 - INFO - __main__ -   Batch number = 62
Evaluating:   9%|         | 62/699 [00:09<01:32,  6.89it/s]12/25/2021 22:26:58 - INFO - __main__ -   Batch number = 63
Evaluating:   9%|         | 63/699 [00:09<01:32,  6.89it/s]12/25/2021 22:26:58 - INFO - __main__ -   Batch number = 64
Evaluating:   9%|         | 64/699 [00:09<01:32,  6.90it/s]12/25/2021 22:26:58 - INFO - __main__ -   Batch number = 65
Evaluating:   9%|         | 65/699 [00:09<01:32,  6.88it/s]12/25/2021 22:26:58 - INFO - __main__ -   Batch number = 66
Evaluating:   9%|         | 66/699 [00:09<01:31,  6.88it/s]12/25/2021 22:26:58 - INFO - __main__ -   Batch number = 67
Evaluating:  10%|         | 67/699 [00:09<01:32,  6.86it/s]12/25/2021 22:26:58 - INFO - __main__ -   Batch number = 68
Evaluating:  10%|         | 68/699 [00:09<01:32,  6.83it/s]12/25/2021 22:26:59 - INFO - __main__ -   Batch number = 69
Evaluating:  10%|         | 69/699 [00:10<01:32,  6.83it/s]12/25/2021 22:26:59 - INFO - __main__ -   Batch number = 70
Evaluating:  10%|         | 70/699 [00:10<01:32,  6.83it/s]12/25/2021 22:26:59 - INFO - __main__ -   Batch number = 71
Evaluating:  10%|         | 71/699 [00:10<01:32,  6.81it/s]12/25/2021 22:26:59 - INFO - __main__ -   Batch number = 72
Evaluating:  10%|         | 72/699 [00:10<01:32,  6.76it/s]12/25/2021 22:26:59 - INFO - __main__ -   Batch number = 73
Evaluating:  10%|         | 73/699 [00:10<01:32,  6.74it/s]12/25/2021 22:26:59 - INFO - __main__ -   Batch number = 74
Evaluating:  11%|         | 74/699 [00:10<01:32,  6.74it/s]12/25/2021 22:26:59 - INFO - __main__ -   Batch number = 75
Evaluating:  11%|         | 75/699 [00:10<01:32,  6.75it/s]12/25/2021 22:27:00 - INFO - __main__ -   Batch number = 76
Evaluating:  11%|         | 76/699 [00:11<01:32,  6.71it/s]12/25/2021 22:27:00 - INFO - __main__ -   Batch number = 77
Evaluating:  11%|         | 77/699 [00:11<01:33,  6.64it/s]12/25/2021 22:27:00 - INFO - __main__ -   Batch number = 78
Evaluating:  11%|         | 78/699 [00:11<01:33,  6.63it/s]12/25/2021 22:27:00 - INFO - __main__ -   Batch number = 79
Evaluating:  11%|        | 79/699 [00:11<01:33,  6.64it/s]12/25/2021 22:27:00 - INFO - __main__ -   Batch number = 80
Evaluating:  11%|        | 80/699 [00:11<01:33,  6.63it/s]12/25/2021 22:27:00 - INFO - __main__ -   Batch number = 81
Evaluating:  12%|        | 81/699 [00:11<01:33,  6.63it/s]12/25/2021 22:27:00 - INFO - __main__ -   Batch number = 82
Evaluating:  12%|        | 82/699 [00:12<01:32,  6.64it/s]12/25/2021 22:27:01 - INFO - __main__ -   Batch number = 83
Evaluating:  12%|        | 83/699 [00:12<01:32,  6.63it/s]12/25/2021 22:27:01 - INFO - __main__ -   Batch number = 84
Evaluating:  12%|        | 84/699 [00:12<01:32,  6.65it/s]12/25/2021 22:27:01 - INFO - __main__ -   Batch number = 85
Evaluating:  12%|        | 85/699 [00:12<01:31,  6.69it/s]12/25/2021 22:27:01 - INFO - __main__ -   Batch number = 86
Evaluating:  12%|        | 86/699 [00:12<01:36,  6.38it/s]12/25/2021 22:27:01 - INFO - __main__ -   Batch number = 87
Evaluating:  12%|        | 87/699 [00:12<01:37,  6.25it/s]12/25/2021 22:27:01 - INFO - __main__ -   Batch number = 88
Evaluating:  13%|        | 88/699 [00:12<01:38,  6.20it/s]12/25/2021 22:27:02 - INFO - __main__ -   Batch number = 89
Evaluating:  13%|        | 89/699 [00:13<01:38,  6.19it/s]12/25/2021 22:27:02 - INFO - __main__ -   Batch number = 90
Evaluating:  13%|        | 90/699 [00:13<03:21,  3.02it/s]12/25/2021 22:27:02 - INFO - __main__ -   Batch number = 91
Evaluating:  13%|        | 91/699 [00:14<02:52,  3.52it/s]12/25/2021 22:27:03 - INFO - __main__ -   Batch number = 92
Evaluating:  13%|        | 92/699 [00:14<02:31,  4.01it/s]12/25/2021 22:27:03 - INFO - __main__ -   Batch number = 93
Evaluating:  13%|        | 93/699 [00:14<02:16,  4.46it/s]12/25/2021 22:27:03 - INFO - __main__ -   Batch number = 94
Evaluating:  13%|        | 94/699 [00:14<02:04,  4.85it/s]12/25/2021 22:27:03 - INFO - __main__ -   Batch number = 95
Evaluating:  14%|        | 95/699 [00:14<01:57,  5.13it/s]12/25/2021 22:27:03 - INFO - __main__ -   Batch number = 96
Evaluating:  14%|        | 96/699 [00:15<02:29,  4.04it/s]12/25/2021 22:27:04 - INFO - __main__ -   Batch number = 97
Evaluating:  14%|        | 97/699 [00:15<02:15,  4.44it/s]12/25/2021 22:27:04 - INFO - __main__ -   Batch number = 98
Evaluating:  14%|        | 98/699 [00:15<02:05,  4.79it/s]12/25/2021 22:27:04 - INFO - __main__ -   Batch number = 99
Evaluating:  14%|        | 99/699 [00:15<02:13,  4.49it/s]12/25/2021 22:27:04 - INFO - __main__ -   Batch number = 100
Evaluating:  14%|        | 100/699 [00:15<02:02,  4.88it/s]12/25/2021 22:27:04 - INFO - __main__ -   Batch number = 101
Evaluating:  14%|        | 101/699 [00:15<01:55,  5.19it/s]12/25/2021 22:27:05 - INFO - __main__ -   Batch number = 102
Evaluating:  15%|        | 102/699 [00:16<02:06,  4.73it/s]12/25/2021 22:27:05 - INFO - __main__ -   Batch number = 103
Evaluating:  15%|        | 103/699 [00:16<01:57,  5.07it/s]12/25/2021 22:27:05 - INFO - __main__ -   Batch number = 104
Evaluating:  15%|        | 104/699 [00:16<01:52,  5.31it/s]12/25/2021 22:27:05 - INFO - __main__ -   Batch number = 105
Evaluating:  15%|        | 105/699 [00:16<02:05,  4.74it/s]12/25/2021 22:27:05 - INFO - __main__ -   Batch number = 106
Evaluating:  15%|        | 106/699 [00:17<01:58,  5.01it/s]12/25/2021 22:27:06 - INFO - __main__ -   Batch number = 107
Evaluating:  15%|        | 107/699 [00:17<02:10,  4.55it/s]12/25/2021 22:27:06 - INFO - __main__ -   Batch number = 108
Evaluating:  15%|        | 108/699 [00:17<02:02,  4.84it/s]12/25/2021 22:27:06 - INFO - __main__ -   Batch number = 109
Evaluating:  16%|        | 109/699 [00:17<01:57,  5.02it/s]12/25/2021 22:27:06 - INFO - __main__ -   Batch number = 110
Evaluating:  16%|        | 110/699 [00:17<02:09,  4.56it/s]12/25/2021 22:27:07 - INFO - __main__ -   Batch number = 111
Evaluating:  16%|        | 111/699 [00:18<02:00,  4.88it/s]12/25/2021 22:27:07 - INFO - __main__ -   Batch number = 112
Evaluating:  16%|        | 112/699 [00:18<02:09,  4.53it/s]12/25/2021 22:27:07 - INFO - __main__ -   Batch number = 113
Evaluating:  16%|        | 113/699 [00:18<02:00,  4.87it/s]12/25/2021 22:27:07 - INFO - __main__ -   Batch number = 114
Evaluating:  16%|        | 114/699 [00:18<01:53,  5.16it/s]12/25/2021 22:27:07 - INFO - __main__ -   Batch number = 115
Evaluating:  16%|        | 115/699 [00:18<02:05,  4.64it/s]12/25/2021 22:27:08 - INFO - __main__ -   Batch number = 116
Evaluating:  17%|        | 116/699 [00:19<01:57,  4.97it/s]12/25/2021 22:27:08 - INFO - __main__ -   Batch number = 117
Evaluating:  17%|        | 117/699 [00:19<02:06,  4.59it/s]12/25/2021 22:27:08 - INFO - __main__ -   Batch number = 118
Evaluating:  17%|        | 118/699 [00:19<01:57,  4.93it/s]12/25/2021 22:27:08 - INFO - __main__ -   Batch number = 119
Evaluating:  17%|        | 119/699 [00:19<02:09,  4.49it/s]12/25/2021 22:27:08 - INFO - __main__ -   Batch number = 120
Evaluating:  17%|        | 120/699 [00:19<01:59,  4.83it/s]12/25/2021 22:27:09 - INFO - __main__ -   Batch number = 121
Evaluating:  17%|        | 121/699 [00:20<02:09,  4.47it/s]12/25/2021 22:27:09 - INFO - __main__ -   Batch number = 122
Evaluating:  17%|        | 122/699 [00:20<01:59,  4.81it/s]12/25/2021 22:27:09 - INFO - __main__ -   Batch number = 123
Evaluating:  18%|        | 123/699 [00:20<01:53,  5.08it/s]12/25/2021 22:27:09 - INFO - __main__ -   Batch number = 124
Evaluating:  18%|        | 124/699 [00:20<02:05,  4.57it/s]12/25/2021 22:27:09 - INFO - __main__ -   Batch number = 125
Evaluating:  18%|        | 125/699 [00:21<01:57,  4.87it/s]12/25/2021 22:27:10 - INFO - __main__ -   Batch number = 126
Evaluating:  18%|        | 126/699 [00:21<02:09,  4.44it/s]12/25/2021 22:27:10 - INFO - __main__ -   Batch number = 127
Evaluating:  18%|        | 127/699 [00:21<02:01,  4.72it/s]12/25/2021 22:27:10 - INFO - __main__ -   Batch number = 128
Evaluating:  18%|        | 128/699 [00:21<02:11,  4.34it/s]12/25/2021 22:27:10 - INFO - __main__ -   Batch number = 129
Evaluating:  18%|        | 129/699 [00:21<02:04,  4.59it/s]12/25/2021 22:27:11 - INFO - __main__ -   Batch number = 130
Evaluating:  19%|        | 130/699 [00:22<02:13,  4.26it/s]12/25/2021 22:27:11 - INFO - __main__ -   Batch number = 131
Evaluating:  19%|        | 131/699 [00:22<02:03,  4.58it/s]12/25/2021 22:27:11 - INFO - __main__ -   Batch number = 132
Evaluating:  19%|        | 132/699 [00:22<02:10,  4.35it/s]12/25/2021 22:27:11 - INFO - __main__ -   Batch number = 133
Evaluating:  19%|        | 133/699 [00:22<02:02,  4.63it/s]12/25/2021 22:27:11 - INFO - __main__ -   Batch number = 134
Evaluating:  19%|        | 134/699 [00:23<02:09,  4.37it/s]12/25/2021 22:27:12 - INFO - __main__ -   Batch number = 135
Evaluating:  19%|        | 135/699 [00:23<01:59,  4.70it/s]12/25/2021 22:27:12 - INFO - __main__ -   Batch number = 136
Evaluating:  19%|        | 136/699 [00:23<02:07,  4.42it/s]12/25/2021 22:27:12 - INFO - __main__ -   Batch number = 137
Evaluating:  20%|        | 137/699 [00:23<01:58,  4.75it/s]12/25/2021 22:27:12 - INFO - __main__ -   Batch number = 138
Evaluating:  20%|        | 138/699 [00:23<02:08,  4.36it/s]12/25/2021 22:27:13 - INFO - __main__ -   Batch number = 139
Evaluating:  20%|        | 139/699 [00:24<02:01,  4.59it/s]12/25/2021 22:27:13 - INFO - __main__ -   Batch number = 140
Evaluating:  20%|        | 140/699 [00:24<02:10,  4.27it/s]12/25/2021 22:27:13 - INFO - __main__ -   Batch number = 141
Evaluating:  20%|        | 141/699 [00:24<02:02,  4.55it/s]12/25/2021 22:27:13 - INFO - __main__ -   Batch number = 142
Evaluating:  20%|        | 142/699 [00:24<02:10,  4.26it/s]12/25/2021 22:27:14 - INFO - __main__ -   Batch number = 143
Evaluating:  20%|        | 143/699 [00:25<02:22,  3.89it/s]12/25/2021 22:27:14 - INFO - __main__ -   Batch number = 144
Evaluating:  21%|        | 144/699 [00:25<02:09,  4.30it/s]12/25/2021 22:27:14 - INFO - __main__ -   Batch number = 145
Evaluating:  21%|        | 145/699 [00:25<02:18,  4.01it/s]12/25/2021 22:27:14 - INFO - __main__ -   Batch number = 146
Evaluating:  21%|        | 146/699 [00:25<02:07,  4.33it/s]12/25/2021 22:27:14 - INFO - __main__ -   Batch number = 147
Evaluating:  21%|        | 147/699 [00:26<02:16,  4.04it/s]12/25/2021 22:27:15 - INFO - __main__ -   Batch number = 148
Evaluating:  21%|        | 148/699 [00:26<02:05,  4.39it/s]12/25/2021 22:27:15 - INFO - __main__ -   Batch number = 149
Evaluating:  21%|       | 149/699 [00:26<02:13,  4.13it/s]12/25/2021 22:27:15 - INFO - __main__ -   Batch number = 150
Evaluating:  21%|       | 150/699 [00:26<02:02,  4.49it/s]12/25/2021 22:27:15 - INFO - __main__ -   Batch number = 151
Evaluating:  22%|       | 151/699 [00:27<02:09,  4.23it/s]12/25/2021 22:27:16 - INFO - __main__ -   Batch number = 152
Evaluating:  22%|       | 152/699 [00:27<02:16,  4.02it/s]12/25/2021 22:27:16 - INFO - __main__ -   Batch number = 153
Evaluating:  22%|       | 153/699 [00:27<02:06,  4.32it/s]12/25/2021 22:27:16 - INFO - __main__ -   Batch number = 154
Evaluating:  22%|       | 154/699 [00:27<02:13,  4.07it/s]12/25/2021 22:27:16 - INFO - __main__ -   Batch number = 155
Evaluating:  22%|       | 155/699 [00:27<02:02,  4.45it/s]12/25/2021 22:27:17 - INFO - __main__ -   Batch number = 156
Evaluating:  22%|       | 156/699 [00:28<02:11,  4.13it/s]12/25/2021 22:27:17 - INFO - __main__ -   Batch number = 157
Evaluating:  22%|       | 157/699 [00:28<02:16,  3.96it/s]12/25/2021 22:27:17 - INFO - __main__ -   Batch number = 158
Evaluating:  23%|       | 158/699 [00:28<02:06,  4.27it/s]12/25/2021 22:27:17 - INFO - __main__ -   Batch number = 159
Evaluating:  23%|       | 159/699 [00:28<02:14,  4.02it/s]12/25/2021 22:27:18 - INFO - __main__ -   Batch number = 160
Evaluating:  23%|       | 160/699 [00:29<02:02,  4.38it/s]12/25/2021 22:27:18 - INFO - __main__ -   Batch number = 161
Evaluating:  23%|       | 161/699 [00:29<02:10,  4.11it/s]12/25/2021 22:27:18 - INFO - __main__ -   Batch number = 162
Evaluating:  23%|       | 162/699 [00:29<02:16,  3.94it/s]12/25/2021 22:27:18 - INFO - __main__ -   Batch number = 163
Evaluating:  23%|       | 163/699 [00:29<02:05,  4.26it/s]12/25/2021 22:27:19 - INFO - __main__ -   Batch number = 164
Evaluating:  23%|       | 164/699 [00:30<02:18,  3.86it/s]12/25/2021 22:27:19 - INFO - __main__ -   Batch number = 165
Evaluating:  24%|       | 165/699 [00:30<02:22,  3.74it/s]12/25/2021 22:27:19 - INFO - __main__ -   Batch number = 166
Evaluating:  24%|       | 166/699 [00:30<02:08,  4.15it/s]12/25/2021 22:27:19 - INFO - __main__ -   Batch number = 167
Evaluating:  24%|       | 167/699 [00:30<02:17,  3.87it/s]12/25/2021 22:27:20 - INFO - __main__ -   Batch number = 168
Evaluating:  24%|       | 168/699 [00:31<02:23,  3.71it/s]12/25/2021 22:27:20 - INFO - __main__ -   Batch number = 169
Evaluating:  24%|       | 169/699 [00:31<02:10,  4.07it/s]12/25/2021 22:27:20 - INFO - __main__ -   Batch number = 170
Evaluating:  24%|       | 170/699 [00:31<02:17,  3.84it/s]12/25/2021 22:27:20 - INFO - __main__ -   Batch number = 171
Evaluating:  24%|       | 171/699 [00:32<02:22,  3.70it/s]12/25/2021 22:27:21 - INFO - __main__ -   Batch number = 172
Evaluating:  25%|       | 172/699 [00:32<02:10,  4.04it/s]12/25/2021 22:27:21 - INFO - __main__ -   Batch number = 173
Evaluating:  25%|       | 173/699 [00:32<02:16,  3.85it/s]12/25/2021 22:27:21 - INFO - __main__ -   Batch number = 174
Evaluating:  25%|       | 174/699 [00:32<02:19,  3.76it/s]12/25/2021 22:27:21 - INFO - __main__ -   Batch number = 175
Evaluating:  25%|       | 175/699 [00:33<02:08,  4.07it/s]12/25/2021 22:27:22 - INFO - __main__ -   Batch number = 176
Evaluating:  25%|       | 176/699 [00:33<02:15,  3.85it/s]12/25/2021 22:27:22 - INFO - __main__ -   Batch number = 177
Evaluating:  25%|       | 177/699 [00:33<02:20,  3.72it/s]12/25/2021 22:27:22 - INFO - __main__ -   Batch number = 178
Evaluating:  25%|       | 178/699 [00:33<02:06,  4.10it/s]12/25/2021 22:27:22 - INFO - __main__ -   Batch number = 179
Evaluating:  26%|       | 179/699 [00:34<02:14,  3.87it/s]12/25/2021 22:27:23 - INFO - __main__ -   Batch number = 180
Evaluating:  26%|       | 180/699 [00:34<02:20,  3.70it/s]12/25/2021 22:27:23 - INFO - __main__ -   Batch number = 181
Evaluating:  26%|       | 181/699 [00:34<02:23,  3.62it/s]12/25/2021 22:27:23 - INFO - __main__ -   Batch number = 182
Evaluating:  26%|       | 182/699 [00:34<02:26,  3.52it/s]12/25/2021 22:27:24 - INFO - __main__ -   Batch number = 183
Evaluating:  26%|       | 183/699 [00:35<02:25,  3.56it/s]12/25/2021 22:27:24 - INFO - __main__ -   Batch number = 184
Evaluating:  26%|       | 184/699 [00:35<02:27,  3.49it/s]12/25/2021 22:27:24 - INFO - __main__ -   Batch number = 185
Evaluating:  26%|       | 185/699 [00:35<02:27,  3.50it/s]12/25/2021 22:27:24 - INFO - __main__ -   Batch number = 186
Evaluating:  27%|       | 186/699 [00:36<02:27,  3.47it/s]12/25/2021 22:27:25 - INFO - __main__ -   Batch number = 187
Evaluating:  27%|       | 187/699 [00:36<02:14,  3.80it/s]12/25/2021 22:27:25 - INFO - __main__ -   Batch number = 188
Evaluating:  27%|       | 188/699 [00:36<02:18,  3.70it/s]12/25/2021 22:27:25 - INFO - __main__ -   Batch number = 189
Evaluating:  27%|       | 189/699 [00:36<02:21,  3.60it/s]12/25/2021 22:27:26 - INFO - __main__ -   Batch number = 190
Evaluating:  27%|       | 190/699 [00:37<02:25,  3.50it/s]12/25/2021 22:27:26 - INFO - __main__ -   Batch number = 191
Evaluating:  27%|       | 191/699 [00:37<02:27,  3.44it/s]12/25/2021 22:27:26 - INFO - __main__ -   Batch number = 192
Evaluating:  27%|       | 192/699 [00:37<02:12,  3.81it/s]12/25/2021 22:27:26 - INFO - __main__ -   Batch number = 193
Evaluating:  28%|       | 193/699 [00:38<02:17,  3.68it/s]12/25/2021 22:27:27 - INFO - __main__ -   Batch number = 194
Evaluating:  28%|       | 194/699 [00:38<02:20,  3.60it/s]12/25/2021 22:27:27 - INFO - __main__ -   Batch number = 195
Evaluating:  28%|       | 195/699 [00:38<02:21,  3.57it/s]12/25/2021 22:27:27 - INFO - __main__ -   Batch number = 196
Evaluating:  28%|       | 196/699 [00:38<02:24,  3.47it/s]12/25/2021 22:27:27 - INFO - __main__ -   Batch number = 197
Evaluating:  28%|       | 197/699 [00:39<02:11,  3.81it/s]12/25/2021 22:27:28 - INFO - __main__ -   Batch number = 198
Evaluating:  28%|       | 198/699 [00:39<02:18,  3.62it/s]12/25/2021 22:27:28 - INFO - __main__ -   Batch number = 199
Evaluating:  28%|       | 199/699 [00:39<02:24,  3.47it/s]12/25/2021 22:27:28 - INFO - __main__ -   Batch number = 200
Evaluating:  29%|       | 200/699 [00:40<02:26,  3.41it/s]12/25/2021 22:27:29 - INFO - __main__ -   Batch number = 201
Evaluating:  29%|       | 201/699 [00:40<02:28,  3.35it/s]12/25/2021 22:27:29 - INFO - __main__ -   Batch number = 202
Evaluating:  29%|       | 202/699 [00:40<02:27,  3.37it/s]12/25/2021 22:27:29 - INFO - __main__ -   Batch number = 203
Evaluating:  29%|       | 203/699 [00:40<02:13,  3.72it/s]12/25/2021 22:27:29 - INFO - __main__ -   Batch number = 204
Evaluating:  29%|       | 204/699 [00:41<02:18,  3.58it/s]12/25/2021 22:27:30 - INFO - __main__ -   Batch number = 205
Evaluating:  29%|       | 205/699 [00:41<02:22,  3.46it/s]12/25/2021 22:27:30 - INFO - __main__ -   Batch number = 206
Evaluating:  29%|       | 206/699 [00:41<02:25,  3.38it/s]12/25/2021 22:27:30 - INFO - __main__ -   Batch number = 207
Evaluating:  30%|       | 207/699 [00:42<02:27,  3.34it/s]12/25/2021 22:27:31 - INFO - __main__ -   Batch number = 208
Evaluating:  30%|       | 208/699 [00:42<02:29,  3.29it/s]12/25/2021 22:27:31 - INFO - __main__ -   Batch number = 209
Evaluating:  30%|       | 209/699 [00:42<02:27,  3.33it/s]12/25/2021 22:27:31 - INFO - __main__ -   Batch number = 210
Evaluating:  30%|       | 210/699 [00:42<02:26,  3.34it/s]12/25/2021 22:27:32 - INFO - __main__ -   Batch number = 211
Evaluating:  30%|       | 211/699 [00:43<02:12,  3.67it/s]12/25/2021 22:27:32 - INFO - __main__ -   Batch number = 212
Evaluating:  30%|       | 212/699 [00:43<02:17,  3.53it/s]12/25/2021 22:27:32 - INFO - __main__ -   Batch number = 213
Evaluating:  30%|       | 213/699 [00:43<02:23,  3.39it/s]12/25/2021 22:27:32 - INFO - __main__ -   Batch number = 214
Evaluating:  31%|       | 214/699 [00:44<02:27,  3.29it/s]12/25/2021 22:27:33 - INFO - __main__ -   Batch number = 215
Evaluating:  31%|       | 215/699 [00:44<02:27,  3.27it/s]12/25/2021 22:27:33 - INFO - __main__ -   Batch number = 216
Evaluating:  31%|       | 216/699 [00:44<02:26,  3.29it/s]12/25/2021 22:27:33 - INFO - __main__ -   Batch number = 217
Evaluating:  31%|       | 217/699 [00:45<02:28,  3.25it/s]12/25/2021 22:27:34 - INFO - __main__ -   Batch number = 218
Evaluating:  31%|       | 218/699 [00:45<02:26,  3.29it/s]12/25/2021 22:27:34 - INFO - __main__ -   Batch number = 219
Evaluating:  31%|      | 219/699 [00:45<02:25,  3.31it/s]12/25/2021 22:27:34 - INFO - __main__ -   Batch number = 220
Evaluating:  31%|      | 220/699 [00:45<02:24,  3.32it/s]12/25/2021 22:27:35 - INFO - __main__ -   Batch number = 221
Evaluating:  32%|      | 221/699 [00:46<02:23,  3.32it/s]12/25/2021 22:27:35 - INFO - __main__ -   Batch number = 222
Evaluating:  32%|      | 222/699 [00:46<02:09,  3.69it/s]12/25/2021 22:27:35 - INFO - __main__ -   Batch number = 223
Evaluating:  32%|      | 223/699 [00:46<02:15,  3.52it/s]12/25/2021 22:27:35 - INFO - __main__ -   Batch number = 224
Evaluating:  32%|      | 224/699 [00:47<02:21,  3.37it/s]12/25/2021 22:27:36 - INFO - __main__ -   Batch number = 225
Evaluating:  32%|      | 225/699 [00:47<02:23,  3.30it/s]12/25/2021 22:27:36 - INFO - __main__ -   Batch number = 226
Evaluating:  32%|      | 226/699 [00:47<02:27,  3.21it/s]12/25/2021 22:27:36 - INFO - __main__ -   Batch number = 227
Evaluating:  32%|      | 227/699 [00:48<02:27,  3.19it/s]12/25/2021 22:27:37 - INFO - __main__ -   Batch number = 228
Evaluating:  33%|      | 228/699 [00:48<02:28,  3.18it/s]12/25/2021 22:27:37 - INFO - __main__ -   Batch number = 229
Evaluating:  33%|      | 229/699 [00:48<02:27,  3.18it/s]12/25/2021 22:27:37 - INFO - __main__ -   Batch number = 230
Evaluating:  33%|      | 230/699 [00:49<02:27,  3.19it/s]12/25/2021 22:27:38 - INFO - __main__ -   Batch number = 231
Evaluating:  33%|      | 231/699 [00:49<02:26,  3.20it/s]12/25/2021 22:27:38 - INFO - __main__ -   Batch number = 232
Evaluating:  33%|      | 232/699 [00:49<02:27,  3.17it/s]12/25/2021 22:27:38 - INFO - __main__ -   Batch number = 233
Evaluating:  33%|      | 233/699 [00:49<02:26,  3.18it/s]12/25/2021 22:27:39 - INFO - __main__ -   Batch number = 234
Evaluating:  33%|      | 234/699 [00:50<02:29,  3.10it/s]12/25/2021 22:27:39 - INFO - __main__ -   Batch number = 235
Evaluating:  34%|      | 235/699 [00:50<02:28,  3.13it/s]12/25/2021 22:27:39 - INFO - __main__ -   Batch number = 236
Evaluating:  34%|      | 236/699 [00:50<02:25,  3.18it/s]12/25/2021 22:27:40 - INFO - __main__ -   Batch number = 237
Evaluating:  34%|      | 237/699 [00:51<02:24,  3.19it/s]12/25/2021 22:27:40 - INFO - __main__ -   Batch number = 238
Evaluating:  34%|      | 238/699 [00:51<02:23,  3.21it/s]12/25/2021 22:27:40 - INFO - __main__ -   Batch number = 239
Evaluating:  34%|      | 239/699 [00:51<02:21,  3.25it/s]12/25/2021 22:27:40 - INFO - __main__ -   Batch number = 240
Evaluating:  34%|      | 240/699 [00:52<02:20,  3.27it/s]12/25/2021 22:27:41 - INFO - __main__ -   Batch number = 241
Evaluating:  34%|      | 241/699 [00:52<02:22,  3.22it/s]12/25/2021 22:27:41 - INFO - __main__ -   Batch number = 242
Evaluating:  35%|      | 242/699 [00:52<02:20,  3.24it/s]12/25/2021 22:27:41 - INFO - __main__ -   Batch number = 243
Evaluating:  35%|      | 243/699 [00:53<02:21,  3.22it/s]12/25/2021 22:27:42 - INFO - __main__ -   Batch number = 244
Evaluating:  35%|      | 244/699 [00:53<02:21,  3.21it/s]12/25/2021 22:27:42 - INFO - __main__ -   Batch number = 245
Evaluating:  35%|      | 245/699 [00:53<02:20,  3.23it/s]12/25/2021 22:27:42 - INFO - __main__ -   Batch number = 246
Evaluating:  35%|      | 246/699 [00:54<02:20,  3.23it/s]12/25/2021 22:27:43 - INFO - __main__ -   Batch number = 247
Evaluating:  35%|      | 247/699 [00:54<02:22,  3.18it/s]12/25/2021 22:27:43 - INFO - __main__ -   Batch number = 248
Evaluating:  35%|      | 248/699 [00:54<02:21,  3.18it/s]12/25/2021 22:27:43 - INFO - __main__ -   Batch number = 249
Evaluating:  36%|      | 249/699 [00:54<02:23,  3.14it/s]12/25/2021 22:27:44 - INFO - __main__ -   Batch number = 250
Evaluating:  36%|      | 250/699 [00:55<02:25,  3.08it/s]12/25/2021 22:27:44 - INFO - __main__ -   Batch number = 251
Evaluating:  36%|      | 251/699 [00:55<02:26,  3.06it/s]12/25/2021 22:27:44 - INFO - __main__ -   Batch number = 252
Evaluating:  36%|      | 252/699 [00:56<02:42,  2.75it/s]12/25/2021 22:27:45 - INFO - __main__ -   Batch number = 253
Evaluating:  36%|      | 253/699 [00:56<02:35,  2.86it/s]12/25/2021 22:27:45 - INFO - __main__ -   Batch number = 254
Evaluating:  36%|      | 254/699 [00:56<02:32,  2.92it/s]12/25/2021 22:27:45 - INFO - __main__ -   Batch number = 255
Evaluating:  36%|      | 255/699 [00:57<02:27,  3.02it/s]12/25/2021 22:27:46 - INFO - __main__ -   Batch number = 256
Evaluating:  37%|      | 256/699 [00:57<02:26,  3.02it/s]12/25/2021 22:27:46 - INFO - __main__ -   Batch number = 257
Evaluating:  37%|      | 257/699 [00:57<02:24,  3.06it/s]12/25/2021 22:27:46 - INFO - __main__ -   Batch number = 258
Evaluating:  37%|      | 258/699 [00:58<02:22,  3.09it/s]12/25/2021 22:27:47 - INFO - __main__ -   Batch number = 259
Evaluating:  37%|      | 259/699 [00:58<02:21,  3.11it/s]12/25/2021 22:27:47 - INFO - __main__ -   Batch number = 260
Evaluating:  37%|      | 260/699 [00:58<02:19,  3.14it/s]12/25/2021 22:27:47 - INFO - __main__ -   Batch number = 261
Evaluating:  37%|      | 261/699 [00:58<02:19,  3.13it/s]12/25/2021 22:27:48 - INFO - __main__ -   Batch number = 262
Evaluating:  37%|      | 262/699 [00:59<02:20,  3.12it/s]12/25/2021 22:27:48 - INFO - __main__ -   Batch number = 263
Evaluating:  38%|      | 263/699 [00:59<02:36,  2.78it/s]12/25/2021 22:27:48 - INFO - __main__ -   Batch number = 264
Evaluating:  38%|      | 264/699 [01:00<02:31,  2.88it/s]12/25/2021 22:27:49 - INFO - __main__ -   Batch number = 265
Evaluating:  38%|      | 265/699 [01:00<02:29,  2.91it/s]12/25/2021 22:27:49 - INFO - __main__ -   Batch number = 266
Evaluating:  38%|      | 266/699 [01:00<02:26,  2.95it/s]12/25/2021 22:27:49 - INFO - __main__ -   Batch number = 267
Evaluating:  38%|      | 267/699 [01:01<02:22,  3.03it/s]12/25/2021 22:27:50 - INFO - __main__ -   Batch number = 268
Evaluating:  38%|      | 268/699 [01:01<02:21,  3.05it/s]12/25/2021 22:27:50 - INFO - __main__ -   Batch number = 269
Evaluating:  38%|      | 269/699 [01:01<02:22,  3.03it/s]12/25/2021 22:27:50 - INFO - __main__ -   Batch number = 270
Evaluating:  39%|      | 270/699 [01:02<02:22,  3.02it/s]12/25/2021 22:27:51 - INFO - __main__ -   Batch number = 271
Evaluating:  39%|      | 271/699 [01:02<02:37,  2.73it/s]12/25/2021 22:27:51 - INFO - __main__ -   Batch number = 272
Evaluating:  39%|      | 272/699 [01:02<02:31,  2.83it/s]12/25/2021 22:27:51 - INFO - __main__ -   Batch number = 273
Evaluating:  39%|      | 273/699 [01:03<02:27,  2.89it/s]12/25/2021 22:27:52 - INFO - __main__ -   Batch number = 274
Evaluating:  39%|      | 274/699 [01:03<02:25,  2.91it/s]12/25/2021 22:27:52 - INFO - __main__ -   Batch number = 275
Evaluating:  39%|      | 275/699 [01:03<02:23,  2.95it/s]12/25/2021 22:27:52 - INFO - __main__ -   Batch number = 276
Evaluating:  39%|      | 276/699 [01:04<02:23,  2.96it/s]12/25/2021 22:27:53 - INFO - __main__ -   Batch number = 277
Evaluating:  40%|      | 277/699 [01:04<02:37,  2.69it/s]12/25/2021 22:27:53 - INFO - __main__ -   Batch number = 278
Evaluating:  40%|      | 278/699 [01:04<02:28,  2.83it/s]12/25/2021 22:27:53 - INFO - __main__ -   Batch number = 279
Evaluating:  40%|      | 279/699 [01:05<02:23,  2.92it/s]12/25/2021 22:27:54 - INFO - __main__ -   Batch number = 280
Evaluating:  40%|      | 280/699 [01:05<02:23,  2.92it/s]12/25/2021 22:27:54 - INFO - __main__ -   Batch number = 281
Evaluating:  40%|      | 281/699 [01:05<02:21,  2.96it/s]12/25/2021 22:27:54 - INFO - __main__ -   Batch number = 282
Evaluating:  40%|      | 282/699 [01:06<02:35,  2.68it/s]12/25/2021 22:27:55 - INFO - __main__ -   Batch number = 283
Evaluating:  40%|      | 283/699 [01:06<02:29,  2.79it/s]12/25/2021 22:27:55 - INFO - __main__ -   Batch number = 284
Evaluating:  41%|      | 284/699 [01:06<02:25,  2.86it/s]12/25/2021 22:27:56 - INFO - __main__ -   Batch number = 285
Evaluating:  41%|      | 285/699 [01:07<02:23,  2.89it/s]12/25/2021 22:27:56 - INFO - __main__ -   Batch number = 286
Evaluating:  41%|      | 286/699 [01:07<02:21,  2.92it/s]12/25/2021 22:27:56 - INFO - __main__ -   Batch number = 287
Evaluating:  41%|      | 287/699 [01:08<02:35,  2.65it/s]12/25/2021 22:27:57 - INFO - __main__ -   Batch number = 288
Evaluating:  41%|      | 288/699 [01:08<02:28,  2.76it/s]12/25/2021 22:27:57 - INFO - __main__ -   Batch number = 289
Evaluating:  41%|     | 289/699 [01:08<02:24,  2.85it/s]12/25/2021 22:27:57 - INFO - __main__ -   Batch number = 290
Evaluating:  41%|     | 290/699 [01:09<02:20,  2.92it/s]12/25/2021 22:27:58 - INFO - __main__ -   Batch number = 291
Evaluating:  42%|     | 291/699 [01:09<02:33,  2.66it/s]12/25/2021 22:27:58 - INFO - __main__ -   Batch number = 292
Evaluating:  42%|     | 292/699 [01:09<02:27,  2.76it/s]12/25/2021 22:27:58 - INFO - __main__ -   Batch number = 293
Evaluating:  42%|     | 293/699 [01:10<02:22,  2.85it/s]12/25/2021 22:27:59 - INFO - __main__ -   Batch number = 294
Evaluating:  42%|     | 294/699 [01:10<02:19,  2.90it/s]12/25/2021 22:27:59 - INFO - __main__ -   Batch number = 295
Evaluating:  42%|     | 295/699 [01:10<02:31,  2.66it/s]12/25/2021 22:28:00 - INFO - __main__ -   Batch number = 296
Evaluating:  42%|     | 296/699 [01:11<02:24,  2.79it/s]12/25/2021 22:28:00 - INFO - __main__ -   Batch number = 297
Evaluating:  42%|     | 297/699 [01:11<02:19,  2.88it/s]12/25/2021 22:28:00 - INFO - __main__ -   Batch number = 298
Evaluating:  43%|     | 298/699 [01:11<02:18,  2.90it/s]12/25/2021 22:28:01 - INFO - __main__ -   Batch number = 299
Evaluating:  43%|     | 299/699 [01:12<02:30,  2.66it/s]12/25/2021 22:28:01 - INFO - __main__ -   Batch number = 300
Evaluating:  43%|     | 300/699 [01:12<02:24,  2.76it/s]12/25/2021 22:28:01 - INFO - __main__ -   Batch number = 301
Evaluating:  43%|     | 301/699 [01:13<02:20,  2.84it/s]12/25/2021 22:28:02 - INFO - __main__ -   Batch number = 302
Evaluating:  43%|     | 302/699 [01:13<02:16,  2.91it/s]12/25/2021 22:28:02 - INFO - __main__ -   Batch number = 303
Evaluating:  43%|     | 303/699 [01:13<02:29,  2.65it/s]12/25/2021 22:28:02 - INFO - __main__ -   Batch number = 304
Evaluating:  43%|     | 304/699 [01:14<02:23,  2.76it/s]12/25/2021 22:28:03 - INFO - __main__ -   Batch number = 305
Evaluating:  44%|     | 305/699 [01:14<02:17,  2.87it/s]12/25/2021 22:28:03 - INFO - __main__ -   Batch number = 306
Evaluating:  44%|     | 306/699 [01:14<02:29,  2.63it/s]12/25/2021 22:28:04 - INFO - __main__ -   Batch number = 307
Evaluating:  44%|     | 307/699 [01:15<02:29,  2.61it/s]12/25/2021 22:28:04 - INFO - __main__ -   Batch number = 308
Evaluating:  44%|     | 308/699 [01:15<02:23,  2.72it/s]12/25/2021 22:28:04 - INFO - __main__ -   Batch number = 309
Evaluating:  44%|     | 309/699 [01:16<02:33,  2.55it/s]12/25/2021 22:28:05 - INFO - __main__ -   Batch number = 310
Evaluating:  44%|     | 310/699 [01:16<02:24,  2.69it/s]12/25/2021 22:28:05 - INFO - __main__ -   Batch number = 311
Evaluating:  44%|     | 311/699 [01:16<02:17,  2.81it/s]12/25/2021 22:28:05 - INFO - __main__ -   Batch number = 312
Evaluating:  45%|     | 312/699 [01:17<02:28,  2.60it/s]12/25/2021 22:28:06 - INFO - __main__ -   Batch number = 313
Evaluating:  45%|     | 313/699 [01:17<02:22,  2.71it/s]12/25/2021 22:28:06 - INFO - __main__ -   Batch number = 314
Evaluating:  45%|     | 314/699 [01:17<02:17,  2.80it/s]12/25/2021 22:28:06 - INFO - __main__ -   Batch number = 315
Evaluating:  45%|     | 315/699 [01:18<02:29,  2.57it/s]12/25/2021 22:28:07 - INFO - __main__ -   Batch number = 316
Evaluating:  45%|     | 316/699 [01:18<02:23,  2.67it/s]12/25/2021 22:28:07 - INFO - __main__ -   Batch number = 317
Evaluating:  45%|     | 317/699 [01:18<02:16,  2.79it/s]12/25/2021 22:28:08 - INFO - __main__ -   Batch number = 318
Evaluating:  45%|     | 318/699 [01:19<02:26,  2.59it/s]12/25/2021 22:28:08 - INFO - __main__ -   Batch number = 319
Evaluating:  46%|     | 319/699 [01:19<02:20,  2.71it/s]12/25/2021 22:28:08 - INFO - __main__ -   Batch number = 320
Evaluating:  46%|     | 320/699 [01:20<02:15,  2.79it/s]12/25/2021 22:28:09 - INFO - __main__ -   Batch number = 321
Evaluating:  46%|     | 321/699 [01:20<02:28,  2.55it/s]12/25/2021 22:28:09 - INFO - __main__ -   Batch number = 322
Evaluating:  46%|     | 322/699 [01:20<02:23,  2.63it/s]12/25/2021 22:28:10 - INFO - __main__ -   Batch number = 323
Evaluating:  46%|     | 323/699 [01:21<02:30,  2.49it/s]12/25/2021 22:28:10 - INFO - __main__ -   Batch number = 324
Evaluating:  46%|     | 324/699 [01:21<02:24,  2.60it/s]12/25/2021 22:28:10 - INFO - __main__ -   Batch number = 325
Evaluating:  46%|     | 325/699 [01:22<02:18,  2.69it/s]12/25/2021 22:28:11 - INFO - __main__ -   Batch number = 326
Evaluating:  47%|     | 326/699 [01:22<02:28,  2.52it/s]12/25/2021 22:28:11 - INFO - __main__ -   Batch number = 327
Evaluating:  47%|     | 327/699 [01:22<02:21,  2.62it/s]12/25/2021 22:28:11 - INFO - __main__ -   Batch number = 328
Evaluating:  47%|     | 328/699 [01:23<02:15,  2.73it/s]12/25/2021 22:28:12 - INFO - __main__ -   Batch number = 329
Evaluating:  47%|     | 329/699 [01:23<02:24,  2.55it/s]12/25/2021 22:28:12 - INFO - __main__ -   Batch number = 330
Evaluating:  47%|     | 330/699 [01:23<02:18,  2.67it/s]12/25/2021 22:28:13 - INFO - __main__ -   Batch number = 331
Evaluating:  47%|     | 331/699 [01:24<02:25,  2.53it/s]12/25/2021 22:28:13 - INFO - __main__ -   Batch number = 332
Evaluating:  47%|     | 332/699 [01:24<02:18,  2.66it/s]12/25/2021 22:28:13 - INFO - __main__ -   Batch number = 333
Evaluating:  48%|     | 333/699 [01:25<02:25,  2.52it/s]12/25/2021 22:28:14 - INFO - __main__ -   Batch number = 334
Evaluating:  48%|     | 334/699 [01:25<02:18,  2.63it/s]12/25/2021 22:28:14 - INFO - __main__ -   Batch number = 335
Evaluating:  48%|     | 335/699 [01:25<02:12,  2.74it/s]12/25/2021 22:28:14 - INFO - __main__ -   Batch number = 336
Evaluating:  48%|     | 336/699 [01:26<02:23,  2.53it/s]12/25/2021 22:28:15 - INFO - __main__ -   Batch number = 337
Evaluating:  48%|     | 337/699 [01:26<02:17,  2.63it/s]12/25/2021 22:28:15 - INFO - __main__ -   Batch number = 338
Evaluating:  48%|     | 338/699 [01:27<02:25,  2.48it/s]12/25/2021 22:28:16 - INFO - __main__ -   Batch number = 339
Evaluating:  48%|     | 339/699 [01:27<02:18,  2.60it/s]12/25/2021 22:28:16 - INFO - __main__ -   Batch number = 340
Evaluating:  49%|     | 340/699 [01:27<02:25,  2.47it/s]12/25/2021 22:28:17 - INFO - __main__ -   Batch number = 341
Evaluating:  49%|     | 341/699 [01:28<02:17,  2.60it/s]12/25/2021 22:28:17 - INFO - __main__ -   Batch number = 342
Evaluating:  49%|     | 342/699 [01:28<02:23,  2.49it/s]12/25/2021 22:28:17 - INFO - __main__ -   Batch number = 343
Evaluating:  49%|     | 343/699 [01:29<02:16,  2.60it/s]12/25/2021 22:28:18 - INFO - __main__ -   Batch number = 344
Evaluating:  49%|     | 344/699 [01:29<02:11,  2.70it/s]12/25/2021 22:28:18 - INFO - __main__ -   Batch number = 345
Evaluating:  49%|     | 345/699 [01:29<02:22,  2.49it/s]12/25/2021 22:28:18 - INFO - __main__ -   Batch number = 346
Evaluating:  49%|     | 346/699 [01:30<02:18,  2.55it/s]12/25/2021 22:28:19 - INFO - __main__ -   Batch number = 347
Evaluating:  50%|     | 347/699 [01:30<02:27,  2.38it/s]12/25/2021 22:28:19 - INFO - __main__ -   Batch number = 348
Evaluating:  50%|     | 348/699 [01:31<02:21,  2.48it/s]12/25/2021 22:28:20 - INFO - __main__ -   Batch number = 349
Evaluating:  50%|     | 349/699 [01:31<02:27,  2.37it/s]12/25/2021 22:28:20 - INFO - __main__ -   Batch number = 350
Evaluating:  50%|     | 350/699 [01:31<02:19,  2.49it/s]12/25/2021 22:28:20 - INFO - __main__ -   Batch number = 351
Evaluating:  50%|     | 351/699 [01:32<02:25,  2.38it/s]12/25/2021 22:28:21 - INFO - __main__ -   Batch number = 352
Evaluating:  50%|     | 352/699 [01:32<02:18,  2.50it/s]12/25/2021 22:28:21 - INFO - __main__ -   Batch number = 353
Evaluating:  51%|     | 353/699 [01:33<02:26,  2.36it/s]12/25/2021 22:28:22 - INFO - __main__ -   Batch number = 354
Evaluating:  51%|     | 354/699 [01:33<02:20,  2.46it/s]12/25/2021 22:28:22 - INFO - __main__ -   Batch number = 355
Evaluating:  51%|     | 355/699 [01:34<02:25,  2.36it/s]12/25/2021 22:28:23 - INFO - __main__ -   Batch number = 356
Evaluating:  51%|     | 356/699 [01:34<02:17,  2.50it/s]12/25/2021 22:28:23 - INFO - __main__ -   Batch number = 357
Evaluating:  51%|     | 357/699 [01:34<02:23,  2.39it/s]12/25/2021 22:28:23 - INFO - __main__ -   Batch number = 358
Evaluating:  51%|     | 358/699 [01:35<02:17,  2.48it/s]12/25/2021 22:28:24 - INFO - __main__ -   Batch number = 359
Evaluating:  51%|    | 359/699 [01:35<02:24,  2.35it/s]12/25/2021 22:28:24 - INFO - __main__ -   Batch number = 360
Evaluating:  52%|    | 360/699 [01:36<02:30,  2.25it/s]12/25/2021 22:28:25 - INFO - __main__ -   Batch number = 361
Evaluating:  52%|    | 361/699 [01:36<02:22,  2.37it/s]12/25/2021 22:28:25 - INFO - __main__ -   Batch number = 362
Evaluating:  52%|    | 362/699 [01:36<02:25,  2.31it/s]12/25/2021 22:28:26 - INFO - __main__ -   Batch number = 363
Evaluating:  52%|    | 363/699 [01:37<02:17,  2.45it/s]12/25/2021 22:28:26 - INFO - __main__ -   Batch number = 364
Evaluating:  52%|    | 364/699 [01:37<02:21,  2.37it/s]12/25/2021 22:28:26 - INFO - __main__ -   Batch number = 365
Evaluating:  52%|    | 365/699 [01:38<02:13,  2.50it/s]12/25/2021 22:28:27 - INFO - __main__ -   Batch number = 366
Evaluating:  52%|    | 366/699 [01:38<02:19,  2.39it/s]12/25/2021 22:28:27 - INFO - __main__ -   Batch number = 367
Evaluating:  53%|    | 367/699 [01:38<02:12,  2.50it/s]12/25/2021 22:28:28 - INFO - __main__ -   Batch number = 368
Evaluating:  53%|    | 368/699 [01:39<02:19,  2.38it/s]12/25/2021 22:28:28 - INFO - __main__ -   Batch number = 369
Evaluating:  53%|    | 369/699 [01:39<02:24,  2.29it/s]12/25/2021 22:28:29 - INFO - __main__ -   Batch number = 370
Evaluating:  53%|    | 370/699 [01:40<02:15,  2.42it/s]12/25/2021 22:28:29 - INFO - __main__ -   Batch number = 371
Evaluating:  53%|    | 371/699 [01:40<02:19,  2.34it/s]12/25/2021 22:28:29 - INFO - __main__ -   Batch number = 372
Evaluating:  53%|    | 372/699 [01:41<02:14,  2.44it/s]12/25/2021 22:28:30 - INFO - __main__ -   Batch number = 373
Evaluating:  53%|    | 373/699 [01:41<02:20,  2.32it/s]12/25/2021 22:28:30 - INFO - __main__ -   Batch number = 374
Evaluating:  54%|    | 374/699 [01:42<02:25,  2.23it/s]12/25/2021 22:28:31 - INFO - __main__ -   Batch number = 375
Evaluating:  54%|    | 375/699 [01:42<02:17,  2.35it/s]12/25/2021 22:28:31 - INFO - __main__ -   Batch number = 376
Evaluating:  54%|    | 376/699 [01:42<02:21,  2.29it/s]12/25/2021 22:28:31 - INFO - __main__ -   Batch number = 377
Evaluating:  54%|    | 377/699 [01:43<02:12,  2.42it/s]12/25/2021 22:28:32 - INFO - __main__ -   Batch number = 378
Evaluating:  54%|    | 378/699 [01:43<02:17,  2.33it/s]12/25/2021 22:28:32 - INFO - __main__ -   Batch number = 379
Evaluating:  54%|    | 379/699 [01:44<02:22,  2.25it/s]12/25/2021 22:28:33 - INFO - __main__ -   Batch number = 380
Evaluating:  54%|    | 380/699 [01:44<02:12,  2.40it/s]12/25/2021 22:28:33 - INFO - __main__ -   Batch number = 381
Evaluating:  55%|    | 381/699 [01:45<02:17,  2.31it/s]12/25/2021 22:28:34 - INFO - __main__ -   Batch number = 382
Evaluating:  55%|    | 382/699 [01:45<02:24,  2.20it/s]12/25/2021 22:28:34 - INFO - __main__ -   Batch number = 383
Evaluating:  55%|    | 383/699 [01:45<02:14,  2.35it/s]12/25/2021 22:28:34 - INFO - __main__ -   Batch number = 384
Evaluating:  55%|    | 384/699 [01:46<02:18,  2.27it/s]12/25/2021 22:28:35 - INFO - __main__ -   Batch number = 385
Evaluating:  55%|    | 385/699 [01:46<02:22,  2.21it/s]12/25/2021 22:28:35 - INFO - __main__ -   Batch number = 386
Evaluating:  55%|    | 386/699 [01:47<02:13,  2.34it/s]12/25/2021 22:28:36 - INFO - __main__ -   Batch number = 387
Evaluating:  55%|    | 387/699 [01:47<02:17,  2.27it/s]12/25/2021 22:28:36 - INFO - __main__ -   Batch number = 388
Evaluating:  56%|    | 388/699 [01:48<02:10,  2.39it/s]12/25/2021 22:28:37 - INFO - __main__ -   Batch number = 389
Evaluating:  56%|    | 389/699 [01:48<02:14,  2.31it/s]12/25/2021 22:28:37 - INFO - __main__ -   Batch number = 390
Evaluating:  56%|    | 390/699 [01:49<02:19,  2.22it/s]12/25/2021 22:28:38 - INFO - __main__ -   Batch number = 391
Evaluating:  56%|    | 391/699 [01:49<02:22,  2.17it/s]12/25/2021 22:28:38 - INFO - __main__ -   Batch number = 392
Evaluating:  56%|    | 392/699 [01:49<02:13,  2.30it/s]12/25/2021 22:28:38 - INFO - __main__ -   Batch number = 393
Evaluating:  56%|    | 393/699 [01:50<02:16,  2.23it/s]12/25/2021 22:28:39 - INFO - __main__ -   Batch number = 394
Evaluating:  56%|    | 394/699 [01:50<02:20,  2.17it/s]12/25/2021 22:28:39 - INFO - __main__ -   Batch number = 395
Evaluating:  57%|    | 395/699 [01:51<02:11,  2.31it/s]12/25/2021 22:28:40 - INFO - __main__ -   Batch number = 396
Evaluating:  57%|    | 396/699 [01:51<02:14,  2.25it/s]12/25/2021 22:28:40 - INFO - __main__ -   Batch number = 397
Evaluating:  57%|    | 397/699 [01:52<02:18,  2.19it/s]12/25/2021 22:28:41 - INFO - __main__ -   Batch number = 398
Evaluating:  57%|    | 398/699 [01:52<02:08,  2.34it/s]12/25/2021 22:28:41 - INFO - __main__ -   Batch number = 399
Evaluating:  57%|    | 399/699 [01:53<02:13,  2.25it/s]12/25/2021 22:28:42 - INFO - __main__ -   Batch number = 400
Evaluating:  57%|    | 400/699 [01:53<02:17,  2.18it/s]12/25/2021 22:28:42 - INFO - __main__ -   Batch number = 401
Evaluating:  57%|    | 401/699 [01:53<02:17,  2.16it/s]12/25/2021 22:28:43 - INFO - __main__ -   Batch number = 402
Evaluating:  58%|    | 402/699 [01:54<02:08,  2.31it/s]12/25/2021 22:28:43 - INFO - __main__ -   Batch number = 403
Evaluating:  58%|    | 403/699 [01:54<02:11,  2.26it/s]12/25/2021 22:28:43 - INFO - __main__ -   Batch number = 404
Evaluating:  58%|    | 404/699 [01:55<02:13,  2.21it/s]12/25/2021 22:28:44 - INFO - __main__ -   Batch number = 405
Evaluating:  58%|    | 405/699 [01:55<02:05,  2.35it/s]12/25/2021 22:28:44 - INFO - __main__ -   Batch number = 406
Evaluating:  58%|    | 406/699 [01:56<02:09,  2.26it/s]12/25/2021 22:28:45 - INFO - __main__ -   Batch number = 407
Evaluating:  58%|    | 407/699 [01:56<02:15,  2.16it/s]12/25/2021 22:28:45 - INFO - __main__ -   Batch number = 408
Evaluating:  58%|    | 408/699 [01:57<02:16,  2.13it/s]12/25/2021 22:28:46 - INFO - __main__ -   Batch number = 409
Evaluating:  59%|    | 409/699 [01:57<02:07,  2.28it/s]12/25/2021 22:28:46 - INFO - __main__ -   Batch number = 410
Evaluating:  59%|    | 410/699 [01:57<02:09,  2.23it/s]12/25/2021 22:28:47 - INFO - __main__ -   Batch number = 411
Evaluating:  59%|    | 411/699 [01:58<02:13,  2.16it/s]12/25/2021 22:28:47 - INFO - __main__ -   Batch number = 412
Evaluating:  59%|    | 412/699 [01:58<02:15,  2.12it/s]12/25/2021 22:28:48 - INFO - __main__ -   Batch number = 413
Evaluating:  59%|    | 413/699 [01:59<02:16,  2.10it/s]12/25/2021 22:28:48 - INFO - __main__ -   Batch number = 414
Evaluating:  59%|    | 414/699 [01:59<02:07,  2.23it/s]12/25/2021 22:28:48 - INFO - __main__ -   Batch number = 415
Evaluating:  59%|    | 415/699 [02:00<02:09,  2.18it/s]12/25/2021 22:28:49 - INFO - __main__ -   Batch number = 416
Evaluating:  60%|    | 416/699 [02:00<02:12,  2.14it/s]12/25/2021 22:28:49 - INFO - __main__ -   Batch number = 417
Evaluating:  60%|    | 417/699 [02:01<02:13,  2.11it/s]12/25/2021 22:28:50 - INFO - __main__ -   Batch number = 418
Evaluating:  60%|    | 418/699 [02:01<02:04,  2.25it/s]12/25/2021 22:28:50 - INFO - __main__ -   Batch number = 419
Evaluating:  60%|    | 419/699 [02:02<02:06,  2.21it/s]12/25/2021 22:28:51 - INFO - __main__ -   Batch number = 420
Evaluating:  60%|    | 420/699 [02:02<02:10,  2.14it/s]12/25/2021 22:28:51 - INFO - __main__ -   Batch number = 421
Evaluating:  60%|    | 421/699 [02:03<02:12,  2.10it/s]12/25/2021 22:28:52 - INFO - __main__ -   Batch number = 422
Evaluating:  60%|    | 422/699 [02:03<02:11,  2.10it/s]12/25/2021 22:28:52 - INFO - __main__ -   Batch number = 423
Evaluating:  61%|    | 423/699 [02:03<02:02,  2.25it/s]12/25/2021 22:28:53 - INFO - __main__ -   Batch number = 424
Evaluating:  61%|    | 424/699 [02:04<02:05,  2.20it/s]12/25/2021 22:28:53 - INFO - __main__ -   Batch number = 425
Evaluating:  61%|    | 425/699 [02:04<02:08,  2.14it/s]12/25/2021 22:28:54 - INFO - __main__ -   Batch number = 426
Evaluating:  61%|    | 426/699 [02:05<02:09,  2.10it/s]12/25/2021 22:28:54 - INFO - __main__ -   Batch number = 427
Evaluating:  61%|    | 427/699 [02:05<02:10,  2.08it/s]12/25/2021 22:28:55 - INFO - __main__ -   Batch number = 428
Evaluating:  61%|    | 428/699 [02:06<02:10,  2.08it/s]12/25/2021 22:28:55 - INFO - __main__ -   Batch number = 429
Evaluating:  61%|   | 429/699 [02:06<02:02,  2.21it/s]12/25/2021 22:28:55 - INFO - __main__ -   Batch number = 430
Evaluating:  62%|   | 430/699 [02:07<02:05,  2.14it/s]12/25/2021 22:28:56 - INFO - __main__ -   Batch number = 431
Evaluating:  62%|   | 431/699 [02:07<02:08,  2.09it/s]12/25/2021 22:28:56 - INFO - __main__ -   Batch number = 432
Evaluating:  62%|   | 432/699 [02:08<02:09,  2.06it/s]12/25/2021 22:28:57 - INFO - __main__ -   Batch number = 433
Evaluating:  62%|   | 433/699 [02:08<02:10,  2.05it/s]12/25/2021 22:28:57 - INFO - __main__ -   Batch number = 434
Evaluating:  62%|   | 434/699 [02:09<02:08,  2.06it/s]12/25/2021 22:28:58 - INFO - __main__ -   Batch number = 435
Evaluating:  62%|   | 435/699 [02:09<02:08,  2.05it/s]12/25/2021 22:28:58 - INFO - __main__ -   Batch number = 436
Evaluating:  62%|   | 436/699 [02:10<02:00,  2.18it/s]12/25/2021 22:28:59 - INFO - __main__ -   Batch number = 437
Evaluating:  63%|   | 437/699 [02:10<02:02,  2.14it/s]12/25/2021 22:28:59 - INFO - __main__ -   Batch number = 438
Evaluating:  63%|   | 438/699 [02:11<02:05,  2.08it/s]12/25/2021 22:29:00 - INFO - __main__ -   Batch number = 439
Evaluating:  63%|   | 439/699 [02:11<02:08,  2.02it/s]12/25/2021 22:29:00 - INFO - __main__ -   Batch number = 440
Evaluating:  63%|   | 440/699 [02:12<02:09,  2.01it/s]12/25/2021 22:29:01 - INFO - __main__ -   Batch number = 441
Evaluating:  63%|   | 441/699 [02:12<02:08,  2.01it/s]12/25/2021 22:29:01 - INFO - __main__ -   Batch number = 442
Evaluating:  63%|   | 442/699 [02:13<02:08,  2.01it/s]12/25/2021 22:29:02 - INFO - __main__ -   Batch number = 443
Evaluating:  63%|   | 443/699 [02:13<02:07,  2.01it/s]12/25/2021 22:29:02 - INFO - __main__ -   Batch number = 444
Evaluating:  64%|   | 444/699 [02:14<02:06,  2.01it/s]12/25/2021 22:29:03 - INFO - __main__ -   Batch number = 445
Evaluating:  64%|   | 444/699 [02:14<01:17,  3.31it/s]
Traceback (most recent call last):
  File "third_party/my_run_tag.py", line 1116, in <module>
    main()
  File "third_party/my_run_tag.py", line 1057, in main
    predict_and_save(args, adapter_args, model, tokenizer, labels, lang2id, pad_token_label_id, lang_adapter_names, task_name, 'test')
  File "third_party/my_run_tag.py", line 856, in predict_and_save
    result, predictions = evaluate(args, model, tokenizer, labels, pad_token_label_id, mode=split, lang=lang, lang2id=lang2id, adapter_weight=adapter_weight, lang_adapter_names=lang_adapter_names, task_name=task_name, calc_weight_step=args.calc_weight_step)
  File "third_party/my_run_tag.py", line 463, in evaluate
    outputs = model(**inputs)
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/abhijeet/rohan/cloud-emea/src/transformers/modeling_bert.py", line 1761, in forward
    tmp_score=tmp_score
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/abhijeet/rohan/cloud-emea/src/transformers/modeling_bert.py", line 942, in forward
    input_ids=input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/abhijeet/rohan/cloud-emea/src/transformers/modeling_bert.py", line 214, in forward
    embeddings = self.dropout(embeddings)
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/abhijeet/rohan/venvs/cloud-emea/lib/python3.7/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
KeyboardInterrupt
Command exited with non-zero status 1
77.37user 81.99system 2:35.31elapsed 102%CPU (0avgtext+0avgdata 3990064maxresident)k
74328inputs+288outputs (0major+8545470minor)pagefaults 0swaps
Command terminated by signal 2
0.00user 0.00system 0:00.00elapsed 100%CPU (0avgtext+0avgdata 6576maxresident)k
0inputs+0outputs (0major+488minor)pagefaults 0swaps
Fatal Python error: initsite: Failed to import the site module

Command exited with non-zero status 1
0.01user 0.01system 0:00.07elapsed 33%CPU (0avgtext+0avgdata 9272maxresident)k
0inputs+0outputs (0major+972minor)pagefaults 0swaps
Traceback (most recent call last):
  File "third_party/my_run_tag.py", line 17, in <module>
    """Fine-tuning models for NER and POS tagging."""
KeyboardInterrupt
Command exited with non-zero status 1
0.03user 0.00system 0:00.04elapsed 100%CPU (0avgtext+0avgdata 13148maxresident)k
0inputs+0outputs (0major+1947minor)pagefaults 0swaps
Traceback (most recent call last):
  File "third_party/my_run_tag.py", line 21, in <module>
    import argparse
  File "/usr/lib/python3.7/argparse.py", line 87, in <module>
    import re as _re
  File "/usr/lib/python3.7/re.py", line 125, in <module>
    import sre_compile
  File "<frozen importlib._bootstrap>", line 983, in _find_and_load
  File "<frozen importlib._bootstrap>", line 967, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 677, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 857, in get_code
  File "<frozen importlib._bootstrap_external>", line 525, in _compile_bytecode
KeyboardInterrupt
Command exited with non-zero status 1
0.02user 0.00system 0:00.02elapsed 100%CPU (0avgtext+0avgdata 13212maxresident)k
0inputs+0outputs (0major+1969minor)pagefaults 0swaps
Fatal Python error: initsite: Failed to import the site module
Traceback (most recent call last):
  File "/usr/lib/python3.7/site.py", line 596, in <module>
    main()
  File "/usr/lib/python3.7/site.py", line 579, in main
  File "/usr/lib/python3.7/site.py", line 511, in venv
    addsitepackages(known_paths, [sys.prefix])
  File "/usr/lib/python3.7/site.py", line 366, in addsitepackages
    addsitedir(sitedir, known_paths)
  File "/usr/lib/python3.7/site.py", line 213, in addsitedir
    addpackage(sitedir, name, known_paths)
  File "/usr/lib/python3.7/site.py", line 174, in addpackage
    exec(line)
  File "<string>", line 1, in <module>
  File "<frozen importlib._bootstrap_external>", line 1280, in find_spec
  File "<frozen importlib._bootstrap_external>", line 1252, in _get_spec
  File "<frozen importlib._bootstrap_external>", line 1388, in find_spec
  File "<frozen importlib._bootstrap_external>", line 102, in _path_isdir
  File "<frozen importlib._bootstrap_external>", line 87, in _path_is_mode_type
  File "<frozen importlib._bootstrap_external>", line 81, in _path_stat
KeyboardInterrupt
Command exited with non-zero status 1
0.03user 0.00system 0:00.03elapsed 100%CPU (0avgtext+0avgdata 9808maxresident)k
0inputs+0outputs (0major+1073minor)pagefaults 0swaps
Fatal Python error: initsite: Failed to import the site module
Traceback (most recent call last):
  File "/usr/lib/python3.7/site.py", line 596, in <module>
    main()
  File "/usr/lib/python3.7/site.py", line 579, in main
    known_paths = venv(known_paths)
  File "/usr/lib/python3.7/site.py", line 511, in venv
    addsitepackages(known_paths, [sys.prefix])
  File "/usr/lib/python3.7/site.py", line 366, in addsitepackages
    addsitedir(sitedir, known_paths)
  File "/usr/lib/python3.7/site.py", line 213, in addsitedir
    addpackage(sitedir, name, known_paths)
  File "/usr/lib/python3.7/site.py", line 174, in addpackage
    exec(line)
  File "<string>", line 1, in <module>
  File "/usr/lib/python3.7/importlib/util.py", line 14, in <module>
    from contextlib import contextmanager
  File "/usr/lib/python3.7/contextlib.py", line 5, in <module>
    from collections import deque
  File "/usr/lib/python3.7/collections/__init__.py", line 21, in <module>
    from operator import itemgetter as _itemgetter, eq as _eq
  File "/usr/lib/python3.7/operator.py", line 412, in <module>
    from _operator import *
KeyboardInterrupt
Command exited with non-zero status 1
0.02user 0.00system 0:00.02elapsed 96%CPU (0avgtext+0avgdata 9324maxresident)k
0inputs+0outputs (0major+990minor)pagefaults 0swaps
Fatal Python error: init_sys_streams: can't initialize sys standard streams
Traceback (most recent call last):
  File "/usr/lib/python3.7/io.py", line 75, in <module>
    class RawIOBase(_io._RawIOBase, IOBase):
  File "/usr/lib/python3.7/abc.py", line 126, in __new__
    cls = super().__new__(mcls, name, bases, namespace, **kwargs)
KeyboardInterrupt
Command terminated by signal 6
0.01user 0.00system 0:00.14elapsed 12%CPU (0avgtext+0avgdata 8064maxresident)k
0inputs+0outputs (0major+708minor)pagefaults 0swaps
Command terminated by signal 2
0.00user 0.00system 0:00.00elapsed 80%CPU (0avgtext+0avgdata 5536maxresident)k
0inputs+0outputs (0major+339minor)pagefaults 0swaps
Fatal Python error: init_sys_streams: can't initialize sys standard streams
Traceback (most recent call last):
  File "/usr/lib/python3.7/io.py", line 95, in <module>
    from _io import _WindowsConsoleIO
  File "<frozen importlib._bootstrap>", line 1009, in _handle_fromlist
KeyboardInterrupt
Command terminated by signal 6
0.01user 0.00system 0:00.13elapsed 12%CPU (0avgtext+0avgdata 8088maxresident)k
0inputs+0outputs (0major+711minor)pagefaults 0swaps
Fatal Python error: initsite: Failed to import the site module
Traceback (most recent call last):
  File "/usr/lib/python3.7/site.py", line 596, in <module>
    main()
  File "/usr/lib/python3.7/site.py", line 579, in main
    known_paths = venv(known_paths)
  File "/usr/lib/python3.7/site.py", line 511, in venv
    addsitepackages(known_paths, [sys.prefix])
  File "/usr/lib/python3.7/site.py", line 366, in addsitepackages
    addsitedir(sitedir, known_paths)
  File "/usr/lib/python3.7/site.py", line 213, in addsitedir
    addpackage(sitedir, name, known_paths)
  File "/usr/lib/python3.7/site.py", line 174, in addpackage
    exec(line)
  File "<string>", line 1, in <module>
  File "/usr/lib/python3.7/importlib/util.py", line 14, in <module>
    from contextlib import contextmanager
  File "/usr/lib/python3.7/contextlib.py", line 5, in <module>
    from collections import deque
  File "/usr/lib/python3.7/collections/__init__.py", line 27, in <module>
    from reprlib import recursive_repr as _recursive_repr
  File "<frozen importlib._bootstrap>", line 983, in _find_and_load
  File "<frozen importlib._bootstrap>", line 967, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 677, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
KeyboardInterrupt
Command exited with non-zero status 1
0.01user 0.00system 0:00.02elapsed 96%CPU (0avgtext+0avgdata 9388maxresident)k
0inputs+8outputs (0major+1003minor)pagefaults 0swaps
