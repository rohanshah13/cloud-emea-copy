12/25/2021 22:23:48 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_ensemble_oracle_top1/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ar', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_ensemble_oracle_top1//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
12/25/2021 22:23:48 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
12/25/2021 22:23:48 - INFO - __main__ -   Seed = 1
12/25/2021 22:23:48 - INFO - root -   save model
12/25/2021 22:23:48 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_ensemble_oracle_top1/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ar', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_ensemble_oracle_top1//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
12/25/2021 22:23:48 - INFO - __main__ -   Loading pretrained model and tokenizer
12/25/2021 22:23:51 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
12/25/2021 22:23:56 - INFO - __main__ -   Using lang2id = None
12/25/2021 22:23:56 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
12/25/2021 22:23:56 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
12/25/2021 22:23:56 - INFO - root -   Trying to decide if add adapter
12/25/2021 22:23:56 - INFO - root -   loading task adapter
12/25/2021 22:23:57 - INFO - root -   loading lang adpater topk
12/25/2021 22:23:57 - INFO - __main__ -   Loading Adapter Languages from scripts/udpos/en/ar.json
12/25/2021 22:23:57 - INFO - __main__ -   Adapter Languages : ['{"adapter": "ka", "num_seeds": 3, "f1": 58.37731855040207}'], Length : 1
12/25/2021 22:23:57 - INFO - __main__ -   Adapter Names ['{"adapter": "ka", "num_seeds": 3, "f1": 58.37731855040207}/wiki@ukp'], Length : 1
12/25/2021 22:23:57 - INFO - __main__ -   Language = {"adapter": "ka", "num_seeds": 3, "f1": 58.37731855040207}
12/25/2021 22:23:57 - INFO - __main__ -   Adapter Name = {"adapter": "ka", "num_seeds": 3, "f1": 58.37731855040207}/wiki@ukp
12/25/2021 22:23:59 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_ensemble_oracle_top1/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ar', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_ensemble_oracle_top1//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
12/25/2021 22:23:59 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
12/25/2021 22:23:59 - INFO - __main__ -   Seed = 2
12/25/2021 22:23:59 - INFO - root -   save model
12/25/2021 22:23:59 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_ensemble_oracle_top1/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ar', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_ensemble_oracle_top1//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
12/25/2021 22:23:59 - INFO - __main__ -   Loading pretrained model and tokenizer
12/25/2021 22:24:01 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
12/25/2021 22:24:07 - INFO - __main__ -   Using lang2id = None
12/25/2021 22:24:07 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
12/25/2021 22:24:07 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
12/25/2021 22:24:07 - INFO - root -   Trying to decide if add adapter
12/25/2021 22:24:07 - INFO - root -   loading task adapter
12/25/2021 22:24:07 - INFO - root -   loading lang adpater topk
12/25/2021 22:24:07 - INFO - __main__ -   Loading Adapter Languages from scripts/udpos/en/ar.json
12/25/2021 22:24:07 - INFO - __main__ -   Adapter Languages : ['{"adapter": "ka", "num_seeds": 3, "f1": 58.37731855040207}'], Length : 1
12/25/2021 22:24:07 - INFO - __main__ -   Adapter Names ['{"adapter": "ka", "num_seeds": 3, "f1": 58.37731855040207}/wiki@ukp'], Length : 1
12/25/2021 22:24:07 - INFO - __main__ -   Language = {"adapter": "ka", "num_seeds": 3, "f1": 58.37731855040207}
12/25/2021 22:24:07 - INFO - __main__ -   Adapter Name = {"adapter": "ka", "num_seeds": 3, "f1": 58.37731855040207}/wiki@ukp
12/25/2021 22:24:09 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_ensemble_oracle_top1/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ar', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_ensemble_oracle_top1//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
12/25/2021 22:24:09 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
12/25/2021 22:24:09 - INFO - __main__ -   Seed = 3
12/25/2021 22:24:09 - INFO - root -   save model
12/25/2021 22:24:09 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_ensemble_oracle_top1/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ar', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_ensemble_oracle_top1//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
12/25/2021 22:24:09 - INFO - __main__ -   Loading pretrained model and tokenizer
12/25/2021 22:25:12 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_ensemble_oracle_top1/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ar', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_ensemble_oracle_top1//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
12/25/2021 22:25:12 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
12/25/2021 22:25:12 - INFO - __main__ -   Seed = 1
12/25/2021 22:25:12 - INFO - root -   save model
12/25/2021 22:25:12 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_ensemble_oracle_top1/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ar', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_ensemble_oracle_top1//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
12/25/2021 22:25:12 - INFO - __main__ -   Loading pretrained model and tokenizer
12/25/2021 22:25:14 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
12/25/2021 22:25:20 - INFO - __main__ -   Using lang2id = None
12/25/2021 22:25:20 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
12/25/2021 22:25:20 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
12/25/2021 22:25:20 - INFO - root -   Trying to decide if add adapter
12/25/2021 22:25:20 - INFO - root -   loading task adapter
12/25/2021 22:25:20 - INFO - root -   loading lang adpater topk
12/25/2021 22:25:20 - INFO - __main__ -   Loading Adapter Languages from scripts/udpos/en/ar.json
12/25/2021 22:25:20 - INFO - __main__ -   Adapter Languages : ['ka'], Length : 1
12/25/2021 22:25:20 - INFO - __main__ -   Adapter Names ['ka/wiki@ukp'], Length : 1
12/25/2021 22:25:20 - INFO - __main__ -   Language = ka
12/25/2021 22:25:20 - INFO - __main__ -   Adapter Name = ka/wiki@ukp
12/25/2021 22:25:27 - INFO - __main__ -   Args Adapter Weight = equal
12/25/2021 22:25:27 - INFO - __main__ -   Adapter Languages = ['ka']
12/25/2021 22:25:27 - INFO - __main__ -   Adapter Weights = [1.0]
12/25/2021 22:25:27 - INFO - __main__ -   Sum of Adapter Weights = 1.0
12/25/2021 22:25:27 - INFO - __main__ -   Length of Adapter Weights = 1
12/25/2021 22:25:27 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_ar_bert-base-multilingual-cased_128
12/25/2021 22:25:28 - INFO - __main__ -   ***** Running evaluation  in ar *****
12/25/2021 22:25:28 - INFO - __main__ -     Num examples = 1784
12/25/2021 22:25:28 - INFO - __main__ -     Batch size = 32
12/25/2021 22:25:28 - INFO - __main__ -   Batch number = 1
12/25/2021 22:25:28 - INFO - __main__ -   Batch number = 2
12/25/2021 22:25:28 - INFO - __main__ -   Batch number = 3
12/25/2021 22:25:28 - INFO - __main__ -   Batch number = 4
12/25/2021 22:25:28 - INFO - __main__ -   Batch number = 5
12/25/2021 22:25:29 - INFO - __main__ -   Batch number = 6
12/25/2021 22:25:29 - INFO - __main__ -   Batch number = 7
12/25/2021 22:25:29 - INFO - __main__ -   Batch number = 8
12/25/2021 22:25:29 - INFO - __main__ -   Batch number = 9
12/25/2021 22:25:29 - INFO - __main__ -   Batch number = 10
12/25/2021 22:25:29 - INFO - __main__ -   Batch number = 11
12/25/2021 22:25:29 - INFO - __main__ -   Batch number = 12
12/25/2021 22:25:30 - INFO - __main__ -   Batch number = 13
12/25/2021 22:25:30 - INFO - __main__ -   Batch number = 14
12/25/2021 22:25:30 - INFO - __main__ -   Batch number = 15
12/25/2021 22:25:30 - INFO - __main__ -   Batch number = 16
12/25/2021 22:25:30 - INFO - __main__ -   Batch number = 17
12/25/2021 22:25:30 - INFO - __main__ -   Batch number = 18
12/25/2021 22:25:30 - INFO - __main__ -   Batch number = 19
12/25/2021 22:25:30 - INFO - __main__ -   Batch number = 20
12/25/2021 22:25:31 - INFO - __main__ -   Batch number = 21
12/25/2021 22:25:31 - INFO - __main__ -   Batch number = 22
12/25/2021 22:25:31 - INFO - __main__ -   Batch number = 23
12/25/2021 22:25:31 - INFO - __main__ -   Batch number = 24
12/25/2021 22:25:31 - INFO - __main__ -   Batch number = 25
12/25/2021 22:25:31 - INFO - __main__ -   Batch number = 26
12/25/2021 22:25:31 - INFO - __main__ -   Batch number = 27
12/25/2021 22:25:31 - INFO - __main__ -   Batch number = 28
12/25/2021 22:25:32 - INFO - __main__ -   Batch number = 29
12/25/2021 22:25:32 - INFO - __main__ -   Batch number = 30
12/25/2021 22:25:32 - INFO - __main__ -   Batch number = 31
12/25/2021 22:25:32 - INFO - __main__ -   Batch number = 32
12/25/2021 22:25:32 - INFO - __main__ -   Batch number = 33
12/25/2021 22:25:32 - INFO - __main__ -   Batch number = 34
12/25/2021 22:25:32 - INFO - __main__ -   Batch number = 35
12/25/2021 22:25:33 - INFO - __main__ -   Batch number = 36
12/25/2021 22:25:33 - INFO - __main__ -   Batch number = 37
12/25/2021 22:25:33 - INFO - __main__ -   Batch number = 38
12/25/2021 22:25:33 - INFO - __main__ -   Batch number = 39
12/25/2021 22:25:33 - INFO - __main__ -   Batch number = 40
12/25/2021 22:25:33 - INFO - __main__ -   Batch number = 41
12/25/2021 22:25:33 - INFO - __main__ -   Batch number = 42
12/25/2021 22:25:34 - INFO - __main__ -   Batch number = 43
12/25/2021 22:25:34 - INFO - __main__ -   Batch number = 44
12/25/2021 22:25:34 - INFO - __main__ -   Batch number = 45
12/25/2021 22:25:34 - INFO - __main__ -   Batch number = 46
12/25/2021 22:25:34 - INFO - __main__ -   Batch number = 47
12/25/2021 22:25:34 - INFO - __main__ -   Batch number = 48
12/25/2021 22:25:34 - INFO - __main__ -   Batch number = 49
12/25/2021 22:25:35 - INFO - __main__ -   Batch number = 50
12/25/2021 22:25:35 - INFO - __main__ -   Batch number = 51
12/25/2021 22:25:35 - INFO - __main__ -   Batch number = 52
12/25/2021 22:25:35 - INFO - __main__ -   Batch number = 53
12/25/2021 22:25:35 - INFO - __main__ -   Batch number = 54
12/25/2021 22:25:35 - INFO - __main__ -   Batch number = 55
12/25/2021 22:25:35 - INFO - __main__ -   Batch number = 56
12/25/2021 22:25:36 - INFO - __main__ -   ***** Evaluation result  in ar *****
12/25/2021 22:25:36 - INFO - __main__ -     f1 = 0.598398509760294
12/25/2021 22:25:36 - INFO - __main__ -     loss = 1.4291882035987717
12/25/2021 22:25:36 - INFO - __main__ -     precision = 0.5879257752923234
12/25/2021 22:25:36 - INFO - __main__ -     recall = 0.6092511129257435
12/25/2021 22:25:39 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_ensemble_oracle_top1/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ar', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_ensemble_oracle_top1//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
12/25/2021 22:25:39 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
12/25/2021 22:25:39 - INFO - __main__ -   Seed = 2
12/25/2021 22:25:39 - INFO - root -   save model
12/25/2021 22:25:39 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_ensemble_oracle_top1/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ar', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_ensemble_oracle_top1//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
12/25/2021 22:25:39 - INFO - __main__ -   Loading pretrained model and tokenizer
12/25/2021 22:25:41 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
12/25/2021 22:25:47 - INFO - __main__ -   Using lang2id = None
12/25/2021 22:25:47 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
12/25/2021 22:25:47 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
12/25/2021 22:25:47 - INFO - root -   Trying to decide if add adapter
12/25/2021 22:25:47 - INFO - root -   loading task adapter
12/25/2021 22:25:47 - INFO - root -   loading lang adpater topk
12/25/2021 22:25:47 - INFO - __main__ -   Loading Adapter Languages from scripts/udpos/en/ar.json
12/25/2021 22:25:47 - INFO - __main__ -   Adapter Languages : ['ka'], Length : 1
12/25/2021 22:25:47 - INFO - __main__ -   Adapter Names ['ka/wiki@ukp'], Length : 1
12/25/2021 22:25:47 - INFO - __main__ -   Language = ka
12/25/2021 22:25:47 - INFO - __main__ -   Adapter Name = ka/wiki@ukp
12/25/2021 22:25:53 - INFO - __main__ -   Args Adapter Weight = equal
12/25/2021 22:25:53 - INFO - __main__ -   Adapter Languages = ['ka']
12/25/2021 22:25:53 - INFO - __main__ -   Adapter Weights = [1.0]
12/25/2021 22:25:53 - INFO - __main__ -   Sum of Adapter Weights = 1.0
12/25/2021 22:25:53 - INFO - __main__ -   Length of Adapter Weights = 1
12/25/2021 22:25:53 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_ar_bert-base-multilingual-cased_128
12/25/2021 22:25:53 - INFO - __main__ -   ***** Running evaluation  in ar *****
12/25/2021 22:25:53 - INFO - __main__ -     Num examples = 1784
12/25/2021 22:25:53 - INFO - __main__ -     Batch size = 32
12/25/2021 22:25:53 - INFO - __main__ -   Batch number = 1
12/25/2021 22:25:53 - INFO - __main__ -   Batch number = 2
12/25/2021 22:25:54 - INFO - __main__ -   Batch number = 3
12/25/2021 22:25:54 - INFO - __main__ -   Batch number = 4
12/25/2021 22:25:54 - INFO - __main__ -   Batch number = 5
12/25/2021 22:25:54 - INFO - __main__ -   Batch number = 6
12/25/2021 22:25:54 - INFO - __main__ -   Batch number = 7
12/25/2021 22:25:54 - INFO - __main__ -   Batch number = 8
12/25/2021 22:25:54 - INFO - __main__ -   Batch number = 9
12/25/2021 22:25:54 - INFO - __main__ -   Batch number = 10
12/25/2021 22:25:55 - INFO - __main__ -   Batch number = 11
12/25/2021 22:25:55 - INFO - __main__ -   Batch number = 12
12/25/2021 22:25:55 - INFO - __main__ -   Batch number = 13
12/25/2021 22:25:55 - INFO - __main__ -   Batch number = 14
12/25/2021 22:25:55 - INFO - __main__ -   Batch number = 15
12/25/2021 22:25:55 - INFO - __main__ -   Batch number = 16
12/25/2021 22:25:55 - INFO - __main__ -   Batch number = 17
12/25/2021 22:25:56 - INFO - __main__ -   Batch number = 18
12/25/2021 22:25:56 - INFO - __main__ -   Batch number = 19
12/25/2021 22:25:56 - INFO - __main__ -   Batch number = 20
12/25/2021 22:25:56 - INFO - __main__ -   Batch number = 21
12/25/2021 22:25:56 - INFO - __main__ -   Batch number = 22
12/25/2021 22:25:56 - INFO - __main__ -   Batch number = 23
12/25/2021 22:25:56 - INFO - __main__ -   Batch number = 24
12/25/2021 22:25:56 - INFO - __main__ -   Batch number = 25
12/25/2021 22:25:57 - INFO - __main__ -   Batch number = 26
12/25/2021 22:25:57 - INFO - __main__ -   Batch number = 27
12/25/2021 22:25:57 - INFO - __main__ -   Batch number = 28
12/25/2021 22:25:57 - INFO - __main__ -   Batch number = 29
12/25/2021 22:25:57 - INFO - __main__ -   Batch number = 30
12/25/2021 22:25:57 - INFO - __main__ -   Batch number = 31
12/25/2021 22:25:57 - INFO - __main__ -   Batch number = 32
12/25/2021 22:25:58 - INFO - __main__ -   Batch number = 33
12/25/2021 22:25:58 - INFO - __main__ -   Batch number = 34
12/25/2021 22:25:58 - INFO - __main__ -   Batch number = 35
12/25/2021 22:25:58 - INFO - __main__ -   Batch number = 36
12/25/2021 22:25:58 - INFO - __main__ -   Batch number = 37
12/25/2021 22:25:58 - INFO - __main__ -   Batch number = 38
12/25/2021 22:25:58 - INFO - __main__ -   Batch number = 39
12/25/2021 22:25:58 - INFO - __main__ -   Batch number = 40
12/25/2021 22:25:59 - INFO - __main__ -   Batch number = 41
12/25/2021 22:25:59 - INFO - __main__ -   Batch number = 42
12/25/2021 22:25:59 - INFO - __main__ -   Batch number = 43
12/25/2021 22:25:59 - INFO - __main__ -   Batch number = 44
12/25/2021 22:25:59 - INFO - __main__ -   Batch number = 45
12/25/2021 22:25:59 - INFO - __main__ -   Batch number = 46
12/25/2021 22:25:59 - INFO - __main__ -   Batch number = 47
12/25/2021 22:26:00 - INFO - __main__ -   Batch number = 48
12/25/2021 22:26:00 - INFO - __main__ -   Batch number = 49
12/25/2021 22:26:00 - INFO - __main__ -   Batch number = 50
12/25/2021 22:26:00 - INFO - __main__ -   Batch number = 51
12/25/2021 22:26:00 - INFO - __main__ -   Batch number = 52
12/25/2021 22:26:00 - INFO - __main__ -   Batch number = 53
12/25/2021 22:26:00 - INFO - __main__ -   Batch number = 54
12/25/2021 22:26:00 - INFO - __main__ -   Batch number = 55
12/25/2021 22:26:01 - INFO - __main__ -   Batch number = 56
12/25/2021 22:26:02 - INFO - __main__ -   ***** Evaluation result  in ar *****
12/25/2021 22:26:02 - INFO - __main__ -     f1 = 0.5692174965555427
12/25/2021 22:26:02 - INFO - __main__ -     loss = 1.7461669647267886
12/25/2021 22:26:02 - INFO - __main__ -     precision = 0.5567786790266512
12/25/2021 22:26:02 - INFO - __main__ -     recall = 0.5822247978294656
12/25/2021 22:26:04 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_ensemble_oracle_top1/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ar', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_ensemble_oracle_top1//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
12/25/2021 22:26:04 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
12/25/2021 22:26:04 - INFO - __main__ -   Seed = 3
12/25/2021 22:26:04 - INFO - root -   save model
12/25/2021 22:26:04 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_ensemble_oracle_top1/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ar', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_ensemble_oracle_top1//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
12/25/2021 22:26:04 - INFO - __main__ -   Loading pretrained model and tokenizer
12/25/2021 22:26:07 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
12/25/2021 22:26:12 - INFO - __main__ -   Using lang2id = None
12/25/2021 22:26:12 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
12/25/2021 22:26:12 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
12/25/2021 22:26:12 - INFO - root -   Trying to decide if add adapter
12/25/2021 22:26:12 - INFO - root -   loading task adapter
12/25/2021 22:26:12 - INFO - root -   loading lang adpater topk
12/25/2021 22:26:12 - INFO - __main__ -   Loading Adapter Languages from scripts/udpos/en/ar.json
12/25/2021 22:26:12 - INFO - __main__ -   Adapter Languages : ['ka'], Length : 1
12/25/2021 22:26:12 - INFO - __main__ -   Adapter Names ['ka/wiki@ukp'], Length : 1
12/25/2021 22:26:12 - INFO - __main__ -   Language = ka
12/25/2021 22:26:12 - INFO - __main__ -   Adapter Name = ka/wiki@ukp
12/25/2021 22:26:18 - INFO - __main__ -   Args Adapter Weight = equal
12/25/2021 22:26:18 - INFO - __main__ -   Adapter Languages = ['ka']
12/25/2021 22:26:18 - INFO - __main__ -   Adapter Weights = [1.0]
12/25/2021 22:26:18 - INFO - __main__ -   Sum of Adapter Weights = 1.0
12/25/2021 22:26:18 - INFO - __main__ -   Length of Adapter Weights = 1
12/25/2021 22:26:18 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_ar_bert-base-multilingual-cased_128
12/25/2021 22:26:19 - INFO - __main__ -   ***** Running evaluation  in ar *****
12/25/2021 22:26:19 - INFO - __main__ -     Num examples = 1784
12/25/2021 22:26:19 - INFO - __main__ -     Batch size = 32
12/25/2021 22:26:19 - INFO - __main__ -   Batch number = 1
12/25/2021 22:26:19 - INFO - __main__ -   Batch number = 2
12/25/2021 22:26:19 - INFO - __main__ -   Batch number = 3
12/25/2021 22:26:19 - INFO - __main__ -   Batch number = 4
12/25/2021 22:26:19 - INFO - __main__ -   Batch number = 5
12/25/2021 22:26:19 - INFO - __main__ -   Batch number = 6
12/25/2021 22:26:19 - INFO - __main__ -   Batch number = 7
12/25/2021 22:26:20 - INFO - __main__ -   Batch number = 8
12/25/2021 22:26:20 - INFO - __main__ -   Batch number = 9
12/25/2021 22:26:20 - INFO - __main__ -   Batch number = 10
12/25/2021 22:26:20 - INFO - __main__ -   Batch number = 11
12/25/2021 22:26:20 - INFO - __main__ -   Batch number = 12
12/25/2021 22:26:20 - INFO - __main__ -   Batch number = 13
12/25/2021 22:26:20 - INFO - __main__ -   Batch number = 14
12/25/2021 22:26:20 - INFO - __main__ -   Batch number = 15
12/25/2021 22:26:21 - INFO - __main__ -   Batch number = 16
12/25/2021 22:26:21 - INFO - __main__ -   Batch number = 17
12/25/2021 22:26:21 - INFO - __main__ -   Batch number = 18
12/25/2021 22:26:21 - INFO - __main__ -   Batch number = 19
12/25/2021 22:26:21 - INFO - __main__ -   Batch number = 20
12/25/2021 22:26:21 - INFO - __main__ -   Batch number = 21
12/25/2021 22:26:21 - INFO - __main__ -   Batch number = 22
12/25/2021 22:26:21 - INFO - __main__ -   Batch number = 23
12/25/2021 22:26:22 - INFO - __main__ -   Batch number = 24
12/25/2021 22:26:22 - INFO - __main__ -   Batch number = 25
12/25/2021 22:26:22 - INFO - __main__ -   Batch number = 26
12/25/2021 22:26:22 - INFO - __main__ -   Batch number = 27
12/25/2021 22:26:22 - INFO - __main__ -   Batch number = 28
12/25/2021 22:26:22 - INFO - __main__ -   Batch number = 29
12/25/2021 22:26:22 - INFO - __main__ -   Batch number = 30
12/25/2021 22:26:23 - INFO - __main__ -   Batch number = 31
12/25/2021 22:26:23 - INFO - __main__ -   Batch number = 32
12/25/2021 22:26:23 - INFO - __main__ -   Batch number = 33
12/25/2021 22:26:23 - INFO - __main__ -   Batch number = 34
12/25/2021 22:26:23 - INFO - __main__ -   Batch number = 35
12/25/2021 22:26:23 - INFO - __main__ -   Batch number = 36
12/25/2021 22:26:23 - INFO - __main__ -   Batch number = 37
12/25/2021 22:26:24 - INFO - __main__ -   Batch number = 38
12/25/2021 22:26:24 - INFO - __main__ -   Batch number = 39
12/25/2021 22:26:24 - INFO - __main__ -   Batch number = 40
12/25/2021 22:26:24 - INFO - __main__ -   Batch number = 41
12/25/2021 22:26:24 - INFO - __main__ -   Batch number = 42
12/25/2021 22:26:24 - INFO - __main__ -   Batch number = 43
12/25/2021 22:26:24 - INFO - __main__ -   Batch number = 44
12/25/2021 22:26:24 - INFO - __main__ -   Batch number = 45
12/25/2021 22:26:25 - INFO - __main__ -   Batch number = 46
12/25/2021 22:26:25 - INFO - __main__ -   Batch number = 47
12/25/2021 22:26:25 - INFO - __main__ -   Batch number = 48
12/25/2021 22:26:25 - INFO - __main__ -   Batch number = 49
12/25/2021 22:26:25 - INFO - __main__ -   Batch number = 50
12/25/2021 22:26:25 - INFO - __main__ -   Batch number = 51
12/25/2021 22:26:26 - INFO - __main__ -   Batch number = 52
12/25/2021 22:26:26 - INFO - __main__ -   Batch number = 53
12/25/2021 22:26:26 - INFO - __main__ -   Batch number = 54
12/25/2021 22:26:26 - INFO - __main__ -   Batch number = 55
12/25/2021 22:26:26 - INFO - __main__ -   Batch number = 56
12/25/2021 22:26:27 - INFO - __main__ -   ***** Evaluation result  in ar *****
12/25/2021 22:26:27 - INFO - __main__ -     f1 = 0.5837035501962257
12/25/2021 22:26:27 - INFO - __main__ -     loss = 1.4693767045225417
12/25/2021 22:26:27 - INFO - __main__ -     precision = 0.5741769442462542
12/25/2021 22:26:27 - INFO - __main__ -     recall = 0.5935516160472039
12/25/2021 22:26:30 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_ensemble_oracle_top1/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='de', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_ensemble_oracle_top1//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
12/25/2021 22:26:30 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
12/25/2021 22:26:30 - INFO - __main__ -   Seed = 1
12/25/2021 22:26:30 - INFO - root -   save model
12/25/2021 22:26:30 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_ensemble_oracle_top1/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='de', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_ensemble_oracle_top1//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
12/25/2021 22:26:30 - INFO - __main__ -   Loading pretrained model and tokenizer
12/25/2021 22:26:32 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
12/25/2021 22:26:38 - INFO - __main__ -   Using lang2id = None
12/25/2021 22:26:38 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
12/25/2021 22:26:38 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
12/25/2021 22:26:38 - INFO - root -   Trying to decide if add adapter
12/25/2021 22:26:38 - INFO - root -   loading task adapter
12/25/2021 22:26:38 - INFO - root -   loading lang adpater topk
12/25/2021 22:26:38 - INFO - __main__ -   Loading Adapter Languages from scripts/udpos/en/de.json
12/25/2021 22:26:38 - INFO - __main__ -   Adapter Languages : ['id'], Length : 1
12/25/2021 22:26:38 - INFO - __main__ -   Adapter Names ['id/wiki@ukp'], Length : 1
12/25/2021 22:26:38 - INFO - __main__ -   Language = id
12/25/2021 22:26:38 - INFO - __main__ -   Adapter Name = id/wiki@ukp
12/25/2021 22:26:45 - INFO - __main__ -   Args Adapter Weight = equal
12/25/2021 22:26:45 - INFO - __main__ -   Adapter Languages = ['id']
12/25/2021 22:26:45 - INFO - __main__ -   Adapter Weights = [1.0]
12/25/2021 22:26:45 - INFO - __main__ -   Sum of Adapter Weights = 1.0
12/25/2021 22:26:45 - INFO - __main__ -   Length of Adapter Weights = 1
12/25/2021 22:26:45 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea/data//udpos/udpos_processed_maxlen128/cached_test_de_bert-base-multilingual-cased_128
12/25/2021 22:26:49 - INFO - __main__ -   ***** Running evaluation  in de *****
12/25/2021 22:26:49 - INFO - __main__ -     Num examples = 22360
12/25/2021 22:26:49 - INFO - __main__ -     Batch size = 32
12/25/2021 22:26:49 - INFO - __main__ -   Batch number = 1
12/25/2021 22:26:49 - INFO - __main__ -   Batch number = 2
12/25/2021 22:26:49 - INFO - __main__ -   Batch number = 3
12/25/2021 22:26:49 - INFO - __main__ -   Batch number = 4
12/25/2021 22:26:49 - INFO - __main__ -   Batch number = 5
12/25/2021 22:26:49 - INFO - __main__ -   Batch number = 6
12/25/2021 22:26:49 - INFO - __main__ -   Batch number = 7
12/25/2021 22:26:50 - INFO - __main__ -   Batch number = 8
12/25/2021 22:26:50 - INFO - __main__ -   Batch number = 9
12/25/2021 22:26:50 - INFO - __main__ -   Batch number = 10
12/25/2021 22:26:50 - INFO - __main__ -   Batch number = 11
12/25/2021 22:26:50 - INFO - __main__ -   Batch number = 12
12/25/2021 22:26:50 - INFO - __main__ -   Batch number = 13
12/25/2021 22:26:50 - INFO - __main__ -   Batch number = 14
12/25/2021 22:26:51 - INFO - __main__ -   Batch number = 15
12/25/2021 22:26:51 - INFO - __main__ -   Batch number = 16
12/25/2021 22:26:51 - INFO - __main__ -   Batch number = 17
12/25/2021 22:26:51 - INFO - __main__ -   Batch number = 18
12/25/2021 22:26:51 - INFO - __main__ -   Batch number = 19
12/25/2021 22:26:51 - INFO - __main__ -   Batch number = 20
12/25/2021 22:26:51 - INFO - __main__ -   Batch number = 21
12/25/2021 22:26:52 - INFO - __main__ -   Batch number = 22
12/25/2021 22:26:52 - INFO - __main__ -   Batch number = 23
12/25/2021 22:26:52 - INFO - __main__ -   Batch number = 24
12/25/2021 22:26:52 - INFO - __main__ -   Batch number = 25
12/25/2021 22:26:52 - INFO - __main__ -   Batch number = 26
12/25/2021 22:26:52 - INFO - __main__ -   Batch number = 27
12/25/2021 22:26:53 - INFO - __main__ -   Batch number = 28
12/25/2021 22:26:53 - INFO - __main__ -   Batch number = 29
12/25/2021 22:26:53 - INFO - __main__ -   Batch number = 30
12/25/2021 22:26:53 - INFO - __main__ -   Batch number = 31
12/25/2021 22:26:53 - INFO - __main__ -   Batch number = 32
12/25/2021 22:26:53 - INFO - __main__ -   Batch number = 33
12/25/2021 22:26:53 - INFO - __main__ -   Batch number = 34
12/25/2021 22:26:54 - INFO - __main__ -   Batch number = 35
12/25/2021 22:26:54 - INFO - __main__ -   Batch number = 36
12/25/2021 22:26:54 - INFO - __main__ -   Batch number = 37
12/25/2021 22:26:54 - INFO - __main__ -   Batch number = 38
12/25/2021 22:26:54 - INFO - __main__ -   Batch number = 39
12/25/2021 22:26:54 - INFO - __main__ -   Batch number = 40
12/25/2021 22:26:54 - INFO - __main__ -   Batch number = 41
12/25/2021 22:26:55 - INFO - __main__ -   Batch number = 42
12/25/2021 22:26:55 - INFO - __main__ -   Batch number = 43
12/25/2021 22:26:55 - INFO - __main__ -   Batch number = 44
12/25/2021 22:26:55 - INFO - __main__ -   Batch number = 45
12/25/2021 22:26:55 - INFO - __main__ -   Batch number = 46
12/25/2021 22:26:55 - INFO - __main__ -   Batch number = 47
12/25/2021 22:26:55 - INFO - __main__ -   Batch number = 48
12/25/2021 22:26:56 - INFO - __main__ -   Batch number = 49
12/25/2021 22:26:56 - INFO - __main__ -   Batch number = 50
12/25/2021 22:26:56 - INFO - __main__ -   Batch number = 51
12/25/2021 22:26:56 - INFO - __main__ -   Batch number = 52
12/25/2021 22:26:56 - INFO - __main__ -   Batch number = 53
12/25/2021 22:26:56 - INFO - __main__ -   Batch number = 54
12/25/2021 22:26:56 - INFO - __main__ -   Batch number = 55
12/25/2021 22:26:57 - INFO - __main__ -   Batch number = 56
12/25/2021 22:26:57 - INFO - __main__ -   Batch number = 57
12/25/2021 22:26:57 - INFO - __main__ -   Batch number = 58
12/25/2021 22:26:57 - INFO - __main__ -   Batch number = 59
12/25/2021 22:26:57 - INFO - __main__ -   Batch number = 60
12/25/2021 22:26:57 - INFO - __main__ -   Batch number = 61
12/25/2021 22:26:57 - INFO - __main__ -   Batch number = 62
12/25/2021 22:26:58 - INFO - __main__ -   Batch number = 63
12/25/2021 22:26:58 - INFO - __main__ -   Batch number = 64
12/25/2021 22:26:58 - INFO - __main__ -   Batch number = 65
12/25/2021 22:26:58 - INFO - __main__ -   Batch number = 66
12/25/2021 22:26:58 - INFO - __main__ -   Batch number = 67
12/25/2021 22:26:58 - INFO - __main__ -   Batch number = 68
12/25/2021 22:26:59 - INFO - __main__ -   Batch number = 69
12/25/2021 22:26:59 - INFO - __main__ -   Batch number = 70
12/25/2021 22:26:59 - INFO - __main__ -   Batch number = 71
12/25/2021 22:26:59 - INFO - __main__ -   Batch number = 72
12/25/2021 22:26:59 - INFO - __main__ -   Batch number = 73
12/25/2021 22:26:59 - INFO - __main__ -   Batch number = 74
12/25/2021 22:26:59 - INFO - __main__ -   Batch number = 75
12/25/2021 22:27:00 - INFO - __main__ -   Batch number = 76
12/25/2021 22:27:00 - INFO - __main__ -   Batch number = 77
12/25/2021 22:27:00 - INFO - __main__ -   Batch number = 78
12/25/2021 22:27:00 - INFO - __main__ -   Batch number = 79
12/25/2021 22:27:00 - INFO - __main__ -   Batch number = 80
12/25/2021 22:27:00 - INFO - __main__ -   Batch number = 81
12/25/2021 22:27:00 - INFO - __main__ -   Batch number = 82
12/25/2021 22:27:01 - INFO - __main__ -   Batch number = 83
12/25/2021 22:27:01 - INFO - __main__ -   Batch number = 84
12/25/2021 22:27:01 - INFO - __main__ -   Batch number = 85
12/25/2021 22:27:01 - INFO - __main__ -   Batch number = 86
12/25/2021 22:27:01 - INFO - __main__ -   Batch number = 87
12/25/2021 22:27:01 - INFO - __main__ -   Batch number = 88
12/25/2021 22:27:02 - INFO - __main__ -   Batch number = 89
12/25/2021 22:27:02 - INFO - __main__ -   Batch number = 90
12/25/2021 22:27:02 - INFO - __main__ -   Batch number = 91
12/25/2021 22:27:03 - INFO - __main__ -   Batch number = 92
12/25/2021 22:27:03 - INFO - __main__ -   Batch number = 93
12/25/2021 22:27:03 - INFO - __main__ -   Batch number = 94
12/25/2021 22:27:03 - INFO - __main__ -   Batch number = 95
12/25/2021 22:27:03 - INFO - __main__ -   Batch number = 96
12/25/2021 22:27:04 - INFO - __main__ -   Batch number = 97
12/25/2021 22:27:04 - INFO - __main__ -   Batch number = 98
12/25/2021 22:27:04 - INFO - __main__ -   Batch number = 99
12/25/2021 22:27:04 - INFO - __main__ -   Batch number = 100
12/25/2021 22:27:04 - INFO - __main__ -   Batch number = 101
12/25/2021 22:27:05 - INFO - __main__ -   Batch number = 102
12/25/2021 22:27:05 - INFO - __main__ -   Batch number = 103
12/25/2021 22:27:05 - INFO - __main__ -   Batch number = 104
12/25/2021 22:27:05 - INFO - __main__ -   Batch number = 105
12/25/2021 22:27:05 - INFO - __main__ -   Batch number = 106
12/25/2021 22:27:06 - INFO - __main__ -   Batch number = 107
12/25/2021 22:27:06 - INFO - __main__ -   Batch number = 108
12/25/2021 22:27:06 - INFO - __main__ -   Batch number = 109
12/25/2021 22:27:06 - INFO - __main__ -   Batch number = 110
12/25/2021 22:27:07 - INFO - __main__ -   Batch number = 111
12/25/2021 22:27:07 - INFO - __main__ -   Batch number = 112
12/25/2021 22:27:07 - INFO - __main__ -   Batch number = 113
12/25/2021 22:27:07 - INFO - __main__ -   Batch number = 114
12/25/2021 22:27:07 - INFO - __main__ -   Batch number = 115
12/25/2021 22:27:08 - INFO - __main__ -   Batch number = 116
12/25/2021 22:27:08 - INFO - __main__ -   Batch number = 117
12/25/2021 22:27:08 - INFO - __main__ -   Batch number = 118
12/25/2021 22:27:08 - INFO - __main__ -   Batch number = 119
12/25/2021 22:27:08 - INFO - __main__ -   Batch number = 120
12/25/2021 22:27:09 - INFO - __main__ -   Batch number = 121
12/25/2021 22:27:09 - INFO - __main__ -   Batch number = 122
12/25/2021 22:27:09 - INFO - __main__ -   Batch number = 123
12/25/2021 22:27:09 - INFO - __main__ -   Batch number = 124
12/25/2021 22:27:09 - INFO - __main__ -   Batch number = 125
12/25/2021 22:27:10 - INFO - __main__ -   Batch number = 126
12/25/2021 22:27:10 - INFO - __main__ -   Batch number = 127
12/25/2021 22:27:10 - INFO - __main__ -   Batch number = 128
12/25/2021 22:27:10 - INFO - __main__ -   Batch number = 129
12/25/2021 22:27:11 - INFO - __main__ -   Batch number = 130
12/25/2021 22:27:11 - INFO - __main__ -   Batch number = 131
12/25/2021 22:27:11 - INFO - __main__ -   Batch number = 132
12/25/2021 22:27:11 - INFO - __main__ -   Batch number = 133
12/25/2021 22:27:11 - INFO - __main__ -   Batch number = 134
12/25/2021 22:27:12 - INFO - __main__ -   Batch number = 135
12/25/2021 22:27:12 - INFO - __main__ -   Batch number = 136
12/25/2021 22:27:12 - INFO - __main__ -   Batch number = 137
12/25/2021 22:27:12 - INFO - __main__ -   Batch number = 138
12/25/2021 22:27:13 - INFO - __main__ -   Batch number = 139
12/25/2021 22:27:13 - INFO - __main__ -   Batch number = 140
12/25/2021 22:27:13 - INFO - __main__ -   Batch number = 141
12/25/2021 22:27:13 - INFO - __main__ -   Batch number = 142
12/25/2021 22:27:14 - INFO - __main__ -   Batch number = 143
12/25/2021 22:27:14 - INFO - __main__ -   Batch number = 144
12/25/2021 22:27:14 - INFO - __main__ -   Batch number = 145
12/25/2021 22:27:14 - INFO - __main__ -   Batch number = 146
12/25/2021 22:27:14 - INFO - __main__ -   Batch number = 147
12/25/2021 22:27:15 - INFO - __main__ -   Batch number = 148
12/25/2021 22:27:15 - INFO - __main__ -   Batch number = 149
12/25/2021 22:27:15 - INFO - __main__ -   Batch number = 150
12/25/2021 22:27:15 - INFO - __main__ -   Batch number = 151
12/25/2021 22:27:16 - INFO - __main__ -   Batch number = 152
12/25/2021 22:27:16 - INFO - __main__ -   Batch number = 153
12/25/2021 22:27:16 - INFO - __main__ -   Batch number = 154
12/25/2021 22:27:16 - INFO - __main__ -   Batch number = 155
12/25/2021 22:27:17 - INFO - __main__ -   Batch number = 156
12/25/2021 22:27:17 - INFO - __main__ -   Batch number = 157
12/25/2021 22:27:17 - INFO - __main__ -   Batch number = 158
12/25/2021 22:27:17 - INFO - __main__ -   Batch number = 159
12/25/2021 22:27:18 - INFO - __main__ -   Batch number = 160
12/25/2021 22:27:18 - INFO - __main__ -   Batch number = 161
12/25/2021 22:27:18 - INFO - __main__ -   Batch number = 162
12/25/2021 22:27:18 - INFO - __main__ -   Batch number = 163
12/25/2021 22:27:19 - INFO - __main__ -   Batch number = 164
12/25/2021 22:27:19 - INFO - __main__ -   Batch number = 165
12/25/2021 22:27:19 - INFO - __main__ -   Batch number = 166
12/25/2021 22:27:19 - INFO - __main__ -   Batch number = 167
12/25/2021 22:27:20 - INFO - __main__ -   Batch number = 168
12/25/2021 22:27:20 - INFO - __main__ -   Batch number = 169
12/25/2021 22:27:20 - INFO - __main__ -   Batch number = 170
12/25/2021 22:27:20 - INFO - __main__ -   Batch number = 171
12/25/2021 22:27:21 - INFO - __main__ -   Batch number = 172
12/25/2021 22:27:21 - INFO - __main__ -   Batch number = 173
12/25/2021 22:27:21 - INFO - __main__ -   Batch number = 174
12/25/2021 22:27:21 - INFO - __main__ -   Batch number = 175
12/25/2021 22:27:22 - INFO - __main__ -   Batch number = 176
12/25/2021 22:27:22 - INFO - __main__ -   Batch number = 177
12/25/2021 22:27:22 - INFO - __main__ -   Batch number = 178
12/25/2021 22:27:22 - INFO - __main__ -   Batch number = 179
12/25/2021 22:27:23 - INFO - __main__ -   Batch number = 180
12/25/2021 22:27:23 - INFO - __main__ -   Batch number = 181
12/25/2021 22:27:23 - INFO - __main__ -   Batch number = 182
12/25/2021 22:27:24 - INFO - __main__ -   Batch number = 183
12/25/2021 22:27:24 - INFO - __main__ -   Batch number = 184
12/25/2021 22:27:24 - INFO - __main__ -   Batch number = 185
12/25/2021 22:27:24 - INFO - __main__ -   Batch number = 186
12/25/2021 22:27:25 - INFO - __main__ -   Batch number = 187
12/25/2021 22:27:25 - INFO - __main__ -   Batch number = 188
12/25/2021 22:27:25 - INFO - __main__ -   Batch number = 189
12/25/2021 22:27:26 - INFO - __main__ -   Batch number = 190
12/25/2021 22:27:26 - INFO - __main__ -   Batch number = 191
12/25/2021 22:27:26 - INFO - __main__ -   Batch number = 192
12/25/2021 22:27:26 - INFO - __main__ -   Batch number = 193
12/25/2021 22:27:27 - INFO - __main__ -   Batch number = 194
12/25/2021 22:27:27 - INFO - __main__ -   Batch number = 195
12/25/2021 22:27:27 - INFO - __main__ -   Batch number = 196
12/25/2021 22:27:27 - INFO - __main__ -   Batch number = 197
12/25/2021 22:27:28 - INFO - __main__ -   Batch number = 198
12/25/2021 22:27:28 - INFO - __main__ -   Batch number = 199
12/25/2021 22:27:28 - INFO - __main__ -   Batch number = 200
12/25/2021 22:27:29 - INFO - __main__ -   Batch number = 201
12/25/2021 22:27:29 - INFO - __main__ -   Batch number = 202
12/25/2021 22:27:29 - INFO - __main__ -   Batch number = 203
12/25/2021 22:27:29 - INFO - __main__ -   Batch number = 204
12/25/2021 22:27:30 - INFO - __main__ -   Batch number = 205
12/25/2021 22:27:30 - INFO - __main__ -   Batch number = 206
12/25/2021 22:27:30 - INFO - __main__ -   Batch number = 207
12/25/2021 22:27:31 - INFO - __main__ -   Batch number = 208
12/25/2021 22:27:31 - INFO - __main__ -   Batch number = 209
12/25/2021 22:27:31 - INFO - __main__ -   Batch number = 210
12/25/2021 22:27:32 - INFO - __main__ -   Batch number = 211
12/25/2021 22:27:32 - INFO - __main__ -   Batch number = 212
12/25/2021 22:27:32 - INFO - __main__ -   Batch number = 213
12/25/2021 22:27:32 - INFO - __main__ -   Batch number = 214
12/25/2021 22:27:33 - INFO - __main__ -   Batch number = 215
12/25/2021 22:27:33 - INFO - __main__ -   Batch number = 216
12/25/2021 22:27:33 - INFO - __main__ -   Batch number = 217
12/25/2021 22:27:34 - INFO - __main__ -   Batch number = 218
12/25/2021 22:27:34 - INFO - __main__ -   Batch number = 219
12/25/2021 22:27:34 - INFO - __main__ -   Batch number = 220
12/25/2021 22:27:35 - INFO - __main__ -   Batch number = 221
12/25/2021 22:27:35 - INFO - __main__ -   Batch number = 222
12/25/2021 22:27:35 - INFO - __main__ -   Batch number = 223
12/25/2021 22:27:35 - INFO - __main__ -   Batch number = 224
12/25/2021 22:27:36 - INFO - __main__ -   Batch number = 225
12/25/2021 22:27:36 - INFO - __main__ -   Batch number = 226
12/25/2021 22:27:36 - INFO - __main__ -   Batch number = 227
12/25/2021 22:27:37 - INFO - __main__ -   Batch number = 228
12/25/2021 22:27:37 - INFO - __main__ -   Batch number = 229
12/25/2021 22:27:37 - INFO - __main__ -   Batch number = 230
12/25/2021 22:27:38 - INFO - __main__ -   Batch number = 231
12/25/2021 22:27:38 - INFO - __main__ -   Batch number = 232
12/25/2021 22:27:38 - INFO - __main__ -   Batch number = 233
12/25/2021 22:27:39 - INFO - __main__ -   Batch number = 234
12/25/2021 22:27:39 - INFO - __main__ -   Batch number = 235
12/25/2021 22:27:39 - INFO - __main__ -   Batch number = 236
12/25/2021 22:27:40 - INFO - __main__ -   Batch number = 237
12/25/2021 22:27:40 - INFO - __main__ -   Batch number = 238
12/25/2021 22:27:40 - INFO - __main__ -   Batch number = 239
12/25/2021 22:27:40 - INFO - __main__ -   Batch number = 240
12/25/2021 22:27:41 - INFO - __main__ -   Batch number = 241
12/25/2021 22:27:41 - INFO - __main__ -   Batch number = 242
12/25/2021 22:27:41 - INFO - __main__ -   Batch number = 243
12/25/2021 22:27:42 - INFO - __main__ -   Batch number = 244
12/25/2021 22:27:42 - INFO - __main__ -   Batch number = 245
12/25/2021 22:27:42 - INFO - __main__ -   Batch number = 246
12/25/2021 22:27:43 - INFO - __main__ -   Batch number = 247
12/25/2021 22:27:43 - INFO - __main__ -   Batch number = 248
12/25/2021 22:27:43 - INFO - __main__ -   Batch number = 249
12/25/2021 22:27:44 - INFO - __main__ -   Batch number = 250
12/25/2021 22:27:44 - INFO - __main__ -   Batch number = 251
12/25/2021 22:27:44 - INFO - __main__ -   Batch number = 252
12/25/2021 22:27:45 - INFO - __main__ -   Batch number = 253
12/25/2021 22:27:45 - INFO - __main__ -   Batch number = 254
12/25/2021 22:27:45 - INFO - __main__ -   Batch number = 255
12/25/2021 22:27:46 - INFO - __main__ -   Batch number = 256
12/25/2021 22:27:46 - INFO - __main__ -   Batch number = 257
12/25/2021 22:27:46 - INFO - __main__ -   Batch number = 258
12/25/2021 22:27:47 - INFO - __main__ -   Batch number = 259
12/25/2021 22:27:47 - INFO - __main__ -   Batch number = 260
12/25/2021 22:27:47 - INFO - __main__ -   Batch number = 261
12/25/2021 22:27:48 - INFO - __main__ -   Batch number = 262
12/25/2021 22:27:48 - INFO - __main__ -   Batch number = 263
12/25/2021 22:27:48 - INFO - __main__ -   Batch number = 264
12/25/2021 22:27:49 - INFO - __main__ -   Batch number = 265
12/25/2021 22:27:49 - INFO - __main__ -   Batch number = 266
12/25/2021 22:27:49 - INFO - __main__ -   Batch number = 267
12/25/2021 22:27:50 - INFO - __main__ -   Batch number = 268
12/25/2021 22:27:50 - INFO - __main__ -   Batch number = 269
12/25/2021 22:27:50 - INFO - __main__ -   Batch number = 270
12/25/2021 22:27:51 - INFO - __main__ -   Batch number = 271
12/25/2021 22:27:51 - INFO - __main__ -   Batch number = 272
12/25/2021 22:27:51 - INFO - __main__ -   Batch number = 273
12/25/2021 22:27:52 - INFO - __main__ -   Batch number = 274
12/25/2021 22:27:52 - INFO - __main__ -   Batch number = 275
12/25/2021 22:27:52 - INFO - __main__ -   Batch number = 276
12/25/2021 22:27:53 - INFO - __main__ -   Batch number = 277
12/25/2021 22:27:53 - INFO - __main__ -   Batch number = 278
12/25/2021 22:27:53 - INFO - __main__ -   Batch number = 279
12/25/2021 22:27:54 - INFO - __main__ -   Batch number = 280
12/25/2021 22:27:54 - INFO - __main__ -   Batch number = 281
12/25/2021 22:27:54 - INFO - __main__ -   Batch number = 282
12/25/2021 22:27:55 - INFO - __main__ -   Batch number = 283
12/25/2021 22:27:55 - INFO - __main__ -   Batch number = 284
12/25/2021 22:27:56 - INFO - __main__ -   Batch number = 285
12/25/2021 22:27:56 - INFO - __main__ -   Batch number = 286
12/25/2021 22:27:56 - INFO - __main__ -   Batch number = 287
12/25/2021 22:27:57 - INFO - __main__ -   Batch number = 288
12/25/2021 22:27:57 - INFO - __main__ -   Batch number = 289
12/25/2021 22:27:57 - INFO - __main__ -   Batch number = 290
12/25/2021 22:27:58 - INFO - __main__ -   Batch number = 291
12/25/2021 22:27:58 - INFO - __main__ -   Batch number = 292
12/25/2021 22:27:58 - INFO - __main__ -   Batch number = 293
12/25/2021 22:27:59 - INFO - __main__ -   Batch number = 294
12/25/2021 22:27:59 - INFO - __main__ -   Batch number = 295
12/25/2021 22:28:00 - INFO - __main__ -   Batch number = 296
12/25/2021 22:28:00 - INFO - __main__ -   Batch number = 297
12/25/2021 22:28:00 - INFO - __main__ -   Batch number = 298
12/25/2021 22:28:01 - INFO - __main__ -   Batch number = 299
12/25/2021 22:28:01 - INFO - __main__ -   Batch number = 300
12/25/2021 22:28:01 - INFO - __main__ -   Batch number = 301
12/25/2021 22:28:02 - INFO - __main__ -   Batch number = 302
12/25/2021 22:28:02 - INFO - __main__ -   Batch number = 303
12/25/2021 22:28:02 - INFO - __main__ -   Batch number = 304
12/25/2021 22:28:03 - INFO - __main__ -   Batch number = 305
12/25/2021 22:28:03 - INFO - __main__ -   Batch number = 306
12/25/2021 22:28:04 - INFO - __main__ -   Batch number = 307
12/25/2021 22:28:04 - INFO - __main__ -   Batch number = 308
12/25/2021 22:28:04 - INFO - __main__ -   Batch number = 309
12/25/2021 22:28:05 - INFO - __main__ -   Batch number = 310
12/25/2021 22:28:05 - INFO - __main__ -   Batch number = 311
12/25/2021 22:28:05 - INFO - __main__ -   Batch number = 312
12/25/2021 22:28:06 - INFO - __main__ -   Batch number = 313
12/25/2021 22:28:06 - INFO - __main__ -   Batch number = 314
12/25/2021 22:28:06 - INFO - __main__ -   Batch number = 315
12/25/2021 22:28:07 - INFO - __main__ -   Batch number = 316
12/25/2021 22:28:07 - INFO - __main__ -   Batch number = 317
12/25/2021 22:28:08 - INFO - __main__ -   Batch number = 318
12/25/2021 22:28:08 - INFO - __main__ -   Batch number = 319
12/25/2021 22:28:08 - INFO - __main__ -   Batch number = 320
12/25/2021 22:28:09 - INFO - __main__ -   Batch number = 321
12/25/2021 22:28:09 - INFO - __main__ -   Batch number = 322
12/25/2021 22:28:10 - INFO - __main__ -   Batch number = 323
12/25/2021 22:28:10 - INFO - __main__ -   Batch number = 324
12/25/2021 22:28:10 - INFO - __main__ -   Batch number = 325
12/25/2021 22:28:11 - INFO - __main__ -   Batch number = 326
12/25/2021 22:28:11 - INFO - __main__ -   Batch number = 327
12/25/2021 22:28:11 - INFO - __main__ -   Batch number = 328
12/25/2021 22:28:12 - INFO - __main__ -   Batch number = 329
12/25/2021 22:28:12 - INFO - __main__ -   Batch number = 330
12/25/2021 22:28:13 - INFO - __main__ -   Batch number = 331
12/25/2021 22:28:13 - INFO - __main__ -   Batch number = 332
12/25/2021 22:28:13 - INFO - __main__ -   Batch number = 333
12/25/2021 22:28:14 - INFO - __main__ -   Batch number = 334
12/25/2021 22:28:14 - INFO - __main__ -   Batch number = 335
12/25/2021 22:28:14 - INFO - __main__ -   Batch number = 336
12/25/2021 22:28:15 - INFO - __main__ -   Batch number = 337
12/25/2021 22:28:15 - INFO - __main__ -   Batch number = 338
12/25/2021 22:28:16 - INFO - __main__ -   Batch number = 339
12/25/2021 22:28:16 - INFO - __main__ -   Batch number = 340
12/25/2021 22:28:17 - INFO - __main__ -   Batch number = 341
12/25/2021 22:28:17 - INFO - __main__ -   Batch number = 342
12/25/2021 22:28:17 - INFO - __main__ -   Batch number = 343
12/25/2021 22:28:18 - INFO - __main__ -   Batch number = 344
12/25/2021 22:28:18 - INFO - __main__ -   Batch number = 345
12/25/2021 22:28:18 - INFO - __main__ -   Batch number = 346
12/25/2021 22:28:19 - INFO - __main__ -   Batch number = 347
12/25/2021 22:28:19 - INFO - __main__ -   Batch number = 348
12/25/2021 22:28:20 - INFO - __main__ -   Batch number = 349
12/25/2021 22:28:20 - INFO - __main__ -   Batch number = 350
12/25/2021 22:28:20 - INFO - __main__ -   Batch number = 351
12/25/2021 22:28:21 - INFO - __main__ -   Batch number = 352
12/25/2021 22:28:21 - INFO - __main__ -   Batch number = 353
12/25/2021 22:28:22 - INFO - __main__ -   Batch number = 354
12/25/2021 22:28:22 - INFO - __main__ -   Batch number = 355
12/25/2021 22:28:23 - INFO - __main__ -   Batch number = 356
12/25/2021 22:28:23 - INFO - __main__ -   Batch number = 357
12/25/2021 22:28:23 - INFO - __main__ -   Batch number = 358
12/25/2021 22:28:24 - INFO - __main__ -   Batch number = 359
12/25/2021 22:28:24 - INFO - __main__ -   Batch number = 360
12/25/2021 22:28:25 - INFO - __main__ -   Batch number = 361
12/25/2021 22:28:25 - INFO - __main__ -   Batch number = 362
12/25/2021 22:28:26 - INFO - __main__ -   Batch number = 363
12/25/2021 22:28:26 - INFO - __main__ -   Batch number = 364
12/25/2021 22:28:26 - INFO - __main__ -   Batch number = 365
12/25/2021 22:28:27 - INFO - __main__ -   Batch number = 366
12/25/2021 22:28:27 - INFO - __main__ -   Batch number = 367
12/25/2021 22:28:28 - INFO - __main__ -   Batch number = 368
12/25/2021 22:28:28 - INFO - __main__ -   Batch number = 369
12/25/2021 22:28:29 - INFO - __main__ -   Batch number = 370
12/25/2021 22:28:29 - INFO - __main__ -   Batch number = 371
12/25/2021 22:28:29 - INFO - __main__ -   Batch number = 372
12/25/2021 22:28:30 - INFO - __main__ -   Batch number = 373
12/25/2021 22:28:30 - INFO - __main__ -   Batch number = 374
12/25/2021 22:28:31 - INFO - __main__ -   Batch number = 375
12/25/2021 22:28:31 - INFO - __main__ -   Batch number = 376
12/25/2021 22:28:31 - INFO - __main__ -   Batch number = 377
12/25/2021 22:28:32 - INFO - __main__ -   Batch number = 378
12/25/2021 22:28:32 - INFO - __main__ -   Batch number = 379
12/25/2021 22:28:33 - INFO - __main__ -   Batch number = 380
12/25/2021 22:28:33 - INFO - __main__ -   Batch number = 381
12/25/2021 22:28:34 - INFO - __main__ -   Batch number = 382
12/25/2021 22:28:34 - INFO - __main__ -   Batch number = 383
12/25/2021 22:28:34 - INFO - __main__ -   Batch number = 384
12/25/2021 22:28:35 - INFO - __main__ -   Batch number = 385
12/25/2021 22:28:35 - INFO - __main__ -   Batch number = 386
12/25/2021 22:28:36 - INFO - __main__ -   Batch number = 387
12/25/2021 22:28:36 - INFO - __main__ -   Batch number = 388
12/25/2021 22:28:37 - INFO - __main__ -   Batch number = 389
12/25/2021 22:28:37 - INFO - __main__ -   Batch number = 390
12/25/2021 22:28:38 - INFO - __main__ -   Batch number = 391
12/25/2021 22:28:38 - INFO - __main__ -   Batch number = 392
12/25/2021 22:28:38 - INFO - __main__ -   Batch number = 393
12/25/2021 22:28:39 - INFO - __main__ -   Batch number = 394
12/25/2021 22:28:39 - INFO - __main__ -   Batch number = 395
12/25/2021 22:28:40 - INFO - __main__ -   Batch number = 396
12/25/2021 22:28:40 - INFO - __main__ -   Batch number = 397
12/25/2021 22:28:41 - INFO - __main__ -   Batch number = 398
12/25/2021 22:28:41 - INFO - __main__ -   Batch number = 399
12/25/2021 22:28:42 - INFO - __main__ -   Batch number = 400
12/25/2021 22:28:42 - INFO - __main__ -   Batch number = 401
12/25/2021 22:28:43 - INFO - __main__ -   Batch number = 402
12/25/2021 22:28:43 - INFO - __main__ -   Batch number = 403
12/25/2021 22:28:43 - INFO - __main__ -   Batch number = 404
12/25/2021 22:28:44 - INFO - __main__ -   Batch number = 405
12/25/2021 22:28:44 - INFO - __main__ -   Batch number = 406
12/25/2021 22:28:45 - INFO - __main__ -   Batch number = 407
12/25/2021 22:28:45 - INFO - __main__ -   Batch number = 408
12/25/2021 22:28:46 - INFO - __main__ -   Batch number = 409
12/25/2021 22:28:46 - INFO - __main__ -   Batch number = 410
12/25/2021 22:28:47 - INFO - __main__ -   Batch number = 411
12/25/2021 22:28:47 - INFO - __main__ -   Batch number = 412
12/25/2021 22:28:48 - INFO - __main__ -   Batch number = 413
12/25/2021 22:28:48 - INFO - __main__ -   Batch number = 414
12/25/2021 22:28:48 - INFO - __main__ -   Batch number = 415
12/25/2021 22:28:49 - INFO - __main__ -   Batch number = 416
12/25/2021 22:28:49 - INFO - __main__ -   Batch number = 417
12/25/2021 22:28:50 - INFO - __main__ -   Batch number = 418
12/25/2021 22:28:50 - INFO - __main__ -   Batch number = 419
12/25/2021 22:28:51 - INFO - __main__ -   Batch number = 420
12/25/2021 22:28:51 - INFO - __main__ -   Batch number = 421
12/25/2021 22:28:52 - INFO - __main__ -   Batch number = 422
12/25/2021 22:28:52 - INFO - __main__ -   Batch number = 423
12/25/2021 22:28:53 - INFO - __main__ -   Batch number = 424
12/25/2021 22:28:53 - INFO - __main__ -   Batch number = 425
12/25/2021 22:28:54 - INFO - __main__ -   Batch number = 426
12/25/2021 22:28:54 - INFO - __main__ -   Batch number = 427
12/25/2021 22:28:55 - INFO - __main__ -   Batch number = 428
12/25/2021 22:28:55 - INFO - __main__ -   Batch number = 429
12/25/2021 22:28:55 - INFO - __main__ -   Batch number = 430
12/25/2021 22:28:56 - INFO - __main__ -   Batch number = 431
12/25/2021 22:28:56 - INFO - __main__ -   Batch number = 432
12/25/2021 22:28:57 - INFO - __main__ -   Batch number = 433
12/25/2021 22:28:57 - INFO - __main__ -   Batch number = 434
12/25/2021 22:28:58 - INFO - __main__ -   Batch number = 435
12/25/2021 22:28:58 - INFO - __main__ -   Batch number = 436
12/25/2021 22:28:59 - INFO - __main__ -   Batch number = 437
12/25/2021 22:28:59 - INFO - __main__ -   Batch number = 438
12/25/2021 22:29:00 - INFO - __main__ -   Batch number = 439
12/25/2021 22:29:00 - INFO - __main__ -   Batch number = 440
12/25/2021 22:29:01 - INFO - __main__ -   Batch number = 441
12/25/2021 22:29:01 - INFO - __main__ -   Batch number = 442
12/25/2021 22:29:02 - INFO - __main__ -   Batch number = 443
12/25/2021 22:29:02 - INFO - __main__ -   Batch number = 444
12/25/2021 22:29:03 - INFO - __main__ -   Batch number = 445
