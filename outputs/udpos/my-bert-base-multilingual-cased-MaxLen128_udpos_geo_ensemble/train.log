01/16/2022 00:21:49 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mr', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/16/2022 00:21:49 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/16/2022 00:21:49 - INFO - __main__ -   Seed = 1
01/16/2022 00:21:49 - INFO - root -   save model
01/16/2022 00:21:49 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mr', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/16/2022 00:21:49 - INFO - __main__ -   Loading pretrained model and tokenizer
01/16/2022 00:21:51 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/16/2022 00:21:57 - INFO - __main__ -   Using lang2id = None
01/16/2022 00:21:57 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/16/2022 00:21:57 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
01/16/2022 00:21:57 - INFO - root -   Trying to decide if add adapter
01/16/2022 00:21:57 - INFO - root -   loading task adapter
01/16/2022 00:21:57 - INFO - root -   loading lang adpater hi/wiki@ukp,my/wiki@ukp,fa/wiki@ukp,tk/wiki@ukp,vi/wiki@ukp,zh/wiki@ukp,ka/wiki@ukp,zh_yue/wiki@ukp,hy/wiki@ukp,ru/wiki@ukp
01/16/2022 00:21:57 - INFO - __main__ -   Adapter Languages : ['hi', 'my', 'fa', 'tk', 'vi', 'zh', 'ka', 'zh_yue', 'hy', 'ru'], Length : 10
01/16/2022 00:21:57 - INFO - __main__ -   Adapter Names ['hi/wiki@ukp', 'my/wiki@ukp', 'fa/wiki@ukp', 'tk/wiki@ukp', 'vi/wiki@ukp', 'zh/wiki@ukp', 'ka/wiki@ukp', 'zh_yue/wiki@ukp', 'hy/wiki@ukp', 'ru/wiki@ukp'], Length : 10
01/16/2022 00:21:57 - INFO - __main__ -   Language = hi
01/16/2022 00:21:57 - INFO - __main__ -   Adapter Name = hi/wiki@ukp
01/16/2022 00:21:59 - INFO - __main__ -   Language = my
01/16/2022 00:21:59 - INFO - __main__ -   Adapter Name = my/wiki@ukp
01/16/2022 00:22:01 - INFO - __main__ -   Language = fa
01/16/2022 00:22:01 - INFO - __main__ -   Adapter Name = fa/wiki@ukp
01/16/2022 00:22:04 - INFO - __main__ -   Language = tk
01/16/2022 00:22:04 - INFO - __main__ -   Adapter Name = tk/wiki@ukp
01/16/2022 00:22:05 - INFO - __main__ -   Language = vi
01/16/2022 00:22:05 - INFO - __main__ -   Adapter Name = vi/wiki@ukp
01/16/2022 00:22:08 - INFO - __main__ -   Language = zh
01/16/2022 00:22:08 - INFO - __main__ -   Adapter Name = zh/wiki@ukp
01/16/2022 00:22:09 - INFO - __main__ -   Language = ka
01/16/2022 00:22:09 - INFO - __main__ -   Adapter Name = ka/wiki@ukp
01/16/2022 00:22:12 - INFO - __main__ -   Language = zh_yue
01/16/2022 00:22:12 - INFO - __main__ -   Adapter Name = zh_yue/wiki@ukp
01/16/2022 00:22:14 - INFO - __main__ -   Language = hy
01/16/2022 00:22:14 - INFO - __main__ -   Adapter Name = hy/wiki@ukp
01/16/2022 00:22:17 - INFO - __main__ -   Language = ru
01/16/2022 00:22:17 - INFO - __main__ -   Adapter Name = ru/wiki@ukp
01/16/2022 00:22:22 - INFO - __main__ -   Args Adapter Weight = equal
01/16/2022 00:22:22 - INFO - __main__ -   Adapter Languages = ['hi', 'my', 'fa', 'tk', 'vi', 'zh', 'ka', 'zh_yue', 'hy', 'ru']
01/16/2022 00:22:22 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/16/2022 00:22:22 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/16/2022 00:22:22 - INFO - __main__ -   Length of Adapter Weights = 10
01/16/2022 00:22:22 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/cached_test_mr_bert-base-multilingual-cased_128
01/16/2022 00:22:22 - INFO - __main__ -   ***** Running evaluation  in mr *****
01/16/2022 00:22:22 - INFO - __main__ -     Num examples = 47
01/16/2022 00:22:22 - INFO - __main__ -     Batch size = 32
01/16/2022 00:22:22 - INFO - __main__ -   Batch number = 1
01/16/2022 00:22:22 - INFO - __main__ -   Batch number = 2
01/16/2022 00:22:22 - INFO - __main__ -   ***** Evaluation result  in mr *****
01/16/2022 00:22:22 - INFO - __main__ -     f1 = 0.6574074074074076
01/16/2022 00:22:22 - INFO - __main__ -     loss = 0.9859829246997833
01/16/2022 00:22:22 - INFO - __main__ -     precision = 0.6761904761904762
01/16/2022 00:22:22 - INFO - __main__ -     recall = 0.6396396396396397
01/16/2022 00:22:24 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mr', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/16/2022 00:22:24 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/16/2022 00:22:24 - INFO - __main__ -   Seed = 2
01/16/2022 00:22:24 - INFO - root -   save model
01/16/2022 00:22:24 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mr', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/16/2022 00:22:24 - INFO - __main__ -   Loading pretrained model and tokenizer
01/16/2022 00:22:27 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/16/2022 00:22:32 - INFO - __main__ -   Using lang2id = None
01/16/2022 00:22:32 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/16/2022 00:22:32 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
01/16/2022 00:22:32 - INFO - root -   Trying to decide if add adapter
01/16/2022 00:22:32 - INFO - root -   loading task adapter
01/16/2022 00:22:33 - INFO - root -   loading lang adpater hi/wiki@ukp,my/wiki@ukp,fa/wiki@ukp,tk/wiki@ukp,vi/wiki@ukp,zh/wiki@ukp,ka/wiki@ukp,zh_yue/wiki@ukp,hy/wiki@ukp,ru/wiki@ukp
01/16/2022 00:22:33 - INFO - __main__ -   Adapter Languages : ['hi', 'my', 'fa', 'tk', 'vi', 'zh', 'ka', 'zh_yue', 'hy', 'ru'], Length : 10
01/16/2022 00:22:33 - INFO - __main__ -   Adapter Names ['hi/wiki@ukp', 'my/wiki@ukp', 'fa/wiki@ukp', 'tk/wiki@ukp', 'vi/wiki@ukp', 'zh/wiki@ukp', 'ka/wiki@ukp', 'zh_yue/wiki@ukp', 'hy/wiki@ukp', 'ru/wiki@ukp'], Length : 10
01/16/2022 00:22:33 - INFO - __main__ -   Language = hi
01/16/2022 00:22:33 - INFO - __main__ -   Adapter Name = hi/wiki@ukp
01/16/2022 00:22:33 - INFO - __main__ -   Language = my
01/16/2022 00:22:33 - INFO - __main__ -   Adapter Name = my/wiki@ukp
01/16/2022 00:22:34 - INFO - __main__ -   Language = fa
01/16/2022 00:22:34 - INFO - __main__ -   Adapter Name = fa/wiki@ukp
01/16/2022 00:22:36 - INFO - __main__ -   Language = tk
01/16/2022 00:22:36 - INFO - __main__ -   Adapter Name = tk/wiki@ukp
01/16/2022 00:22:37 - INFO - __main__ -   Language = vi
01/16/2022 00:22:37 - INFO - __main__ -   Adapter Name = vi/wiki@ukp
01/16/2022 00:22:38 - INFO - __main__ -   Language = zh
01/16/2022 00:22:38 - INFO - __main__ -   Adapter Name = zh/wiki@ukp
01/16/2022 00:22:39 - INFO - __main__ -   Language = ka
01/16/2022 00:22:39 - INFO - __main__ -   Adapter Name = ka/wiki@ukp
01/16/2022 00:22:41 - INFO - __main__ -   Language = zh_yue
01/16/2022 00:22:41 - INFO - __main__ -   Adapter Name = zh_yue/wiki@ukp
01/16/2022 00:22:42 - INFO - __main__ -   Language = hy
01/16/2022 00:22:42 - INFO - __main__ -   Adapter Name = hy/wiki@ukp
01/16/2022 00:22:44 - INFO - __main__ -   Language = ru
01/16/2022 00:22:44 - INFO - __main__ -   Adapter Name = ru/wiki@ukp
01/16/2022 00:22:48 - INFO - __main__ -   Args Adapter Weight = equal
01/16/2022 00:22:48 - INFO - __main__ -   Adapter Languages = ['hi', 'my', 'fa', 'tk', 'vi', 'zh', 'ka', 'zh_yue', 'hy', 'ru']
01/16/2022 00:22:48 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/16/2022 00:22:48 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/16/2022 00:22:48 - INFO - __main__ -   Length of Adapter Weights = 10
01/16/2022 00:22:48 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/cached_test_mr_bert-base-multilingual-cased_128
01/16/2022 00:22:48 - INFO - __main__ -   ***** Running evaluation  in mr *****
01/16/2022 00:22:48 - INFO - __main__ -     Num examples = 47
01/16/2022 00:22:48 - INFO - __main__ -     Batch size = 32
01/16/2022 00:22:48 - INFO - __main__ -   Batch number = 1
01/16/2022 00:22:49 - INFO - __main__ -   Batch number = 2
01/16/2022 00:22:49 - INFO - __main__ -   ***** Evaluation result  in mr *****
01/16/2022 00:22:49 - INFO - __main__ -     f1 = 0.6738461538461539
01/16/2022 00:22:49 - INFO - __main__ -     loss = 1.0248869359493256
01/16/2022 00:22:49 - INFO - __main__ -     precision = 0.6908517350157729
01/16/2022 00:22:49 - INFO - __main__ -     recall = 0.6576576576576577
01/16/2022 00:22:51 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mr', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/16/2022 00:22:51 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/16/2022 00:22:51 - INFO - __main__ -   Seed = 3
01/16/2022 00:22:51 - INFO - root -   save model
01/16/2022 00:22:51 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mr', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/16/2022 00:22:51 - INFO - __main__ -   Loading pretrained model and tokenizer
01/16/2022 00:22:54 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/16/2022 00:22:59 - INFO - __main__ -   Using lang2id = None
01/16/2022 00:22:59 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/16/2022 00:22:59 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
01/16/2022 00:22:59 - INFO - root -   Trying to decide if add adapter
01/16/2022 00:22:59 - INFO - root -   loading task adapter
01/16/2022 00:22:59 - INFO - root -   loading lang adpater hi/wiki@ukp,my/wiki@ukp,fa/wiki@ukp,tk/wiki@ukp,vi/wiki@ukp,zh/wiki@ukp,ka/wiki@ukp,zh_yue/wiki@ukp,hy/wiki@ukp,ru/wiki@ukp
01/16/2022 00:22:59 - INFO - __main__ -   Adapter Languages : ['hi', 'my', 'fa', 'tk', 'vi', 'zh', 'ka', 'zh_yue', 'hy', 'ru'], Length : 10
01/16/2022 00:22:59 - INFO - __main__ -   Adapter Names ['hi/wiki@ukp', 'my/wiki@ukp', 'fa/wiki@ukp', 'tk/wiki@ukp', 'vi/wiki@ukp', 'zh/wiki@ukp', 'ka/wiki@ukp', 'zh_yue/wiki@ukp', 'hy/wiki@ukp', 'ru/wiki@ukp'], Length : 10
01/16/2022 00:22:59 - INFO - __main__ -   Language = hi
01/16/2022 00:22:59 - INFO - __main__ -   Adapter Name = hi/wiki@ukp
01/16/2022 00:23:00 - INFO - __main__ -   Language = my
01/16/2022 00:23:00 - INFO - __main__ -   Adapter Name = my/wiki@ukp
01/16/2022 00:23:01 - INFO - __main__ -   Language = fa
01/16/2022 00:23:01 - INFO - __main__ -   Adapter Name = fa/wiki@ukp
01/16/2022 00:23:04 - INFO - __main__ -   Language = tk
01/16/2022 00:23:04 - INFO - __main__ -   Adapter Name = tk/wiki@ukp
01/16/2022 00:23:05 - INFO - __main__ -   Language = vi
01/16/2022 00:23:05 - INFO - __main__ -   Adapter Name = vi/wiki@ukp
01/16/2022 00:23:07 - INFO - __main__ -   Language = zh
01/16/2022 00:23:07 - INFO - __main__ -   Adapter Name = zh/wiki@ukp
01/16/2022 00:23:08 - INFO - __main__ -   Language = ka
01/16/2022 00:23:08 - INFO - __main__ -   Adapter Name = ka/wiki@ukp
01/16/2022 00:23:10 - INFO - __main__ -   Language = zh_yue
01/16/2022 00:23:10 - INFO - __main__ -   Adapter Name = zh_yue/wiki@ukp
01/16/2022 00:23:12 - INFO - __main__ -   Language = hy
01/16/2022 00:23:12 - INFO - __main__ -   Adapter Name = hy/wiki@ukp
01/16/2022 00:23:14 - INFO - __main__ -   Language = ru
01/16/2022 00:23:14 - INFO - __main__ -   Adapter Name = ru/wiki@ukp
01/16/2022 00:23:18 - INFO - __main__ -   Args Adapter Weight = equal
01/16/2022 00:23:18 - INFO - __main__ -   Adapter Languages = ['hi', 'my', 'fa', 'tk', 'vi', 'zh', 'ka', 'zh_yue', 'hy', 'ru']
01/16/2022 00:23:18 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/16/2022 00:23:18 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/16/2022 00:23:18 - INFO - __main__ -   Length of Adapter Weights = 10
01/16/2022 00:23:18 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/cached_test_mr_bert-base-multilingual-cased_128
01/16/2022 00:23:18 - INFO - __main__ -   ***** Running evaluation  in mr *****
01/16/2022 00:23:18 - INFO - __main__ -     Num examples = 47
01/16/2022 00:23:18 - INFO - __main__ -     Batch size = 32
01/16/2022 00:23:18 - INFO - __main__ -   Batch number = 1
01/16/2022 00:23:19 - INFO - __main__ -   Batch number = 2
01/16/2022 00:23:19 - INFO - __main__ -   ***** Evaluation result  in mr *****
01/16/2022 00:23:19 - INFO - __main__ -     f1 = 0.6913580246913581
01/16/2022 00:23:19 - INFO - __main__ -     loss = 1.0153461694717407
01/16/2022 00:23:19 - INFO - __main__ -     precision = 0.7111111111111111
01/16/2022 00:23:19 - INFO - __main__ -     recall = 0.6726726726726727
01/16/2022 00:32:01 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bho', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/16/2022 00:32:01 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/16/2022 00:32:01 - INFO - __main__ -   Seed = 1
01/16/2022 00:32:01 - INFO - root -   save model
01/16/2022 00:32:01 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bho', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/16/2022 00:32:01 - INFO - __main__ -   Loading pretrained model and tokenizer
01/16/2022 00:32:04 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/16/2022 00:32:09 - INFO - __main__ -   Using lang2id = None
01/16/2022 00:32:09 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/16/2022 00:32:09 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
01/16/2022 00:32:09 - INFO - root -   Trying to decide if add adapter
01/16/2022 00:32:09 - INFO - root -   loading task adapter
01/16/2022 00:32:09 - INFO - root -   loading lang adpater hi/wiki@ukp,my/wiki@ukp,fa/wiki@ukp,vi/wiki@ukp,zh/wiki@ukp,tk/wiki@ukp,zh_yue/wiki@ukp,ru/wiki@ukp,bxr/wiki@ukp,cdo/wiki@ukp
01/16/2022 00:32:09 - INFO - __main__ -   Adapter Languages : ['hi', 'my', 'fa', 'vi', 'zh', 'tk', 'zh_yue', 'ru', 'bxr', 'cdo'], Length : 10
01/16/2022 00:32:09 - INFO - __main__ -   Adapter Names ['hi/wiki@ukp', 'my/wiki@ukp', 'fa/wiki@ukp', 'vi/wiki@ukp', 'zh/wiki@ukp', 'tk/wiki@ukp', 'zh_yue/wiki@ukp', 'ru/wiki@ukp', 'bxr/wiki@ukp', 'cdo/wiki@ukp'], Length : 10
01/16/2022 00:32:09 - INFO - __main__ -   Language = hi
01/16/2022 00:32:09 - INFO - __main__ -   Adapter Name = hi/wiki@ukp
01/16/2022 00:32:18 - INFO - __main__ -   Language = my
01/16/2022 00:32:18 - INFO - __main__ -   Adapter Name = my/wiki@ukp
01/16/2022 00:32:26 - INFO - __main__ -   Language = fa
01/16/2022 00:32:26 - INFO - __main__ -   Adapter Name = fa/wiki@ukp
01/16/2022 00:32:36 - INFO - __main__ -   Language = vi
01/16/2022 00:32:36 - INFO - __main__ -   Adapter Name = vi/wiki@ukp
01/16/2022 00:32:53 - INFO - __main__ -   Language = zh
01/16/2022 00:32:53 - INFO - __main__ -   Adapter Name = zh/wiki@ukp
01/16/2022 00:33:08 - INFO - __main__ -   Language = tk
01/16/2022 00:33:08 - INFO - __main__ -   Adapter Name = tk/wiki@ukp
01/16/2022 00:33:24 - INFO - __main__ -   Language = zh_yue
01/16/2022 00:33:24 - INFO - __main__ -   Adapter Name = zh_yue/wiki@ukp
01/16/2022 00:33:40 - INFO - __main__ -   Language = ru
01/16/2022 00:33:40 - INFO - __main__ -   Adapter Name = ru/wiki@ukp
01/16/2022 00:33:56 - INFO - __main__ -   Language = bxr
01/16/2022 00:33:56 - INFO - __main__ -   Adapter Name = bxr/wiki@ukp
01/16/2022 00:34:07 - INFO - __main__ -   Language = cdo
01/16/2022 00:34:07 - INFO - __main__ -   Adapter Name = cdo/wiki@ukp
01/16/2022 00:34:19 - INFO - __main__ -   Args Adapter Weight = equal
01/16/2022 00:34:19 - INFO - __main__ -   Adapter Languages = ['hi', 'my', 'fa', 'vi', 'zh', 'tk', 'zh_yue', 'ru', 'bxr', 'cdo']
01/16/2022 00:34:19 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/16/2022 00:34:19 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/16/2022 00:34:19 - INFO - __main__ -   Length of Adapter Weights = 10
01/16/2022 00:34:19 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/cached_test_bho_bert-base-multilingual-cased_128
01/16/2022 00:34:20 - INFO - __main__ -   ***** Running evaluation  in bho *****
01/16/2022 00:34:20 - INFO - __main__ -     Num examples = 361
01/16/2022 00:34:20 - INFO - __main__ -     Batch size = 32
01/16/2022 00:34:20 - INFO - __main__ -   Batch number = 1
01/16/2022 00:34:20 - INFO - __main__ -   Batch number = 2
01/16/2022 00:34:20 - INFO - __main__ -   Batch number = 3
01/16/2022 00:34:20 - INFO - __main__ -   Batch number = 4
01/16/2022 00:34:21 - INFO - __main__ -   Batch number = 5
01/16/2022 00:34:21 - INFO - __main__ -   Batch number = 6
01/16/2022 00:34:21 - INFO - __main__ -   Batch number = 7
01/16/2022 00:34:22 - INFO - __main__ -   Batch number = 8
01/16/2022 00:34:22 - INFO - __main__ -   Batch number = 9
01/16/2022 00:34:22 - INFO - __main__ -   Batch number = 10
01/16/2022 00:34:22 - INFO - __main__ -   Batch number = 11
01/16/2022 00:34:23 - INFO - __main__ -   Batch number = 12
01/16/2022 00:34:23 - INFO - __main__ -   ***** Evaluation result  in bho *****
01/16/2022 00:34:23 - INFO - __main__ -     f1 = 0.48619261304798067
01/16/2022 00:34:23 - INFO - __main__ -     loss = 2.3152365684509277
01/16/2022 00:34:23 - INFO - __main__ -     precision = 0.5063814488585295
01/16/2022 00:34:23 - INFO - __main__ -     recall = 0.46755186721991704
01/16/2022 00:34:25 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bho', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/16/2022 00:34:25 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/16/2022 00:34:25 - INFO - __main__ -   Seed = 2
01/16/2022 00:34:25 - INFO - root -   save model
01/16/2022 00:34:25 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bho', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/16/2022 00:34:25 - INFO - __main__ -   Loading pretrained model and tokenizer
01/16/2022 00:34:28 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/16/2022 00:34:33 - INFO - __main__ -   Using lang2id = None
01/16/2022 00:34:33 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/16/2022 00:34:33 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
01/16/2022 00:34:33 - INFO - root -   Trying to decide if add adapter
01/16/2022 00:34:33 - INFO - root -   loading task adapter
01/16/2022 00:34:33 - INFO - root -   loading lang adpater hi/wiki@ukp,my/wiki@ukp,fa/wiki@ukp,vi/wiki@ukp,zh/wiki@ukp,tk/wiki@ukp,zh_yue/wiki@ukp,ru/wiki@ukp,bxr/wiki@ukp,cdo/wiki@ukp
01/16/2022 00:34:33 - INFO - __main__ -   Adapter Languages : ['hi', 'my', 'fa', 'vi', 'zh', 'tk', 'zh_yue', 'ru', 'bxr', 'cdo'], Length : 10
01/16/2022 00:34:33 - INFO - __main__ -   Adapter Names ['hi/wiki@ukp', 'my/wiki@ukp', 'fa/wiki@ukp', 'vi/wiki@ukp', 'zh/wiki@ukp', 'tk/wiki@ukp', 'zh_yue/wiki@ukp', 'ru/wiki@ukp', 'bxr/wiki@ukp', 'cdo/wiki@ukp'], Length : 10
01/16/2022 00:34:33 - INFO - __main__ -   Language = hi
01/16/2022 00:34:33 - INFO - __main__ -   Adapter Name = hi/wiki@ukp
01/16/2022 00:34:49 - INFO - __main__ -   Language = my
01/16/2022 00:34:49 - INFO - __main__ -   Adapter Name = my/wiki@ukp
01/16/2022 00:34:57 - INFO - __main__ -   Language = fa
01/16/2022 00:34:57 - INFO - __main__ -   Adapter Name = fa/wiki@ukp
01/16/2022 00:35:12 - INFO - __main__ -   Language = vi
01/16/2022 00:35:12 - INFO - __main__ -   Adapter Name = vi/wiki@ukp
01/16/2022 00:35:28 - INFO - __main__ -   Language = zh
01/16/2022 00:35:28 - INFO - __main__ -   Adapter Name = zh/wiki@ukp
01/16/2022 00:35:36 - INFO - __main__ -   Language = tk
01/16/2022 00:35:36 - INFO - __main__ -   Adapter Name = tk/wiki@ukp
01/16/2022 00:35:37 - INFO - __main__ -   Language = zh_yue
01/16/2022 00:35:37 - INFO - __main__ -   Adapter Name = zh_yue/wiki@ukp
01/16/2022 00:35:46 - INFO - __main__ -   Language = ru
01/16/2022 00:35:46 - INFO - __main__ -   Adapter Name = ru/wiki@ukp
01/16/2022 00:35:54 - INFO - __main__ -   Language = bxr
01/16/2022 00:35:54 - INFO - __main__ -   Adapter Name = bxr/wiki@ukp
01/16/2022 00:36:10 - INFO - __main__ -   Language = cdo
01/16/2022 00:36:10 - INFO - __main__ -   Adapter Name = cdo/wiki@ukp
01/16/2022 00:36:29 - INFO - __main__ -   Args Adapter Weight = equal
01/16/2022 00:36:29 - INFO - __main__ -   Adapter Languages = ['hi', 'my', 'fa', 'vi', 'zh', 'tk', 'zh_yue', 'ru', 'bxr', 'cdo']
01/16/2022 00:36:29 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/16/2022 00:36:29 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/16/2022 00:36:29 - INFO - __main__ -   Length of Adapter Weights = 10
01/16/2022 00:36:29 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/cached_test_bho_bert-base-multilingual-cased_128
01/16/2022 00:36:29 - INFO - __main__ -   ***** Running evaluation  in bho *****
01/16/2022 00:36:29 - INFO - __main__ -     Num examples = 361
01/16/2022 00:36:29 - INFO - __main__ -     Batch size = 32
01/16/2022 00:36:29 - INFO - __main__ -   Batch number = 1
01/16/2022 00:36:29 - INFO - __main__ -   Batch number = 2
01/16/2022 00:36:30 - INFO - __main__ -   Batch number = 3
01/16/2022 00:36:30 - INFO - __main__ -   Batch number = 4
01/16/2022 00:36:30 - INFO - __main__ -   Batch number = 5
01/16/2022 00:36:30 - INFO - __main__ -   Batch number = 6
01/16/2022 00:36:31 - INFO - __main__ -   Batch number = 7
01/16/2022 00:36:31 - INFO - __main__ -   Batch number = 8
01/16/2022 00:36:31 - INFO - __main__ -   Batch number = 9
01/16/2022 00:36:31 - INFO - __main__ -   Batch number = 10
01/16/2022 00:36:32 - INFO - __main__ -   Batch number = 11
01/16/2022 00:36:32 - INFO - __main__ -   Batch number = 12
01/16/2022 00:36:32 - INFO - __main__ -   ***** Evaluation result  in bho *****
01/16/2022 00:36:32 - INFO - __main__ -     f1 = 0.40785418684511754
01/16/2022 00:36:32 - INFO - __main__ -     loss = 2.7474190493424735
01/16/2022 00:36:32 - INFO - __main__ -     precision = 0.43435858964741186
01/16/2022 00:36:32 - INFO - __main__ -     recall = 0.38439834024896263
01/16/2022 00:36:34 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bho', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/16/2022 00:36:34 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/16/2022 00:36:34 - INFO - __main__ -   Seed = 3
01/16/2022 00:36:34 - INFO - root -   save model
01/16/2022 00:36:34 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bho', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/16/2022 00:36:34 - INFO - __main__ -   Loading pretrained model and tokenizer
01/16/2022 00:36:37 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/16/2022 00:36:43 - INFO - __main__ -   Using lang2id = None
01/16/2022 00:36:43 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/16/2022 00:36:43 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
01/16/2022 00:36:43 - INFO - root -   Trying to decide if add adapter
01/16/2022 00:36:43 - INFO - root -   loading task adapter
01/16/2022 00:36:43 - INFO - root -   loading lang adpater hi/wiki@ukp,my/wiki@ukp,fa/wiki@ukp,vi/wiki@ukp,zh/wiki@ukp,tk/wiki@ukp,zh_yue/wiki@ukp,ru/wiki@ukp,bxr/wiki@ukp,cdo/wiki@ukp
01/16/2022 00:36:43 - INFO - __main__ -   Adapter Languages : ['hi', 'my', 'fa', 'vi', 'zh', 'tk', 'zh_yue', 'ru', 'bxr', 'cdo'], Length : 10
01/16/2022 00:36:43 - INFO - __main__ -   Adapter Names ['hi/wiki@ukp', 'my/wiki@ukp', 'fa/wiki@ukp', 'vi/wiki@ukp', 'zh/wiki@ukp', 'tk/wiki@ukp', 'zh_yue/wiki@ukp', 'ru/wiki@ukp', 'bxr/wiki@ukp', 'cdo/wiki@ukp'], Length : 10
01/16/2022 00:36:43 - INFO - __main__ -   Language = hi
01/16/2022 00:36:43 - INFO - __main__ -   Adapter Name = hi/wiki@ukp
01/16/2022 00:36:58 - INFO - __main__ -   Language = my
01/16/2022 00:36:58 - INFO - __main__ -   Adapter Name = my/wiki@ukp
01/16/2022 00:37:14 - INFO - __main__ -   Language = fa
01/16/2022 00:37:14 - INFO - __main__ -   Adapter Name = fa/wiki@ukp
01/16/2022 00:37:23 - INFO - __main__ -   Language = vi
01/16/2022 00:37:23 - INFO - __main__ -   Adapter Name = vi/wiki@ukp
01/16/2022 00:37:40 - INFO - __main__ -   Language = zh
01/16/2022 00:37:40 - INFO - __main__ -   Adapter Name = zh/wiki@ukp
01/16/2022 00:37:55 - INFO - __main__ -   Language = tk
01/16/2022 00:37:55 - INFO - __main__ -   Adapter Name = tk/wiki@ukp
01/16/2022 00:38:10 - INFO - __main__ -   Language = zh_yue
01/16/2022 00:38:10 - INFO - __main__ -   Adapter Name = zh_yue/wiki@ukp
01/16/2022 00:38:27 - INFO - __main__ -   Language = ru
01/16/2022 00:38:27 - INFO - __main__ -   Adapter Name = ru/wiki@ukp
01/16/2022 00:38:35 - INFO - __main__ -   Language = bxr
01/16/2022 00:38:35 - INFO - __main__ -   Adapter Name = bxr/wiki@ukp
01/16/2022 00:38:51 - INFO - __main__ -   Language = cdo
01/16/2022 00:38:51 - INFO - __main__ -   Adapter Name = cdo/wiki@ukp
01/16/2022 00:39:03 - INFO - __main__ -   Args Adapter Weight = equal
01/16/2022 00:39:03 - INFO - __main__ -   Adapter Languages = ['hi', 'my', 'fa', 'vi', 'zh', 'tk', 'zh_yue', 'ru', 'bxr', 'cdo']
01/16/2022 00:39:03 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/16/2022 00:39:03 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/16/2022 00:39:03 - INFO - __main__ -   Length of Adapter Weights = 10
01/16/2022 00:39:03 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/cached_test_bho_bert-base-multilingual-cased_128
01/16/2022 00:39:03 - INFO - __main__ -   ***** Running evaluation  in bho *****
01/16/2022 00:39:03 - INFO - __main__ -     Num examples = 361
01/16/2022 00:39:03 - INFO - __main__ -     Batch size = 32
01/16/2022 00:39:03 - INFO - __main__ -   Batch number = 1
01/16/2022 00:39:03 - INFO - __main__ -   Batch number = 2
01/16/2022 00:39:03 - INFO - __main__ -   Batch number = 3
01/16/2022 00:39:04 - INFO - __main__ -   Batch number = 4
01/16/2022 00:39:04 - INFO - __main__ -   Batch number = 5
01/16/2022 00:39:04 - INFO - __main__ -   Batch number = 6
01/16/2022 00:39:04 - INFO - __main__ -   Batch number = 7
01/16/2022 00:39:05 - INFO - __main__ -   Batch number = 8
01/16/2022 00:39:05 - INFO - __main__ -   Batch number = 9
01/16/2022 00:39:05 - INFO - __main__ -   Batch number = 10
01/16/2022 00:39:06 - INFO - __main__ -   Batch number = 11
01/16/2022 00:39:06 - INFO - __main__ -   Batch number = 12
01/16/2022 00:39:06 - INFO - __main__ -   ***** Evaluation result  in bho *****
01/16/2022 00:39:06 - INFO - __main__ -     f1 = 0.4604241207784274
01/16/2022 00:39:06 - INFO - __main__ -     loss = 2.4151559869448342
01/16/2022 00:39:06 - INFO - __main__ -     precision = 0.4854619065145381
01/16/2022 00:39:06 - INFO - __main__ -     recall = 0.43784232365145226
01/16/2022 00:39:08 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ta', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/16/2022 00:39:08 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/16/2022 00:39:08 - INFO - __main__ -   Seed = 1
01/16/2022 00:39:08 - INFO - root -   save model
01/16/2022 00:39:08 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ta', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/16/2022 00:39:08 - INFO - __main__ -   Loading pretrained model and tokenizer
01/16/2022 00:39:11 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/16/2022 00:39:16 - INFO - __main__ -   Using lang2id = None
01/16/2022 00:39:16 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/16/2022 00:39:16 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
01/16/2022 00:39:16 - INFO - root -   Trying to decide if add adapter
01/16/2022 00:39:16 - INFO - root -   loading task adapter
01/16/2022 00:39:16 - INFO - root -   loading lang adpater hi/wiki@ukp,my/wiki@ukp,fa/wiki@ukp,vi/wiki@ukp,tk/wiki@ukp,jv/wiki@ukp,zh_yue/wiki@ukp,id/wiki@ukp,zh/wiki@ukp,am/wiki@ukp
01/16/2022 00:39:16 - INFO - __main__ -   Adapter Languages : ['hi', 'my', 'fa', 'vi', 'tk', 'jv', 'zh_yue', 'id', 'zh', 'am'], Length : 10
01/16/2022 00:39:16 - INFO - __main__ -   Adapter Names ['hi/wiki@ukp', 'my/wiki@ukp', 'fa/wiki@ukp', 'vi/wiki@ukp', 'tk/wiki@ukp', 'jv/wiki@ukp', 'zh_yue/wiki@ukp', 'id/wiki@ukp', 'zh/wiki@ukp', 'am/wiki@ukp'], Length : 10
01/16/2022 00:39:16 - INFO - __main__ -   Language = hi
01/16/2022 00:39:16 - INFO - __main__ -   Adapter Name = hi/wiki@ukp
01/16/2022 00:39:25 - INFO - __main__ -   Language = my
01/16/2022 00:39:25 - INFO - __main__ -   Adapter Name = my/wiki@ukp
01/16/2022 00:39:40 - INFO - __main__ -   Language = fa
01/16/2022 00:39:40 - INFO - __main__ -   Adapter Name = fa/wiki@ukp
01/16/2022 00:39:56 - INFO - __main__ -   Language = vi
01/16/2022 00:39:56 - INFO - __main__ -   Adapter Name = vi/wiki@ukp
01/16/2022 00:40:13 - INFO - __main__ -   Language = tk
01/16/2022 00:40:13 - INFO - __main__ -   Adapter Name = tk/wiki@ukp
01/16/2022 00:40:21 - INFO - __main__ -   Language = jv
01/16/2022 00:40:21 - INFO - __main__ -   Adapter Name = jv/wiki@ukp
01/16/2022 00:40:36 - INFO - __main__ -   Language = zh_yue
01/16/2022 00:40:36 - INFO - __main__ -   Adapter Name = zh_yue/wiki@ukp
01/16/2022 00:40:46 - INFO - __main__ -   Language = id
01/16/2022 00:40:46 - INFO - __main__ -   Adapter Name = id/wiki@ukp
01/16/2022 00:41:03 - INFO - __main__ -   Language = zh
01/16/2022 00:41:03 - INFO - __main__ -   Adapter Name = zh/wiki@ukp
01/16/2022 00:41:18 - INFO - __main__ -   Language = am
01/16/2022 00:41:18 - INFO - __main__ -   Adapter Name = am/wiki@ukp
01/16/2022 00:41:38 - INFO - __main__ -   Args Adapter Weight = equal
01/16/2022 00:41:38 - INFO - __main__ -   Adapter Languages = ['hi', 'my', 'fa', 'vi', 'tk', 'jv', 'zh_yue', 'id', 'zh', 'am']
01/16/2022 00:41:38 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/16/2022 00:41:38 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/16/2022 00:41:38 - INFO - __main__ -   Length of Adapter Weights = 10
01/16/2022 00:41:38 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/cached_test_ta_bert-base-multilingual-cased_128
01/16/2022 00:41:38 - INFO - __main__ -   ***** Running evaluation  in ta *****
01/16/2022 00:41:38 - INFO - __main__ -     Num examples = 656
01/16/2022 00:41:38 - INFO - __main__ -     Batch size = 32
01/16/2022 00:41:38 - INFO - __main__ -   Batch number = 1
01/16/2022 00:41:38 - INFO - __main__ -   Batch number = 2
01/16/2022 00:41:38 - INFO - __main__ -   Batch number = 3
01/16/2022 00:41:39 - INFO - __main__ -   Batch number = 4
01/16/2022 00:41:39 - INFO - __main__ -   Batch number = 5
01/16/2022 00:41:39 - INFO - __main__ -   Batch number = 6
01/16/2022 00:41:39 - INFO - __main__ -   Batch number = 7
01/16/2022 00:41:40 - INFO - __main__ -   Batch number = 8
01/16/2022 00:41:40 - INFO - __main__ -   Batch number = 9
01/16/2022 00:41:40 - INFO - __main__ -   Batch number = 10
01/16/2022 00:41:40 - INFO - __main__ -   Batch number = 11
01/16/2022 00:41:41 - INFO - __main__ -   Batch number = 12
01/16/2022 00:41:41 - INFO - __main__ -   Batch number = 13
01/16/2022 00:41:41 - INFO - __main__ -   Batch number = 14
01/16/2022 00:41:41 - INFO - __main__ -   Batch number = 15
01/16/2022 00:41:42 - INFO - __main__ -   Batch number = 16
01/16/2022 00:41:42 - INFO - __main__ -   Batch number = 17
01/16/2022 00:41:42 - INFO - __main__ -   Batch number = 18
01/16/2022 00:41:43 - INFO - __main__ -   Batch number = 19
01/16/2022 00:41:43 - INFO - __main__ -   Batch number = 20
01/16/2022 00:41:43 - INFO - __main__ -   Batch number = 21
01/16/2022 00:41:43 - INFO - __main__ -   ***** Evaluation result  in ta *****
01/16/2022 00:41:43 - INFO - __main__ -     f1 = 0.6752865902426021
01/16/2022 00:41:43 - INFO - __main__ -     loss = 1.0212128829388392
01/16/2022 00:41:43 - INFO - __main__ -     precision = 0.7028301886792453
01/16/2022 00:41:43 - INFO - __main__ -     recall = 0.6498204207285787
01/16/2022 00:41:45 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ta', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/16/2022 00:41:45 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/16/2022 00:41:45 - INFO - __main__ -   Seed = 2
01/16/2022 00:41:45 - INFO - root -   save model
01/16/2022 00:41:45 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ta', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/16/2022 00:41:45 - INFO - __main__ -   Loading pretrained model and tokenizer
01/16/2022 00:41:48 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/16/2022 00:41:54 - INFO - __main__ -   Using lang2id = None
01/16/2022 00:41:54 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/16/2022 00:41:54 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
01/16/2022 00:41:54 - INFO - root -   Trying to decide if add adapter
01/16/2022 00:41:54 - INFO - root -   loading task adapter
01/16/2022 00:41:54 - INFO - root -   loading lang adpater hi/wiki@ukp,my/wiki@ukp,fa/wiki@ukp,vi/wiki@ukp,tk/wiki@ukp,jv/wiki@ukp,zh_yue/wiki@ukp,id/wiki@ukp,zh/wiki@ukp,am/wiki@ukp
01/16/2022 00:41:54 - INFO - __main__ -   Adapter Languages : ['hi', 'my', 'fa', 'vi', 'tk', 'jv', 'zh_yue', 'id', 'zh', 'am'], Length : 10
01/16/2022 00:41:54 - INFO - __main__ -   Adapter Names ['hi/wiki@ukp', 'my/wiki@ukp', 'fa/wiki@ukp', 'vi/wiki@ukp', 'tk/wiki@ukp', 'jv/wiki@ukp', 'zh_yue/wiki@ukp', 'id/wiki@ukp', 'zh/wiki@ukp', 'am/wiki@ukp'], Length : 10
01/16/2022 00:41:54 - INFO - __main__ -   Language = hi
01/16/2022 00:41:54 - INFO - __main__ -   Adapter Name = hi/wiki@ukp
01/16/2022 00:42:02 - INFO - __main__ -   Language = my
01/16/2022 00:42:02 - INFO - __main__ -   Adapter Name = my/wiki@ukp
01/16/2022 00:42:17 - INFO - __main__ -   Language = fa
01/16/2022 00:42:17 - INFO - __main__ -   Adapter Name = fa/wiki@ukp
01/16/2022 00:42:19 - INFO - __main__ -   Language = vi
01/16/2022 00:42:19 - INFO - __main__ -   Adapter Name = vi/wiki@ukp
01/16/2022 00:42:35 - INFO - __main__ -   Language = tk
01/16/2022 00:42:35 - INFO - __main__ -   Adapter Name = tk/wiki@ukp
01/16/2022 00:42:50 - INFO - __main__ -   Language = jv
01/16/2022 00:42:50 - INFO - __main__ -   Adapter Name = jv/wiki@ukp
01/16/2022 00:43:05 - INFO - __main__ -   Language = zh_yue
01/16/2022 00:43:05 - INFO - __main__ -   Adapter Name = zh_yue/wiki@ukp
01/16/2022 00:43:21 - INFO - __main__ -   Language = id
01/16/2022 00:43:21 - INFO - __main__ -   Adapter Name = id/wiki@ukp
01/16/2022 00:43:37 - INFO - __main__ -   Language = zh
01/16/2022 00:43:37 - INFO - __main__ -   Adapter Name = zh/wiki@ukp
01/16/2022 00:43:52 - INFO - __main__ -   Language = am
01/16/2022 00:43:52 - INFO - __main__ -   Adapter Name = am/wiki@ukp
01/16/2022 00:44:04 - INFO - __main__ -   Args Adapter Weight = equal
01/16/2022 00:44:04 - INFO - __main__ -   Adapter Languages = ['hi', 'my', 'fa', 'vi', 'tk', 'jv', 'zh_yue', 'id', 'zh', 'am']
01/16/2022 00:44:04 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/16/2022 00:44:04 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/16/2022 00:44:04 - INFO - __main__ -   Length of Adapter Weights = 10
01/16/2022 00:44:04 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/cached_test_ta_bert-base-multilingual-cased_128
01/16/2022 00:44:04 - INFO - __main__ -   ***** Running evaluation  in ta *****
01/16/2022 00:44:04 - INFO - __main__ -     Num examples = 656
01/16/2022 00:44:04 - INFO - __main__ -     Batch size = 32
01/16/2022 00:44:04 - INFO - __main__ -   Batch number = 1
01/16/2022 00:44:04 - INFO - __main__ -   Batch number = 2
01/16/2022 00:44:05 - INFO - __main__ -   Batch number = 3
01/16/2022 00:44:05 - INFO - __main__ -   Batch number = 4
01/16/2022 00:44:05 - INFO - __main__ -   Batch number = 5
01/16/2022 00:44:05 - INFO - __main__ -   Batch number = 6
01/16/2022 00:44:06 - INFO - __main__ -   Batch number = 7
01/16/2022 00:44:06 - INFO - __main__ -   Batch number = 8
01/16/2022 00:44:06 - INFO - __main__ -   Batch number = 9
01/16/2022 00:44:06 - INFO - __main__ -   Batch number = 10
01/16/2022 00:44:07 - INFO - __main__ -   Batch number = 11
01/16/2022 00:44:07 - INFO - __main__ -   Batch number = 12
01/16/2022 00:44:07 - INFO - __main__ -   Batch number = 13
01/16/2022 00:44:07 - INFO - __main__ -   Batch number = 14
01/16/2022 00:44:08 - INFO - __main__ -   Batch number = 15
01/16/2022 00:44:08 - INFO - __main__ -   Batch number = 16
01/16/2022 00:44:08 - INFO - __main__ -   Batch number = 17
01/16/2022 00:44:09 - INFO - __main__ -   Batch number = 18
01/16/2022 00:44:09 - INFO - __main__ -   Batch number = 19
01/16/2022 00:44:09 - INFO - __main__ -   Batch number = 20
01/16/2022 00:44:09 - INFO - __main__ -   Batch number = 21
01/16/2022 00:44:10 - INFO - __main__ -   ***** Evaluation result  in ta *****
01/16/2022 00:44:10 - INFO - __main__ -     f1 = 0.6678191489361701
01/16/2022 00:44:10 - INFO - __main__ -     loss = 1.0436999513989402
01/16/2022 00:44:10 - INFO - __main__ -     precision = 0.6932633903920485
01/16/2022 00:44:10 - INFO - __main__ -     recall = 0.6441765007696254
01/16/2022 00:44:12 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ta', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/16/2022 00:44:12 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/16/2022 00:44:12 - INFO - __main__ -   Seed = 3
01/16/2022 00:44:12 - INFO - root -   save model
01/16/2022 00:44:12 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ta', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/16/2022 00:44:12 - INFO - __main__ -   Loading pretrained model and tokenizer
01/16/2022 00:44:15 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/16/2022 00:44:21 - INFO - __main__ -   Using lang2id = None
01/16/2022 00:44:21 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/16/2022 00:44:21 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
01/16/2022 00:44:21 - INFO - root -   Trying to decide if add adapter
01/16/2022 00:44:21 - INFO - root -   loading task adapter
01/16/2022 00:44:21 - INFO - root -   loading lang adpater hi/wiki@ukp,my/wiki@ukp,fa/wiki@ukp,vi/wiki@ukp,tk/wiki@ukp,jv/wiki@ukp,zh_yue/wiki@ukp,id/wiki@ukp,zh/wiki@ukp,am/wiki@ukp
01/16/2022 00:44:21 - INFO - __main__ -   Adapter Languages : ['hi', 'my', 'fa', 'vi', 'tk', 'jv', 'zh_yue', 'id', 'zh', 'am'], Length : 10
01/16/2022 00:44:21 - INFO - __main__ -   Adapter Names ['hi/wiki@ukp', 'my/wiki@ukp', 'fa/wiki@ukp', 'vi/wiki@ukp', 'tk/wiki@ukp', 'jv/wiki@ukp', 'zh_yue/wiki@ukp', 'id/wiki@ukp', 'zh/wiki@ukp', 'am/wiki@ukp'], Length : 10
01/16/2022 00:44:21 - INFO - __main__ -   Language = hi
01/16/2022 00:44:21 - INFO - __main__ -   Adapter Name = hi/wiki@ukp
01/16/2022 00:44:36 - INFO - __main__ -   Language = my
01/16/2022 00:44:36 - INFO - __main__ -   Adapter Name = my/wiki@ukp
01/16/2022 00:44:51 - INFO - __main__ -   Language = fa
01/16/2022 00:44:51 - INFO - __main__ -   Adapter Name = fa/wiki@ukp
01/16/2022 00:45:00 - INFO - __main__ -   Language = vi
01/16/2022 00:45:00 - INFO - __main__ -   Adapter Name = vi/wiki@ukp
01/16/2022 00:45:17 - INFO - __main__ -   Language = tk
01/16/2022 00:45:17 - INFO - __main__ -   Adapter Name = tk/wiki@ukp
01/16/2022 00:45:32 - INFO - __main__ -   Language = jv
01/16/2022 00:45:32 - INFO - __main__ -   Adapter Name = jv/wiki@ukp
01/16/2022 00:45:48 - INFO - __main__ -   Language = zh_yue
01/16/2022 00:45:48 - INFO - __main__ -   Adapter Name = zh_yue/wiki@ukp
01/16/2022 00:45:57 - INFO - __main__ -   Language = id
01/16/2022 00:45:57 - INFO - __main__ -   Adapter Name = id/wiki@ukp
01/16/2022 00:46:07 - INFO - __main__ -   Language = zh
01/16/2022 00:46:07 - INFO - __main__ -   Adapter Name = zh/wiki@ukp
01/16/2022 00:46:22 - INFO - __main__ -   Language = am
01/16/2022 00:46:22 - INFO - __main__ -   Adapter Name = am/wiki@ukp
01/16/2022 00:46:42 - INFO - __main__ -   Args Adapter Weight = equal
01/16/2022 00:46:42 - INFO - __main__ -   Adapter Languages = ['hi', 'my', 'fa', 'vi', 'tk', 'jv', 'zh_yue', 'id', 'zh', 'am']
01/16/2022 00:46:42 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/16/2022 00:46:42 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/16/2022 00:46:42 - INFO - __main__ -   Length of Adapter Weights = 10
01/16/2022 00:46:42 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/cached_test_ta_bert-base-multilingual-cased_128
01/16/2022 00:46:42 - INFO - __main__ -   ***** Running evaluation  in ta *****
01/16/2022 00:46:42 - INFO - __main__ -     Num examples = 656
01/16/2022 00:46:42 - INFO - __main__ -     Batch size = 32
01/16/2022 00:46:42 - INFO - __main__ -   Batch number = 1
01/16/2022 00:46:42 - INFO - __main__ -   Batch number = 2
01/16/2022 00:46:43 - INFO - __main__ -   Batch number = 3
01/16/2022 00:46:43 - INFO - __main__ -   Batch number = 4
01/16/2022 00:46:43 - INFO - __main__ -   Batch number = 5
01/16/2022 00:46:43 - INFO - __main__ -   Batch number = 6
01/16/2022 00:46:44 - INFO - __main__ -   Batch number = 7
01/16/2022 00:46:44 - INFO - __main__ -   Batch number = 8
01/16/2022 00:46:44 - INFO - __main__ -   Batch number = 9
01/16/2022 00:46:45 - INFO - __main__ -   Batch number = 10
01/16/2022 00:46:45 - INFO - __main__ -   Batch number = 11
01/16/2022 00:46:45 - INFO - __main__ -   Batch number = 12
01/16/2022 00:46:46 - INFO - __main__ -   Batch number = 13
01/16/2022 00:46:46 - INFO - __main__ -   Batch number = 14
01/16/2022 00:46:46 - INFO - __main__ -   Batch number = 15
01/16/2022 00:46:46 - INFO - __main__ -   Batch number = 16
01/16/2022 00:46:47 - INFO - __main__ -   Batch number = 17
01/16/2022 00:46:47 - INFO - __main__ -   Batch number = 18
01/16/2022 00:46:47 - INFO - __main__ -   Batch number = 19
01/16/2022 00:46:47 - INFO - __main__ -   Batch number = 20
01/16/2022 00:46:48 - INFO - __main__ -   Batch number = 21
01/16/2022 00:46:48 - INFO - __main__ -   ***** Evaluation result  in ta *****
01/16/2022 00:46:48 - INFO - __main__ -     f1 = 0.6949398863786498
01/16/2022 00:46:48 - INFO - __main__ -     loss = 0.9465570038273221
01/16/2022 00:46:48 - INFO - __main__ -     precision = 0.7164260419504223
01/16/2022 00:46:48 - INFO - __main__ -     recall = 0.6747049769112365
01/16/2022 00:46:50 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/16/2022 00:46:50 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/16/2022 00:46:50 - INFO - __main__ -   Seed = 1
01/16/2022 00:46:50 - INFO - root -   save model
01/16/2022 00:46:50 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/16/2022 00:46:50 - INFO - __main__ -   Loading pretrained model and tokenizer
01/16/2022 00:46:53 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/16/2022 00:46:59 - INFO - __main__ -   Using lang2id = None
01/16/2022 00:46:59 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/16/2022 00:46:59 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
01/16/2022 00:46:59 - INFO - root -   Trying to decide if add adapter
01/16/2022 00:46:59 - INFO - root -   loading task adapter
01/16/2022 00:46:59 - INFO - root -   loading lang adpater is/wiki@ukp,en/wiki@ukp,se/wiki@ukp,et/wiki@ukp,fi/wiki@ukp,fr/wiki@ukp,kv/wiki@ukp,cs/wiki@ukp,de/wiki@ukp,eu/wiki@ukp
01/16/2022 00:46:59 - INFO - __main__ -   Adapter Languages : ['is', 'en', 'se', 'et', 'fi', 'fr', 'kv', 'cs', 'de', 'eu'], Length : 10
01/16/2022 00:46:59 - INFO - __main__ -   Adapter Names ['is/wiki@ukp', 'en/wiki@ukp', 'se/wiki@ukp', 'et/wiki@ukp', 'fi/wiki@ukp', 'fr/wiki@ukp', 'kv/wiki@ukp', 'cs/wiki@ukp', 'de/wiki@ukp', 'eu/wiki@ukp'], Length : 10
01/16/2022 00:46:59 - INFO - __main__ -   Language = is
01/16/2022 00:46:59 - INFO - __main__ -   Adapter Name = is/wiki@ukp
01/16/2022 00:47:14 - INFO - __main__ -   Language = en
01/16/2022 00:47:14 - INFO - __main__ -   Adapter Name = en/wiki@ukp
01/16/2022 00:47:30 - INFO - __main__ -   Language = se
01/16/2022 00:47:30 - INFO - __main__ -   Adapter Name = se/wiki@ukp
01/16/2022 00:47:49 - INFO - __main__ -   Language = et
01/16/2022 00:47:49 - INFO - __main__ -   Adapter Name = et/wiki@ukp
01/16/2022 00:48:05 - INFO - __main__ -   Language = fi
01/16/2022 00:48:05 - INFO - __main__ -   Adapter Name = fi/wiki@ukp
01/16/2022 00:48:21 - INFO - __main__ -   Language = fr
01/16/2022 00:48:21 - INFO - __main__ -   Adapter Name = fr/wiki@ukp
01/16/2022 00:48:38 - INFO - __main__ -   Language = kv
01/16/2022 00:48:38 - INFO - __main__ -   Adapter Name = kv/wiki@ukp
01/16/2022 00:48:54 - INFO - __main__ -   Language = cs
01/16/2022 00:48:54 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
01/16/2022 00:49:11 - INFO - __main__ -   Language = de
01/16/2022 00:49:11 - INFO - __main__ -   Adapter Name = de/wiki@ukp
01/16/2022 00:49:27 - INFO - __main__ -   Language = eu
01/16/2022 00:49:27 - INFO - __main__ -   Adapter Name = eu/wiki@ukp
01/16/2022 00:49:47 - INFO - __main__ -   Args Adapter Weight = equal
01/16/2022 00:49:47 - INFO - __main__ -   Adapter Languages = ['is', 'en', 'se', 'et', 'fi', 'fr', 'kv', 'cs', 'de', 'eu']
01/16/2022 00:49:47 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/16/2022 00:49:47 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/16/2022 00:49:47 - INFO - __main__ -   Length of Adapter Weights = 10
01/16/2022 00:49:47 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/cached_test_fo_bert-base-multilingual-cased_128
01/16/2022 00:49:47 - INFO - __main__ -   ***** Running evaluation  in fo *****
01/16/2022 00:49:47 - INFO - __main__ -     Num examples = 1516
01/16/2022 00:49:47 - INFO - __main__ -     Batch size = 32
01/16/2022 00:49:47 - INFO - __main__ -   Batch number = 1
01/16/2022 00:49:47 - INFO - __main__ -   Batch number = 2
01/16/2022 00:49:47 - INFO - __main__ -   Batch number = 3
01/16/2022 00:49:48 - INFO - __main__ -   Batch number = 4
01/16/2022 00:49:48 - INFO - __main__ -   Batch number = 5
01/16/2022 00:49:48 - INFO - __main__ -   Batch number = 6
01/16/2022 00:49:49 - INFO - __main__ -   Batch number = 7
01/16/2022 00:49:49 - INFO - __main__ -   Batch number = 8
01/16/2022 00:49:49 - INFO - __main__ -   Batch number = 9
01/16/2022 00:49:49 - INFO - __main__ -   Batch number = 10
01/16/2022 00:49:50 - INFO - __main__ -   Batch number = 11
01/16/2022 00:49:50 - INFO - __main__ -   Batch number = 12
01/16/2022 00:49:50 - INFO - __main__ -   Batch number = 13
01/16/2022 00:49:50 - INFO - __main__ -   Batch number = 14
01/16/2022 00:49:51 - INFO - __main__ -   Batch number = 15
01/16/2022 00:49:51 - INFO - __main__ -   Batch number = 16
01/16/2022 00:49:51 - INFO - __main__ -   Batch number = 17
01/16/2022 00:49:52 - INFO - __main__ -   Batch number = 18
01/16/2022 00:49:52 - INFO - __main__ -   Batch number = 19
01/16/2022 00:49:52 - INFO - __main__ -   Batch number = 20
01/16/2022 00:49:52 - INFO - __main__ -   Batch number = 21
01/16/2022 00:49:53 - INFO - __main__ -   Batch number = 22
01/16/2022 00:49:53 - INFO - __main__ -   Batch number = 23
01/16/2022 00:49:53 - INFO - __main__ -   Batch number = 24
01/16/2022 00:49:54 - INFO - __main__ -   Batch number = 25
01/16/2022 00:49:54 - INFO - __main__ -   Batch number = 26
01/16/2022 00:49:54 - INFO - __main__ -   Batch number = 27
01/16/2022 00:49:54 - INFO - __main__ -   Batch number = 28
01/16/2022 00:49:55 - INFO - __main__ -   Batch number = 29
01/16/2022 00:49:55 - INFO - __main__ -   Batch number = 30
01/16/2022 00:49:55 - INFO - __main__ -   Batch number = 31
01/16/2022 00:49:56 - INFO - __main__ -   Batch number = 32
01/16/2022 00:49:56 - INFO - __main__ -   Batch number = 33
01/16/2022 00:49:56 - INFO - __main__ -   Batch number = 34
01/16/2022 00:49:56 - INFO - __main__ -   Batch number = 35
01/16/2022 00:49:57 - INFO - __main__ -   Batch number = 36
01/16/2022 00:49:57 - INFO - __main__ -   Batch number = 37
01/16/2022 00:49:57 - INFO - __main__ -   Batch number = 38
01/16/2022 00:49:58 - INFO - __main__ -   Batch number = 39
01/16/2022 00:49:58 - INFO - __main__ -   Batch number = 40
01/16/2022 00:49:58 - INFO - __main__ -   Batch number = 41
01/16/2022 00:49:58 - INFO - __main__ -   Batch number = 42
01/16/2022 00:49:59 - INFO - __main__ -   Batch number = 43
01/16/2022 00:49:59 - INFO - __main__ -   Batch number = 44
01/16/2022 00:49:59 - INFO - __main__ -   Batch number = 45
01/16/2022 00:50:00 - INFO - __main__ -   Batch number = 46
01/16/2022 00:50:00 - INFO - __main__ -   Batch number = 47
01/16/2022 00:50:00 - INFO - __main__ -   Batch number = 48
01/16/2022 00:50:01 - INFO - __main__ -   ***** Evaluation result  in fo *****
01/16/2022 00:50:01 - INFO - __main__ -     f1 = 0.7407097888461753
01/16/2022 00:50:01 - INFO - __main__ -     loss = 0.897670783723394
01/16/2022 00:50:01 - INFO - __main__ -     precision = 0.7476661792824204
01/16/2022 00:50:01 - INFO - __main__ -     recall = 0.733881651578715
01/16/2022 00:50:03 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/16/2022 00:50:03 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/16/2022 00:50:03 - INFO - __main__ -   Seed = 2
01/16/2022 00:50:03 - INFO - root -   save model
01/16/2022 00:50:03 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/16/2022 00:50:03 - INFO - __main__ -   Loading pretrained model and tokenizer
01/16/2022 00:50:06 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/16/2022 00:50:11 - INFO - __main__ -   Using lang2id = None
01/16/2022 00:50:11 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/16/2022 00:50:11 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
01/16/2022 00:50:11 - INFO - root -   Trying to decide if add adapter
01/16/2022 00:50:11 - INFO - root -   loading task adapter
01/16/2022 00:50:12 - INFO - root -   loading lang adpater is/wiki@ukp,en/wiki@ukp,se/wiki@ukp,et/wiki@ukp,fi/wiki@ukp,fr/wiki@ukp,kv/wiki@ukp,cs/wiki@ukp,de/wiki@ukp,eu/wiki@ukp
01/16/2022 00:50:12 - INFO - __main__ -   Adapter Languages : ['is', 'en', 'se', 'et', 'fi', 'fr', 'kv', 'cs', 'de', 'eu'], Length : 10
01/16/2022 00:50:12 - INFO - __main__ -   Adapter Names ['is/wiki@ukp', 'en/wiki@ukp', 'se/wiki@ukp', 'et/wiki@ukp', 'fi/wiki@ukp', 'fr/wiki@ukp', 'kv/wiki@ukp', 'cs/wiki@ukp', 'de/wiki@ukp', 'eu/wiki@ukp'], Length : 10
01/16/2022 00:50:12 - INFO - __main__ -   Language = is
01/16/2022 00:50:12 - INFO - __main__ -   Adapter Name = is/wiki@ukp
01/16/2022 00:50:27 - INFO - __main__ -   Language = en
01/16/2022 00:50:27 - INFO - __main__ -   Adapter Name = en/wiki@ukp
01/16/2022 00:50:35 - INFO - __main__ -   Language = se
01/16/2022 00:50:35 - INFO - __main__ -   Adapter Name = se/wiki@ukp
01/16/2022 00:50:50 - INFO - __main__ -   Language = et
01/16/2022 00:50:50 - INFO - __main__ -   Adapter Name = et/wiki@ukp
01/16/2022 00:50:58 - INFO - __main__ -   Language = fi
01/16/2022 00:50:58 - INFO - __main__ -   Adapter Name = fi/wiki@ukp
01/16/2022 00:51:14 - INFO - __main__ -   Language = fr
01/16/2022 00:51:14 - INFO - __main__ -   Adapter Name = fr/wiki@ukp
01/16/2022 00:51:22 - INFO - __main__ -   Language = kv
01/16/2022 00:51:22 - INFO - __main__ -   Adapter Name = kv/wiki@ukp
01/16/2022 00:51:30 - INFO - __main__ -   Language = cs
01/16/2022 00:51:30 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
01/16/2022 00:51:46 - INFO - __main__ -   Language = de
01/16/2022 00:51:46 - INFO - __main__ -   Adapter Name = de/wiki@ukp
01/16/2022 00:52:01 - INFO - __main__ -   Language = eu
01/16/2022 00:52:01 - INFO - __main__ -   Adapter Name = eu/wiki@ukp
01/16/2022 00:52:13 - INFO - __main__ -   Args Adapter Weight = equal
01/16/2022 00:52:13 - INFO - __main__ -   Adapter Languages = ['is', 'en', 'se', 'et', 'fi', 'fr', 'kv', 'cs', 'de', 'eu']
01/16/2022 00:52:13 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/16/2022 00:52:13 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/16/2022 00:52:13 - INFO - __main__ -   Length of Adapter Weights = 10
01/16/2022 00:52:13 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/cached_test_fo_bert-base-multilingual-cased_128
01/16/2022 00:52:13 - INFO - __main__ -   ***** Running evaluation  in fo *****
01/16/2022 00:52:13 - INFO - __main__ -     Num examples = 1516
01/16/2022 00:52:13 - INFO - __main__ -     Batch size = 32
01/16/2022 00:52:13 - INFO - __main__ -   Batch number = 1
01/16/2022 00:52:14 - INFO - __main__ -   Batch number = 2
01/16/2022 00:52:14 - INFO - __main__ -   Batch number = 3
01/16/2022 00:52:14 - INFO - __main__ -   Batch number = 4
01/16/2022 00:52:14 - INFO - __main__ -   Batch number = 5
01/16/2022 00:52:15 - INFO - __main__ -   Batch number = 6
01/16/2022 00:52:15 - INFO - __main__ -   Batch number = 7
01/16/2022 00:52:15 - INFO - __main__ -   Batch number = 8
01/16/2022 00:52:15 - INFO - __main__ -   Batch number = 9
01/16/2022 00:52:16 - INFO - __main__ -   Batch number = 10
01/16/2022 00:52:16 - INFO - __main__ -   Batch number = 11
01/16/2022 00:52:16 - INFO - __main__ -   Batch number = 12
01/16/2022 00:52:17 - INFO - __main__ -   Batch number = 13
01/16/2022 00:52:17 - INFO - __main__ -   Batch number = 14
01/16/2022 00:52:17 - INFO - __main__ -   Batch number = 15
01/16/2022 00:52:18 - INFO - __main__ -   Batch number = 16
01/16/2022 00:52:18 - INFO - __main__ -   Batch number = 17
01/16/2022 00:52:18 - INFO - __main__ -   Batch number = 18
01/16/2022 00:52:18 - INFO - __main__ -   Batch number = 19
01/16/2022 00:52:19 - INFO - __main__ -   Batch number = 20
01/16/2022 00:52:19 - INFO - __main__ -   Batch number = 21
01/16/2022 00:52:19 - INFO - __main__ -   Batch number = 22
01/16/2022 00:52:19 - INFO - __main__ -   Batch number = 23
01/16/2022 00:52:20 - INFO - __main__ -   Batch number = 24
01/16/2022 00:52:20 - INFO - __main__ -   Batch number = 25
01/16/2022 00:52:20 - INFO - __main__ -   Batch number = 26
01/16/2022 00:52:21 - INFO - __main__ -   Batch number = 27
01/16/2022 00:52:21 - INFO - __main__ -   Batch number = 28
01/16/2022 00:52:21 - INFO - __main__ -   Batch number = 29
01/16/2022 00:52:21 - INFO - __main__ -   Batch number = 30
01/16/2022 00:52:22 - INFO - __main__ -   Batch number = 31
01/16/2022 00:52:22 - INFO - __main__ -   Batch number = 32
01/16/2022 00:52:22 - INFO - __main__ -   Batch number = 33
01/16/2022 00:52:23 - INFO - __main__ -   Batch number = 34
01/16/2022 00:52:23 - INFO - __main__ -   Batch number = 35
01/16/2022 00:52:23 - INFO - __main__ -   Batch number = 36
01/16/2022 00:52:23 - INFO - __main__ -   Batch number = 37
01/16/2022 00:52:24 - INFO - __main__ -   Batch number = 38
01/16/2022 00:52:24 - INFO - __main__ -   Batch number = 39
01/16/2022 00:52:24 - INFO - __main__ -   Batch number = 40
01/16/2022 00:52:25 - INFO - __main__ -   Batch number = 41
01/16/2022 00:52:25 - INFO - __main__ -   Batch number = 42
01/16/2022 00:52:25 - INFO - __main__ -   Batch number = 43
01/16/2022 00:52:25 - INFO - __main__ -   Batch number = 44
01/16/2022 00:52:26 - INFO - __main__ -   Batch number = 45
01/16/2022 00:52:26 - INFO - __main__ -   Batch number = 46
01/16/2022 00:52:26 - INFO - __main__ -   Batch number = 47
01/16/2022 00:52:27 - INFO - __main__ -   Batch number = 48
01/16/2022 00:52:27 - INFO - __main__ -   ***** Evaluation result  in fo *****
01/16/2022 00:52:27 - INFO - __main__ -     f1 = 0.7196669088470352
01/16/2022 00:52:27 - INFO - __main__ -     loss = 1.039465958873431
01/16/2022 00:52:27 - INFO - __main__ -     precision = 0.7287492925863045
01/16/2022 00:52:27 - INFO - __main__ -     recall = 0.7108081254139986
01/16/2022 00:52:29 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/16/2022 00:52:29 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/16/2022 00:52:29 - INFO - __main__ -   Seed = 3
01/16/2022 00:52:29 - INFO - root -   save model
01/16/2022 00:52:29 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fo', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/16/2022 00:52:29 - INFO - __main__ -   Loading pretrained model and tokenizer
01/16/2022 00:52:32 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/16/2022 00:52:38 - INFO - __main__ -   Using lang2id = None
01/16/2022 00:52:38 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/16/2022 00:52:38 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
01/16/2022 00:52:38 - INFO - root -   Trying to decide if add adapter
01/16/2022 00:52:38 - INFO - root -   loading task adapter
01/16/2022 00:52:38 - INFO - root -   loading lang adpater is/wiki@ukp,en/wiki@ukp,se/wiki@ukp,et/wiki@ukp,fi/wiki@ukp,fr/wiki@ukp,kv/wiki@ukp,cs/wiki@ukp,de/wiki@ukp,eu/wiki@ukp
01/16/2022 00:52:38 - INFO - __main__ -   Adapter Languages : ['is', 'en', 'se', 'et', 'fi', 'fr', 'kv', 'cs', 'de', 'eu'], Length : 10
01/16/2022 00:52:38 - INFO - __main__ -   Adapter Names ['is/wiki@ukp', 'en/wiki@ukp', 'se/wiki@ukp', 'et/wiki@ukp', 'fi/wiki@ukp', 'fr/wiki@ukp', 'kv/wiki@ukp', 'cs/wiki@ukp', 'de/wiki@ukp', 'eu/wiki@ukp'], Length : 10
01/16/2022 00:52:38 - INFO - __main__ -   Language = is
01/16/2022 00:52:38 - INFO - __main__ -   Adapter Name = is/wiki@ukp
01/16/2022 00:52:54 - INFO - __main__ -   Language = en
01/16/2022 00:52:54 - INFO - __main__ -   Adapter Name = en/wiki@ukp
01/16/2022 00:53:09 - INFO - __main__ -   Language = se
01/16/2022 00:53:09 - INFO - __main__ -   Adapter Name = se/wiki@ukp
01/16/2022 00:53:26 - INFO - __main__ -   Language = et
01/16/2022 00:53:26 - INFO - __main__ -   Adapter Name = et/wiki@ukp
01/16/2022 00:53:42 - INFO - __main__ -   Language = fi
01/16/2022 00:53:42 - INFO - __main__ -   Adapter Name = fi/wiki@ukp
01/16/2022 00:53:57 - INFO - __main__ -   Language = fr
01/16/2022 00:53:57 - INFO - __main__ -   Adapter Name = fr/wiki@ukp
01/16/2022 00:54:05 - INFO - __main__ -   Language = kv
01/16/2022 00:54:05 - INFO - __main__ -   Adapter Name = kv/wiki@ukp
01/16/2022 00:54:15 - INFO - __main__ -   Language = cs
01/16/2022 00:54:15 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
01/16/2022 00:54:33 - INFO - __main__ -   Language = de
01/16/2022 00:54:33 - INFO - __main__ -   Adapter Name = de/wiki@ukp
01/16/2022 00:54:41 - INFO - __main__ -   Language = eu
01/16/2022 00:54:41 - INFO - __main__ -   Adapter Name = eu/wiki@ukp
01/16/2022 00:54:55 - INFO - __main__ -   Args Adapter Weight = equal
01/16/2022 00:54:55 - INFO - __main__ -   Adapter Languages = ['is', 'en', 'se', 'et', 'fi', 'fr', 'kv', 'cs', 'de', 'eu']
01/16/2022 00:54:55 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/16/2022 00:54:55 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/16/2022 00:54:55 - INFO - __main__ -   Length of Adapter Weights = 10
01/16/2022 00:54:55 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/cached_test_fo_bert-base-multilingual-cased_128
01/16/2022 00:54:55 - INFO - __main__ -   ***** Running evaluation  in fo *****
01/16/2022 00:54:55 - INFO - __main__ -     Num examples = 1516
01/16/2022 00:54:55 - INFO - __main__ -     Batch size = 32
01/16/2022 00:54:55 - INFO - __main__ -   Batch number = 1
01/16/2022 00:54:55 - INFO - __main__ -   Batch number = 2
01/16/2022 00:54:55 - INFO - __main__ -   Batch number = 3
01/16/2022 00:54:56 - INFO - __main__ -   Batch number = 4
01/16/2022 00:54:56 - INFO - __main__ -   Batch number = 5
01/16/2022 00:54:56 - INFO - __main__ -   Batch number = 6
01/16/2022 00:54:57 - INFO - __main__ -   Batch number = 7
01/16/2022 00:54:57 - INFO - __main__ -   Batch number = 8
01/16/2022 00:54:57 - INFO - __main__ -   Batch number = 9
01/16/2022 00:54:57 - INFO - __main__ -   Batch number = 10
01/16/2022 00:54:58 - INFO - __main__ -   Batch number = 11
01/16/2022 00:54:58 - INFO - __main__ -   Batch number = 12
01/16/2022 00:54:58 - INFO - __main__ -   Batch number = 13
01/16/2022 00:54:59 - INFO - __main__ -   Batch number = 14
01/16/2022 00:54:59 - INFO - __main__ -   Batch number = 15
01/16/2022 00:54:59 - INFO - __main__ -   Batch number = 16
01/16/2022 00:54:59 - INFO - __main__ -   Batch number = 17
01/16/2022 00:55:00 - INFO - __main__ -   Batch number = 18
01/16/2022 00:55:00 - INFO - __main__ -   Batch number = 19
01/16/2022 00:55:00 - INFO - __main__ -   Batch number = 20
01/16/2022 00:55:00 - INFO - __main__ -   Batch number = 21
01/16/2022 00:55:01 - INFO - __main__ -   Batch number = 22
01/16/2022 00:55:01 - INFO - __main__ -   Batch number = 23
01/16/2022 00:55:01 - INFO - __main__ -   Batch number = 24
01/16/2022 00:55:02 - INFO - __main__ -   Batch number = 25
01/16/2022 00:55:02 - INFO - __main__ -   Batch number = 26
01/16/2022 00:55:02 - INFO - __main__ -   Batch number = 27
01/16/2022 00:55:03 - INFO - __main__ -   Batch number = 28
01/16/2022 00:55:03 - INFO - __main__ -   Batch number = 29
01/16/2022 00:55:03 - INFO - __main__ -   Batch number = 30
01/16/2022 00:55:04 - INFO - __main__ -   Batch number = 31
01/16/2022 00:55:04 - INFO - __main__ -   Batch number = 32
01/16/2022 00:55:04 - INFO - __main__ -   Batch number = 33
01/16/2022 00:55:05 - INFO - __main__ -   Batch number = 34
01/16/2022 00:55:05 - INFO - __main__ -   Batch number = 35
01/16/2022 00:55:05 - INFO - __main__ -   Batch number = 36
01/16/2022 00:55:05 - INFO - __main__ -   Batch number = 37
01/16/2022 00:55:06 - INFO - __main__ -   Batch number = 38
01/16/2022 00:55:06 - INFO - __main__ -   Batch number = 39
01/16/2022 00:55:06 - INFO - __main__ -   Batch number = 40
01/16/2022 00:55:07 - INFO - __main__ -   Batch number = 41
01/16/2022 00:55:07 - INFO - __main__ -   Batch number = 42
01/16/2022 00:55:07 - INFO - __main__ -   Batch number = 43
01/16/2022 00:55:07 - INFO - __main__ -   Batch number = 44
01/16/2022 00:55:08 - INFO - __main__ -   Batch number = 45
01/16/2022 00:55:08 - INFO - __main__ -   Batch number = 46
01/16/2022 00:55:08 - INFO - __main__ -   Batch number = 47
01/16/2022 00:55:09 - INFO - __main__ -   Batch number = 48
01/16/2022 00:55:09 - INFO - __main__ -   ***** Evaluation result  in fo *****
01/16/2022 00:55:09 - INFO - __main__ -     f1 = 0.748121875610914
01/16/2022 00:55:09 - INFO - __main__ -     loss = 0.8777679403622946
01/16/2022 00:55:09 - INFO - __main__ -     precision = 0.7571081340794754
01/16/2022 00:55:09 - INFO - __main__ -     recall = 0.7393464340914109
01/16/2022 00:55:11 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='no', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/16/2022 00:55:11 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/16/2022 00:55:11 - INFO - __main__ -   Seed = 1
01/16/2022 00:55:11 - INFO - root -   save model
01/16/2022 00:55:11 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='no', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/16/2022 00:55:11 - INFO - __main__ -   Loading pretrained model and tokenizer
01/16/2022 00:55:14 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/16/2022 00:55:20 - INFO - __main__ -   Using lang2id = None
01/16/2022 00:55:20 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/16/2022 00:55:20 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
01/16/2022 00:55:20 - INFO - root -   Trying to decide if add adapter
01/16/2022 00:55:20 - INFO - root -   loading task adapter
01/16/2022 00:55:20 - INFO - root -   loading lang adpater et/wiki@ukp,lv/wiki@ukp,fi/wiki@ukp,en/wiki@ukp,se/wiki@ukp,cs/wiki@ukp,de/wiki@ukp,fr/wiki@ukp,is/wiki@ukp,hu/wiki@ukp
01/16/2022 00:55:20 - INFO - __main__ -   Adapter Languages : ['et', 'lv', 'fi', 'en', 'se', 'cs', 'de', 'fr', 'is', 'hu'], Length : 10
01/16/2022 00:55:20 - INFO - __main__ -   Adapter Names ['et/wiki@ukp', 'lv/wiki@ukp', 'fi/wiki@ukp', 'en/wiki@ukp', 'se/wiki@ukp', 'cs/wiki@ukp', 'de/wiki@ukp', 'fr/wiki@ukp', 'is/wiki@ukp', 'hu/wiki@ukp'], Length : 10
01/16/2022 00:55:20 - INFO - __main__ -   Language = et
01/16/2022 00:55:20 - INFO - __main__ -   Adapter Name = et/wiki@ukp
01/16/2022 00:55:35 - INFO - __main__ -   Language = lv
01/16/2022 00:55:35 - INFO - __main__ -   Adapter Name = lv/wiki@ukp
01/16/2022 00:55:52 - INFO - __main__ -   Language = fi
01/16/2022 00:55:52 - INFO - __main__ -   Adapter Name = fi/wiki@ukp
01/16/2022 00:56:07 - INFO - __main__ -   Language = en
01/16/2022 00:56:07 - INFO - __main__ -   Adapter Name = en/wiki@ukp
01/16/2022 00:56:15 - INFO - __main__ -   Language = se
01/16/2022 00:56:15 - INFO - __main__ -   Adapter Name = se/wiki@ukp
01/16/2022 00:56:32 - INFO - __main__ -   Language = cs
01/16/2022 00:56:32 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
01/16/2022 00:56:41 - INFO - __main__ -   Language = de
01/16/2022 00:56:41 - INFO - __main__ -   Adapter Name = de/wiki@ukp
01/16/2022 00:56:56 - INFO - __main__ -   Language = fr
01/16/2022 00:56:56 - INFO - __main__ -   Adapter Name = fr/wiki@ukp
01/16/2022 00:57:11 - INFO - __main__ -   Language = is
01/16/2022 00:57:11 - INFO - __main__ -   Adapter Name = is/wiki@ukp
01/16/2022 00:57:26 - INFO - __main__ -   Language = hu
01/16/2022 00:57:26 - INFO - __main__ -   Adapter Name = hu/wiki@ukp
01/16/2022 00:57:47 - INFO - __main__ -   Args Adapter Weight = equal
01/16/2022 00:57:47 - INFO - __main__ -   Adapter Languages = ['et', 'lv', 'fi', 'en', 'se', 'cs', 'de', 'fr', 'is', 'hu']
01/16/2022 00:57:47 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/16/2022 00:57:47 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/16/2022 00:57:47 - INFO - __main__ -   Length of Adapter Weights = 10
01/16/2022 00:57:47 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/cached_test_no_bert-base-multilingual-cased_128
01/16/2022 00:57:47 - INFO - __main__ -   ***** Running evaluation  in no *****
01/16/2022 00:57:47 - INFO - __main__ -     Num examples = 4408
01/16/2022 00:57:47 - INFO - __main__ -     Batch size = 32
01/16/2022 00:57:47 - INFO - __main__ -   Batch number = 1
01/16/2022 00:57:48 - INFO - __main__ -   Batch number = 2
01/16/2022 00:57:48 - INFO - __main__ -   Batch number = 3
01/16/2022 00:57:48 - INFO - __main__ -   Batch number = 4
01/16/2022 00:57:49 - INFO - __main__ -   Batch number = 5
01/16/2022 00:57:49 - INFO - __main__ -   Batch number = 6
01/16/2022 00:57:49 - INFO - __main__ -   Batch number = 7
01/16/2022 00:57:49 - INFO - __main__ -   Batch number = 8
01/16/2022 00:57:50 - INFO - __main__ -   Batch number = 9
01/16/2022 00:57:50 - INFO - __main__ -   Batch number = 10
01/16/2022 00:57:50 - INFO - __main__ -   Batch number = 11
01/16/2022 00:57:51 - INFO - __main__ -   Batch number = 12
01/16/2022 00:57:51 - INFO - __main__ -   Batch number = 13
01/16/2022 00:57:51 - INFO - __main__ -   Batch number = 14
01/16/2022 00:57:51 - INFO - __main__ -   Batch number = 15
01/16/2022 00:57:52 - INFO - __main__ -   Batch number = 16
01/16/2022 00:57:52 - INFO - __main__ -   Batch number = 17
01/16/2022 00:57:52 - INFO - __main__ -   Batch number = 18
01/16/2022 00:57:53 - INFO - __main__ -   Batch number = 19
01/16/2022 00:57:53 - INFO - __main__ -   Batch number = 20
01/16/2022 00:57:53 - INFO - __main__ -   Batch number = 21
01/16/2022 00:57:53 - INFO - __main__ -   Batch number = 22
01/16/2022 00:57:54 - INFO - __main__ -   Batch number = 23
01/16/2022 00:57:54 - INFO - __main__ -   Batch number = 24
01/16/2022 00:57:54 - INFO - __main__ -   Batch number = 25
01/16/2022 00:57:55 - INFO - __main__ -   Batch number = 26
01/16/2022 00:57:55 - INFO - __main__ -   Batch number = 27
01/16/2022 00:57:55 - INFO - __main__ -   Batch number = 28
01/16/2022 00:57:55 - INFO - __main__ -   Batch number = 29
01/16/2022 00:57:56 - INFO - __main__ -   Batch number = 30
01/16/2022 00:57:56 - INFO - __main__ -   Batch number = 31
01/16/2022 00:57:56 - INFO - __main__ -   Batch number = 32
01/16/2022 00:57:57 - INFO - __main__ -   Batch number = 33
01/16/2022 00:57:57 - INFO - __main__ -   Batch number = 34
01/16/2022 00:57:57 - INFO - __main__ -   Batch number = 35
01/16/2022 00:57:57 - INFO - __main__ -   Batch number = 36
01/16/2022 00:57:58 - INFO - __main__ -   Batch number = 37
01/16/2022 00:57:58 - INFO - __main__ -   Batch number = 38
01/16/2022 00:57:58 - INFO - __main__ -   Batch number = 39
01/16/2022 00:57:59 - INFO - __main__ -   Batch number = 40
01/16/2022 00:57:59 - INFO - __main__ -   Batch number = 41
01/16/2022 00:57:59 - INFO - __main__ -   Batch number = 42
01/16/2022 00:57:59 - INFO - __main__ -   Batch number = 43
01/16/2022 00:58:00 - INFO - __main__ -   Batch number = 44
01/16/2022 00:58:00 - INFO - __main__ -   Batch number = 45
01/16/2022 00:58:00 - INFO - __main__ -   Batch number = 46
01/16/2022 00:58:01 - INFO - __main__ -   Batch number = 47
01/16/2022 00:58:01 - INFO - __main__ -   Batch number = 48
01/16/2022 00:58:01 - INFO - __main__ -   Batch number = 49
01/16/2022 00:58:01 - INFO - __main__ -   Batch number = 50
01/16/2022 00:58:02 - INFO - __main__ -   Batch number = 51
01/16/2022 00:58:02 - INFO - __main__ -   Batch number = 52
01/16/2022 00:58:02 - INFO - __main__ -   Batch number = 53
01/16/2022 00:58:03 - INFO - __main__ -   Batch number = 54
01/16/2022 00:58:03 - INFO - __main__ -   Batch number = 55
01/16/2022 00:58:03 - INFO - __main__ -   Batch number = 56
01/16/2022 00:58:04 - INFO - __main__ -   Batch number = 57
01/16/2022 00:58:04 - INFO - __main__ -   Batch number = 58
01/16/2022 00:58:04 - INFO - __main__ -   Batch number = 59
01/16/2022 00:58:04 - INFO - __main__ -   Batch number = 60
01/16/2022 00:58:05 - INFO - __main__ -   Batch number = 61
01/16/2022 00:58:05 - INFO - __main__ -   Batch number = 62
01/16/2022 00:58:05 - INFO - __main__ -   Batch number = 63
01/16/2022 00:58:06 - INFO - __main__ -   Batch number = 64
01/16/2022 00:58:06 - INFO - __main__ -   Batch number = 65
01/16/2022 00:58:06 - INFO - __main__ -   Batch number = 66
01/16/2022 00:58:06 - INFO - __main__ -   Batch number = 67
01/16/2022 00:58:07 - INFO - __main__ -   Batch number = 68
01/16/2022 00:58:07 - INFO - __main__ -   Batch number = 69
01/16/2022 00:58:07 - INFO - __main__ -   Batch number = 70
01/16/2022 00:58:08 - INFO - __main__ -   Batch number = 71
01/16/2022 00:58:08 - INFO - __main__ -   Batch number = 72
01/16/2022 00:58:08 - INFO - __main__ -   Batch number = 73
01/16/2022 00:58:08 - INFO - __main__ -   Batch number = 74
01/16/2022 00:58:09 - INFO - __main__ -   Batch number = 75
01/16/2022 00:58:09 - INFO - __main__ -   Batch number = 76
01/16/2022 00:58:09 - INFO - __main__ -   Batch number = 77
01/16/2022 00:58:10 - INFO - __main__ -   Batch number = 78
01/16/2022 00:58:10 - INFO - __main__ -   Batch number = 79
01/16/2022 00:58:10 - INFO - __main__ -   Batch number = 80
01/16/2022 00:58:10 - INFO - __main__ -   Batch number = 81
01/16/2022 00:58:11 - INFO - __main__ -   Batch number = 82
01/16/2022 00:58:11 - INFO - __main__ -   Batch number = 83
01/16/2022 00:58:11 - INFO - __main__ -   Batch number = 84
01/16/2022 00:58:12 - INFO - __main__ -   Batch number = 85
01/16/2022 00:58:12 - INFO - __main__ -   Batch number = 86
01/16/2022 00:58:12 - INFO - __main__ -   Batch number = 87
01/16/2022 00:58:13 - INFO - __main__ -   Batch number = 88
01/16/2022 00:58:13 - INFO - __main__ -   Batch number = 89
01/16/2022 00:58:13 - INFO - __main__ -   Batch number = 90
01/16/2022 00:58:13 - INFO - __main__ -   Batch number = 91
01/16/2022 00:58:14 - INFO - __main__ -   Batch number = 92
01/16/2022 00:58:14 - INFO - __main__ -   Batch number = 93
01/16/2022 00:58:14 - INFO - __main__ -   Batch number = 94
01/16/2022 00:58:15 - INFO - __main__ -   Batch number = 95
01/16/2022 00:58:15 - INFO - __main__ -   Batch number = 96
01/16/2022 00:58:15 - INFO - __main__ -   Batch number = 97
01/16/2022 00:58:15 - INFO - __main__ -   Batch number = 98
01/16/2022 00:58:16 - INFO - __main__ -   Batch number = 99
01/16/2022 00:58:16 - INFO - __main__ -   Batch number = 100
01/16/2022 00:58:16 - INFO - __main__ -   Batch number = 101
01/16/2022 00:58:17 - INFO - __main__ -   Batch number = 102
01/16/2022 00:58:17 - INFO - __main__ -   Batch number = 103
01/16/2022 00:58:17 - INFO - __main__ -   Batch number = 104
01/16/2022 00:58:18 - INFO - __main__ -   Batch number = 105
01/16/2022 00:58:18 - INFO - __main__ -   Batch number = 106
01/16/2022 00:58:18 - INFO - __main__ -   Batch number = 107
01/16/2022 00:58:19 - INFO - __main__ -   Batch number = 108
01/16/2022 00:58:19 - INFO - __main__ -   Batch number = 109
01/16/2022 00:58:19 - INFO - __main__ -   Batch number = 110
01/16/2022 00:58:19 - INFO - __main__ -   Batch number = 111
01/16/2022 00:58:20 - INFO - __main__ -   Batch number = 112
01/16/2022 00:58:20 - INFO - __main__ -   Batch number = 113
01/16/2022 00:58:20 - INFO - __main__ -   Batch number = 114
01/16/2022 00:58:21 - INFO - __main__ -   Batch number = 115
01/16/2022 00:58:21 - INFO - __main__ -   Batch number = 116
01/16/2022 00:58:21 - INFO - __main__ -   Batch number = 117
01/16/2022 00:58:22 - INFO - __main__ -   Batch number = 118
01/16/2022 00:58:22 - INFO - __main__ -   Batch number = 119
01/16/2022 00:58:22 - INFO - __main__ -   Batch number = 120
01/16/2022 00:58:22 - INFO - __main__ -   Batch number = 121
01/16/2022 00:58:23 - INFO - __main__ -   Batch number = 122
01/16/2022 00:58:23 - INFO - __main__ -   Batch number = 123
01/16/2022 00:58:23 - INFO - __main__ -   Batch number = 124
01/16/2022 00:58:24 - INFO - __main__ -   Batch number = 125
01/16/2022 00:58:24 - INFO - __main__ -   Batch number = 126
01/16/2022 00:58:24 - INFO - __main__ -   Batch number = 127
01/16/2022 00:58:24 - INFO - __main__ -   Batch number = 128
01/16/2022 00:58:25 - INFO - __main__ -   Batch number = 129
01/16/2022 00:58:25 - INFO - __main__ -   Batch number = 130
01/16/2022 00:58:25 - INFO - __main__ -   Batch number = 131
01/16/2022 00:58:26 - INFO - __main__ -   Batch number = 132
01/16/2022 00:58:26 - INFO - __main__ -   Batch number = 133
01/16/2022 00:58:26 - INFO - __main__ -   Batch number = 134
01/16/2022 00:58:27 - INFO - __main__ -   Batch number = 135
01/16/2022 00:58:27 - INFO - __main__ -   Batch number = 136
01/16/2022 00:58:27 - INFO - __main__ -   Batch number = 137
01/16/2022 00:58:27 - INFO - __main__ -   Batch number = 138
01/16/2022 00:58:29 - INFO - __main__ -   ***** Evaluation result  in no *****
01/16/2022 00:58:29 - INFO - __main__ -     f1 = 0.8361263123758564
01/16/2022 00:58:29 - INFO - __main__ -     loss = 0.5474935385628023
01/16/2022 00:58:29 - INFO - __main__ -     precision = 0.8343472914374475
01/16/2022 00:58:29 - INFO - __main__ -     recall = 0.8379129360913863
01/16/2022 00:58:31 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='no', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/16/2022 00:58:31 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/16/2022 00:58:31 - INFO - __main__ -   Seed = 2
01/16/2022 00:58:31 - INFO - root -   save model
01/16/2022 00:58:31 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='no', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/16/2022 00:58:31 - INFO - __main__ -   Loading pretrained model and tokenizer
01/16/2022 00:58:34 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/16/2022 00:58:40 - INFO - __main__ -   Using lang2id = None
01/16/2022 00:58:40 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/16/2022 00:58:40 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
01/16/2022 00:58:40 - INFO - root -   Trying to decide if add adapter
01/16/2022 00:58:40 - INFO - root -   loading task adapter
01/16/2022 00:58:40 - INFO - root -   loading lang adpater et/wiki@ukp,lv/wiki@ukp,fi/wiki@ukp,en/wiki@ukp,se/wiki@ukp,cs/wiki@ukp,de/wiki@ukp,fr/wiki@ukp,is/wiki@ukp,hu/wiki@ukp
01/16/2022 00:58:40 - INFO - __main__ -   Adapter Languages : ['et', 'lv', 'fi', 'en', 'se', 'cs', 'de', 'fr', 'is', 'hu'], Length : 10
01/16/2022 00:58:40 - INFO - __main__ -   Adapter Names ['et/wiki@ukp', 'lv/wiki@ukp', 'fi/wiki@ukp', 'en/wiki@ukp', 'se/wiki@ukp', 'cs/wiki@ukp', 'de/wiki@ukp', 'fr/wiki@ukp', 'is/wiki@ukp', 'hu/wiki@ukp'], Length : 10
01/16/2022 00:58:40 - INFO - __main__ -   Language = et
01/16/2022 00:58:40 - INFO - __main__ -   Adapter Name = et/wiki@ukp
01/16/2022 00:58:48 - INFO - __main__ -   Language = lv
01/16/2022 00:58:48 - INFO - __main__ -   Adapter Name = lv/wiki@ukp
01/16/2022 00:58:50 - INFO - __main__ -   Language = fi
01/16/2022 00:58:50 - INFO - __main__ -   Adapter Name = fi/wiki@ukp
01/16/2022 00:58:59 - INFO - __main__ -   Language = en
01/16/2022 00:58:59 - INFO - __main__ -   Adapter Name = en/wiki@ukp
01/16/2022 00:58:59 - INFO - __main__ -   Language = se
01/16/2022 00:58:59 - INFO - __main__ -   Adapter Name = se/wiki@ukp
01/16/2022 00:59:15 - INFO - __main__ -   Language = cs
01/16/2022 00:59:15 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
01/16/2022 00:59:24 - INFO - __main__ -   Language = de
01/16/2022 00:59:24 - INFO - __main__ -   Adapter Name = de/wiki@ukp
01/16/2022 00:59:25 - INFO - __main__ -   Language = fr
01/16/2022 00:59:25 - INFO - __main__ -   Adapter Name = fr/wiki@ukp
01/16/2022 00:59:26 - INFO - __main__ -   Language = is
01/16/2022 00:59:26 - INFO - __main__ -   Adapter Name = is/wiki@ukp
01/16/2022 00:59:34 - INFO - __main__ -   Language = hu
01/16/2022 00:59:34 - INFO - __main__ -   Adapter Name = hu/wiki@ukp
01/16/2022 00:59:39 - INFO - __main__ -   Args Adapter Weight = equal
01/16/2022 00:59:39 - INFO - __main__ -   Adapter Languages = ['et', 'lv', 'fi', 'en', 'se', 'cs', 'de', 'fr', 'is', 'hu']
01/16/2022 00:59:39 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/16/2022 00:59:39 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/16/2022 00:59:39 - INFO - __main__ -   Length of Adapter Weights = 10
01/16/2022 00:59:39 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/cached_test_no_bert-base-multilingual-cased_128
01/16/2022 00:59:39 - INFO - __main__ -   ***** Running evaluation  in no *****
01/16/2022 00:59:39 - INFO - __main__ -     Num examples = 4408
01/16/2022 00:59:39 - INFO - __main__ -     Batch size = 32
01/16/2022 00:59:39 - INFO - __main__ -   Batch number = 1
01/16/2022 00:59:40 - INFO - __main__ -   Batch number = 2
01/16/2022 00:59:40 - INFO - __main__ -   Batch number = 3
01/16/2022 00:59:40 - INFO - __main__ -   Batch number = 4
01/16/2022 00:59:40 - INFO - __main__ -   Batch number = 5
01/16/2022 00:59:41 - INFO - __main__ -   Batch number = 6
01/16/2022 00:59:41 - INFO - __main__ -   Batch number = 7
01/16/2022 00:59:41 - INFO - __main__ -   Batch number = 8
01/16/2022 00:59:42 - INFO - __main__ -   Batch number = 9
01/16/2022 00:59:42 - INFO - __main__ -   Batch number = 10
01/16/2022 00:59:42 - INFO - __main__ -   Batch number = 11
01/16/2022 00:59:42 - INFO - __main__ -   Batch number = 12
01/16/2022 00:59:43 - INFO - __main__ -   Batch number = 13
01/16/2022 00:59:43 - INFO - __main__ -   Batch number = 14
01/16/2022 00:59:43 - INFO - __main__ -   Batch number = 15
01/16/2022 00:59:44 - INFO - __main__ -   Batch number = 16
01/16/2022 00:59:44 - INFO - __main__ -   Batch number = 17
01/16/2022 00:59:44 - INFO - __main__ -   Batch number = 18
01/16/2022 00:59:44 - INFO - __main__ -   Batch number = 19
01/16/2022 00:59:45 - INFO - __main__ -   Batch number = 20
01/16/2022 00:59:45 - INFO - __main__ -   Batch number = 21
01/16/2022 00:59:45 - INFO - __main__ -   Batch number = 22
01/16/2022 00:59:46 - INFO - __main__ -   Batch number = 23
01/16/2022 00:59:46 - INFO - __main__ -   Batch number = 24
01/16/2022 00:59:46 - INFO - __main__ -   Batch number = 25
01/16/2022 00:59:46 - INFO - __main__ -   Batch number = 26
01/16/2022 00:59:47 - INFO - __main__ -   Batch number = 27
01/16/2022 00:59:47 - INFO - __main__ -   Batch number = 28
01/16/2022 00:59:47 - INFO - __main__ -   Batch number = 29
01/16/2022 00:59:48 - INFO - __main__ -   Batch number = 30
01/16/2022 00:59:48 - INFO - __main__ -   Batch number = 31
01/16/2022 00:59:48 - INFO - __main__ -   Batch number = 32
01/16/2022 00:59:49 - INFO - __main__ -   Batch number = 33
01/16/2022 00:59:49 - INFO - __main__ -   Batch number = 34
01/16/2022 00:59:49 - INFO - __main__ -   Batch number = 35
01/16/2022 00:59:49 - INFO - __main__ -   Batch number = 36
01/16/2022 00:59:50 - INFO - __main__ -   Batch number = 37
01/16/2022 00:59:50 - INFO - __main__ -   Batch number = 38
01/16/2022 00:59:50 - INFO - __main__ -   Batch number = 39
01/16/2022 00:59:51 - INFO - __main__ -   Batch number = 40
01/16/2022 00:59:51 - INFO - __main__ -   Batch number = 41
01/16/2022 00:59:51 - INFO - __main__ -   Batch number = 42
01/16/2022 00:59:51 - INFO - __main__ -   Batch number = 43
01/16/2022 00:59:52 - INFO - __main__ -   Batch number = 44
01/16/2022 00:59:52 - INFO - __main__ -   Batch number = 45
01/16/2022 00:59:52 - INFO - __main__ -   Batch number = 46
01/16/2022 00:59:53 - INFO - __main__ -   Batch number = 47
01/16/2022 00:59:53 - INFO - __main__ -   Batch number = 48
01/16/2022 00:59:53 - INFO - __main__ -   Batch number = 49
01/16/2022 00:59:53 - INFO - __main__ -   Batch number = 50
01/16/2022 00:59:54 - INFO - __main__ -   Batch number = 51
01/16/2022 00:59:54 - INFO - __main__ -   Batch number = 52
01/16/2022 00:59:54 - INFO - __main__ -   Batch number = 53
01/16/2022 00:59:55 - INFO - __main__ -   Batch number = 54
01/16/2022 00:59:55 - INFO - __main__ -   Batch number = 55
01/16/2022 00:59:55 - INFO - __main__ -   Batch number = 56
01/16/2022 00:59:55 - INFO - __main__ -   Batch number = 57
01/16/2022 00:59:56 - INFO - __main__ -   Batch number = 58
01/16/2022 00:59:56 - INFO - __main__ -   Batch number = 59
01/16/2022 00:59:56 - INFO - __main__ -   Batch number = 60
01/16/2022 00:59:57 - INFO - __main__ -   Batch number = 61
01/16/2022 00:59:57 - INFO - __main__ -   Batch number = 62
01/16/2022 00:59:57 - INFO - __main__ -   Batch number = 63
01/16/2022 00:59:57 - INFO - __main__ -   Batch number = 64
01/16/2022 00:59:58 - INFO - __main__ -   Batch number = 65
01/16/2022 00:59:58 - INFO - __main__ -   Batch number = 66
01/16/2022 00:59:58 - INFO - __main__ -   Batch number = 67
01/16/2022 00:59:59 - INFO - __main__ -   Batch number = 68
01/16/2022 00:59:59 - INFO - __main__ -   Batch number = 69
01/16/2022 00:59:59 - INFO - __main__ -   Batch number = 70
01/16/2022 01:00:00 - INFO - __main__ -   Batch number = 71
01/16/2022 01:00:00 - INFO - __main__ -   Batch number = 72
01/16/2022 01:00:00 - INFO - __main__ -   Batch number = 73
01/16/2022 01:00:00 - INFO - __main__ -   Batch number = 74
01/16/2022 01:00:01 - INFO - __main__ -   Batch number = 75
01/16/2022 01:00:01 - INFO - __main__ -   Batch number = 76
01/16/2022 01:00:01 - INFO - __main__ -   Batch number = 77
01/16/2022 01:00:02 - INFO - __main__ -   Batch number = 78
01/16/2022 01:00:02 - INFO - __main__ -   Batch number = 79
01/16/2022 01:00:02 - INFO - __main__ -   Batch number = 80
01/16/2022 01:00:02 - INFO - __main__ -   Batch number = 81
01/16/2022 01:00:03 - INFO - __main__ -   Batch number = 82
01/16/2022 01:00:03 - INFO - __main__ -   Batch number = 83
01/16/2022 01:00:03 - INFO - __main__ -   Batch number = 84
01/16/2022 01:00:04 - INFO - __main__ -   Batch number = 85
01/16/2022 01:00:04 - INFO - __main__ -   Batch number = 86
01/16/2022 01:00:04 - INFO - __main__ -   Batch number = 87
01/16/2022 01:00:05 - INFO - __main__ -   Batch number = 88
01/16/2022 01:00:05 - INFO - __main__ -   Batch number = 89
01/16/2022 01:00:05 - INFO - __main__ -   Batch number = 90
01/16/2022 01:00:05 - INFO - __main__ -   Batch number = 91
01/16/2022 01:00:06 - INFO - __main__ -   Batch number = 92
01/16/2022 01:00:06 - INFO - __main__ -   Batch number = 93
01/16/2022 01:00:06 - INFO - __main__ -   Batch number = 94
01/16/2022 01:00:07 - INFO - __main__ -   Batch number = 95
01/16/2022 01:00:07 - INFO - __main__ -   Batch number = 96
01/16/2022 01:00:07 - INFO - __main__ -   Batch number = 97
01/16/2022 01:00:07 - INFO - __main__ -   Batch number = 98
01/16/2022 01:00:08 - INFO - __main__ -   Batch number = 99
01/16/2022 01:00:08 - INFO - __main__ -   Batch number = 100
01/16/2022 01:00:08 - INFO - __main__ -   Batch number = 101
01/16/2022 01:00:09 - INFO - __main__ -   Batch number = 102
01/16/2022 01:00:09 - INFO - __main__ -   Batch number = 103
01/16/2022 01:00:09 - INFO - __main__ -   Batch number = 104
01/16/2022 01:00:10 - INFO - __main__ -   Batch number = 105
01/16/2022 01:00:10 - INFO - __main__ -   Batch number = 106
01/16/2022 01:00:10 - INFO - __main__ -   Batch number = 107
01/16/2022 01:00:10 - INFO - __main__ -   Batch number = 108
01/16/2022 01:00:11 - INFO - __main__ -   Batch number = 109
01/16/2022 01:00:11 - INFO - __main__ -   Batch number = 110
01/16/2022 01:00:11 - INFO - __main__ -   Batch number = 111
01/16/2022 01:00:12 - INFO - __main__ -   Batch number = 112
01/16/2022 01:00:12 - INFO - __main__ -   Batch number = 113
01/16/2022 01:00:12 - INFO - __main__ -   Batch number = 114
01/16/2022 01:00:13 - INFO - __main__ -   Batch number = 115
01/16/2022 01:00:13 - INFO - __main__ -   Batch number = 116
01/16/2022 01:00:13 - INFO - __main__ -   Batch number = 117
01/16/2022 01:00:14 - INFO - __main__ -   Batch number = 118
01/16/2022 01:00:14 - INFO - __main__ -   Batch number = 119
01/16/2022 01:00:14 - INFO - __main__ -   Batch number = 120
01/16/2022 01:00:14 - INFO - __main__ -   Batch number = 121
01/16/2022 01:00:15 - INFO - __main__ -   Batch number = 122
01/16/2022 01:00:15 - INFO - __main__ -   Batch number = 123
01/16/2022 01:00:15 - INFO - __main__ -   Batch number = 124
01/16/2022 01:00:16 - INFO - __main__ -   Batch number = 125
01/16/2022 01:00:16 - INFO - __main__ -   Batch number = 126
01/16/2022 01:00:16 - INFO - __main__ -   Batch number = 127
01/16/2022 01:00:17 - INFO - __main__ -   Batch number = 128
01/16/2022 01:00:17 - INFO - __main__ -   Batch number = 129
01/16/2022 01:00:17 - INFO - __main__ -   Batch number = 130
01/16/2022 01:00:18 - INFO - __main__ -   Batch number = 131
01/16/2022 01:00:18 - INFO - __main__ -   Batch number = 132
01/16/2022 01:00:18 - INFO - __main__ -   Batch number = 133
01/16/2022 01:00:19 - INFO - __main__ -   Batch number = 134
01/16/2022 01:00:19 - INFO - __main__ -   Batch number = 135
01/16/2022 01:00:19 - INFO - __main__ -   Batch number = 136
01/16/2022 01:00:20 - INFO - __main__ -   Batch number = 137
01/16/2022 01:00:20 - INFO - __main__ -   Batch number = 138
01/16/2022 01:00:22 - INFO - __main__ -   ***** Evaluation result  in no *****
01/16/2022 01:00:22 - INFO - __main__ -     f1 = 0.8403795115695839
01/16/2022 01:00:22 - INFO - __main__ -     loss = 0.600466638047626
01/16/2022 01:00:22 - INFO - __main__ -     precision = 0.83950056753689
01/16/2022 01:00:22 - INFO - __main__ -     recall = 0.841260298012707
01/16/2022 01:00:24 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='no', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/16/2022 01:00:24 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/16/2022 01:00:24 - INFO - __main__ -   Seed = 3
01/16/2022 01:00:24 - INFO - root -   save model
01/16/2022 01:00:24 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='no', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/16/2022 01:00:24 - INFO - __main__ -   Loading pretrained model and tokenizer
01/16/2022 01:00:27 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/16/2022 01:00:32 - INFO - __main__ -   Using lang2id = None
01/16/2022 01:00:32 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/16/2022 01:00:32 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
01/16/2022 01:00:32 - INFO - root -   Trying to decide if add adapter
01/16/2022 01:00:32 - INFO - root -   loading task adapter
01/16/2022 01:00:33 - INFO - root -   loading lang adpater et/wiki@ukp,lv/wiki@ukp,fi/wiki@ukp,en/wiki@ukp,se/wiki@ukp,cs/wiki@ukp,de/wiki@ukp,fr/wiki@ukp,is/wiki@ukp,hu/wiki@ukp
01/16/2022 01:00:33 - INFO - __main__ -   Adapter Languages : ['et', 'lv', 'fi', 'en', 'se', 'cs', 'de', 'fr', 'is', 'hu'], Length : 10
01/16/2022 01:00:33 - INFO - __main__ -   Adapter Names ['et/wiki@ukp', 'lv/wiki@ukp', 'fi/wiki@ukp', 'en/wiki@ukp', 'se/wiki@ukp', 'cs/wiki@ukp', 'de/wiki@ukp', 'fr/wiki@ukp', 'is/wiki@ukp', 'hu/wiki@ukp'], Length : 10
01/16/2022 01:00:33 - INFO - __main__ -   Language = et
01/16/2022 01:00:33 - INFO - __main__ -   Adapter Name = et/wiki@ukp
01/16/2022 01:00:48 - INFO - __main__ -   Language = lv
01/16/2022 01:00:48 - INFO - __main__ -   Adapter Name = lv/wiki@ukp
01/16/2022 01:01:04 - INFO - __main__ -   Language = fi
01/16/2022 01:01:04 - INFO - __main__ -   Adapter Name = fi/wiki@ukp
01/16/2022 01:01:05 - INFO - __main__ -   Language = en
01/16/2022 01:01:05 - INFO - __main__ -   Adapter Name = en/wiki@ukp
01/16/2022 01:01:20 - INFO - __main__ -   Language = se
01/16/2022 01:01:20 - INFO - __main__ -   Adapter Name = se/wiki@ukp
01/16/2022 01:01:22 - INFO - __main__ -   Language = cs
01/16/2022 01:01:22 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
01/16/2022 01:01:39 - INFO - __main__ -   Language = de
01/16/2022 01:01:39 - INFO - __main__ -   Adapter Name = de/wiki@ukp
01/16/2022 01:01:55 - INFO - __main__ -   Language = fr
01/16/2022 01:01:55 - INFO - __main__ -   Adapter Name = fr/wiki@ukp
01/16/2022 01:02:10 - INFO - __main__ -   Language = is
01/16/2022 01:02:10 - INFO - __main__ -   Adapter Name = is/wiki@ukp
01/16/2022 01:02:25 - INFO - __main__ -   Language = hu
01/16/2022 01:02:25 - INFO - __main__ -   Adapter Name = hu/wiki@ukp
01/16/2022 01:02:45 - INFO - __main__ -   Args Adapter Weight = equal
01/16/2022 01:02:45 - INFO - __main__ -   Adapter Languages = ['et', 'lv', 'fi', 'en', 'se', 'cs', 'de', 'fr', 'is', 'hu']
01/16/2022 01:02:45 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/16/2022 01:02:45 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/16/2022 01:02:45 - INFO - __main__ -   Length of Adapter Weights = 10
01/16/2022 01:02:45 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/cached_test_no_bert-base-multilingual-cased_128
01/16/2022 01:02:45 - INFO - __main__ -   ***** Running evaluation  in no *****
01/16/2022 01:02:45 - INFO - __main__ -     Num examples = 4408
01/16/2022 01:02:45 - INFO - __main__ -     Batch size = 32
01/16/2022 01:02:45 - INFO - __main__ -   Batch number = 1
01/16/2022 01:02:46 - INFO - __main__ -   Batch number = 2
01/16/2022 01:02:46 - INFO - __main__ -   Batch number = 3
01/16/2022 01:02:46 - INFO - __main__ -   Batch number = 4
01/16/2022 01:02:46 - INFO - __main__ -   Batch number = 5
01/16/2022 01:02:47 - INFO - __main__ -   Batch number = 6
01/16/2022 01:02:47 - INFO - __main__ -   Batch number = 7
01/16/2022 01:02:47 - INFO - __main__ -   Batch number = 8
01/16/2022 01:02:47 - INFO - __main__ -   Batch number = 9
01/16/2022 01:02:48 - INFO - __main__ -   Batch number = 10
01/16/2022 01:02:48 - INFO - __main__ -   Batch number = 11
01/16/2022 01:02:48 - INFO - __main__ -   Batch number = 12
01/16/2022 01:02:49 - INFO - __main__ -   Batch number = 13
01/16/2022 01:02:49 - INFO - __main__ -   Batch number = 14
01/16/2022 01:02:49 - INFO - __main__ -   Batch number = 15
01/16/2022 01:02:49 - INFO - __main__ -   Batch number = 16
01/16/2022 01:02:50 - INFO - __main__ -   Batch number = 17
01/16/2022 01:02:50 - INFO - __main__ -   Batch number = 18
01/16/2022 01:02:50 - INFO - __main__ -   Batch number = 19
01/16/2022 01:02:51 - INFO - __main__ -   Batch number = 20
01/16/2022 01:02:51 - INFO - __main__ -   Batch number = 21
01/16/2022 01:02:51 - INFO - __main__ -   Batch number = 22
01/16/2022 01:02:51 - INFO - __main__ -   Batch number = 23
01/16/2022 01:02:52 - INFO - __main__ -   Batch number = 24
01/16/2022 01:02:52 - INFO - __main__ -   Batch number = 25
01/16/2022 01:02:52 - INFO - __main__ -   Batch number = 26
01/16/2022 01:02:53 - INFO - __main__ -   Batch number = 27
01/16/2022 01:02:53 - INFO - __main__ -   Batch number = 28
01/16/2022 01:02:53 - INFO - __main__ -   Batch number = 29
01/16/2022 01:02:53 - INFO - __main__ -   Batch number = 30
01/16/2022 01:02:54 - INFO - __main__ -   Batch number = 31
01/16/2022 01:02:54 - INFO - __main__ -   Batch number = 32
01/16/2022 01:02:54 - INFO - __main__ -   Batch number = 33
01/16/2022 01:02:55 - INFO - __main__ -   Batch number = 34
01/16/2022 01:02:55 - INFO - __main__ -   Batch number = 35
01/16/2022 01:02:55 - INFO - __main__ -   Batch number = 36
01/16/2022 01:02:55 - INFO - __main__ -   Batch number = 37
01/16/2022 01:02:56 - INFO - __main__ -   Batch number = 38
01/16/2022 01:02:56 - INFO - __main__ -   Batch number = 39
01/16/2022 01:02:56 - INFO - __main__ -   Batch number = 40
01/16/2022 01:02:56 - INFO - __main__ -   Batch number = 41
01/16/2022 01:02:57 - INFO - __main__ -   Batch number = 42
01/16/2022 01:02:57 - INFO - __main__ -   Batch number = 43
01/16/2022 01:02:57 - INFO - __main__ -   Batch number = 44
01/16/2022 01:02:58 - INFO - __main__ -   Batch number = 45
01/16/2022 01:02:58 - INFO - __main__ -   Batch number = 46
01/16/2022 01:02:58 - INFO - __main__ -   Batch number = 47
01/16/2022 01:02:58 - INFO - __main__ -   Batch number = 48
01/16/2022 01:02:59 - INFO - __main__ -   Batch number = 49
01/16/2022 01:02:59 - INFO - __main__ -   Batch number = 50
01/16/2022 01:02:59 - INFO - __main__ -   Batch number = 51
01/16/2022 01:03:00 - INFO - __main__ -   Batch number = 52
01/16/2022 01:03:00 - INFO - __main__ -   Batch number = 53
01/16/2022 01:03:00 - INFO - __main__ -   Batch number = 54
01/16/2022 01:03:00 - INFO - __main__ -   Batch number = 55
01/16/2022 01:03:01 - INFO - __main__ -   Batch number = 56
01/16/2022 01:03:01 - INFO - __main__ -   Batch number = 57
01/16/2022 01:03:01 - INFO - __main__ -   Batch number = 58
01/16/2022 01:03:02 - INFO - __main__ -   Batch number = 59
01/16/2022 01:03:02 - INFO - __main__ -   Batch number = 60
01/16/2022 01:03:02 - INFO - __main__ -   Batch number = 61
01/16/2022 01:03:02 - INFO - __main__ -   Batch number = 62
01/16/2022 01:03:03 - INFO - __main__ -   Batch number = 63
01/16/2022 01:03:03 - INFO - __main__ -   Batch number = 64
01/16/2022 01:03:03 - INFO - __main__ -   Batch number = 65
01/16/2022 01:03:04 - INFO - __main__ -   Batch number = 66
01/16/2022 01:03:04 - INFO - __main__ -   Batch number = 67
01/16/2022 01:03:04 - INFO - __main__ -   Batch number = 68
01/16/2022 01:03:05 - INFO - __main__ -   Batch number = 69
01/16/2022 01:03:05 - INFO - __main__ -   Batch number = 70
01/16/2022 01:03:05 - INFO - __main__ -   Batch number = 71
01/16/2022 01:03:05 - INFO - __main__ -   Batch number = 72
01/16/2022 01:03:06 - INFO - __main__ -   Batch number = 73
01/16/2022 01:03:06 - INFO - __main__ -   Batch number = 74
01/16/2022 01:03:06 - INFO - __main__ -   Batch number = 75
01/16/2022 01:03:07 - INFO - __main__ -   Batch number = 76
01/16/2022 01:03:07 - INFO - __main__ -   Batch number = 77
01/16/2022 01:03:07 - INFO - __main__ -   Batch number = 78
01/16/2022 01:03:07 - INFO - __main__ -   Batch number = 79
01/16/2022 01:03:08 - INFO - __main__ -   Batch number = 80
01/16/2022 01:03:08 - INFO - __main__ -   Batch number = 81
01/16/2022 01:03:08 - INFO - __main__ -   Batch number = 82
01/16/2022 01:03:09 - INFO - __main__ -   Batch number = 83
01/16/2022 01:03:09 - INFO - __main__ -   Batch number = 84
01/16/2022 01:03:09 - INFO - __main__ -   Batch number = 85
01/16/2022 01:03:09 - INFO - __main__ -   Batch number = 86
01/16/2022 01:03:10 - INFO - __main__ -   Batch number = 87
01/16/2022 01:03:10 - INFO - __main__ -   Batch number = 88
01/16/2022 01:03:10 - INFO - __main__ -   Batch number = 89
01/16/2022 01:03:11 - INFO - __main__ -   Batch number = 90
01/16/2022 01:03:11 - INFO - __main__ -   Batch number = 91
01/16/2022 01:03:11 - INFO - __main__ -   Batch number = 92
01/16/2022 01:03:11 - INFO - __main__ -   Batch number = 93
01/16/2022 01:03:12 - INFO - __main__ -   Batch number = 94
01/16/2022 01:03:12 - INFO - __main__ -   Batch number = 95
01/16/2022 01:03:12 - INFO - __main__ -   Batch number = 96
01/16/2022 01:03:13 - INFO - __main__ -   Batch number = 97
01/16/2022 01:03:13 - INFO - __main__ -   Batch number = 98
01/16/2022 01:03:13 - INFO - __main__ -   Batch number = 99
01/16/2022 01:03:14 - INFO - __main__ -   Batch number = 100
01/16/2022 01:03:14 - INFO - __main__ -   Batch number = 101
01/16/2022 01:03:14 - INFO - __main__ -   Batch number = 102
01/16/2022 01:03:14 - INFO - __main__ -   Batch number = 103
01/16/2022 01:03:15 - INFO - __main__ -   Batch number = 104
01/16/2022 01:03:15 - INFO - __main__ -   Batch number = 105
01/16/2022 01:03:15 - INFO - __main__ -   Batch number = 106
01/16/2022 01:03:16 - INFO - __main__ -   Batch number = 107
01/16/2022 01:03:16 - INFO - __main__ -   Batch number = 108
01/16/2022 01:03:16 - INFO - __main__ -   Batch number = 109
01/16/2022 01:03:16 - INFO - __main__ -   Batch number = 110
01/16/2022 01:03:17 - INFO - __main__ -   Batch number = 111
01/16/2022 01:03:17 - INFO - __main__ -   Batch number = 112
01/16/2022 01:03:17 - INFO - __main__ -   Batch number = 113
01/16/2022 01:03:18 - INFO - __main__ -   Batch number = 114
01/16/2022 01:03:18 - INFO - __main__ -   Batch number = 115
01/16/2022 01:03:18 - INFO - __main__ -   Batch number = 116
01/16/2022 01:03:19 - INFO - __main__ -   Batch number = 117
01/16/2022 01:03:19 - INFO - __main__ -   Batch number = 118
01/16/2022 01:03:19 - INFO - __main__ -   Batch number = 119
01/16/2022 01:03:19 - INFO - __main__ -   Batch number = 120
01/16/2022 01:03:20 - INFO - __main__ -   Batch number = 121
01/16/2022 01:03:20 - INFO - __main__ -   Batch number = 122
01/16/2022 01:03:20 - INFO - __main__ -   Batch number = 123
01/16/2022 01:03:21 - INFO - __main__ -   Batch number = 124
01/16/2022 01:03:21 - INFO - __main__ -   Batch number = 125
01/16/2022 01:03:21 - INFO - __main__ -   Batch number = 126
01/16/2022 01:03:21 - INFO - __main__ -   Batch number = 127
01/16/2022 01:03:22 - INFO - __main__ -   Batch number = 128
01/16/2022 01:03:22 - INFO - __main__ -   Batch number = 129
01/16/2022 01:03:22 - INFO - __main__ -   Batch number = 130
01/16/2022 01:03:23 - INFO - __main__ -   Batch number = 131
01/16/2022 01:03:23 - INFO - __main__ -   Batch number = 132
01/16/2022 01:03:23 - INFO - __main__ -   Batch number = 133
01/16/2022 01:03:24 - INFO - __main__ -   Batch number = 134
01/16/2022 01:03:24 - INFO - __main__ -   Batch number = 135
01/16/2022 01:03:24 - INFO - __main__ -   Batch number = 136
01/16/2022 01:03:24 - INFO - __main__ -   Batch number = 137
01/16/2022 01:03:25 - INFO - __main__ -   Batch number = 138
01/16/2022 01:03:27 - INFO - __main__ -   ***** Evaluation result  in no *****
01/16/2022 01:03:27 - INFO - __main__ -     f1 = 0.8459854014598541
01/16/2022 01:03:27 - INFO - __main__ -     loss = 0.556901627584644
01/16/2022 01:03:27 - INFO - __main__ -     precision = 0.8444923007173044
01/16/2022 01:03:27 - INFO - __main__ -     recall = 0.8474837912936092
01/16/2022 01:03:29 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='da', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/16/2022 01:03:29 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/16/2022 01:03:29 - INFO - __main__ -   Seed = 1
01/16/2022 01:03:29 - INFO - root -   save model
01/16/2022 01:03:29 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='da', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/16/2022 01:03:29 - INFO - __main__ -   Loading pretrained model and tokenizer
01/16/2022 01:03:32 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/16/2022 01:03:37 - INFO - __main__ -   Using lang2id = None
01/16/2022 01:03:37 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/16/2022 01:03:37 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
01/16/2022 01:03:37 - INFO - root -   Trying to decide if add adapter
01/16/2022 01:03:37 - INFO - root -   loading task adapter
01/16/2022 01:03:37 - INFO - root -   loading lang adpater cs/wiki@ukp,en/wiki@ukp,de/wiki@ukp,fr/wiki@ukp,lv/wiki@ukp,et/wiki@ukp,hu/wiki@ukp,fi/wiki@ukp,la/wiki@ukp,eu/wiki@ukp
01/16/2022 01:03:37 - INFO - __main__ -   Adapter Languages : ['cs', 'en', 'de', 'fr', 'lv', 'et', 'hu', 'fi', 'la', 'eu'], Length : 10
01/16/2022 01:03:37 - INFO - __main__ -   Adapter Names ['cs/wiki@ukp', 'en/wiki@ukp', 'de/wiki@ukp', 'fr/wiki@ukp', 'lv/wiki@ukp', 'et/wiki@ukp', 'hu/wiki@ukp', 'fi/wiki@ukp', 'la/wiki@ukp', 'eu/wiki@ukp'], Length : 10
01/16/2022 01:03:37 - INFO - __main__ -   Language = cs
01/16/2022 01:03:37 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
01/16/2022 01:03:46 - INFO - __main__ -   Language = en
01/16/2022 01:03:46 - INFO - __main__ -   Adapter Name = en/wiki@ukp
01/16/2022 01:04:01 - INFO - __main__ -   Language = de
01/16/2022 01:04:01 - INFO - __main__ -   Adapter Name = de/wiki@ukp
01/16/2022 01:04:17 - INFO - __main__ -   Language = fr
01/16/2022 01:04:17 - INFO - __main__ -   Adapter Name = fr/wiki@ukp
01/16/2022 01:04:32 - INFO - __main__ -   Language = lv
01/16/2022 01:04:32 - INFO - __main__ -   Adapter Name = lv/wiki@ukp
01/16/2022 01:04:48 - INFO - __main__ -   Language = et
01/16/2022 01:04:48 - INFO - __main__ -   Adapter Name = et/wiki@ukp
01/16/2022 01:05:03 - INFO - __main__ -   Language = hu
01/16/2022 01:05:03 - INFO - __main__ -   Adapter Name = hu/wiki@ukp
01/16/2022 01:05:12 - INFO - __main__ -   Language = fi
01/16/2022 01:05:12 - INFO - __main__ -   Adapter Name = fi/wiki@ukp
01/16/2022 01:05:20 - INFO - __main__ -   Language = la
01/16/2022 01:05:20 - INFO - __main__ -   Adapter Name = la/wiki@ukp
01/16/2022 01:05:29 - INFO - __main__ -   Language = eu
01/16/2022 01:05:29 - INFO - __main__ -   Adapter Name = eu/wiki@ukp
01/16/2022 01:05:48 - INFO - __main__ -   Args Adapter Weight = equal
01/16/2022 01:05:48 - INFO - __main__ -   Adapter Languages = ['cs', 'en', 'de', 'fr', 'lv', 'et', 'hu', 'fi', 'la', 'eu']
01/16/2022 01:05:48 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/16/2022 01:05:48 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/16/2022 01:05:48 - INFO - __main__ -   Length of Adapter Weights = 10
01/16/2022 01:05:48 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/cached_test_da_bert-base-multilingual-cased_128
01/16/2022 01:05:48 - INFO - __main__ -   ***** Running evaluation  in da *****
01/16/2022 01:05:48 - INFO - __main__ -     Num examples = 565
01/16/2022 01:05:48 - INFO - __main__ -     Batch size = 32
01/16/2022 01:05:48 - INFO - __main__ -   Batch number = 1
01/16/2022 01:05:49 - INFO - __main__ -   Batch number = 2
01/16/2022 01:05:49 - INFO - __main__ -   Batch number = 3
01/16/2022 01:05:49 - INFO - __main__ -   Batch number = 4
01/16/2022 01:05:49 - INFO - __main__ -   Batch number = 5
01/16/2022 01:05:50 - INFO - __main__ -   Batch number = 6
01/16/2022 01:05:50 - INFO - __main__ -   Batch number = 7
01/16/2022 01:05:50 - INFO - __main__ -   Batch number = 8
01/16/2022 01:05:51 - INFO - __main__ -   Batch number = 9
01/16/2022 01:05:51 - INFO - __main__ -   Batch number = 10
01/16/2022 01:05:51 - INFO - __main__ -   Batch number = 11
01/16/2022 01:05:52 - INFO - __main__ -   Batch number = 12
01/16/2022 01:05:52 - INFO - __main__ -   Batch number = 13
01/16/2022 01:05:52 - INFO - __main__ -   Batch number = 14
01/16/2022 01:05:52 - INFO - __main__ -   Batch number = 15
01/16/2022 01:05:53 - INFO - __main__ -   Batch number = 16
01/16/2022 01:05:53 - INFO - __main__ -   Batch number = 17
01/16/2022 01:05:53 - INFO - __main__ -   Batch number = 18
01/16/2022 01:05:54 - INFO - __main__ -   ***** Evaluation result  in da *****
01/16/2022 01:05:54 - INFO - __main__ -     f1 = 0.8860477453580902
01/16/2022 01:05:54 - INFO - __main__ -     loss = 0.34347332848442924
01/16/2022 01:05:54 - INFO - __main__ -     precision = 0.8864239465024945
01/16/2022 01:05:54 - INFO - __main__ -     recall = 0.8856718634001485
01/16/2022 01:05:56 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='da', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/16/2022 01:05:56 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/16/2022 01:05:56 - INFO - __main__ -   Seed = 2
01/16/2022 01:05:56 - INFO - root -   save model
01/16/2022 01:05:56 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='da', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/16/2022 01:05:56 - INFO - __main__ -   Loading pretrained model and tokenizer
01/16/2022 01:05:59 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/16/2022 01:06:04 - INFO - __main__ -   Using lang2id = None
01/16/2022 01:06:04 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/16/2022 01:06:04 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
01/16/2022 01:06:04 - INFO - root -   Trying to decide if add adapter
01/16/2022 01:06:04 - INFO - root -   loading task adapter
01/16/2022 01:06:04 - INFO - root -   loading lang adpater cs/wiki@ukp,en/wiki@ukp,de/wiki@ukp,fr/wiki@ukp,lv/wiki@ukp,et/wiki@ukp,hu/wiki@ukp,fi/wiki@ukp,la/wiki@ukp,eu/wiki@ukp
01/16/2022 01:06:04 - INFO - __main__ -   Adapter Languages : ['cs', 'en', 'de', 'fr', 'lv', 'et', 'hu', 'fi', 'la', 'eu'], Length : 10
01/16/2022 01:06:04 - INFO - __main__ -   Adapter Names ['cs/wiki@ukp', 'en/wiki@ukp', 'de/wiki@ukp', 'fr/wiki@ukp', 'lv/wiki@ukp', 'et/wiki@ukp', 'hu/wiki@ukp', 'fi/wiki@ukp', 'la/wiki@ukp', 'eu/wiki@ukp'], Length : 10
01/16/2022 01:06:04 - INFO - __main__ -   Language = cs
01/16/2022 01:06:04 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
01/16/2022 01:06:13 - INFO - __main__ -   Language = en
01/16/2022 01:06:13 - INFO - __main__ -   Adapter Name = en/wiki@ukp
01/16/2022 01:06:13 - INFO - __main__ -   Language = de
01/16/2022 01:06:13 - INFO - __main__ -   Adapter Name = de/wiki@ukp
01/16/2022 01:06:29 - INFO - __main__ -   Language = fr
01/16/2022 01:06:29 - INFO - __main__ -   Adapter Name = fr/wiki@ukp
01/16/2022 01:06:44 - INFO - __main__ -   Language = lv
01/16/2022 01:06:44 - INFO - __main__ -   Adapter Name = lv/wiki@ukp
01/16/2022 01:07:00 - INFO - __main__ -   Language = et
01/16/2022 01:07:00 - INFO - __main__ -   Adapter Name = et/wiki@ukp
01/16/2022 01:07:15 - INFO - __main__ -   Language = hu
01/16/2022 01:07:15 - INFO - __main__ -   Adapter Name = hu/wiki@ukp
01/16/2022 01:07:31 - INFO - __main__ -   Language = fi
01/16/2022 01:07:31 - INFO - __main__ -   Adapter Name = fi/wiki@ukp
01/16/2022 01:07:46 - INFO - __main__ -   Language = la
01/16/2022 01:07:46 - INFO - __main__ -   Adapter Name = la/wiki@ukp
01/16/2022 01:08:02 - INFO - __main__ -   Language = eu
01/16/2022 01:08:02 - INFO - __main__ -   Adapter Name = eu/wiki@ukp
01/16/2022 01:08:15 - INFO - __main__ -   Args Adapter Weight = equal
01/16/2022 01:08:15 - INFO - __main__ -   Adapter Languages = ['cs', 'en', 'de', 'fr', 'lv', 'et', 'hu', 'fi', 'la', 'eu']
01/16/2022 01:08:15 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/16/2022 01:08:15 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/16/2022 01:08:15 - INFO - __main__ -   Length of Adapter Weights = 10
01/16/2022 01:08:15 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/cached_test_da_bert-base-multilingual-cased_128
01/16/2022 01:08:15 - INFO - __main__ -   ***** Running evaluation  in da *****
01/16/2022 01:08:15 - INFO - __main__ -     Num examples = 565
01/16/2022 01:08:15 - INFO - __main__ -     Batch size = 32
01/16/2022 01:08:15 - INFO - __main__ -   Batch number = 1
01/16/2022 01:08:15 - INFO - __main__ -   Batch number = 2
01/16/2022 01:08:15 - INFO - __main__ -   Batch number = 3
01/16/2022 01:08:16 - INFO - __main__ -   Batch number = 4
01/16/2022 01:08:16 - INFO - __main__ -   Batch number = 5
01/16/2022 01:08:16 - INFO - __main__ -   Batch number = 6
01/16/2022 01:08:16 - INFO - __main__ -   Batch number = 7
01/16/2022 01:08:17 - INFO - __main__ -   Batch number = 8
01/16/2022 01:08:17 - INFO - __main__ -   Batch number = 9
01/16/2022 01:08:17 - INFO - __main__ -   Batch number = 10
01/16/2022 01:08:18 - INFO - __main__ -   Batch number = 11
01/16/2022 01:08:18 - INFO - __main__ -   Batch number = 12
01/16/2022 01:08:18 - INFO - __main__ -   Batch number = 13
01/16/2022 01:08:18 - INFO - __main__ -   Batch number = 14
01/16/2022 01:08:19 - INFO - __main__ -   Batch number = 15
01/16/2022 01:08:19 - INFO - __main__ -   Batch number = 16
01/16/2022 01:08:19 - INFO - __main__ -   Batch number = 17
01/16/2022 01:08:20 - INFO - __main__ -   Batch number = 18
01/16/2022 01:08:20 - INFO - __main__ -   ***** Evaluation result  in da *****
01/16/2022 01:08:20 - INFO - __main__ -     f1 = 0.8723313860860329
01/16/2022 01:08:20 - INFO - __main__ -     loss = 0.43173061145676506
01/16/2022 01:08:20 - INFO - __main__ -     precision = 0.8736304648441655
01/16/2022 01:08:20 - INFO - __main__ -     recall = 0.871036165022802
01/16/2022 01:08:22 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='da', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/16/2022 01:08:22 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/16/2022 01:08:22 - INFO - __main__ -   Seed = 3
01/16/2022 01:08:22 - INFO - root -   save model
01/16/2022 01:08:22 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='da', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/16/2022 01:08:22 - INFO - __main__ -   Loading pretrained model and tokenizer
01/16/2022 01:08:25 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/16/2022 01:08:30 - INFO - __main__ -   Using lang2id = None
01/16/2022 01:08:30 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/16/2022 01:08:30 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
01/16/2022 01:08:30 - INFO - root -   Trying to decide if add adapter
01/16/2022 01:08:30 - INFO - root -   loading task adapter
01/16/2022 01:08:30 - INFO - root -   loading lang adpater cs/wiki@ukp,en/wiki@ukp,de/wiki@ukp,fr/wiki@ukp,lv/wiki@ukp,et/wiki@ukp,hu/wiki@ukp,fi/wiki@ukp,la/wiki@ukp,eu/wiki@ukp
01/16/2022 01:08:30 - INFO - __main__ -   Adapter Languages : ['cs', 'en', 'de', 'fr', 'lv', 'et', 'hu', 'fi', 'la', 'eu'], Length : 10
01/16/2022 01:08:30 - INFO - __main__ -   Adapter Names ['cs/wiki@ukp', 'en/wiki@ukp', 'de/wiki@ukp', 'fr/wiki@ukp', 'lv/wiki@ukp', 'et/wiki@ukp', 'hu/wiki@ukp', 'fi/wiki@ukp', 'la/wiki@ukp', 'eu/wiki@ukp'], Length : 10
01/16/2022 01:08:30 - INFO - __main__ -   Language = cs
01/16/2022 01:08:30 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
01/16/2022 01:08:46 - INFO - __main__ -   Language = en
01/16/2022 01:08:46 - INFO - __main__ -   Adapter Name = en/wiki@ukp
01/16/2022 01:09:02 - INFO - __main__ -   Language = de
01/16/2022 01:09:02 - INFO - __main__ -   Adapter Name = de/wiki@ukp
01/16/2022 01:09:17 - INFO - __main__ -   Language = fr
01/16/2022 01:09:17 - INFO - __main__ -   Adapter Name = fr/wiki@ukp
01/16/2022 01:09:25 - INFO - __main__ -   Language = lv
01/16/2022 01:09:25 - INFO - __main__ -   Adapter Name = lv/wiki@ukp
01/16/2022 01:09:34 - INFO - __main__ -   Language = et
01/16/2022 01:09:34 - INFO - __main__ -   Adapter Name = et/wiki@ukp
01/16/2022 01:09:49 - INFO - __main__ -   Language = hu
01/16/2022 01:09:49 - INFO - __main__ -   Adapter Name = hu/wiki@ukp
01/16/2022 01:10:05 - INFO - __main__ -   Language = fi
01/16/2022 01:10:05 - INFO - __main__ -   Adapter Name = fi/wiki@ukp
01/16/2022 01:10:20 - INFO - __main__ -   Language = la
01/16/2022 01:10:20 - INFO - __main__ -   Adapter Name = la/wiki@ukp
01/16/2022 01:10:36 - INFO - __main__ -   Language = eu
01/16/2022 01:10:36 - INFO - __main__ -   Adapter Name = eu/wiki@ukp
01/16/2022 01:10:41 - INFO - __main__ -   Args Adapter Weight = equal
01/16/2022 01:10:41 - INFO - __main__ -   Adapter Languages = ['cs', 'en', 'de', 'fr', 'lv', 'et', 'hu', 'fi', 'la', 'eu']
01/16/2022 01:10:41 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/16/2022 01:10:41 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/16/2022 01:10:41 - INFO - __main__ -   Length of Adapter Weights = 10
01/16/2022 01:10:41 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/cached_test_da_bert-base-multilingual-cased_128
01/16/2022 01:10:41 - INFO - __main__ -   ***** Running evaluation  in da *****
01/16/2022 01:10:41 - INFO - __main__ -     Num examples = 565
01/16/2022 01:10:41 - INFO - __main__ -     Batch size = 32
01/16/2022 01:10:41 - INFO - __main__ -   Batch number = 1
01/16/2022 01:10:41 - INFO - __main__ -   Batch number = 2
01/16/2022 01:10:41 - INFO - __main__ -   Batch number = 3
01/16/2022 01:10:42 - INFO - __main__ -   Batch number = 4
01/16/2022 01:10:42 - INFO - __main__ -   Batch number = 5
01/16/2022 01:10:42 - INFO - __main__ -   Batch number = 6
01/16/2022 01:10:43 - INFO - __main__ -   Batch number = 7
01/16/2022 01:10:43 - INFO - __main__ -   Batch number = 8
01/16/2022 01:10:43 - INFO - __main__ -   Batch number = 9
01/16/2022 01:10:43 - INFO - __main__ -   Batch number = 10
01/16/2022 01:10:44 - INFO - __main__ -   Batch number = 11
01/16/2022 01:10:44 - INFO - __main__ -   Batch number = 12
01/16/2022 01:10:44 - INFO - __main__ -   Batch number = 13
01/16/2022 01:10:45 - INFO - __main__ -   Batch number = 14
01/16/2022 01:10:45 - INFO - __main__ -   Batch number = 15
01/16/2022 01:10:45 - INFO - __main__ -   Batch number = 16
01/16/2022 01:10:45 - INFO - __main__ -   Batch number = 17
01/16/2022 01:10:46 - INFO - __main__ -   Batch number = 18
01/16/2022 01:10:46 - INFO - __main__ -   ***** Evaluation result  in da *****
01/16/2022 01:10:46 - INFO - __main__ -     f1 = 0.8846072186836518
01/16/2022 01:10:46 - INFO - __main__ -     loss = 0.346522041492992
01/16/2022 01:10:46 - INFO - __main__ -     precision = 0.8854531930719372
01/16/2022 01:10:46 - INFO - __main__ -     recall = 0.8837628592639728
01/16/2022 01:10:48 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='be', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/16/2022 01:10:48 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/16/2022 01:10:48 - INFO - __main__ -   Seed = 1
01/16/2022 01:10:48 - INFO - root -   save model
01/16/2022 01:10:48 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='be', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/16/2022 01:10:48 - INFO - __main__ -   Loading pretrained model and tokenizer
01/16/2022 01:10:51 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/16/2022 01:10:56 - INFO - __main__ -   Using lang2id = None
01/16/2022 01:10:56 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/16/2022 01:10:56 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
01/16/2022 01:10:56 - INFO - root -   Trying to decide if add adapter
01/16/2022 01:10:56 - INFO - root -   loading task adapter
01/16/2022 01:10:56 - INFO - root -   loading lang adpater lv/wiki@ukp,et/wiki@ukp,cs/wiki@ukp,hu/wiki@ukp,de/wiki@ukp,el/wiki@ukp,fi/wiki@ukp,myv/wiki@ukp,mhr/wiki@ukp,tr/wiki@ukp
01/16/2022 01:10:56 - INFO - __main__ -   Adapter Languages : ['lv', 'et', 'cs', 'hu', 'de', 'el', 'fi', 'myv', 'mhr', 'tr'], Length : 10
01/16/2022 01:10:56 - INFO - __main__ -   Adapter Names ['lv/wiki@ukp', 'et/wiki@ukp', 'cs/wiki@ukp', 'hu/wiki@ukp', 'de/wiki@ukp', 'el/wiki@ukp', 'fi/wiki@ukp', 'myv/wiki@ukp', 'mhr/wiki@ukp', 'tr/wiki@ukp'], Length : 10
01/16/2022 01:10:56 - INFO - __main__ -   Language = lv
01/16/2022 01:10:56 - INFO - __main__ -   Adapter Name = lv/wiki@ukp
01/16/2022 01:11:12 - INFO - __main__ -   Language = et
01/16/2022 01:11:12 - INFO - __main__ -   Adapter Name = et/wiki@ukp
01/16/2022 01:11:28 - INFO - __main__ -   Language = cs
01/16/2022 01:11:28 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
01/16/2022 01:11:44 - INFO - __main__ -   Language = hu
01/16/2022 01:11:44 - INFO - __main__ -   Adapter Name = hu/wiki@ukp
01/16/2022 01:12:00 - INFO - __main__ -   Language = de
01/16/2022 01:12:00 - INFO - __main__ -   Adapter Name = de/wiki@ukp
01/16/2022 01:12:15 - INFO - __main__ -   Language = el
01/16/2022 01:12:15 - INFO - __main__ -   Adapter Name = el/wiki@ukp
01/16/2022 01:12:17 - INFO - __main__ -   Language = fi
01/16/2022 01:12:17 - INFO - __main__ -   Adapter Name = fi/wiki@ukp
01/16/2022 01:12:32 - INFO - __main__ -   Language = myv
01/16/2022 01:12:32 - INFO - __main__ -   Adapter Name = myv/wiki@ukp
01/16/2022 01:12:48 - INFO - __main__ -   Language = mhr
01/16/2022 01:12:48 - INFO - __main__ -   Adapter Name = mhr/wiki@ukp
01/16/2022 01:13:04 - INFO - __main__ -   Language = tr
01/16/2022 01:13:04 - INFO - __main__ -   Adapter Name = tr/wiki@ukp
01/16/2022 01:13:23 - INFO - __main__ -   Args Adapter Weight = equal
01/16/2022 01:13:23 - INFO - __main__ -   Adapter Languages = ['lv', 'et', 'cs', 'hu', 'de', 'el', 'fi', 'myv', 'mhr', 'tr']
01/16/2022 01:13:23 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/16/2022 01:13:23 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/16/2022 01:13:23 - INFO - __main__ -   Length of Adapter Weights = 10
01/16/2022 01:13:23 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/cached_test_be_bert-base-multilingual-cased_128
01/16/2022 01:13:23 - INFO - __main__ -   ***** Running evaluation  in be *****
01/16/2022 01:13:23 - INFO - __main__ -     Num examples = 932
01/16/2022 01:13:23 - INFO - __main__ -     Batch size = 32
01/16/2022 01:13:23 - INFO - __main__ -   Batch number = 1
01/16/2022 01:13:23 - INFO - __main__ -   Batch number = 2
01/16/2022 01:13:24 - INFO - __main__ -   Batch number = 3
01/16/2022 01:13:24 - INFO - __main__ -   Batch number = 4
01/16/2022 01:13:24 - INFO - __main__ -   Batch number = 5
01/16/2022 01:13:25 - INFO - __main__ -   Batch number = 6
01/16/2022 01:13:25 - INFO - __main__ -   Batch number = 7
01/16/2022 01:13:25 - INFO - __main__ -   Batch number = 8
01/16/2022 01:13:25 - INFO - __main__ -   Batch number = 9
01/16/2022 01:13:26 - INFO - __main__ -   Batch number = 10
01/16/2022 01:13:26 - INFO - __main__ -   Batch number = 11
01/16/2022 01:13:26 - INFO - __main__ -   Batch number = 12
01/16/2022 01:13:27 - INFO - __main__ -   Batch number = 13
01/16/2022 01:13:27 - INFO - __main__ -   Batch number = 14
01/16/2022 01:13:27 - INFO - __main__ -   Batch number = 15
01/16/2022 01:13:27 - INFO - __main__ -   Batch number = 16
01/16/2022 01:13:28 - INFO - __main__ -   Batch number = 17
01/16/2022 01:13:28 - INFO - __main__ -   Batch number = 18
01/16/2022 01:13:28 - INFO - __main__ -   Batch number = 19
01/16/2022 01:13:29 - INFO - __main__ -   Batch number = 20
01/16/2022 01:13:29 - INFO - __main__ -   Batch number = 21
01/16/2022 01:13:29 - INFO - __main__ -   Batch number = 22
01/16/2022 01:13:30 - INFO - __main__ -   Batch number = 23
01/16/2022 01:13:30 - INFO - __main__ -   Batch number = 24
01/16/2022 01:13:30 - INFO - __main__ -   Batch number = 25
01/16/2022 01:13:30 - INFO - __main__ -   Batch number = 26
01/16/2022 01:13:31 - INFO - __main__ -   Batch number = 27
01/16/2022 01:13:31 - INFO - __main__ -   Batch number = 28
01/16/2022 01:13:31 - INFO - __main__ -   Batch number = 29
01/16/2022 01:13:32 - INFO - __main__ -   Batch number = 30
01/16/2022 01:13:32 - INFO - __main__ -   ***** Evaluation result  in be *****
01/16/2022 01:13:32 - INFO - __main__ -     f1 = 0.8229644182492905
01/16/2022 01:13:32 - INFO - __main__ -     loss = 0.7250492523113886
01/16/2022 01:13:32 - INFO - __main__ -     precision = 0.8269720101781171
01/16/2022 01:13:32 - INFO - __main__ -     recall = 0.8189954814042405
01/16/2022 01:13:34 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='be', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/16/2022 01:13:34 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/16/2022 01:13:34 - INFO - __main__ -   Seed = 2
01/16/2022 01:13:34 - INFO - root -   save model
01/16/2022 01:13:34 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='be', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/16/2022 01:13:34 - INFO - __main__ -   Loading pretrained model and tokenizer
01/16/2022 01:13:37 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/16/2022 01:13:42 - INFO - __main__ -   Using lang2id = None
01/16/2022 01:13:42 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/16/2022 01:13:42 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
01/16/2022 01:13:42 - INFO - root -   Trying to decide if add adapter
01/16/2022 01:13:42 - INFO - root -   loading task adapter
01/16/2022 01:13:42 - INFO - root -   loading lang adpater lv/wiki@ukp,et/wiki@ukp,cs/wiki@ukp,hu/wiki@ukp,de/wiki@ukp,el/wiki@ukp,fi/wiki@ukp,myv/wiki@ukp,mhr/wiki@ukp,tr/wiki@ukp
01/16/2022 01:13:42 - INFO - __main__ -   Adapter Languages : ['lv', 'et', 'cs', 'hu', 'de', 'el', 'fi', 'myv', 'mhr', 'tr'], Length : 10
01/16/2022 01:13:42 - INFO - __main__ -   Adapter Names ['lv/wiki@ukp', 'et/wiki@ukp', 'cs/wiki@ukp', 'hu/wiki@ukp', 'de/wiki@ukp', 'el/wiki@ukp', 'fi/wiki@ukp', 'myv/wiki@ukp', 'mhr/wiki@ukp', 'tr/wiki@ukp'], Length : 10
01/16/2022 01:13:42 - INFO - __main__ -   Language = lv
01/16/2022 01:13:42 - INFO - __main__ -   Adapter Name = lv/wiki@ukp
01/16/2022 01:13:58 - INFO - __main__ -   Language = et
01/16/2022 01:13:58 - INFO - __main__ -   Adapter Name = et/wiki@ukp
01/16/2022 01:14:14 - INFO - __main__ -   Language = cs
01/16/2022 01:14:14 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
01/16/2022 01:14:30 - INFO - __main__ -   Language = hu
01/16/2022 01:14:30 - INFO - __main__ -   Adapter Name = hu/wiki@ukp
01/16/2022 01:14:46 - INFO - __main__ -   Language = de
01/16/2022 01:14:46 - INFO - __main__ -   Adapter Name = de/wiki@ukp
01/16/2022 01:15:02 - INFO - __main__ -   Language = el
01/16/2022 01:15:02 - INFO - __main__ -   Adapter Name = el/wiki@ukp
01/16/2022 01:15:03 - INFO - __main__ -   Language = fi
01/16/2022 01:15:03 - INFO - __main__ -   Adapter Name = fi/wiki@ukp
01/16/2022 01:15:11 - INFO - __main__ -   Language = myv
01/16/2022 01:15:11 - INFO - __main__ -   Adapter Name = myv/wiki@ukp
01/16/2022 01:15:13 - INFO - __main__ -   Language = mhr
01/16/2022 01:15:13 - INFO - __main__ -   Adapter Name = mhr/wiki@ukp
01/16/2022 01:15:28 - INFO - __main__ -   Language = tr
01/16/2022 01:15:28 - INFO - __main__ -   Adapter Name = tr/wiki@ukp
01/16/2022 01:15:48 - INFO - __main__ -   Args Adapter Weight = equal
01/16/2022 01:15:48 - INFO - __main__ -   Adapter Languages = ['lv', 'et', 'cs', 'hu', 'de', 'el', 'fi', 'myv', 'mhr', 'tr']
01/16/2022 01:15:48 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/16/2022 01:15:48 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/16/2022 01:15:48 - INFO - __main__ -   Length of Adapter Weights = 10
01/16/2022 01:15:48 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/cached_test_be_bert-base-multilingual-cased_128
01/16/2022 01:15:48 - INFO - __main__ -   ***** Running evaluation  in be *****
01/16/2022 01:15:48 - INFO - __main__ -     Num examples = 932
01/16/2022 01:15:48 - INFO - __main__ -     Batch size = 32
01/16/2022 01:15:48 - INFO - __main__ -   Batch number = 1
01/16/2022 01:15:48 - INFO - __main__ -   Batch number = 2
01/16/2022 01:15:49 - INFO - __main__ -   Batch number = 3
01/16/2022 01:15:49 - INFO - __main__ -   Batch number = 4
01/16/2022 01:15:49 - INFO - __main__ -   Batch number = 5
01/16/2022 01:15:50 - INFO - __main__ -   Batch number = 6
01/16/2022 01:15:50 - INFO - __main__ -   Batch number = 7
01/16/2022 01:15:50 - INFO - __main__ -   Batch number = 8
01/16/2022 01:15:50 - INFO - __main__ -   Batch number = 9
01/16/2022 01:15:51 - INFO - __main__ -   Batch number = 10
01/16/2022 01:15:51 - INFO - __main__ -   Batch number = 11
01/16/2022 01:15:51 - INFO - __main__ -   Batch number = 12
01/16/2022 01:15:52 - INFO - __main__ -   Batch number = 13
01/16/2022 01:15:52 - INFO - __main__ -   Batch number = 14
01/16/2022 01:15:52 - INFO - __main__ -   Batch number = 15
01/16/2022 01:15:52 - INFO - __main__ -   Batch number = 16
01/16/2022 01:15:53 - INFO - __main__ -   Batch number = 17
01/16/2022 01:15:53 - INFO - __main__ -   Batch number = 18
01/16/2022 01:15:53 - INFO - __main__ -   Batch number = 19
01/16/2022 01:15:54 - INFO - __main__ -   Batch number = 20
01/16/2022 01:15:54 - INFO - __main__ -   Batch number = 21
01/16/2022 01:15:54 - INFO - __main__ -   Batch number = 22
01/16/2022 01:15:54 - INFO - __main__ -   Batch number = 23
01/16/2022 01:15:55 - INFO - __main__ -   Batch number = 24
01/16/2022 01:15:55 - INFO - __main__ -   Batch number = 25
01/16/2022 01:15:55 - INFO - __main__ -   Batch number = 26
01/16/2022 01:15:56 - INFO - __main__ -   Batch number = 27
01/16/2022 01:15:56 - INFO - __main__ -   Batch number = 28
01/16/2022 01:15:56 - INFO - __main__ -   Batch number = 29
01/16/2022 01:15:56 - INFO - __main__ -   Batch number = 30
01/16/2022 01:15:57 - INFO - __main__ -   ***** Evaluation result  in be *****
01/16/2022 01:15:57 - INFO - __main__ -     f1 = 0.8196893458761759
01/16/2022 01:15:57 - INFO - __main__ -     loss = 0.7790309389432272
01/16/2022 01:15:57 - INFO - __main__ -     precision = 0.8255045386445756
01/16/2022 01:15:57 - INFO - __main__ -     recall = 0.8139555092109837
01/16/2022 01:15:59 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='be', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/16/2022 01:15:59 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/16/2022 01:15:59 - INFO - __main__ -   Seed = 3
01/16/2022 01:15:59 - INFO - root -   save model
01/16/2022 01:15:59 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='be', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/16/2022 01:15:59 - INFO - __main__ -   Loading pretrained model and tokenizer
01/16/2022 01:16:02 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/16/2022 01:16:07 - INFO - __main__ -   Using lang2id = None
01/16/2022 01:16:07 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/16/2022 01:16:07 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
01/16/2022 01:16:07 - INFO - root -   Trying to decide if add adapter
01/16/2022 01:16:07 - INFO - root -   loading task adapter
01/16/2022 01:16:07 - INFO - root -   loading lang adpater lv/wiki@ukp,et/wiki@ukp,cs/wiki@ukp,hu/wiki@ukp,de/wiki@ukp,el/wiki@ukp,fi/wiki@ukp,myv/wiki@ukp,mhr/wiki@ukp,tr/wiki@ukp
01/16/2022 01:16:07 - INFO - __main__ -   Adapter Languages : ['lv', 'et', 'cs', 'hu', 'de', 'el', 'fi', 'myv', 'mhr', 'tr'], Length : 10
01/16/2022 01:16:07 - INFO - __main__ -   Adapter Names ['lv/wiki@ukp', 'et/wiki@ukp', 'cs/wiki@ukp', 'hu/wiki@ukp', 'de/wiki@ukp', 'el/wiki@ukp', 'fi/wiki@ukp', 'myv/wiki@ukp', 'mhr/wiki@ukp', 'tr/wiki@ukp'], Length : 10
01/16/2022 01:16:07 - INFO - __main__ -   Language = lv
01/16/2022 01:16:07 - INFO - __main__ -   Adapter Name = lv/wiki@ukp
01/16/2022 01:16:24 - INFO - __main__ -   Language = et
01/16/2022 01:16:24 - INFO - __main__ -   Adapter Name = et/wiki@ukp
01/16/2022 01:16:40 - INFO - __main__ -   Language = cs
01/16/2022 01:16:40 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
01/16/2022 01:16:56 - INFO - __main__ -   Language = hu
01/16/2022 01:16:56 - INFO - __main__ -   Adapter Name = hu/wiki@ukp
01/16/2022 01:17:06 - INFO - __main__ -   Language = de
01/16/2022 01:17:06 - INFO - __main__ -   Adapter Name = de/wiki@ukp
01/16/2022 01:17:14 - INFO - __main__ -   Language = el
01/16/2022 01:17:14 - INFO - __main__ -   Adapter Name = el/wiki@ukp
01/16/2022 01:17:30 - INFO - __main__ -   Language = fi
01/16/2022 01:17:30 - INFO - __main__ -   Adapter Name = fi/wiki@ukp
01/16/2022 01:17:38 - INFO - __main__ -   Language = myv
01/16/2022 01:17:38 - INFO - __main__ -   Adapter Name = myv/wiki@ukp
01/16/2022 01:17:55 - INFO - __main__ -   Language = mhr
01/16/2022 01:17:55 - INFO - __main__ -   Adapter Name = mhr/wiki@ukp
01/16/2022 01:18:10 - INFO - __main__ -   Language = tr
01/16/2022 01:18:10 - INFO - __main__ -   Adapter Name = tr/wiki@ukp
01/16/2022 01:18:16 - INFO - __main__ -   Args Adapter Weight = equal
01/16/2022 01:18:16 - INFO - __main__ -   Adapter Languages = ['lv', 'et', 'cs', 'hu', 'de', 'el', 'fi', 'myv', 'mhr', 'tr']
01/16/2022 01:18:16 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/16/2022 01:18:16 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/16/2022 01:18:16 - INFO - __main__ -   Length of Adapter Weights = 10
01/16/2022 01:18:16 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/cached_test_be_bert-base-multilingual-cased_128
01/16/2022 01:18:16 - INFO - __main__ -   ***** Running evaluation  in be *****
01/16/2022 01:18:16 - INFO - __main__ -     Num examples = 932
01/16/2022 01:18:16 - INFO - __main__ -     Batch size = 32
01/16/2022 01:18:16 - INFO - __main__ -   Batch number = 1
01/16/2022 01:18:16 - INFO - __main__ -   Batch number = 2
01/16/2022 01:18:17 - INFO - __main__ -   Batch number = 3
01/16/2022 01:18:17 - INFO - __main__ -   Batch number = 4
01/16/2022 01:18:17 - INFO - __main__ -   Batch number = 5
01/16/2022 01:18:18 - INFO - __main__ -   Batch number = 6
01/16/2022 01:18:18 - INFO - __main__ -   Batch number = 7
01/16/2022 01:18:18 - INFO - __main__ -   Batch number = 8
01/16/2022 01:18:18 - INFO - __main__ -   Batch number = 9
01/16/2022 01:18:19 - INFO - __main__ -   Batch number = 10
01/16/2022 01:18:19 - INFO - __main__ -   Batch number = 11
01/16/2022 01:18:19 - INFO - __main__ -   Batch number = 12
01/16/2022 01:18:20 - INFO - __main__ -   Batch number = 13
01/16/2022 01:18:20 - INFO - __main__ -   Batch number = 14
01/16/2022 01:18:21 - INFO - __main__ -   Batch number = 15
01/16/2022 01:18:21 - INFO - __main__ -   Batch number = 16
01/16/2022 01:18:21 - INFO - __main__ -   Batch number = 17
01/16/2022 01:18:22 - INFO - __main__ -   Batch number = 18
01/16/2022 01:18:22 - INFO - __main__ -   Batch number = 19
01/16/2022 01:18:22 - INFO - __main__ -   Batch number = 20
01/16/2022 01:18:23 - INFO - __main__ -   Batch number = 21
01/16/2022 01:18:23 - INFO - __main__ -   Batch number = 22
01/16/2022 01:18:23 - INFO - __main__ -   Batch number = 23
01/16/2022 01:18:23 - INFO - __main__ -   Batch number = 24
01/16/2022 01:18:24 - INFO - __main__ -   Batch number = 25
01/16/2022 01:18:24 - INFO - __main__ -   Batch number = 26
01/16/2022 01:18:24 - INFO - __main__ -   Batch number = 27
01/16/2022 01:18:25 - INFO - __main__ -   Batch number = 28
01/16/2022 01:18:25 - INFO - __main__ -   Batch number = 29
01/16/2022 01:18:25 - INFO - __main__ -   Batch number = 30
01/16/2022 01:18:25 - INFO - __main__ -   ***** Evaluation result  in be *****
01/16/2022 01:18:25 - INFO - __main__ -     f1 = 0.8204476762013229
01/16/2022 01:18:25 - INFO - __main__ -     loss = 0.7252413655320803
01/16/2022 01:18:25 - INFO - __main__ -     precision = 0.8272237434855578
01/16/2022 01:18:25 - INFO - __main__ -     recall = 0.8137817170663886
01/16/2022 01:18:28 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='uk', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/16/2022 01:18:28 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/16/2022 01:18:28 - INFO - __main__ -   Seed = 1
01/16/2022 01:18:28 - INFO - root -   save model
01/16/2022 01:18:28 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='uk', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/16/2022 01:18:28 - INFO - __main__ -   Loading pretrained model and tokenizer
01/16/2022 01:18:30 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/16/2022 01:18:36 - INFO - __main__ -   Using lang2id = None
01/16/2022 01:18:36 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/16/2022 01:18:36 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
01/16/2022 01:18:36 - INFO - root -   Trying to decide if add adapter
01/16/2022 01:18:36 - INFO - root -   loading task adapter
01/16/2022 01:18:36 - INFO - root -   loading lang adpater hu/wiki@ukp,el/wiki@ukp,cs/wiki@ukp,de/wiki@ukp,lv/wiki@ukp,tr/wiki@ukp,la/wiki@ukp,et/wiki@ukp,xmf/wiki@ukp,myv/wiki@ukp
01/16/2022 01:18:36 - INFO - __main__ -   Adapter Languages : ['hu', 'el', 'cs', 'de', 'lv', 'tr', 'la', 'et', 'xmf', 'myv'], Length : 10
01/16/2022 01:18:36 - INFO - __main__ -   Adapter Names ['hu/wiki@ukp', 'el/wiki@ukp', 'cs/wiki@ukp', 'de/wiki@ukp', 'lv/wiki@ukp', 'tr/wiki@ukp', 'la/wiki@ukp', 'et/wiki@ukp', 'xmf/wiki@ukp', 'myv/wiki@ukp'], Length : 10
01/16/2022 01:18:36 - INFO - __main__ -   Language = hu
01/16/2022 01:18:36 - INFO - __main__ -   Adapter Name = hu/wiki@ukp
01/16/2022 01:18:52 - INFO - __main__ -   Language = el
01/16/2022 01:18:52 - INFO - __main__ -   Adapter Name = el/wiki@ukp
01/16/2022 01:19:07 - INFO - __main__ -   Language = cs
01/16/2022 01:19:07 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
01/16/2022 01:19:23 - INFO - __main__ -   Language = de
01/16/2022 01:19:23 - INFO - __main__ -   Adapter Name = de/wiki@ukp
01/16/2022 01:19:31 - INFO - __main__ -   Language = lv
01/16/2022 01:19:31 - INFO - __main__ -   Adapter Name = lv/wiki@ukp
01/16/2022 01:19:40 - INFO - __main__ -   Language = tr
01/16/2022 01:19:40 - INFO - __main__ -   Adapter Name = tr/wiki@ukp
01/16/2022 01:19:57 - INFO - __main__ -   Language = la
01/16/2022 01:19:57 - INFO - __main__ -   Adapter Name = la/wiki@ukp
01/16/2022 01:20:12 - INFO - __main__ -   Language = et
01/16/2022 01:20:12 - INFO - __main__ -   Adapter Name = et/wiki@ukp
01/16/2022 01:20:20 - INFO - __main__ -   Language = xmf
01/16/2022 01:20:20 - INFO - __main__ -   Adapter Name = xmf/wiki@ukp
01/16/2022 01:20:22 - INFO - __main__ -   Language = myv
01/16/2022 01:20:22 - INFO - __main__ -   Adapter Name = myv/wiki@ukp
01/16/2022 01:20:34 - INFO - __main__ -   Args Adapter Weight = equal
01/16/2022 01:20:34 - INFO - __main__ -   Adapter Languages = ['hu', 'el', 'cs', 'de', 'lv', 'tr', 'la', 'et', 'xmf', 'myv']
01/16/2022 01:20:34 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/16/2022 01:20:34 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/16/2022 01:20:34 - INFO - __main__ -   Length of Adapter Weights = 10
01/16/2022 01:20:34 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/cached_test_uk_bert-base-multilingual-cased_128
01/16/2022 01:20:34 - INFO - __main__ -   ***** Running evaluation  in uk *****
01/16/2022 01:20:34 - INFO - __main__ -     Num examples = 915
01/16/2022 01:20:34 - INFO - __main__ -     Batch size = 32
01/16/2022 01:20:34 - INFO - __main__ -   Batch number = 1
01/16/2022 01:20:34 - INFO - __main__ -   Batch number = 2
01/16/2022 01:20:35 - INFO - __main__ -   Batch number = 3
01/16/2022 01:20:35 - INFO - __main__ -   Batch number = 4
01/16/2022 01:20:35 - INFO - __main__ -   Batch number = 5
01/16/2022 01:20:36 - INFO - __main__ -   Batch number = 6
01/16/2022 01:20:36 - INFO - __main__ -   Batch number = 7
01/16/2022 01:20:36 - INFO - __main__ -   Batch number = 8
01/16/2022 01:20:36 - INFO - __main__ -   Batch number = 9
01/16/2022 01:20:37 - INFO - __main__ -   Batch number = 10
01/16/2022 01:20:37 - INFO - __main__ -   Batch number = 11
01/16/2022 01:20:37 - INFO - __main__ -   Batch number = 12
01/16/2022 01:20:38 - INFO - __main__ -   Batch number = 13
01/16/2022 01:20:38 - INFO - __main__ -   Batch number = 14
01/16/2022 01:20:38 - INFO - __main__ -   Batch number = 15
01/16/2022 01:20:38 - INFO - __main__ -   Batch number = 16
01/16/2022 01:20:39 - INFO - __main__ -   Batch number = 17
01/16/2022 01:20:39 - INFO - __main__ -   Batch number = 18
01/16/2022 01:20:39 - INFO - __main__ -   Batch number = 19
01/16/2022 01:20:40 - INFO - __main__ -   Batch number = 20
01/16/2022 01:20:40 - INFO - __main__ -   Batch number = 21
01/16/2022 01:20:40 - INFO - __main__ -   Batch number = 22
01/16/2022 01:20:40 - INFO - __main__ -   Batch number = 23
01/16/2022 01:20:41 - INFO - __main__ -   Batch number = 24
01/16/2022 01:20:41 - INFO - __main__ -   Batch number = 25
01/16/2022 01:20:41 - INFO - __main__ -   Batch number = 26
01/16/2022 01:20:42 - INFO - __main__ -   Batch number = 27
01/16/2022 01:20:42 - INFO - __main__ -   Batch number = 28
01/16/2022 01:20:42 - INFO - __main__ -   Batch number = 29
01/16/2022 01:20:43 - INFO - __main__ -   ***** Evaluation result  in uk *****
01/16/2022 01:20:43 - INFO - __main__ -     f1 = 0.8192646964136243
01/16/2022 01:20:43 - INFO - __main__ -     loss = 0.6870035909373184
01/16/2022 01:20:43 - INFO - __main__ -     precision = 0.8204797523858653
01/16/2022 01:20:43 - INFO - __main__ -     recall = 0.8180532338948181
01/16/2022 01:20:45 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='uk', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/16/2022 01:20:45 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/16/2022 01:20:45 - INFO - __main__ -   Seed = 2
01/16/2022 01:20:45 - INFO - root -   save model
01/16/2022 01:20:45 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='uk', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/16/2022 01:20:45 - INFO - __main__ -   Loading pretrained model and tokenizer
01/16/2022 01:20:48 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/16/2022 01:20:53 - INFO - __main__ -   Using lang2id = None
01/16/2022 01:20:53 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/16/2022 01:20:53 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
01/16/2022 01:20:53 - INFO - root -   Trying to decide if add adapter
01/16/2022 01:20:53 - INFO - root -   loading task adapter
01/16/2022 01:20:53 - INFO - root -   loading lang adpater hu/wiki@ukp,el/wiki@ukp,cs/wiki@ukp,de/wiki@ukp,lv/wiki@ukp,tr/wiki@ukp,la/wiki@ukp,et/wiki@ukp,xmf/wiki@ukp,myv/wiki@ukp
01/16/2022 01:20:53 - INFO - __main__ -   Adapter Languages : ['hu', 'el', 'cs', 'de', 'lv', 'tr', 'la', 'et', 'xmf', 'myv'], Length : 10
01/16/2022 01:20:53 - INFO - __main__ -   Adapter Names ['hu/wiki@ukp', 'el/wiki@ukp', 'cs/wiki@ukp', 'de/wiki@ukp', 'lv/wiki@ukp', 'tr/wiki@ukp', 'la/wiki@ukp', 'et/wiki@ukp', 'xmf/wiki@ukp', 'myv/wiki@ukp'], Length : 10
01/16/2022 01:20:53 - INFO - __main__ -   Language = hu
01/16/2022 01:20:53 - INFO - __main__ -   Adapter Name = hu/wiki@ukp
01/16/2022 01:21:09 - INFO - __main__ -   Language = el
01/16/2022 01:21:09 - INFO - __main__ -   Adapter Name = el/wiki@ukp
01/16/2022 01:21:24 - INFO - __main__ -   Language = cs
01/16/2022 01:21:24 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
01/16/2022 01:21:40 - INFO - __main__ -   Language = de
01/16/2022 01:21:40 - INFO - __main__ -   Adapter Name = de/wiki@ukp
01/16/2022 01:21:48 - INFO - __main__ -   Language = lv
01/16/2022 01:21:48 - INFO - __main__ -   Adapter Name = lv/wiki@ukp
01/16/2022 01:22:04 - INFO - __main__ -   Language = tr
01/16/2022 01:22:04 - INFO - __main__ -   Adapter Name = tr/wiki@ukp
01/16/2022 01:22:13 - INFO - __main__ -   Language = la
01/16/2022 01:22:13 - INFO - __main__ -   Adapter Name = la/wiki@ukp
01/16/2022 01:22:28 - INFO - __main__ -   Language = et
01/16/2022 01:22:28 - INFO - __main__ -   Adapter Name = et/wiki@ukp
01/16/2022 01:22:36 - INFO - __main__ -   Language = xmf
01/16/2022 01:22:36 - INFO - __main__ -   Adapter Name = xmf/wiki@ukp
01/16/2022 01:22:44 - INFO - __main__ -   Language = myv
01/16/2022 01:22:44 - INFO - __main__ -   Adapter Name = myv/wiki@ukp
01/16/2022 01:22:57 - INFO - __main__ -   Args Adapter Weight = equal
01/16/2022 01:22:57 - INFO - __main__ -   Adapter Languages = ['hu', 'el', 'cs', 'de', 'lv', 'tr', 'la', 'et', 'xmf', 'myv']
01/16/2022 01:22:57 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/16/2022 01:22:57 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/16/2022 01:22:57 - INFO - __main__ -   Length of Adapter Weights = 10
01/16/2022 01:22:57 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/cached_test_uk_bert-base-multilingual-cased_128
01/16/2022 01:22:57 - INFO - __main__ -   ***** Running evaluation  in uk *****
01/16/2022 01:22:57 - INFO - __main__ -     Num examples = 915
01/16/2022 01:22:57 - INFO - __main__ -     Batch size = 32
01/16/2022 01:22:57 - INFO - __main__ -   Batch number = 1
01/16/2022 01:22:57 - INFO - __main__ -   Batch number = 2
01/16/2022 01:22:57 - INFO - __main__ -   Batch number = 3
01/16/2022 01:22:58 - INFO - __main__ -   Batch number = 4
01/16/2022 01:22:58 - INFO - __main__ -   Batch number = 5
01/16/2022 01:22:58 - INFO - __main__ -   Batch number = 6
01/16/2022 01:22:59 - INFO - __main__ -   Batch number = 7
01/16/2022 01:22:59 - INFO - __main__ -   Batch number = 8
01/16/2022 01:22:59 - INFO - __main__ -   Batch number = 9
01/16/2022 01:22:59 - INFO - __main__ -   Batch number = 10
01/16/2022 01:23:00 - INFO - __main__ -   Batch number = 11
01/16/2022 01:23:00 - INFO - __main__ -   Batch number = 12
01/16/2022 01:23:00 - INFO - __main__ -   Batch number = 13
01/16/2022 01:23:01 - INFO - __main__ -   Batch number = 14
01/16/2022 01:23:01 - INFO - __main__ -   Batch number = 15
01/16/2022 01:23:01 - INFO - __main__ -   Batch number = 16
01/16/2022 01:23:01 - INFO - __main__ -   Batch number = 17
01/16/2022 01:23:02 - INFO - __main__ -   Batch number = 18
01/16/2022 01:23:02 - INFO - __main__ -   Batch number = 19
01/16/2022 01:23:02 - INFO - __main__ -   Batch number = 20
01/16/2022 01:23:03 - INFO - __main__ -   Batch number = 21
01/16/2022 01:23:03 - INFO - __main__ -   Batch number = 22
01/16/2022 01:23:03 - INFO - __main__ -   Batch number = 23
01/16/2022 01:23:03 - INFO - __main__ -   Batch number = 24
01/16/2022 01:23:04 - INFO - __main__ -   Batch number = 25
01/16/2022 01:23:04 - INFO - __main__ -   Batch number = 26
01/16/2022 01:23:04 - INFO - __main__ -   Batch number = 27
01/16/2022 01:23:05 - INFO - __main__ -   Batch number = 28
01/16/2022 01:23:05 - INFO - __main__ -   Batch number = 29
01/16/2022 01:23:05 - INFO - __main__ -   ***** Evaluation result  in uk *****
01/16/2022 01:23:05 - INFO - __main__ -     f1 = 0.8166338169246179
01/16/2022 01:23:05 - INFO - __main__ -     loss = 0.7498735651887697
01/16/2022 01:23:05 - INFO - __main__ -     precision = 0.820850925625203
01/16/2022 01:23:05 - INFO - __main__ -     recall = 0.8124598174103125
01/16/2022 01:23:08 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='uk', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/16/2022 01:23:08 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/16/2022 01:23:08 - INFO - __main__ -   Seed = 3
01/16/2022 01:23:08 - INFO - root -   save model
01/16/2022 01:23:08 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='uk', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/16/2022 01:23:08 - INFO - __main__ -   Loading pretrained model and tokenizer
01/16/2022 01:23:10 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/16/2022 01:23:16 - INFO - __main__ -   Using lang2id = None
01/16/2022 01:23:16 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/16/2022 01:23:16 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
01/16/2022 01:23:16 - INFO - root -   Trying to decide if add adapter
01/16/2022 01:23:16 - INFO - root -   loading task adapter
01/16/2022 01:23:16 - INFO - root -   loading lang adpater hu/wiki@ukp,el/wiki@ukp,cs/wiki@ukp,de/wiki@ukp,lv/wiki@ukp,tr/wiki@ukp,la/wiki@ukp,et/wiki@ukp,xmf/wiki@ukp,myv/wiki@ukp
01/16/2022 01:23:16 - INFO - __main__ -   Adapter Languages : ['hu', 'el', 'cs', 'de', 'lv', 'tr', 'la', 'et', 'xmf', 'myv'], Length : 10
01/16/2022 01:23:16 - INFO - __main__ -   Adapter Names ['hu/wiki@ukp', 'el/wiki@ukp', 'cs/wiki@ukp', 'de/wiki@ukp', 'lv/wiki@ukp', 'tr/wiki@ukp', 'la/wiki@ukp', 'et/wiki@ukp', 'xmf/wiki@ukp', 'myv/wiki@ukp'], Length : 10
01/16/2022 01:23:16 - INFO - __main__ -   Language = hu
01/16/2022 01:23:16 - INFO - __main__ -   Adapter Name = hu/wiki@ukp
01/16/2022 01:23:32 - INFO - __main__ -   Language = el
01/16/2022 01:23:32 - INFO - __main__ -   Adapter Name = el/wiki@ukp
01/16/2022 01:23:47 - INFO - __main__ -   Language = cs
01/16/2022 01:23:47 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
01/16/2022 01:24:03 - INFO - __main__ -   Language = de
01/16/2022 01:24:03 - INFO - __main__ -   Adapter Name = de/wiki@ukp
01/16/2022 01:24:19 - INFO - __main__ -   Language = lv
01/16/2022 01:24:19 - INFO - __main__ -   Adapter Name = lv/wiki@ukp
01/16/2022 01:24:34 - INFO - __main__ -   Language = tr
01/16/2022 01:24:34 - INFO - __main__ -   Adapter Name = tr/wiki@ukp
01/16/2022 01:24:43 - INFO - __main__ -   Language = la
01/16/2022 01:24:43 - INFO - __main__ -   Adapter Name = la/wiki@ukp
01/16/2022 01:24:45 - INFO - __main__ -   Language = et
01/16/2022 01:24:45 - INFO - __main__ -   Adapter Name = et/wiki@ukp
01/16/2022 01:24:46 - INFO - __main__ -   Language = xmf
01/16/2022 01:24:46 - INFO - __main__ -   Adapter Name = xmf/wiki@ukp
01/16/2022 01:24:47 - INFO - __main__ -   Language = myv
01/16/2022 01:24:47 - INFO - __main__ -   Adapter Name = myv/wiki@ukp
01/16/2022 01:24:52 - INFO - __main__ -   Args Adapter Weight = equal
01/16/2022 01:24:52 - INFO - __main__ -   Adapter Languages = ['hu', 'el', 'cs', 'de', 'lv', 'tr', 'la', 'et', 'xmf', 'myv']
01/16/2022 01:24:52 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/16/2022 01:24:52 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/16/2022 01:24:52 - INFO - __main__ -   Length of Adapter Weights = 10
01/16/2022 01:24:52 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/cached_test_uk_bert-base-multilingual-cased_128
01/16/2022 01:24:52 - INFO - __main__ -   ***** Running evaluation  in uk *****
01/16/2022 01:24:52 - INFO - __main__ -     Num examples = 915
01/16/2022 01:24:52 - INFO - __main__ -     Batch size = 32
01/16/2022 01:24:52 - INFO - __main__ -   Batch number = 1
01/16/2022 01:24:52 - INFO - __main__ -   Batch number = 2
01/16/2022 01:24:52 - INFO - __main__ -   Batch number = 3
01/16/2022 01:24:53 - INFO - __main__ -   Batch number = 4
01/16/2022 01:24:53 - INFO - __main__ -   Batch number = 5
01/16/2022 01:24:53 - INFO - __main__ -   Batch number = 6
01/16/2022 01:24:54 - INFO - __main__ -   Batch number = 7
01/16/2022 01:24:54 - INFO - __main__ -   Batch number = 8
01/16/2022 01:24:54 - INFO - __main__ -   Batch number = 9
01/16/2022 01:24:54 - INFO - __main__ -   Batch number = 10
01/16/2022 01:24:55 - INFO - __main__ -   Batch number = 11
01/16/2022 01:24:55 - INFO - __main__ -   Batch number = 12
01/16/2022 01:24:55 - INFO - __main__ -   Batch number = 13
01/16/2022 01:24:56 - INFO - __main__ -   Batch number = 14
01/16/2022 01:24:56 - INFO - __main__ -   Batch number = 15
01/16/2022 01:24:56 - INFO - __main__ -   Batch number = 16
01/16/2022 01:24:56 - INFO - __main__ -   Batch number = 17
01/16/2022 01:24:57 - INFO - __main__ -   Batch number = 18
01/16/2022 01:24:57 - INFO - __main__ -   Batch number = 19
01/16/2022 01:24:57 - INFO - __main__ -   Batch number = 20
01/16/2022 01:24:58 - INFO - __main__ -   Batch number = 21
01/16/2022 01:24:58 - INFO - __main__ -   Batch number = 22
01/16/2022 01:24:58 - INFO - __main__ -   Batch number = 23
01/16/2022 01:24:58 - INFO - __main__ -   Batch number = 24
01/16/2022 01:24:59 - INFO - __main__ -   Batch number = 25
01/16/2022 01:24:59 - INFO - __main__ -   Batch number = 26
01/16/2022 01:24:59 - INFO - __main__ -   Batch number = 27
01/16/2022 01:25:00 - INFO - __main__ -   Batch number = 28
01/16/2022 01:25:00 - INFO - __main__ -   Batch number = 29
01/16/2022 01:25:00 - INFO - __main__ -   ***** Evaluation result  in uk *****
01/16/2022 01:25:00 - INFO - __main__ -     f1 = 0.8225493994575744
01/16/2022 01:25:00 - INFO - __main__ -     loss = 0.6714720931546442
01/16/2022 01:25:00 - INFO - __main__ -     precision = 0.8261771954857958
01/16/2022 01:25:00 - INFO - __main__ -     recall = 0.8189533239038189
01/16/2022 01:25:03 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bg', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/16/2022 01:25:03 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/16/2022 01:25:03 - INFO - __main__ -   Seed = 1
01/16/2022 01:25:03 - INFO - root -   save model
01/16/2022 01:25:03 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bg', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/16/2022 01:25:03 - INFO - __main__ -   Loading pretrained model and tokenizer
01/16/2022 01:25:06 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/16/2022 01:25:11 - INFO - __main__ -   Using lang2id = None
01/16/2022 01:25:11 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/16/2022 01:25:11 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
01/16/2022 01:25:11 - INFO - root -   Trying to decide if add adapter
01/16/2022 01:25:11 - INFO - root -   loading task adapter
01/16/2022 01:25:11 - INFO - root -   loading lang adpater el/wiki@ukp,hu/wiki@ukp,tr/wiki@ukp,la/wiki@ukp,cs/wiki@ukp,de/wiki@ukp,xmf/wiki@ukp,lv/wiki@ukp,hy/wiki@ukp,ka/wiki@ukp
01/16/2022 01:25:11 - INFO - __main__ -   Adapter Languages : ['el', 'hu', 'tr', 'la', 'cs', 'de', 'xmf', 'lv', 'hy', 'ka'], Length : 10
01/16/2022 01:25:11 - INFO - __main__ -   Adapter Names ['el/wiki@ukp', 'hu/wiki@ukp', 'tr/wiki@ukp', 'la/wiki@ukp', 'cs/wiki@ukp', 'de/wiki@ukp', 'xmf/wiki@ukp', 'lv/wiki@ukp', 'hy/wiki@ukp', 'ka/wiki@ukp'], Length : 10
01/16/2022 01:25:11 - INFO - __main__ -   Language = el
01/16/2022 01:25:11 - INFO - __main__ -   Adapter Name = el/wiki@ukp
01/16/2022 01:25:12 - INFO - __main__ -   Language = hu
01/16/2022 01:25:12 - INFO - __main__ -   Adapter Name = hu/wiki@ukp
01/16/2022 01:25:14 - INFO - __main__ -   Language = tr
01/16/2022 01:25:14 - INFO - __main__ -   Adapter Name = tr/wiki@ukp
01/16/2022 01:25:17 - INFO - __main__ -   Language = la
01/16/2022 01:25:17 - INFO - __main__ -   Adapter Name = la/wiki@ukp
01/16/2022 01:25:19 - INFO - __main__ -   Language = cs
01/16/2022 01:25:19 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
01/16/2022 01:25:21 - INFO - __main__ -   Language = de
01/16/2022 01:25:21 - INFO - __main__ -   Adapter Name = de/wiki@ukp
01/16/2022 01:25:22 - INFO - __main__ -   Language = xmf
01/16/2022 01:25:22 - INFO - __main__ -   Adapter Name = xmf/wiki@ukp
01/16/2022 01:25:24 - INFO - __main__ -   Language = lv
01/16/2022 01:25:24 - INFO - __main__ -   Adapter Name = lv/wiki@ukp
01/16/2022 01:25:26 - INFO - __main__ -   Language = hy
01/16/2022 01:25:26 - INFO - __main__ -   Adapter Name = hy/wiki@ukp
01/16/2022 01:25:28 - INFO - __main__ -   Language = ka
01/16/2022 01:25:28 - INFO - __main__ -   Adapter Name = ka/wiki@ukp
01/16/2022 01:25:34 - INFO - __main__ -   Args Adapter Weight = equal
01/16/2022 01:25:34 - INFO - __main__ -   Adapter Languages = ['el', 'hu', 'tr', 'la', 'cs', 'de', 'xmf', 'lv', 'hy', 'ka']
01/16/2022 01:25:34 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/16/2022 01:25:34 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/16/2022 01:25:34 - INFO - __main__ -   Length of Adapter Weights = 10
01/16/2022 01:25:34 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/cached_test_bg_bert-base-multilingual-cased_128
01/16/2022 01:25:35 - INFO - __main__ -   ***** Running evaluation  in bg *****
01/16/2022 01:25:35 - INFO - __main__ -     Num examples = 1117
01/16/2022 01:25:35 - INFO - __main__ -     Batch size = 32
01/16/2022 01:25:35 - INFO - __main__ -   Batch number = 1
01/16/2022 01:25:35 - INFO - __main__ -   Batch number = 2
01/16/2022 01:25:35 - INFO - __main__ -   Batch number = 3
01/16/2022 01:25:35 - INFO - __main__ -   Batch number = 4
01/16/2022 01:25:36 - INFO - __main__ -   Batch number = 5
01/16/2022 01:25:36 - INFO - __main__ -   Batch number = 6
01/16/2022 01:25:36 - INFO - __main__ -   Batch number = 7
01/16/2022 01:25:37 - INFO - __main__ -   Batch number = 8
01/16/2022 01:25:37 - INFO - __main__ -   Batch number = 9
01/16/2022 01:25:37 - INFO - __main__ -   Batch number = 10
01/16/2022 01:25:37 - INFO - __main__ -   Batch number = 11
01/16/2022 01:25:38 - INFO - __main__ -   Batch number = 12
01/16/2022 01:25:38 - INFO - __main__ -   Batch number = 13
01/16/2022 01:25:38 - INFO - __main__ -   Batch number = 14
01/16/2022 01:25:39 - INFO - __main__ -   Batch number = 15
01/16/2022 01:25:39 - INFO - __main__ -   Batch number = 16
01/16/2022 01:25:39 - INFO - __main__ -   Batch number = 17
01/16/2022 01:25:39 - INFO - __main__ -   Batch number = 18
01/16/2022 01:25:40 - INFO - __main__ -   Batch number = 19
01/16/2022 01:25:40 - INFO - __main__ -   Batch number = 20
01/16/2022 01:25:40 - INFO - __main__ -   Batch number = 21
01/16/2022 01:25:41 - INFO - __main__ -   Batch number = 22
01/16/2022 01:25:41 - INFO - __main__ -   Batch number = 23
01/16/2022 01:25:41 - INFO - __main__ -   Batch number = 24
01/16/2022 01:25:42 - INFO - __main__ -   Batch number = 25
01/16/2022 01:25:42 - INFO - __main__ -   Batch number = 26
01/16/2022 01:25:42 - INFO - __main__ -   Batch number = 27
01/16/2022 01:25:43 - INFO - __main__ -   Batch number = 28
01/16/2022 01:25:43 - INFO - __main__ -   Batch number = 29
01/16/2022 01:25:43 - INFO - __main__ -   Batch number = 30
01/16/2022 01:25:43 - INFO - __main__ -   Batch number = 31
01/16/2022 01:25:44 - INFO - __main__ -   Batch number = 32
01/16/2022 01:25:44 - INFO - __main__ -   Batch number = 33
01/16/2022 01:25:44 - INFO - __main__ -   Batch number = 34
01/16/2022 01:25:45 - INFO - __main__ -   Batch number = 35
01/16/2022 01:25:45 - INFO - __main__ -   ***** Evaluation result  in bg *****
01/16/2022 01:25:45 - INFO - __main__ -     f1 = 0.8509178132391645
01/16/2022 01:25:45 - INFO - __main__ -     loss = 0.5787569309983934
01/16/2022 01:25:45 - INFO - __main__ -     precision = 0.8533342242416143
01/16/2022 01:25:45 - INFO - __main__ -     recall = 0.8485150488339646
01/16/2022 01:25:47 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bg', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/16/2022 01:25:47 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/16/2022 01:25:47 - INFO - __main__ -   Seed = 2
01/16/2022 01:25:47 - INFO - root -   save model
01/16/2022 01:25:47 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bg', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/16/2022 01:25:47 - INFO - __main__ -   Loading pretrained model and tokenizer
01/16/2022 01:25:50 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/16/2022 01:25:56 - INFO - __main__ -   Using lang2id = None
01/16/2022 01:25:56 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/16/2022 01:25:56 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
01/16/2022 01:25:56 - INFO - root -   Trying to decide if add adapter
01/16/2022 01:25:56 - INFO - root -   loading task adapter
01/16/2022 01:25:56 - INFO - root -   loading lang adpater el/wiki@ukp,hu/wiki@ukp,tr/wiki@ukp,la/wiki@ukp,cs/wiki@ukp,de/wiki@ukp,xmf/wiki@ukp,lv/wiki@ukp,hy/wiki@ukp,ka/wiki@ukp
01/16/2022 01:25:56 - INFO - __main__ -   Adapter Languages : ['el', 'hu', 'tr', 'la', 'cs', 'de', 'xmf', 'lv', 'hy', 'ka'], Length : 10
01/16/2022 01:25:56 - INFO - __main__ -   Adapter Names ['el/wiki@ukp', 'hu/wiki@ukp', 'tr/wiki@ukp', 'la/wiki@ukp', 'cs/wiki@ukp', 'de/wiki@ukp', 'xmf/wiki@ukp', 'lv/wiki@ukp', 'hy/wiki@ukp', 'ka/wiki@ukp'], Length : 10
01/16/2022 01:25:56 - INFO - __main__ -   Language = el
01/16/2022 01:25:56 - INFO - __main__ -   Adapter Name = el/wiki@ukp
01/16/2022 01:25:57 - INFO - __main__ -   Language = hu
01/16/2022 01:25:57 - INFO - __main__ -   Adapter Name = hu/wiki@ukp
01/16/2022 01:25:58 - INFO - __main__ -   Language = tr
01/16/2022 01:25:58 - INFO - __main__ -   Adapter Name = tr/wiki@ukp
01/16/2022 01:26:00 - INFO - __main__ -   Language = la
01/16/2022 01:26:00 - INFO - __main__ -   Adapter Name = la/wiki@ukp
01/16/2022 01:26:02 - INFO - __main__ -   Language = cs
01/16/2022 01:26:02 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
01/16/2022 01:26:03 - INFO - __main__ -   Language = de
01/16/2022 01:26:03 - INFO - __main__ -   Adapter Name = de/wiki@ukp
01/16/2022 01:26:04 - INFO - __main__ -   Language = xmf
01/16/2022 01:26:04 - INFO - __main__ -   Adapter Name = xmf/wiki@ukp
01/16/2022 01:26:05 - INFO - __main__ -   Language = lv
01/16/2022 01:26:05 - INFO - __main__ -   Adapter Name = lv/wiki@ukp
01/16/2022 01:26:06 - INFO - __main__ -   Language = hy
01/16/2022 01:26:06 - INFO - __main__ -   Adapter Name = hy/wiki@ukp
01/16/2022 01:26:08 - INFO - __main__ -   Language = ka
01/16/2022 01:26:08 - INFO - __main__ -   Adapter Name = ka/wiki@ukp
01/16/2022 01:26:13 - INFO - __main__ -   Args Adapter Weight = equal
01/16/2022 01:26:13 - INFO - __main__ -   Adapter Languages = ['el', 'hu', 'tr', 'la', 'cs', 'de', 'xmf', 'lv', 'hy', 'ka']
01/16/2022 01:26:13 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/16/2022 01:26:13 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/16/2022 01:26:13 - INFO - __main__ -   Length of Adapter Weights = 10
01/16/2022 01:26:13 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/cached_test_bg_bert-base-multilingual-cased_128
01/16/2022 01:26:13 - INFO - __main__ -   ***** Running evaluation  in bg *****
01/16/2022 01:26:13 - INFO - __main__ -     Num examples = 1117
01/16/2022 01:26:13 - INFO - __main__ -     Batch size = 32
01/16/2022 01:26:13 - INFO - __main__ -   Batch number = 1
01/16/2022 01:26:14 - INFO - __main__ -   Batch number = 2
01/16/2022 01:26:14 - INFO - __main__ -   Batch number = 3
01/16/2022 01:26:14 - INFO - __main__ -   Batch number = 4
01/16/2022 01:26:14 - INFO - __main__ -   Batch number = 5
01/16/2022 01:26:15 - INFO - __main__ -   Batch number = 6
01/16/2022 01:26:15 - INFO - __main__ -   Batch number = 7
01/16/2022 01:26:15 - INFO - __main__ -   Batch number = 8
01/16/2022 01:26:16 - INFO - __main__ -   Batch number = 9
01/16/2022 01:26:16 - INFO - __main__ -   Batch number = 10
01/16/2022 01:26:16 - INFO - __main__ -   Batch number = 11
01/16/2022 01:26:16 - INFO - __main__ -   Batch number = 12
01/16/2022 01:26:17 - INFO - __main__ -   Batch number = 13
01/16/2022 01:26:17 - INFO - __main__ -   Batch number = 14
01/16/2022 01:26:17 - INFO - __main__ -   Batch number = 15
01/16/2022 01:26:18 - INFO - __main__ -   Batch number = 16
01/16/2022 01:26:18 - INFO - __main__ -   Batch number = 17
01/16/2022 01:26:18 - INFO - __main__ -   Batch number = 18
01/16/2022 01:26:18 - INFO - __main__ -   Batch number = 19
01/16/2022 01:26:19 - INFO - __main__ -   Batch number = 20
01/16/2022 01:26:19 - INFO - __main__ -   Batch number = 21
01/16/2022 01:26:19 - INFO - __main__ -   Batch number = 22
01/16/2022 01:26:19 - INFO - __main__ -   Batch number = 23
01/16/2022 01:26:20 - INFO - __main__ -   Batch number = 24
01/16/2022 01:26:20 - INFO - __main__ -   Batch number = 25
01/16/2022 01:26:20 - INFO - __main__ -   Batch number = 26
01/16/2022 01:26:21 - INFO - __main__ -   Batch number = 27
01/16/2022 01:26:21 - INFO - __main__ -   Batch number = 28
01/16/2022 01:26:21 - INFO - __main__ -   Batch number = 29
01/16/2022 01:26:21 - INFO - __main__ -   Batch number = 30
01/16/2022 01:26:22 - INFO - __main__ -   Batch number = 31
01/16/2022 01:26:22 - INFO - __main__ -   Batch number = 32
01/16/2022 01:26:22 - INFO - __main__ -   Batch number = 33
01/16/2022 01:26:23 - INFO - __main__ -   Batch number = 34
01/16/2022 01:26:23 - INFO - __main__ -   Batch number = 35
01/16/2022 01:26:23 - INFO - __main__ -   ***** Evaluation result  in bg *****
01/16/2022 01:26:23 - INFO - __main__ -     f1 = 0.8471199653529667
01/16/2022 01:26:23 - INFO - __main__ -     loss = 0.6225800965513502
01/16/2022 01:26:23 - INFO - __main__ -     precision = 0.8495255913403715
01/16/2022 01:26:23 - INFO - __main__ -     recall = 0.8447279250548136
01/16/2022 01:26:26 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bg', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/16/2022 01:26:26 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
01/16/2022 01:26:26 - INFO - __main__ -   Seed = 3
01/16/2022 01:26:26 - INFO - root -   save model
01/16/2022 01:26:26 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bg', train_langs='en', log_file='/home/abhijeet/rohan/cloud-emea-copy/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_geo_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='equal', lang_to_vec=None, calc_weight_step=0, predict_save_prefix='', en_weight=None, temperature=1.0, get_attr=False, topk=1, task='udpos')
01/16/2022 01:26:26 - INFO - __main__ -   Loading pretrained model and tokenizer
01/16/2022 01:26:28 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
01/16/2022 01:26:34 - INFO - __main__ -   Using lang2id = None
01/16/2022 01:26:34 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
01/16/2022 01:26:34 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
01/16/2022 01:26:34 - INFO - root -   Trying to decide if add adapter
01/16/2022 01:26:34 - INFO - root -   loading task adapter
01/16/2022 01:26:34 - INFO - root -   loading lang adpater el/wiki@ukp,hu/wiki@ukp,tr/wiki@ukp,la/wiki@ukp,cs/wiki@ukp,de/wiki@ukp,xmf/wiki@ukp,lv/wiki@ukp,hy/wiki@ukp,ka/wiki@ukp
01/16/2022 01:26:34 - INFO - __main__ -   Adapter Languages : ['el', 'hu', 'tr', 'la', 'cs', 'de', 'xmf', 'lv', 'hy', 'ka'], Length : 10
01/16/2022 01:26:34 - INFO - __main__ -   Adapter Names ['el/wiki@ukp', 'hu/wiki@ukp', 'tr/wiki@ukp', 'la/wiki@ukp', 'cs/wiki@ukp', 'de/wiki@ukp', 'xmf/wiki@ukp', 'lv/wiki@ukp', 'hy/wiki@ukp', 'ka/wiki@ukp'], Length : 10
01/16/2022 01:26:34 - INFO - __main__ -   Language = el
01/16/2022 01:26:34 - INFO - __main__ -   Adapter Name = el/wiki@ukp
01/16/2022 01:26:35 - INFO - __main__ -   Language = hu
01/16/2022 01:26:35 - INFO - __main__ -   Adapter Name = hu/wiki@ukp
01/16/2022 01:26:37 - INFO - __main__ -   Language = tr
01/16/2022 01:26:37 - INFO - __main__ -   Adapter Name = tr/wiki@ukp
01/16/2022 01:26:39 - INFO - __main__ -   Language = la
01/16/2022 01:26:39 - INFO - __main__ -   Adapter Name = la/wiki@ukp
01/16/2022 01:26:41 - INFO - __main__ -   Language = cs
01/16/2022 01:26:41 - INFO - __main__ -   Adapter Name = cs/wiki@ukp
01/16/2022 01:26:43 - INFO - __main__ -   Language = de
01/16/2022 01:26:43 - INFO - __main__ -   Adapter Name = de/wiki@ukp
01/16/2022 01:26:45 - INFO - __main__ -   Language = xmf
01/16/2022 01:26:45 - INFO - __main__ -   Adapter Name = xmf/wiki@ukp
01/16/2022 01:26:45 - INFO - __main__ -   Language = lv
01/16/2022 01:26:45 - INFO - __main__ -   Adapter Name = lv/wiki@ukp
01/16/2022 01:26:48 - INFO - __main__ -   Language = hy
01/16/2022 01:26:48 - INFO - __main__ -   Adapter Name = hy/wiki@ukp
01/16/2022 01:26:50 - INFO - __main__ -   Language = ka
01/16/2022 01:26:50 - INFO - __main__ -   Adapter Name = ka/wiki@ukp
01/16/2022 01:26:55 - INFO - __main__ -   Args Adapter Weight = equal
01/16/2022 01:26:55 - INFO - __main__ -   Adapter Languages = ['el', 'hu', 'tr', 'la', 'cs', 'de', 'xmf', 'lv', 'hy', 'ka']
01/16/2022 01:26:55 - INFO - __main__ -   Adapter Weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
01/16/2022 01:26:55 - INFO - __main__ -   Sum of Adapter Weights = 0.9999999999999999
01/16/2022 01:26:55 - INFO - __main__ -   Length of Adapter Weights = 10
01/16/2022 01:26:55 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/cloud-emea-copy/data//udpos/udpos_processed_maxlen128/cached_test_bg_bert-base-multilingual-cased_128
01/16/2022 01:26:55 - INFO - __main__ -   ***** Running evaluation  in bg *****
01/16/2022 01:26:55 - INFO - __main__ -     Num examples = 1117
01/16/2022 01:26:55 - INFO - __main__ -     Batch size = 32
01/16/2022 01:26:55 - INFO - __main__ -   Batch number = 1
01/16/2022 01:26:56 - INFO - __main__ -   Batch number = 2
01/16/2022 01:26:56 - INFO - __main__ -   Batch number = 3
01/16/2022 01:26:56 - INFO - __main__ -   Batch number = 4
01/16/2022 01:26:56 - INFO - __main__ -   Batch number = 5
01/16/2022 01:26:57 - INFO - __main__ -   Batch number = 6
01/16/2022 01:26:57 - INFO - __main__ -   Batch number = 7
01/16/2022 01:26:57 - INFO - __main__ -   Batch number = 8
01/16/2022 01:26:58 - INFO - __main__ -   Batch number = 9
01/16/2022 01:26:58 - INFO - __main__ -   Batch number = 10
01/16/2022 01:26:58 - INFO - __main__ -   Batch number = 11
01/16/2022 01:26:58 - INFO - __main__ -   Batch number = 12
01/16/2022 01:26:59 - INFO - __main__ -   Batch number = 13
01/16/2022 01:26:59 - INFO - __main__ -   Batch number = 14
01/16/2022 01:26:59 - INFO - __main__ -   Batch number = 15
01/16/2022 01:26:59 - INFO - __main__ -   Batch number = 16
01/16/2022 01:27:00 - INFO - __main__ -   Batch number = 17
01/16/2022 01:27:00 - INFO - __main__ -   Batch number = 18
01/16/2022 01:27:00 - INFO - __main__ -   Batch number = 19
01/16/2022 01:27:01 - INFO - __main__ -   Batch number = 20
01/16/2022 01:27:01 - INFO - __main__ -   Batch number = 21
01/16/2022 01:27:01 - INFO - __main__ -   Batch number = 22
01/16/2022 01:27:01 - INFO - __main__ -   Batch number = 23
01/16/2022 01:27:02 - INFO - __main__ -   Batch number = 24
01/16/2022 01:27:02 - INFO - __main__ -   Batch number = 25
01/16/2022 01:27:02 - INFO - __main__ -   Batch number = 26
01/16/2022 01:27:03 - INFO - __main__ -   Batch number = 27
01/16/2022 01:27:03 - INFO - __main__ -   Batch number = 28
01/16/2022 01:27:03 - INFO - __main__ -   Batch number = 29
01/16/2022 01:27:03 - INFO - __main__ -   Batch number = 30
01/16/2022 01:27:04 - INFO - __main__ -   Batch number = 31
01/16/2022 01:27:04 - INFO - __main__ -   Batch number = 32
01/16/2022 01:27:04 - INFO - __main__ -   Batch number = 33
01/16/2022 01:27:04 - INFO - __main__ -   Batch number = 34
01/16/2022 01:27:05 - INFO - __main__ -   Batch number = 35
01/16/2022 01:27:05 - INFO - __main__ -   ***** Evaluation result  in bg *****
01/16/2022 01:27:05 - INFO - __main__ -     f1 = 0.8531314613486294
01/16/2022 01:27:05 - INFO - __main__ -     loss = 0.5524335622787475
01/16/2022 01:27:05 - INFO - __main__ -     precision = 0.8564445932373619
01/16/2022 01:27:05 - INFO - __main__ -     recall = 0.8498438641950701
